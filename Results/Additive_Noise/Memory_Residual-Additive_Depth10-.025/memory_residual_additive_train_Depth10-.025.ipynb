{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.025)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.025)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.025)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.025)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.29226484093401167\n",
      "Average test loss: 0.014470576970113648\n",
      "Epoch 2/300\n",
      "Average training loss: 0.10718460506863065\n",
      "Average test loss: 0.01159836443596416\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08579908765024609\n",
      "Average test loss: 0.014052307860718833\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07687689756684833\n",
      "Average test loss: 0.009344310615625646\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07065223094489839\n",
      "Average test loss: 0.013758775475124519\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0651403179499838\n",
      "Average test loss: 0.009775422536664538\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06331278208229277\n",
      "Average test loss: 0.009309874988264508\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0592847058640586\n",
      "Average test loss: 0.009526011659453312\n",
      "Epoch 9/300\n",
      "Average training loss: 0.057672742472754585\n",
      "Average test loss: 0.00903537759060661\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05527841803100374\n",
      "Average test loss: 0.009431371692981985\n",
      "Epoch 11/300\n",
      "Average training loss: 0.052883171336518396\n",
      "Average test loss: 0.008588704219294918\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05137976892126931\n",
      "Average test loss: 0.00880430910570754\n",
      "Epoch 13/300\n",
      "Average training loss: 0.049756074398756026\n",
      "Average test loss: 0.007585988421407011\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04850035029980872\n",
      "Average test loss: 0.010020496525698239\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04718425494763586\n",
      "Average test loss: 0.015563045240110821\n",
      "Epoch 16/300\n",
      "Average training loss: 0.045411868813965056\n",
      "Average test loss: 0.010085480380389426\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04403571346733305\n",
      "Average test loss: 0.007382813442498445\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04293465741144286\n",
      "Average test loss: 0.006996411932839288\n",
      "Epoch 19/300\n",
      "Average training loss: 0.042295698631140924\n",
      "Average test loss: 0.011727602476874988\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04149429831239912\n",
      "Average test loss: 0.01389503867427508\n",
      "Epoch 21/300\n",
      "Average training loss: 0.040579012513160706\n",
      "Average test loss: 0.00676758227787084\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03995342135760519\n",
      "Average test loss: 0.006864397766275538\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0393364776538478\n",
      "Average test loss: 0.006801743028064569\n",
      "Epoch 24/300\n",
      "Average training loss: 0.038830499000019496\n",
      "Average test loss: 0.006579646757079495\n",
      "Epoch 25/300\n",
      "Average training loss: 0.038203328354491126\n",
      "Average test loss: 0.006519660094959868\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03792508745690187\n",
      "Average test loss: 0.006478081154119637\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03749283068047629\n",
      "Average test loss: 0.023467033925983642\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03717134439614084\n",
      "Average test loss: 0.007140806095053753\n",
      "Epoch 29/300\n",
      "Average training loss: 0.036741748289929495\n",
      "Average test loss: 0.24452222759856118\n",
      "Epoch 30/300\n",
      "Average training loss: 0.036296423044469624\n",
      "Average test loss: 0.006717334967520502\n",
      "Epoch 31/300\n",
      "Average training loss: 0.035917107823822236\n",
      "Average test loss: 0.006443091188040044\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03567325051956707\n",
      "Average test loss: 0.006540470904774136\n",
      "Epoch 33/300\n",
      "Average training loss: 0.035694312079085244\n",
      "Average test loss: 0.006610799566325214\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03509117979473538\n",
      "Average test loss: 0.006318662376039558\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03479446777370241\n",
      "Average test loss: 0.006325826522790724\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0346178969376617\n",
      "Average test loss: 0.009629088209735022\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0345509947521819\n",
      "Average test loss: 0.006300071602066358\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03441993850138452\n",
      "Average test loss: 0.006462543294661575\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0339735310210122\n",
      "Average test loss: 0.006211723942723539\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03378766366508272\n",
      "Average test loss: 0.007005666414896647\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0335590464009179\n",
      "Average test loss: 0.006693670833276378\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03342811044388347\n",
      "Average test loss: 0.008202508966955874\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03322337796952989\n",
      "Average test loss: 0.006240970069335567\n",
      "Epoch 44/300\n",
      "Average training loss: 0.033134080529212954\n",
      "Average test loss: 0.008576350268390443\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03293708341320356\n",
      "Average test loss: 0.006259284457398786\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03273668190174633\n",
      "Average test loss: 0.008563102915469143\n",
      "Epoch 47/300\n",
      "Average training loss: 0.032623098694615896\n",
      "Average test loss: 0.0063783242561750945\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03258275870978832\n",
      "Average test loss: 0.006593301117627157\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03246848895483547\n",
      "Average test loss: 0.006834784168336126\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03218526206082768\n",
      "Average test loss: 0.006616505747040113\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03212038125594457\n",
      "Average test loss: 0.0073480216740734045\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0319921385579639\n",
      "Average test loss: 0.006162950093133582\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03182080389890406\n",
      "Average test loss: 0.0075342806867427295\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03177418887284067\n",
      "Average test loss: 0.010568622679346138\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03157827047010263\n",
      "Average test loss: 0.006931944508933359\n",
      "Epoch 56/300\n",
      "Average training loss: 0.031529428078068626\n",
      "Average test loss: 0.006857040465292004\n",
      "Epoch 57/300\n",
      "Average training loss: 0.031434586024946636\n",
      "Average test loss: 0.006356891039758921\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03135195177131229\n",
      "Average test loss: 0.006467406480676598\n",
      "Epoch 59/300\n",
      "Average training loss: 0.031230110786027377\n",
      "Average test loss: 0.00628991181568967\n",
      "Epoch 60/300\n",
      "Average training loss: 0.031166216535700692\n",
      "Average test loss: 0.006516009619252549\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03105863757431507\n",
      "Average test loss: 0.006363962449133396\n",
      "Epoch 62/300\n",
      "Average training loss: 0.030934321559137768\n",
      "Average test loss: 0.007469253571083148\n",
      "Epoch 63/300\n",
      "Average training loss: 0.030838284100095432\n",
      "Average test loss: 0.006780984931108024\n",
      "Epoch 64/300\n",
      "Average training loss: 0.030775663144058652\n",
      "Average test loss: 0.006522419678668181\n",
      "Epoch 65/300\n",
      "Average training loss: 0.030694677501916887\n",
      "Average test loss: 0.006651339694857598\n",
      "Epoch 66/300\n",
      "Average training loss: 0.030633065150843727\n",
      "Average test loss: 0.006914675775087542\n",
      "Epoch 67/300\n",
      "Average training loss: 0.030548648827605777\n",
      "Average test loss: 0.006539498514599271\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03044504635863834\n",
      "Average test loss: 0.006255378110127317\n",
      "Epoch 69/300\n",
      "Average training loss: 0.030371720073123772\n",
      "Average test loss: 0.011781423681312138\n",
      "Epoch 70/300\n",
      "Average training loss: 0.030076335973209804\n",
      "Average test loss: 0.006314913464917077\n",
      "Epoch 75/300\n",
      "Average training loss: 0.029999989731444254\n",
      "Average test loss: 0.006225638239747948\n",
      "Epoch 76/300\n",
      "Average training loss: 0.029917729215489495\n",
      "Average test loss: 0.0064680558343728385\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02987921342584822\n",
      "Average test loss: 0.006295619235270553\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02976001293791665\n",
      "Average test loss: 0.008556843158271578\n",
      "Epoch 79/300\n",
      "Average training loss: 0.029702809820572534\n",
      "Average test loss: 0.006680889485610856\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02966776143014431\n",
      "Average test loss: 0.006236468918207619\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02959231631623374\n",
      "Average test loss: 0.00655930421832535\n",
      "Epoch 82/300\n",
      "Average training loss: 0.037765324774715635\n",
      "Average test loss: 0.0068729142828120125\n",
      "Epoch 84/300\n",
      "Average training loss: 0.032543139331870606\n",
      "Average test loss: 0.007103010461562209\n",
      "Epoch 85/300\n",
      "Average training loss: 0.030141069200303818\n",
      "Average test loss: 0.006323338785933124\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02958124785953098\n",
      "Average test loss: 0.006164984960108996\n",
      "Epoch 87/300\n",
      "Average training loss: 0.029206848017043537\n",
      "Average test loss: 0.006252699523750279\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02918099072906706\n",
      "Average test loss: 0.007306126437253422\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02906602821747462\n",
      "Average test loss: 0.006324258610192273\n",
      "Epoch 94/300\n",
      "Average training loss: 0.029014188922113843\n",
      "Average test loss: 0.006525731500652101\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02907344009478887\n",
      "Average test loss: 0.007158758737560776\n",
      "Epoch 96/300\n",
      "Average training loss: 0.029099953687853283\n",
      "Average test loss: 0.006248878117236826\n",
      "Epoch 97/300\n",
      "Average training loss: 0.028918870700730216\n",
      "Average test loss: 0.007280742265284062\n",
      "Epoch 98/300\n",
      "Average training loss: 0.028839769118362002\n",
      "Average test loss: 0.01107222894165251\n",
      "Epoch 99/300\n",
      "Average training loss: 0.028905308292971717\n",
      "Average test loss: 0.00632209808462196\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02885419742597474\n",
      "Average test loss: 0.008815839224391513\n",
      "Epoch 101/300\n",
      "Average training loss: 0.028784416298071543\n",
      "Average test loss: 0.006181586984958913\n",
      "Epoch 102/300\n",
      "Average training loss: 0.028854246906108327\n",
      "Average test loss: 0.006608107491913769\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02861457602183024\n",
      "Average test loss: 0.00630760615815719\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02863584574063619\n",
      "Average test loss: 0.006531637041519086\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02862850232919057\n",
      "Average test loss: 0.006290775860763259\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02858781285583973\n",
      "Average test loss: 0.006839648691316445\n",
      "Epoch 107/300\n",
      "Average training loss: 0.028562796683775055\n",
      "Average test loss: 0.00664686569198966\n",
      "Epoch 108/300\n",
      "Average training loss: 0.028508331732617485\n",
      "Average test loss: 0.00653585847467184\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02844024452732669\n",
      "Average test loss: 0.010630658062795798\n",
      "Epoch 110/300\n",
      "Average training loss: 0.028340626479850875\n",
      "Average test loss: 0.007069419745769766\n",
      "Epoch 111/300\n",
      "Average training loss: 0.028448656630184914\n",
      "Average test loss: 0.006590882405638695\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02829278431998359\n",
      "Average test loss: 0.007011280095825593\n",
      "Epoch 113/300\n",
      "Average training loss: 0.028264530402090815\n",
      "Average test loss: 0.006859208355347315\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028263495248225\n",
      "Average test loss: 0.0062496407251391144\n",
      "Epoch 115/300\n",
      "Average training loss: 0.028259994922412768\n",
      "Average test loss: 0.007433085405578216\n",
      "Epoch 116/300\n",
      "Average training loss: 0.028265997413131926\n",
      "Average test loss: 0.0064516690894961355\n",
      "Epoch 117/300\n",
      "Average training loss: 0.028230409844054116\n",
      "Average test loss: 0.006507844766808881\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02814819940427939\n",
      "Average test loss: 0.006679521178205808\n",
      "Epoch 119/300\n",
      "Average training loss: 0.028092631389697392\n",
      "Average test loss: 0.041780870699220235\n",
      "Epoch 120/300\n",
      "Average training loss: 0.028105767879221175\n",
      "Average test loss: 0.006664180764307579\n",
      "Epoch 121/300\n",
      "Average training loss: 0.028079461670584147\n",
      "Average test loss: 0.0065569463707506655\n",
      "Epoch 122/300\n",
      "Average training loss: 0.027898019942972394\n",
      "Average test loss: 0.0066287615543438325\n",
      "Epoch 127/300\n",
      "Average training loss: 0.027800297465589312\n",
      "Average test loss: 0.006345296633326345\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027837175091107686\n",
      "Average test loss: 0.006692539387279086\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02778939052919547\n",
      "Average test loss: 0.0070820547060834035\n",
      "Epoch 130/300\n",
      "Average training loss: 0.027772297534677717\n",
      "Average test loss: 0.006243048196037611\n",
      "Epoch 131/300\n",
      "Average training loss: 0.027736500249968633\n",
      "Average test loss: 0.006535086107336813\n",
      "Epoch 132/300\n",
      "Average training loss: 0.027744320183992387\n",
      "Average test loss: 0.00632925892331534\n",
      "Epoch 133/300\n",
      "Average training loss: 0.027706722940007845\n",
      "Average test loss: 0.00632567174360156\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0277097223682536\n",
      "Average test loss: 0.0067094878297713064\n",
      "Epoch 135/300\n",
      "Average training loss: 0.027716683606306712\n",
      "Average test loss: 0.007328915893203682\n",
      "Epoch 136/300\n",
      "Average training loss: 0.027568940546777512\n",
      "Average test loss: 0.006558406596382459\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02761426556441519\n",
      "Average test loss: 0.006817165482789278\n",
      "Epoch 138/300\n",
      "Average training loss: 0.027557669035262532\n",
      "Average test loss: 0.006526510239475303\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02748454608519872\n",
      "Average test loss: 0.006335014075040817\n",
      "Epoch 140/300\n",
      "Average training loss: 0.027522547605964873\n",
      "Average test loss: 0.0067460238205062015\n",
      "Epoch 141/300\n",
      "Average training loss: 0.027478127400080363\n",
      "Average test loss: 0.007897483210182852\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02743994896941715\n",
      "Average test loss: 0.006347409528990587\n",
      "Epoch 143/300\n",
      "Average training loss: 0.027324926488929323\n",
      "Average test loss: 0.0063309548824197715\n",
      "Epoch 144/300\n",
      "Average training loss: 0.027478877463274532\n",
      "Average test loss: 0.006941750263588296\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02735881406068802\n",
      "Average test loss: 0.006360678156216939\n",
      "Epoch 146/300\n",
      "Average training loss: 0.027310875142614047\n",
      "Average test loss: 0.006618110796643628\n",
      "Epoch 147/300\n",
      "Average training loss: 0.027323086380958556\n",
      "Average test loss: 0.0063920072373002765\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02739391603734758\n",
      "Average test loss: 0.006955604135576222\n",
      "Epoch 149/300\n",
      "Average training loss: 0.027330147923694717\n",
      "Average test loss: 0.007413714028894901\n",
      "Epoch 150/300\n",
      "Average training loss: 0.027229323907030954\n",
      "Average test loss: 0.0070271059088408945\n",
      "Epoch 151/300\n",
      "Average training loss: 0.027223608856399854\n",
      "Average test loss: 0.0067026554546836346\n",
      "Epoch 152/300\n",
      "Average training loss: 0.027300743755367066\n",
      "Average test loss: 0.008940690732664533\n",
      "Epoch 153/300\n",
      "Average training loss: 0.027155035409662458\n",
      "Average test loss: 0.006537977147433493\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0271074149939749\n",
      "Average test loss: 0.006542375105950567\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02710505805744065\n",
      "Average test loss: 0.00655161175918248\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02713520904713207\n",
      "Average test loss: 0.006659830096695158\n",
      "Epoch 157/300\n",
      "Average training loss: 0.027215491314729053\n",
      "Average test loss: 0.010314177042080297\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02701859926349587\n",
      "Average test loss: 0.007424786841703786\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02713916249407662\n",
      "Average test loss: 0.006457168667680688\n",
      "Epoch 160/300\n",
      "Average training loss: 0.028757954518000284\n",
      "Average test loss: 0.006593321854041682\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0269971917602751\n",
      "Average test loss: 0.006455072039945258\n",
      "Epoch 162/300\n",
      "Average training loss: 0.026940063567625152\n",
      "Average test loss: 0.006556876158962647\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02695752650499344\n",
      "Average test loss: 0.006492625868568818\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026985074528389507\n",
      "Average test loss: 0.006918372788363033\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02695189556810591\n",
      "Average test loss: 0.009292586034370793\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02697511685391267\n",
      "Average test loss: 0.008173845927748415\n",
      "Epoch 167/300\n",
      "Average training loss: 0.026907708236740695\n",
      "Average test loss: 0.007375032271362014\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02695983156396283\n",
      "Average test loss: 0.0070490132326053246\n",
      "Epoch 169/300\n",
      "Average training loss: 0.026850781046681933\n",
      "Average test loss: 0.00656230581800143\n",
      "Epoch 170/300\n",
      "Average training loss: 0.026905240015851126\n",
      "Average test loss: 0.007187407228267855\n",
      "Epoch 171/300\n",
      "Average training loss: 0.026857668805453514\n",
      "Average test loss: 0.006878026866664489\n",
      "Epoch 172/300\n",
      "Average training loss: 0.026787209279007382\n",
      "Average test loss: 0.0066898255190915535\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0268504667977492\n",
      "Average test loss: 0.0070193259422149925\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026822631703482733\n",
      "Average test loss: 0.006822837470720212\n",
      "Epoch 175/300\n",
      "Average training loss: 0.026827616141902075\n",
      "Average test loss: 0.006548705128331979\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026846684403717517\n",
      "Average test loss: 0.0069916462832027015\n",
      "Epoch 177/300\n",
      "Average training loss: 0.026738958583937752\n",
      "Average test loss: 0.006643356822017167\n",
      "Epoch 178/300\n",
      "Average training loss: 0.026740891898671786\n",
      "Average test loss: 0.006568695125687454\n",
      "Epoch 179/300\n",
      "Average training loss: 0.026704291731119156\n",
      "Average test loss: 0.0064621778072582355\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026715620825688045\n",
      "Average test loss: 0.006679192713979217\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026716468051075935\n",
      "Average test loss: 0.0064036236223247315\n",
      "Epoch 182/300\n",
      "Average training loss: 0.026667320056094065\n",
      "Average test loss: 0.006573713121728764\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02670706599454085\n",
      "Average test loss: 0.006437556433180968\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026645927445756064\n",
      "Average test loss: 0.006558044039954742\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0266138778924942\n",
      "Average test loss: 0.007367823088541627\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02661708397169908\n",
      "Average test loss: 0.007318140252182881\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02654984230465359\n",
      "Average test loss: 0.006834714526103603\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02667432955900828\n",
      "Average test loss: 0.007371547452277608\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026661669578817156\n",
      "Average test loss: 0.007003915238711569\n",
      "Epoch 190/300\n",
      "Average training loss: 0.026536104265186523\n",
      "Average test loss: 0.006459870222955942\n",
      "Epoch 191/300\n",
      "Average training loss: 0.026471429934104285\n",
      "Average test loss: 0.00663551680992047\n",
      "Epoch 192/300\n",
      "Average training loss: 0.026546039240227807\n",
      "Average test loss: 0.006484979280994998\n",
      "Epoch 193/300\n",
      "Average training loss: 0.026484589709175957\n",
      "Average test loss: 0.0067769687415825\n",
      "Epoch 194/300\n",
      "Average training loss: 0.026468388575646613\n",
      "Average test loss: 0.006956908255815506\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026545507936014068\n",
      "Average test loss: 0.007787767559289932\n",
      "Epoch 196/300\n",
      "Average training loss: 0.026458738245897822\n",
      "Average test loss: 0.006708182865132888\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026482189575831094\n",
      "Average test loss: 0.006657840078075727\n",
      "Epoch 198/300\n",
      "Average training loss: 0.026435194596648216\n",
      "Average test loss: 0.012571640741493967\n",
      "Epoch 199/300\n",
      "Average training loss: 0.026414895618955294\n",
      "Average test loss: 0.006644472923129797\n",
      "Epoch 200/300\n",
      "Average training loss: 0.026424747480286493\n",
      "Average test loss: 0.006667775502635373\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02638673965467347\n",
      "Average test loss: 0.0065831596847209664\n",
      "Epoch 202/300\n",
      "Average training loss: 0.026415156490272946\n",
      "Average test loss: 0.006590949166152212\n",
      "Epoch 203/300\n",
      "Average training loss: 0.026330468969212636\n",
      "Average test loss: 0.007195830060376061\n",
      "Epoch 204/300\n",
      "Average training loss: 0.026423473452528316\n",
      "Average test loss: 0.006458024420671993\n",
      "Epoch 205/300\n",
      "Average training loss: 0.026280618434151012\n",
      "Average test loss: 0.006733133734514316\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02637067420946227\n",
      "Average test loss: 0.0066564433640903895\n",
      "Epoch 207/300\n",
      "Average training loss: 0.026328052004178364\n",
      "Average test loss: 0.006767545051044888\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02625671258734332\n",
      "Average test loss: 0.006590312382827202\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0262896751380629\n",
      "Average test loss: 0.00669671284324593\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026231932310594452\n",
      "Average test loss: 0.0068073715327514545\n",
      "Epoch 211/300\n",
      "Average training loss: 0.026299965176317427\n",
      "Average test loss: 0.006636852995802959\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026202131978339618\n",
      "Average test loss: 0.006631750884983275\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02623449957039621\n",
      "Average test loss: 0.0066372712850570675\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02621035084625085\n",
      "Average test loss: 0.00659004841827684\n",
      "Epoch 215/300\n",
      "Average training loss: 0.026187952352894676\n",
      "Average test loss: 0.006569563242710299\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026165505246983634\n",
      "Average test loss: 0.006739689699891541\n",
      "Epoch 217/300\n",
      "Average training loss: 0.026172935757372114\n",
      "Average test loss: 0.006708615269511938\n",
      "Epoch 218/300\n",
      "Average training loss: 0.026169252889023886\n",
      "Average test loss: 0.006598953196571933\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02613531382381916\n",
      "Average test loss: 0.0070601113467580745\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026105614281362956\n",
      "Average test loss: 0.00828243693171276\n",
      "Epoch 221/300\n",
      "Average training loss: 0.026144865262839528\n",
      "Average test loss: 0.0070636854304207694\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026119538807206685\n",
      "Average test loss: 0.006614059362974432\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026132754974895055\n",
      "Average test loss: 0.006501049204832978\n",
      "Epoch 224/300\n",
      "Average training loss: 0.026052346823943986\n",
      "Average test loss: 0.0065677022151649\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02610561591386795\n",
      "Average test loss: 0.006564253789683183\n",
      "Epoch 226/300\n",
      "Average training loss: 0.026041678516401184\n",
      "Average test loss: 0.007145673563083013\n",
      "Epoch 227/300\n",
      "Average training loss: 0.026048366917504204\n",
      "Average test loss: 0.0067102301389806805\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02602931199636724\n",
      "Average test loss: 0.006532085485756397\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026053190627031857\n",
      "Average test loss: 0.006555943676994906\n",
      "Epoch 230/300\n",
      "Average training loss: 0.025995611919297112\n",
      "Average test loss: 0.006619330704212189\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026106844719913272\n",
      "Average test loss: 0.007081809479329321\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02602701837817828\n",
      "Average test loss: 0.007110841342144542\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02598688446978728\n",
      "Average test loss: 0.0070036504595643945\n",
      "Epoch 234/300\n",
      "Average training loss: 0.026010571092367173\n",
      "Average test loss: 0.0065946460482147005\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02597170478767819\n",
      "Average test loss: 0.006621845501164595\n",
      "Epoch 236/300\n",
      "Average training loss: 0.025954005072514216\n",
      "Average test loss: 0.006759732981108957\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02594411985576153\n",
      "Average test loss: 0.007250365101214912\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02592540556523535\n",
      "Average test loss: 0.006547333168900675\n",
      "Epoch 239/300\n",
      "Average training loss: 0.025933919961253802\n",
      "Average test loss: 0.007017375610354874\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02591334685848819\n",
      "Average test loss: 0.007258395826650991\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02593513181143337\n",
      "Average test loss: 0.0073555330075323585\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02582030601799488\n",
      "Average test loss: 0.007073873428006967\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02583342926038636\n",
      "Average test loss: 0.0066077675223350524\n",
      "Epoch 244/300\n",
      "Average training loss: 0.025862355493836933\n",
      "Average test loss: 0.006885192696832948\n",
      "Epoch 245/300\n",
      "Average training loss: 0.025866668502489726\n",
      "Average test loss: 0.006499394698689381\n",
      "Epoch 246/300\n",
      "Average training loss: 0.025858877471751637\n",
      "Average test loss: 0.006555655161953635\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02590492086774773\n",
      "Average test loss: 0.007301205198383994\n",
      "Epoch 248/300\n",
      "Average training loss: 0.025756221450037427\n",
      "Average test loss: 0.006540659908826152\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02585738433069653\n",
      "Average test loss: 0.00649371115076873\n",
      "Epoch 250/300\n",
      "Average training loss: 0.025792514719896846\n",
      "Average test loss: 0.007188708327296707\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02578343276348379\n",
      "Average test loss: 0.00671480088846551\n",
      "Epoch 252/300\n",
      "Average training loss: 0.025794174013866317\n",
      "Average test loss: 0.006544438575290972\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02584404895371861\n",
      "Average test loss: 0.006517735631101662\n",
      "Epoch 254/300\n",
      "Average training loss: 0.025801713819305103\n",
      "Average test loss: 0.00658001350859801\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025702327190173995\n",
      "Average test loss: 0.006545496745655934\n",
      "Epoch 256/300\n",
      "Average training loss: 0.025706189594335026\n",
      "Average test loss: 0.00872709274540345\n",
      "Epoch 257/300\n",
      "Average training loss: 0.025734810459944937\n",
      "Average test loss: 0.006701714914292097\n",
      "Epoch 258/300\n",
      "Average training loss: 0.025760831905735863\n",
      "Average test loss: 0.007358881982664267\n",
      "Epoch 259/300\n",
      "Average training loss: 0.025697805012265842\n",
      "Average test loss: 0.006964508567833238\n",
      "Epoch 260/300\n",
      "Average training loss: 0.025702790637811026\n",
      "Average test loss: 0.0068191995359957215\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02569286676413483\n",
      "Average test loss: 0.006913302927381463\n",
      "Epoch 262/300\n",
      "Average training loss: 0.025716490384605196\n",
      "Average test loss: 0.007090736774934663\n",
      "Epoch 263/300\n",
      "Average training loss: 0.025748697267638312\n",
      "Average test loss: 0.0067723934058513905\n",
      "Epoch 264/300\n",
      "Average training loss: 0.025684537667367193\n",
      "Average test loss: 0.00697752414105667\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025647133639289273\n",
      "Average test loss: 0.007035623395608531\n",
      "Epoch 266/300\n",
      "Average training loss: 0.025613648073540794\n",
      "Average test loss: 0.006502408778915803\n",
      "Epoch 267/300\n",
      "Average training loss: 0.025704586633377605\n",
      "Average test loss: 0.0067460731027854815\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02563976068298022\n",
      "Average test loss: 0.006751559152785275\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02562871297531658\n",
      "Average test loss: 0.006601144513322247\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02563273836672306\n",
      "Average test loss: 0.006565621296564738\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025600368802746135\n",
      "Average test loss: 0.006661191624485784\n",
      "Epoch 272/300\n",
      "Average training loss: 0.025609440608157053\n",
      "Average test loss: 0.006935856061677138\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02553907852868239\n",
      "Average test loss: 0.006676233121504387\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02561574484076765\n",
      "Average test loss: 0.006916800045718749\n",
      "Epoch 275/300\n",
      "Average training loss: 0.025568871880571047\n",
      "Average test loss: 0.006877285358806451\n",
      "Epoch 276/300\n",
      "Average training loss: 0.025537596500582165\n",
      "Average test loss: 0.006785839102334447\n",
      "Epoch 277/300\n",
      "Average training loss: 0.025596507989697988\n",
      "Average test loss: 0.0065251252982351515\n",
      "Epoch 278/300\n",
      "Average training loss: 0.025541040342715052\n",
      "Average test loss: 0.006865411868939797\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025505784547991222\n",
      "Average test loss: 0.00679322499781847\n",
      "Epoch 280/300\n",
      "Average training loss: 0.025516012821760442\n",
      "Average test loss: 0.006798388722456164\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02554486747748322\n",
      "Average test loss: 0.0069050557961066565\n",
      "Epoch 282/300\n",
      "Average training loss: 0.025465551677677365\n",
      "Average test loss: 0.006616357241239813\n",
      "Epoch 283/300\n",
      "Average training loss: 0.025559405624866485\n",
      "Average test loss: 0.006779461249295208\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02549721384048462\n",
      "Average test loss: 0.007072359649671449\n",
      "Epoch 285/300\n",
      "Average training loss: 0.025560222044587137\n",
      "Average test loss: 0.006612119701587492\n",
      "Epoch 286/300\n",
      "Average training loss: 0.025447456315159798\n",
      "Average test loss: 0.0067413192350003455\n",
      "Epoch 287/300\n",
      "Average training loss: 0.025494347789221338\n",
      "Average test loss: 0.006633742649522093\n",
      "Epoch 288/300\n",
      "Average training loss: 0.025371853234039413\n",
      "Average test loss: 0.006694965281006363\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02543279994527499\n",
      "Average test loss: 0.006703369765232007\n",
      "Epoch 290/300\n",
      "Average training loss: 0.025484434607128303\n",
      "Average test loss: 0.006887960681898726\n",
      "Epoch 291/300\n",
      "Average training loss: 0.025399525759948624\n",
      "Average test loss: 0.008156916893190808\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02541146722104814\n",
      "Average test loss: 0.007432318512764242\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025432516341408095\n",
      "Average test loss: 0.0068479425145520105\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025342492310537233\n",
      "Average test loss: 0.006786438316934639\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02540646467109521\n",
      "Average test loss: 0.0078072735476824974\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025366461325022908\n",
      "Average test loss: 0.006776168879949384\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025455150900615585\n",
      "Average test loss: 0.006721289865672588\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025419038946429887\n",
      "Average test loss: 0.007889238756563928\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025411942942274942\n",
      "Average test loss: 0.006862905470861329\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02535990135702822\n",
      "Average test loss: 0.006655070382273859\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.27012226144472756\n",
      "Average test loss: 0.01756122739613056\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09638345639573204\n",
      "Average test loss: 0.009280022048287921\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07415699821048313\n",
      "Average test loss: 0.008839710688425435\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06389097060097589\n",
      "Average test loss: 0.008072164114150735\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05760715792576472\n",
      "Average test loss: 0.007688942787547906\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05265102410647604\n",
      "Average test loss: 0.005822045887509982\n",
      "Epoch 7/300\n",
      "Average training loss: 0.049274035467041864\n",
      "Average test loss: 0.005491100069135427\n",
      "Epoch 8/300\n",
      "Average training loss: 0.046792217324177425\n",
      "Average test loss: 0.005753532344268428\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04388082919518153\n",
      "Average test loss: 0.03324745363990466\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04174324384993977\n",
      "Average test loss: 0.005419068459007475\n",
      "Epoch 11/300\n",
      "Average training loss: 0.039505105432536866\n",
      "Average test loss: 0.0056467949503825774\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03827628122766813\n",
      "Average test loss: 0.0049799596692125005\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03662416019704607\n",
      "Average test loss: 0.0048186107431021\n",
      "Epoch 14/300\n",
      "Average training loss: 0.035259410806828075\n",
      "Average test loss: 0.004776291970991426\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03395890232258373\n",
      "Average test loss: 0.005074701937122478\n",
      "Epoch 16/300\n",
      "Average training loss: 0.032839663488997355\n",
      "Average test loss: 0.00455156629698144\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03160274128946993\n",
      "Average test loss: 0.007067044124421146\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030882553613848156\n",
      "Average test loss: 0.00450401899466912\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03000156712697612\n",
      "Average test loss: 0.006048256697340144\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02926204025414255\n",
      "Average test loss: 0.004492288346919749\n",
      "Epoch 21/300\n",
      "Average training loss: 0.028721382366286383\n",
      "Average test loss: 0.00458456085373958\n",
      "Epoch 22/300\n",
      "Average training loss: 0.028278937775227758\n",
      "Average test loss: 0.004084425818588999\n",
      "Epoch 23/300\n",
      "Average training loss: 0.027732807270354695\n",
      "Average test loss: 0.004242575617714061\n",
      "Epoch 24/300\n",
      "Average training loss: 0.027568133236633405\n",
      "Average test loss: 0.004280024535126156\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026916874471637937\n",
      "Average test loss: 0.004435598833900359\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02665865550438563\n",
      "Average test loss: 0.004078974593844678\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02639225242204136\n",
      "Average test loss: 0.003906030750936932\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025967110729879804\n",
      "Average test loss: 0.003961147011361188\n",
      "Epoch 29/300\n",
      "Average training loss: 0.025760305073526172\n",
      "Average test loss: 0.004078953855567508\n",
      "Epoch 30/300\n",
      "Average training loss: 0.025602628413173886\n",
      "Average test loss: 0.0038308980017900465\n",
      "Epoch 31/300\n",
      "Average training loss: 0.025185430021749604\n",
      "Average test loss: 0.004033234177157283\n",
      "Epoch 32/300\n",
      "Average training loss: 0.025132193192839623\n",
      "Average test loss: 0.003819622014131811\n",
      "Epoch 33/300\n",
      "Average training loss: 0.024931284200814034\n",
      "Average test loss: 0.004082924928102229\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02469182491964764\n",
      "Average test loss: 0.004125990805526575\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024596365509761704\n",
      "Average test loss: 0.003783646170877748\n",
      "Epoch 36/300\n",
      "Average training loss: 0.024390992739134365\n",
      "Average test loss: 0.0037642923523154522\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024247814948360126\n",
      "Average test loss: 0.004197460619939698\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02407555123170217\n",
      "Average test loss: 0.0038317771781649856\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02404353170428011\n",
      "Average test loss: 0.0038201643729375468\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02379467554555999\n",
      "Average test loss: 0.004387404547797309\n",
      "Epoch 41/300\n",
      "Average training loss: 0.023771954433785544\n",
      "Average test loss: 0.003765502222710186\n",
      "Epoch 42/300\n",
      "Average training loss: 0.023658516271246804\n",
      "Average test loss: 0.0037037858064803814\n",
      "Epoch 43/300\n",
      "Average training loss: 0.023462019208404752\n",
      "Average test loss: 0.003870920528554254\n",
      "Epoch 44/300\n",
      "Average training loss: 0.023384241172009046\n",
      "Average test loss: 0.003834514544241958\n",
      "Epoch 45/300\n",
      "Average training loss: 0.023259883764717312\n",
      "Average test loss: 0.0037954088906861014\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023202869868940778\n",
      "Average test loss: 0.0043207446676161556\n",
      "Epoch 47/300\n",
      "Average training loss: 0.023190805984867943\n",
      "Average test loss: 0.0037463323053800396\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023018674991197058\n",
      "Average test loss: 0.003771026181471017\n",
      "Epoch 49/300\n",
      "Average training loss: 0.022914821025398043\n",
      "Average test loss: 0.003812461632407374\n",
      "Epoch 50/300\n",
      "Average training loss: 0.022839121484094196\n",
      "Average test loss: 0.0038483177543514307\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02278886471192042\n",
      "Average test loss: 0.0037610523017744223\n",
      "Epoch 52/300\n",
      "Average training loss: 0.022700215304891268\n",
      "Average test loss: 0.0038084961858888466\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02264665367619859\n",
      "Average test loss: 0.005118047918710443\n",
      "Epoch 54/300\n",
      "Average training loss: 0.022600578902496234\n",
      "Average test loss: 0.0036783053864621455\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0224768089701732\n",
      "Average test loss: 0.003632162179590927\n",
      "Epoch 56/300\n",
      "Average training loss: 0.022409261110756133\n",
      "Average test loss: 0.0041644007389744124\n",
      "Epoch 57/300\n",
      "Average training loss: 0.022350163752834003\n",
      "Average test loss: 0.0037488366663455965\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02227067944407463\n",
      "Average test loss: 0.003789801973849535\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02233099372850524\n",
      "Average test loss: 0.003942292974640926\n",
      "Epoch 60/300\n",
      "Average training loss: 0.022148473268581763\n",
      "Average test loss: 0.0036901781025032204\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022133508087860212\n",
      "Average test loss: 0.0037538525813983546\n",
      "Epoch 62/300\n",
      "Average training loss: 0.022034139927890564\n",
      "Average test loss: 0.003778942899985446\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02200434683263302\n",
      "Average test loss: 0.003721088096085522\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02192747782998615\n",
      "Average test loss: 0.003860477718214194\n",
      "Epoch 65/300\n",
      "Average training loss: 0.021897813397977087\n",
      "Average test loss: 0.003779721862739987\n",
      "Epoch 66/300\n",
      "Average training loss: 0.021858037393954064\n",
      "Average test loss: 0.003666075347405341\n",
      "Epoch 67/300\n",
      "Average training loss: 0.021754634506172606\n",
      "Average test loss: 0.0037945928033441306\n",
      "Epoch 68/300\n",
      "Average training loss: 0.021727284143368403\n",
      "Average test loss: 0.00378953817486763\n",
      "Epoch 69/300\n",
      "Average training loss: 0.021707878126038447\n",
      "Average test loss: 0.003737453401916557\n",
      "Epoch 70/300\n",
      "Average training loss: 0.021628005436725085\n",
      "Average test loss: 0.0038054444307668344\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0215908921311299\n",
      "Average test loss: 0.0038231172549227873\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0215603809191121\n",
      "Average test loss: 0.0038195227972335284\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02153299807343218\n",
      "Average test loss: 0.0038180766188436084\n",
      "Epoch 74/300\n",
      "Average training loss: 0.021498808826009432\n",
      "Average test loss: 0.0038503266589509115\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02144333484189378\n",
      "Average test loss: 0.004466059006750583\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02139641788436307\n",
      "Average test loss: 0.003803572436173757\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02135787426100837\n",
      "Average test loss: 0.005084679914431439\n",
      "Epoch 78/300\n",
      "Average training loss: 0.021283825427293776\n",
      "Average test loss: 0.003802552632159657\n",
      "Epoch 79/300\n",
      "Average training loss: 0.021264278900292186\n",
      "Average test loss: 0.003736853329671754\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02121038549310631\n",
      "Average test loss: 0.003660139321246081\n",
      "Epoch 81/300\n",
      "Average training loss: 0.021222769061724343\n",
      "Average test loss: 0.003697741451362769\n",
      "Epoch 82/300\n",
      "Average training loss: 0.021135113982690704\n",
      "Average test loss: 0.0038520573530760075\n",
      "Epoch 83/300\n",
      "Average training loss: 0.021138006140788396\n",
      "Average test loss: 0.0036674920016278824\n",
      "Epoch 84/300\n",
      "Average training loss: 0.021081596435772048\n",
      "Average test loss: 0.003953621222327153\n",
      "Epoch 85/300\n",
      "Average training loss: 0.021082903972102537\n",
      "Average test loss: 0.003778271664554874\n",
      "Epoch 86/300\n",
      "Average training loss: 0.021043625492188665\n",
      "Average test loss: 0.0036304839708738855\n",
      "Epoch 87/300\n",
      "Average training loss: 0.021002328977816634\n",
      "Average test loss: 0.0045189048225680985\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020925392367773587\n",
      "Average test loss: 0.0038893139478233126\n",
      "Epoch 89/300\n",
      "Average training loss: 0.020937333774235515\n",
      "Average test loss: 0.0037196010692665976\n",
      "Epoch 90/300\n",
      "Average training loss: 0.020844304266903136\n",
      "Average test loss: 0.0038292984883818364\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02086118210852146\n",
      "Average test loss: 0.0037915036645200517\n",
      "Epoch 92/300\n",
      "Average training loss: 0.020844574237863223\n",
      "Average test loss: 0.003996325314458873\n",
      "Epoch 93/300\n",
      "Average training loss: 0.020863032165500853\n",
      "Average test loss: 0.006349515038232009\n",
      "Epoch 94/300\n",
      "Average training loss: 0.020752290949225427\n",
      "Average test loss: 0.0037704252414405347\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020698713956607714\n",
      "Average test loss: 0.003939554505464103\n",
      "Epoch 96/300\n",
      "Average training loss: 0.020731000469790564\n",
      "Average test loss: 0.004023727830913332\n",
      "Epoch 97/300\n",
      "Average training loss: 0.020689651391572424\n",
      "Average test loss: 0.0036909830421209336\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0206266864620977\n",
      "Average test loss: 0.0036626798309799696\n",
      "Epoch 99/300\n",
      "Average training loss: 0.020683070657981768\n",
      "Average test loss: 0.0038832911025318832\n",
      "Epoch 100/300\n",
      "Average training loss: 0.020598386108875273\n",
      "Average test loss: 0.00388605401913325\n",
      "Epoch 101/300\n",
      "Average training loss: 0.020587651333875127\n",
      "Average test loss: 0.003665901372830073\n",
      "Epoch 102/300\n",
      "Average training loss: 0.020505019423034457\n",
      "Average test loss: 0.0037484546701113384\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02051533135606183\n",
      "Average test loss: 0.0038878303085350327\n",
      "Epoch 104/300\n",
      "Average training loss: 0.020512949981623226\n",
      "Average test loss: 0.00633713677773873\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02048294654985269\n",
      "Average test loss: 0.003762198387334744\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02045913825929165\n",
      "Average test loss: 0.003924460921229588\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0204387212610907\n",
      "Average test loss: 0.004122346854251292\n",
      "Epoch 108/300\n",
      "Average training loss: 0.020420681666996746\n",
      "Average test loss: 0.003701046414880289\n",
      "Epoch 109/300\n",
      "Average training loss: 0.020402531330784162\n",
      "Average test loss: 0.003748675779542989\n",
      "Epoch 110/300\n",
      "Average training loss: 0.020308653655979367\n",
      "Average test loss: 0.005484507827295197\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02037337673786614\n",
      "Average test loss: 0.0039009749591350554\n",
      "Epoch 112/300\n",
      "Average training loss: 0.020303439112173187\n",
      "Average test loss: 0.003796530332002375\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02025094291402234\n",
      "Average test loss: 0.003703955323745807\n",
      "Epoch 114/300\n",
      "Average training loss: 0.020272622992595038\n",
      "Average test loss: 0.0038536126367333863\n",
      "Epoch 115/300\n",
      "Average training loss: 0.020267575752404\n",
      "Average test loss: 0.004080958488086859\n",
      "Epoch 116/300\n",
      "Average training loss: 0.020233224819103876\n",
      "Average test loss: 0.0038025695039994186\n",
      "Epoch 117/300\n",
      "Average training loss: 0.020215908610158498\n",
      "Average test loss: 0.003984891852156984\n",
      "Epoch 118/300\n",
      "Average training loss: 0.020194577563140127\n",
      "Average test loss: 0.004283454949657122\n",
      "Epoch 119/300\n",
      "Average training loss: 0.020222878721025257\n",
      "Average test loss: 0.004806268599298265\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02012759730219841\n",
      "Average test loss: 0.0039951264270477824\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02009429663254155\n",
      "Average test loss: 0.006919728894614511\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02008575988643699\n",
      "Average test loss: 0.0037720848437812595\n",
      "Epoch 123/300\n",
      "Average training loss: 0.020087573155760766\n",
      "Average test loss: 0.0038814703313012917\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02005286888115936\n",
      "Average test loss: 0.0037260345617930093\n",
      "Epoch 125/300\n",
      "Average training loss: 0.020028137831224334\n",
      "Average test loss: 0.003943125973559088\n",
      "Epoch 126/300\n",
      "Average training loss: 0.020030204282866584\n",
      "Average test loss: 0.0038214832502934666\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02003735979894797\n",
      "Average test loss: 0.00376960890326235\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019948007865084542\n",
      "Average test loss: 0.003943038136801786\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019951563485794598\n",
      "Average test loss: 0.0037921108092284863\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01993907904956076\n",
      "Average test loss: 0.003914033417279522\n",
      "Epoch 131/300\n",
      "Average training loss: 0.019969224193029933\n",
      "Average test loss: 0.0037964024792114894\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01985887595017751\n",
      "Average test loss: 0.0040051197401351395\n",
      "Epoch 133/300\n",
      "Average training loss: 0.019936923977401523\n",
      "Average test loss: 0.003786622903827164\n",
      "Epoch 134/300\n",
      "Average training loss: 0.019876020272572837\n",
      "Average test loss: 0.0038056480199512507\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0198369470735391\n",
      "Average test loss: 0.0041267051171097495\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019833000719547272\n",
      "Average test loss: 0.003756535162114435\n",
      "Epoch 137/300\n",
      "Average training loss: 0.019906434608830344\n",
      "Average test loss: 0.003917716276314523\n",
      "Epoch 138/300\n",
      "Average training loss: 0.019821213616265192\n",
      "Average test loss: 0.0037354126572608946\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01978672973314921\n",
      "Average test loss: 0.0039038737904694347\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01975130282673571\n",
      "Average test loss: 0.004159166309154696\n",
      "Epoch 141/300\n",
      "Average training loss: 0.019760253825949297\n",
      "Average test loss: 0.003904181007709768\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01976143952541881\n",
      "Average test loss: 0.0037443960420787334\n",
      "Epoch 143/300\n",
      "Average training loss: 0.019739286445081234\n",
      "Average test loss: 0.0038251022982100646\n",
      "Epoch 144/300\n",
      "Average training loss: 0.019668103127015963\n",
      "Average test loss: 0.0037635382409724923\n",
      "Epoch 145/300\n",
      "Average training loss: 0.019680298414495256\n",
      "Average test loss: 0.0039783077099257045\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01965913055009312\n",
      "Average test loss: 0.0037530919820484188\n",
      "Epoch 147/300\n",
      "Average training loss: 0.019670798892776172\n",
      "Average test loss: 0.0037412821220027077\n",
      "Epoch 148/300\n",
      "Average training loss: 0.019703888421257337\n",
      "Average test loss: 0.0037631323296162815\n",
      "Epoch 149/300\n",
      "Average training loss: 0.019635979110168088\n",
      "Average test loss: 0.003824800841924217\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01963249741660224\n",
      "Average test loss: 0.003837513328012493\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01958487922946612\n",
      "Average test loss: 0.003884515529498458\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019605167691906292\n",
      "Average test loss: 0.0037984169467041888\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01954233165913158\n",
      "Average test loss: 0.0039002424660656188\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019553963793648613\n",
      "Average test loss: 0.0039016177443166574\n",
      "Epoch 155/300\n",
      "Average training loss: 0.019505321946409013\n",
      "Average test loss: 0.004197115160110924\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019614787040485276\n",
      "Average test loss: 0.0037802096948855455\n",
      "Epoch 157/300\n",
      "Average training loss: 0.019503799261318313\n",
      "Average test loss: 0.003768885618282689\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01950061189962758\n",
      "Average test loss: 0.005465995142029391\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01945890886253781\n",
      "Average test loss: 0.003930012870786918\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01951966974635919\n",
      "Average test loss: 0.0038443672218256525\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01942303089135223\n",
      "Average test loss: 0.003777336015469498\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01947508385280768\n",
      "Average test loss: 0.0038504873166481652\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019465864050719473\n",
      "Average test loss: 0.0040457816786236235\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019453144287069637\n",
      "Average test loss: 0.0041894626439445545\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019426289124621285\n",
      "Average test loss: 0.003986198198050261\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01933275478084882\n",
      "Average test loss: 0.0037486402820795773\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019384503584769036\n",
      "Average test loss: 0.004357501277907027\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01942842747271061\n",
      "Average test loss: 0.004625193709507584\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019335709907114505\n",
      "Average test loss: 0.003999112443791496\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01937904212706619\n",
      "Average test loss: 0.0038495110819737116\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019350395995709632\n",
      "Average test loss: 0.003975257362342543\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01930832118789355\n",
      "Average test loss: 0.0044401130920483\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019299522661500507\n",
      "Average test loss: 0.004095550486404035\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01931814502676328\n",
      "Average test loss: 0.003769500837971767\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019319001469347213\n",
      "Average test loss: 0.003873703120690253\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019280175377925236\n",
      "Average test loss: 0.003986085058086448\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01921328057679865\n",
      "Average test loss: 0.0038305746362441114\n",
      "Epoch 178/300\n",
      "Average training loss: 0.019232962787151336\n",
      "Average test loss: 0.0037951493167007965\n",
      "Epoch 179/300\n",
      "Average training loss: 0.019379100887311828\n",
      "Average test loss: 0.004044336487849553\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01921507769326369\n",
      "Average test loss: 0.0037932880686389076\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019243568402197625\n",
      "Average test loss: 0.0038275458365678787\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019146857403218746\n",
      "Average test loss: 0.004214279880540238\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019281857995523347\n",
      "Average test loss: 0.004100231087870068\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01913790892726845\n",
      "Average test loss: 0.004115859260161718\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01921244564321306\n",
      "Average test loss: 0.003926160209915704\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0191465620390243\n",
      "Average test loss: 0.004346000697877672\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019129095198379624\n",
      "Average test loss: 0.0040843603615131644\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01908510583308008\n",
      "Average test loss: 0.003932858453856574\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01907514185541206\n",
      "Average test loss: 0.004054090902209282\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01906706764962938\n",
      "Average test loss: 0.003906498380419281\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019121030777692796\n",
      "Average test loss: 0.00394526115556558\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01907013057337867\n",
      "Average test loss: 0.003984759839044677\n",
      "Epoch 197/300\n",
      "Average training loss: 0.019082282026608784\n",
      "Average test loss: 0.0038965358856237597\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01905388942692015\n",
      "Average test loss: 0.003867984402510855\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01906153139968713\n",
      "Average test loss: 0.0038755528399099905\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01903735286162959\n",
      "Average test loss: 0.004252916022928224\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019010242715477944\n",
      "Average test loss: 0.003861077044159174\n",
      "Epoch 202/300\n",
      "Average training loss: 0.019026076599127716\n",
      "Average test loss: 0.0039392869178619646\n",
      "Epoch 203/300\n",
      "Average training loss: 0.018991128434737523\n",
      "Average test loss: 0.0038796198022448356\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01897096836235788\n",
      "Average test loss: 0.004015207655417422\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019026670651303396\n",
      "Average test loss: 0.0038714057438903383\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01898043884999222\n",
      "Average test loss: 0.004348200506427222\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0190025754355722\n",
      "Average test loss: 0.003972145106643439\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018948543609844312\n",
      "Average test loss: 0.004106455696332786\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01890849321004417\n",
      "Average test loss: 0.00418609105091956\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01893001665837235\n",
      "Average test loss: 0.004288427956402302\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018884526390168403\n",
      "Average test loss: 0.003915368961791198\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01891462926566601\n",
      "Average test loss: 0.004108794304852685\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018893444980184238\n",
      "Average test loss: 0.0043838068499333326\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018898392512566514\n",
      "Average test loss: 0.0038763200470970736\n",
      "Epoch 215/300\n",
      "Average training loss: 0.018893565538856717\n",
      "Average test loss: 0.003955859490152863\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01885665044354068\n",
      "Average test loss: 0.0038553057495090697\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018857906775342093\n",
      "Average test loss: 0.004356781232274241\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018811872210767536\n",
      "Average test loss: 0.00396484221600824\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01877619147962994\n",
      "Average test loss: 0.004145904179662466\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018833321669035487\n",
      "Average test loss: 0.003790769846075111\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01876614053795735\n",
      "Average test loss: 0.003977272015271915\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018807604796356625\n",
      "Average test loss: 0.0038564668339159756\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018785752400755883\n",
      "Average training loss: 0.018753634012407726\n",
      "Average test loss: 0.003915328943274087\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01873820990986294\n",
      "Average test loss: 0.0038827426727447243\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018766490383280647\n",
      "Average test loss: 0.004031813342952066\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01868755572537581\n",
      "Average test loss: 0.003956499673426151\n",
      "Epoch 238/300\n",
      "Average training loss: 0.018736886415216657\n",
      "Average test loss: 0.004226827155798673\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018683259290125636\n",
      "Average test loss: 0.003955716488882899\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018705289241340427\n",
      "Average test loss: 0.003927473903116253\n",
      "Epoch 241/300\n",
      "Average training loss: 0.018711062187121975\n",
      "Average test loss: 0.003985751618113782\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018751327955888377\n",
      "Average test loss: 0.003931479293853044\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01869040689037906\n",
      "Average test loss: 0.004095030915406015\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018688947460717625\n",
      "Average test loss: 0.004251883457311326\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01871087752448188\n",
      "Average test loss: 0.0038773098347915542\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018670530173513625\n",
      "Average test loss: 0.003899934820416901\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018696230818827948\n",
      "Average test loss: 0.004047227022134596\n",
      "Epoch 248/300\n",
      "Average training loss: 0.018665517908003595\n",
      "Average test loss: 0.004024634157824848\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018663019753164714\n",
      "Average test loss: 0.004003829851332638\n",
      "Epoch 250/300\n",
      "Average training loss: 0.018630985935529074\n",
      "Average test loss: 0.004243868485507038\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018635730657312607\n",
      "Average test loss: 0.004008129319383038\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018592462145619923\n",
      "Average test loss: 0.003937641978263855\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018610440135002137\n",
      "Average test loss: 0.003943607463190953\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018621274317304294\n",
      "Average test loss: 0.003939520708802674\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01860407137042946\n",
      "Average test loss: 0.004336721355302466\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018613374536236127\n",
      "Average test loss: 0.004025083023640845\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018604042212168374\n",
      "Average test loss: 0.004365212488919496\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01858047947453128\n",
      "Average test loss: 0.004885963601784574\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01857846612069342\n",
      "Average test loss: 0.004027922487921185\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01855981000761191\n",
      "Average test loss: 0.0040036452724080945\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018536031742890677\n",
      "Average test loss: 0.004071693351078365\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0185700506567955\n",
      "Average test loss: 0.0041791826209260355\n",
      "Epoch 263/300\n",
      "Average training loss: 0.018568688632713423\n",
      "Average test loss: 0.004004329403448436\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01857756532397535\n",
      "Average test loss: 0.003931749597191811\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018532626825902197\n",
      "Average test loss: 0.003974138230706255\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018519065130088063\n",
      "Average test loss: 0.00409933650203877\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018509865775704383\n",
      "Average test loss: 0.004950180487086376\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01849956408722533\n",
      "Average test loss: 0.003967563304222293\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018508491445746688\n",
      "Average test loss: 0.004023109828846322\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018494983471102185\n",
      "Average test loss: 0.003907897882991367\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01848639554116461\n",
      "Average test loss: 0.003929179264646437\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018451292963491545\n",
      "Average test loss: 0.003994482602096266\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018474764161639744\n",
      "Average test loss: 0.0039354153395526935\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018417216315865518\n",
      "Average test loss: 0.004113488604211145\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018555072483089234\n",
      "Average test loss: 0.003934780456953579\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018461944291989007\n",
      "Average test loss: 0.004024066563074787\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018405233666300775\n",
      "Average test loss: 0.004043335370305511\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018477589216497208\n",
      "Average test loss: 0.003911035211963786\n",
      "Epoch 282/300\n",
      "Average training loss: 0.018447267078691057\n",
      "Average test loss: 0.004000914473707477\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01846541188326147\n",
      "Average test loss: 0.004072701043966744\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018420191186997625\n",
      "Average test loss: 0.0039381073609822325\n",
      "Epoch 285/300\n",
      "Average training loss: 0.018416174913446107\n",
      "Average test loss: 0.003993316120985481\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018419132598572308\n",
      "Average test loss: 0.003956099939843019\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018398043301370407\n",
      "Average test loss: 0.003938557964232233\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0184075954573022\n",
      "Average test loss: 0.003932947248220443\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018426479001839954\n",
      "Average test loss: 0.003958712179627683\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0183635571292705\n",
      "Average test loss: 0.003946914588411649\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018366940943731203\n",
      "Average test loss: 0.003968650648577346\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01840169716378053\n",
      "Average test loss: 0.004022444806993008\n",
      "Epoch 296/300\n",
      "Average training loss: 0.018386768284771177\n",
      "Average test loss: 0.004161822259426117\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018382084207402335\n",
      "Average test loss: 0.003934654546694623\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01833892259332869\n",
      "Average test loss: 0.003998169333984455\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018367927114168802\n",
      "Average test loss: 0.003982135638594627\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018385160946183733\n",
      "Average test loss: 0.004268602416747146\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.25773365902900697\n",
      "Average test loss: 0.007578682372967402\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0887319480114513\n",
      "Average test loss: 0.005889136888914638\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06668456276920107\n",
      "Average test loss: 0.008894440218806266\n",
      "Epoch 4/300\n",
      "Average training loss: 0.055697003020180595\n",
      "Average test loss: 0.00496007331336538\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04896332056985961\n",
      "Average test loss: 0.00464991948981252\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04484300332930353\n",
      "Average test loss: 0.009618620325293805\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04106156576010916\n",
      "Average test loss: 0.004119793542350332\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03879522493481636\n",
      "Average test loss: 0.0041866157357063555\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03665692845483621\n",
      "Average test loss: 0.004035411271370119\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03471524497866631\n",
      "Average test loss: 0.004480492920511299\n",
      "Epoch 11/300\n",
      "Average training loss: 0.032578925963905124\n",
      "Average test loss: 0.003594516681300269\n",
      "Epoch 12/300\n",
      "Average training loss: 0.031407160232464475\n",
      "Average test loss: 0.007006381653249264\n",
      "Epoch 13/300\n",
      "Average training loss: 0.029831864012612237\n",
      "Average test loss: 0.003655708139141401\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02876464911301931\n",
      "Average test loss: 0.0034109048715068235\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02743930372926924\n",
      "Average test loss: 0.0035537595438460507\n",
      "Epoch 16/300\n",
      "Average training loss: 0.026524350966016453\n",
      "Average test loss: 0.0031563037706332075\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02558337179488606\n",
      "Average test loss: 0.003312647640498148\n",
      "Epoch 18/300\n",
      "Average training loss: 0.024660849238435426\n",
      "Average test loss: 0.0029897763900872732\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02412374421954155\n",
      "Average test loss: 0.0031917314827442167\n",
      "Epoch 20/300\n",
      "Average training loss: 0.023320610773232248\n",
      "Average test loss: 0.0034990820847451685\n",
      "Epoch 21/300\n",
      "Average training loss: 0.022956172959672082\n",
      "Average test loss: 0.0029863927511291372\n",
      "Epoch 22/300\n",
      "Average training loss: 0.022499554148978657\n",
      "Average test loss: 0.003039434772502217\n",
      "Epoch 23/300\n",
      "Average training loss: 0.022102003499865533\n",
      "Average test loss: 0.005100862994376156\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021910198897123336\n",
      "Average training loss: 0.02095635881026586\n",
      "Average test loss: 0.0027399735764289898\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020653878811332915\n",
      "Average test loss: 0.002765053873985178\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020479874248305958\n",
      "Average test loss: 0.0026722058674527537\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020224964262710678\n",
      "Average test loss: 0.0026485474382837615\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020123518246743414\n",
      "Average test loss: 0.0029191129037903414\n",
      "Epoch 32/300\n",
      "Average training loss: 0.019934881441295148\n",
      "Average test loss: 0.0027963524560133614\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01965508178042041\n",
      "Average test loss: 0.002882999032528864\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019549054748482175\n",
      "Average test loss: 0.0026881613766567575\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019440564506583744\n",
      "Average test loss: 0.0026770364300658304\n",
      "Epoch 36/300\n",
      "Average training loss: 0.019360864530007044\n",
      "Average test loss: 0.0028646263606432413\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019240032027165094\n",
      "Average test loss: 0.002639642493385408\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019093243077397345\n",
      "Average test loss: 0.0026842998003380166\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018965759163101514\n",
      "Average test loss: 0.002634797474162446\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01899920027454694\n",
      "Average test loss: 0.0027810400142851805\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0187909873219\n",
      "Average test loss: 0.0026868636388745573\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018725340065856775\n",
      "Average test loss: 0.0029372737006180816\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018591711804270744\n",
      "Average test loss: 0.0028456336667554245\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018525450420876343\n",
      "Average test loss: 0.0026235851684792174\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018429193466073936\n",
      "Average test loss: 0.0026314287754810517\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01839914734330442\n",
      "Average test loss: 0.0027604308002110983\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01827508786651823\n",
      "Average test loss: 0.002562450203630659\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01821200453986724\n",
      "Average test loss: 0.0029634174627976287\n",
      "Epoch 49/300\n",
      "Average training loss: 0.018181384899550013\n",
      "Average test loss: 0.0025452399651209514\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01810688486860858\n",
      "Average test loss: 0.0025728074794428217\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018066924918029043\n",
      "Average test loss: 0.0026379391406145362\n",
      "Epoch 52/300\n",
      "Average training loss: 0.017993805557489397\n",
      "Average test loss: 0.0027023188080638645\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01792294995403952\n",
      "Average test loss: 0.0025665937706621155\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01783074932462639\n",
      "Average test loss: 0.0025761920346154105\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017826880441771612\n",
      "Average test loss: 0.0028917127144005563\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017749426291220716\n",
      "Average test loss: 0.002599558250978589\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017681996086405385\n",
      "Average test loss: 0.002643262513809734\n",
      "Epoch 58/300\n",
      "Average training loss: 0.017654814592666095\n",
      "Average test loss: 0.0027405024394392967\n",
      "Epoch 59/300\n",
      "Average training loss: 0.017546722968419394\n",
      "Average test loss: 0.0025189862209889623\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017579457092616293\n",
      "Average test loss: 0.002606617831521564\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017560627094573445\n",
      "Average test loss: 0.0026167743671685457\n",
      "Epoch 62/300\n",
      "Average training loss: 0.017504377694593537\n",
      "Average test loss: 0.0025287212234818275\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017394421751300494\n",
      "Average test loss: 0.002553900028268496\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017352851420640945\n",
      "Average test loss: 0.002582311054277751\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017300728597574765\n",
      "Average test loss: 0.0025311634021086826\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01728694675862789\n",
      "Average test loss: 0.0028071815179040033\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017264815938141612\n",
      "Average test loss: 0.0025423992220312357\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01719004924181435\n",
      "Average test loss: 0.0027666435440381366\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017179085027840404\n",
      "Average test loss: 0.002549198290747073\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017089997251828512\n",
      "Average test loss: 0.002605996841357814\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017081884562969208\n",
      "Average test loss: 0.002516251450818446\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017020836084253258\n",
      "Average test loss: 0.002524584767098228\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017012607188688383\n",
      "Average test loss: 0.0025649380456242298\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01698488784664207\n",
      "Average test loss: 0.0032730607725679874\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01697997844964266\n",
      "Average test loss: 0.0026573585343236724\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01687869897650348\n",
      "Average test loss: 0.002652748436253104\n",
      "Epoch 77/300\n",
      "Average training loss: 0.016888211271829077\n",
      "Average test loss: 0.002519043093961146\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01681561361832751\n",
      "Average test loss: 0.002600014759745035\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01680252682334847\n",
      "Average test loss: 0.0026623327777617507\n",
      "Epoch 80/300\n",
      "Average training loss: 0.016754503193828794\n",
      "Average test loss: 0.002590851283321778\n",
      "Epoch 81/300\n",
      "Average training loss: 0.016735309867395295\n",
      "Average test loss: 0.002592990233045485\n",
      "Epoch 82/300\n",
      "Average training loss: 0.016707448174556095\n",
      "Average test loss: 0.0026073595262649987\n",
      "Epoch 83/300\n",
      "Average training loss: 0.016669125770529113\n",
      "Average test loss: 0.002539734688690967\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01662663738760683\n",
      "Average test loss: 0.0025356712579313254\n",
      "Epoch 85/300\n",
      "Average training loss: 0.016608482961853344\n",
      "Average test loss: 0.002500777770454685\n",
      "Epoch 86/300\n",
      "Average training loss: 0.016589260836442313\n",
      "Average test loss: 0.0025672561327616376\n",
      "Epoch 87/300\n",
      "Average training loss: 0.016568440329697398\n",
      "Average test loss: 0.0025310678984969853\n",
      "Epoch 88/300\n",
      "Average training loss: 0.016524087502724594\n",
      "Average test loss: 0.0025727351808713544\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016498918860322898\n",
      "Average test loss: 0.002532128451185094\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016465663072135713\n",
      "Average test loss: 0.0025613022092729806\n",
      "Epoch 91/300\n",
      "Average training loss: 0.016423763344685236\n",
      "Average test loss: 0.0025467886766418813\n",
      "Epoch 92/300\n",
      "Average training loss: 0.016416285728414852\n",
      "Average test loss: 0.0026000698887639574\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01641021187769042\n",
      "Average test loss: 0.002627506334748533\n",
      "Epoch 94/300\n",
      "Average training loss: 0.016363900259137153\n",
      "Average test loss: 0.0026936885385463634\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01630818294816547\n",
      "Average test loss: 0.0025667355683528714\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01631853398680687\n",
      "Average test loss: 0.0025393911782238218\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016270613552795517\n",
      "Average test loss: 0.002593708850443363\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016286927764614424\n",
      "Average test loss: 0.0044125207006517385\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01626874392810795\n",
      "Average test loss: 0.0026359782388640776\n",
      "Epoch 100/300\n",
      "Average training loss: 0.016204221766855983\n",
      "Average test loss: 0.002660752969690495\n",
      "Epoch 101/300\n",
      "Average training loss: 0.016222689845495755\n",
      "Average test loss: 0.002547253879201081\n",
      "Epoch 102/300\n",
      "Average training loss: 0.016148385574420294\n",
      "Average test loss: 0.002530686267755098\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016154733707507453\n",
      "Average test loss: 0.00258941973425034\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01611903719438447\n",
      "Average test loss: 0.002633039173256192\n",
      "Epoch 105/300\n",
      "Average training loss: 0.016089741299549737\n",
      "Average test loss: 0.0025298546807219586\n",
      "Epoch 106/300\n",
      "Average training loss: 0.016117371868756083\n",
      "Average test loss: 0.0025360895587752262\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016120520522197087\n",
      "Average test loss: 0.002550714732044273\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01605364305774371\n",
      "Average test loss: 0.0025919207963678572\n",
      "Epoch 109/300\n",
      "Average training loss: 0.016033036336302757\n",
      "Average test loss: 0.0025373762818053365\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015994164728456075\n",
      "Average test loss: 0.0025748900667660764\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01596511575579643\n",
      "Average test loss: 0.0027091506469166943\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015969281286001204\n",
      "Average test loss: 0.002636084993887279\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015932323379649057\n",
      "Average test loss: 0.002699206408320202\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01594976204716497\n",
      "Average test loss: 0.0026044450911382835\n",
      "Epoch 115/300\n",
      "Average training loss: 0.015919775958690378\n",
      "Average test loss: 0.002586104858459698\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01588173941605621\n",
      "Average test loss: 0.0025526419505476953\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015853874079883097\n",
      "Average test loss: 0.0025902856890526083\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015860585277279217\n",
      "Average test loss: 0.0026037379000335933\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015837302428152827\n",
      "Average test loss: 0.002704040595019857\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01579372337460518\n",
      "Average test loss: 0.002621063775382936\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015805674790508217\n",
      "Average test loss: 0.0025743654403421615\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0157611020108064\n",
      "Average test loss: 0.002731486272687713\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015787270097268952\n",
      "Average test loss: 0.0026300430945638154\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015768090036180283\n",
      "Average test loss: 0.0025943332008189625\n",
      "Epoch 125/300\n",
      "Average training loss: 0.015695262195335496\n",
      "Average test loss: 0.002617317128719555\n",
      "Epoch 126/300\n",
      "Average training loss: 0.015730499936474695\n",
      "Average test loss: 0.002805820779150559\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015701062122980752\n",
      "Average test loss: 0.00300435527248515\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01566997782389323\n",
      "Average test loss: 0.002601074362380637\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015661346807248063\n",
      "Average test loss: 0.002730597479475869\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01565462740841839\n",
      "Average test loss: 0.002635720117845469\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015637625633014574\n",
      "Average test loss: 0.0025856056689388222\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015591588436729378\n",
      "Average test loss: 0.0025767220246295133\n",
      "Epoch 133/300\n",
      "Average training loss: 0.015573764383792878\n",
      "Average test loss: 0.0027070776851226886\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015583819169137213\n",
      "Average test loss: 0.002874793351524406\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015586600169539451\n",
      "Average test loss: 0.0027211573674447006\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015577756785684162\n",
      "Average test loss: 0.0026465700438453093\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015530267151693504\n",
      "Average test loss: 0.0026399646827744115\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015537573009729385\n",
      "Average test loss: 0.002864363219175074\n",
      "Epoch 139/300\n",
      "Average training loss: 0.015503669594724974\n",
      "Average test loss: 0.0026093562750352755\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015515357904964023\n",
      "Average test loss: 0.0027343705064720577\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015505320047338804\n",
      "Average test loss: 0.002661682361943854\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015472643930051062\n",
      "Average test loss: 0.002696885388551487\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015484280291530822\n",
      "Average test loss: 0.0025611780799097484\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015428294328351815\n",
      "Average test loss: 0.0026144758258014916\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015440410794483292\n",
      "Average test loss: 0.002630817548268371\n",
      "Epoch 146/300\n",
      "Average training loss: 0.015436582406361898\n",
      "Average test loss: 0.0025754394160790576\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0154080889183614\n",
      "Average test loss: 0.002896987386254801\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01539077863842249\n",
      "Average test loss: 0.0026994995284411643\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015422608234816127\n",
      "Average test loss: 0.0027352351237916283\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015366062309179042\n",
      "Average test loss: 0.0028307979301446014\n",
      "Epoch 151/300\n",
      "Average training loss: 0.015359615378909642\n",
      "Average test loss: 0.0027435370673322013\n",
      "Epoch 152/300\n",
      "Average training loss: 0.015350754923290676\n",
      "Average test loss: 0.0026911944126089414\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015329429828458363\n",
      "Average test loss: 0.002789526321614782\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015340988255209393\n",
      "Average test loss: 0.002795006616765426\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015328699447214603\n",
      "Average test loss: 0.002685114872952302\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01530433643857638\n",
      "Average test loss: 0.0026493942762414614\n",
      "Epoch 157/300\n",
      "Average training loss: 0.015254962695969475\n",
      "Average test loss: 0.002667728542867634\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015288120134009256\n",
      "Average test loss: 0.002698433529999521\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015288695707089371\n",
      "Average test loss: 0.002598498595981962\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0152745783759488\n",
      "Average test loss: 0.0028258898220956327\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015252875140557686\n",
      "Average test loss: 0.002830615823674533\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015257629781961442\n",
      "Average test loss: 0.002608150183533629\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015215877720879184\n",
      "Average test loss: 0.002656320823356509\n",
      "Epoch 164/300\n",
      "Average training loss: 0.015222690841390028\n",
      "Average test loss: 0.0032416333792110283\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015212826901011996\n",
      "Average test loss: 0.0027915831034382183\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015216758916775385\n",
      "Average test loss: 0.002669252203260031\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015216339415974087\n",
      "Average test loss: 0.002728901955092119\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015185343795352512\n",
      "Average test loss: 0.0027886773909752568\n",
      "Epoch 169/300\n",
      "Average training loss: 0.015187449771496985\n",
      "Average test loss: 0.0028986379572500786\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015165907788607809\n",
      "Average test loss: 0.002655445193664895\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01514704641368654\n",
      "Average test loss: 0.0026524591574238407\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015148589208722115\n",
      "Average test loss: 0.0026902262297355465\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015104890675180489\n",
      "Average test loss: 0.0027298523402876323\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015127177937163247\n",
      "Average test loss: 0.002698672155746155\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015117185882396168\n",
      "Average test loss: 0.002689427411183715\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015111761470635731\n",
      "Average test loss: 0.0026671602942256465\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015076777673429914\n",
      "Average test loss: 0.0027083042753446433\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015063644702649778\n",
      "Average test loss: 0.0027673480688697764\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015099985556470023\n",
      "Average test loss: 0.0026877388974858654\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015088768397768338\n",
      "Average test loss: 0.002731882230482168\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015049184444049995\n",
      "Average test loss: 0.002741319557858838\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01509798959394296\n",
      "Average test loss: 0.0027207421296172674\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015037346293528875\n",
      "Average test loss: 0.0027523230136268667\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015004882285164462\n",
      "Average test loss: 0.0027877305048621365\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015008557936383619\n",
      "Average test loss: 0.0026853264584723448\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01501584090789159\n",
      "Average test loss: 0.0027260304811514086\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01503951003195511\n",
      "Average test loss: 0.002803920895482103\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014996801373859246\n",
      "Average test loss: 0.0028595961566186615\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01499481903678841\n",
      "Average test loss: 0.0028242683514124816\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014983780385719405\n",
      "Average test loss: 0.00269648329127166\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014980148040586047\n",
      "Average test loss: 0.0028580290234337252\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01498459661172496\n",
      "Average test loss: 0.0026794030814328127\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014953156030012502\n",
      "Average test loss: 0.0029655004911538626\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014958522147602505\n",
      "Average test loss: 0.002673715680009789\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014972956763373482\n",
      "Average test loss: 0.0027112484047603276\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014938915866944525\n",
      "Average test loss: 0.002718352462682459\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0149346539452672\n",
      "Average test loss: 0.0028775367935498556\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01490547654363844\n",
      "Average test loss: 0.002709772561987241\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014895679735475117\n",
      "Average test loss: 0.0028288320071167415\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014902963717778524\n",
      "Average test loss: 0.0026671452681637474\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014891402032640244\n",
      "Average test loss: 0.002683610548575719\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014870201135675113\n",
      "Average test loss: 0.0027836884156697324\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014865695741441514\n",
      "Average test loss: 0.00278931545248876\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014892125379708078\n",
      "Average test loss: 0.002767589098877377\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014902358185913827\n",
      "Average test loss: 0.0028668519279195204\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014862880289554596\n",
      "Average test loss: 0.0026390645125259956\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014884848822322157\n",
      "Average test loss: 0.002973519699441062\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014853655081656245\n",
      "Average test loss: 0.0027084063701331615\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014851302416788207\n",
      "Average test loss: 0.002720133234022392\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014795468780729506\n",
      "Average test loss: 0.0026763737888799775\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014833703118893836\n",
      "Average test loss: 0.002737972667026851\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014851716038253573\n",
      "Average test loss: 0.0027046678562959036\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014802765864464973\n",
      "Average test loss: 0.002712095231645637\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014818864978022046\n",
      "Average test loss: 0.002916847438034084\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014824994786745972\n",
      "Average test loss: 0.0028248417559597226\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014767165152562989\n",
      "Average test loss: 0.002713863680139184\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0147825495534473\n",
      "Average test loss: 0.002721750487883886\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014790648642513487\n",
      "Average test loss: 0.0026861950303945278\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014774928036663268\n",
      "Average test loss: 0.0031025374794585838\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014797980699274274\n",
      "Average test loss: 0.0028047713908470337\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014738739164339171\n",
      "Average test loss: 0.0027253145445138216\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014735551765395535\n",
      "Average test loss: 0.002823849556967616\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014704162034723493\n",
      "Average test loss: 0.002673569682985544\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014760650124814775\n",
      "Average test loss: 0.00279889898478157\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014717859190371302\n",
      "Average test loss: 0.0027701157081044384\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014728852066728804\n",
      "Average test loss: 0.0027352778238968717\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014788426636821695\n",
      "Average test loss: 0.0027860482777986263\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014717877465817663\n",
      "Average test loss: 0.0027275181031889387\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014708491881688436\n",
      "Average test loss: 0.0032575053373972577\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014717749762866232\n",
      "Average test loss: 0.0027783572052915892\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014700110865963829\n",
      "Average test loss: 0.002719653907335467\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014704917429222001\n",
      "Average test loss: 0.0026341034181209073\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014700929060578346\n",
      "Average test loss: 0.002840053479497631\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014629428587026067\n",
      "Average test loss: 0.002824436069569654\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014695303567581707\n",
      "Average test loss: 0.00276555042165435\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014691786458094915\n",
      "Average test loss: 0.002663239847868681\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014642922417985068\n",
      "Average test loss: 0.0027960895862844254\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014664558445413907\n",
      "Average test loss: 0.0026859326724790866\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014646315728624662\n",
      "Average test loss: 0.002779151339911752\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014659443266689777\n",
      "Average test loss: 0.0028154552264346017\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014649468564324909\n",
      "Average test loss: 0.0027891067042946816\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014638867595129543\n",
      "Average test loss: 0.0027667617827860846\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014648665726184845\n",
      "Average test loss: 0.002724893815918929\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014610167789790365\n",
      "Average test loss: 0.002766438263364964\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014616971593764093\n",
      "Average test loss: 0.003616437784085671\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01463837894052267\n",
      "Average test loss: 0.0028699764789392552\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014610347163346079\n",
      "Average test loss: 0.0027161969592173893\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014638854573170344\n",
      "Average test loss: 0.0027857888518936103\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014597021440664926\n",
      "Average test loss: 0.0027307168842396804\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0145792279375924\n",
      "Average test loss: 0.0032454341111911667\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014608543548319074\n",
      "Average test loss: 0.0028323455078320373\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014584866397082806\n",
      "Average test loss: 0.0027991358517772623\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014567426699731085\n",
      "Average test loss: 0.0027742931281940805\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01456053320484029\n",
      "Average test loss: 0.002772400701418519\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014559333814515008\n",
      "Average test loss: 0.0030434279549452995\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01456836820476585\n",
      "Average test loss: 0.0027815211207295456\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014556778240534994\n",
      "Average test loss: 0.0027329493140180908\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014551040127873421\n",
      "Average test loss: 0.002834459870846735\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014527784531315167\n",
      "Average test loss: 0.0028177814056269\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014559726436932883\n",
      "Average test loss: 0.0028613680706669887\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014546798939506212\n",
      "Average test loss: 0.0027672987309181026\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014516709607508447\n",
      "Average test loss: 0.002763611037491096\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014545532549420993\n",
      "Average test loss: 0.002788395330102907\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014513841066095564\n",
      "Average test loss: 0.0027511638943105935\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014522497029768097\n",
      "Average test loss: 0.002752059259555406\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014525850507948133\n",
      "Average test loss: 0.003059400151587195\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014508951311310133\n",
      "Average test loss: 0.002788791900707616\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014508882333834966\n",
      "Average test loss: 0.002811541545515259\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014475095060964425\n",
      "Average test loss: 0.0027788961943652894\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014513415004644129\n",
      "Average test loss: 0.002720947839319706\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014509193170401785\n",
      "Average test loss: 0.002681907564608587\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014473195892241266\n",
      "Average test loss: 0.002778115800685353\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014491575959655974\n",
      "Average test loss: 0.0029084831178188323\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014483783880041705\n",
      "Average test loss: 0.0027864794803576336\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014506652864317099\n",
      "Average test loss: 0.002861992172379461\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014450036011636257\n",
      "Average test loss: 0.0027494898095933927\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014454075976378388\n",
      "Average test loss: 0.0027782710583673584\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014502706852224137\n",
      "Average test loss: 0.0026923406875381866\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014456965507732497\n",
      "Average test loss: 0.003027751600266331\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014424529179102844\n",
      "Average test loss: 0.0027449194021109077\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014437715813517571\n",
      "Average test loss: 0.0028770946082141663\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014465589271651374\n",
      "Average test loss: 0.0029071909813210366\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014451982157097922\n",
      "Average test loss: 0.002714371499295036\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014416082778738605\n",
      "Average test loss: 0.0035928063173260956\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0144425541145934\n",
      "Average test loss: 0.002762629898265004\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01439913389666213\n",
      "Average test loss: 0.002990178912257155\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014463641155925062\n",
      "Average test loss: 0.002835624373621411\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014395721493495835\n",
      "Average test loss: 0.003083718203629057\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014414122154315312\n",
      "Average test loss: 0.002831858813348744\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014436176089776888\n",
      "Average test loss: 0.002749450696632266\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014410199705925253\n",
      "Average test loss: 0.0028333436186528866\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014406354645888011\n",
      "Average test loss: 0.0028400033185672427\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01439394999295473\n",
      "Average test loss: 0.002819793509112464\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014385447767045763\n",
      "Average test loss: 0.0027816998694712917\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014408470298680994\n",
      "Average test loss: 0.00277278133502437\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014386921148333285\n",
      "Average test loss: 0.0028714595577783056\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014410779485272036\n",
      "Average test loss: 0.002779173319331474\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014389272767636511\n",
      "Average test loss: 0.0029171208072867657\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014401937428447935\n",
      "Average test loss: 0.0029684415169888073\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014360567420721055\n",
      "Average test loss: 0.002833512840171655\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.25307064412699803\n",
      "Average test loss: 0.009372527017361588\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08459836160474353\n",
      "Average test loss: 0.00925986597687006\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06236176563633813\n",
      "Average test loss: 0.004115330139795939\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05142404521505038\n",
      "Average test loss: 0.0037579810000542137\n",
      "Epoch 5/300\n",
      "Average training loss: 0.045279778745439314\n",
      "Average test loss: 0.004046816438436508\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04025846219062805\n",
      "Average test loss: 0.004521541036251519\n",
      "Epoch 7/300\n",
      "Average training loss: 0.037363299707571664\n",
      "Average test loss: 0.0033157425553848345\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03424070630305343\n",
      "Average test loss: 0.006360423603819477\n",
      "Epoch 9/300\n",
      "Average training loss: 0.031816935420036316\n",
      "Average test loss: 0.0029905469491043023\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03053230642610126\n",
      "Average test loss: 0.009587815631594923\n",
      "Epoch 11/300\n",
      "Average training loss: 0.028616280416647592\n",
      "Average test loss: 0.0035810541672011216\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02727844693015019\n",
      "Average test loss: 0.0027611245361881124\n",
      "Epoch 13/300\n",
      "Average training loss: 0.026054865926504134\n",
      "Average test loss: 0.006444716626571284\n",
      "Epoch 14/300\n",
      "Average training loss: 0.024586710936493342\n",
      "Average test loss: 0.0026477488219324085\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023695300976435345\n",
      "Average test loss: 0.0025531828422099352\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02261936834951242\n",
      "Average test loss: 0.002547830061366161\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021999949369165633\n",
      "Average test loss: 0.0038562456965446474\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02103759853541851\n",
      "Average test loss: 0.002547454742093881\n",
      "Epoch 19/300\n",
      "Average training loss: 0.020408196901281674\n",
      "Average test loss: 0.0022671388455977044\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020015930316514437\n",
      "Average test loss: 0.002344649854219622\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01944378331800302\n",
      "Average test loss: 0.0024041150340603458\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01915190944241153\n",
      "Average test loss: 0.0025170913413167\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018726845620407\n",
      "Average test loss: 0.0024886720028395454\n",
      "Epoch 24/300\n",
      "Average training loss: 0.018302309291230306\n",
      "Average test loss: 0.0022613799764464298\n",
      "Epoch 25/300\n",
      "Average training loss: 0.018054420247673987\n",
      "Average test loss: 0.0021396788506665163\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017820494737890033\n",
      "Average test loss: 0.0021683829926575225\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017601757107509507\n",
      "Average test loss: 0.002066463847230706\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01732336588535044\n",
      "Average test loss: 0.0021075727730575536\n",
      "Epoch 29/300\n",
      "Average training loss: 0.017192748657531207\n",
      "Average test loss: 0.002193188475444913\n",
      "Epoch 30/300\n",
      "Average training loss: 0.017121811653176944\n",
      "Average test loss: 0.00204714353589548\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01678901662925879\n",
      "Average test loss: 0.0023651472108645573\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01669195584124989\n",
      "Average test loss: 0.0020379824291707743\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016528521041075387\n",
      "Average test loss: 0.0024486787271582417\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016469939975274933\n",
      "Average test loss: 0.002342803418636322\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016313298960526783\n",
      "Average test loss: 0.0021015034955408837\n",
      "Epoch 36/300\n",
      "Average training loss: 0.016187570250696608\n",
      "Average test loss: 0.0019426080245110724\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01612738692926036\n",
      "Average test loss: 0.002048694209713075\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015946930557489396\n",
      "Average test loss: 0.0019799477450756563\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015879374782244365\n",
      "Average test loss: 0.002026826883562737\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015800625258849725\n",
      "Average test loss: 0.002050849404806892\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015756755429837437\n",
      "Average test loss: 0.004028829858017465\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015668958142399787\n",
      "Average test loss: 0.0019228022063357963\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015559106775456005\n",
      "Average test loss: 0.0020931547752891977\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015517411790788173\n",
      "Average test loss: 0.0019164086383663946\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015453600208792422\n",
      "Average test loss: 0.002089465599714054\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015372455793950293\n",
      "Average test loss: 0.001927165259917577\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015298540671666463\n",
      "Average test loss: 0.001918854700608386\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015250258180002372\n",
      "Average test loss: 0.0019239854584965441\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015154429204761982\n",
      "Average test loss: 0.0018881916541399228\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015118536428444915\n",
      "Average test loss: 0.001910576187695066\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015070019903282324\n",
      "Average test loss: 0.0022803377500838704\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015031422461072605\n",
      "Average test loss: 0.0019934794389539295\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014994241389135519\n",
      "Average test loss: 0.0022823048999740018\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014942640107538966\n",
      "Average test loss: 0.0019130880013108254\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014852425989177492\n",
      "Average test loss: 0.0019102010150543517\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01480048200984796\n",
      "Average test loss: 0.0019197840314979355\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014809424793554677\n",
      "Average test loss: 0.0019005014521794187\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014706819414264626\n",
      "Average test loss: 0.001905502735533648\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014711959750288063\n",
      "Average test loss: 0.0021245190096605155\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014652381027738253\n",
      "Average test loss: 0.0019100010376423596\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014609644657207859\n",
      "Average test loss: 0.04245466145210796\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014557339073883162\n",
      "Average test loss: 0.0019511270250918136\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014533294456700483\n",
      "Average test loss: 0.001877943299089869\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014446360679136383\n",
      "Average test loss: 0.0018865872874028152\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014442073404788971\n",
      "Average test loss: 0.0019234997305191226\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01442665868666437\n",
      "Average test loss: 0.0018883881210866901\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01435388086984555\n",
      "Average test loss: 0.0019110484985220763\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01433455678489473\n",
      "Average test loss: 0.001909041578984923\n",
      "Epoch 69/300\n",
      "Average training loss: 0.014345774967637327\n",
      "Average test loss: 0.0019248630437586043\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014246484375662274\n",
      "Average test loss: 0.001910721208072371\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01426335673365328\n",
      "Average test loss: 0.0018729067493437066\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014195784296426508\n",
      "Average test loss: 0.0018964697658601735\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014186327498820093\n",
      "Average test loss: 0.001900799539250632\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014119172524246904\n",
      "Average test loss: 0.001881468882577287\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014086263199647267\n",
      "Average test loss: 0.0019334204867482185\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01408149398697747\n",
      "Average test loss: 0.00195219023215274\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014039694824152522\n",
      "Average test loss: 0.0018869820460677147\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014025070768263604\n",
      "Average test loss: 0.0019252071394067671\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014005432893832525\n",
      "Average test loss: 0.0019031346316138904\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01395240426229106\n",
      "Average test loss: 0.00219936586668094\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013939545118974314\n",
      "Average test loss: 0.0020066844920317334\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013918482227457894\n",
      "Average test loss: 0.0019574467935081987\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013883677076962258\n",
      "Average test loss: 0.0020581658457716305\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013854043387704426\n",
      "Average test loss: 0.0020061130939672393\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013823241207334731\n",
      "Average test loss: 0.0018824242212706142\n",
      "Epoch 86/300\n",
      "Average training loss: 0.013778702561226156\n",
      "Average test loss: 0.0019162152961103452\n",
      "Epoch 87/300\n",
      "Average training loss: 0.013773686349391938\n",
      "Average test loss: 0.002011304063929452\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01376115745306015\n",
      "Average test loss: 0.0019920187894668846\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01375325038863553\n",
      "Average test loss: 0.0019670458055204816\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013741284117102622\n",
      "Average test loss: 0.0019348977647411327\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013678777928981516\n",
      "Average test loss: 0.0020245504294418627\n",
      "Epoch 92/300\n",
      "Average training loss: 0.013690586018893454\n",
      "Average test loss: 0.0019679984491732384\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01361616470416387\n",
      "Average test loss: 0.0019289375718476044\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013602980110794307\n",
      "Average test loss: 0.0019552772370063595\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013594534094962809\n",
      "Average test loss: 0.001966648092286454\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01363198772072792\n",
      "Average test loss: 0.0022807079090012444\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013560463433464369\n",
      "Average test loss: 0.0019592529422500066\n",
      "Epoch 98/300\n",
      "Average training loss: 0.013517974577016301\n",
      "Average test loss: 0.0029046045649382805\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013530674151248402\n",
      "Average test loss: 0.0019456136154217853\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013503244847887091\n",
      "Average test loss: 0.0018811564828372664\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013476023350324896\n",
      "Average test loss: 0.0018844751332783036\n",
      "Epoch 102/300\n",
      "Average training loss: 0.013454269746939342\n",
      "Average test loss: 0.002003061696059174\n",
      "Epoch 103/300\n",
      "Average training loss: 0.013423723825977908\n",
      "Average test loss: 0.001902757375087175\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013451100980242093\n",
      "Average test loss: 0.0018716173110943702\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013408598550491863\n",
      "Average test loss: 0.0021809146816117895\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013359214430054028\n",
      "Average test loss: 0.002039585610230764\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013373608686857753\n",
      "Average test loss: 0.0020815820743640264\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013358650911185476\n",
      "Average test loss: 0.0020234845324109\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013331730775535107\n",
      "Average test loss: 0.0019142855175046456\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013303521560298072\n",
      "Average test loss: 0.0021458119683795507\n",
      "Epoch 111/300\n",
      "Average training loss: 0.013300522859725687\n",
      "Average test loss: 0.0019391073914658693\n",
      "Epoch 112/300\n",
      "Average training loss: 0.013296989478170871\n",
      "Average test loss: 0.0019405013945781521\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0132781696220239\n",
      "Average test loss: 0.0019314009990129206\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013261582284337945\n",
      "Average test loss: 0.002397009530208177\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013236420550280147\n",
      "Average test loss: 0.00205114310938451\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013237013597455289\n",
      "Average test loss: 0.0019356088077442514\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013185568460159832\n",
      "Average test loss: 0.0019186021215799782\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013188842202226322\n",
      "Average test loss: 0.00203853278224253\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013166183701819843\n",
      "Average test loss: 0.0020584397653324736\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013148491740226746\n",
      "Average test loss: 0.00194418940320611\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013159341101845105\n",
      "Average test loss: 0.0019214948434382677\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013114241927862168\n",
      "Average test loss: 0.0019412664452360736\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013141124546527862\n",
      "Average test loss: 0.0019868915203130905\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013098570919699138\n",
      "Average test loss: 0.0019529039319604634\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013093468691739772\n",
      "Average test loss: 0.0020152566495041054\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013062506943113275\n",
      "Average test loss: 0.0019354918263852596\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013057961604661411\n",
      "Average test loss: 0.002029229523614049\n",
      "Epoch 128/300\n",
      "Average training loss: 0.013026267402701908\n",
      "Average test loss: 0.002004055955550737\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013041207247310215\n",
      "Average test loss: 0.002026212742345201\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013020751343005234\n",
      "Average test loss: 0.0019939615118006867\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013025103434920311\n",
      "Average test loss: 0.001945368364867237\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012999928557210499\n",
      "Average test loss: 0.0020083222358177107\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012996238525542948\n",
      "Average test loss: 0.001918772824626002\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012954719965656598\n",
      "Average test loss: 0.002095021285530594\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012981517115400897\n",
      "Average test loss: 0.001985144440188176\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012945438657369879\n",
      "Average test loss: 0.001991716537728078\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012933491365777122\n",
      "Average test loss: 0.0020536979168860447\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012924731374614768\n",
      "Average test loss: 0.002128863790796863\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012898012499014537\n",
      "Average test loss: 0.0020153366468018954\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012901628833678033\n",
      "Average test loss: 0.008504240896966723\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012909731271366279\n",
      "Average test loss: 0.00217159238912993\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012889055689175923\n",
      "Average test loss: 0.0019393152953642938\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0128550566782554\n",
      "Average test loss: 0.002023841148035394\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012847661682301097\n",
      "Average test loss: 0.002025784017518163\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012838103145360947\n",
      "Average test loss: 0.0019415314171670212\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012882532423569097\n",
      "Average test loss: 0.0021277516316622496\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012828622034854359\n",
      "Average test loss: 0.001959954843752914\n",
      "Epoch 148/300\n",
      "Average training loss: 0.012810957416064209\n",
      "Average test loss: 0.0019669950732754335\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012796387583017349\n",
      "Average test loss: 0.0019763256339356302\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012795422290762266\n",
      "Average test loss: 0.0019737106488189765\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012784977250629001\n",
      "Average test loss: 0.0020089996349480417\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012771453965869215\n",
      "Average test loss: 0.001999905394597186\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012753396713071399\n",
      "Average test loss: 0.0021610121561421287\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012759049851033422\n",
      "Average test loss: 0.0020316837978445823\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012746347799069352\n",
      "Average test loss: 0.001980272076196141\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012732412426008118\n",
      "Average test loss: 0.00202354490922557\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012726663495931361\n",
      "Average test loss: 0.002060835412082573\n",
      "Epoch 158/300\n",
      "Average training loss: 0.012719631666110622\n",
      "Average test loss: 0.0020209257962803045\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012680321935150358\n",
      "Average test loss: 0.0020120922787528897\n",
      "Epoch 160/300\n",
      "Average training loss: 0.012707882285945946\n",
      "Average test loss: 0.0020106827079628903\n",
      "Epoch 161/300\n",
      "Average training loss: 0.012736003793776035\n",
      "Average test loss: 0.002007817894220352\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01266788432912694\n",
      "Average test loss: 0.0021317159305844044\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012683059075640308\n",
      "Average test loss: 0.0019712298178217475\n",
      "Epoch 164/300\n",
      "Average training loss: 0.012641478021111754\n",
      "Average test loss: 0.0019790172382361357\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01264001601603296\n",
      "Average test loss: 0.0020210957864506378\n",
      "Epoch 166/300\n",
      "Average training loss: 0.012643527738749981\n",
      "Average test loss: 0.0020582672588320243\n",
      "Epoch 167/300\n",
      "Average training loss: 0.012624190230336453\n",
      "Average test loss: 0.002009722229610715\n",
      "Epoch 168/300\n",
      "Average training loss: 0.012624016745222939\n",
      "Average test loss: 0.0021444559215257567\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012586974423792627\n",
      "Average test loss: 0.0020184365820346607\n",
      "Epoch 170/300\n",
      "Average training loss: 0.012586421849826971\n",
      "Average test loss: 0.0020068602112846244\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012598523307177756\n",
      "Average test loss: 0.0019760089471108386\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012583739998439947\n",
      "Average test loss: 0.002097337212206589\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012587853808369902\n",
      "Average test loss: 0.0020158215494205556\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012553544137212965\n",
      "Average test loss: 0.002024696905993753\n",
      "Epoch 175/300\n",
      "Average training loss: 0.012574366728464762\n",
      "Average test loss: 0.002048426311877039\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01255618314527803\n",
      "Average test loss: 0.0020647035303215187\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012532814239462217\n",
      "Average test loss: 0.0020811754850049813\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012554794497787953\n",
      "Average test loss: 0.0019887174483802584\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012535350400540563\n",
      "Average test loss: 0.002023623573904236\n",
      "Epoch 180/300\n",
      "Average training loss: 0.012530921081701915\n",
      "Average test loss: 0.002043398181235211\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01249982262071636\n",
      "Average test loss: 0.002079328811623984\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01250030035773913\n",
      "Average test loss: 0.0021727798622515468\n",
      "Epoch 183/300\n",
      "Average training loss: 0.012546701360079977\n",
      "Average test loss: 0.001998493671727677\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01248550760663218\n",
      "Average test loss: 0.0019848252820471924\n",
      "Epoch 185/300\n",
      "Average training loss: 0.012489275697204802\n",
      "Average test loss: 0.002030495358320574\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012488097256256474\n",
      "Average test loss: 0.0020257220117168295\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01248605468041367\n",
      "Average test loss: 0.0020311149520178635\n",
      "Epoch 188/300\n",
      "Average training loss: 0.012470853298902511\n",
      "Average test loss: 0.002033954412986835\n",
      "Epoch 189/300\n",
      "Average training loss: 0.012451750838094288\n",
      "Average test loss: 0.00209040254747702\n",
      "Epoch 190/300\n",
      "Average training loss: 0.012476323257717822\n",
      "Average test loss: 0.0021613977619757254\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012438577709926499\n",
      "Average test loss: 0.0022751234612531134\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012485524204042222\n",
      "Average test loss: 0.0020202850010246037\n",
      "Epoch 193/300\n",
      "Average training loss: 0.012397633949087725\n",
      "Average test loss: 0.0020181644736892647\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012417256279951997\n",
      "Average test loss: 0.00204875319213089\n",
      "Epoch 195/300\n",
      "Average training loss: 0.012428695518937376\n",
      "Average test loss: 0.0022223757507486477\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012432173449132177\n",
      "Average test loss: 0.002053460668772459\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012425762474536896\n",
      "Average test loss: 0.0020265403910436565\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012430092077288363\n",
      "Average test loss: 0.002003323467137913\n",
      "Epoch 199/300\n",
      "Average training loss: 0.012395501043233607\n",
      "Average test loss: 0.0020195420997010335\n",
      "Epoch 200/300\n",
      "Average training loss: 0.012394962332314916\n",
      "Average test loss: 0.002031735633810361\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012371689109338654\n",
      "Average test loss: 0.002038710832802786\n",
      "Epoch 202/300\n",
      "Average training loss: 0.012376246401005321\n",
      "Average test loss: 0.0020782460578613812\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012375267937779426\n",
      "Average test loss: 0.0020430139739894206\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01241653068943156\n",
      "Average test loss: 0.0019618925930311283\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012348792172968387\n",
      "Average test loss: 0.0020599722889148527\n",
      "Epoch 206/300\n",
      "Average training loss: 0.012345062817964289\n",
      "Average test loss: 0.002025907829817798\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012367085887326134\n",
      "Average test loss: 0.0020307648548235496\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012341766117347611\n",
      "Average test loss: 0.002013066470416056\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012317660288678275\n",
      "Average test loss: 0.002007892942469981\n",
      "Epoch 210/300\n",
      "Average training loss: 0.012322504108978642\n",
      "Average test loss: 0.0021627146148433286\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012329816095530986\n",
      "Average test loss: 0.002044354731630948\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012329420326484574\n",
      "Average test loss: 0.0021291394961170025\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012330641424490345\n",
      "Average test loss: 0.0019824637414680587\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012342169439627064\n",
      "Average test loss: 0.002045366219348378\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012323243569996622\n",
      "Average test loss: 0.0020182320406246517\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01229942204306523\n",
      "Average test loss: 0.002016569733412729\n",
      "Epoch 217/300\n",
      "Average training loss: 0.012290178697970179\n",
      "Average test loss: 0.0020290658637467356\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012277603486345873\n",
      "Average test loss: 0.0020455260432014864\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012287214087943235\n",
      "Average test loss: 0.0021331921886238786\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01227004032747613\n",
      "Average test loss: 0.002015883603754143\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012228297631773685\n",
      "Average test loss: 0.0020339489906198447\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012273495538367166\n",
      "Average test loss: 0.0020489967368129227\n",
      "Epoch 223/300\n",
      "Average training loss: 0.012271406410468949\n",
      "Average test loss: 0.001994857644662261\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0122752821991841\n",
      "Average test loss: 0.0020138648870504563\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012273854547904597\n",
      "Average test loss: 0.0020326026369714076\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012241425351964102\n",
      "Average test loss: 0.0020985584223849906\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01223495063599613\n",
      "Average test loss: 0.002036953954026103\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012234959956672457\n",
      "Average test loss: 0.001976707841994034\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012211485972007117\n",
      "Average test loss: 0.002039944224473503\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01222020133998659\n",
      "Average test loss: 0.002059620959477292\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012213247388601304\n",
      "Average test loss: 0.0019985587535839944\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012221766461100843\n",
      "Average test loss: 0.001989871851582494\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012196043148636818\n",
      "Average test loss: 0.0022080588042736053\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012210800330672\n",
      "Average test loss: 0.0021102706632680364\n",
      "Epoch 235/300\n",
      "Average training loss: 0.012201096195727587\n",
      "Average test loss: 0.002117910824923052\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012182208123306433\n",
      "Average test loss: 0.0020738637782633305\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012186789873573516\n",
      "Average test loss: 0.0020027424664133125\n",
      "Epoch 238/300\n",
      "Average training loss: 0.012219099229408635\n",
      "Average test loss: 0.0020394786371745995\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012193592954840925\n",
      "Average test loss: 0.0020904576970885197\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01215880247205496\n",
      "Average test loss: 0.0020406044809561635\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012194336684213745\n",
      "Average test loss: 0.002082107941309611\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012155183179510965\n",
      "Average test loss: 0.0020530555124084154\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012156415339973237\n",
      "Average test loss: 0.0020540816068856252\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012169309625195133\n",
      "Average test loss: 0.002009916073021789\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012140428703692224\n",
      "Average test loss: 0.002050056619052258\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012188905719253752\n",
      "Average test loss: 0.0022050019297748806\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01214140882756975\n",
      "Average test loss: 0.0020225020478376082\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012129994242555566\n",
      "Average test loss: 0.0020489126187231807\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012160112821393542\n",
      "Average test loss: 0.0023604319157699742\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012147586579124132\n",
      "Average test loss: 0.002121052358092533\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012115227499769794\n",
      "Average test loss: 0.0022358153520358934\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012117450365589725\n",
      "Average test loss: 0.002055878776539531\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012119655882318815\n",
      "Average test loss: 0.0020829263568545382\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01209135972791248\n",
      "Average test loss: 0.0021778591704658336\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012118822414014074\n",
      "Average test loss: 0.002061215787091189\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012071951785021358\n",
      "Average test loss: 0.0020909865382644863\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01212221324443817\n",
      "Average test loss: 0.0021110282300247085\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01207083822372887\n",
      "Average test loss: 0.002005149031057954\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012095631917317709\n",
      "Average test loss: 0.00203390796472215\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012081293876800273\n",
      "Average test loss: 0.0020028535808540053\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012083074265056186\n",
      "Average test loss: 0.0021391315534710882\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012084185015824107\n",
      "Average test loss: 0.0020792372909684974\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012055629644956854\n",
      "Average test loss: 0.002073842335699333\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012073935059209664\n",
      "Average test loss: 0.002026021474144525\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01206872585333056\n",
      "Average test loss: 0.002145392026545273\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012068336847755644\n",
      "Average test loss: 0.0021295131574281386\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01206319790499078\n",
      "Average test loss: 0.0021304290867928\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012053209960460663\n",
      "Average test loss: 0.002033322835444576\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012068308581494623\n",
      "Average test loss: 0.002060313892033365\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012040449371768368\n",
      "Average test loss: 0.0020841604102816848\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012063948411908415\n",
      "Average test loss: 0.002158644222137001\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012031826356218921\n",
      "Average test loss: 0.002087554625235498\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012015598841011525\n",
      "Average test loss: 0.002041489972008599\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012033295341663891\n",
      "Average test loss: 0.0020965359375501673\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01199852540012863\n",
      "Average test loss: 0.002071998386540347\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012056208170950412\n",
      "Average test loss: 0.002060673057826029\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012013773757550451\n",
      "Average test loss: 0.002048707006395691\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012009414411253399\n",
      "Average test loss: 0.0020788209103047846\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011984360996219847\n",
      "Average test loss: 0.002096552315271563\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01200438010527028\n",
      "Average test loss: 0.002109824060979817\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012024265550904804\n",
      "Average test loss: 0.0022079624727161396\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012003182901276482\n",
      "Average test loss: 0.0020949825280242498\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012013768235842387\n",
      "Average test loss: 0.0021296561809463635\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01200508100208309\n",
      "Average test loss: 0.002013872023154464\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011978909766508474\n",
      "Average test loss: 0.0020688116453174087\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01198789067980316\n",
      "Average test loss: 0.002020793562858469\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01198327798065212\n",
      "Average test loss: 0.002045090140360925\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011972052524487178\n",
      "Average test loss: 0.0020503522114207347\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011988790521191226\n",
      "Average test loss: 0.0020165627505630253\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011957189881139332\n",
      "Average test loss: 0.002069867282691929\n",
      "Epoch 291/300\n",
      "Average training loss: 0.011994825371437602\n",
      "Average test loss: 0.0020806631354822053\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011967444407443205\n",
      "Average test loss: 0.0021259360702501405\n",
      "Epoch 293/300\n",
      "Average training loss: 0.011946224469277594\n",
      "Average test loss: 0.002051280476566818\n",
      "Epoch 294/300\n",
      "Average training loss: 0.011951986650625865\n",
      "Average test loss: 0.0021674796388381056\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011928914124766985\n",
      "Average test loss: 0.0021891184442987045\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011972410692936844\n",
      "Average test loss: 0.002195654986219274\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01194685747143295\n",
      "Average test loss: 0.002158614862917198\n",
      "Epoch 298/300\n",
      "Average training loss: 0.011931523677375582\n",
      "Average test loss: 0.0020713415210031802\n",
      "Epoch 299/300\n",
      "Average training loss: 0.011949594140052795\n",
      "Average test loss: 0.0021608952906810577\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011965850941008991\n",
      "Average test loss: 0.0022030464633264477\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive_Depth10-.025/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.22\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.67\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.20\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.50\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.99\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.55\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.02\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.82\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.50\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.862733202616374\n",
      "Average test loss: 6.236868918922212\n",
      "Epoch 2/300\n",
      "Average training loss: 2.6285554127163357\n",
      "Average test loss: 0.018840435643990835\n",
      "Epoch 3/300\n",
      "Average training loss: 1.7523709715737237\n",
      "Average test loss: 0.012949433013796805\n",
      "Epoch 4/300\n",
      "Average training loss: 1.367715786827935\n",
      "Average test loss: 0.018997218386994467\n",
      "Epoch 5/300\n",
      "Average training loss: 1.1091278901629977\n",
      "Average test loss: 0.033275230257047546\n",
      "Epoch 6/300\n",
      "Average training loss: 0.9384700948397319\n",
      "Average test loss: 0.010741315353247854\n",
      "Epoch 7/300\n",
      "Average training loss: 0.7907541596094767\n",
      "Average test loss: 0.009173011912239923\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6819882068104214\n",
      "Average test loss: 0.009446519528826077\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5986994434992472\n",
      "Average test loss: 0.008711050896181\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5286616548167334\n",
      "Average test loss: 0.008043576459089916\n",
      "Epoch 11/300\n",
      "Average training loss: 0.47003032485644025\n",
      "Average test loss: 0.008550252735614777\n",
      "Epoch 12/300\n",
      "Average training loss: 0.4194840571615431\n",
      "Average test loss: 0.011743732185827362\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3771589785416921\n",
      "Average test loss: 0.008520738816095724\n",
      "Epoch 14/300\n",
      "Average training loss: 0.34056833985116747\n",
      "Average test loss: 0.007424766092664666\n",
      "Epoch 15/300\n",
      "Average training loss: 0.30776219884554545\n",
      "Average test loss: 0.007230332169267866\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2809871739546458\n",
      "Average test loss: 0.006985247724586063\n",
      "Epoch 17/300\n",
      "Average training loss: 0.25842154921425714\n",
      "Average test loss: 0.009039608584923876\n",
      "Epoch 18/300\n",
      "Average training loss: 0.23767936502562628\n",
      "Average test loss: 0.006540483146905899\n",
      "Epoch 19/300\n",
      "Average training loss: 0.22035704534583622\n",
      "Average test loss: 0.0071530955872601935\n",
      "Epoch 20/300\n",
      "Average training loss: 0.20626340256796943\n",
      "Average test loss: 0.006445156881792678\n",
      "Epoch 21/300\n",
      "Average training loss: 0.19432803085115222\n",
      "Average test loss: 0.006433460760861635\n",
      "Epoch 22/300\n",
      "Average training loss: 0.18483973485893673\n",
      "Average test loss: 0.019282051301664778\n",
      "Epoch 23/300\n",
      "Average training loss: 0.17510635373327468\n",
      "Average test loss: 0.08629068919685152\n",
      "Epoch 24/300\n",
      "Average training loss: 0.16869374185138278\n",
      "Average test loss: 0.00670054629072547\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1619704500834147\n",
      "Average test loss: 0.0115514425188303\n",
      "Epoch 26/300\n",
      "Average training loss: 0.15514112720224593\n",
      "Average test loss: 0.005971301380544901\n",
      "Epoch 27/300\n",
      "Average training loss: 0.14883448926607767\n",
      "Average test loss: 0.03186993331048224\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14324211098088158\n",
      "Average test loss: 0.006071502777023448\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13989772991339366\n",
      "Average test loss: 0.006136982837071021\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13653663569026522\n",
      "Average test loss: 0.006814211956742737\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1311748466624154\n",
      "Average test loss: 0.005796299349930551\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12870113122463225\n",
      "Average test loss: 0.005908217177622848\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12598643920818964\n",
      "Average test loss: 0.007881976254698303\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12281314693556891\n",
      "Average test loss: 0.005696751728653908\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11951173160473505\n",
      "Average test loss: 0.005916316488550769\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11652582703034083\n",
      "Average test loss: 0.21475640169282753\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11505216830306583\n",
      "Average test loss: 0.005817068703058693\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11181946428616842\n",
      "Average test loss: 0.0067150010996394685\n",
      "Epoch 39/300\n",
      "Average training loss: 0.109761631515291\n",
      "Average test loss: 0.005760184925463464\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10805317395263248\n",
      "Average test loss: 0.15642040558656056\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10627663149436314\n",
      "Average test loss: 0.005566407290597757\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10421956864992778\n",
      "Average test loss: 0.005725799031555652\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10250488130251567\n",
      "Average test loss: 0.005570197710560428\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10132812160915798\n",
      "Average test loss: 0.006554591758383645\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09946783077716827\n",
      "Average test loss: 0.005878125632802645\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09848210477166705\n",
      "Average test loss: 0.006107198319501347\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09742664988173379\n",
      "Average test loss: 0.005680051787032021\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09607991695404053\n",
      "Average test loss: 0.11458762562274932\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09537913776106305\n",
      "Average test loss: 0.0074492467985385\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09411419875091977\n",
      "Average test loss: 0.0727738569461637\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09338726384772195\n",
      "Average test loss: 0.0057489401325583455\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09203930501143137\n",
      "Average test loss: 0.00836845289543271\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09158806346522437\n",
      "Average test loss: 0.005573319914440314\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0912395445505778\n",
      "Average test loss: 0.05816142698584331\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08988354202111562\n",
      "Average test loss: 0.019236540991399022\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08948270604345533\n",
      "Average test loss: 1435.0729891849094\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08871904332770242\n",
      "Average test loss: 0.0056932132906383935\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08802288913726806\n",
      "Average test loss: 0.005512334093865421\n",
      "Epoch 59/300\n",
      "Average training loss: 0.087013050970104\n",
      "Average test loss: 0.005548273995104764\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08681819521718555\n",
      "Average test loss: 0.006057103059358067\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08837113764550951\n",
      "Average test loss: 0.008230773559461037\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08655362676911883\n",
      "Average test loss: 0.006287755694240332\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08494725119074185\n",
      "Average test loss: 0.006631095757914914\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08450044788916906\n",
      "Average test loss: 0.006300495301683744\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08399898202551735\n",
      "Average test loss: 0.031424319180142546\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08358736470672819\n",
      "Average test loss: 0.00760720552628239\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08337060912450155\n",
      "Average test loss: 0.0057302592251863745\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0826925655470954\n",
      "Average test loss: 0.005598510441680749\n",
      "Epoch 69/300\n",
      "Average training loss: 0.44837229007482526\n",
      "Average test loss: 0.011600586362597014\n",
      "Epoch 70/300\n",
      "Average training loss: 0.309001957376798\n",
      "Average test loss: 0.006838232231636842\n",
      "Epoch 71/300\n",
      "Average training loss: 0.16427110531595018\n",
      "Average test loss: 0.006134908330109384\n",
      "Epoch 72/300\n",
      "Average training loss: 0.14315782419840495\n",
      "Average test loss: 0.0059283807352185245\n",
      "Epoch 73/300\n",
      "Average training loss: 0.13153594202465482\n",
      "Average test loss: 0.11922039823896355\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12381359123521381\n",
      "Average test loss: 0.02240966969397333\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11787341233094534\n",
      "Average test loss: 0.005696059110677904\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11333504792054494\n",
      "Average test loss: 0.005849829181200928\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1099423552221722\n",
      "Average test loss: 0.005714821350243356\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10679838679896461\n",
      "Average test loss: 0.005735166658129957\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10460442413224115\n",
      "Average test loss: 0.0054947129591471616\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10237604097525278\n",
      "Average test loss: 0.0055007210524959695\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10035841178894044\n",
      "Average test loss: 0.005567428807831473\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09867195326089859\n",
      "Average test loss: 0.005470011291404565\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09704776355955336\n",
      "Average test loss: 0.0062837759016288654\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09550474966896905\n",
      "Average test loss: 0.005907865877780649\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09417597394519382\n",
      "Average test loss: 0.005429303861326642\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09293955224752426\n",
      "Average test loss: 0.005581490274104807\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09172739094495773\n",
      "Average test loss: 0.005459097432179583\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09054384829269516\n",
      "Average test loss: 0.006029462174822886\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08979542684555054\n",
      "Average test loss: 0.0054870117733048064\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08856625548998515\n",
      "Average test loss: 0.005597741302516725\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0877860747244623\n",
      "Average test loss: 0.005481527299516731\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0867462521923913\n",
      "Average test loss: 0.0063019159692857\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08572968133952882\n",
      "Average test loss: 0.005518107345741656\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08521640905737878\n",
      "Average test loss: 0.005609260558254189\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08443436775604884\n",
      "Average test loss: 0.005890668893853823\n",
      "Epoch 96/300\n",
      "Average training loss: 0.083956454747253\n",
      "Average test loss: 0.0057484329620169266\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08319892879989411\n",
      "Average test loss: 0.009125883514682452\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08248121194044748\n",
      "Average test loss: 0.005565443824562762\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08208635465966331\n",
      "Average test loss: 0.006371381535298294\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08181352655092876\n",
      "Average test loss: 0.005486864945954747\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0810096692442894\n",
      "Average test loss: 0.006204413927677605\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08257919238011042\n",
      "Average test loss: 0.0057656012492047415\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08005012975136439\n",
      "Average test loss: 0.03981248851120472\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07987586928738488\n",
      "Average test loss: 0.005582684787197246\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07946286966403325\n",
      "Average test loss: 0.005524898697104719\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07930476948287751\n",
      "Average test loss: 0.005594010292862853\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07931961231761508\n",
      "Average test loss: 0.005578317302589615\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07866055962774489\n",
      "Average test loss: 0.0069162264354527\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07853217655420304\n",
      "Average test loss: 0.007828396445761124\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07775086716810863\n",
      "Average test loss: 0.009282115676336819\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07744835812515682\n",
      "Average test loss: 0.005745123981187741\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07724016983641518\n",
      "Average test loss: 0.005680288599183162\n",
      "Epoch 113/300\n",
      "Average training loss: 18.449135665920046\n",
      "Average test loss: 0.031702619420157535\n",
      "Epoch 114/300\n",
      "Average training loss: 2.616987025366889\n",
      "Average test loss: 0.12811433083481258\n",
      "Epoch 115/300\n",
      "Average training loss: 1.4104297937817043\n",
      "Average test loss: 0.009030095634361109\n",
      "Epoch 116/300\n",
      "Average training loss: 1.027065500418345\n",
      "Average test loss: 0.015621292208217912\n",
      "Epoch 117/300\n",
      "Average training loss: 0.7959997149573432\n",
      "Average test loss: 0.008701631706621912\n",
      "Epoch 118/300\n",
      "Average training loss: 0.6352637010150486\n",
      "Average test loss: 0.007776362310681078\n",
      "Epoch 119/300\n",
      "Average training loss: 0.5187001593642765\n",
      "Average test loss: 0.009246730657915274\n",
      "Epoch 120/300\n",
      "Average training loss: 0.43124441753493414\n",
      "Average test loss: 0.007206147894263267\n",
      "Epoch 121/300\n",
      "Average training loss: 0.3613734595245785\n",
      "Average test loss: 0.009171526835196548\n",
      "Epoch 122/300\n",
      "Average training loss: 0.30519165261586506\n",
      "Average test loss: 0.012635102231469419\n",
      "Epoch 123/300\n",
      "Average training loss: 0.2610254633956485\n",
      "Average test loss: 0.008530388895008298\n",
      "Epoch 124/300\n",
      "Average training loss: 0.2279916334549586\n",
      "Average test loss: 0.006368477512978845\n",
      "Epoch 125/300\n",
      "Average training loss: 0.20474792904324002\n",
      "Average test loss: 0.0061814944930374625\n",
      "Epoch 126/300\n",
      "Average training loss: 0.18712874858909184\n",
      "Average test loss: 0.006251471791830328\n",
      "Epoch 127/300\n",
      "Average training loss: 0.1746429646677441\n",
      "Average test loss: 0.006074364209340678\n",
      "Epoch 128/300\n",
      "Average training loss: 0.1641968719959259\n",
      "Average test loss: 0.007599282253533602\n",
      "Epoch 129/300\n",
      "Average training loss: 0.1560250897407532\n",
      "Average test loss: 0.005864016584224171\n",
      "Epoch 130/300\n",
      "Average training loss: 0.14909531897968717\n",
      "Average test loss: 0.006959436681121587\n",
      "Epoch 131/300\n",
      "Average training loss: 0.1421637683974372\n",
      "Average test loss: 0.006560112645228704\n",
      "Epoch 132/300\n",
      "Average training loss: 0.13684053436252805\n",
      "Average test loss: 0.005971983907951249\n",
      "Epoch 133/300\n",
      "Average training loss: 0.1314149272110727\n",
      "Average test loss: 0.005780190130902661\n",
      "Epoch 134/300\n",
      "Average training loss: 0.1261263853046629\n",
      "Average test loss: 0.006049663235329919\n",
      "Epoch 135/300\n",
      "Average training loss: 0.12093563481171926\n",
      "Average test loss: 0.005583667825079627\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11675529864761565\n",
      "Average test loss: 0.006151578377518389\n",
      "Epoch 137/300\n",
      "Average training loss: 0.11146803098254733\n",
      "Average test loss: 0.005534209109842777\n",
      "Epoch 138/300\n",
      "Average training loss: 0.1079220213757621\n",
      "Average test loss: 0.00551200970262289\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10351841831869549\n",
      "Average test loss: 0.005697307538241148\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10046550800071823\n",
      "Average test loss: 0.005532286415911383\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09870663247505823\n",
      "Average test loss: 0.006789779119193554\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10245454239183002\n",
      "Average test loss: 0.005766200010975202\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09620088858736886\n",
      "Average test loss: 0.005445686199598843\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09294225825203789\n",
      "Average test loss: 0.005475446975065602\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09108960860305362\n",
      "Average test loss: 0.005477287032951911\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08904990769757165\n",
      "Average test loss: 0.0054506293887065516\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08762507273753484\n",
      "Average test loss: 0.005451211920215024\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08583895648850336\n",
      "Average test loss: 0.00548573218492998\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08421579625209173\n",
      "Average test loss: 0.005966050895551841\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08270671624276373\n",
      "Average test loss: 0.005591855425387621\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08149679197867711\n",
      "Average test loss: 0.005633519907792409\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08017948255936305\n",
      "Average test loss: 0.005589809045195579\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07982906601164076\n",
      "Average test loss: 0.0056535239012704954\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07891107865836886\n",
      "Average test loss: 0.10422291898727418\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07787696486049228\n",
      "Average test loss: 0.0092490666790141\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07754868187506994\n",
      "Average test loss: 0.01138210117775533\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07723517837789323\n",
      "Average test loss: 0.0059040425883399115\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07671404978964064\n",
      "Average test loss: 0.005607716314080689\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07649618162049188\n",
      "Average test loss: 0.005702372846711013\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07621798121266894\n",
      "Average test loss: 0.005868545718491078\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07573724936114418\n",
      "Average test loss: 0.007160505942586395\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0755540596379174\n",
      "Average test loss: 0.013320752081771692\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07523641822073195\n",
      "Average test loss: 0.024897453187240494\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07504859967364205\n",
      "Average test loss: 0.0057731338271664245\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07455458720525106\n",
      "Average test loss: 0.005829051535576582\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07424595918920306\n",
      "Average test loss: 0.005783018758313523\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07436903507841958\n",
      "Average test loss: 0.00577615941191713\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07365999252597491\n",
      "Average test loss: 0.00576979363626904\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0735983999967575\n",
      "Average test loss: 0.00583135571765403\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07342103372017543\n",
      "Average test loss: 0.005746473965131574\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07337506300873227\n",
      "Average test loss: 0.005818626460101869\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07310462333096399\n",
      "Average test loss: 0.0108687501632505\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07282113643156157\n",
      "Average test loss: 0.006020640878213777\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07261214912599988\n",
      "Average test loss: 0.00615812981997927\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07234850311941571\n",
      "Average test loss: 0.006059487273295721\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07251634647448858\n",
      "Average test loss: 0.006220585445562999\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07195586368772719\n",
      "Average test loss: 0.006479411650862959\n",
      "Epoch 178/300\n",
      "Average training loss: 0.1034126698507203\n",
      "Average test loss: 0.0060654761571851045\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09234763864013884\n",
      "Average test loss: 0.0055704575031995775\n",
      "Epoch 180/300\n",
      "Average training loss: 0.081830720262395\n",
      "Average test loss: 0.005815121053821511\n",
      "Epoch 181/300\n",
      "Average training loss: 0.12153971413771311\n",
      "Average test loss: 0.005986750387483173\n",
      "Epoch 183/300\n",
      "Average training loss: 0.1075137593348821\n",
      "Average test loss: 0.005582526132464409\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10012389647960662\n",
      "Average test loss: 0.005464762702170346\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09476521004570855\n",
      "Average test loss: 0.005502384165922801\n",
      "Epoch 186/300\n",
      "Average training loss: 0.090843912853135\n",
      "Average test loss: 0.005552905985464652\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08494273963570595\n",
      "Average test loss: 0.0057530211032264765\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08245866261588203\n",
      "Average test loss: 0.006209535239057409\n",
      "Epoch 190/300\n",
      "Average training loss: 0.079887585706181\n",
      "Average test loss: 0.005678051775942246\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07741820639371871\n",
      "Average test loss: 0.005638694442808628\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0758992152147823\n",
      "Average test loss: 0.0059507479030225015\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0745587519009908\n",
      "Average test loss: 0.005871820206650429\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07372706771559186\n",
      "Average test loss: 0.009983644058307012\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08191113157404793\n",
      "Average test loss: 0.04060600078105926\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09504372733831405\n",
      "Average test loss: 0.005689594575928317\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07680714446306229\n",
      "Average test loss: 0.005747508181465997\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07361822993887795\n",
      "Average test loss: 0.00600550459490882\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07249908618132274\n",
      "Average test loss: 0.005909842230379581\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07204284770621194\n",
      "Average test loss: 0.005849810660299328\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07185140552123388\n",
      "Average test loss: 0.005935153322501315\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07178685941629939\n",
      "Average test loss: 0.005859900901714961\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0711380152437422\n",
      "Average test loss: 0.0061110081068343584\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07150038849976327\n",
      "Average test loss: 0.005942304486201869\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07079242623514599\n",
      "Average test loss: 0.00894198055896494\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07076333496305677\n",
      "Average test loss: 0.009251301352348593\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07046969620055622\n",
      "Average test loss: 0.0058035734941562014\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07139108495579825\n",
      "Average test loss: 0.019135223537683488\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07008090758323669\n",
      "Average test loss: 0.00745994227213992\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07000881489780214\n",
      "Average test loss: 0.005863857708043522\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06993344862924682\n",
      "Average test loss: 0.005922284481012159\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07007303149170346\n",
      "Average test loss: 0.005945984821766615\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07167805265718036\n",
      "Average test loss: 0.005638918564965328\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07123494897617234\n",
      "Average test loss: 0.005880848236175047\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06941195533010695\n",
      "Average test loss: 0.006360062684035964\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06911602032515737\n",
      "Average test loss: 0.0063572172522544865\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06935556219683753\n",
      "Average test loss: 0.021605990760856203\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06901818988720576\n",
      "Average test loss: 0.011703429065644741\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06898260490761862\n",
      "Average test loss: 0.0064569986052811145\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06911984153257476\n",
      "Average test loss: 0.006474963256882297\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06876911975940068\n",
      "Average test loss: 0.005985422801226378\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06946792674726911\n",
      "Average test loss: 0.005934437987705072\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08077983781033092\n",
      "Average test loss: 0.005624974672165182\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07929895345038838\n",
      "Average test loss: 0.006019540841380755\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07111136583487193\n",
      "Average test loss: 0.005766736479269134\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0691715745528539\n",
      "Average test loss: 0.005843583736982611\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06844745806521839\n",
      "Average test loss: 0.0061018328186538484\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06856586066881816\n",
      "Average test loss: 0.0059065019823610785\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06809489146206114\n",
      "Average test loss: 0.005954850858698289\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06810750088426802\n",
      "Average test loss: 0.005962222798830933\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0679264857504103\n",
      "Average test loss: 0.006186129511644443\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06802172210481432\n",
      "Average test loss: 0.005912979402268926\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06836722871661186\n",
      "Average test loss: 0.0063481771888004405\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06771981851922142\n",
      "Average test loss: 0.006092668809410598\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0675720924006568\n",
      "Average test loss: 0.005962553195655346\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06747394651174546\n",
      "Average test loss: 0.006361803103652266\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06752689764565892\n",
      "Average test loss: 0.0059784196503460405\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06729938230249617\n",
      "Average test loss: 0.00763205993672212\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06748722014162276\n",
      "Average test loss: 0.005973553331361877\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06720686704582639\n",
      "Average test loss: 0.006219091263496213\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0670693422721492\n",
      "Average test loss: 0.00639039766912659\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06703799775573942\n",
      "Average test loss: 0.006093244493835502\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06839420774910185\n",
      "Average test loss: 11.139564420196745\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06700217576159372\n",
      "Average test loss: 0.006013159488638242\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06653494994507896\n",
      "Average test loss: 0.006008016901297702\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06650567787223392\n",
      "Average test loss: 0.005991096771839592\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06672725013229582\n",
      "Average test loss: 0.006155617690748638\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06607703506946563\n",
      "Average test loss: 0.005909118776934014\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06605335389243232\n",
      "Average test loss: 0.00815918263875776\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06634298676914639\n",
      "Average test loss: 0.006494450837373733\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06607306204239527\n",
      "Average test loss: 0.0060756279673013424\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06606501557429631\n",
      "Average test loss: 0.006010223117139605\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06606909677717421\n",
      "Average test loss: 0.00683137448463175\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06574090087413788\n",
      "Average test loss: 0.006149213011066119\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06576297518279817\n",
      "Average test loss: 0.011606132581830025\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06589115479257372\n",
      "Average test loss: 0.006220624806980292\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06550895693567065\n",
      "Average test loss: 0.006089136778066556\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06568456310033798\n",
      "Average test loss: 0.0062875922491980926\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06568436766995324\n",
      "Average test loss: 0.00603730553181635\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06552620560924212\n",
      "Average test loss: 0.006063631673239999\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06540377625491885\n",
      "Average test loss: 0.0068938837411503\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06567687772711119\n",
      "Average test loss: 0.006089848069681061\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06507706129550934\n",
      "Average test loss: 0.009059732739296224\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06512232226795621\n",
      "Average test loss: 0.058108078204923204\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06582822258604898\n",
      "Average training loss: 0.06520179465081957\n",
      "Average test loss: 0.005919292553431458\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06658839717176225\n",
      "Average test loss: 0.0061227067021860015\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06480548852682114\n",
      "Average test loss: 0.006117678371568521\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06470496873060862\n",
      "Average test loss: 0.006293550888697306\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06459422975116307\n",
      "Average test loss: 0.0059961602232522435\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06463296052482392\n",
      "Average test loss: 0.00618323354754183\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06483010107941098\n",
      "Average test loss: 0.006014874830014176\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06510641619894239\n",
      "Average test loss: 0.006032020649148358\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06456667375233438\n",
      "Average test loss: 0.006536503935025798\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0644550234410498\n",
      "Average test loss: 0.006127012191547288\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06528528881735272\n",
      "Average test loss: 0.0062648564941353265\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06415821650955413\n",
      "Average test loss: 0.006286854048156076\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06404732814762328\n",
      "Average test loss: 0.006074502483838134\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06435980139838325\n",
      "Average test loss: 0.0060419954173266885\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06414125453763538\n",
      "Average test loss: 0.006227177355852392\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06432188487715192\n",
      "Average test loss: 0.005984069135453966\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06430386343267229\n",
      "Average test loss: 0.0060584616884589194\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06403338062763214\n",
      "Average test loss: 0.006174813872410191\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06540379366278648\n",
      "Average test loss: 0.0062759591725965345\n",
      "Epoch 291/300\n",
      "Average test loss: 0.006209102895938688\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06374771491686503\n",
      "Average test loss: 0.007208394330408838\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06390023991796706\n",
      "Average test loss: 0.04882100839250617\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06368106079763837\n",
      "Average test loss: 0.0062033495290411845\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0645854929553138\n",
      "Average test loss: 0.005986789693848954\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06332239228155878\n",
      "Average test loss: 0.006102802760485145\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06336206752724118\n",
      "Average test loss: 0.007009021511094437\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06441442357500395\n",
      "Average test loss: 0.006137036233726475\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.087266638649835\n",
      "Average test loss: 0.29654339223437837\n",
      "Epoch 2/300\n",
      "Average training loss: 2.353850738313463\n",
      "Average test loss: 0.011270819688836734\n",
      "Epoch 3/300\n",
      "Average training loss: 1.677370915942722\n",
      "Average test loss: 0.03751764185561074\n",
      "Epoch 4/300\n",
      "Average training loss: 1.259936486032274\n",
      "Average test loss: 0.006815172291050354\n",
      "Epoch 5/300\n",
      "Average training loss: 1.0384461705419752\n",
      "Average test loss: 0.006320407834317949\n",
      "Epoch 6/300\n",
      "Average training loss: 0.890883404413859\n",
      "Average test loss: 0.006207268862260713\n",
      "Epoch 7/300\n",
      "Average training loss: 0.7735367111629909\n",
      "Average test loss: 0.006111242943339878\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6761547900835673\n",
      "Average test loss: 0.005704578284588125\n",
      "Epoch 9/300\n",
      "Average training loss: 0.593710488266415\n",
      "Average test loss: 0.007636827880723609\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5267105986277263\n",
      "Average test loss: 0.005374413224971957\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4697586168977949\n",
      "Average test loss: 0.027001620964871513\n",
      "Epoch 12/300\n",
      "Average training loss: 0.42032956012090045\n",
      "Average test loss: 0.004646369587423073\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3768044477833642\n",
      "Average test loss: 0.004813365384936333\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2715801137420866\n",
      "Average test loss: 0.011587695341971186\n",
      "Epoch 17/300\n",
      "Average training loss: 0.24408293101522657\n",
      "Average test loss: 0.005538667756650183\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2210262467066447\n",
      "Average test loss: 0.0071600702609866855\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2010309608247545\n",
      "Average test loss: 0.0040696987501449055\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1826249074406094\n",
      "Average test loss: 0.003969189197238949\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16829125136799283\n",
      "Average test loss: 0.004499641872321566\n",
      "Epoch 22/300\n",
      "Average training loss: 0.15687979233264923\n",
      "Average test loss: 0.0039506938577526145\n",
      "Epoch 23/300\n",
      "Average training loss: 0.14457048987017737\n",
      "Average test loss: 0.00536160038328833\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1356473436686728\n",
      "Average test loss: 0.01376383973678781\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12693014646901024\n",
      "Average test loss: 0.0036389759845203823\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12024104271994697\n",
      "Average test loss: 0.012934275182998842\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11371797269582748\n",
      "Average test loss: 0.0035638046980732015\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10729976281854842\n",
      "Average test loss: 0.0036122017167508603\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10271012834707896\n",
      "Average test loss: 0.003551962044917875\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09908244329690934\n",
      "Average test loss: 0.5624534382356537\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09535748812887404\n",
      "Average test loss: 0.020915473797255092\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08553000579939948\n",
      "Average test loss: 0.0033940559507658085\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0825709767209159\n",
      "Average test loss: 0.0033911809225877125\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08072217885984315\n",
      "Average test loss: 0.0033629958325376115\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07864724874496459\n",
      "Average test loss: 0.0034341216509540876\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07632266467147403\n",
      "Average test loss: 0.003413415030679769\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07412477443615595\n",
      "Average test loss: 0.0034977419148716662\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07239914180835089\n",
      "Average test loss: 0.003429443909683161\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0706273413863447\n",
      "Average test loss: 0.008237714231842094\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06946840135256449\n",
      "Average test loss: 0.004096063285238213\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06758437943458558\n",
      "Average test loss: 0.0033182642865512107\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06621240316496955\n",
      "Average test loss: 0.00361213767197397\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06501910813649496\n",
      "Average test loss: 0.003364928255478541\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06398479805390041\n",
      "Average test loss: 0.0034948759312844937\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06309385052654479\n",
      "Average test loss: 0.0045618085174096955\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06228940242197779\n",
      "Average test loss: 0.0034560604852934677\n",
      "Epoch 49/300\n",
      "Average training loss: 0.061495344350735344\n",
      "Average test loss: 0.0040505471010175015\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06302816351254781\n",
      "Average test loss: 0.0033748942917833724\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06878987216618326\n",
      "Average test loss: 0.0035798185161418387\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06604552909400728\n",
      "Average test loss: 0.005483690398848719\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06426017446650399\n",
      "Average test loss: 0.0034621649258252648\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06299201748106215\n",
      "Average test loss: 0.0037550346126986873\n",
      "Epoch 57/300\n",
      "Average training loss: 0.062433011521895725\n",
      "Average test loss: 0.0032827755999233986\n",
      "Epoch 58/300\n",
      "Average training loss: 0.061226909266577825\n",
      "Average test loss: 0.004607931578324901\n",
      "Epoch 59/300\n",
      "Average training loss: 0.060544147862328423\n",
      "Average test loss: 0.003277613050614794\n",
      "Epoch 60/300\n",
      "Average training loss: 0.060381752490997316\n",
      "Average test loss: 0.0050179036590788095\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0595934798154566\n",
      "Average test loss: 0.003303409234724111\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05888751428657108\n",
      "Average test loss: 0.004175951462239027\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05850375066532029\n",
      "Average test loss: 0.0032253510943717426\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05802263497312864\n",
      "Average test loss: 0.05561673477705982\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05797961179084248\n",
      "Average test loss: 0.003315351729384727\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05723704845375485\n",
      "Average test loss: 0.003252289423098167\n",
      "Epoch 67/300\n",
      "Average training loss: 0.056771263817946116\n",
      "Average test loss: 0.005675092674377892\n",
      "Epoch 68/300\n",
      "Average training loss: 0.44595026710298324\n",
      "Average test loss: 0.004223702861203087\n",
      "Epoch 71/300\n",
      "Average training loss: 0.14530281534459855\n",
      "Average test loss: 0.003620662845257256\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11234465401040183\n",
      "Average test loss: 0.0036222520814173753\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09696987666686376\n",
      "Average test loss: 0.0033900146091149913\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08619729206959406\n",
      "Average test loss: 0.003337325681414869\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07923048604196972\n",
      "Average test loss: 0.003332215880561206\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07490493465794458\n",
      "Average test loss: 0.0033669687381221187\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07225788991649945\n",
      "Average test loss: 0.003626017400994897\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07006514042284753\n",
      "Average test loss: 0.0033733620297991566\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06797947176297506\n",
      "Average test loss: 0.003814285621874862\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06645202758577134\n",
      "Average test loss: 0.004785046371113923\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06508741499318017\n",
      "Average test loss: 0.0034031638546536368\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06373170190387302\n",
      "Average test loss: 0.00408480289992359\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06243597752186987\n",
      "Average training loss: 0.060781550063027275\n",
      "Average test loss: 0.0033583463242070543\n",
      "Epoch 86/300\n",
      "Average training loss: 0.059783891446060604\n",
      "Average test loss: 0.004532401170581579\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05944537498222457\n",
      "Average test loss: 0.005156775072630909\n",
      "Epoch 88/300\n",
      "Average training loss: 0.058739790625042386\n",
      "Average test loss: 0.0032371751051396133\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0582096441951063\n",
      "Average test loss: 0.00328941321435074\n",
      "Epoch 90/300\n",
      "Average training loss: 0.057685168312655556\n",
      "Average test loss: 0.0034981331798351473\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05727152222063806\n",
      "Average test loss: 0.0032337820385065345\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0568742034567727\n",
      "Average test loss: 0.003392735893527667\n",
      "Epoch 93/300\n",
      "Average training loss: 0.056558016068405575\n",
      "Average test loss: 0.003368408410085572\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05611828730834855\n",
      "Average test loss: 0.00332789101327459\n",
      "Epoch 95/300\n",
      "Average training loss: 0.055759064101510575\n",
      "Average test loss: 0.8460835858964257\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05550847966141171\n",
      "Average test loss: 0.0032958951253030037\n",
      "Epoch 97/300\n",
      "Average training loss: 0.055222113957007726\n",
      "Average test loss: 0.005662924737566047\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05489578786823485\n",
      "Average test loss: 0.0033226876573430168\n",
      "Epoch 99/300\n",
      "Average training loss: 0.054701162639591426\n",
      "Average test loss: 0.0032940635768075785\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05432028434342808\n",
      "Average test loss: 0.0033151212160785993\n",
      "Epoch 101/300\n",
      "Average training loss: 0.054087721824645996\n",
      "Average test loss: 0.825310283260213\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05358367116914855\n",
      "Average test loss: 0.0032778033278882505\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05371049934956763\n",
      "Average test loss: 0.0033547473207323087\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05306239456600613\n",
      "Average test loss: 0.005019034308484859\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05233167711562581\n",
      "Average test loss: 0.010628727855781714\n",
      "Epoch 108/300\n",
      "Average training loss: 0.052341213610437184\n",
      "Average test loss: 0.004769587470839421\n",
      "Epoch 109/300\n",
      "Average training loss: 0.051859453807274504\n",
      "Average test loss: 0.00396196649927232\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05156326224737697\n",
      "Average test loss: 0.0033058345429599284\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05128289822737376\n",
      "Average test loss: 0.0051378995436761115\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05110652556187577\n",
      "Average test loss: 0.003380588461748428\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05089758445488082\n",
      "Average test loss: 0.0033959443295995396\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05081536440385712\n",
      "Average test loss: 0.003290554045803017\n",
      "Epoch 115/300\n",
      "Average training loss: 0.050500666681263184\n",
      "Average test loss: 0.0038825313217110105\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05019745560487111\n",
      "Average test loss: 0.003386813593821393\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05647309212552177\n",
      "Average test loss: 0.003534195265008344\n",
      "Epoch 118/300\n",
      "Average training loss: 0.39768659337361656\n",
      "Average test loss: 0.008502224209201005\n",
      "Epoch 119/300\n",
      "Average training loss: 0.1519628064367506\n",
      "Average test loss: 0.004587764579388831\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11280098791917165\n",
      "Average test loss: 0.003502065715276533\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08244990391201443\n",
      "Average test loss: 0.003408441566551725\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0766801668074396\n",
      "Average test loss: 0.0033491472680535583\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07291545703676011\n",
      "Average test loss: 0.00332862720431553\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06961234807637003\n",
      "Average test loss: 0.0033643419413516918\n",
      "Epoch 126/300\n",
      "Average training loss: 0.067203971558147\n",
      "Average test loss: 0.0033273572909335294\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06520857437451681\n",
      "Average test loss: 0.003307205697728528\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0631579597989718\n",
      "Average test loss: 0.00394998233165178\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06151624923944473\n",
      "Average test loss: 0.0034356802428762117\n",
      "Epoch 130/300\n",
      "Average training loss: 0.059810387829939526\n",
      "Average test loss: 0.0032758459196322496\n",
      "Epoch 131/300\n",
      "Average training loss: 0.058484519127342434\n",
      "Average test loss: 0.003392812395157913\n",
      "Epoch 132/300\n",
      "Average training loss: 0.057461377551158266\n",
      "Average test loss: 0.0033069025410546195\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05628056130806605\n",
      "Average test loss: 0.0033120803125202655\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0553552488817109\n",
      "Average test loss: 0.0032899758944080937\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0547578316297796\n",
      "Average test loss: 0.0032689004076851737\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05407400955094231\n",
      "Average test loss: 0.0036629821583628653\n",
      "Epoch 137/300\n",
      "Average training loss: 0.053286812365055086\n",
      "Average test loss: 0.0036581235805319416\n",
      "Epoch 138/300\n",
      "Average training loss: 0.051784835706154504\n",
      "Average test loss: 0.003326189470787843\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05133825414379438\n",
      "Average test loss: 0.003589612689904041\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05106006059381697\n",
      "Average test loss: 0.0033717851998905342\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05059103036257956\n",
      "Average test loss: 0.0033987067656384576\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05042582808269395\n",
      "Average test loss: 0.0037558060967259935\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05032618273297946\n",
      "Average test loss: 0.003322264675464895\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05003223568863339\n",
      "Average test loss: 0.003326938709244132\n",
      "Epoch 147/300\n",
      "Average training loss: 0.049910189890199234\n",
      "Average test loss: 0.0034406731368766892\n",
      "Epoch 148/300\n",
      "Average training loss: 0.049634043501483066\n",
      "Average test loss: 0.0034202172389874858\n",
      "Epoch 149/300\n",
      "Average training loss: 0.049369369784990944\n",
      "Average test loss: 0.010590897521831923\n",
      "Epoch 150/300\n",
      "Average training loss: 0.049328443388144176\n",
      "Average test loss: 0.003316902174924811\n",
      "Epoch 151/300\n",
      "Average training loss: 0.049020384483867224\n",
      "Average test loss: 0.003359057242878609\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04884372352229224\n",
      "Average test loss: 0.003299491588026285\n",
      "Epoch 153/300\n",
      "Average training loss: 0.048701811290449565\n",
      "Average test loss: 256.6872784849985\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04961790473262469\n",
      "Average test loss: 0.003524105873786741\n",
      "Epoch 155/300\n",
      "Average training loss: 0.048249827527337606\n",
      "Average test loss: 0.003445466402090258\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04890189849999216\n",
      "Average test loss: 0.003599379032022423\n",
      "Epoch 157/300\n",
      "Average training loss: 0.19637461970912085\n",
      "Average test loss: 0.01430229413219624\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08804372863637076\n",
      "Average test loss: 0.00396981436221136\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07521102803283268\n",
      "Average test loss: 0.003486979235584537\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06924215502209134\n",
      "Average test loss: 0.0034156031823820536\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05982963819636239\n",
      "Average test loss: 0.003528526896610856\n",
      "Epoch 164/300\n",
      "Average training loss: 0.057661050925652185\n",
      "Average test loss: 0.003371162899252441\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05586906476815542\n",
      "Average test loss: 0.0033110187579360275\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05431522373027272\n",
      "Average test loss: 0.003349083984063731\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05278437308470408\n",
      "Average test loss: 0.003411312107410696\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05156571282611953\n",
      "Average test loss: 0.0037306775550047556\n",
      "Epoch 169/300\n",
      "Average training loss: 0.050663836459318795\n",
      "Average test loss: 0.0033793879753599566\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04996898406412866\n",
      "Average test loss: 0.0033402215144079594\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04943971099125014\n",
      "Average test loss: 0.003415313566310538\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04907520571351051\n",
      "Average test loss: 0.003361848846077919\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04860307761033376\n",
      "Average test loss: 0.003545066779272424\n",
      "Epoch 174/300\n",
      "Average training loss: 0.048495920995871224\n",
      "Average test loss: 0.0034875650271359417\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04818385779195362\n",
      "Average test loss: 0.0035430733499427635\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04788215831915538\n",
      "Average test loss: 0.0034330256556471188\n",
      "Epoch 177/300\n",
      "Average training loss: 0.047696803086333804\n",
      "Average test loss: 0.0034530237464027273\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04760266690121757\n",
      "Average test loss: 0.0034887868822034863\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04743605715036392\n",
      "Average test loss: 0.0034206737135019567\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04739501322309176\n",
      "Average test loss: 0.003523340411691202\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04718995364175903\n",
      "Average test loss: 0.003360312145203352\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04722431961695353\n",
      "Average test loss: 0.0037617955940465134\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04737112474441528\n",
      "Average test loss: 0.008030771631954444\n",
      "Epoch 185/300\n",
      "Average training loss: 0.049538222928841906\n",
      "Average test loss: 0.0033911496493965387\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04984963204132186\n",
      "Average test loss: 0.00343105245857603\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04693861797120836\n",
      "Average test loss: 0.007958564492149486\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04650216569503148\n",
      "Average test loss: 0.003633514840569761\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04653337058093813\n",
      "Average test loss: 0.0033737715675185125\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04628438239958551\n",
      "Average test loss: 0.003429358058505588\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04626930547091696\n",
      "Average test loss: 0.0034593407130903667\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04624562802248531\n",
      "Average test loss: 0.0043754964731633665\n",
      "Epoch 193/300\n",
      "Average training loss: 0.046075154536300233\n",
      "Average test loss: 0.003414780281070206\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04595666015148163\n",
      "Average test loss: 0.004295102128552066\n",
      "Epoch 195/300\n",
      "Average training loss: 0.046060015718142194\n",
      "Average test loss: 0.003908780363698801\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04589528861310747\n",
      "Average test loss: 0.0034511592930389777\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0457420648932457\n",
      "Average test loss: 0.003540973474168115\n",
      "Epoch 198/300\n",
      "Average training loss: 0.045682312892542945\n",
      "Average test loss: 0.0034733486502534815\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04558347543080648\n",
      "Average test loss: 0.00487515829068919\n",
      "Epoch 200/300\n",
      "Average training loss: 0.045439318862226276\n",
      "Average test loss: 0.003554403793480661\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04560596350166533\n",
      "Average test loss: 101.93000595690475\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04565007059110535\n",
      "Average test loss: 0.0035745952708853617\n",
      "Epoch 203/300\n",
      "Average training loss: 0.046519166678190234\n",
      "Average test loss: 0.0046083426401019095\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04613131023115582\n",
      "Average test loss: 0.00351350761577487\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04521504843566153\n",
      "Average test loss: 0.0036008020074417193\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0450272704925802\n",
      "Average test loss: 0.00359169078597592\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04500619356499778\n",
      "Average test loss: 0.0035652112929771343\n",
      "Epoch 208/300\n",
      "Average training loss: 0.044890307661559846\n",
      "Average test loss: 0.0036391984282268417\n",
      "Epoch 209/300\n",
      "Average training loss: 0.044985158989826836\n",
      "Average test loss: 0.004550489274577962\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04472653352883127\n",
      "Average test loss: 0.003504268057644367\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04463783174422052\n",
      "Average test loss: 0.003521730552530951\n",
      "Epoch 212/300\n",
      "Average training loss: 0.044685639652940964\n",
      "Average test loss: 0.0035680903473662004\n",
      "Epoch 213/300\n",
      "Average training loss: 0.044841890374819435\n",
      "Average test loss: 0.16802067634794446\n",
      "Epoch 214/300\n",
      "Average training loss: 0.044670776610573136\n",
      "Average test loss: 0.004533279134374526\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04442369856437047\n",
      "Average test loss: 0.04549084866295258\n",
      "Epoch 216/300\n",
      "Average training loss: 0.044441969523827236\n",
      "Average test loss: 0.0037981259727643594\n",
      "Epoch 217/300\n",
      "Average training loss: 0.044474539286560484\n",
      "Average test loss: 0.0034361669381873473\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04420440262887213\n",
      "Average test loss: 0.0035670205135312346\n",
      "Epoch 219/300\n",
      "Average training loss: 0.044217237773868774\n",
      "Average test loss: 0.0037409260438548194\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04414894909328885\n",
      "Average test loss: 0.0035515974426849023\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04415353221363492\n",
      "Average test loss: 0.0036546094429989657\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04375474610262447\n",
      "Average test loss: 0.0034692926226804655\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04396526253885693\n",
      "Average test loss: 0.0035824673703561227\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04385371352566613\n",
      "Average test loss: 0.0035616725803249414\n",
      "Epoch 225/300\n",
      "Average training loss: 0.043839618881543474\n",
      "Average test loss: 0.0035123333752983144\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04382962572905753\n",
      "Average test loss: 0.004692002233117819\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04387615276376406\n",
      "Average test loss: 0.003540549685143762\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04360909296737777\n",
      "Average test loss: 0.003515467465751701\n",
      "Epoch 229/300\n",
      "Average training loss: 0.043742568400171065\n",
      "Average test loss: 0.00357314034530686\n",
      "Epoch 230/300\n",
      "Average training loss: 0.043518775284290316\n",
      "Average test loss: 0.0034634008403453564\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04347401179207696\n",
      "Average test loss: 0.003615741480555799\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04354697694049941\n",
      "Average test loss: 0.006321586245463954\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04340135238236851\n",
      "Average test loss: 0.003550380679882235\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04614002590046989\n",
      "Average test loss: 0.0036543449540105133\n",
      "Epoch 235/300\n",
      "Average training loss: 0.044662340299950705\n",
      "Average test loss: 0.0035560214769923023\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04315508133835263\n",
      "Average test loss: 0.00538022309790055\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04312563998169369\n",
      "Average test loss: 0.003589398908118407\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04307325655221939\n",
      "Average test loss: 0.003680623921669192\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04302033322387271\n",
      "Average test loss: 0.003514517252229982\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04299954697820876\n",
      "Average test loss: 0.0036658459868696\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04353795336352454\n",
      "Average test loss: 0.0035598027569552263\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04324581134981579\n",
      "Average test loss: 0.004862204326937596\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04309704405069351\n",
      "Average test loss: 0.004204569227579567\n",
      "Epoch 244/300\n",
      "Average training loss: 0.17137685996956295\n",
      "Average test loss: 0.003441771290782425\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0720928164654308\n",
      "Average test loss: 0.003402098226464457\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0656808174153169\n",
      "Average test loss: 0.0034594429013215834\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06162043282720778\n",
      "Average test loss: 0.003330544855652584\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05885874229338434\n",
      "Average test loss: 0.003408466021840771\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05634930292765299\n",
      "Average test loss: 0.0033744549732655288\n",
      "Epoch 250/300\n",
      "Average training loss: 0.054920961985985436\n",
      "Average test loss: 0.0034104817112286887\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05290943771600723\n",
      "Average test loss: 0.003392760498035285\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0512921237581306\n",
      "Average test loss: 0.0036579047404229643\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05012157600455814\n",
      "Average test loss: 0.003497744734502501\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04896581706735823\n",
      "Average test loss: 0.003444343723356724\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04793890142440796\n",
      "Average test loss: 0.004201627079811361\n",
      "Epoch 256/300\n",
      "Average training loss: 0.046983611282375125\n",
      "Average test loss: 0.0035250106896791192\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0462194789217578\n",
      "Average test loss: 0.0037733422430853048\n",
      "Epoch 258/300\n",
      "Average training loss: 0.045639147861136334\n",
      "Average test loss: 0.0034661618628435663\n",
      "Epoch 259/300\n",
      "Average training loss: 0.045124780568811626\n",
      "Average test loss: 0.00357327000838187\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04452091998855273\n",
      "Average test loss: 0.0035318885867794353\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04433465546700689\n",
      "Average test loss: 0.0037192260287702084\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04387331090370814\n",
      "Average test loss: 0.003578851190292173\n",
      "Epoch 263/300\n",
      "Average training loss: 0.043735148519277574\n",
      "Average test loss: 0.0037000351208779547\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04358988685740365\n",
      "Average test loss: 0.0038792441530774036\n",
      "Epoch 265/300\n",
      "Average training loss: 0.043505669901768365\n",
      "Average test loss: 0.004747988592212399\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04358504628141721\n",
      "Average test loss: 0.0034897419176995754\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04335092506143782\n",
      "Average test loss: 0.0035771263303856055\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04312607868512471\n",
      "Average test loss: 0.0036962635020414986\n",
      "Epoch 269/300\n",
      "Average training loss: 0.042920576211478975\n",
      "Average test loss: 0.0036129893528090585\n",
      "Epoch 270/300\n",
      "Average training loss: 0.042802326139476564\n",
      "Average test loss: 0.003857784133611454\n",
      "Epoch 271/300\n",
      "Average training loss: 0.043820725212494534\n",
      "Average test loss: 0.00358067076984379\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04287336626648903\n",
      "Average test loss: 0.0040766653917315935\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04266458017627398\n",
      "Average test loss: 0.0035196973310990466\n",
      "Epoch 274/300\n",
      "Average training loss: 0.042515017476346756\n",
      "Average test loss: 0.003569199999794364\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04261990563405885\n",
      "Average test loss: 0.003569970557259189\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04257285154528088\n",
      "Average test loss: 0.0035045922187467416\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0424201550549931\n",
      "Average test loss: 0.0036489658111499416\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04243532279464934\n",
      "Average test loss: 0.0035839924895101124\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04244214701652527\n",
      "Average test loss: 0.003649387602176931\n",
      "Epoch 280/300\n",
      "Average training loss: 0.042328104413217965\n",
      "Average test loss: 0.0036909516341984273\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04222074916958809\n",
      "Average test loss: 0.003606235482626491\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04213089955184195\n",
      "Average test loss: 0.00356262816902664\n",
      "Epoch 283/300\n",
      "Average training loss: 0.042276681751012804\n",
      "Average test loss: 0.003606147706715597\n",
      "Epoch 284/300\n",
      "Average training loss: 0.042136686596605515\n",
      "Average test loss: 0.007147626818468173\n",
      "Epoch 285/300\n",
      "Average training loss: 0.042083943665027615\n",
      "Average test loss: 0.003641156502068043\n",
      "Epoch 286/300\n",
      "Average training loss: 0.041984747681352824\n",
      "Average test loss: 0.0045057760265966255\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0420668020149072\n",
      "Average test loss: 0.0038756025118960274\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04187725266151958\n",
      "Average test loss: 0.00368440988763339\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04197401147418552\n",
      "Average test loss: 0.0035879274706045786\n",
      "Epoch 290/300\n",
      "Average training loss: 0.042070917523569534\n",
      "Average test loss: 0.0036467516327069864\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0418917108575503\n",
      "Average test loss: 0.0036799919476939572\n",
      "Epoch 292/300\n",
      "Average training loss: 0.041792660074101554\n",
      "Average test loss: 0.0035940993022587566\n",
      "Epoch 293/300\n",
      "Average training loss: 0.041843445274564954\n",
      "Average test loss: 0.00359309901855886\n",
      "Epoch 294/300\n",
      "Average training loss: 0.041712589982483124\n",
      "Average test loss: 0.0035811705361637803\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04174486237102085\n",
      "Average test loss: 0.0036908651925623417\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04163020712468359\n",
      "Average test loss: 0.003634521121573117\n",
      "Epoch 297/300\n",
      "Average training loss: 0.041710759371519086\n",
      "Average test loss: 0.00378490101504657\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04163870330982738\n",
      "Average test loss: 0.003631379131020771\n",
      "Epoch 299/300\n",
      "Average training loss: 0.041592472357882396\n",
      "Average test loss: 0.0037507094856765533\n",
      "Epoch 300/300\n",
      "Average training loss: 0.041688335766394934\n",
      "Average test loss: 0.003690402207068271\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.3984898908403185\n",
      "Average test loss: 0.0594692231979635\n",
      "Epoch 2/300\n",
      "Average training loss: 2.108283289803399\n",
      "Average test loss: 0.010035174984898832\n",
      "Epoch 3/300\n",
      "Average training loss: 1.5414486332999335\n",
      "Average test loss: 0.019225112665030693\n",
      "Epoch 4/300\n",
      "Average training loss: 1.221510529200236\n",
      "Average test loss: 0.004902430311673218\n",
      "Epoch 5/300\n",
      "Average training loss: 1.0095866074032254\n",
      "Average test loss: 0.004637994765407509\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8554545343716939\n",
      "Average test loss: 0.0043810231892599\n",
      "Epoch 7/300\n",
      "Average training loss: 0.7407576068242391\n",
      "Average test loss: 0.004388545436577664\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6484114136166043\n",
      "Average test loss: 0.005835517572446002\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5729155548943413\n",
      "Average test loss: 0.004808865270680852\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5076850672033097\n",
      "Average test loss: 0.0036074922279351286\n",
      "Epoch 11/300\n",
      "Average training loss: 0.45176735838254295\n",
      "Average test loss: 0.003554801759827468\n",
      "Epoch 12/300\n",
      "Average training loss: 0.40077579442660016\n",
      "Average test loss: 0.004227731214422319\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3575195492638482\n",
      "Average test loss: 0.003398681926851471\n",
      "Epoch 14/300\n",
      "Average training loss: 0.31849144882626\n",
      "Average test loss: 0.003545686693241199\n",
      "Epoch 15/300\n",
      "Average training loss: 0.28490965225961473\n",
      "Average test loss: 0.003349306444947918\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2547170686456892\n",
      "Average test loss: 0.003315354441603025\n",
      "Epoch 17/300\n",
      "Average training loss: 0.22987297776010301\n",
      "Average test loss: 0.0037956334439416726\n",
      "Epoch 18/300\n",
      "Average training loss: 0.20735613527562882\n",
      "Average test loss: 0.00296075042264743\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18778190559811062\n",
      "Average test loss: 0.0029929801755481295\n",
      "Epoch 20/300\n",
      "Average training loss: 0.17175805984603035\n",
      "Average test loss: 0.0027390937691347465\n",
      "Epoch 21/300\n",
      "Average training loss: 0.15793274505933125\n",
      "Average test loss: 0.0031972725635601414\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1451715396642685\n",
      "Average test loss: 0.007573131216896905\n",
      "Epoch 23/300\n",
      "Average training loss: 0.13345278443230524\n",
      "Average test loss: 0.0028473898195144204\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1236955470641454\n",
      "Average test loss: 0.002599341945722699\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11416622585720486\n",
      "Average test loss: 0.011101616648336251\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10658701839049657\n",
      "Average test loss: 0.002594614072702825\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09862968413035075\n",
      "Average test loss: 0.00338025112512211\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09288479299015469\n",
      "Average test loss: 0.0025775148663669824\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08772247397237354\n",
      "Average test loss: 0.002451460440746612\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08232423581017388\n",
      "Average test loss: 0.004527577298796839\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0784219654334916\n",
      "Average test loss: 0.003017111082250873\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07450584606991874\n",
      "Average test loss: 0.0272161971140239\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07087238972054588\n",
      "Average test loss: 0.002616226656983296\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06899474036362437\n",
      "Average test loss: 0.0071801408533420825\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06627649236718813\n",
      "Average test loss: 0.0028676893199897475\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06376067099968592\n",
      "Average test loss: 1.1460972395481335\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06081239988406499\n",
      "Average test loss: 0.003191014915290806\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06023536295360989\n",
      "Average test loss: 0.002328934750209252\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05763530389136738\n",
      "Average test loss: 0.0024458711362547346\n",
      "Epoch 40/300\n",
      "Average training loss: 0.056613226423660915\n",
      "Average test loss: 0.002656950115226209\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05509164694613881\n",
      "Average test loss: 0.008851898253791863\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07862837592429585\n",
      "Average test loss: 0.0026250997361623577\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05956497427490023\n",
      "Average test loss: 0.0030313728728021184\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05594636642270618\n",
      "Average test loss: 0.003272946166909403\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05305262055330806\n",
      "Average test loss: 0.0031521810995828775\n",
      "Epoch 46/300\n",
      "Average training loss: 0.051576110972298514\n",
      "Average test loss: 0.0022788635596839918\n",
      "Epoch 47/300\n",
      "Average training loss: 0.050580551657411786\n",
      "Average test loss: 0.002271725961007178\n",
      "Epoch 48/300\n",
      "Average training loss: 0.049428490744696726\n",
      "Average test loss: 0.002503907554058565\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04850871804356575\n",
      "Average test loss: 0.002612367175105545\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04810932380292151\n",
      "Average test loss: 0.0024561698316699927\n",
      "Epoch 51/300\n",
      "Average training loss: 0.047286295980215075\n",
      "Average test loss: 0.0022769510209974314\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0466947647134463\n",
      "Average test loss: 0.005255795714755853\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04625254942642318\n",
      "Average test loss: 0.002389698032910625\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04548975321319368\n",
      "Average test loss: 0.002262193128777047\n",
      "Epoch 55/300\n",
      "Average training loss: 0.046023798492219714\n",
      "Average test loss: 0.002357612786297169\n",
      "Epoch 56/300\n",
      "Average training loss: 0.12075528244177501\n",
      "Average test loss: 0.0024426772728976275\n",
      "Epoch 57/300\n",
      "Average training loss: 0.062204982042312625\n",
      "Average test loss: 0.003917331620843874\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05591467892130216\n",
      "Average test loss: 0.0022952922667480178\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05280512525803513\n",
      "Average test loss: 0.002590379945312937\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05074059705601798\n",
      "Average test loss: 0.002276252955612209\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0494160355064604\n",
      "Average test loss: 0.0030184212763690287\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04817481290300687\n",
      "Average test loss: 0.002326870519357423\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04730416726403766\n",
      "Average test loss: 0.0022726850780761905\n",
      "Epoch 64/300\n",
      "Average training loss: 0.046650600706537565\n",
      "Average test loss: 0.0031596440161681836\n",
      "Epoch 65/300\n",
      "Average training loss: 0.046148438609308666\n",
      "Average test loss: 0.40674859458208085\n",
      "Epoch 66/300\n",
      "Average training loss: 0.045536002818081116\n",
      "Average test loss: 0.002420572546414203\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04520190996925036\n",
      "Average test loss: 0.0029784481355713474\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04558486522071891\n",
      "Average test loss: 0.002507978790957067\n",
      "Epoch 69/300\n",
      "Average training loss: 0.044473560521999994\n",
      "Average test loss: 0.009110731397858925\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04416588881611824\n",
      "Average test loss: 148432.72519618055\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04386638895008299\n",
      "Average test loss: 0.010520632549913393\n",
      "Epoch 72/300\n",
      "Average training loss: 0.043551522933774525\n",
      "Average test loss: 0.002280557152711683\n",
      "Epoch 73/300\n",
      "Average training loss: 0.043302061837580466\n",
      "Average test loss: 0.002888260612471236\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04296525938312212\n",
      "Average test loss: 0.0022311029471457004\n",
      "Epoch 75/300\n",
      "Average training loss: 0.042104376524686814\n",
      "Average test loss: 0.0027669278207338517\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04196757845746146\n",
      "Average test loss: 0.0022989766982694465\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04143883694542779\n",
      "Average test loss: 0.0029118739486568503\n",
      "Epoch 80/300\n",
      "Average training loss: 0.041291956775718265\n",
      "Average test loss: 0.0022621645311721498\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0409012120531665\n",
      "Average test loss: 0.002232714475856887\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04083675605389807\n",
      "Average test loss: 0.0025310238341076504\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04044892161422305\n",
      "Average test loss: 0.002287545846982135\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04021769341495302\n",
      "Average test loss: 0.015332032274454833\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03990079151756234\n",
      "Average test loss: 0.003910197989808188\n",
      "Epoch 86/300\n",
      "Average test loss: 0.021031401944657167\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03958751704129908\n",
      "Average test loss: 0.002615637195813987\n",
      "Epoch 88/300\n",
      "Average training loss: 0.041025098754300014\n",
      "Average test loss: 0.0025711935692363315\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09177231620417702\n",
      "Average test loss: 0.004780630998934309\n",
      "Epoch 90/300\n",
      "Average training loss: 0.052836174060901006\n",
      "Average test loss: 0.002372060359145204\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04820533243815104\n",
      "Average test loss: 0.0023808225142872997\n",
      "Epoch 92/300\n",
      "Average training loss: 0.045762184692753685\n",
      "Average test loss: 0.0022991569687922796\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04419229622350799\n",
      "Average test loss: 0.003911566857041584\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04303389960858557\n",
      "Average test loss: 0.0027182244832317033\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04209474520881971\n",
      "Average test loss: 0.002234638438663549\n",
      "Epoch 96/300\n",
      "Average training loss: 0.041332591712474824\n",
      "Average test loss: 0.002218240132762326\n",
      "Epoch 97/300\n",
      "Average training loss: 0.040768755164411336\n",
      "Average test loss: 0.002303162303235796\n",
      "Epoch 98/300\n",
      "Average training loss: 0.040384632151987815\n",
      "Average test loss: 0.0030541341704212956\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03992466926905844\n",
      "Average test loss: 0.0027362383197372156\n",
      "Epoch 100/300\n",
      "Average training loss: 0.039735457274648875\n",
      "Average test loss: 0.002265884295312895\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03935173168778419\n",
      "Average test loss: 0.0032190240466346345\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03926255567206277\n",
      "Average test loss: 0.002310437301380767\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03900757043229209\n",
      "Average test loss: 0.0029035750512654583\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03881964711348216\n",
      "Average test loss: 0.0045086221827401055\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03872922686239084\n",
      "Average test loss: 0.0024182594062553513\n",
      "Epoch 106/300\n",
      "Average training loss: 0.038526133658157456\n",
      "Average test loss: 0.024969363284607728\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03842429463399781\n",
      "Average test loss: 0.002443917519102494\n",
      "Epoch 108/300\n",
      "Average training loss: 0.038151509195566174\n",
      "Average test loss: 0.002290537405966057\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03802629984418551\n",
      "Average test loss: 0.0026850130622171693\n",
      "Epoch 110/300\n",
      "Average training loss: 0.037907142066293295\n",
      "Average test loss: 0.0023671888738042777\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03770111456844542\n",
      "Average test loss: 14240.1523171658\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03755946944819556\n",
      "Average test loss: 0.002386751189207037\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03741773648725616\n",
      "Average test loss: 0.0022806683994001813\n",
      "Epoch 114/300\n",
      "Average training loss: 0.037333692921532526\n",
      "Average test loss: 0.0029289316679868434\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03715930417842335\n",
      "Average test loss: 0.0023937147537039386\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03706600627965397\n",
      "Average test loss: 0.0026143538556579085\n",
      "Epoch 117/300\n",
      "Average training loss: 0.037002176567912104\n",
      "Average test loss: 0.0030110537180056173\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03838489955001407\n",
      "Average test loss: 0.002532811240810487\n",
      "Epoch 119/300\n",
      "Average training loss: 0.041566280878252454\n",
      "Average test loss: 0.0025519952902363406\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03984446518619855\n",
      "Average test loss: 0.0022760250191721653\n",
      "Epoch 121/300\n",
      "Average training loss: 0.037753209309445486\n",
      "Average test loss: 0.00240293134500583\n",
      "Epoch 122/300\n",
      "Average training loss: 0.037113596118158766\n",
      "Average test loss: 0.0022782145322610935\n",
      "Epoch 123/300\n",
      "Average training loss: 0.036626508437924914\n",
      "Average test loss: 0.002740110448665089\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03637188183930185\n",
      "Average test loss: 0.002768347593645255\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03623337672154109\n",
      "Average test loss: 0.003414388656616211\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03622014389104313\n",
      "Average test loss: 0.0022910763366768756\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03607372384601169\n",
      "Average test loss: 0.002481916488872634\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03608151431216134\n",
      "Average test loss: 0.0025350223564439348\n",
      "Epoch 129/300\n",
      "Average training loss: 0.035948128928740816\n",
      "Average test loss: 0.004844286194071174\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03574275632699331\n",
      "Average test loss: 229.91459444628822\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03602467618054814\n",
      "Average test loss: 0.0023268713160521453\n",
      "Epoch 132/300\n",
      "Average training loss: 0.035568427824311785\n",
      "Average test loss: 0.0030765268868870204\n",
      "Epoch 133/300\n",
      "Average training loss: 0.035480618407328926\n",
      "Average test loss: 0.002319896539259288\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03531939778228601\n",
      "Average test loss: 0.005239970119049152\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03535415009988679\n",
      "Average test loss: 0.002342141969439884\n",
      "Epoch 136/300\n",
      "Average training loss: 0.035179509263899594\n",
      "Average test loss: 0.004084448003106647\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03564939208163156\n",
      "Average test loss: 0.016830929024351966\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03493384169869953\n",
      "Average test loss: 0.002627245664803518\n",
      "Epoch 139/300\n",
      "Average training loss: 0.034902053107817965\n",
      "Average test loss: 0.002447486554996835\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03474332939253913\n",
      "Average test loss: 0.005218981160678798\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03476779319180383\n",
      "Average test loss: 0.010086976505815982\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03470998065504763\n",
      "Average test loss: 0.0028608509329044155\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03462134380804168\n",
      "Average test loss: 0.002592590557411313\n",
      "Epoch 144/300\n",
      "Average training loss: 0.034483488879270024\n",
      "Average test loss: 0.003095601997234755\n",
      "Epoch 145/300\n",
      "Average training loss: 0.034423392727971076\n",
      "Average test loss: 0.0023717692331928345\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03440730507506264\n",
      "Average test loss: 0.007881243389513758\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03432881353298823\n",
      "Average test loss: 0.002721417336414258\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03425221534570058\n",
      "Average test loss: 0.0023568696874297326\n",
      "Epoch 149/300\n",
      "Average training loss: 0.034149043437507416\n",
      "Average test loss: 0.0025742806412486566\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03405080096092489\n",
      "Average test loss: 0.0024095127342475785\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03405072398483753\n",
      "Average test loss: 0.003169583320410715\n",
      "Epoch 152/300\n",
      "Average training loss: 0.034006966441869735\n",
      "Average test loss: 0.003924910461116168\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03393499588800801\n",
      "Average test loss: 0.0031479507755074235\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03383595823413796\n",
      "Average test loss: 0.00246743821973602\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03383147133555677\n",
      "Average test loss: 0.0023872155911392637\n",
      "Epoch 156/300\n",
      "Average training loss: 0.033694781319962605\n",
      "Average test loss: 0.0023635879160008497\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03364533356494374\n",
      "Average test loss: 0.0035597322487996683\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03363668487138218\n",
      "Average test loss: 0.002429044762212369\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03359139279855622\n",
      "Average test loss: 0.0023919918201863767\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0336500614715947\n",
      "Average test loss: 0.0029680778661535846\n",
      "Epoch 161/300\n",
      "Average training loss: 0.033454737573862074\n",
      "Average test loss: 0.14989585185030269\n",
      "Epoch 162/300\n",
      "Average training loss: 0.033567735918694074\n",
      "Average test loss: 0.00242446339275274\n",
      "Epoch 163/300\n",
      "Average training loss: 0.033410709180765684\n",
      "Average test loss: 0.002455614719953802\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03338568815920088\n",
      "Average test loss: 0.004407476712846093\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03329069986111588\n",
      "Average test loss: 0.002445851095020771\n",
      "Epoch 166/300\n",
      "Average training loss: 0.033150078723828\n",
      "Average test loss: 0.0024488517681343688\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03319311791658402\n",
      "Average test loss: 0.0027082741163257096\n",
      "Epoch 168/300\n",
      "Average training loss: 0.033076249884234535\n",
      "Average test loss: 0.007651810403499338\n",
      "Epoch 169/300\n",
      "Average training loss: 0.033134924276007546\n",
      "Average test loss: 0.12557408789131377\n",
      "Epoch 170/300\n",
      "Average training loss: 0.033100177999999786\n",
      "Average test loss: 0.0027259067359069983\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03292744549115499\n",
      "Average test loss: 0.002909531014660994\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03293209825787279\n",
      "Average test loss: 0.008968653965327474\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03301656712094943\n",
      "Average test loss: 0.007129179414361715\n",
      "Epoch 174/300\n",
      "Average training loss: 0.032797066122293474\n",
      "Average test loss: 0.0026330717754446797\n",
      "Epoch 175/300\n",
      "Average training loss: 0.032958516561322745\n",
      "Average test loss: 0.0025014062551781536\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03272656597362624\n",
      "Average test loss: 0.002470604520911972\n",
      "Epoch 177/300\n",
      "Average training loss: 0.032671869552797744\n",
      "Average test loss: 0.0024397326512262227\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03270066177513864\n",
      "Average test loss: 0.0026443403048647773\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03266315188010534\n",
      "Average test loss: 0.002413034715379278\n",
      "Epoch 180/300\n",
      "Average training loss: 0.032594940624303285\n",
      "Average test loss: 0.005143750054347846\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0326065961321195\n",
      "Average test loss: 0.0028284133976946273\n",
      "Epoch 182/300\n",
      "Average training loss: 0.032599986099534566\n",
      "Average test loss: 0.002512650979268882\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03253919720980856\n",
      "Average test loss: 0.005014083583321836\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03254138364394506\n",
      "Average test loss: 0.0025625971216294502\n",
      "Epoch 185/300\n",
      "Average training loss: 0.032498232149415544\n",
      "Average test loss: 0.0024963018198808033\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03236768531302611\n",
      "Average test loss: 0.0026913680935071573\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03234353693326314\n",
      "Average test loss: 0.002537148311527239\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03234171445502175\n",
      "Average test loss: 0.002682244318847855\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03244577129350768\n",
      "Average test loss: 0.0025507494581656323\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03229342202014393\n",
      "Average test loss: 0.0026113404497090315\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03234173093570603\n",
      "Average test loss: 0.003852597908841239\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03221371923552619\n",
      "Average test loss: 0.003533881644614869\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03217039687765969\n",
      "Average test loss: 0.002486583776358101\n",
      "Epoch 194/300\n",
      "Average training loss: 0.032139981880784034\n",
      "Average test loss: 0.002679680564751228\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03222675666213035\n",
      "Average test loss: 0.002456864533531997\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03205711848537127\n",
      "Average test loss: 0.002507055090119441\n",
      "Epoch 197/300\n",
      "Average training loss: 0.032066210829549364\n",
      "Average test loss: 0.0025412556510418655\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03203849149081442\n",
      "Average test loss: 0.018060169323037067\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03203685612148709\n",
      "Average test loss: 0.002550383240605394\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03196600709358851\n",
      "Average test loss: 0.0027278120186593796\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03202262911862797\n",
      "Average test loss: 0.0029321005031880407\n",
      "Epoch 202/300\n",
      "Average training loss: 0.031914636520875825\n",
      "Average test loss: 0.006132282271153397\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03200076195928785\n",
      "Average test loss: 0.003707224605501526\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03192775206764539\n",
      "Average test loss: 0.002653396185280548\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03181925739596288\n",
      "Average test loss: 0.0028128699598213036\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03179302787780762\n",
      "Average test loss: 0.002729422038110594\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03182217914031612\n",
      "Average test loss: 0.0026350634383658566\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0317367815805806\n",
      "Average test loss: 0.0024438495840877292\n",
      "Epoch 209/300\n",
      "Average training loss: 0.031741923651761476\n",
      "Average test loss: 0.002439438056407703\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03173009745279948\n",
      "Average test loss: 0.0026526074566774896\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03166561591790782\n",
      "Average test loss: 0.002587908083572984\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03169248148798943\n",
      "Average test loss: 0.002531056806031201\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03168033832642767\n",
      "Average test loss: 0.0026198408934805127\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03158539498183462\n",
      "Average test loss: 0.002558504876163271\n",
      "Epoch 215/300\n",
      "Average training loss: 0.031550408457716304\n",
      "Average test loss: 0.003163626887525121\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03158847549226549\n",
      "Average test loss: 0.0025110096778306694\n",
      "Epoch 217/300\n",
      "Average training loss: 0.031564906504419114\n",
      "Average test loss: 0.002459475495128168\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031464874320560034\n",
      "Average test loss: 0.0026057928810930913\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03150081882543034\n",
      "Average test loss: 0.0027017753852738276\n",
      "Epoch 220/300\n",
      "Average training loss: 0.031404738085137475\n",
      "Average test loss: 0.0027006395496428012\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03152372843689389\n",
      "Average test loss: 0.004217197380959987\n",
      "Epoch 222/300\n",
      "Average training loss: 0.031475871986813016\n",
      "Average test loss: 0.004813248514301247\n",
      "Epoch 223/300\n",
      "Average training loss: 0.031437367730670504\n",
      "Average test loss: 0.0025256002930303416\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03132663169503212\n",
      "Average test loss: 0.002527023726142943\n",
      "Epoch 225/300\n",
      "Average training loss: 0.031322498336434364\n",
      "Average test loss: 0.0026163165283699832\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03138254803087976\n",
      "Average test loss: 0.003045278917170233\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03132760154538684\n",
      "Average test loss: 0.0025013603034118813\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03141201949119568\n",
      "Average test loss: 0.002593388226400647\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03121932302912076\n",
      "Average test loss: 0.005830778794984023\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03127082056138251\n",
      "Average test loss: 0.0026209597186081937\n",
      "Epoch 231/300\n",
      "Average training loss: 0.031260347372955744\n",
      "Average test loss: 0.0027083985762049755\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03118182541926702\n",
      "Average test loss: 0.005885003967417611\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03121555197570059\n",
      "Average test loss: 0.0025121000711288716\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03119396652281284\n",
      "Average test loss: 0.0025497615846494836\n",
      "Epoch 235/300\n",
      "Average training loss: 0.031147079451216593\n",
      "Average test loss: 0.0028046598980824154\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03110029961168766\n",
      "Average test loss: 0.0028019383682145013\n",
      "Epoch 237/300\n",
      "Average training loss: 0.031039616958962547\n",
      "Average test loss: 0.0025236273030233053\n",
      "Epoch 238/300\n",
      "Average training loss: 0.031144982448882527\n",
      "Average test loss: 0.0027199685561160244\n",
      "Epoch 239/300\n",
      "Average training loss: 0.031056316978401607\n",
      "Average test loss: 0.0027141814861032696\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03106768206589752\n",
      "Average test loss: 0.00267981383163068\n",
      "Epoch 241/300\n",
      "Average training loss: 0.031015911022822064\n",
      "Average test loss: 0.0033933174359715647\n",
      "Epoch 242/300\n",
      "Average training loss: 0.031149262203110588\n",
      "Average test loss: 0.0025249518706566758\n",
      "Epoch 243/300\n",
      "Average training loss: 0.030947674496306313\n",
      "Average test loss: 0.002856819249068697\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030995627062188256\n",
      "Average test loss: 0.0027117952316378555\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030947137991587322\n",
      "Average test loss: 0.0025111835576179953\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0308623668120967\n",
      "Average test loss: 0.002717551647581988\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030985796646939383\n",
      "Average test loss: 0.0025302751513404977\n",
      "Epoch 248/300\n",
      "Average training loss: 0.030853161392940417\n",
      "Average test loss: 0.002729756706704696\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0308232957455847\n",
      "Average test loss: 0.0025854934921695125\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030876875127355256\n",
      "Average test loss: 0.002622080301658975\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03093459490769439\n",
      "Average test loss: 0.0028813463495009476\n",
      "Epoch 252/300\n",
      "Average training loss: 0.030830405440595416\n",
      "Average test loss: 0.002552025631070137\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03083433147933748\n",
      "Average test loss: 0.0026754236173712545\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03078023186326027\n",
      "Average test loss: 0.0025311635962376993\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03069648670156797\n",
      "Average test loss: 0.0032046251837164162\n",
      "Epoch 256/300\n",
      "Average training loss: 0.030702138718631533\n",
      "Average test loss: 172.90195024438037\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030829572753773796\n",
      "Average test loss: 0.003453442350857788\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03069964206715425\n",
      "Average test loss: 0.00544574156941639\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03067686151133643\n",
      "Average test loss: 0.002835255497445663\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03067680937051773\n",
      "Average test loss: 0.0028095780656569535\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03069932379987505\n",
      "Average test loss: 0.004681930690589879\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0306716893878248\n",
      "Average test loss: 0.002881746874190867\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030740035214357905\n",
      "Average test loss: 0.0029062474220991133\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03060282254219055\n",
      "Average test loss: 0.002616765366660224\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030640757522649234\n",
      "Average test loss: 0.0025582076249023276\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030640371647146013\n",
      "Average test loss: 0.0026190914421652754\n",
      "Epoch 267/300\n",
      "Average training loss: 0.034099922253025904\n",
      "Average test loss: 0.0027952941006256474\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030966546184486812\n",
      "Average test loss: 0.002506936313274006\n",
      "Epoch 269/300\n",
      "Average training loss: 0.030466649428009987\n",
      "Average test loss: 0.003554680140585535\n",
      "Epoch 270/300\n",
      "Average training loss: 0.030530252769589424\n",
      "Average test loss: 0.002617426637560129\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030454809013340207\n",
      "Average test loss: 0.0029644255263523924\n",
      "Epoch 272/300\n",
      "Average training loss: 0.030458325813213984\n",
      "Average test loss: 0.0025248233148207266\n",
      "Epoch 273/300\n",
      "Average training loss: 0.030544390194945866\n",
      "Average test loss: 0.0026820793046305577\n",
      "Epoch 274/300\n",
      "Average training loss: 0.030421657866901822\n",
      "Average test loss: 0.0027248733672830794\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030570696334044137\n",
      "Average test loss: 0.002528985802705089\n",
      "Epoch 276/300\n",
      "Average training loss: 0.030450286358594893\n",
      "Average test loss: 0.0025914771645847295\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030418660569522117\n",
      "Average test loss: 0.0027059731733881766\n",
      "Epoch 278/300\n",
      "Average training loss: 0.030402846947312354\n",
      "Average test loss: 0.0026554867956373428\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03041776500476731\n",
      "Average test loss: 0.0026304887021995254\n",
      "Epoch 280/300\n",
      "Average training loss: 0.030431554430060917\n",
      "Average test loss: 0.0025926085811936194\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03034582098490662\n",
      "Average test loss: 0.003027694506156776\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030381620662079917\n",
      "Average test loss: 0.002535325499044524\n",
      "Epoch 283/300\n",
      "Average training loss: 0.030323970208565393\n",
      "Average test loss: 0.00575075353599257\n",
      "Epoch 284/300\n",
      "Average training loss: 0.030390550605124897\n",
      "Average test loss: 0.004413567932943503\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030372385879357654\n",
      "Average test loss: 0.002901862337357468\n",
      "Epoch 286/300\n",
      "Average training loss: 0.030323381029897265\n",
      "Average test loss: 0.00264528075647023\n",
      "Epoch 287/300\n",
      "Average training loss: 0.030313595616155202\n",
      "Average test loss: 0.0026024962883028717\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030239745752678977\n",
      "Average test loss: 0.005539885107841757\n",
      "Epoch 289/300\n",
      "Average training loss: 0.030329289936357074\n",
      "Average test loss: 0.0027494646939966415\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030282021368543306\n",
      "Average test loss: 0.0025396742617918387\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03014311009314325\n",
      "Average test loss: 0.0025866510385854375\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03029939728892512\n",
      "Average test loss: 0.002623722886873616\n",
      "Epoch 293/300\n",
      "Average training loss: 0.030229452828566233\n",
      "Average test loss: 0.0025662786821938222\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03017261954314179\n",
      "Average test loss: 0.002739311131131318\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030189284245173136\n",
      "Average test loss: 0.002646746858747469\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030116385193334684\n",
      "Average test loss: 0.002602509603732162\n",
      "Epoch 297/300\n",
      "Average training loss: 0.030142491209838126\n",
      "Average test loss: 0.0026922270064759584\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03018091270658705\n",
      "Average test loss: 0.002619815825795134\n",
      "Epoch 299/300\n",
      "Average training loss: 0.030140529799792502\n",
      "Average test loss: 0.0027172919534560708\n",
      "Epoch 300/300\n",
      "Average training loss: 0.030162935947378476\n",
      "Average test loss: 0.0027636060014160145\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.127756180233425\n",
      "Average test loss: 0.1455958850350645\n",
      "Epoch 2/300\n",
      "Average training loss: 2.5627370238833955\n",
      "Average test loss: 0.005209658588386244\n",
      "Epoch 3/300\n",
      "Average training loss: 1.8758152926762899\n",
      "Average test loss: 0.006074247923576169\n",
      "Epoch 4/300\n",
      "Average training loss: 1.5246621509128147\n",
      "Average test loss: 0.003971901442027755\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2367167539596557\n",
      "Average test loss: 0.003761353865886728\n",
      "Epoch 6/300\n",
      "Average training loss: 1.041470209757487\n",
      "Average test loss: 0.14136555703977743\n",
      "Epoch 7/300\n",
      "Average training loss: 0.8875825662612915\n",
      "Average test loss: 0.005689288492831919\n",
      "Epoch 8/300\n",
      "Average training loss: 0.766685472647349\n",
      "Average test loss: 0.003297791992003719\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6745288224750094\n",
      "Average test loss: 0.003111997024466594\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5974845048056708\n",
      "Average test loss: 0.0031184996912876767\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5320546551545461\n",
      "Average test loss: 0.002976783121625582\n",
      "Epoch 12/300\n",
      "Average training loss: 0.4741664730707804\n",
      "Average test loss: 0.00281066744422747\n",
      "Epoch 13/300\n",
      "Average training loss: 0.42423514485359193\n",
      "Average test loss: 0.002765938790515065\n",
      "Epoch 14/300\n",
      "Average training loss: 0.37937847807672287\n",
      "Average test loss: 0.002670743519026372\n",
      "Epoch 15/300\n",
      "Average training loss: 0.33982213144832185\n",
      "Average test loss: 0.0024595934765206445\n",
      "Epoch 16/300\n",
      "Average training loss: 0.3039507564968533\n",
      "Average test loss: 0.0037766292782293424\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2718313156498803\n",
      "Average test loss: 0.002400722573614783\n",
      "Epoch 18/300\n",
      "Average training loss: 0.24306817401780023\n",
      "Average test loss: 0.00236033084967898\n",
      "Epoch 19/300\n",
      "Average training loss: 0.21840087138281927\n",
      "Average test loss: 0.0022564728285910354\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1962961940103107\n",
      "Average test loss: 0.002484707408481174\n",
      "Epoch 21/300\n",
      "Average training loss: 0.17755985396438176\n",
      "Average test loss: 0.002217974684925543\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1609987287653817\n",
      "Average test loss: 0.002682218269341522\n",
      "Epoch 23/300\n",
      "Average training loss: 0.14652459439966414\n",
      "Average test loss: 0.0021932391381512084\n",
      "Epoch 24/300\n",
      "Average training loss: 0.13369079728921254\n",
      "Average test loss: 0.0021164625220828587\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12205074436134762\n",
      "Average test loss: 0.0021975173987448214\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11194084189997779\n",
      "Average test loss: 0.0020224229184289772\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10279780102438398\n",
      "Average test loss: 0.002041137592453096\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09457056683301926\n",
      "Average test loss: 0.0019847287837829854\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08753668509589302\n",
      "Average test loss: 0.0018849257657097445\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08208074706130558\n",
      "Average test loss: 0.0018934456345935662\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0768502381576432\n",
      "Average test loss: 0.0022930606734007596\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07131334961122937\n",
      "Average test loss: 0.001896033882887827\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06705477262867822\n",
      "Average test loss: 0.0019583807931178145\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06394524745808708\n",
      "Average test loss: 0.002002053518469135\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06065709873371654\n",
      "Average test loss: 8.513530012094932\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05766234956185023\n",
      "Average test loss: 0.0018133130851719114\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05509392131699456\n",
      "Average test loss: 0.0018690870329737663\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05251144333680471\n",
      "Average test loss: 0.0019492645331968864\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05054811926351653\n",
      "Average test loss: 0.0017661736148099105\n",
      "Epoch 40/300\n",
      "Average training loss: 0.048511546985970606\n",
      "Average test loss: 1.038161447313097\n",
      "Epoch 41/300\n",
      "Average training loss: 0.047392818126413556\n",
      "Average test loss: 0.00173420836786843\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04493864055805736\n",
      "Average test loss: 0.061686099544271\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04387462994125154\n",
      "Average test loss: 0.0017618704034636419\n",
      "Epoch 44/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive_Depth10-.025/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive_Depth10-.025/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
