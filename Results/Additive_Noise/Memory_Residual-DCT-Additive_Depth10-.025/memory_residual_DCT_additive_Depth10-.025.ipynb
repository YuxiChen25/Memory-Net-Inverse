{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.025)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.025)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.025)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.025)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11471334538526005\n",
      "Average test loss: 0.005014081900732385\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02854664697084162\n",
      "Average test loss: 0.004729208871929182\n",
      "Epoch 3/300\n",
      "Average training loss: 0.024939228620794084\n",
      "Average test loss: 0.004596915629381935\n",
      "Epoch 4/300\n",
      "Average training loss: 0.023691738203167917\n",
      "Average test loss: 0.004417796780044834\n",
      "Epoch 5/300\n",
      "Average training loss: 0.023048692178395058\n",
      "Average test loss: 0.004378911610278818\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022664727807044982\n",
      "Average test loss: 0.004324963355643882\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02240383119550016\n",
      "Average test loss: 0.004312098530845509\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022175846444235908\n",
      "Average test loss: 0.004279693557777338\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02199753185113271\n",
      "Average test loss: 0.004283875272091892\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021846761125657293\n",
      "Average test loss: 0.004240718091941542\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021699113700124954\n",
      "Average test loss: 0.004216680426150561\n",
      "Epoch 12/300\n",
      "Average training loss: 0.021554429292678833\n",
      "Average test loss: 0.004190743628889322\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021447790703839727\n",
      "Average test loss: 0.0041886717511547935\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02132258910768562\n",
      "Average test loss: 0.004170352931651804\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02120848859184318\n",
      "Average test loss: 0.004154660451329417\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021095384309689203\n",
      "Average test loss: 0.004244939568556017\n",
      "Epoch 17/300\n",
      "Average training loss: 0.020988252060280904\n",
      "Average test loss: 0.004155398483905527\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02089468958311611\n",
      "Average test loss: 0.004108666901373201\n",
      "Epoch 19/300\n",
      "Average training loss: 0.020797788325283263\n",
      "Average test loss: 0.004118228670623568\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02069564813044336\n",
      "Average test loss: 0.004105610185613235\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020622698767317666\n",
      "Average test loss: 0.004088646560079521\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020535016690691313\n",
      "Average test loss: 0.004080045063462523\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020468643613987498\n",
      "Average test loss: 0.004076425655020608\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020389551328288184\n",
      "Average test loss: 0.004045424758146206\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02031830583512783\n",
      "Average test loss: 0.004042595409684711\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02027402230931653\n",
      "Average test loss: 0.00403648379093243\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020211561626858183\n",
      "Average test loss: 0.004038373130684098\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02015598594976796\n",
      "Average test loss: 0.004059555752409829\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020095663698183167\n",
      "Average test loss: 0.004064991486983167\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02003957058654891\n",
      "Average test loss: 0.0040351821287638616\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019995742311080297\n",
      "Average test loss: 0.004010490001489719\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01993641382869747\n",
      "Average test loss: 0.0040033629453844494\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01990216647254096\n",
      "Average test loss: 0.004019922905912002\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019851783860060902\n",
      "Average test loss: 0.004029693162896567\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019807906525002587\n",
      "Average test loss: 0.004008958922699094\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0197581495112843\n",
      "Average test loss: 0.004023683724717961\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019704664424061775\n",
      "Average test loss: 0.0040254339360528526\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01966499595509635\n",
      "Average test loss: 0.004002458355493016\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01962034014529652\n",
      "Average test loss: 0.004002189240107933\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019590744425853093\n",
      "Average test loss: 0.0040013681056184905\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019540902537604172\n",
      "Average test loss: 0.00403979822455181\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019497690088219114\n",
      "Average test loss: 0.004016852123041948\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019468903601997427\n",
      "Average test loss: 0.003994686741795804\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019408067453238698\n",
      "Average test loss: 0.0040437657849656215\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019370798016587892\n",
      "Average test loss: 0.003999654466907183\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019335620653298167\n",
      "Average test loss: 0.004009504026836819\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019288371092743343\n",
      "Average test loss: 0.004024053623278936\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01924741721815533\n",
      "Average test loss: 0.004104537731450465\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01921588146686554\n",
      "Average test loss: 0.0040272450171824955\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019162610583007337\n",
      "Average test loss: 0.0040216821525245906\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019116014778614045\n",
      "Average test loss: 0.004025602557489441\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019099232016338244\n",
      "Average test loss: 0.004011877154103584\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019025720361206266\n",
      "Average test loss: 0.00407525075889296\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01900093514058325\n",
      "Average test loss: 0.0040222080155379245\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018951971602108743\n",
      "Average test loss: 0.004016691774543789\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01890831455257204\n",
      "Average test loss: 0.00405784728191793\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018881199608246486\n",
      "Average test loss: 0.0040173189006745815\n",
      "Epoch 58/300\n",
      "Average training loss: 0.018817417259017626\n",
      "Average test loss: 0.004048703103429741\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01878382386184401\n",
      "Average test loss: 0.004066928531560633\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018752345206009016\n",
      "Average test loss: 0.0040711617939588095\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018728581678536204\n",
      "Average test loss: 0.004108891313274701\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01869477476345168\n",
      "Average test loss: 0.004032046791373028\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01864087324010001\n",
      "Average test loss: 0.0041004731187389955\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018587993183069758\n",
      "Average test loss: 0.004081792353341977\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01856753484242492\n",
      "Average test loss: 0.004034772143181827\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018517398397127786\n",
      "Average test loss: 0.004053047580437528\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01847811107337475\n",
      "Average test loss: 0.004143165306084686\n",
      "Epoch 68/300\n",
      "Average training loss: 0.018462785489029354\n",
      "Average test loss: 0.004084109228932195\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018405717978047\n",
      "Average test loss: 0.0042536547837985885\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01836624010072814\n",
      "Average test loss: 0.004082478763328659\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01836685876879427\n",
      "Average test loss: 0.004071875755571656\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01830150323278374\n",
      "Average test loss: 0.004104300388859378\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01825844857427809\n",
      "Average test loss: 0.004128326655469007\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018257578309211465\n",
      "Average test loss: 0.004142008730934726\n",
      "Epoch 75/300\n",
      "Average training loss: 0.018199434113171365\n",
      "Average test loss: 0.004075885445707374\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018172433725661703\n",
      "Average test loss: 0.0040835461926956975\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018159289745820893\n",
      "Average test loss: 0.0042312935147848394\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018102886446648175\n",
      "Average test loss: 0.004147407229989767\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018071203135781817\n",
      "Average test loss: 0.004092062347465091\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018038332914312682\n",
      "Average test loss: 0.004128805026825931\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018000803266962368\n",
      "Average test loss: 0.004112846502413352\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01798022145860725\n",
      "Average test loss: 0.004100916366817223\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01796707900199625\n",
      "Average test loss: 0.004074419958723916\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0179308651495311\n",
      "Average test loss: 0.004184764572315746\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017887439192997085\n",
      "Average test loss: 0.004146485027960605\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017871670237845846\n",
      "Average test loss: 0.004073694632906053\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017830444931156104\n",
      "Average test loss: 0.004150423475851615\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01780837724275059\n",
      "Average test loss: 0.004207352158510023\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01777985012365712\n",
      "Average test loss: 0.004073445666167471\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017751045299900903\n",
      "Average test loss: 0.0042741555859231286\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017726641434762214\n",
      "Average test loss: 0.004213872673817807\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017699273942245377\n",
      "Average test loss: 0.004127665364907848\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017676491771307257\n",
      "Average test loss: 0.004137606475502252\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017651512427462473\n",
      "Average test loss: 0.0041816206400593125\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017631369906995032\n",
      "Average test loss: 0.004122953194710943\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017593844233287706\n",
      "Average test loss: 0.0041238925949566895\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017564540640347535\n",
      "Average test loss: 0.00419376060904728\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01754397080010838\n",
      "Average test loss: 0.004181944703476296\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01753253329048554\n",
      "Average test loss: 0.004175907006694211\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017493494662973615\n",
      "Average test loss: 0.0041875590214298835\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017485889847079914\n",
      "Average test loss: 0.004211577590141032\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017467270402444734\n",
      "Average test loss: 0.004171894129572643\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017427344064745637\n",
      "Average test loss: 0.004138015439526902\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01740738995621602\n",
      "Average test loss: 0.004288044754415751\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017378324409325917\n",
      "Average test loss: 0.004211661180274354\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017368966607583892\n",
      "Average test loss: 0.004286182615078158\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017361250571078724\n",
      "Average test loss: 0.0042451396741800835\n",
      "Epoch 108/300\n",
      "Average training loss: 0.017325779120955204\n",
      "Average test loss: 0.004184358421713114\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017302742009361584\n",
      "Average test loss: 0.0041840958936760824\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017282685884998903\n",
      "Average test loss: 0.0043055821388132045\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017269831380910344\n",
      "Average test loss: 0.004439731061458588\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01726474342909124\n",
      "Average test loss: 0.004143916919827461\n",
      "Epoch 113/300\n",
      "Average training loss: 0.017224977986680136\n",
      "Average test loss: 0.004243385253888038\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01719459241628647\n",
      "Average test loss: 0.0041559224923451744\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017189374428656368\n",
      "Average test loss: 0.0043052228117982545\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01716288919415739\n",
      "Average test loss: 0.004267797388136387\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01713744125349654\n",
      "Average test loss: 0.0042646586431397335\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01713985624578264\n",
      "Average test loss: 0.004199122785280148\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01710993517604139\n",
      "Average test loss: 0.004200764442483584\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017091462016105653\n",
      "Average test loss: 0.00427576088367237\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017073306139972474\n",
      "Average test loss: 0.004278658751398325\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017058457991315258\n",
      "Average test loss: 0.00432943153505524\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01703802763753467\n",
      "Average test loss: 0.004130691145857175\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017020901904337936\n",
      "Average test loss: 0.004209923933363623\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01700989615668853\n",
      "Average test loss: 0.0042683987863775755\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016979271054267882\n",
      "Average test loss: 0.004358308866620064\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01697620562215646\n",
      "Average test loss: 0.004319723795271582\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01695656379395061\n",
      "Average test loss: 0.004328571303023232\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016933651516007054\n",
      "Average test loss: 0.0043572682576874895\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01690940277940697\n",
      "Average test loss: 0.0042313102674153115\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01690856913725535\n",
      "Average test loss: 0.00430479132797983\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016891871843073104\n",
      "Average test loss: 0.004195553269030319\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016872848846018314\n",
      "Average test loss: 0.00430537752310435\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016847849640581343\n",
      "Average test loss: 0.0042853991956346565\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016848086257775625\n",
      "Average test loss: 0.004297713426252206\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01682691106200218\n",
      "Average test loss: 0.004248685744487577\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016822782326075765\n",
      "Average test loss: 0.00420714498621722\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016785452565385234\n",
      "Average test loss: 0.004405327814734644\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016780020135972237\n",
      "Average test loss: 0.004245897714876466\n",
      "Epoch 140/300\n",
      "Average test loss: 0.004239288380162584\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016758635437322988\n",
      "Average test loss: 0.004333252973440621\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01673296036902401\n",
      "Average test loss: 0.004251563116080231\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016716191407707002\n",
      "Average test loss: 0.0042162471020387275\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016710564482543204\n",
      "Average test loss: 0.004328135159487526\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016682352107432154\n",
      "Average test loss: 0.004192643520732721\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016669858131143783\n",
      "Average test loss: 0.004409468914071719\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016600697913103632\n",
      "Average test loss: 0.004364344367550479\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016607764681180318\n",
      "Average test loss: 0.004308451957586739\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016584413806597392\n",
      "Average test loss: 0.004215333378977245\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016573010795646242\n",
      "Average test loss: 0.0042837111403544745\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01655765931556622\n",
      "Average test loss: 0.004275327518996265\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01653345958309041\n",
      "Average test loss: 0.004418751853000787\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01651702137870921\n",
      "Average test loss: 0.004460195353461637\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016524169075820182\n",
      "Average test loss: 0.004380889387594329\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016511338676015536\n",
      "Average test loss: 0.004224538372208674\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01650030729588535\n",
      "Average test loss: 0.004379313924453325\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016479520582490498\n",
      "Average test loss: 0.004345160177598397\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01648075471901231\n",
      "Average test loss: 0.004315881796181202\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016458899709913467\n",
      "Average test loss: 0.004299262381676171\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016469299491080974\n",
      "Average test loss: 0.00432060872659915\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016384689101742372\n",
      "Average test loss: 0.004272325394882096\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01639156887680292\n",
      "Average test loss: 0.004396927391075426\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016367631525629096\n",
      "Average test loss: 0.0043565020863380695\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01635311466289891\n",
      "Average test loss: 0.004299134329789215\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01634347574247254\n",
      "Average test loss: 0.004346287729011642\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016337386028634177\n",
      "Average test loss: 0.004416126410580343\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01634074431988928\n",
      "Average test loss: 0.004351651585350434\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01632402827259567\n",
      "Average test loss: 0.0043451833447648415\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016301497181256612\n",
      "Average test loss: 0.004205970519739721\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01630155120790005\n",
      "Average test loss: 0.0043710766798920105\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016290396486719448\n",
      "Average test loss: 0.0044008417584829864\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016273873432642885\n",
      "Average test loss: 0.004358975187978811\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01626130247116089\n",
      "Average test loss: 0.004331031666447719\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01625783172332578\n",
      "Average test loss: 0.004321802846259541\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016245696546302903\n",
      "Average test loss: 0.004315169308334589\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01624194212920136\n",
      "Average test loss: 0.004335558886743254\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016246613445381322\n",
      "Average test loss: 0.004249211631715297\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016224201038479803\n",
      "Average test loss: 0.004394137827058634\n",
      "Epoch 188/300\n",
      "Average training loss: 0.016224839372767343\n",
      "Average test loss: 0.004387942746695545\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01621260619825787\n",
      "Average test loss: 0.004537681142489115\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016203415527111954\n",
      "Average test loss: 0.004302648480774628\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01618916694819927\n",
      "Average test loss: 0.004269192255205579\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016178863851560487\n",
      "Average test loss: 0.004299389652080006\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01617390349507332\n",
      "Average test loss: 0.004276640411880282\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016159056685037084\n",
      "Average test loss: 0.004461192675762706\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016173342987895013\n",
      "Average test loss: 0.00429551507325636\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016132736802101134\n",
      "Average test loss: 0.004384309330334266\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016146563289066156\n",
      "Average test loss: 0.0043244105575399264\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016125701366199386\n",
      "Average test loss: 0.0045606768341321085\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016115681317117478\n",
      "Average test loss: 0.004368169146279494\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016097056337528757\n",
      "Average test loss: 0.004301625370151467\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0160990479869975\n",
      "Average test loss: 0.004464403846197658\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01609673219588068\n",
      "Average test loss: 0.0042394302810231845\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016095845746497313\n",
      "Average test loss: 0.0044251700672838425\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01607165066235595\n",
      "Average test loss: 0.004373091436508629\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016042043648660183\n",
      "Average test loss: 0.004452917056158185\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016027559061017303\n",
      "Average test loss: 0.004326823819014761\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016039533380005095\n",
      "Average test loss: 0.00436232974835568\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0160010155853298\n",
      "Average test loss: 0.004276913485179345\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016001427402926816\n",
      "Average test loss: 0.004311003375798463\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0159834547440211\n",
      "Average test loss: 0.004500541806634929\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015980201466215982\n",
      "Average test loss: 0.00431532539986074\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015975735356410344\n",
      "Average test loss: 0.004356618057108587\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015971931936012373\n",
      "Average test loss: 0.004494955946587854\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015965765366951625\n",
      "Average test loss: 0.004291014618757698\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01595848287145297\n",
      "Average test loss: 0.004505011731551753\n",
      "Epoch 220/300\n",
      "Average training loss: 0.015941216393477387\n",
      "Average test loss: 0.004397088961261842\n",
      "Epoch 221/300\n",
      "Average training loss: 0.015959346604843935\n",
      "Average test loss: 0.004391831724180116\n",
      "Epoch 222/300\n",
      "Average training loss: 0.015941335729426808\n",
      "Average test loss: 0.004301569323158927\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01591755366242594\n",
      "Average test loss: 0.004588253063460191\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0159201312172744\n",
      "Average test loss: 0.004517415105882618\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01592565187315146\n",
      "Average test loss: 0.004447885853548845\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01590180984305011\n",
      "Average test loss: 0.004343364673356215\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015891393941309718\n",
      "Average test loss: 0.004579003208627304\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015901223969128398\n",
      "Average test loss: 0.004441496750960747\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015893589322765668\n",
      "Average test loss: 0.004304262269288301\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01588247917178604\n",
      "Average test loss: 0.00448282684923874\n",
      "Epoch 231/300\n",
      "Average training loss: 0.015886031297345955\n",
      "Average test loss: 0.004489299407849709\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01586966295954254\n",
      "Average test loss: 0.004461608796897862\n",
      "Epoch 233/300\n",
      "Average training loss: 0.015856799664596716\n",
      "Average test loss: 0.004533557834517625\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0158571669186155\n",
      "Average test loss: 0.004457480030755202\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01585090595152643\n",
      "Average test loss: 0.004288717479962441\n",
      "Epoch 236/300\n",
      "Average training loss: 0.015837334846456847\n",
      "Average test loss: 0.0043588541025916736\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015844857840074434\n",
      "Average test loss: 0.004382831250006954\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015812949599491224\n",
      "Average test loss: 0.004307703737169504\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01581510720319218\n",
      "Average test loss: 0.0043812356800254845\n",
      "Epoch 240/300\n",
      "Average training loss: 0.015819060939053695\n",
      "Average test loss: 0.004455750839784741\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015815980337560178\n",
      "Average test loss: 0.004571781951313218\n",
      "Epoch 242/300\n",
      "Average training loss: 0.015789397375451195\n",
      "Average test loss: 0.004370003430172801\n",
      "Epoch 243/300\n",
      "Average training loss: 0.015796810859607324\n",
      "Average test loss: 0.0044441105963455305\n",
      "Epoch 244/300\n",
      "Average training loss: 0.015789985658393967\n",
      "Average test loss: 0.004405783712656961\n",
      "Epoch 245/300\n",
      "Average training loss: 0.015782837917407352\n",
      "Average test loss: 0.004294187787092394\n",
      "Epoch 246/300\n",
      "Average training loss: 0.015774720858368608\n",
      "Average test loss: 0.004400264843884442\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015776092676652802\n",
      "Average test loss: 0.004455178110963768\n",
      "Epoch 248/300\n",
      "Average training loss: 0.015753760914007823\n",
      "Average test loss: 0.0043084955062303275\n",
      "Epoch 249/300\n",
      "Average training loss: 0.015758982541660468\n",
      "Average test loss: 0.004347160290512774\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015767234486838182\n",
      "Average test loss: 0.004498692196897335\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01574322924266259\n",
      "Average test loss: 0.00452012658615907\n",
      "Epoch 252/300\n",
      "Average training loss: 0.015741250914004114\n",
      "Average test loss: 0.0044196054442889165\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01572617821726534\n",
      "Average test loss: 0.004326116881436772\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015734495140612124\n",
      "Average test loss: 0.004393691581984361\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015734411569933097\n",
      "Average test loss: 0.004573949219038089\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015711182597610687\n",
      "Average test loss: 0.004442946145104037\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015701291913787524\n",
      "Average test loss: 0.004540658173875676\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01571571457468801\n",
      "Average test loss: 0.00451864456716511\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015708566755056382\n",
      "Average test loss: 0.004421334226512246\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015705977943208483\n",
      "Average test loss: 0.004460464096317689\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015685521548820868\n",
      "Average test loss: 0.004501133893306057\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015685657191607687\n",
      "Average test loss: 0.004528942142095831\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015674456478820906\n",
      "Average test loss: 0.004466672819935613\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01568690169023143\n",
      "Average test loss: 0.0045061881716052694\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0156508153643873\n",
      "Average test loss: 0.0046395670316285555\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015663428942362466\n",
      "Average test loss: 0.004418023218918178\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015656002009908358\n",
      "Average test loss: 0.004304625362571743\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015656126932965385\n",
      "Average test loss: 0.004428207393735647\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015637925172845523\n",
      "Average test loss: 0.004427969800929229\n",
      "Epoch 270/300\n",
      "Average training loss: 0.015648346569803025\n",
      "Average test loss: 0.00434449208362235\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015612128350469802\n",
      "Average test loss: 0.004355309332410495\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015629077857567203\n",
      "Average test loss: 0.004467809771498044\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015613886347247495\n",
      "Average test loss: 0.004406872362519304\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015624389222098722\n",
      "Average test loss: 0.004404187625067102\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015606966954138544\n",
      "Average test loss: 0.004354353600078159\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015595077047331466\n",
      "Average test loss: 0.004508093133568764\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015599559870031145\n",
      "Average test loss: 0.0043619483663804\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015600047262178528\n",
      "Average test loss: 0.004388219879733191\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015581984311342239\n",
      "Average test loss: 0.004397618629038334\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0155829097111192\n",
      "Average test loss: 0.0044389136417044535\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015589395296242502\n",
      "Average test loss: 0.0046015977230336935\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015580059773392147\n",
      "Average test loss: 0.004479725080438786\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015584820926189423\n",
      "Average test loss: 0.0044399417618082625\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0155513252524866\n",
      "Average test loss: 0.004412102215819888\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015557780995965003\n",
      "Average test loss: 0.0044404451702204015\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015551366696755092\n",
      "Average test loss: 0.004460882383709152\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015553718527158101\n",
      "Average test loss: 0.004419262105599045\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015532188213533825\n",
      "Average test loss: 0.004435041702662905\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01553557266543309\n",
      "Average test loss: 0.004464374090234439\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015546940937638283\n",
      "Average test loss: 0.004526842166566187\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015527100180586178\n",
      "Average test loss: 0.004616407848480675\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01552167454858621\n",
      "Average test loss: 0.004555792045262125\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015536983750760555\n",
      "Average test loss: 0.0044383119162586\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01551811964975463\n",
      "Average test loss: 0.004402499727076954\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015511709070867963\n",
      "Average test loss: 0.004445163559168577\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01550177795936664\n",
      "Average test loss: 0.0044925228195885816\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015501312502556376\n",
      "Average test loss: 0.004375980235222313\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015506328915556271\n",
      "Average test loss: 0.004483871024929815\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01549874168799983\n",
      "Average test loss: 0.004441261482735475\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015488370700014962\n",
      "Average test loss: 0.004401431401777599\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11377666165100203\n",
      "Average test loss: 0.004744349770247936\n",
      "Epoch 2/300\n",
      "Average training loss: 0.027010596008764372\n",
      "Average test loss: 0.004193406223836872\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022980013171831768\n",
      "Average test loss: 0.004168790669076972\n",
      "Epoch 4/300\n",
      "Average training loss: 0.021559497038523355\n",
      "Average test loss: 0.0038925863098767067\n",
      "Epoch 5/300\n",
      "Average training loss: 0.020815374382668072\n",
      "Average test loss: 0.0038878245631025895\n",
      "Epoch 6/300\n",
      "Average training loss: 0.020335335354010265\n",
      "Average test loss: 0.003729263491100735\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0199670249554846\n",
      "Average test loss: 0.003699598533411821\n",
      "Epoch 8/300\n",
      "Average training loss: 0.019675583112570973\n",
      "Average test loss: 0.0036392076143787967\n",
      "Epoch 9/300\n",
      "Average training loss: 0.019419493819276493\n",
      "Average test loss: 0.0036100518935256536\n",
      "Epoch 10/300\n",
      "Average training loss: 0.019202397166026963\n",
      "Average test loss: 0.0035885525519649187\n",
      "Epoch 11/300\n",
      "Average training loss: 0.018993729298313458\n",
      "Average test loss: 0.003497304311643044\n",
      "Epoch 12/300\n",
      "Average training loss: 0.018794446893864208\n",
      "Average test loss: 0.0034659091021037765\n",
      "Epoch 13/300\n",
      "Average training loss: 0.018594600435760285\n",
      "Average test loss: 0.0034403727035969495\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01840865147776074\n",
      "Average test loss: 0.0034409880087607437\n",
      "Epoch 15/300\n",
      "Average training loss: 0.018207913676897686\n",
      "Average test loss: 0.003390146145804061\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01803135079393784\n",
      "Average test loss: 0.0033613890272875626\n",
      "Epoch 17/300\n",
      "Average training loss: 0.017856916674309307\n",
      "Average test loss: 0.0033495504736072487\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0176889511777295\n",
      "Average test loss: 0.0034270746443006727\n",
      "Epoch 19/300\n",
      "Average training loss: 0.017520576232009463\n",
      "Average test loss: 0.0033419676329940556\n",
      "Epoch 20/300\n",
      "Average training loss: 0.017362515404820443\n",
      "Average test loss: 0.0032611243184655905\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017208617207904658\n",
      "Average test loss: 0.003269806714521514\n",
      "Epoch 22/300\n",
      "Average training loss: 0.017076158032649093\n",
      "Average test loss: 0.0032286441311654116\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016943187318742274\n",
      "Average test loss: 0.0032402972674204244\n",
      "Epoch 24/300\n",
      "Average training loss: 0.016811018539799584\n",
      "Average test loss: 0.0032463240300615627\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016689838671849835\n",
      "Average test loss: 0.003195301542472508\n",
      "Epoch 26/300\n",
      "Average training loss: 0.016573136472039753\n",
      "Average test loss: 0.0031917192731052636\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01646198842343357\n",
      "Average test loss: 0.0031839091815054417\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016355117712583807\n",
      "Average test loss: 0.0031606072125335534\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01627019200225671\n",
      "Average test loss: 0.003178020968205399\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016151129233340424\n",
      "Average test loss: 0.0031394286954568493\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016071517631411552\n",
      "Average test loss: 0.0031261042648305496\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01599124966810147\n",
      "Average test loss: 0.0031834856476634743\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015910654204587142\n",
      "Average test loss: 0.0031720151830878524\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01582173193825616\n",
      "Average test loss: 0.003125391482892964\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015738333926432663\n",
      "Average test loss: 0.003107487077928252\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015669762060046197\n",
      "Average test loss: 0.0030998983283837636\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015588105450073877\n",
      "Average test loss: 0.003180937254180511\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015510885900921291\n",
      "Average test loss: 0.0031601629226158065\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015448686705695259\n",
      "Average test loss: 0.003105942370576991\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015353426343864865\n",
      "Average test loss: 0.003131015944812033\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015311812298165427\n",
      "Average test loss: 0.0032228250563558604\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015216782473027705\n",
      "Average test loss: 0.003109457432395882\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015163718659844663\n",
      "Average test loss: 0.003114891576891144\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015084836593932576\n",
      "Average test loss: 0.0031047140165335603\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015018151070508691\n",
      "Average test loss: 0.003184709626974331\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01495673103051053\n",
      "Average test loss: 0.0032295729426874056\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014905461915665203\n",
      "Average test loss: 0.003140211528994971\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014838040507502027\n",
      "Average test loss: 0.003200106888388594\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014764841809868813\n",
      "Average test loss: 0.003199680211643378\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01470088673548566\n",
      "Average test loss: 0.0032706062787522874\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014646250829100609\n",
      "Average test loss: 0.0031238242940356334\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01458780684073766\n",
      "Average test loss: 0.003151051219138834\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014531289377146298\n",
      "Average test loss: 0.0031694938265201117\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014485661181310812\n",
      "Average test loss: 0.0031086549709240597\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01443686664932304\n",
      "Average test loss: 0.0031010935803254445\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014369120242695014\n",
      "Average test loss: 0.0031450960472640065\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014305705266694228\n",
      "Average test loss: 0.003122742518161734\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014257060926821497\n",
      "Average test loss: 0.003262062809947464\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014231185379127662\n",
      "Average test loss: 0.0031853373421149122\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014159633698562782\n",
      "Average test loss: 0.003183991972770956\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014107516839272445\n",
      "Average test loss: 0.003134999860285057\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014080184765987927\n",
      "Average test loss: 0.00330119230494731\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014031615581777361\n",
      "Average test loss: 0.003159037289313144\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013991329901748234\n",
      "Average test loss: 0.0031780056895481215\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013948361400928762\n",
      "Average test loss: 0.003180234412766165\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013903560923205481\n",
      "Average test loss: 0.003178404955400361\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013860001315673193\n",
      "Average test loss: 0.0032200666161047086\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013836200288600392\n",
      "Average test loss: 0.003195724149545034\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013784481363164054\n",
      "Average test loss: 0.0031327680788106388\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013742809534072876\n",
      "Average test loss: 0.0032695565352009403\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013725412398576737\n",
      "Average test loss: 0.0031743033226165505\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01368122493972381\n",
      "Average test loss: 0.0033700710116989084\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013642684923277961\n",
      "Average test loss: 0.0034078686013817787\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013616751722991467\n",
      "Average test loss: 0.003182612729362316\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013597113261206283\n",
      "Average test loss: 0.003249450296991401\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013562393910355039\n",
      "Average test loss: 0.003231811574763722\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013524736331568823\n",
      "Average test loss: 0.0032756275151752763\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013481917671859264\n",
      "Average test loss: 0.003258531769530641\n",
      "Epoch 79/300\n",
      "Average training loss: 0.013460733306076791\n",
      "Average test loss: 0.0033409097445093925\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013428760690821542\n",
      "Average test loss: 0.00326654112421804\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013395669870078564\n",
      "Average test loss: 0.003382977126787106\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013363899012406666\n",
      "Average test loss: 0.0033453048910531734\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013336874057021405\n",
      "Average test loss: 0.003203286904634701\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013322526406082842\n",
      "Average test loss: 0.003264169897263249\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013301397580239509\n",
      "Average test loss: 0.0033358163943307266\n",
      "Epoch 86/300\n",
      "Average training loss: 0.013275065852536096\n",
      "Average test loss: 0.0031991356511910755\n",
      "Epoch 87/300\n",
      "Average training loss: 0.013246540904872947\n",
      "Average test loss: 0.0032360982992168928\n",
      "Epoch 88/300\n",
      "Average training loss: 0.013213035193582376\n",
      "Average test loss: 0.0032062391315897305\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01319113970713483\n",
      "Average test loss: 0.0033893182567424243\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01317041621522771\n",
      "Average test loss: 0.00349277351775931\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01315815258026123\n",
      "Average test loss: 0.0032587339277896617\n",
      "Epoch 92/300\n",
      "Average training loss: 0.013130255872176753\n",
      "Average test loss: 0.0032640957348048687\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01312355397310522\n",
      "Average test loss: 0.003197719478772746\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013062389771143596\n",
      "Average test loss: 0.0035333947607626517\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013056162413623599\n",
      "Average test loss: 0.003262405303410358\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013037965973218282\n",
      "Average test loss: 0.003333074862137437\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013026887672642868\n",
      "Average test loss: 0.003342875892089473\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012995903749432828\n",
      "Average test loss: 0.003193469322183066\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012976267672247356\n",
      "Average test loss: 0.003271730083351334\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012962957608203093\n",
      "Average test loss: 0.003320022763684392\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012932655525704224\n",
      "Average test loss: 0.003295912414168318\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012904911061127981\n",
      "Average test loss: 0.003236591071097387\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012904438040322727\n",
      "Average test loss: 0.0033483141836606794\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01287073700543907\n",
      "Average test loss: 0.0033924906477332114\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012866440566049681\n",
      "Average test loss: 0.0033137439895007345\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012833345436387592\n",
      "Average test loss: 0.003280669611568252\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012856797524624401\n",
      "Average test loss: 0.0032776268385350704\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012806194220152166\n",
      "Average test loss: 0.0032451207356320485\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012784189173744784\n",
      "Average test loss: 0.003335635860967967\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012765241973929935\n",
      "Average test loss: 0.0033118807178818517\n",
      "Epoch 111/300\n",
      "Average training loss: 0.012780264991852973\n",
      "Average test loss: 0.003369087503188186\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012726907140678829\n",
      "Average test loss: 0.003332302379111449\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012737616904907757\n",
      "Average test loss: 0.0033588646973172825\n",
      "Epoch 114/300\n",
      "Average training loss: 0.012701272250877487\n",
      "Average test loss: 0.003370591367284457\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012696724580807818\n",
      "Average test loss: 0.003495707521835963\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012669992425375514\n",
      "Average test loss: 0.0034030503866573176\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012681901524464289\n",
      "Average test loss: 0.0033677554167807103\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01265403316832251\n",
      "Average test loss: 0.0034163582486410934\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012655429295367665\n",
      "Average test loss: 0.003350886232323117\n",
      "Epoch 120/300\n",
      "Average training loss: 0.012603484787874751\n",
      "Average test loss: 0.003276412319391966\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012592620142632061\n",
      "Average test loss: 0.0033011273592710496\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012600737636288007\n",
      "Average test loss: 0.0033058198599351775\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012582013605369462\n",
      "Average test loss: 0.003299169504808055\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012566938790182272\n",
      "Average test loss: 0.0034053020024051267\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01256012020425664\n",
      "Average test loss: 0.003385243509378698\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012531062444051107\n",
      "Average test loss: 0.003368657129092349\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012537444848153327\n",
      "Average test loss: 0.0033870498910546303\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0125084664258692\n",
      "Average test loss: 0.0032591536374141773\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01250987932085991\n",
      "Average test loss: 0.003381783986878064\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012487811668051614\n",
      "Average test loss: 0.0033466626331210138\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012470792995558845\n",
      "Average test loss: 0.0034476030150221455\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01245479512380229\n",
      "Average test loss: 0.0033304461222141983\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012470724096728695\n",
      "Average test loss: 0.0033405763664179377\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012442712855007913\n",
      "Average test loss: 0.0033594757966283295\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012403948977589608\n",
      "Average test loss: 0.003437811449997955\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012437793417109384\n",
      "Average test loss: 0.003319008013750944\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012412705844475163\n",
      "Average test loss: 0.0032694332603779106\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012396482655571566\n",
      "Average test loss: 0.0033697774240540133\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01239667961994807\n",
      "Average test loss: 0.003509043925959203\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012363669952584637\n",
      "Average test loss: 0.0036417421319832406\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012360519937756989\n",
      "Average test loss: 0.0034003835076259242\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01235841228812933\n",
      "Average test loss: 0.003361454620336493\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01233768664383226\n",
      "Average test loss: 0.0033317389283329247\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012324434623950057\n",
      "Average test loss: 0.0033145519006583424\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012317389990720485\n",
      "Average test loss: 0.0033748751008469196\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012295603908598423\n",
      "Average test loss: 0.003363472477222482\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012298031454284986\n",
      "Average test loss: 0.003325570187634892\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01228063533289565\n",
      "Average test loss: 0.0034008071165945795\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012285121940076351\n",
      "Average test loss: 0.003458419239769379\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012269971629811657\n",
      "Average test loss: 0.0033752518167926206\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012248411171138287\n",
      "Average test loss: 0.003468813178853856\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012273059569299222\n",
      "Average test loss: 0.0034527103265540467\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012228630541927285\n",
      "Average test loss: 0.003390605681265394\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012229551620781421\n",
      "Average test loss: 0.0034271728504035206\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012222699536217584\n",
      "Average test loss: 0.0033254782089756595\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012221505497064855\n",
      "Average test loss: 0.003432234308992823\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01220675123317374\n",
      "Average test loss: 0.003335267125732369\n",
      "Epoch 158/300\n",
      "Average training loss: 0.012200183354318142\n",
      "Average test loss: 0.0034347950609193907\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012181413011418448\n",
      "Average test loss: 0.0034172432778610123\n",
      "Epoch 160/300\n",
      "Average training loss: 0.012166879224280516\n",
      "Average test loss: 0.003420316990671886\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01215466128124131\n",
      "Average test loss: 0.003343791493939029\n",
      "Epoch 162/300\n",
      "Average training loss: 0.012137958721154266\n",
      "Average test loss: 0.003353868332794971\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012157955331934823\n",
      "Average test loss: 0.0033925946975747745\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01213523153298431\n",
      "Average test loss: 0.0033632213808596134\n",
      "Epoch 165/300\n",
      "Average training loss: 0.012124096632003784\n",
      "Average test loss: 0.003350391993506087\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01211669167296754\n",
      "Average test loss: 0.003392712567001581\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01210997067971362\n",
      "Average test loss: 0.003449374182563689\n",
      "Epoch 168/300\n",
      "Average training loss: 0.012112836688756942\n",
      "Average test loss: 0.0035051915302044814\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012086128251420128\n",
      "Average test loss: 0.003521604574802849\n",
      "Epoch 170/300\n",
      "Average training loss: 0.012097390104499128\n",
      "Average test loss: 0.003313312449389034\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012077463318076399\n",
      "Average test loss: 0.003507008806698852\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012057862232956622\n",
      "Average test loss: 0.0035706434156745673\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012080776698473426\n",
      "Average test loss: 0.0035562856880327065\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012054122866855727\n",
      "Average test loss: 0.003368117394960589\n",
      "Epoch 175/300\n",
      "Average training loss: 0.012055362573928302\n",
      "Average test loss: 0.0033275577305919594\n",
      "Epoch 176/300\n",
      "Average training loss: 0.012036288811100854\n",
      "Average test loss: 0.00341222210538884\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012022257708013058\n",
      "Average test loss: 0.0035141430482682253\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012017633741928472\n",
      "Average test loss: 0.003422555806529191\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012048163624273406\n",
      "Average test loss: 0.0035180189464655187\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01200468548718426\n",
      "Average test loss: 0.003395385087157289\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011994435634050104\n",
      "Average test loss: 0.003418621422515975\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01199490968303548\n",
      "Average test loss: 0.003528485144591994\n",
      "Epoch 183/300\n",
      "Average training loss: 0.012003332038720449\n",
      "Average test loss: 0.003448646173915929\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011977095925145679\n",
      "Average test loss: 0.0035084914279480774\n",
      "Epoch 185/300\n",
      "Average training loss: 0.012000590484175417\n",
      "Average test loss: 0.0034210091487814984\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011977896940377023\n",
      "Average test loss: 0.003319006590379609\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011965326464838453\n",
      "Average test loss: 0.0034453290682286025\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011951634261343214\n",
      "Average test loss: 0.003693800690273444\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011938440221051375\n",
      "Average test loss: 0.0033283309290806453\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011941286707503928\n",
      "Average test loss: 0.003565505508747366\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01193458717978663\n",
      "Average test loss: 0.003490375381377008\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011926233514315552\n",
      "Average test loss: 0.0034674837638934454\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011904876576529609\n",
      "Average test loss: 0.0035307522275381618\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011918177092240917\n",
      "Average test loss: 0.0034148770421743393\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011931569337844849\n",
      "Average test loss: 0.0035024794273906283\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011886444937851693\n",
      "Average test loss: 0.0034147708693312274\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011903067416614956\n",
      "Average test loss: 0.0033111038932369815\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011879691075119707\n",
      "Average test loss: 0.0034372385723723305\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011872560327251752\n",
      "Average test loss: 0.0034047938532506426\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011862138224972618\n",
      "Average test loss: 0.003554601354731454\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01188437479486068\n",
      "Average test loss: 0.003417043247984515\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011871808531797595\n",
      "Average test loss: 0.0036002275285621486\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011840304584138922\n",
      "Average test loss: 0.0034148777253511877\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011851421741975678\n",
      "Average test loss: 0.0035168163950761986\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01186758169449038\n",
      "Average test loss: 0.003356223824330502\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011841217749648624\n",
      "Average test loss: 0.003549472909213768\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011832916618221336\n",
      "Average test loss: 0.003555116074780623\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011833882378207313\n",
      "Average test loss: 0.0034564993017249636\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011814248855743142\n",
      "Average test loss: 0.0035022688826753033\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011816581226057476\n",
      "Average test loss: 0.003365204967558384\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011801839600834581\n",
      "Average test loss: 0.003386500425222847\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01181451457656092\n",
      "Average test loss: 0.0034394576822717985\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011795601366708677\n",
      "Average test loss: 0.00355690656374726\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011794221661157078\n",
      "Average test loss: 0.003524219428913461\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01178119072649214\n",
      "Average test loss: 0.003433118652552366\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011761770764986674\n",
      "Average test loss: 0.003466766143631604\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01177155447171794\n",
      "Average test loss: 0.0034534670818183156\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011769638835555978\n",
      "Average test loss: 0.003646348022131456\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011768003067208661\n",
      "Average test loss: 0.00342540227352745\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011757815076245202\n",
      "Average test loss: 0.0033987740060935417\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011755178841451804\n",
      "Average test loss: 0.003434507004916668\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011741311352286074\n",
      "Average test loss: 0.00344624122335679\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011753545901841588\n",
      "Average test loss: 0.0034168564710352154\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011723584622144699\n",
      "Average test loss: 0.0036197099309000706\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011726790450513364\n",
      "Average test loss: 0.003497772616955141\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01174124235080348\n",
      "Average test loss: 0.003723337292464243\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011734110839664936\n",
      "Average test loss: 0.003430058777746227\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01172616333183315\n",
      "Average test loss: 0.003475410137532486\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011722419382797347\n",
      "Average test loss: 0.0033929782451854813\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011715478668610255\n",
      "Average test loss: 0.0036544458388040463\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011715598536034426\n",
      "Average test loss: 0.0037378227884570757\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011701693506704436\n",
      "Average test loss: 0.003410441203870707\n",
      "Epoch 233/300\n",
      "Average training loss: 0.011706273403432634\n",
      "Average test loss: 0.0034950634779201614\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011685995815528764\n",
      "Average test loss: 0.003408321927405066\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011671163886785508\n",
      "Average test loss: 0.0034729571073419518\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011682339349554644\n",
      "Average test loss: 0.003556189250614908\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01167021027372943\n",
      "Average test loss: 0.0034683632337384755\n",
      "Epoch 238/300\n",
      "Average training loss: 0.011678538220624129\n",
      "Average test loss: 0.003469206748737229\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011655710659093326\n",
      "Average test loss: 0.003542831867312392\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011675799999799993\n",
      "Average test loss: 0.0034791873699675006\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01167384108983808\n",
      "Average test loss: 0.003738110435919629\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011654772275851832\n",
      "Average test loss: 0.00349181002833777\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011662298831674788\n",
      "Average test loss: 0.003510103165275521\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011627486204521524\n",
      "Average test loss: 0.0035388142340299156\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011662898594306574\n",
      "Average test loss: 0.003423652676244577\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01163514691674047\n",
      "Average test loss: 0.0034177580482016006\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011623580251302983\n",
      "Average test loss: 0.003571837128036552\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011621741165717443\n",
      "Average test loss: 0.0034889272399660615\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011619990699821049\n",
      "Average test loss: 0.003516279869195488\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011607063147756789\n",
      "Average test loss: 0.0036532121180660195\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011613306863440408\n",
      "Average test loss: 0.003584112639228503\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011622307154867385\n",
      "Average test loss: 0.003559925394753615\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011589668597612117\n",
      "Average test loss: 0.003697961368701524\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011586542062461376\n",
      "Average test loss: 0.0035175520645247564\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01161775887840324\n",
      "Average test loss: 0.003498738410572211\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01159482432405154\n",
      "Average test loss: 0.0036119319645480977\n",
      "Epoch 257/300\n",
      "Average training loss: 0.011588010242001878\n",
      "Average test loss: 0.0036122303333961303\n",
      "Epoch 258/300\n",
      "Average training loss: 0.011593276349206766\n",
      "Average test loss: 0.0034766259120984208\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011573011410733063\n",
      "Average test loss: 0.0035502040477262604\n",
      "Epoch 260/300\n",
      "Average training loss: 0.011569825656712055\n",
      "Average test loss: 0.0035931750977825786\n",
      "Epoch 261/300\n",
      "Average training loss: 0.011550118484430843\n",
      "Average test loss: 0.0035513818005306853\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011563346264262994\n",
      "Average test loss: 0.003505498293787241\n",
      "Epoch 263/300\n",
      "Average training loss: 0.011560805505348576\n",
      "Average test loss: 0.0036216853289968436\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011585210264556937\n",
      "Average test loss: 0.0035613668939719597\n",
      "Epoch 265/300\n",
      "Average training loss: 0.011563040315277046\n",
      "Average test loss: 0.0035719954423192473\n",
      "Epoch 266/300\n",
      "Average training loss: 0.011566592485540443\n",
      "Average test loss: 0.0036222018391514816\n",
      "Epoch 267/300\n",
      "Average training loss: 0.011534326725535922\n",
      "Average test loss: 0.003464233935293224\n",
      "Epoch 268/300\n",
      "Average training loss: 0.011531039607193734\n",
      "Average test loss: 0.003457013953891065\n",
      "Epoch 269/300\n",
      "Average training loss: 0.011538969645069706\n",
      "Average test loss: 0.003480828121304512\n",
      "Epoch 270/300\n",
      "Average training loss: 0.011539000165131357\n",
      "Average test loss: 0.003454067094872395\n",
      "Epoch 271/300\n",
      "Average training loss: 0.011520515901347\n",
      "Average test loss: 0.0034992717129902705\n",
      "Epoch 272/300\n",
      "Average training loss: 0.011517823138998615\n",
      "Average test loss: 0.0036412589634872146\n",
      "Epoch 273/300\n",
      "Average training loss: 0.011510551423662238\n",
      "Average test loss: 0.0034261890417999694\n",
      "Epoch 274/300\n",
      "Average training loss: 0.011519950870010587\n",
      "Average test loss: 0.0035471852299653822\n",
      "Epoch 275/300\n",
      "Average training loss: 0.011506847264866034\n",
      "Average test loss: 0.0035145391455541053\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01151466636442476\n",
      "Average test loss: 0.003521402499328057\n",
      "Epoch 277/300\n",
      "Average training loss: 0.011503576855692598\n",
      "Average test loss: 0.003479516626853082\n",
      "Epoch 278/300\n",
      "Average training loss: 0.011508797869086266\n",
      "Average test loss: 0.003454564782480399\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011508738917609056\n",
      "Average test loss: 0.0035992080751392577\n",
      "Epoch 280/300\n",
      "Average training loss: 0.011485383928649956\n",
      "Average test loss: 0.0035075423185610107\n",
      "Epoch 281/300\n",
      "Average training loss: 0.011494213464359443\n",
      "Average test loss: 0.003436932113021612\n",
      "Epoch 282/300\n",
      "Average training loss: 0.011503145216239824\n",
      "Average test loss: 0.0034734718994134\n",
      "Epoch 283/300\n",
      "Average training loss: 0.011492455151345995\n",
      "Average test loss: 0.003549847199064162\n",
      "Epoch 284/300\n",
      "Average training loss: 0.011492530839310752\n",
      "Average test loss: 0.003560928222619825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011476237343417274\n",
      "Average test loss: 0.003586763901842965\n",
      "Epoch 286/300\n",
      "Average training loss: 0.011466564540233877\n",
      "Average test loss: 0.003432755358310209\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011467167468534576\n",
      "Average test loss: 0.0033851089529279206\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011465003521906004\n",
      "Average test loss: 0.0035638913988239236\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011450639014442762\n",
      "Average test loss: 0.0034817584799602627\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011451789850162136\n",
      "Average test loss: 0.003523310607092248\n",
      "Epoch 291/300\n",
      "Average training loss: 0.011466955909298526\n",
      "Average test loss: 0.0033491327992329996\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011457040953967306\n",
      "Average test loss: 0.0035157174507362975\n",
      "Epoch 293/300\n",
      "Average training loss: 0.011454395889408059\n",
      "Average test loss: 0.0035887034231175978\n",
      "Epoch 294/300\n",
      "Average training loss: 0.011447742556532223\n",
      "Average test loss: 0.003579996625582377\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011435630020168093\n",
      "Average test loss: 0.0034574267893201774\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011451346283157667\n",
      "Average test loss: 0.003615169328533941\n",
      "Epoch 297/300\n",
      "Average training loss: 0.011432093086342018\n",
      "Average test loss: 0.0035309690290855037\n",
      "Epoch 298/300\n",
      "Average training loss: 0.011436464409033457\n",
      "Average test loss: 0.003474462060464753\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01144343339651823\n",
      "Average test loss: 0.003568684757997592\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011442325362728701\n",
      "Average test loss: 0.0035259998738765715\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10584744351771143\n",
      "Average test loss: 0.004429041863936517\n",
      "Epoch 2/300\n",
      "Average training loss: 0.024087590901388064\n",
      "Average test loss: 0.003624829769962364\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020336375991503397\n",
      "Average test loss: 0.0034543016377008625\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018933546404043832\n",
      "Average test loss: 0.0032827043823069994\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018156871484385596\n",
      "Average test loss: 0.0031780878762818045\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01763222777015633\n",
      "Average test loss: 0.003153832955078946\n",
      "Epoch 7/300\n",
      "Average training loss: 0.017216818602548704\n",
      "Average test loss: 0.003005577678895659\n",
      "Epoch 8/300\n",
      "Average training loss: 0.016870173615713913\n",
      "Average test loss: 0.002916864987048838\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015521523611413108\n",
      "Average test loss: 0.0027273080742193593\n",
      "Epoch 14/300\n",
      "Average training loss: 0.015287970332635773\n",
      "Average test loss: 0.0027017161844091284\n",
      "Epoch 15/300\n",
      "Average training loss: 0.015050908613536093\n",
      "Average test loss: 0.002671905213760005\n",
      "Epoch 16/300\n",
      "Average training loss: 0.014829699603219827\n",
      "Average test loss: 0.0026016683797869416\n",
      "Epoch 17/300\n",
      "Average training loss: 0.014623252211345566\n",
      "Average test loss: 0.0026178812865788738\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01442831187032991\n",
      "Average test loss: 0.0025471864452378617\n",
      "Epoch 19/300\n",
      "Average training loss: 0.014220996118254132\n",
      "Average test loss: 0.0025435193079627223\n",
      "Epoch 20/300\n",
      "Average training loss: 0.014050026059978538\n",
      "Average test loss: 0.0025983781839410466\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013867656307915846\n",
      "Average test loss: 0.0025480931521289878\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013694588065975241\n",
      "Average test loss: 0.0025707882052908343\n",
      "Epoch 23/300\n",
      "Average training loss: 0.013567907684793075\n",
      "Average test loss: 0.002801885157616602\n",
      "Epoch 24/300\n",
      "Average training loss: 0.013414342429074976\n",
      "Average test loss: 0.00246420681455897\n",
      "Epoch 25/300\n",
      "Average training loss: 0.013288281540075938\n",
      "Average test loss: 0.002431865770576729\n",
      "Epoch 26/300\n",
      "Average training loss: 0.013163465356661214\n",
      "Average test loss: 0.0024161300769903592\n",
      "Epoch 27/300\n",
      "Average training loss: 0.013054954105781185\n",
      "Average test loss: 0.0024123924002051353\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012943472919364771\n",
      "Average test loss: 0.002387269502091739\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012860380182663599\n",
      "Average test loss: 0.002382118454823891\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012751342676579952\n",
      "Average test loss: 0.0023741980019128983\n",
      "Epoch 31/300\n",
      "Average training loss: 0.012666217518349489\n",
      "Average test loss: 0.0023685097553663784\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012595408781535096\n",
      "Average test loss: 0.0024815090547005334\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012512438995142778\n",
      "Average test loss: 0.0024538687782155143\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012421680399941074\n",
      "Average test loss: 0.002347630233814319\n",
      "Epoch 35/300\n",
      "Average training loss: 0.012355303511023522\n",
      "Average test loss: 0.0023576414388500984\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012270916579498185\n",
      "Average test loss: 0.002354906815207667\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01222542696694533\n",
      "Average test loss: 0.0023455592182775337\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012140820290479396\n",
      "Average test loss: 0.0023639502870953745\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012067234290970697\n",
      "Average test loss: 0.0023406278007767267\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012008792302674718\n",
      "Average test loss: 0.0024536185692995787\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01199633604205317\n",
      "Average test loss: 0.0023970819012158447\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011617176550130049\n",
      "Average test loss: 0.0023562175089286434\n",
      "Epoch 48/300\n",
      "Average training loss: 0.011559801263941658\n",
      "Average test loss: 0.00232912428656386\n",
      "Epoch 49/300\n",
      "Average training loss: 0.011520400476124551\n",
      "Average test loss: 0.002360490340532528\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011464266816775004\n",
      "Average test loss: 0.002439213132692708\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01141445318940613\n",
      "Average test loss: 0.0023692009076476095\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011372847225103113\n",
      "Average test loss: 0.002325273373681638\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011320477971600162\n",
      "Average test loss: 0.0024159784744390185\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011269995705121093\n",
      "Average test loss: 0.0023580311466422344\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011226275137729115\n",
      "Average test loss: 0.0024090002555814055\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011203485533595086\n",
      "Average test loss: 0.0023882820434454415\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011133014452954133\n",
      "Average test loss: 0.002446406628108687\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011097984892626603\n",
      "Average test loss: 0.0023967294343229797\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01104374930428134\n",
      "Average test loss: 0.0023406029528834755\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010909692846238613\n",
      "Average test loss: 0.0023779712559448348\n",
      "Epoch 64/300\n",
      "Average training loss: 0.010870327156451013\n",
      "Average test loss: 0.002356967462433709\n",
      "Epoch 65/300\n",
      "Average training loss: 0.010850342133806812\n",
      "Average test loss: 0.0023966284011387165\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01079712095277177\n",
      "Average test loss: 0.002515858582117491\n",
      "Epoch 67/300\n",
      "Average training loss: 0.010770846753484673\n",
      "Average test loss: 0.0023695163175256715\n",
      "Epoch 68/300\n",
      "Average training loss: 0.010746402614646489\n",
      "Average test loss: 0.0025170941921985814\n",
      "Epoch 69/300\n",
      "Average training loss: 0.010698201898899343\n",
      "Average test loss: 0.0023783963169488644\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01068588001943297\n",
      "Average test loss: 0.002402211496606469\n",
      "Epoch 71/300\n",
      "Average training loss: 0.010668206066721015\n",
      "Average test loss: 0.002424982112314966\n",
      "Epoch 72/300\n",
      "Average training loss: 0.010616268874870407\n",
      "Average test loss: 0.00238299202442997\n",
      "Epoch 73/300\n",
      "Average training loss: 0.010594082074860732\n",
      "Average test loss: 0.0024669001715050803\n",
      "Epoch 74/300\n",
      "Average training loss: 0.010562946387463145\n",
      "Average test loss: 0.002457621487064494\n",
      "Epoch 75/300\n",
      "Average training loss: 0.010555214019285308\n",
      "Average test loss: 0.0025158230217380657\n",
      "Epoch 76/300\n",
      "Average training loss: 0.010515629924833774\n",
      "Average test loss: 0.0024259152752864692\n",
      "Epoch 77/300\n",
      "Average training loss: 0.010483382783830165\n",
      "Average test loss: 0.002367538710228271\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01044747418579128\n",
      "Average test loss: 0.002377800563039879\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01043315997802549\n",
      "Average test loss: 0.002420271375940906\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010421044668389692\n",
      "Average test loss: 0.002431081276593937\n",
      "Epoch 81/300\n",
      "Average training loss: 0.010391779865655634\n",
      "Average test loss: 0.002518040811229083\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010367324604756303\n",
      "Average test loss: 0.00251611314734651\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01034993762936857\n",
      "Average test loss: 0.0024472216638839906\n",
      "Epoch 84/300\n",
      "Average training loss: 0.010313153548373116\n",
      "Average test loss: 0.002470874268354641\n",
      "Epoch 85/300\n",
      "Average training loss: 0.010299181778397825\n",
      "Average test loss: 0.002412302308405439\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010274767085082001\n",
      "Average test loss: 0.002426586393059956\n",
      "Epoch 87/300\n",
      "Average training loss: 0.010277334597375658\n",
      "Average test loss: 0.0024052751983205476\n",
      "Epoch 88/300\n",
      "Average training loss: 0.010250058034641876\n",
      "Average test loss: 0.002403288901059164\n",
      "Epoch 89/300\n",
      "Average training loss: 0.010220081660482619\n",
      "Average test loss: 0.0024239297666483455\n",
      "Epoch 90/300\n",
      "Average training loss: 0.010199808043324285\n",
      "Average test loss: 0.0024897895283583138\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010183459617197514\n",
      "Average test loss: 0.002427472518860466\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01016140968683693\n",
      "Average test loss: 0.002546795212974151\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010146573511262734\n",
      "Average test loss: 0.0024890108371360435\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01014827327306072\n",
      "Average test loss: 0.0025188586600124836\n",
      "Epoch 95/300\n",
      "Average training loss: 0.010107888533837265\n",
      "Average test loss: 0.002464515638848146\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010102250943581264\n",
      "Average test loss: 0.002455358416462938\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010081704884767532\n",
      "Average test loss: 0.002482052884168095\n",
      "Epoch 98/300\n",
      "Average training loss: 0.010069393989112642\n",
      "Average test loss: 0.0025240793797290987\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010049902854694261\n",
      "Average test loss: 0.0025196091414739687\n",
      "Epoch 100/300\n",
      "Average training loss: 0.009972178555197187\n",
      "Average test loss: 0.002461894670294391\n",
      "Epoch 105/300\n",
      "Average training loss: 0.009945161865817176\n",
      "Average test loss: 0.002486249461563097\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009958116014798483\n",
      "Average test loss: 0.0024608837796582118\n",
      "Epoch 107/300\n",
      "Average training loss: 0.009936630981663863\n",
      "Average test loss: 0.002493208405147824\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00991837618003289\n",
      "Average test loss: 0.0025058530039257475\n",
      "Epoch 109/300\n",
      "Average training loss: 0.009899151175386376\n",
      "Average test loss: 0.0024725511226182184\n",
      "Epoch 110/300\n",
      "Average training loss: 0.009888914215068023\n",
      "Average test loss: 0.002472594118159678\n",
      "Epoch 111/300\n",
      "Average training loss: 0.009877650392552216\n",
      "Average test loss: 0.0025555938589904045\n",
      "Epoch 112/300\n",
      "Average training loss: 0.009850345415373643\n",
      "Average test loss: 0.002494666486047208\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009838736208776632\n",
      "Average test loss: 0.0025719223237699934\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00985068818844027\n",
      "Average test loss: 0.0024831442694283196\n",
      "Epoch 115/300\n",
      "Average training loss: 0.009835584920313624\n",
      "Average test loss: 0.002532566588682433\n",
      "Epoch 116/300\n",
      "Average training loss: 0.009828515658775966\n",
      "Average test loss: 0.0024837383389886883\n",
      "Epoch 117/300\n",
      "Average training loss: 0.009821232971217897\n",
      "Average test loss: 0.0024931750309964023\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00978792256116867\n",
      "Average test loss: 0.0025247535647617446\n",
      "Epoch 119/300\n",
      "Average training loss: 0.00978382441898187\n",
      "Average test loss: 0.0025464058514270515\n",
      "Epoch 120/300\n",
      "Average training loss: 0.009763367137147322\n",
      "Average test loss: 0.0024631863891457517\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009767406722737683\n",
      "Average test loss: 0.0025439943437361055\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00974396251887083\n",
      "Average test loss: 0.0025389117409164706\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009742965998748939\n",
      "Average test loss: 0.0025056262132194308\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009717365368372864\n",
      "Average test loss: 0.0026453429558831783\n",
      "Epoch 125/300\n",
      "Average training loss: 0.009706683182054096\n",
      "Average test loss: 0.002537785559478733\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009719700558318032\n",
      "Average test loss: 0.002665243276601864\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00968235175891055\n",
      "Average test loss: 0.0024998279373264977\n",
      "Epoch 128/300\n",
      "Average training loss: 0.009664752724270026\n",
      "Average test loss: 0.0025458062450504967\n",
      "Epoch 132/300\n",
      "Average training loss: 0.009646449695030848\n",
      "Average test loss: 0.0025646103219025664\n",
      "Epoch 133/300\n",
      "Average training loss: 0.00964308481083976\n",
      "Average test loss: 0.002601441725364162\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009622686902268066\n",
      "Average test loss: 0.002600042467522952\n",
      "Epoch 135/300\n",
      "Average training loss: 0.00962134722289112\n",
      "Average test loss: 0.002495492237516575\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009597461227741506\n",
      "Average test loss: 0.0025012864135205747\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00960817547639211\n",
      "Average test loss: 0.0025900257523689004\n",
      "Epoch 138/300\n",
      "Average training loss: 0.009612612242499987\n",
      "Average test loss: 0.0025834366641938685\n",
      "Epoch 139/300\n",
      "Average training loss: 0.009589080159034993\n",
      "Average test loss: 0.002585802043063773\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009568057549496491\n",
      "Average test loss: 0.0026073867640354566\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009566640041768552\n",
      "Average test loss: 0.0026236304367582004\n",
      "Epoch 142/300\n",
      "Average training loss: 0.009572014121545685\n",
      "Average test loss: 0.002551499981019232\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00956051038702329\n",
      "Average test loss: 0.002483136156780852\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009546959723863336\n",
      "Average test loss: 0.0025834654946294095\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009522703845467832\n",
      "Average test loss: 0.002567819901638561\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009533466137945652\n",
      "Average test loss: 0.0025355947452286877\n",
      "Epoch 147/300\n",
      "Average training loss: 0.009511764397223791\n",
      "Average test loss: 0.002605050492617819\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009501478112406201\n",
      "Average test loss: 0.0026061523347679113\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009514067043032912\n",
      "Average test loss: 0.002522872677693764\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00951978010353115\n",
      "Average test loss: 0.002510054998927646\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009480513580971295\n",
      "Average test loss: 0.002676661602738831\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009490968676904837\n",
      "Average test loss: 0.0024887865831454596\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009475347774724165\n",
      "Average test loss: 0.0026031020825935734\n",
      "Epoch 154/300\n",
      "Average training loss: 0.00947530975441138\n",
      "Average test loss: 0.0027457521276341543\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009465712891684639\n",
      "Average test loss: 0.002516925447723932\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00946613133036428\n",
      "Average test loss: 0.002654707916908794\n",
      "Epoch 157/300\n",
      "Average training loss: 0.009445692636072636\n",
      "Average test loss: 0.002554330354349481\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00944285015637676\n",
      "Average test loss: 0.002674873351222939\n",
      "Epoch 159/300\n",
      "Average training loss: 0.009420631227393944\n",
      "Average test loss: 0.002518850840628147\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009441166603730785\n",
      "Average test loss: 0.0027006307993498115\n",
      "Epoch 161/300\n",
      "Average training loss: 0.00941803318551845\n",
      "Average test loss: 0.0026765070439626775\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009408445353309314\n",
      "Average test loss: 0.002513617524256309\n",
      "Epoch 163/300\n",
      "Average training loss: 0.009409639487663905\n",
      "Average test loss: 0.0026873310623276563\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00940214468538761\n",
      "Average test loss: 0.002616210110899475\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009381029010232952\n",
      "Average test loss: 0.0026866335559429396\n",
      "Epoch 169/300\n",
      "Average training loss: 0.009377828871210416\n",
      "Average test loss: 0.002535460838634107\n",
      "Epoch 170/300\n",
      "Average training loss: 0.009357608416014247\n",
      "Average test loss: 0.00256298897208439\n",
      "Epoch 171/300\n",
      "Average training loss: 0.009354697310262255\n",
      "Average test loss: 0.002614223614955942\n",
      "Epoch 172/300\n",
      "Average training loss: 0.009354695773786968\n",
      "Average test loss: 0.002534840158497294\n",
      "Epoch 173/300\n",
      "Average training loss: 0.009336416252785258\n",
      "Average test loss: 0.0025683007964657414\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00934121139595906\n",
      "Average test loss: 0.0025775199751887057\n",
      "Epoch 175/300\n",
      "Average training loss: 0.009322960747612847\n",
      "Average test loss: 0.0026646803131120072\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009340595957305697\n",
      "Average test loss: 0.002662129106000066\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009332101246549023\n",
      "Average test loss: 0.002588792789313528\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009334695407085949\n",
      "Average test loss: 0.0027485837758415275\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009308218234115177\n",
      "Average test loss: 0.0026014847996541197\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009309182916664415\n",
      "Average test loss: 0.0026168330574615132\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00930941443807549\n",
      "Average test loss: 0.002564872395661142\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009306089863181113\n",
      "Average test loss: 0.0027006119655238257\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009307893997265233\n",
      "Average test loss: 0.002638864775705669\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00927394263404939\n",
      "Average test loss: 0.0026820251784390875\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009286635885636012\n",
      "Average test loss: 0.0025431981007051135\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009278146862155862\n",
      "Average test loss: 0.002644241320176257\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009289850817579362\n",
      "Average test loss: 0.002717156955351432\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009261567007336352\n",
      "Average test loss: 0.0026354342443454595\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009267669107351039\n",
      "Average test loss: 0.0027507907127340635\n",
      "Epoch 190/300\n",
      "Average training loss: 0.009261825001074208\n",
      "Average test loss: 0.002597140012308955\n",
      "Epoch 191/300\n",
      "Average training loss: 0.00924513264828258\n",
      "Average test loss: 0.002633948120805952\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009258172863887417\n",
      "Average test loss: 0.0026294229537662532\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009235400716463725\n",
      "Average test loss: 0.002561975890977515\n",
      "Epoch 194/300\n",
      "Average training loss: 0.009245060148338476\n",
      "Average test loss: 0.0026651988370964925\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009230297457012865\n",
      "Average test loss: 0.0026127920359787014\n",
      "Epoch 196/300\n",
      "Average training loss: 0.009228879972464509\n",
      "Average test loss: 0.002563082723774844\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009232564116517702\n",
      "Average test loss: 0.002742986430310541\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009208435276316271\n",
      "Average test loss: 0.002594884858570165\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009202885038322872\n",
      "Average test loss: 0.0026676222967604796\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00921275992029243\n",
      "Average test loss: 0.0026638324489403104\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009205976820654339\n",
      "Average test loss: 0.0025964930786026848\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00919939717484845\n",
      "Average test loss: 0.0026000413544889954\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009193575250605742\n",
      "Average test loss: 0.0026322299185105496\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009200857385993005\n",
      "Average test loss: 0.0026504749529477623\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009189081395665804\n",
      "Average test loss: 0.00272157412312097\n",
      "Epoch 206/300\n",
      "Average training loss: 0.009188826687633992\n",
      "Average test loss: 0.002564880514724387\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009188027358717389\n",
      "Average test loss: 0.0026954253781586884\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009159877600769202\n",
      "Average test loss: 0.0026805066528419654\n",
      "Epoch 209/300\n",
      "Average training loss: 0.00917300814224614\n",
      "Average test loss: 0.0026292208873977263\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009162548547817602\n",
      "Average test loss: 0.0025879490048521096\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009159293554723263\n",
      "Average test loss: 0.002534977183987697\n",
      "Epoch 212/300\n",
      "Average training loss: 0.009161213575137985\n",
      "Average test loss: 0.002688902080886894\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00914494125213888\n",
      "Average test loss: 0.0025766811764074697\n",
      "Epoch 214/300\n",
      "Average training loss: 0.009164128279106485\n",
      "Average test loss: 0.002684130525837342\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009152828945881791\n",
      "Average test loss: 0.00266432165602843\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009136925941540135\n",
      "Average test loss: 0.0026942556467321184\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009134174642463525\n",
      "Average test loss: 0.00270386231256028\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009140192934208447\n",
      "Average test loss: 0.002622085020980901\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009122778513365322\n",
      "Average test loss: 0.0027331072048594556\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009132233976489967\n",
      "Average test loss: 0.002708495984474818\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009120432306494978\n",
      "Average test loss: 0.0025827958995683327\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009117733423908552\n",
      "Average test loss: 0.0027619971181783412\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009113871316942903\n",
      "Average test loss: 0.002630203575723701\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00911360214319494\n",
      "Average test loss: 0.002740600193126334\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009108512996385494\n",
      "Average test loss: 0.0026959341038018465\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009105913565390639\n",
      "Average test loss: 0.002615827459635006\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009097447700798511\n",
      "Average test loss: 0.0027532589706695744\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00908259864565399\n",
      "Average test loss: 0.002683478929309381\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009093458962937196\n",
      "Average test loss: 0.00262216554582119\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009094767332904869\n",
      "Average test loss: 0.00263591119978163\n",
      "Epoch 231/300\n",
      "Average training loss: 0.009086393194893997\n",
      "Average test loss: 0.002711783768609166\n",
      "Epoch 232/300\n",
      "Average training loss: 0.009078855682578352\n",
      "Average test loss: 0.0025809266372687286\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00907991534885433\n",
      "Average test loss: 0.0026710542268637153\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009066056400537491\n",
      "Average test loss: 0.0026848217673185798\n",
      "Epoch 235/300\n",
      "Average training loss: 0.009074127634366354\n",
      "Average test loss: 0.0026306852392024463\n",
      "Epoch 236/300\n",
      "Average training loss: 0.009063825560112794\n",
      "Average test loss: 0.002583354657722844\n",
      "Epoch 237/300\n",
      "Average training loss: 0.009068614305721388\n",
      "Average test loss: 0.002707977800733513\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009053784268597763\n",
      "Average test loss: 0.0026386167380130954\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00906214333160056\n",
      "Average test loss: 0.0026403882840855255\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009041809991829925\n",
      "Average test loss: 0.002676480252916614\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00905000520331992\n",
      "Average test loss: 0.0027058366826838917\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009040880178411802\n",
      "Average test loss: 0.002676881389071544\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00903716205060482\n",
      "Average test loss: 0.0026285459926972785\n",
      "Epoch 244/300\n",
      "Average training loss: 0.009035550130738153\n",
      "Average test loss: 0.0026993377699206273\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009036236451731787\n",
      "Average test loss: 0.0027176983426842423\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00902791090019875\n",
      "Average test loss: 0.002729995443796118\n",
      "Epoch 247/300\n",
      "Average training loss: 0.00902070446146859\n",
      "Average test loss: 0.0025992590859532354\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009037228831814395\n",
      "Average test loss: 0.0026975603426496186\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009024889493981997\n",
      "Average test loss: 0.002689272185580598\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009013785421848297\n",
      "Average test loss: 0.0026588242770069176\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009017283945447869\n",
      "Average test loss: 0.00269748920119471\n",
      "Epoch 252/300\n",
      "Average training loss: 0.00900419347981612\n",
      "Average test loss: 0.002695501371804211\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00902109478496843\n",
      "Average test loss: 0.002616830200991697\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008993012700643805\n",
      "Average test loss: 0.0027177176268564332\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008998786733382278\n",
      "Average test loss: 0.002736624915152788\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008996016315288013\n",
      "Average test loss: 0.0027371116814514\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008998645482791794\n",
      "Average test loss: 0.002644214929598901\n",
      "Epoch 258/300\n",
      "Average training loss: 0.009005373235378\n",
      "Average test loss: 0.0027119388449937105\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008988834271828334\n",
      "Average test loss: 0.0026891085929754708\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008990693498402834\n",
      "Average test loss: 0.0027572121880948546\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008985943933741913\n",
      "Average test loss: 0.0026901383900807966\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008979131095939213\n",
      "Average test loss: 0.0026777630891236994\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008986111886799335\n",
      "Average test loss: 0.002668070688015885\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00897120362934139\n",
      "Average test loss: 0.0027202768017434414\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008976666888429059\n",
      "Average test loss: 0.002642897354852822\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008954389977786276\n",
      "Average test loss: 0.002743740897004803\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008976357095357445\n",
      "Average test loss: 0.0027492827392286727\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008969673799971739\n",
      "Average test loss: 0.0027495862255907722\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008940925216509236\n",
      "Average test loss: 0.0026268187976545756\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008976028602156374\n",
      "Average test loss: 0.002661633249786165\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008953895380927456\n",
      "Average test loss: 0.002608838553023007\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008952585943043233\n",
      "Average test loss: 0.002639077265643411\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008961639758613374\n",
      "Average test loss: 0.002699218392579092\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008960200038221146\n",
      "Average test loss: 0.0027727967208872237\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008937548168831402\n",
      "Average test loss: 0.0026247938567151626\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008931182995438576\n",
      "Average test loss: 0.0027020901263588005\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008926364356444942\n",
      "Average test loss: 0.002611546346503827\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008934863371981515\n",
      "Average test loss: 0.0026254803289969763\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008936194532861312\n",
      "Average test loss: 0.0027680003121495246\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008939160384237766\n",
      "Average test loss: 0.002667598992586136\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008926971970746914\n",
      "Average test loss: 0.0027034396881030665\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008935248401429919\n",
      "Average test loss: 0.002683758843276236\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008913659936024084\n",
      "Average test loss: 0.0026109711815499597\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008902976450820764\n",
      "Average test loss: 0.0026563902468317085\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008921164909170734\n",
      "Average test loss: 0.002690307347724835\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008922753809226884\n",
      "Average test loss: 0.002770578780108028\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008915894662340482\n",
      "Average test loss: 0.002631310194730759\n",
      "Epoch 288/300\n",
      "Average training loss: 0.00891174949788385\n",
      "Average test loss: 0.002696330935590797\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008905113841924403\n",
      "Average test loss: 0.0026469464980893665\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00891040391392178\n",
      "Average test loss: 0.0027205969225615263\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008896164019074705\n",
      "Average test loss: 0.002755339289911919\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008896736582120259\n",
      "Average test loss: 0.002657216768608325\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008895122750765748\n",
      "Average test loss: 0.0027728703969882593\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008894261383348041\n",
      "Average test loss: 0.002683646203432646\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008892351429495546\n",
      "Average test loss: 0.002660409708507359\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008889355420652363\n",
      "Average test loss: 0.002820779286738899\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008888118695881632\n",
      "Average test loss: 0.002733749796325962\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008891791306436062\n",
      "Average test loss: 0.002707863185968664\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008873243057893382\n",
      "Average test loss: 0.002698605260294345\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008871317884160413\n",
      "Average test loss: 0.002752107782703307\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10792669556041559\n",
      "Average test loss: 0.0037667055419749683\n",
      "Epoch 2/300\n",
      "Average training loss: 0.021825759278403387\n",
      "Average test loss: 0.003257066843410333\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017616217570172416\n",
      "Average test loss: 0.0028282472292582195\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01610724435415533\n",
      "Average test loss: 0.0026680730728225574\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015311381987399526\n",
      "Average test loss: 0.002624494711971945\n",
      "Epoch 6/300\n",
      "Average training loss: 0.014767142618695895\n",
      "Average test loss: 0.00251401887078666\n",
      "Epoch 7/300\n",
      "Average training loss: 0.014358022460507021\n",
      "Average test loss: 0.002387901015890141\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014000486335820623\n",
      "Average test loss: 0.0023740049137009516\n",
      "Epoch 9/300\n",
      "Average training loss: 0.013672506334053145\n",
      "Average test loss: 0.0022496848413720728\n",
      "Epoch 10/300\n",
      "Average training loss: 0.013392314777606064\n",
      "Average test loss: 0.002225469695404172\n",
      "Epoch 11/300\n",
      "Average training loss: 0.013116992849442694\n",
      "Average test loss: 0.00216926959860656\n",
      "Epoch 12/300\n",
      "Average training loss: 0.012868321056995128\n",
      "Average test loss: 0.002151448454811341\n",
      "Epoch 13/300\n",
      "Average training loss: 0.012622524864143796\n",
      "Average test loss: 0.002163852289525999\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012387305633889305\n",
      "Average test loss: 0.0020443069622334505\n",
      "Epoch 15/300\n",
      "Average training loss: 0.012156682868798573\n",
      "Average test loss: 0.002023804356654485\n",
      "Epoch 16/300\n",
      "Average training loss: 0.011927919015288353\n",
      "Average test loss: 0.0019710853501326508\n",
      "Epoch 17/300\n",
      "Average training loss: 0.011721838551263014\n",
      "Average test loss: 0.002348304337925381\n",
      "Epoch 18/300\n",
      "Average training loss: 0.011526166309499078\n",
      "Average test loss: 0.001960661305114627\n",
      "Epoch 19/300\n",
      "Average training loss: 0.011329206872731448\n",
      "Average test loss: 0.001906772307948106\n",
      "Epoch 20/300\n",
      "Average training loss: 0.011130009905331665\n",
      "Average test loss: 0.0019109207588351436\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010961454201489687\n",
      "Average test loss: 0.0019888505031251244\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010792910608152548\n",
      "Average test loss: 0.00185698746662173\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01065144393261936\n",
      "Average test loss: 0.0019301636073117455\n",
      "Epoch 24/300\n",
      "Average training loss: 0.010490387045674853\n",
      "Average test loss: 0.0018147852980635233\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01037578504698144\n",
      "Average test loss: 0.0018454767795693544\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010254955329828792\n",
      "Average test loss: 0.0017973134228442278\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010147373966872692\n",
      "Average test loss: 0.0017814447985341152\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010045507815149096\n",
      "Average test loss: 0.001786098862066865\n",
      "Epoch 29/300\n",
      "Average training loss: 0.009961009424593714\n",
      "Average test loss: 0.0017865919894021417\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009882573842588399\n",
      "Average test loss: 0.0017786251625253094\n",
      "Epoch 31/300\n",
      "Average training loss: 0.009794622708525923\n",
      "Average test loss: 0.0017592744868662623\n",
      "Epoch 32/300\n",
      "Average training loss: 0.009722462977800104\n",
      "Average test loss: 0.0017344054322586291\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009658734324491686\n",
      "Average test loss: 0.001732651308696303\n",
      "Epoch 34/300\n",
      "Average training loss: 0.009600942393143972\n",
      "Average test loss: 0.001764641836906473\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009545093645652135\n",
      "Average test loss: 0.0017495183164460792\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009473982268737422\n",
      "Average test loss: 0.0017235605424890915\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009421795007255342\n",
      "Average test loss: 0.0017529026116761897\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009376124920944372\n",
      "Average test loss: 0.0017978696109106143\n",
      "Epoch 39/300\n",
      "Average training loss: 0.009317819522486793\n",
      "Average test loss: 0.0017287231826533874\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009274667176935408\n",
      "Average test loss: 0.001748968521443506\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009266339491638872\n",
      "Average test loss: 0.0019336748702141146\n",
      "Epoch 42/300\n",
      "Average training loss: 0.009173480454418395\n",
      "Average test loss: 0.0017281423474972447\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009150520659155316\n",
      "Average test loss: 0.0017645714830400215\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009101839436011182\n",
      "Average test loss: 0.001726212359757887\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009049089448319541\n",
      "Average test loss: 0.0017504938619418277\n",
      "Epoch 46/300\n",
      "Average training loss: 0.008995459927866857\n",
      "Average test loss: 0.0017279147027681272\n",
      "Epoch 47/300\n",
      "Average training loss: 0.008967729214164945\n",
      "Average test loss: 0.0017100907003300058\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008946331560197803\n",
      "Average test loss: 0.001723138781471385\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008898659194095267\n",
      "Average test loss: 0.0017210256373509764\n",
      "Epoch 50/300\n",
      "Average training loss: 0.00886123924040132\n",
      "Average test loss: 0.0017651436221268442\n",
      "Epoch 51/300\n",
      "Average training loss: 0.008821915477100345\n",
      "Average test loss: 0.0017821804064636429\n",
      "Epoch 52/300\n",
      "Average training loss: 0.008786095349738995\n",
      "Average test loss: 0.0017199362158361409\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00875028079913722\n",
      "Average test loss: 0.0017179550766530964\n",
      "Epoch 54/300\n",
      "Average training loss: 0.008711363312270907\n",
      "Average test loss: 0.0017240296776096025\n",
      "Epoch 55/300\n",
      "Average training loss: 0.008680998590257432\n",
      "Average test loss: 0.0017174647984405358\n",
      "Epoch 56/300\n",
      "Average training loss: 0.008653241814010674\n",
      "Average test loss: 0.0017301706109816828\n",
      "Epoch 57/300\n",
      "Average training loss: 0.008618784893718031\n",
      "Average test loss: 0.0017561116064381268\n",
      "Epoch 58/300\n",
      "Average training loss: 0.008589463423937559\n",
      "Average test loss: 0.0017389468219545152\n",
      "Epoch 59/300\n",
      "Average training loss: 0.008548375849922497\n",
      "Average test loss: 0.001785382182751265\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008531487436758148\n",
      "Average test loss: 0.0017167901272057658\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00850366810709238\n",
      "Average test loss: 0.0019922989596509273\n",
      "Epoch 62/300\n",
      "Average training loss: 0.00848038964966933\n",
      "Average test loss: 0.001740483157336712\n",
      "Epoch 63/300\n",
      "Average training loss: 0.008442087358898587\n",
      "Average test loss: 0.0017349825973829462\n",
      "Epoch 64/300\n",
      "Average training loss: 0.008416890171253018\n",
      "Average test loss: 0.0017758096374778285\n",
      "Epoch 65/300\n",
      "Average training loss: 0.008395366397168902\n",
      "Average test loss: 0.0017421457823365927\n",
      "Epoch 66/300\n",
      "Average training loss: 0.008367663569748402\n",
      "Average test loss: 0.001778554230928421\n",
      "Epoch 67/300\n",
      "Average training loss: 0.008331994300915135\n",
      "Average test loss: 0.0017374104716711574\n",
      "Epoch 68/300\n",
      "Average training loss: 0.00831608667059077\n",
      "Average test loss: 0.0017639884156071478\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008300502268390523\n",
      "Average test loss: 0.0017485281207288304\n",
      "Epoch 70/300\n",
      "Average training loss: 0.008263600862688488\n",
      "Average test loss: 0.0018040043579207526\n",
      "Epoch 71/300\n",
      "Average training loss: 0.008239938852687676\n",
      "Average test loss: 0.0017430110046019157\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00822696430153317\n",
      "Average test loss: 0.0017556368232601218\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008214163663486639\n",
      "Average test loss: 0.0017710443728913864\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008184189168115457\n",
      "Average test loss: 0.0017830964770788947\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008153296825786431\n",
      "Average test loss: 0.00178573102514363\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008161071818735864\n",
      "Average test loss: 0.001776802385950254\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008140198272549444\n",
      "Average test loss: 0.0017713237234080832\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008105813504093224\n",
      "Average test loss: 0.001845542917545471\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008087725236184068\n",
      "Average test loss: 0.001814385421367155\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0080529131649269\n",
      "Average test loss: 0.0018313983375620512\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008055698059499263\n",
      "Average test loss: 0.0018225912171312504\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008019178375601769\n",
      "Average test loss: 0.0017794986441731454\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00801632141901387\n",
      "Average test loss: 0.0018108350694593456\n",
      "Epoch 84/300\n",
      "Average training loss: 0.007999671284937197\n",
      "Average test loss: 0.0019279054641309712\n",
      "Epoch 85/300\n",
      "Average training loss: 0.007990467020206981\n",
      "Average test loss: 0.0018137422695548998\n",
      "Epoch 86/300\n",
      "Average training loss: 0.007952205719219313\n",
      "Average test loss: 0.001787445003580716\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007944526332120101\n",
      "Average test loss: 0.001867807412520051\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007934553778833814\n",
      "Average test loss: 0.0018885980168771412\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007930597975850105\n",
      "Average test loss: 0.0017999299147890674\n",
      "Epoch 90/300\n",
      "Average training loss: 0.00789672372986873\n",
      "Average test loss: 0.0017924347022134396\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00788184991810057\n",
      "Average test loss: 0.0017660081312060355\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007873119274775187\n",
      "Average test loss: 0.0017784846546128393\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007894529768990145\n",
      "Average test loss: 0.0018059186326960723\n",
      "Epoch 94/300\n",
      "Average training loss: 0.00784665776665012\n",
      "Average test loss: 0.0018641137758062946\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007835168291297224\n",
      "Average test loss: 0.0018827709634270933\n",
      "Epoch 96/300\n",
      "Average training loss: 0.007821440541081958\n",
      "Average test loss: 0.0017868971481091447\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007810548213620981\n",
      "Average test loss: 0.0018769688382744788\n",
      "Epoch 98/300\n",
      "Average training loss: 0.007799316712551647\n",
      "Average test loss: 0.0018510181406098936\n",
      "Epoch 99/300\n",
      "Average training loss: 0.007776090088817808\n",
      "Average test loss: 0.001786066396575835\n",
      "Epoch 100/300\n",
      "Average training loss: 0.007759092583838436\n",
      "Average test loss: 0.0018370191777745883\n",
      "Epoch 101/300\n",
      "Average training loss: 0.007764967611266507\n",
      "Average test loss: 0.0018147426111002763\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007760521394097142\n",
      "Average test loss: 0.0018740276690158578\n",
      "Epoch 103/300\n",
      "Average training loss: 0.007713790148496628\n",
      "Average test loss: 0.0017950933147221804\n",
      "Epoch 104/300\n",
      "Average training loss: 0.007729010690417554\n",
      "Average test loss: 0.0018091936220104496\n",
      "Epoch 105/300\n",
      "Average training loss: 0.007706309299502108\n",
      "Average test loss: 0.0018924343254831102\n",
      "Epoch 106/300\n",
      "Average training loss: 0.007683502149664693\n",
      "Average test loss: 0.0018188449039848315\n",
      "Epoch 107/300\n",
      "Average training loss: 0.007694166262944539\n",
      "Average test loss: 0.00186752112965203\n",
      "Epoch 108/300\n",
      "Average training loss: 0.007677877123157184\n",
      "Average test loss: 0.0019050191656375924\n",
      "Epoch 109/300\n",
      "Average training loss: 0.007668402572472891\n",
      "Average test loss: 0.0018312516756769683\n",
      "Epoch 110/300\n",
      "Average training loss: 0.007650376334372494\n",
      "Average test loss: 0.0018972257608547807\n",
      "Epoch 111/300\n",
      "Average training loss: 0.007663954811791579\n",
      "Average test loss: 0.001861800061331855\n",
      "Epoch 112/300\n",
      "Average training loss: 0.007633568266613616\n",
      "Average test loss: 0.0018805729085579514\n",
      "Epoch 113/300\n",
      "Average training loss: 0.007624255466792318\n",
      "Average test loss: 0.001869875012896955\n",
      "Epoch 114/300\n",
      "Average training loss: 0.007610321699331204\n",
      "Average test loss: 0.0019217026330944564\n",
      "Epoch 115/300\n",
      "Average training loss: 0.007607979529433781\n",
      "Average test loss: 0.0018361262052009504\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0076078128524952465\n",
      "Average test loss: 0.0018741220473829244\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0075902955395479995\n",
      "Average test loss: 0.001864379680612021\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007570932918952571\n",
      "Average test loss: 0.0018575955279585387\n",
      "Epoch 119/300\n",
      "Average training loss: 0.007563860412687063\n",
      "Average test loss: 0.0019604023912300666\n",
      "Epoch 120/300\n",
      "Average training loss: 0.007566639912625154\n",
      "Average test loss: 0.0018217165590160424\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007559696776585446\n",
      "Average test loss: 0.0018747059887068139\n",
      "Epoch 122/300\n",
      "Average training loss: 0.007546998863832818\n",
      "Average test loss: 0.0018830956946023637\n",
      "Epoch 123/300\n",
      "Average training loss: 0.007543859307550722\n",
      "Average test loss: 0.0018710998574064837\n",
      "Epoch 124/300\n",
      "Average training loss: 0.007532354780369335\n",
      "Average test loss: 0.0019182518350167406\n",
      "Epoch 125/300\n",
      "Average training loss: 0.007529706552210781\n",
      "Average test loss: 0.0018552155738903416\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0075121105263630545\n",
      "Average test loss: 0.0019208639232027862\n",
      "Epoch 127/300\n",
      "Average training loss: 0.007504299586845769\n",
      "Average test loss: 0.0018754817911734185\n",
      "Epoch 128/300\n",
      "Average training loss: 0.00749065735977557\n",
      "Average test loss: 0.0018557734402517478\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007495966642681096\n",
      "Average test loss: 0.001895909539423883\n",
      "Epoch 130/300\n",
      "Average training loss: 0.007492118775844574\n",
      "Average test loss: 0.0018849365690516101\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007476631633109516\n",
      "Average test loss: 0.0018572436173756918\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007475595874918832\n",
      "Average test loss: 0.0018559175668698218\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007462294087641769\n",
      "Average test loss: 0.001863821827289131\n",
      "Epoch 134/300\n",
      "Average training loss: 0.007446749318805006\n",
      "Average test loss: 0.0018882116135209798\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007460240047838953\n",
      "Average test loss: 0.001869468765747216\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007439058967762523\n",
      "Average test loss: 0.0018701134148157304\n",
      "Epoch 137/300\n",
      "Average training loss: 0.007445536190436946\n",
      "Average test loss: 0.0019533597399584123\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0074353044999556405\n",
      "Average test loss: 0.0019990814080875783\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007417134479930003\n",
      "Average test loss: 0.00194457570484115\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007410446175684531\n",
      "Average test loss: 0.0019159960890602734\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0074146936155027815\n",
      "Average test loss: 0.0019034352277716002\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007412536595430639\n",
      "Average test loss: 0.001933871055021882\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0073931532613933084\n",
      "Average test loss: 0.001934654138982296\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0073878617891007\n",
      "Average test loss: 0.0019601418545676604\n",
      "Epoch 145/300\n",
      "Average training loss: 0.007385143272578716\n",
      "Average test loss: 0.0018575643700444037\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007388178961144553\n",
      "Average test loss: 0.0018912818922350804\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007376537799007363\n",
      "Average test loss: 0.0018727056880791981\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007369313435008128\n",
      "Average test loss: 0.0018783612828701735\n",
      "Epoch 149/300\n",
      "Average training loss: 0.007360612316264047\n",
      "Average test loss: 0.001853770110445718\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007358271448148621\n",
      "Average test loss: 0.0018921388052403926\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0073468094103866155\n",
      "Average test loss: 0.0019506683808027043\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007345383389956421\n",
      "Average test loss: 0.0019199778456240892\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0073291764499412645\n",
      "Average test loss: 0.001953944127385815\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007347959075123072\n",
      "Average test loss: 0.0019480237062606547\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007326412280400594\n",
      "Average test loss: 0.0018865887239161465\n",
      "Epoch 156/300\n",
      "Average training loss: 0.007322816083828608\n",
      "Average test loss: 0.0019277519365358683\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0073173579689529205\n",
      "Average test loss: 0.0018779821461066605\n",
      "Epoch 158/300\n",
      "Average training loss: 0.007304910858886109\n",
      "Average test loss: 0.002001734885283642\n",
      "Epoch 159/300\n",
      "Average training loss: 0.007314272948437267\n",
      "Average test loss: 0.0019171856394451525\n",
      "Epoch 160/300\n",
      "Average training loss: 0.007302906807098124\n",
      "Average test loss: 0.001932282229885459\n",
      "Epoch 161/300\n",
      "Average training loss: 0.007298757257560889\n",
      "Average test loss: 0.0019290325471924411\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0072947292261653475\n",
      "Average test loss: 0.0018907981244329778\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0072957833814952106\n",
      "Average test loss: 0.0019127901963268718\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0072707784312466784\n",
      "Average test loss: 0.0018968861972292264\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0072772891235848266\n",
      "Average test loss: 0.0019820658916400538\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0072722892976469465\n",
      "Average test loss: 0.001907153882396718\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0072710500574774215\n",
      "Average test loss: 0.0018791609153979353\n",
      "Epoch 168/300\n",
      "Average training loss: 0.007260785131404797\n",
      "Average test loss: 0.0020264993272721766\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0072740303451816245\n",
      "Average test loss: 0.0019459036470701298\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007250380355450842\n",
      "Average test loss: 0.0019145604121602244\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0072472858337892425\n",
      "Average test loss: 0.001965671873651445\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0072417930646075145\n",
      "Average test loss: 0.001974369090671341\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0072403960675001145\n",
      "Average test loss: 0.0019583885425494777\n",
      "Epoch 174/300\n",
      "Average training loss: 0.007235912867304351\n",
      "Average test loss: 0.001933905391022563\n",
      "Epoch 175/300\n",
      "Average training loss: 0.007230513547443682\n",
      "Average test loss: 0.002024701854007112\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007225330926063988\n",
      "Average test loss: 0.0018929014065199427\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0072288216696017316\n",
      "Average test loss: 0.001896750959981647\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007212577398038573\n",
      "Average test loss: 0.0019358504565639629\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007207974892523554\n",
      "Average test loss: 0.0019182113914026154\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0072167243158651724\n",
      "Average test loss: 0.0019416987389429576\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007208000456293424\n",
      "Average test loss: 0.0018948943275544378\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007197213412986861\n",
      "Average test loss: 0.0018901417021536164\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007200827447076639\n",
      "Average test loss: 0.0019011992351669405\n",
      "Epoch 184/300\n",
      "Average training loss: 0.007200331557542085\n",
      "Average test loss: 0.0019371179182910256\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007198977428177993\n",
      "Average test loss: 0.001972728605175184\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0071771450179318585\n",
      "Average test loss: 0.0019625469798015224\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007176447026431561\n",
      "Average test loss: 0.001918285521471666\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0071679740258389045\n",
      "Average test loss: 0.0018996869416700469\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00718574150154988\n",
      "Average test loss: 0.0019128237862346901\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007172734614047739\n",
      "Average test loss: 0.0019828635192372735\n",
      "Epoch 191/300\n",
      "Average training loss: 0.007171167158832152\n",
      "Average test loss: 0.0019361831804530487\n",
      "Epoch 192/300\n",
      "Average training loss: 0.007174942372573746\n",
      "Average test loss: 0.0018841642248961662\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007163483074969715\n",
      "Average test loss: 0.0019510199524876144\n",
      "Epoch 194/300\n",
      "Average training loss: 0.007158593431942993\n",
      "Average test loss: 0.0020102226035669446\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0071620671658052335\n",
      "Average test loss: 0.0019186203927836485\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0071523238888217345\n",
      "Average test loss: 0.0019694355068107447\n",
      "Epoch 197/300\n",
      "Average training loss: 0.007154524199250671\n",
      "Average test loss: 0.0019335672672217091\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007134208083152771\n",
      "Average test loss: 0.0019081774820677109\n",
      "Epoch 199/300\n",
      "Average training loss: 0.007139623914741808\n",
      "Average test loss: 0.0019612086374933523\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007137526062627633\n",
      "Average test loss: 0.0019257740653637382\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007132907871570852\n",
      "Average test loss: 0.0019803254758525225\n",
      "Epoch 202/300\n",
      "Average training loss: 0.007136246548758613\n",
      "Average test loss: 0.0020005311274694073\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007125387037793795\n",
      "Average test loss: 0.0019645285031033885\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007146593421697617\n",
      "Average test loss: 0.0019168935991409751\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007123014974097411\n",
      "Average test loss: 0.0018978702150699166\n",
      "Epoch 206/300\n",
      "Average training loss: 0.007122021236767371\n",
      "Average test loss: 0.001945025467313826\n",
      "Epoch 207/300\n",
      "Average training loss: 0.007113866825898488\n",
      "Average test loss: 0.0019560158894293836\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0071025820072326395\n",
      "Average test loss: 0.0019897403156177865\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007103291938288344\n",
      "Average test loss: 0.0019190771737032466\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007112748444080353\n",
      "Average test loss: 0.0020103717594304018\n",
      "Epoch 211/300\n",
      "Average training loss: 0.007097723135103782\n",
      "Average test loss: 0.0019738419220472377\n",
      "Epoch 212/300\n",
      "Average training loss: 0.007101359492374791\n",
      "Average test loss: 0.0019275434882276587\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007096796614428361\n",
      "Average test loss: 0.0019357699271705415\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00710024534083075\n",
      "Average test loss: 0.0019948409559826055\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007099424671381712\n",
      "Average test loss: 0.001897161365693642\n",
      "Epoch 216/300\n",
      "Average training loss: 0.007085049368441105\n",
      "Average test loss: 0.0019475448548586832\n",
      "Epoch 217/300\n",
      "Average training loss: 0.007083719628138674\n",
      "Average test loss: 0.0019482317860755655\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00708120980196529\n",
      "Average test loss: 0.0019762544075234067\n",
      "Epoch 219/300\n",
      "Average training loss: 0.007084236862758796\n",
      "Average test loss: 0.001947666901991599\n",
      "Epoch 220/300\n",
      "Average training loss: 0.007068524794860019\n",
      "Average test loss: 0.001929467102823158\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007063955194006363\n",
      "Average test loss: 0.0019863772907604774\n",
      "Epoch 222/300\n",
      "Average training loss: 0.007071271359920502\n",
      "Average test loss: 0.0019129770940376652\n",
      "Epoch 223/300\n",
      "Average training loss: 0.007076938221851984\n",
      "Average test loss: 0.001968384295908941\n",
      "Epoch 224/300\n",
      "Average training loss: 0.007075596099512444\n",
      "Average test loss: 0.00194519435448779\n",
      "Epoch 225/300\n",
      "Average training loss: 0.007069982096552849\n",
      "Average test loss: 0.0019412378750534522\n",
      "Epoch 226/300\n",
      "Average training loss: 0.007056546440141069\n",
      "Average test loss: 0.001995967562103437\n",
      "Epoch 227/300\n",
      "Average training loss: 0.007052054743799899\n",
      "Average test loss: 0.0019937325687044197\n",
      "Epoch 228/300\n",
      "Average training loss: 0.007048364567259948\n",
      "Average test loss: 0.001964065477769408\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0070531837344169615\n",
      "Average test loss: 0.0020431207741300264\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007046256891969177\n",
      "Average test loss: 0.001967648576117224\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007039560458726353\n",
      "Average test loss: 0.0019296071234469613\n",
      "Epoch 232/300\n",
      "Average training loss: 0.007043567111094793\n",
      "Average test loss: 0.0019516206887654132\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007044993713498116\n",
      "Average test loss: 0.0019459802090293831\n",
      "Epoch 234/300\n",
      "Average training loss: 0.007031608367131816\n",
      "Average test loss: 0.0019773307107388972\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007028897213439147\n",
      "Average test loss: 0.0019616919023295242\n",
      "Epoch 236/300\n",
      "Average training loss: 0.007037887089782291\n",
      "Average test loss: 0.0019568831299742063\n",
      "Epoch 237/300\n",
      "Average training loss: 0.007032379400812918\n",
      "Average test loss: 0.00195742626943522\n",
      "Epoch 238/300\n",
      "Average training loss: 0.007028301219973299\n",
      "Average test loss: 0.001976212554404305\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007024796161593663\n",
      "Average test loss: 0.0020084546332557996\n",
      "Epoch 240/300\n",
      "Average training loss: 0.007019157155520386\n",
      "Average test loss: 0.0019510121368285682\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007010770810974969\n",
      "Average test loss: 0.0019854129912952583\n",
      "Epoch 242/300\n",
      "Average training loss: 0.007018810350447893\n",
      "Average test loss: 0.0020190797981081737\n",
      "Epoch 243/300\n",
      "Average training loss: 0.007016807957241932\n",
      "Average test loss: 0.0019829805084607666\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007009995945625835\n",
      "Average test loss: 0.00196972460548083\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0070115639025138485\n",
      "Average test loss: 0.0019723938314451113\n",
      "Epoch 246/300\n",
      "Average training loss: 0.007005814063466257\n",
      "Average test loss: 0.0019375983828471767\n",
      "Epoch 247/300\n",
      "Average training loss: 0.007007958605057664\n",
      "Average test loss: 0.0019998920992430713\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0070003439680569705\n",
      "Average test loss: 0.001890067745000124\n",
      "Epoch 249/300\n",
      "Average training loss: 0.007001467321068049\n",
      "Average test loss: 0.0019996569371885722\n",
      "Epoch 250/300\n",
      "Average training loss: 0.007000649127695296\n",
      "Average test loss: 0.001940897696134117\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007005186410413848\n",
      "Average test loss: 0.00202089892162217\n",
      "Epoch 252/300\n",
      "Average training loss: 0.006990218367427588\n",
      "Average test loss: 0.001954583750002914\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0069879104933804935\n",
      "Average test loss: 0.00200306397345331\n",
      "Epoch 254/300\n",
      "Average training loss: 0.006974032793194055\n",
      "Average test loss: 0.0020230165982825887\n",
      "Epoch 255/300\n",
      "Average training loss: 0.006977696392271254\n",
      "Average test loss: 0.0019437110357814365\n",
      "Epoch 256/300\n",
      "Average training loss: 0.006985824341575304\n",
      "Average test loss: 0.0019752748859011467\n",
      "Epoch 257/300\n",
      "Average training loss: 0.006981021902213494\n",
      "Average test loss: 0.0020377357218207583\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007014652860247427\n",
      "Average test loss: 0.002001157087998258\n",
      "Epoch 259/300\n",
      "Average training loss: 0.006968623034242127\n",
      "Average test loss: 0.0020201109033077956\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0069678842830989095\n",
      "Average test loss: 0.0018937331375976404\n",
      "Epoch 261/300\n",
      "Average training loss: 0.006974431046595176\n",
      "Average test loss: 0.002003735794996222\n",
      "Epoch 262/300\n",
      "Average training loss: 0.006958655840820736\n",
      "Average test loss: 0.001968165965957774\n",
      "Epoch 263/300\n",
      "Average training loss: 0.006963106947226657\n",
      "Average test loss: 0.0019505212037927574\n",
      "Epoch 264/300\n",
      "Average training loss: 0.006973684208260642\n",
      "Average test loss: 0.001976716658928328\n",
      "Epoch 265/300\n",
      "Average training loss: 0.006962458066642284\n",
      "Average test loss: 0.0019790485063567757\n",
      "Epoch 266/300\n",
      "Average training loss: 0.006966480482369661\n",
      "Average test loss: 0.0018954480084487134\n",
      "Epoch 267/300\n",
      "Average training loss: 0.006955563185943497\n",
      "Average test loss: 0.002013369935254256\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006957641887995932\n",
      "Average test loss: 0.002006700097479754\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006951876135336028\n",
      "Average test loss: 0.0019766972876257365\n",
      "Epoch 270/300\n",
      "Average training loss: 0.006951210545168982\n",
      "Average test loss: 0.002010248175304797\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006956519761433204\n",
      "Average test loss: 0.00204127674901651\n",
      "Epoch 272/300\n",
      "Average training loss: 0.006949208510004812\n",
      "Average test loss: 0.002019198415387008\n",
      "Epoch 273/300\n",
      "Average training loss: 0.006944935168243117\n",
      "Average test loss: 0.001963111183916529\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006940733761423164\n",
      "Average test loss: 0.001942134479785131\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006934900679108169\n",
      "Average test loss: 0.0019819078927652703\n",
      "Epoch 276/300\n",
      "Average training loss: 0.00694839071854949\n",
      "Average test loss: 0.0019380189099659523\n",
      "Epoch 277/300\n",
      "Average training loss: 0.00694078645358483\n",
      "Average test loss: 0.0019911682682318818\n",
      "Epoch 278/300\n",
      "Average training loss: 0.006930777475237846\n",
      "Average test loss: 0.002034339176490903\n",
      "Epoch 279/300\n",
      "Average training loss: 0.006927641387614939\n",
      "Average test loss: 0.0019653634143372376\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0069299531198210185\n",
      "Average test loss: 0.001954807170977195\n",
      "Epoch 281/300\n",
      "Average training loss: 0.006927467291967736\n",
      "Average test loss: 0.001986502951011062\n",
      "Epoch 282/300\n",
      "Average training loss: 0.006924551617354155\n",
      "Average test loss: 0.0019784617635111015\n",
      "Epoch 283/300\n",
      "Average training loss: 0.006925275205738015\n",
      "Average test loss: 0.0020197358406666252\n",
      "Epoch 284/300\n",
      "Average training loss: 0.006928974722408586\n",
      "Average test loss: 0.001896097604288823\n",
      "Epoch 285/300\n",
      "Average training loss: 0.006941894242333041\n",
      "Average test loss: 0.002008947228391965\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0069208270899123615\n",
      "Average test loss: 0.0019717781965931254\n",
      "Epoch 287/300\n",
      "Average training loss: 0.006916204092817174\n",
      "Average test loss: 0.001955424402736955\n",
      "Epoch 288/300\n",
      "Average training loss: 0.006917778403394752\n",
      "Average test loss: 0.0019889296018001106\n",
      "Epoch 289/300\n",
      "Average training loss: 0.006912204541679886\n",
      "Average test loss: 0.0019084636100257437\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0069282170202997\n",
      "Average test loss: 0.001991862554051396\n",
      "Epoch 291/300\n",
      "Average training loss: 0.006904690972218911\n",
      "Average test loss: 0.0019369328003376722\n",
      "Epoch 292/300\n",
      "Average training loss: 0.006913109483404292\n",
      "Average test loss: 0.001998263847289814\n",
      "Epoch 293/300\n",
      "Average training loss: 0.006900978780455059\n",
      "Average test loss: 0.001977508942700095\n",
      "Epoch 294/300\n",
      "Average training loss: 0.006904626854177978\n",
      "Average test loss: 0.0020085146012198594\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006896281523836984\n",
      "Average test loss: 0.0020271409836908182\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00690393757695953\n",
      "Average test loss: 0.0019269402198907402\n",
      "Epoch 297/300\n",
      "Average training loss: 0.006904020906322532\n",
      "Average test loss: 0.002015126762808197\n",
      "Epoch 298/300\n",
      "Average training loss: 0.006888750279529227\n",
      "Average test loss: 0.002060259815098511\n",
      "Epoch 299/300\n",
      "Average training loss: 0.006900201960570282\n",
      "Average test loss: 0.001979229947552085\n",
      "Epoch 300/300\n",
      "Average training loss: 0.006889038111600611\n",
      "Average test loss: 0.0020127354638857973\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive_Depth10-.025/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.35\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.54\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.24\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.23250624540117\n",
      "Average test loss: 0.01485995440226462\n",
      "Epoch 2/300\n",
      "Average training loss: 1.2993019488652546\n",
      "Average test loss: 0.005305695115278165\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8500322567621866\n",
      "Average test loss: 0.0047371646463871\n",
      "Epoch 4/300\n",
      "Average training loss: 0.5932625586721633\n",
      "Average test loss: 0.005876512903306219\n",
      "Epoch 5/300\n",
      "Average training loss: 0.43336970422003\n",
      "Average test loss: 0.006212593199892176\n",
      "Epoch 6/300\n",
      "Average training loss: 0.33328892869419524\n",
      "Average test loss: 0.004440141424122784\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2701881922483444\n",
      "Average test loss: 0.004388049233704805\n",
      "Epoch 8/300\n",
      "Average training loss: 0.22512941008143955\n",
      "Average test loss: 0.004337842708246576\n",
      "Epoch 9/300\n",
      "Average training loss: 0.19132043443785773\n",
      "Average test loss: 0.004313141423381037\n",
      "Epoch 10/300\n",
      "Average training loss: 0.16558483320474623\n",
      "Average test loss: 0.004276697126941549\n",
      "Epoch 11/300\n",
      "Average training loss: 0.14578544453779857\n",
      "Average test loss: 0.004314845837652684\n",
      "Epoch 12/300\n",
      "Average training loss: 0.12959736905495325\n",
      "Average test loss: 0.004266546306096845\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11669335145420498\n",
      "Average test loss: 0.004265188903858264\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10641340805424584\n",
      "Average test loss: 0.00421837873922454\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0975651812089814\n",
      "Average test loss: 0.004179871965199709\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09085068286789788\n",
      "Average test loss: 0.004198806405895286\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08526378188199467\n",
      "Average test loss: 0.004535875785681936\n",
      "Epoch 18/300\n",
      "Average training loss: 0.08083266769515143\n",
      "Average test loss: 0.004153172174468637\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07719529378414154\n",
      "Average test loss: 0.004121148446367847\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07439171773195266\n",
      "Average test loss: 0.004123395856883791\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07200524851348665\n",
      "Average test loss: 0.0040907933327058955\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07024217972159386\n",
      "Average test loss: 0.004129249637325604\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06861065856615703\n",
      "Average test loss: 0.00414525855705142\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06746138963765569\n",
      "Average test loss: 0.004066522715406286\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06645522161987093\n",
      "Average test loss: 0.004063758517718978\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0658780130677753\n",
      "Average test loss: 0.004050727447701825\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06497967341542243\n",
      "Average test loss: 0.004079893101420667\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06458074394861857\n",
      "Average test loss: 0.004032960655788581\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06393779251641697\n",
      "Average test loss: 0.004039538176109393\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06342851361963484\n",
      "Average test loss: 0.00403989810248216\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06291921845409605\n",
      "Average test loss: 0.004015416007488966\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06259951957066853\n",
      "Average test loss: 0.00400793486668004\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06223744926850001\n",
      "Average test loss: 0.004032631951901648\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06190707561042574\n",
      "Average test loss: 0.003999240107834339\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06162657535407278\n",
      "Average test loss: 0.004036857097099225\n",
      "Epoch 36/300\n",
      "Average training loss: 0.061409140669637256\n",
      "Average test loss: 0.004006996861348549\n",
      "Epoch 37/300\n",
      "Average training loss: 0.061163798915015326\n",
      "Average test loss: 0.003988534211077624\n",
      "Epoch 38/300\n",
      "Average training loss: 0.060996898538536494\n",
      "Average test loss: 0.004021184355020523\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06069643501440684\n",
      "Average test loss: 0.004009360682219267\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06049567540817791\n",
      "Average test loss: 0.003995682510236899\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06025615025560061\n",
      "Average test loss: 0.003985149292896191\n",
      "Epoch 42/300\n",
      "Average training loss: 0.060091947121752635\n",
      "Average test loss: 0.00398790601723724\n",
      "Epoch 43/300\n",
      "Average training loss: 0.060001312345266344\n",
      "Average test loss: 0.003996874512483676\n",
      "Epoch 44/300\n",
      "Average training loss: 0.059764143387476605\n",
      "Average test loss: 0.004057310066703293\n",
      "Epoch 45/300\n",
      "Average training loss: 0.059547020928727254\n",
      "Average test loss: 0.003979655379222499\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05942506257030699\n",
      "Average test loss: 0.003971287343444096\n",
      "Epoch 47/300\n",
      "Average training loss: 0.059257631904549066\n",
      "Average test loss: 0.004013505406263802\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05899858087632391\n",
      "Average test loss: 0.003996318090293142\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05886925314532386\n",
      "Average test loss: 0.004061442236519522\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05869892240895165\n",
      "Average test loss: 0.004002043701708317\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05859641635749075\n",
      "Average test loss: 0.003971609060962995\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05837693266736137\n",
      "Average test loss: 0.004010393620158236\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05832806470327907\n",
      "Average test loss: 0.00396741003336178\n",
      "Epoch 54/300\n",
      "Average training loss: 0.058045834610859555\n",
      "Average test loss: 0.004011854029777977\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05796166234877374\n",
      "Average test loss: 0.0040031194864875745\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0577997612423367\n",
      "Average test loss: 0.004015002366569307\n",
      "Epoch 57/300\n",
      "Average training loss: 0.057677937689754696\n",
      "Average test loss: 0.004007343264917532\n",
      "Epoch 58/300\n",
      "Average training loss: 0.057400132993857066\n",
      "Average test loss: 0.003969647039555841\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05728601606024636\n",
      "Average test loss: 0.0040285629400362575\n",
      "Epoch 60/300\n",
      "Average training loss: 0.057077087461948395\n",
      "Average test loss: 0.003969852965532077\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05701758661866188\n",
      "Average test loss: 0.003993706686422229\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05671951329708099\n",
      "Average test loss: 0.003955946297281319\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05660857458247079\n",
      "Average test loss: 0.004000859299260709\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05648513415124681\n",
      "Average test loss: 0.004021857208675809\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05638549330830574\n",
      "Average test loss: 0.003962455890037947\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05613586760395103\n",
      "Average test loss: 0.00407215350949102\n",
      "Epoch 67/300\n",
      "Average training loss: 0.056027891877624726\n",
      "Average test loss: 0.004028179608285427\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05583596428566509\n",
      "Average test loss: 0.004023939470243123\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05569391112195121\n",
      "Average test loss: 0.00398031792913874\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05560461332069503\n",
      "Average test loss: 0.004043210757689344\n",
      "Epoch 71/300\n",
      "Average training loss: 0.055421720408731036\n",
      "Average test loss: 0.004065349852045377\n",
      "Epoch 72/300\n",
      "Average training loss: 0.055268942985269756\n",
      "Average test loss: 0.00403357694670558\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05505521144800716\n",
      "Average test loss: 0.004158300244973765\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05488105984197723\n",
      "Average test loss: 0.0041741295570714605\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05477577156821887\n",
      "Average test loss: 0.004036286777506272\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05456955258382691\n",
      "Average test loss: 0.0040581063814461235\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05451639659206072\n",
      "Average test loss: 0.004044715967857175\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05422831017772357\n",
      "Average test loss: 0.004066399529990223\n",
      "Epoch 79/300\n",
      "Average training loss: 0.054180454479323494\n",
      "Average test loss: 0.004079681939134995\n",
      "Epoch 80/300\n",
      "Average training loss: 0.053999296138683955\n",
      "Average test loss: 0.004065384740837746\n",
      "Epoch 81/300\n",
      "Average training loss: 0.053837831960784066\n",
      "Average test loss: 0.0041020302230285275\n",
      "Epoch 82/300\n",
      "Average training loss: 0.053794319348202814\n",
      "Average test loss: 0.0040716333027101224\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05357942010296716\n",
      "Average test loss: 0.004013416287799676\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05349634731478161\n",
      "Average test loss: 0.004186465074204736\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0534108581410514\n",
      "Average test loss: 0.004036025722200672\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05313888455430667\n",
      "Average test loss: 0.004090402305954033\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05301331638296445\n",
      "Average test loss: 0.0040824355048437915\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05300572384066052\n",
      "Average test loss: 0.004178562046753036\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05277979813681708\n",
      "Average test loss: 0.004167916306604942\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05262741361392869\n",
      "Average test loss: 0.004056474348323212\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05250468586881955\n",
      "Average test loss: 0.0041828210266927875\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05235515473948585\n",
      "Average test loss: 0.004142998192873266\n",
      "Epoch 93/300\n",
      "Average training loss: 0.052195075780153276\n",
      "Average test loss: 0.004271184565706385\n",
      "Epoch 94/300\n",
      "Average training loss: 0.052098746735188696\n",
      "Average test loss: 0.0041282989399300685\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05206193252073394\n",
      "Average test loss: 0.00416384135103888\n",
      "Epoch 96/300\n",
      "Average training loss: 0.051888271904653976\n",
      "Average test loss: 0.004110204456994931\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05178926758302583\n",
      "Average test loss: 0.0041891852248873976\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05167413402928246\n",
      "Average test loss: 0.004127980965707037\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05159705622990926\n",
      "Average test loss: 0.004252926339912745\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05138359127442042\n",
      "Average test loss: 0.004066036278588904\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05122669558061494\n",
      "Average test loss: 0.0040891887297232945\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05128267954124345\n",
      "Average test loss: 0.004164635413636764\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05107894047763613\n",
      "Average test loss: 0.00423660697415471\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05106775220566326\n",
      "Average test loss: 0.00422735430051883\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05087610564629237\n",
      "Average test loss: 0.004093021215664016\n",
      "Epoch 106/300\n",
      "Average training loss: 0.050753398136960136\n",
      "Average test loss: 0.004086790861561895\n",
      "Epoch 107/300\n",
      "Average training loss: 0.050633521682686276\n",
      "Average test loss: 0.004128756302719315\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05055554500553343\n",
      "Average test loss: 0.004186352870634033\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05051388762725724\n",
      "Average test loss: 0.004256089539991485\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05034060395095084\n",
      "Average test loss: 0.004257694115655289\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05021955359975497\n",
      "Average test loss: 0.004168547414657143\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05010369226005342\n",
      "Average test loss: 0.004250964388044344\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0500550777150525\n",
      "Average test loss: 0.004126249294314119\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05003106156322691\n",
      "Average test loss: 0.004164808722419871\n",
      "Epoch 115/300\n",
      "Average training loss: 0.049834098776181536\n",
      "Average test loss: 0.004168054536398914\n",
      "Epoch 116/300\n",
      "Average training loss: 0.049808004349470136\n",
      "Average test loss: 0.0042877962469226785\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04970397485295931\n",
      "Average test loss: 0.00421920677791867\n",
      "Epoch 118/300\n",
      "Average training loss: 0.049552601433462566\n",
      "Average test loss: 0.0042414621520373555\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04954536887341075\n",
      "Average test loss: 0.004244226301502851\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04948448607325554\n",
      "Average test loss: 0.004252634623812305\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04934613240096304\n",
      "Average test loss: 0.004199491531070736\n",
      "Epoch 122/300\n",
      "Average training loss: 0.049372354567050934\n",
      "Average test loss: 0.004136059240334564\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04917010217905045\n",
      "Average test loss: 0.004265880626729793\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04900700090991126\n",
      "Average test loss: 0.004157105234348112\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0490353103544977\n",
      "Average test loss: 0.004283695614586274\n",
      "Epoch 126/300\n",
      "Average training loss: 0.048924107525083756\n",
      "Average test loss: 0.004343160258399116\n",
      "Epoch 127/300\n",
      "Average training loss: 0.048776262674066755\n",
      "Average test loss: 0.004227074566400714\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04879868622289764\n",
      "Average test loss: 0.0042219471881786985\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04864321372244093\n",
      "Average test loss: 0.004240594457834959\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04852590836750136\n",
      "Average test loss: 0.0042760660722851755\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04848856571316719\n",
      "Average test loss: 0.0043141709773076905\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04850347700383928\n",
      "Average test loss: 0.004390856333076954\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04840195934971173\n",
      "Average test loss: 0.004214895066701704\n",
      "Epoch 134/300\n",
      "Average training loss: 0.048242813769314025\n",
      "Average test loss: 0.004349421190304889\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04827040762702624\n",
      "Average test loss: 0.004265192409770357\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04815213469001982\n",
      "Average test loss: 0.004276699300855398\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0480331197877725\n",
      "Average test loss: 0.004221990971308615\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04797340684466892\n",
      "Average test loss: 0.004277142631096973\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04787403169439899\n",
      "Average test loss: 0.004311433690289656\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04787346891065439\n",
      "Average test loss: 0.004268159885994262\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04774527725246217\n",
      "Average test loss: 0.004358297569470273\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04764165922005971\n",
      "Average test loss: 0.0043555579338636664\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04761767686737908\n",
      "Average test loss: 0.004264032611002525\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04754168477654457\n",
      "Average test loss: 0.004308283155990971\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04754119652840826\n",
      "Average test loss: 0.004205230875147714\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04736049273610115\n",
      "Average test loss: 0.004313562944531441\n",
      "Epoch 147/300\n",
      "Average training loss: 0.047334818810224534\n",
      "Average test loss: 0.004408815438548724\n",
      "Epoch 148/300\n",
      "Average training loss: 0.047260293001929916\n",
      "Average test loss: 0.004280263230825464\n",
      "Epoch 149/300\n",
      "Average training loss: 0.047193843778636724\n",
      "Average test loss: 0.00423798586635126\n",
      "Epoch 150/300\n",
      "Average training loss: 0.047141985174682405\n",
      "Average test loss: 0.004347295547111166\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04711630982822842\n",
      "Average test loss: 0.004253329693857166\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04708665852414237\n",
      "Average test loss: 0.004324372400840123\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0470441838602225\n",
      "Average test loss: 0.004246066532615158\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04684335544705391\n",
      "Average test loss: 0.004330628092918131\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04681436633401447\n",
      "Average test loss: 0.00435350428107712\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0467825142343839\n",
      "Average test loss: 0.004254909276341399\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04674878871109751\n",
      "Average test loss: 0.004254600549737612\n",
      "Epoch 158/300\n",
      "Average training loss: 0.046717387613323\n",
      "Average test loss: 0.004269176670246654\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04672221628162596\n",
      "Average test loss: 0.004241112099753486\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04665645992424753\n",
      "Average test loss: 0.004407860383391381\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04648521595199903\n",
      "Average test loss: 0.004361983394664195\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04642663284805086\n",
      "Average test loss: 0.00427829713622729\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04638062722484271\n",
      "Average test loss: 0.00442814144740502\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04642722498708301\n",
      "Average test loss: 0.004404057562351227\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04627319654160076\n",
      "Average test loss: 0.004276219042018056\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0462894545296828\n",
      "Average test loss: 0.004343253246818979\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04617356334792243\n",
      "Average test loss: 0.004440405266152488\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04611437424355083\n",
      "Average test loss: 0.004400710677521096\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04606655820541912\n",
      "Average test loss: 0.004353249030394687\n",
      "Epoch 170/300\n",
      "Average training loss: 0.046016518308056725\n",
      "Average test loss: 0.004392860013163752\n",
      "Epoch 171/300\n",
      "Average training loss: 0.045971588846709995\n",
      "Average test loss: 0.0043947941189010934\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0459268050690492\n",
      "Average test loss: 0.004243543142659797\n",
      "Epoch 173/300\n",
      "Average training loss: 0.045877567499876024\n",
      "Average test loss: 0.004320957873430517\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04583413109183312\n",
      "Average test loss: 0.0043399600175519784\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04569693835907512\n",
      "Average test loss: 0.004326575525518921\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04567078730795118\n",
      "Average test loss: 0.0043821903326445154\n",
      "Epoch 177/300\n",
      "Average training loss: 0.045660040977928376\n",
      "Average test loss: 0.004425733231835895\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04557125454809931\n",
      "Average test loss: 0.004302567781259616\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04557384364803632\n",
      "Average test loss: 0.0043303625920994415\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04551225131584538\n",
      "Average test loss: 0.004319558660189311\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04548545867535803\n",
      "Average test loss: 0.0044141322991086375\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04552133477727572\n",
      "Average test loss: 0.004468951122214397\n",
      "Epoch 183/300\n",
      "Average training loss: 0.045497626175483065\n",
      "Average test loss: 0.004411276996135712\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04529010278317663\n",
      "Average test loss: 0.004378400197045671\n",
      "Epoch 185/300\n",
      "Average training loss: 0.045234447376595606\n",
      "Average test loss: 0.004297204772631328\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04519685011439853\n",
      "Average test loss: 0.004396241619355149\n",
      "Epoch 187/300\n",
      "Average training loss: 0.045135394755336976\n",
      "Average test loss: 0.00434859579635991\n",
      "Epoch 188/300\n",
      "Average training loss: 0.045087711325950096\n",
      "Average test loss: 0.004366512090381649\n",
      "Epoch 189/300\n",
      "Average training loss: 0.045096970150868096\n",
      "Average test loss: 0.004282141833255688\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04513255607419544\n",
      "Average test loss: 0.004312023385945294\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04497008111079534\n",
      "Average test loss: 0.004310784010423554\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04494445961382654\n",
      "Average test loss: 0.004297218101306094\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04494737042652236\n",
      "Average test loss: 0.004379108592040009\n",
      "Epoch 194/300\n",
      "Average training loss: 0.044900686220990284\n",
      "Average test loss: 0.004487830276911457\n",
      "Epoch 195/300\n",
      "Average training loss: 0.044843890478213626\n",
      "Average test loss: 0.004291067820042372\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0447185875442293\n",
      "Average test loss: 0.004256930190655921\n",
      "Epoch 197/300\n",
      "Average training loss: 0.044786415586868925\n",
      "Average test loss: 0.004571304359162847\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04470689265926679\n",
      "Average test loss: 0.00429624561427368\n",
      "Epoch 199/300\n",
      "Average training loss: 0.044691756198803584\n",
      "Average test loss: 0.004374492884510093\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04461559575796127\n",
      "Average test loss: 0.004398928466356463\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04458062258693907\n",
      "Average test loss: 0.004413344017954336\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04459141000111898\n",
      "Average test loss: 0.004283467645860381\n",
      "Epoch 203/300\n",
      "Average training loss: 0.044528768844074675\n",
      "Average test loss: 0.004374241610988975\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04448565892047352\n",
      "Average test loss: 0.004408804003149271\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04439300146036678\n",
      "Average test loss: 0.004521404868198766\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0444399251209365\n",
      "Average test loss: 0.004416941072377893\n",
      "Epoch 207/300\n",
      "Average training loss: 0.044227598441971674\n",
      "Average test loss: 0.004385547448363569\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04425984874036577\n",
      "Average test loss: 0.004392596055650049\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04424129655626085\n",
      "Average test loss: 0.004343657493384348\n",
      "Epoch 210/300\n",
      "Average training loss: 0.044276244481404624\n",
      "Average test loss: 0.004317315914150742\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0441956197851234\n",
      "Average test loss: 0.0043677691858675744\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04412429106566641\n",
      "Average test loss: 0.004363975814647145\n",
      "Epoch 213/300\n",
      "Average training loss: 0.044084181278944014\n",
      "Average test loss: 0.004402692656964064\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04410425645775265\n",
      "Average test loss: 0.004376447542674012\n",
      "Epoch 215/300\n",
      "Average training loss: 0.044108483380741546\n",
      "Average test loss: 0.004278531096254786\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04397976988388432\n",
      "Average test loss: 0.004346328198702799\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04396154313451714\n",
      "Average test loss: 0.004403327866353922\n",
      "Epoch 218/300\n",
      "Average training loss: 0.043870641602410214\n",
      "Average test loss: 0.004515833203577333\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04382792930801709\n",
      "Average test loss: 0.004570680256519053\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04378805179066128\n",
      "Average test loss: 0.004368665628549126\n",
      "Epoch 223/300\n",
      "Average training loss: 0.043717067191998166\n",
      "Average test loss: 0.004351019442288412\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0437522303726938\n",
      "Average test loss: 0.004375615302887228\n",
      "Epoch 225/300\n",
      "Average training loss: 0.043640967941946454\n",
      "Average test loss: 0.004484111637704903\n",
      "Epoch 227/300\n",
      "Average training loss: 0.043603184835778346\n",
      "Average test loss: 0.0043297962260742985\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04358548956612746\n",
      "Average test loss: 0.004344686770604717\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04353870268000497\n",
      "Average test loss: 0.004400870287170013\n",
      "Epoch 230/300\n",
      "Average training loss: 0.043494243366850746\n",
      "Average test loss: 0.0043878967612981795\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04346399579776658\n",
      "Average test loss: 0.004365841197263864\n",
      "Epoch 232/300\n",
      "Average training loss: 0.043479485816425745\n",
      "Average test loss: 0.004354376459287272\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04344957959983084\n",
      "Average test loss: 0.0047150310795340275\n",
      "Epoch 234/300\n",
      "Average training loss: 0.043433037453227574\n",
      "Average test loss: 0.004534357920082079\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04330463280280431\n",
      "Average test loss: 0.004545308161526918\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04329887429210875\n",
      "Average test loss: 0.004364455511172613\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04327299400501781\n",
      "Average test loss: 0.004344778350864847\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04319613494144545\n",
      "Average test loss: 0.004445802432588405\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04320426000157992\n",
      "Average test loss: 0.00435239689424634\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04317607953151067\n",
      "Average test loss: 0.004462417165024413\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0430907361805439\n",
      "Average test loss: 0.004317389239039686\n",
      "Epoch 243/300\n",
      "Average training loss: 0.043050884071323604\n",
      "Average test loss: 0.004792632919011845\n",
      "Epoch 244/300\n",
      "Average training loss: 0.043080291979842714\n",
      "Average test loss: 0.004462766646511025\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04303548449940152\n",
      "Average test loss: 0.00444430671673682\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04304304851426018\n",
      "Average test loss: 0.004366153431435426\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04295459829436408\n",
      "Average test loss: 0.004383517164116104\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04288614318933752\n",
      "Average test loss: 0.004430817586680253\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04289396854738394\n",
      "Average test loss: 0.00449949328125351\n",
      "Epoch 250/300\n",
      "Average training loss: 0.042911203957266275\n",
      "Average test loss: 0.004472579774136344\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04281157018078698\n",
      "Average test loss: 0.004260011744581991\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04281762496630351\n",
      "Average test loss: 0.004458775630841652\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04275549371043841\n",
      "Average test loss: 0.004387171943982442\n",
      "Epoch 254/300\n",
      "Average training loss: 0.042765561213095986\n",
      "Average test loss: 0.0044332091253664755\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04269743261072371\n",
      "Average test loss: 0.004463598508801725\n",
      "Epoch 256/300\n",
      "Average training loss: 0.042708104335599475\n",
      "Average test loss: 0.004524276276636455\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04270316423310174\n",
      "Average test loss: 0.004422373308075799\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04265866447819604\n",
      "Average test loss: 0.004333355184644461\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04260576404465569\n",
      "Average test loss: 0.004425215732306242\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04262040547860993\n",
      "Average test loss: 0.004444046633111106\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04254141940342055\n",
      "Average test loss: 0.004475587211549282\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04256459424230787\n",
      "Average test loss: 0.004595856311834521\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04261409541302257\n",
      "Average test loss: 0.004530340357373158\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04240493251548873\n",
      "Average test loss: 0.004303487476375368\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04243308249447081\n",
      "Average test loss: 0.004376193000210656\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04243354153633118\n",
      "Average test loss: 0.004432506625437075\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04237305269969834\n",
      "Average test loss: 0.004434229891747236\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04240670387281312\n",
      "Average test loss: 0.004585349207330081\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04237811131278674\n",
      "Average test loss: 0.004685172911319467\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04229300066828728\n",
      "Average test loss: 0.004345073447873195\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04231416145629353\n",
      "Average test loss: 0.004500291289968623\n",
      "Epoch 272/300\n",
      "Average training loss: 0.042245700475242405\n",
      "Average test loss: 0.004414648039266467\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04227157615290748\n",
      "Average test loss: 0.004484453910754787\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04225323061810599\n",
      "Average test loss: 0.0044155039625863235\n",
      "Epoch 275/300\n",
      "Average training loss: 0.042225242154465784\n",
      "Average test loss: 0.00441271853653921\n",
      "Epoch 276/300\n",
      "Average training loss: 0.042276681386762195\n",
      "Average test loss: 0.004451054213879009\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04204980373548137\n",
      "Average test loss: 0.004450613729655743\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04205492850144704\n",
      "Average test loss: 0.004522612006713947\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04206454548570845\n",
      "Average test loss: 0.004459828282602959\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04205142804980278\n",
      "Average test loss: 0.004315933852973911\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04202975558903482\n",
      "Average test loss: 0.004426088901029693\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04191556426551607\n",
      "Average test loss: 0.0042737023391657405\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04198246738314629\n",
      "Average test loss: 0.004437580368998978\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04193220317694876\n",
      "Average test loss: 0.004482700810250309\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04191892599397235\n",
      "Average test loss: 0.0044146206983261636\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04190411546164089\n",
      "Average test loss: 0.004346967598423361\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04186202737357882\n",
      "Average test loss: 0.004410885378511416\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0418099917239613\n",
      "Average test loss: 0.004487579548110565\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04178638520174556\n",
      "Average test loss: 0.004412671201551954\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04177128357026312\n",
      "Average test loss: 0.004614754860599836\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04171368796626727\n",
      "Average test loss: 0.0044971059407624935\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04175569955507914\n",
      "Average test loss: 0.004409923273242182\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04166718856162495\n",
      "Average test loss: 0.004274993295056952\n",
      "Epoch 298/300\n",
      "Average training loss: 0.041615495041012765\n",
      "Average test loss: 0.004574919664611419\n",
      "Epoch 299/300\n",
      "Average training loss: 0.041666043526596495\n",
      "Average test loss: 0.0045779354013502594\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04169984648293919\n",
      "Average test loss: 0.004454752645765741\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.7901221470303006\n",
      "Average test loss: 0.006955523840255207\n",
      "Epoch 2/300\n",
      "Average training loss: 1.3849067911571926\n",
      "Average test loss: 0.004864337667201956\n",
      "Epoch 3/300\n",
      "Average training loss: 0.9143367156452603\n",
      "Average test loss: 0.004410963310963577\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6773444890446133\n",
      "Average test loss: 0.004105666954898172\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5205161200894249\n",
      "Average test loss: 0.004005937896668911\n",
      "Epoch 6/300\n",
      "Average training loss: 0.32756516612900627\n",
      "Average test loss: 0.003867817885345883\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2709884400632646\n",
      "Average test loss: 0.0037633329302900367\n",
      "Epoch 9/300\n",
      "Average training loss: 0.22719965926806132\n",
      "Average test loss: 0.003876715591798226\n",
      "Epoch 10/300\n",
      "Average training loss: 0.19354186589188047\n",
      "Average test loss: 0.003700751803608404\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1450865320364634\n",
      "Average test loss: 0.0035506726875901224\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1278128520184093\n",
      "Average test loss: 0.003506684244092968\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11324658397833506\n",
      "Average test loss: 0.003492642092311548\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09240445088346799\n",
      "Average test loss: 0.0034178736702435548\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08472482044166989\n",
      "Average test loss: 0.003357350988520516\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0784682887825701\n",
      "Average test loss: 0.003328947293675608\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07350160550408893\n",
      "Average test loss: 0.0033082758171690835\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06929650068283082\n",
      "Average test loss: 0.00327953632755412\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06600480010112127\n",
      "Average test loss: 0.0032468454885400005\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06339131323496501\n",
      "Average test loss: 0.0032156346796287432\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0611481380297078\n",
      "Average test loss: 0.0033384891046832005\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05943132107456525\n",
      "Average test loss: 0.003186943708194627\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05794343557622698\n",
      "Average test loss: 0.0031660444773733616\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05667939505643315\n",
      "Average test loss: 0.00315428185587128\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05573388179805544\n",
      "Average test loss: 0.003137014547155963\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05489166925350825\n",
      "Average test loss: 0.0031508346005446383\n",
      "Epoch 29/300\n",
      "Average training loss: 0.054155239641666415\n",
      "Average test loss: 0.003145770387724042\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05286390730076366\n",
      "Average test loss: 0.0031225607028851905\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0523216662340694\n",
      "Average test loss: 0.0031230002403673197\n",
      "Epoch 33/300\n",
      "Average training loss: 0.051852608316474494\n",
      "Average test loss: 0.0030946939916660387\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05134302030669318\n",
      "Average test loss: 0.003060977767739031\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05091627731588152\n",
      "Average test loss: 0.0030692725628614424\n",
      "Epoch 36/300\n",
      "Average training loss: 0.050542980045080184\n",
      "Average test loss: 0.0030962690845545795\n",
      "Epoch 37/300\n",
      "Average training loss: 0.050042582925823\n",
      "Average test loss: 0.00305321838044458\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04967440084285206\n",
      "Average test loss: 0.003051351688698762\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04927028661635187\n",
      "Average test loss: 0.003095237822789285\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04886185574200418\n",
      "Average test loss: 0.0030943846617721847\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04846048390203052\n",
      "Average test loss: 0.0031173625505632823\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0481427935494317\n",
      "Average test loss: 0.003031158435054951\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04786067350705465\n",
      "Average test loss: 0.003040881401134862\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04746081418792407\n",
      "Average test loss: 0.003048519663926628\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04685380111138026\n",
      "Average test loss: 0.003117917730369502\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04657812123828464\n",
      "Average test loss: 0.003105059127840731\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04625311350491312\n",
      "Average test loss: 0.003036260991667708\n",
      "Epoch 49/300\n",
      "Average training loss: 0.045928771949476664\n",
      "Average test loss: 0.003067641050244371\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04572934917608897\n",
      "Average test loss: 0.003181968422192666\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04532451697852877\n",
      "Average test loss: 0.0030515067409723996\n",
      "Epoch 52/300\n",
      "Average training loss: 0.045042282839616136\n",
      "Average test loss: 0.003065116028404898\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04476728199422359\n",
      "Average test loss: 0.00312277612897257\n",
      "Epoch 54/300\n",
      "Average training loss: 0.044494042217731476\n",
      "Average test loss: 0.0031065107679201496\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04422540394133992\n",
      "Average test loss: 0.0030536721638507315\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04399449550112088\n",
      "Average test loss: 0.003024469228461385\n",
      "Epoch 57/300\n",
      "Average training loss: 0.043702725539604824\n",
      "Average test loss: 0.003041614055219624\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04346093460255199\n",
      "Average test loss: 0.0030018950040555663\n",
      "Epoch 59/300\n",
      "Average training loss: 0.043286858565277524\n",
      "Average test loss: 0.0030079218703839513\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04299615537789133\n",
      "Average training loss: 0.04247473324007458\n",
      "Average test loss: 0.0030973632362567716\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04230709660715527\n",
      "Average test loss: 0.0030490352999832894\n",
      "Epoch 64/300\n",
      "Average training loss: 0.042000716305441325\n",
      "Average test loss: 0.0031068920228216382\n",
      "Epoch 65/300\n",
      "Average training loss: 0.041805481029881374\n",
      "Average test loss: 0.003137991549240218\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04154331290721893\n",
      "Average test loss: 0.003156316864821646\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04135534360011418\n",
      "Average test loss: 0.0030617761386351455\n",
      "Epoch 68/300\n",
      "Average training loss: 0.041149422807825936\n",
      "Average test loss: 0.0031146013310386074\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04107884373598628\n",
      "Average test loss: 0.0030754035418439243\n",
      "Epoch 70/300\n",
      "Average training loss: 0.040746647278467815\n",
      "Average test loss: 0.003056379069056776\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04066593106587728\n",
      "Average test loss: 0.00316122924681339\n",
      "Epoch 72/300\n",
      "Average training loss: 0.040346294694476655\n",
      "Average test loss: 0.0031066578550057278\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04027000445789761\n",
      "Average test loss: 0.003099675037794643\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03986418625712395\n",
      "Average test loss: 0.0030701868721387453\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03974753259950214\n",
      "Average test loss: 0.0030812527578738\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03948290210962296\n",
      "Average test loss: 0.0032771691729625066\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03944034768972132\n",
      "Average test loss: 0.0031622308380901813\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03930448428127501\n",
      "Average test loss: 0.0031433719268275633\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03908971528377798\n",
      "Average test loss: 0.0031024811116771565\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03898265319069227\n",
      "Average test loss: 0.003146582295704219\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03876309450467427\n",
      "Average test loss: 0.003084723822772503\n",
      "Epoch 83/300\n",
      "Average training loss: 0.038681150863567985\n",
      "Average test loss: 0.003107612451745404\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03849052835504214\n",
      "Average test loss: 0.003191587062759532\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0384129424823655\n",
      "Average test loss: 0.00312027463886059\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03823566409283214\n",
      "Average test loss: 0.0031408836676014794\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03802408506141768\n",
      "Average test loss: 0.003137130475913485\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03785348868370056\n",
      "Average test loss: 0.0033286424811101623\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03780277389950223\n",
      "Average test loss: 0.0031065842979070214\n",
      "Epoch 91/300\n",
      "Average training loss: 0.037878942415118216\n",
      "Average test loss: 0.0031948273001859585\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03751747155686219\n",
      "Average test loss: 0.003335551374695367\n",
      "Epoch 93/300\n",
      "Average training loss: 0.037357128850287864\n",
      "Average test loss: 0.0032373161518739328\n",
      "Epoch 94/300\n",
      "Average training loss: 0.037310268931918675\n",
      "Average test loss: 0.003225856052711606\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03716675422257847\n",
      "Average test loss: 0.0032253466525839435\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0371549330121941\n",
      "Average test loss: 0.0031967850795222652\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03698944479558203\n",
      "Average test loss: 0.003169493337265319\n",
      "Epoch 98/300\n",
      "Average training loss: 0.036893660144673456\n",
      "Average test loss: 0.003130117293447256\n",
      "Epoch 99/300\n",
      "Average training loss: 0.036752382424142624\n",
      "Average test loss: 0.003208074645035797\n",
      "Epoch 100/300\n",
      "Average training loss: 0.036703645533985564\n",
      "Average test loss: 0.0031713151625461047\n",
      "Epoch 101/300\n",
      "Average training loss: 0.036611077244083086\n",
      "Average test loss: 0.003184231643461519\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03655812935365571\n",
      "Average test loss: 0.003166392169892788\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03641538846161631\n",
      "Average test loss: 0.0034292339969219433\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0363198536435763\n",
      "Average test loss: 0.0031476963793651926\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0361214148186975\n",
      "Average test loss: 0.003180987344433864\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03606720520059268\n",
      "Average test loss: 0.003231071339920163\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03602130559417936\n",
      "Average test loss: 0.0031635736831360393\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03594561284614934\n",
      "Average test loss: 0.003230853867199686\n",
      "Epoch 110/300\n",
      "Average training loss: 0.035782004662685926\n",
      "Average test loss: 0.003210660032514069\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03579211203753948\n",
      "Average test loss: 0.0032524197585880755\n",
      "Epoch 112/300\n",
      "Average training loss: 0.035720280768142806\n",
      "Average test loss: 0.0033304169124199283\n",
      "Epoch 113/300\n",
      "Average training loss: 0.035589498354329\n",
      "Average test loss: 0.003252324339830213\n",
      "Epoch 114/300\n",
      "Average training loss: 0.035521849536233475\n",
      "Average test loss: 0.003146207924725281\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0354149006207784\n",
      "Average test loss: 0.0033417975592116515\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03528363215592172\n",
      "Average test loss: 0.003222746672729651\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03527658853265974\n",
      "Average test loss: 0.0032437618453469543\n",
      "Epoch 118/300\n",
      "Average training loss: 0.035206027034256196\n",
      "Average test loss: 0.0034493648282562695\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03518710724181599\n",
      "Average test loss: 0.003180067079762618\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03503391297327148\n",
      "Average test loss: 0.003289984381240275\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03499472969273726\n",
      "Average test loss: 0.003240617050892777\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03486554123626815\n",
      "Average test loss: 0.003290114122339421\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03492251396510336\n",
      "Average test loss: 0.0032384859451817143\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0348691814939181\n",
      "Average test loss: 0.0032788526016390984\n",
      "Epoch 125/300\n",
      "Average training loss: 0.034801687229010794\n",
      "Average test loss: 0.003299570350183381\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03468580731583966\n",
      "Average test loss: 0.0031984392785363728\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03457606084479226\n",
      "Average test loss: 0.003240374766290188\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03454116328557332\n",
      "Average test loss: 0.0031880939273784556\n",
      "Epoch 129/300\n",
      "Average training loss: 0.034546743232342934\n",
      "Average test loss: 0.00333500299975276\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03445998784899711\n",
      "Average test loss: 0.0032814099074651797\n",
      "Epoch 131/300\n",
      "Average training loss: 0.034352832840548624\n",
      "Average test loss: 0.0032677385392081404\n",
      "Epoch 132/300\n",
      "Average training loss: 0.034186936183108224\n",
      "Average test loss: 0.0032346351850363942\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03423937807811631\n",
      "Average test loss: 0.00318052736007505\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03418086851636569\n",
      "Average test loss: 0.003271756019236313\n",
      "Epoch 136/300\n",
      "Average training loss: 0.034070396231280435\n",
      "Average test loss: 0.0033201346680935885\n",
      "Epoch 137/300\n",
      "Average training loss: 0.034052329546875426\n",
      "Average test loss: 0.0033952618009514277\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03402754917409685\n",
      "Average test loss: 0.0032269887199832333\n",
      "Epoch 139/300\n",
      "Average training loss: 0.033907073244452475\n",
      "Average test loss: 0.003279006241096391\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03389421588016881\n",
      "Average test loss: 0.003257120228062073\n",
      "Epoch 141/300\n",
      "Average training loss: 0.033802888653344576\n",
      "Average test loss: 0.003386023248028424\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03378685960835881\n",
      "Average test loss: 0.0033091901277916298\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03365479441483815\n",
      "Average test loss: 0.0032521372942460906\n",
      "Epoch 144/300\n",
      "Average training loss: 0.033681171460284126\n",
      "Average test loss: 0.00328646438403262\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0336495707432429\n",
      "Average test loss: 0.0031985792103740905\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03364298883411619\n",
      "Average test loss: 0.0031917096212920213\n",
      "Epoch 147/300\n",
      "Average training loss: 0.033524012668265234\n",
      "Average test loss: 0.003306635288314687\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03351655255920357\n",
      "Average test loss: 0.0033045317077388366\n",
      "Epoch 149/300\n",
      "Average training loss: 0.033413159999582505\n",
      "Average test loss: 0.0032281879985498058\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03344125865234269\n",
      "Average test loss: 0.0032175823659118677\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03332822001477083\n",
      "Average test loss: 0.0032051289468589756\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03331234369013045\n",
      "Average test loss: 0.003313238067138526\n",
      "Epoch 154/300\n",
      "Average training loss: 0.033293660288055735\n",
      "Average test loss: 0.0032682787006100018\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03315996675690015\n",
      "Average test loss: 0.003291072559439474\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03314017768369781\n",
      "Average test loss: 0.0034195559041367636\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03307478045754963\n",
      "Average test loss: 0.003271909372881055\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03306191152168645\n",
      "Average test loss: 0.0032870102562010286\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0331101094186306\n",
      "Average test loss: 0.0033169920196135837\n",
      "Epoch 160/300\n",
      "Average training loss: 0.032973375615146426\n",
      "Average test loss: 0.0033468099501397877\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03287012861834632\n",
      "Average test loss: 0.003241999640336467\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03288133019540045\n",
      "Average test loss: 0.0033552880650386213\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03281064401732551\n",
      "Average test loss: 0.0033061485187047056\n",
      "Epoch 166/300\n",
      "Average training loss: 0.032660124179389745\n",
      "Average test loss: 0.0033456090630756483\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03275323685010274\n",
      "Average test loss: 0.0034007935244590045\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03265222657057974\n",
      "Average test loss: 0.0034162259083241222\n",
      "Epoch 169/300\n",
      "Average training loss: 0.032600499108433725\n",
      "Average test loss: 0.003369279849446482\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03249607206384341\n",
      "Average test loss: 0.003276152666658163\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03256785656677352\n",
      "Average test loss: 0.0032424677912559776\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03248697415822082\n",
      "Average test loss: 0.003304811945185065\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03243770408961508\n",
      "Average test loss: 0.003328087913700276\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03243543079826567\n",
      "Average test loss: 0.0032349421094275185\n",
      "Epoch 175/300\n",
      "Average training loss: 0.032423717826604845\n",
      "Average test loss: 0.003329241845756769\n",
      "Epoch 176/300\n",
      "Average training loss: 0.032510707906550834\n",
      "Average test loss: 0.0033780960018436113\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03234037797484133\n",
      "Average test loss: 0.0033394737725870476\n",
      "Epoch 178/300\n",
      "Average training loss: 0.032260347819990584\n",
      "Average test loss: 0.0034441129016793436\n",
      "Epoch 179/300\n",
      "Average training loss: 0.032242698338296676\n",
      "Average test loss: 0.0033560368712577554\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03228159188893106\n",
      "Average test loss: 0.003355052130089866\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03218334669205877\n",
      "Average test loss: 0.0034989131233758397\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03217511533035172\n",
      "Average test loss: 0.0033549131620675327\n",
      "Epoch 183/300\n",
      "Average training loss: 0.032050511048899756\n",
      "Average test loss: 0.003329592133147849\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03207764861649937\n",
      "Average test loss: 0.0032899905554950237\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03207128709885809\n",
      "Average test loss: 0.003339875116530392\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03202810002366702\n",
      "Average test loss: 0.0033516250461753873\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03204108674327533\n",
      "Average test loss: 0.0033420026045706537\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03196554059949186\n",
      "Average test loss: 0.003384358428418636\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03191969093349245\n",
      "Average test loss: 0.003336310075595975\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031993204653263094\n",
      "Average test loss: 0.0033186556715518235\n",
      "Epoch 191/300\n",
      "Average training loss: 0.031947176896863515\n",
      "Average test loss: 0.0033392710725052488\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03188741562432713\n",
      "Average test loss: 0.0033981072610865036\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03177456073959668\n",
      "Average test loss: 0.0033748380947444174\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03182226766149203\n",
      "Average test loss: 0.0032972610170642533\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03176582886113061\n",
      "Average test loss: 0.003359939354782303\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03168706351187494\n",
      "Average test loss: 0.003309987158411079\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03170501434968578\n",
      "Average test loss: 0.003280815897302495\n",
      "Epoch 198/300\n",
      "Average training loss: 0.031690736772285566\n",
      "Average test loss: 0.003385328120655484\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03163873530096478\n",
      "Average test loss: 0.0032613302438209454\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03163714922964573\n",
      "Average test loss: 0.0033814787653585277\n",
      "Epoch 201/300\n",
      "Average training loss: 0.031563962378435666\n",
      "Average test loss: 0.003369538607696692\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03152597914801704\n",
      "Average test loss: 0.003339806570774979\n",
      "Epoch 203/300\n",
      "Average training loss: 0.031500864916377595\n",
      "Average test loss: 0.0034331539060092633\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03149774611824089\n",
      "Average test loss: 0.00331147612279488\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03147513743903902\n",
      "Average test loss: 0.003229923491055767\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03146868863536252\n",
      "Average test loss: 0.003335644081855814\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0314127068767945\n",
      "Average test loss: 0.0033897852434052363\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03139658819635709\n",
      "Average test loss: 0.00325503755816155\n",
      "Epoch 209/300\n",
      "Average training loss: 0.031387252100639876\n",
      "Average test loss: 0.003360888811863131\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03130792510012786\n",
      "Average test loss: 0.0034074393481844\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03126407518817319\n",
      "Average test loss: 0.0033453601544929873\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03128120388256179\n",
      "Average test loss: 0.0034506307190491094\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03129087287187576\n",
      "Average test loss: 0.003472981525792016\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03119093901084529\n",
      "Average test loss: 0.0033280464472870033\n",
      "Epoch 215/300\n",
      "Average training loss: 0.031176556239525477\n",
      "Average test loss: 0.0033773270245227548\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03126368540359868\n",
      "Average test loss: 0.0033363555961598954\n",
      "Epoch 217/300\n",
      "Average training loss: 0.031170909466014966\n",
      "Average test loss: 0.003293687366363075\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031106642055842612\n",
      "Average test loss: 0.003445409448610412\n",
      "Epoch 219/300\n",
      "Average training loss: 0.031150511460171806\n",
      "Average test loss: 0.00334979199224876\n",
      "Epoch 220/300\n",
      "Average training loss: 0.031044131139914194\n",
      "Average test loss: 0.0033474374775671295\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03103109510242939\n",
      "Average test loss: 0.003491335226222873\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03102075379755762\n",
      "Average test loss: 0.00329308585015436\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03104949040048652\n",
      "Average test loss: 0.003365254847953717\n",
      "Epoch 224/300\n",
      "Average training loss: 0.030986008635825582\n",
      "Average test loss: 0.00338141942044927\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030967740923166277\n",
      "Average test loss: 0.003324316431250837\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030955388699968656\n",
      "Average test loss: 0.0033299557314150864\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03095365562869443\n",
      "Average test loss: 0.003319230665763219\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030877897638413643\n",
      "Average test loss: 0.003389457973548108\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030844339195224973\n",
      "Average test loss: 0.0034546025136692657\n",
      "Epoch 230/300\n",
      "Average training loss: 0.030890841086705527\n",
      "Average test loss: 0.003271980684250593\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030814692887994977\n",
      "Average test loss: 0.0034150662999600174\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030875572853618197\n",
      "Average test loss: 0.003427536564775639\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03078483293288284\n",
      "Average test loss: 0.003369843811003698\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03077529421283139\n",
      "Average test loss: 0.003320541214611795\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03071892673936155\n",
      "Average test loss: 0.003495822378744682\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030706225570705202\n",
      "Average test loss: 0.0034079299229714606\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03065799706015322\n",
      "Average test loss: 0.0034536465309146377\n",
      "Epoch 238/300\n",
      "Average training loss: 0.030676559623744753\n",
      "Average test loss: 0.0034191734948092038\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030671474115716087\n",
      "Average test loss: 0.003484990144148469\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03056048577196068\n",
      "Average test loss: 0.003437147141123811\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03062591425743368\n",
      "Average test loss: 0.0033900629424800477\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03060327360033989\n",
      "Average test loss: 0.0035797565016481613\n",
      "Epoch 243/300\n",
      "Average training loss: 0.030555427528089946\n",
      "Average test loss: 0.0032759888196984928\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030524901719556914\n",
      "Average test loss: 0.003371305559658342\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03054659925070074\n",
      "Average test loss: 0.003377574958321121\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030454013074437777\n",
      "Average test loss: 0.0034700335131751165\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030457685510317484\n",
      "Average test loss: 0.003333318696874711\n",
      "Epoch 248/300\n",
      "Average training loss: 0.030490272308389344\n",
      "Average test loss: 0.003380416490758459\n",
      "Epoch 249/300\n",
      "Average training loss: 0.030375729800926315\n",
      "Average test loss: 0.003295410642814305\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030386616789632375\n",
      "Average test loss: 0.003414768343584405\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030414099554220836\n",
      "Average test loss: 0.003338312110967106\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03040248100956281\n",
      "Average test loss: 0.0033720331891543337\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030393845634327996\n",
      "Average test loss: 0.00344446811825037\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03043565986222691\n",
      "Average test loss: 0.0033826789994620614\n",
      "Epoch 255/300\n",
      "Average training loss: 0.030343168098065587\n",
      "Average test loss: 0.0034263129722740915\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03024726032714049\n",
      "Average test loss: 0.0034526547752320768\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030270996186468335\n",
      "Average test loss: 0.0034604018727938334\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030298661841286552\n",
      "Average test loss: 0.0033556625809934406\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030201521921488974\n",
      "Average test loss: 0.0034008093525966007\n",
      "Epoch 260/300\n",
      "Average training loss: 0.030226023172338804\n",
      "Average test loss: 0.0034453997202217577\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03024969042506483\n",
      "Average test loss: 0.003472869183661209\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03019032112095091\n",
      "Average test loss: 0.0034222869685747556\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030133293309145503\n",
      "Average test loss: 0.0034578588501446776\n",
      "Epoch 264/300\n",
      "Average training loss: 0.030143605074948735\n",
      "Average test loss: 0.0033882952965795994\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030132442911465964\n",
      "Average test loss: 0.003408701529312465\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030191296754611862\n",
      "Average test loss: 0.0034274156900743645\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030089630024300682\n",
      "Average test loss: 0.0033330536706166133\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030120468490653567\n",
      "Average test loss: 0.0032780318843821685\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03003955554879374\n",
      "Average test loss: 0.0034040601139681205\n",
      "Epoch 270/300\n",
      "Average training loss: 0.030064942644702063\n",
      "Average test loss: 0.0035615561422374514\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030019690121213596\n",
      "Average test loss: 0.003460605276127656\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029951829683449532\n",
      "Average test loss: 0.0033822914448877178\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03000301635762056\n",
      "Average test loss: 0.003381797573839625\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029991571803887684\n",
      "Average test loss: 0.003439548400334186\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029906821740998162\n",
      "Average test loss: 0.003402940292739206\n",
      "Epoch 276/300\n",
      "Average training loss: 0.029925077978107666\n",
      "Average test loss: 0.003428595468401909\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029927930426266458\n",
      "Average test loss: 0.0034849343188107012\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029919383583797348\n",
      "Average test loss: 0.0034667503506773047\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029902607209152644\n",
      "Average test loss: 0.0034057648926973344\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029880135188500086\n",
      "Average test loss: 0.0034084633220401077\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029790799574719536\n",
      "Average test loss: 0.003411012595312463\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029822130312522253\n",
      "Average test loss: 0.0034171642971535525\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029841818140612708\n",
      "Average test loss: 0.0033719265069812538\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029819125360912746\n",
      "Average test loss: 0.003445892654566301\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02976564547750685\n",
      "Average test loss: 0.003384377881677614\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02977175692551666\n",
      "Average test loss: 0.0033901899941265585\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029782788730329936\n",
      "Average test loss: 0.003408727154963546\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0296927580518855\n",
      "Average test loss: 0.0034453336174289383\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029639764661590257\n",
      "Average test loss: 0.003425014077996214\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029665284684962698\n",
      "Average test loss: 0.0034366704237957795\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029744847032758925\n",
      "Average test loss: 0.0033974517370677654\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02966483541826407\n",
      "Average test loss: 0.003414450463735395\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029648507157961527\n",
      "Average test loss: 0.0034587019317679937\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02961177770462301\n",
      "Average test loss: 0.0033417988146344823\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029650555190112855\n",
      "Average test loss: 0.0034165581379913624\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029600986811849805\n",
      "Average test loss: 0.0034304526425484156\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02954618411593967\n",
      "Average test loss: 0.0034853856315215427\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029617389215363398\n",
      "Average test loss: 0.0033103487342596052\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029585631943411298\n",
      "Average test loss: 0.0034603151939809323\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029556500335534412\n",
      "Average test loss: 0.003370184713974595\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.5154593296051027\n",
      "Average test loss: 0.049919477223522135\n",
      "Epoch 2/300\n",
      "Average training loss: 1.320854476292928\n",
      "Average test loss: 0.004182386462680167\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8761144794358148\n",
      "Average test loss: 0.003936690457786123\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6343949970669217\n",
      "Average test loss: 0.003537032298743725\n",
      "Epoch 5/300\n",
      "Average training loss: 0.47938518034087285\n",
      "Average test loss: 0.003459837510974871\n",
      "Epoch 6/300\n",
      "Average training loss: 0.37636910173628063\n",
      "Average test loss: 0.003319792587310076\n",
      "Epoch 7/300\n",
      "Average training loss: 0.3054393966462877\n",
      "Average test loss: 0.003189598146826029\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2529653296205733\n",
      "Average test loss: 0.0037132591025696863\n",
      "Epoch 9/300\n",
      "Average training loss: 0.21329628630479178\n",
      "Average test loss: 0.003501856134376592\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18113894840081532\n",
      "Average test loss: 0.0029790156514694294\n",
      "Epoch 11/300\n",
      "Average training loss: 0.15589129281044006\n",
      "Average test loss: 0.006470151487737894\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13460872366693286\n",
      "Average test loss: 0.017441783538709083\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11738203395737543\n",
      "Average test loss: 0.002769345711916685\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1034964992735121\n",
      "Average test loss: 0.0027358593810349703\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09171719039148754\n",
      "Average test loss: 0.0026893109296345047\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08213486819797092\n",
      "Average test loss: 0.002626688493726154\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07468534643120237\n",
      "Average test loss: 0.0026498717839519184\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06843699944019317\n",
      "Average test loss: 0.0025679773030181726\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06356831386685372\n",
      "Average test loss: 0.0025708403202394643\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05964804191059536\n",
      "Average test loss: 0.002505736271540324\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05651460591952006\n",
      "Average test loss: 0.0025631597505675423\n",
      "Epoch 22/300\n",
      "Average training loss: 0.053915471620029874\n",
      "Average test loss: 0.002499210887071159\n",
      "Epoch 23/300\n",
      "Average training loss: 0.051771481053696736\n",
      "Average test loss: 0.002440517318331533\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04999328455328941\n",
      "Average test loss: 0.0024003775515076186\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04850382331013679\n",
      "Average test loss: 0.0024125737607893017\n",
      "Epoch 26/300\n",
      "Average training loss: 0.045288900257812605\n",
      "Average test loss: 0.0023748395341551968\n",
      "Epoch 29/300\n",
      "Average training loss: 0.044510940190818575\n",
      "Average test loss: 0.002351637413518296\n",
      "Epoch 30/300\n",
      "Average training loss: 0.043693724162048767\n",
      "Average test loss: 0.0025334201014290255\n",
      "Epoch 31/300\n",
      "Average training loss: 0.043051578472057976\n",
      "Average test loss: 0.0023204409775013724\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04251401244931751\n",
      "Average test loss: 0.002314775491133332\n",
      "Epoch 33/300\n",
      "Average training loss: 0.041852834324042\n",
      "Average test loss: 0.002320464786969953\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04125731783774164\n",
      "Average test loss: 0.00233589979302552\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04085372496313519\n",
      "Average test loss: 0.002317802598596447\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04030313845144378\n",
      "Average test loss: 0.0022937150647242863\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03985295552346441\n",
      "Average test loss: 0.0022991556329652668\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03948978318770727\n",
      "Average test loss: 0.002286268393819531\n",
      "Epoch 39/300\n",
      "Average training loss: 0.039064103258980645\n",
      "Average test loss: 0.0022769886849241124\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03866132221950425\n",
      "Average test loss: 0.0022729098171823552\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03818757707211706\n",
      "Average test loss: 0.0022801308292481636\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03788219781054391\n",
      "Average test loss: 0.002303758109712766\n",
      "Epoch 43/300\n",
      "Average training loss: 0.037465485788053934\n",
      "Average test loss: 0.002311272775547372\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03712282389733527\n",
      "Average test loss: 0.002318396903988388\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03674524179432127\n",
      "Average test loss: 0.0022781588967061704\n",
      "Epoch 46/300\n",
      "Average training loss: 0.036487868239482246\n",
      "Average test loss: 0.002249816953928934\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03620905286073685\n",
      "Average test loss: 0.002257549613196817\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03576627438929346\n",
      "Average test loss: 0.0022931608628067707\n",
      "Epoch 49/300\n",
      "Average training loss: 0.035464339928494557\n",
      "Average test loss: 0.002282680349631442\n",
      "Epoch 50/300\n",
      "Average training loss: 0.035344014621443216\n",
      "Average test loss: 0.002272869534169634\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0349002273529768\n",
      "Average test loss: 0.002284317298895783\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03458565585149659\n",
      "Average test loss: 0.002335710316068596\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03434791589611107\n",
      "Average test loss: 0.002305289936562379\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0340699616405699\n",
      "Average test loss: 0.0022774157088456884\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03389988041586346\n",
      "Average test loss: 0.002271999693993065\n",
      "Epoch 56/300\n",
      "Average training loss: 0.033524332630965444\n",
      "Average test loss: 0.002243341526430514\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03332041049665875\n",
      "Average test loss: 0.0038179347289519177\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03303752171165413\n",
      "Average test loss: 0.0022664623018354177\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03280348337358899\n",
      "Average test loss: 0.002301576868630946\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03255034277836482\n",
      "Average test loss: 0.002280399087816477\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03237407366103596\n",
      "Average test loss: 0.00227461756610622\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0321591375619173\n",
      "Average test loss: 0.00230747063147525\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03199005126953125\n",
      "Average test loss: 0.0022796811395221286\n",
      "Epoch 64/300\n",
      "Average training loss: 0.031697945773601535\n",
      "Average test loss: 0.0023620947810510796\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03154051728877756\n",
      "Average test loss: 0.002298594667058852\n",
      "Epoch 66/300\n",
      "Average training loss: 0.031444211630357634\n",
      "Average test loss: 0.0023129211434473594\n",
      "Epoch 67/300\n",
      "Average training loss: 0.031067815186248886\n",
      "Average test loss: 0.0024373456438382467\n",
      "Epoch 68/300\n",
      "Average training loss: 0.030902546856966282\n",
      "Average test loss: 0.0023931682627234196\n",
      "Epoch 69/300\n",
      "Average training loss: 0.030810317286186748\n",
      "Average test loss: 0.0023310400284826754\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03055483388900757\n",
      "Average test loss: 0.002285989478437437\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03038895079990228\n",
      "Average test loss: 0.0023231332896070348\n",
      "Epoch 72/300\n",
      "Average training loss: 0.030358074776000447\n",
      "Average test loss: 0.002332511463926898\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03013851575553417\n",
      "Average test loss: 0.0024103442329085536\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03000008370478948\n",
      "Average test loss: 0.0023092178482976224\n",
      "Epoch 75/300\n",
      "Average training loss: 0.029744561678833433\n",
      "Average test loss: 0.0023101172590007384\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02971350476808018\n",
      "Average test loss: 0.0023389864620856115\n",
      "Epoch 77/300\n",
      "Average training loss: 0.029562976113624042\n",
      "Average test loss: 0.0023324543208711676\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02942273810837004\n",
      "Average test loss: 0.0023515848792675467\n",
      "Epoch 79/300\n",
      "Average training loss: 0.029207647068632974\n",
      "Average test loss: 0.002340979234729376\n",
      "Epoch 80/300\n",
      "Average training loss: 0.029070472477210892\n",
      "Average test loss: 0.002366946236903055\n",
      "Epoch 81/300\n",
      "Average training loss: 0.029032336016496022\n",
      "Average test loss: 0.0023196690051505964\n",
      "Epoch 82/300\n",
      "Average training loss: 0.028937777787446977\n",
      "Average test loss: 0.002468382075635923\n",
      "Epoch 83/300\n",
      "Average training loss: 0.028766803342435095\n",
      "Average test loss: 0.0024301010682764982\n",
      "Epoch 84/300\n",
      "Average training loss: 0.028611925832099386\n",
      "Average test loss: 0.0024358481214278274\n",
      "Epoch 85/300\n",
      "Average training loss: 0.028531737617320484\n",
      "Average test loss: 0.002549034954669575\n",
      "Epoch 86/300\n",
      "Average training loss: 0.028458982303738594\n",
      "Average test loss: 0.0023456356968316766\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02834968742231528\n",
      "Average test loss: 0.0024752469214921195\n",
      "Epoch 88/300\n",
      "Average training loss: 0.028232382161749735\n",
      "Average test loss: 0.002323622816138797\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02822447385887305\n",
      "Average test loss: 0.0023139177254504627\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02805094542933835\n",
      "Average test loss: 0.002388968099736505\n",
      "Epoch 91/300\n",
      "Average training loss: 0.028006330756677522\n",
      "Average test loss: 0.002416821014756958\n",
      "Epoch 92/300\n",
      "Average training loss: 0.027857394196093083\n",
      "Average test loss: 0.0024583255792450572\n",
      "Epoch 93/300\n",
      "Average training loss: 0.027720500618219377\n",
      "Average test loss: 0.0024163988913512894\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0276137341691388\n",
      "Average test loss: 0.002664654581497113\n",
      "Epoch 95/300\n",
      "Average training loss: 0.027551172566082742\n",
      "Average test loss: 0.0023872403318269384\n",
      "Epoch 96/300\n",
      "Average training loss: 0.027555704274111324\n",
      "Average test loss: 0.0023672315886037218\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0274577249445849\n",
      "Average test loss: 0.00240406867240866\n",
      "Epoch 98/300\n",
      "Average training loss: 0.027380156526962917\n",
      "Average test loss: 0.002537818531712724\n",
      "Epoch 99/300\n",
      "Average training loss: 0.027293382816844517\n",
      "Average test loss: 0.00236397470958117\n",
      "Epoch 100/300\n",
      "Average training loss: 0.027179653119709758\n",
      "Average test loss: 0.002425028253553642\n",
      "Epoch 101/300\n",
      "Average training loss: 0.027113906914989154\n",
      "Average test loss: 0.0024034485740380153\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02707940237555239\n",
      "Average test loss: 0.002427995650408169\n",
      "Epoch 103/300\n",
      "Average training loss: 0.026986205339431763\n",
      "Average test loss: 0.00237595334286905\n",
      "Epoch 104/300\n",
      "Average training loss: 0.026934870620568593\n",
      "Average test loss: 0.0026367133466733824\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026858592657579314\n",
      "Average test loss: 0.0024756624213316375\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02676754084229469\n",
      "Average test loss: 0.002458377043199208\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02675086814330684\n",
      "Average test loss: 0.0024602526529795593\n",
      "Epoch 108/300\n",
      "Average training loss: 0.026636414893799358\n",
      "Average test loss: 0.0024283664772907893\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02664246195554733\n",
      "Average test loss: 0.002438459853331248\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02656207093099753\n",
      "Average test loss: 0.0024821837736914557\n",
      "Epoch 111/300\n",
      "Average training loss: 0.026453968715336586\n",
      "Average test loss: 0.0023796211008189454\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02636997936334875\n",
      "Average test loss: 0.0024889983588622677\n",
      "Epoch 113/300\n",
      "Average training loss: 0.026351868725485273\n",
      "Average test loss: 0.0024587220156358346\n",
      "Epoch 114/300\n",
      "Average training loss: 0.026282757515708605\n",
      "Average test loss: 0.0024774816176957553\n",
      "Epoch 115/300\n",
      "Average training loss: 0.026212033241987228\n",
      "Average test loss: 0.0023716753599130444\n",
      "Epoch 116/300\n",
      "Average training loss: 0.026205922350287437\n",
      "Average test loss: 0.0024060141131695776\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02615610741244422\n",
      "Average test loss: 0.0025458404019268023\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02606005991001924\n",
      "Average test loss: 0.0025735381932722196\n",
      "Epoch 119/300\n",
      "Average training loss: 0.026048398007949194\n",
      "Average test loss: 0.002406127780675888\n",
      "Epoch 120/300\n",
      "Average training loss: 0.025972892050941786\n",
      "Average test loss: 0.0024851721968087884\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025947452811731234\n",
      "Average test loss: 0.002458907664443056\n",
      "Epoch 122/300\n",
      "Average training loss: 0.025889916650123065\n",
      "Average test loss: 0.0024045021663316422\n",
      "Epoch 123/300\n",
      "Average training loss: 0.025807109342681038\n",
      "Average test loss: 0.0024581393595370983\n",
      "Epoch 124/300\n",
      "Average training loss: 0.025775610980060367\n",
      "Average test loss: 0.002492182185045547\n",
      "Epoch 125/300\n",
      "Average training loss: 0.025703038146098454\n",
      "Average test loss: 0.0024890137428624764\n",
      "Epoch 126/300\n",
      "Average training loss: 0.025698583021759987\n",
      "Average test loss: 0.002474885947174496\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025590837468703588\n",
      "Average test loss: 0.002474134508313404\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025575943294498655\n",
      "Average test loss: 0.00250502705698212\n",
      "Epoch 129/300\n",
      "Average training loss: 0.025557839459843107\n",
      "Average test loss: 0.0024616244346317316\n",
      "Epoch 130/300\n",
      "Average training loss: 0.025499013089471393\n",
      "Average test loss: 0.0024838334071553415\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02545011161102189\n",
      "Average test loss: 0.0024877868882483907\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02538606621325016\n",
      "Average test loss: 0.0024504126153058474\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02540194818046358\n",
      "Average test loss: 0.0024547350069300997\n",
      "Epoch 134/300\n",
      "Average training loss: 0.025329139813780786\n",
      "Average test loss: 0.002496862458272113\n",
      "Epoch 135/300\n",
      "Average training loss: 0.025297450087136693\n",
      "Average test loss: 0.0024654092300269337\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02522553006807963\n",
      "Average test loss: 0.0025912820142176417\n",
      "Epoch 137/300\n",
      "Average training loss: 0.025262250492970147\n",
      "Average test loss: 0.0024134137011650536\n",
      "Epoch 138/300\n",
      "Average training loss: 0.025193186240063774\n",
      "Average test loss: 0.002413303615214924\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02510967289076911\n",
      "Average test loss: 0.0024781888849619363\n",
      "Epoch 140/300\n",
      "Average training loss: 0.025074975179301368\n",
      "Average test loss: 0.002503676453191373\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025057591479685573\n",
      "Average test loss: 0.0024517429541382526\n",
      "Epoch 142/300\n",
      "Average training loss: 0.025023486559589705\n",
      "Average test loss: 0.002482312041852209\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024982548957069715\n",
      "Average test loss: 0.0024541876665833923\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024978696755237048\n",
      "Average test loss: 0.0024791069254279136\n",
      "Epoch 145/300\n",
      "Average training loss: 0.024941817646225294\n",
      "Average test loss: 0.002468174608424306\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024851442371805508\n",
      "Average test loss: 0.00246417796653178\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024885215093692143\n",
      "Average test loss: 0.002570141076006823\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024775927800271245\n",
      "Average test loss: 0.0024236753744383654\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02481342900627189\n",
      "Average test loss: 0.0025107109068582454\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02477538447247611\n",
      "Average test loss: 0.0024362296803543963\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02470395068327586\n",
      "Average test loss: 0.0025935540864658025\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024688077131907144\n",
      "Average test loss: 0.0025054890939758883\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02462567233045896\n",
      "Average test loss: 0.0025413343703581228\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024619583141472605\n",
      "Average test loss: 0.002514358388880889\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024634401482012538\n",
      "Average test loss: 0.0025077976229497126\n",
      "Epoch 156/300\n",
      "Average training loss: 0.024538899504476124\n",
      "Average test loss: 0.0024719453827581474\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02453240863647726\n",
      "Average test loss: 0.0026436133950741756\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02449518693652418\n",
      "Average test loss: 0.0025757376052853134\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02444224027627044\n",
      "Average test loss: 0.002430197145582901\n",
      "Epoch 160/300\n",
      "Average training loss: 0.024454801759786076\n",
      "Average test loss: 0.0025413787954797346\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02437647177444564\n",
      "Average test loss: 0.0025261778154720862\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024451528504490852\n",
      "Average test loss: 0.002494153995687763\n",
      "Epoch 163/300\n",
      "Average training loss: 0.024357428267598152\n",
      "Average test loss: 0.002465006098151207\n",
      "Epoch 164/300\n",
      "Average training loss: 0.024302015761534373\n",
      "Average test loss: 0.00255408378255864\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024290639289551313\n",
      "Average test loss: 0.0025152615238395004\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024252236641115612\n",
      "Average test loss: 0.0025160153100474013\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024172002913223373\n",
      "Average test loss: 0.0026572681431555087\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02418712374733554\n",
      "Average test loss: 0.0025381077158057857\n",
      "Epoch 169/300\n",
      "Average training loss: 0.024206377625465392\n",
      "Average test loss: 0.002506607823901706\n",
      "Epoch 170/300\n",
      "Average training loss: 0.024175296346346537\n",
      "Average test loss: 0.0025563901557276647\n",
      "Epoch 171/300\n",
      "Average training loss: 0.024161274140079816\n",
      "Average test loss: 0.0025260322311272225\n",
      "Epoch 172/300\n",
      "Average training loss: 0.024079814228746627\n",
      "Average test loss: 0.0025004796000818413\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024092259681887097\n",
      "Average test loss: 0.0025521835380544265\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0240263558725516\n",
      "Average test loss: 0.002491968605460392\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02404508781598674\n",
      "Average test loss: 0.002536201520305541\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024027559906244278\n",
      "Average test loss: 0.0025653244157632193\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023973561013738313\n",
      "Average test loss: 0.002526939596670369\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023930268405212295\n",
      "Average test loss: 0.0025688579705440335\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02393987406293551\n",
      "Average test loss: 0.0024847724247309896\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02394799786143833\n",
      "Average test loss: 0.0025762025264816153\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023844378617074755\n",
      "Average test loss: 0.0026090881295709146\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023855699458056025\n",
      "Average test loss: 0.002620532698308428\n",
      "Epoch 183/300\n",
      "Average training loss: 0.023822706689437232\n",
      "Average test loss: 0.002533523027267721\n",
      "Epoch 184/300\n",
      "Average training loss: 0.023839856939183342\n",
      "Average test loss: 0.0024915160401206876\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023817445087763997\n",
      "Average test loss: 0.00244952000098096\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023741034910082816\n",
      "Average test loss: 0.002460532752590047\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02375518757601579\n",
      "Average test loss: 0.002492015184420678\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023705024603340362\n",
      "Average test loss: 0.002513908198931151\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023714732784363958\n",
      "Average test loss: 0.0025133693961219656\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02369718462559912\n",
      "Average test loss: 0.002531163919096192\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023629131060507562\n",
      "Average test loss: 0.0025422729891207483\n",
      "Epoch 192/300\n",
      "Average training loss: 0.023719017694393794\n",
      "Average test loss: 0.0025490849545846384\n",
      "Epoch 193/300\n",
      "Average training loss: 0.023647004841102492\n",
      "Average test loss: 0.002544637302143706\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023585568216111925\n",
      "Average test loss: 0.0025217203283682465\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023533755764365196\n",
      "Average test loss: 0.002467802612317933\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02350916347404321\n",
      "Average test loss: 0.002527164027094841\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023551166804300413\n",
      "Average test loss: 0.002591384352081352\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023551988548702663\n",
      "Average test loss: 0.002545150615481867\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02348056449658341\n",
      "Average test loss: 0.002472805612617069\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023423561645878686\n",
      "Average test loss: 0.0025792044879247747\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023443021540840468\n",
      "Average test loss: 0.0025554139588235152\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023466550649868117\n",
      "Average test loss: 0.002545870552253392\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02346129228340255\n",
      "Average test loss: 0.002539097873597509\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0234030631929636\n",
      "Average test loss: 0.002528437916396393\n",
      "Epoch 205/300\n",
      "Average training loss: 0.023348951677067412\n",
      "Average test loss: 0.0025931282540162404\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02337125107149283\n",
      "Average test loss: 0.002658173375038637\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023312800114353497\n",
      "Average test loss: 0.0025815834775567053\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023307092270917363\n",
      "Average test loss: 0.002536543521616194\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023292075569430987\n",
      "Average test loss: 0.0025173126957896684\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02328419652912352\n",
      "Average test loss: 0.0025293866925769383\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023328082415792678\n",
      "Average test loss: 0.0026285577492995396\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023231394155157938\n",
      "Average test loss: 0.0026548280101269485\n",
      "Epoch 213/300\n",
      "Average training loss: 0.023250707652833727\n",
      "Average test loss: 0.0026346103801495498\n",
      "Epoch 214/300\n",
      "Average training loss: 0.023181820296578937\n",
      "Average test loss: 0.0025671060778614547\n",
      "Epoch 215/300\n",
      "Average training loss: 0.023169678871830305\n",
      "Average test loss: 0.002639912088919017\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023117360479301876\n",
      "Average test loss: 0.002560682374363144\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023163436157835855\n",
      "Average test loss: 0.0025436526224431066\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02314793782764011\n",
      "Average test loss: 0.0025504359112431606\n",
      "Epoch 219/300\n",
      "Average training loss: 0.023112253250347242\n",
      "Average test loss: 0.002618720181700256\n",
      "Epoch 220/300\n",
      "Average training loss: 0.023090881316198242\n",
      "Average test loss: 0.002655269702482555\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02304441345565849\n",
      "Average test loss: 0.0025380770495782297\n",
      "Epoch 222/300\n",
      "Average training loss: 0.023100712056789135\n",
      "Average test loss: 0.0025964414905756713\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023045483915342224\n",
      "Average test loss: 0.002631187258495225\n",
      "Epoch 224/300\n",
      "Average training loss: 0.023059305606616867\n",
      "Average test loss: 0.002580005365320378\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022988925990131165\n",
      "Average test loss: 0.002595029609898726\n",
      "Epoch 226/300\n",
      "Average training loss: 0.022995732749501865\n",
      "Average test loss: 0.002549694726243615\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023011921596195962\n",
      "Average test loss: 0.002604538435737292\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02299465975165367\n",
      "Average test loss: 0.0025550721720792352\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0229234253068765\n",
      "Average test loss: 0.002619606011029747\n",
      "Epoch 230/300\n",
      "Average training loss: 0.022936829479204283\n",
      "Average test loss: 0.0026644973115374643\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022963275521993638\n",
      "Average test loss: 0.002604003993380401\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022871068408091864\n",
      "Average test loss: 0.002527649058856898\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02289613838493824\n",
      "Average test loss: 0.002602532937708828\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022901216526826224\n",
      "Average test loss: 0.0025681116305705572\n",
      "Epoch 235/300\n",
      "Average training loss: 0.022893192698558173\n",
      "Average test loss: 0.002609526865598228\n",
      "Epoch 236/300\n",
      "Average training loss: 0.022797127712104057\n",
      "Average test loss: 0.0026627706467277474\n",
      "Epoch 237/300\n",
      "Average training loss: 0.022833286402953994\n",
      "Average test loss: 0.0025513608718497885\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022824657440185546\n",
      "Average test loss: 0.002590683187254601\n",
      "Epoch 239/300\n",
      "Average training loss: 0.022792727465430894\n",
      "Average test loss: 0.0024629370905458925\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02276021819975641\n",
      "Average test loss: 0.0025257626082748176\n",
      "Epoch 241/300\n",
      "Average training loss: 0.022746326277653377\n",
      "Average test loss: 0.0025830684043467046\n",
      "Epoch 242/300\n",
      "Average training loss: 0.022796481602721744\n",
      "Average test loss: 0.0025857229549437763\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02270277394188775\n",
      "Average test loss: 0.002592659202714761\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02271198933571577\n",
      "Average test loss: 0.0026386000344322787\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022697759244177076\n",
      "Average test loss: 0.0025284078067375555\n",
      "Epoch 246/300\n",
      "Average training loss: 0.022680370817581813\n",
      "Average test loss: 0.0026350819046298663\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02271268754866388\n",
      "Average test loss: 0.0026270468899359306\n",
      "Epoch 248/300\n",
      "Average training loss: 0.022666981326209173\n",
      "Average test loss: 0.002552438440422217\n",
      "Epoch 249/300\n",
      "Average training loss: 0.022637374364667468\n",
      "Average test loss: 0.0025776612077736192\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022600452918145393\n",
      "Average test loss: 0.002575443874630663\n",
      "Epoch 251/300\n",
      "Average training loss: 0.022673885873622363\n",
      "Average test loss: 0.0026657971303082175\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02256082621547911\n",
      "Average test loss: 0.0025906832808007796\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02257184453143014\n",
      "Average test loss: 0.0025930647609962356\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022572748033536805\n",
      "Average test loss: 0.002569587553333905\n",
      "Epoch 255/300\n",
      "Average training loss: 0.022551900721258587\n",
      "Average test loss: 0.0026805744253926805\n",
      "Epoch 256/300\n",
      "Average training loss: 0.022524914811054866\n",
      "Average test loss: 0.005708779796544048\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02255786690943771\n",
      "Average test loss: 0.0025266330225600137\n",
      "Epoch 258/300\n",
      "Average training loss: 0.022509448021650315\n",
      "Average test loss: 0.0026327804691261714\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022494707672132387\n",
      "Average test loss: 0.0026640977499385677\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022534438459409608\n",
      "Average test loss: 0.0025505808424204587\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022500420237580934\n",
      "Average test loss: 0.0025761587303131817\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02252953699396716\n",
      "Average test loss: 0.002582158342003822\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02246213825047016\n",
      "Average test loss: 0.0025811872557840415\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022434139561322\n",
      "Average test loss: 0.0027487378962751893\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022444374155667093\n",
      "Average test loss: 0.0026167748851908578\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022422462903791004\n",
      "Average test loss: 0.0025994308650907545\n",
      "Epoch 267/300\n",
      "Average training loss: 0.022400156469808686\n",
      "Average test loss: 0.0025412062698354325\n",
      "Epoch 268/300\n",
      "Average training loss: 0.022395444861716695\n",
      "Average test loss: 0.0026371067414681115\n",
      "Epoch 269/300\n",
      "Average training loss: 0.022371172178122734\n",
      "Average test loss: 0.0027076015362723005\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02240568818979793\n",
      "Average test loss: 0.002586028811418348\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022348077831996813\n",
      "Average test loss: 0.0025991900391462777\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02233434027598964\n",
      "Average test loss: 0.0025709054625282684\n",
      "Epoch 273/300\n",
      "Average training loss: 0.022328871121009192\n",
      "Average test loss: 0.002601634612513913\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02231410778562228\n",
      "Average test loss: 0.002649028237702118\n",
      "Epoch 275/300\n",
      "Average training loss: 0.022336730115943487\n",
      "Average test loss: 0.0025593036140004796\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022269465838869414\n",
      "Average test loss: 0.0026199742710838717\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022296424720022412\n",
      "Average test loss: 0.002556060845756696\n",
      "Epoch 278/300\n",
      "Average training loss: 0.022283822778198455\n",
      "Average test loss: 0.002632215597977241\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022305909395217895\n",
      "Average test loss: 0.0025851093428209426\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022257447020875083\n",
      "Average test loss: 0.0025765302379926044\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022238439459767608\n",
      "Average test loss: 0.0026435059344189034\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022224571042590673\n",
      "Average test loss: 0.0025397238232609297\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02219577421082391\n",
      "Average test loss: 0.0031059826852546796\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02223434295422501\n",
      "Average test loss: 0.0027702419097638794\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022208832245734004\n",
      "Average test loss: 0.0026270807064655752\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02218791458176242\n",
      "Average test loss: 0.002606552958074543\n",
      "Epoch 287/300\n",
      "Average training loss: 0.022183688362439474\n",
      "Average test loss: 0.0025778253179871372\n",
      "Epoch 288/300\n",
      "Average training loss: 0.022152353434099093\n",
      "Average test loss: 0.0026131629050812786\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022173194262716505\n",
      "Average test loss: 0.002654382195323706\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022126719794339604\n",
      "Average test loss: 0.0025762320324364636\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02212628271182378\n",
      "Average test loss: 0.002583631109446287\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02213692210117976\n",
      "Average test loss: 0.002618552281624741\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022100246058570014\n",
      "Average test loss: 0.0026439989868344534\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022108552632232506\n",
      "Average test loss: 0.002597598548150725\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022108267640074095\n",
      "Average test loss: 0.0025699060074985025\n",
      "Epoch 296/300\n",
      "Average training loss: 0.022060868034760157\n",
      "Average test loss: 0.0026107175933818023\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02208138682610459\n",
      "Average test loss: 0.0026227939103005663\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022055067936579388\n",
      "Average test loss: 0.0026241568591859605\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02201140415088998\n",
      "Average test loss: 0.002638492372093929\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022057440006070666\n",
      "Average test loss: 0.0026749886096351675\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.203520736482408\n",
      "Average test loss: 0.005194703996595409\n",
      "Epoch 2/300\n",
      "Average training loss: 1.2648302308188544\n",
      "Average test loss: 0.003518918852425284\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8567425299750434\n",
      "Average test loss: 0.0033414429815279115\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6224972593519422\n",
      "Average test loss: 0.0029591071388373772\n",
      "Epoch 5/300\n",
      "Average training loss: 0.4919335033363766\n",
      "Average test loss: 0.002836657444118626\n",
      "Epoch 6/300\n",
      "Average training loss: 0.4004558748669095\n",
      "Average test loss: 0.005785926938056946\n",
      "Epoch 7/300\n",
      "Average training loss: 0.33284717599550884\n",
      "Average test loss: 0.0036215607929560873\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2804719900555081\n",
      "Average test loss: 0.058270277673171625\n",
      "Epoch 9/300\n",
      "Average training loss: 0.24009561067157323\n",
      "Average test loss: 0.0024263336358384953\n",
      "Epoch 10/300\n",
      "Average training loss: 0.20639749028947618\n",
      "Average test loss: 0.005136961482051346\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17837369146611956\n",
      "Average test loss: 0.002492143857810232\n",
      "Epoch 12/300\n",
      "Average training loss: 0.15458336436748504\n",
      "Average test loss: 0.002250736525696185\n",
      "Epoch 13/300\n",
      "Average training loss: 0.13380535327063667\n",
      "Average test loss: 0.002702811103521122\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11578829175896115\n",
      "Average test loss: 0.002112631054388152\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10127031988567776\n",
      "Average test loss: 0.0020589781653963858\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08828633208407297\n",
      "Average test loss: 0.0054988203239109785\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07777535496817695\n",
      "Average test loss: 0.0019948016748660142\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06893231019046571\n",
      "Average test loss: 0.001967863367042608\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06208532865842183\n",
      "Average test loss: 0.002043402492172188\n",
      "Epoch 20/300\n",
      "Average training loss: 0.056543685184584726\n",
      "Average test loss: 0.0019035652350220416\n",
      "Epoch 21/300\n",
      "Average training loss: 0.051994549920161566\n",
      "Average test loss: 0.0019157359411733018\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04847698172926903\n",
      "Average test loss: 0.0018814304133670198\n",
      "Epoch 23/300\n",
      "Average training loss: 0.045566124237245986\n",
      "Average test loss: 0.001862060367026263\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04322061911225319\n",
      "Average test loss: 0.001850849910846187\n",
      "Epoch 25/300\n",
      "Average training loss: 0.041187055243386166\n",
      "Average test loss: 0.0018201598516768879\n",
      "Epoch 26/300\n",
      "Average training loss: 0.039631125777959825\n",
      "Average test loss: 0.0017960151800264915\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0382510476940208\n",
      "Average test loss: 0.001760211116530829\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03716630204684204\n",
      "Average test loss: 0.001759461315969626\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03626242351200846\n",
      "Average test loss: 0.0017927574759556187\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03542186632917987\n",
      "Average test loss: 0.0017415698488346405\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03468128146065606\n",
      "Average test loss: 0.0017987816284100214\n",
      "Epoch 32/300\n",
      "Average training loss: 0.034052192096908886\n",
      "Average test loss: 0.0017504262701711721\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03345759799414211\n",
      "Average test loss: 0.0017339021430040399\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03292114312450091\n",
      "Average test loss: 0.0017182948083306353\n",
      "Epoch 35/300\n",
      "Average training loss: 0.032413207123676936\n",
      "Average test loss: 0.0017711553481510943\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03194596109787623\n",
      "Average test loss: 0.0016964661540049646\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03148582217097282\n",
      "Average test loss: 0.0016925609689205886\n",
      "Epoch 38/300\n",
      "Average training loss: 0.031030777409672736\n",
      "Average test loss: 0.0016837404755254586\n",
      "Epoch 39/300\n",
      "Average training loss: 0.030601866783367262\n",
      "Average test loss: 0.0016817875418605075\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03018074805703428\n",
      "Average test loss: 0.0027792902551591397\n",
      "Epoch 41/300\n",
      "Average training loss: 0.029875929989748532\n",
      "Average test loss: 0.0016651755809370015\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02939905711842908\n",
      "Average test loss: 0.0016672148601048523\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02901920810672972\n",
      "Average test loss: 0.0016866324046212766\n",
      "Epoch 44/300\n",
      "Average training loss: 0.028706496170825428\n",
      "Average test loss: 0.0016940164000002875\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02834247318903605\n",
      "Average test loss: 0.0017641864685962597\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02802956554128064\n",
      "Average test loss: 0.0017665826179501082\n",
      "Epoch 47/300\n",
      "Average training loss: 0.027701556543509167\n",
      "Average test loss: 0.0019344106560779941\n",
      "Epoch 48/300\n",
      "Average training loss: 0.027518154152565533\n",
      "Average test loss: 0.0016563168834480974\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02714762758877542\n",
      "Average test loss: 0.00226013264618814\n",
      "Epoch 50/300\n",
      "Average training loss: 0.026846398795644443\n",
      "Average test loss: 0.0017016347015483511\n",
      "Epoch 51/300\n",
      "Average training loss: 0.026667322958509126\n",
      "Average test loss: 0.00165341459090511\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02646133565571573\n",
      "Average test loss: 0.001669254101295438\n",
      "Epoch 53/300\n",
      "Average training loss: 0.026102612217267354\n",
      "Average test loss: 0.0016506361215271883\n",
      "Epoch 54/300\n",
      "Average training loss: 0.025880320578813554\n",
      "Average test loss: 0.0016485979903696313\n",
      "Epoch 55/300\n",
      "Average training loss: 0.025686541439758407\n",
      "Average test loss: 0.0016541495670874913\n",
      "Epoch 56/300\n",
      "Average training loss: 0.025466106540626948\n",
      "Average test loss: 0.001653402557079163\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025185756478044722\n",
      "Average test loss: 0.0016719891514836087\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02506856778760751\n",
      "Average test loss: 0.0016838361954109537\n",
      "Epoch 59/300\n",
      "Average training loss: 0.024837464271320237\n",
      "Average test loss: 0.0016923912698403002\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02461951388253106\n",
      "Average test loss: 0.0016649164665076468\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02443653293285105\n",
      "Average test loss: 0.00171572341873414\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02421699163483249\n",
      "Average test loss: 0.0016791594621414939\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02406303601298067\n",
      "Average test loss: 0.0016858854365742041\n",
      "Epoch 64/300\n",
      "Average training loss: 0.023927727687689995\n",
      "Average test loss: 0.0016700254993306267\n",
      "Epoch 65/300\n",
      "Average training loss: 0.023714312771956125\n",
      "Average test loss: 0.001836132787168026\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02325123242702749\n",
      "Average test loss: 0.0016703856501521337\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02302680878672335\n",
      "Average test loss: 0.0016876521000845565\n",
      "Epoch 70/300\n",
      "Average training loss: 0.023004350986745623\n",
      "Average test loss: 0.0018402824809567796\n",
      "Epoch 71/300\n",
      "Average training loss: 0.022783456094563007\n",
      "Average test loss: 0.001842451084819105\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022694203020797835\n",
      "Average test loss: 0.0017084119969771967\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022538190914524928\n",
      "Average test loss: 0.0017099895953304237\n",
      "Epoch 74/300\n",
      "Average training loss: 0.022369459754890866\n",
      "Average test loss: 0.0017114369888893432\n",
      "Epoch 75/300\n",
      "Average training loss: 0.022346414847506418\n",
      "Average test loss: 0.0017268554493784904\n",
      "Epoch 76/300\n",
      "Average training loss: 0.022170276451441977\n",
      "Average test loss: 0.0016832036210430992\n",
      "Epoch 77/300\n",
      "Average training loss: 0.022075664687487815\n",
      "Average test loss: 0.0017048378607465162\n",
      "Epoch 78/300\n",
      "Average training loss: 0.021904380411737496\n",
      "Average test loss: 0.0018143600720084375\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02181076776815785\n",
      "Average test loss: 0.0017093899307979478\n",
      "Epoch 80/300\n",
      "Average training loss: 0.021635391134354802\n",
      "Average test loss: 0.0017609084027094973\n",
      "Epoch 82/300\n",
      "Average training loss: 0.021565353688266543\n",
      "Average test loss: 0.001716076376537482\n",
      "Epoch 83/300\n",
      "Average training loss: 0.021411215749051835\n",
      "Average test loss: 0.0017627778716592325\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02131789025829898\n",
      "Average test loss: 0.0017246435485366318\n",
      "Epoch 85/300\n",
      "Average training loss: 0.021233794662687513\n",
      "Average test loss: 0.001708982025894026\n",
      "Epoch 86/300\n",
      "Average training loss: 0.021164116298158963\n",
      "Average test loss: 0.0017603186109724145\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02110968827870157\n",
      "Average test loss: 0.0017315037505080302\n",
      "Epoch 88/300\n",
      "Average training loss: 0.021014567931493124\n",
      "Average test loss: 0.0017712475573644043\n",
      "Epoch 89/300\n",
      "Average training loss: 0.020880907815363673\n",
      "Average test loss: 0.001739075235505071\n",
      "Epoch 90/300\n",
      "Average training loss: 0.020837571115957364\n",
      "Average test loss: 0.0018049216276655594\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020781537120540938\n",
      "Average test loss: 0.001742637223460608\n",
      "Epoch 92/300\n",
      "Average training loss: 0.020621387678715918\n",
      "Average test loss: 0.0018341761203482746\n",
      "Epoch 94/300\n",
      "Average training loss: 0.020525880773862203\n",
      "Average test loss: 0.0017652271521381205\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020467579727371534\n",
      "Average test loss: 0.0017786783079306284\n",
      "Epoch 96/300\n",
      "Average training loss: 0.020427384649713835\n",
      "Average test loss: 0.0018177987003905906\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0203499246471458\n",
      "Average test loss: 0.0017286468406932222\n",
      "Epoch 98/300\n",
      "Average training loss: 0.020251215517520904\n",
      "Average test loss: 0.0017734570449425115\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02020951139430205\n",
      "Average test loss: 0.0017645342567314704\n",
      "Epoch 100/300\n",
      "Average training loss: 0.020178363184134164\n",
      "Average test loss: 0.0017743735560733411\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02010619179573324\n",
      "Average test loss: 0.0017407870423048735\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02005084067583084\n",
      "Average test loss: 0.0017944103642884228\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019914928130805493\n",
      "Average test loss: 0.001770172546307246\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019897827163338662\n",
      "Average test loss: 0.001812324867480331\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019807447044385803\n",
      "Average test loss: 0.0018324665762484073\n",
      "Epoch 107/300\n",
      "Average training loss: 0.019799570507473416\n",
      "Average test loss: 0.0018234804158823357\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019730782714154984\n",
      "Average test loss: 0.0018214627140098149\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019711672274602784\n",
      "Average test loss: 0.0018028710784080129\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019680733038319482\n",
      "Average test loss: 0.001794851855892274\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01957887187600136\n",
      "Average test loss: 0.0017970991966625054\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01950683590107494\n",
      "Average test loss: 0.001771367597911093\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01947362104886108\n",
      "Average test loss: 0.0018257455194575919\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019504020005464553\n",
      "Average test loss: 0.0018746544967095057\n",
      "Epoch 115/300\n",
      "Average training loss: 0.019434662621882228\n",
      "Average test loss: 0.0017763457414176728\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01940244499180052\n",
      "Average test loss: 0.001807675591773457\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01928762783937984\n",
      "Average test loss: 0.0018004921071438325\n",
      "Epoch 119/300\n",
      "Average training loss: 0.019274013901750247\n",
      "Average test loss: 0.0018130102600488398\n",
      "Epoch 120/300\n",
      "Average training loss: 0.019236336360375087\n",
      "Average test loss: 0.0018468619276665978\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019155410339434943\n",
      "Average test loss: 0.0018728390294644568\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0191347244448132\n",
      "Average test loss: 0.00178415553458035\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019156485478083293\n",
      "Average test loss: 0.0018237164549322591\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019073488797578548\n",
      "Average test loss: 0.0018442153027281164\n",
      "Epoch 125/300\n",
      "Average training loss: 0.019021914730469387\n",
      "Average test loss: 0.0018005844133181705\n",
      "Epoch 126/300\n",
      "Average training loss: 0.019038993005951247\n",
      "Average test loss: 0.001786189849798878\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019009182820717493\n",
      "Average test loss: 0.001798480642442074\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018938584612475502\n",
      "Average test loss: 0.001854251219994492\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01890034106373787\n",
      "Average test loss: 0.0017907033076302872\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018831709068682458\n",
      "Average test loss: 0.0018652556911110878\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018792930223875577\n",
      "Average test loss: 0.001858054760015673\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018786270913150577\n",
      "Average test loss: 0.0018841373905953433\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018710915638340844\n",
      "Average test loss: 0.0018919459766604835\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01869643323454592\n",
      "Average test loss: 0.0018059308912812008\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018686670187446807\n",
      "Average test loss: 0.0018717721032185687\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018646164319581455\n",
      "Average test loss: 0.0018093846514821053\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01866765363762776\n",
      "Average test loss: 0.001816998085214032\n",
      "Epoch 139/300\n",
      "Average training loss: 0.018611244610614246\n",
      "Average test loss: 0.0018974658408098752\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018557331720987957\n",
      "Average test loss: 0.001860374683721198\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01854529381295045\n",
      "Average test loss: 0.0018392079431149694\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018534482141335806\n",
      "Average test loss: 0.0018510822807438672\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018401920169591905\n",
      "Average test loss: 0.0027096547287785346\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018402287441823217\n",
      "Average test loss: 0.0018828570546789302\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018380280538565583\n",
      "Average test loss: 0.001902152894789146\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018364555974801383\n",
      "Average test loss: 0.0018312988074289428\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018361192821628517\n",
      "Average test loss: 0.0018212428439615502\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01829177452872197\n",
      "Average test loss: 0.001873043962340388\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018337163676818213\n",
      "Average test loss: 0.0020625749977512494\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0182735611051321\n",
      "Average test loss: 0.0019563515002114906\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018215495662556754\n",
      "Average test loss: 0.0018519036583602429\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018183461459146604\n",
      "Average test loss: 0.0018520237586150566\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01816391861438751\n",
      "Average test loss: 0.001871799532117115\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018192425249351395\n",
      "Average test loss: 0.0018361204399002923\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018146251489718754\n",
      "Average test loss: 0.0018565188477643662\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018101477823323674\n",
      "Average test loss: 0.0018285928087102042\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0180736580275827\n",
      "Average test loss: 0.0018719773233557742\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018068511275781525\n",
      "Average test loss: 0.001838305896251566\n",
      "Epoch 162/300\n",
      "Average training loss: 0.018059324458241464\n",
      "Average test loss: 0.0018624497910754549\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01799678202635712\n",
      "Average test loss: 0.001829508956314789\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018013880928357442\n",
      "Average test loss: 0.0018391595267587238\n",
      "Epoch 165/300\n",
      "Average training loss: 0.018023677026232084\n",
      "Average test loss: 0.0020411193384271527\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017976338368323113\n",
      "Average test loss: 0.0018490987847455673\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017927770015266208\n",
      "Average test loss: 0.001841591437243753\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01793035223086675\n",
      "Average test loss: 0.001860319582124551\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017885886639356613\n",
      "Average test loss: 0.0019327232397368386\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017871554457479052\n",
      "Average test loss: 0.0034558386051406465\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017873296668132147\n",
      "Average test loss: 0.001889140648353431\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01784414018690586\n",
      "Average test loss: 0.019991279086305037\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017841848611831666\n",
      "Average test loss: 0.001897753136853377\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01781764460106691\n",
      "Average test loss: 0.0019446126661366886\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01781575370331605\n",
      "Average test loss: 0.0018622437728982833\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017772469616598553\n",
      "Average test loss: 0.00188200960898151\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017744436073634358\n",
      "Average test loss: 0.0018802215452823374\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01770270200818777\n",
      "Average test loss: 0.0018409509828521146\n",
      "Epoch 181/300\n",
      "Average training loss: 0.017698904053204588\n",
      "Average test loss: 0.001961969517999225\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01763461219271024\n",
      "Average test loss: 0.0019244714793231752\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01766547372771634\n",
      "Average test loss: 0.001902942263831695\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01763420636124081\n",
      "Average test loss: 0.004005972346497907\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017623095724317764\n",
      "Average test loss: 0.0018810090114259057\n",
      "Epoch 186/300\n",
      "Average training loss: 0.017597809827990002\n",
      "Average test loss: 0.0018522232899235355\n",
      "Epoch 187/300\n",
      "Average training loss: 0.017562402152352864\n",
      "Average test loss: 0.0018434143487571014\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01759301526347796\n",
      "Average test loss: 0.001929726594645116\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01753556707335843\n",
      "Average test loss: 0.0019280693307518958\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017540746573772695\n",
      "Average test loss: 0.001915926452105244\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017495665174391533\n",
      "Average test loss: 0.0018851341764546104\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017493630612889926\n",
      "Average test loss: 0.0018881944991234276\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01746068037963576\n",
      "Average test loss: 0.0018800431767271625\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01745200477209356\n",
      "Average test loss: 0.0018983239150709575\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01747480294936233\n",
      "Average test loss: 0.0019114506107030644\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017450102377269003\n",
      "Average test loss: 0.0018713089097291231\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01739864198780722\n",
      "Average test loss: 0.001895058509790235\n",
      "Epoch 199/300\n",
      "Average training loss: 0.017415357536739774\n",
      "Average test loss: 0.0028292172168278033\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017379633345537715\n",
      "Average test loss: 0.0019043490690075688\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01735744472510285\n",
      "Average test loss: 0.0019460592609312798\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017370905987090533\n",
      "Average training loss: 0.01733429770502779\n",
      "Average test loss: 0.0019022020559964908\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017328903741306727\n",
      "Average test loss: 0.001901635025938352\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01733184208224217\n",
      "Average test loss: 0.0018924046761045854\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01729529013733069\n",
      "Average test loss: 0.0018683745863123072\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01730864793144994\n",
      "Average test loss: 0.0019140388845569557\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017258532418145074\n",
      "Average test loss: 0.0019113252590306931\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017261297962731785\n",
      "Average test loss: 0.001865132665158146\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0172085396250089\n",
      "Average test loss: 0.001869174426007602\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01723182744781176\n",
      "Average test loss: 0.0018892980673764314\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017235668424102995\n",
      "Average test loss: 0.0019123911004927424\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017218020039300123\n",
      "Average test loss: 0.001967872314258582\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01717776631563902\n",
      "Average test loss: 0.0019327265282885896\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017164209549625716\n",
      "Average test loss: 0.001946058649983671\n",
      "Epoch 218/300\n",
      "Average training loss: 0.017140251704388196\n",
      "Average test loss: 0.0019551090920964876\n",
      "Epoch 219/300\n",
      "Average training loss: 0.017117199687494172\n",
      "Average test loss: 0.001911481938428349\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01714597033792072\n",
      "Average test loss: 0.0019313142370018695\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017124568722314305\n",
      "Average test loss: 0.0019092464722279044\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017109257626864646\n",
      "Average test loss: 0.002044934334543844\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017081052041716047\n",
      "Average test loss: 0.0019133095629513264\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01709412969648838\n",
      "Average test loss: 0.0019378307368606328\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017127815761499934\n",
      "Average test loss: 0.001918374081970089\n",
      "Epoch 226/300\n",
      "Average training loss: 0.017054286802808443\n",
      "Average test loss: 0.0018822765811863872\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01701936059527927\n",
      "Average test loss: 0.0019705743588921097\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01701207448873255\n",
      "Average test loss: 0.001941036846799155\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01706530042986075\n",
      "Average test loss: 0.0018541257785012325\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017002008995248213\n",
      "Average test loss: 0.0019685695465240212\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016976554972430072\n",
      "Average test loss: 0.0025861004148092534\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016982915543019773\n",
      "Average test loss: 0.0019189995672657257\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016944425978594357\n",
      "Average test loss: 0.0019185845199972391\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01698006082408958\n",
      "Average test loss: 0.001904363659883125\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016932769399550227\n",
      "Average test loss: 0.0019347794542296066\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01691537968152099\n",
      "Average test loss: 0.0019934434508904813\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016949601222243572\n",
      "Average test loss: 0.0019698366259949074\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01690881923172209\n",
      "Average test loss: 0.0020686978564287226\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016898470325602426\n",
      "Average test loss: 0.00187947155504177\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016891682755615978\n",
      "Average test loss: 0.0018813512280790342\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0168923274949193\n",
      "Average test loss: 0.0019729049012271894\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016851567338738178\n",
      "Average test loss: 0.001975351933079461\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016886827626162105\n",
      "Average test loss: 0.0019970383880039055\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016856091620193588\n",
      "Average test loss: 0.0018946823899944623\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01683954971863164\n",
      "Average test loss: 0.0020673864817039836\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016851541340351106\n",
      "Average test loss: 0.0019276609933003782\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01681789208783044\n",
      "Average test loss: 0.001901185828778479\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016825272354814742\n",
      "Average test loss: 0.001906821760866377\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016838391572237016\n",
      "Average test loss: 0.0018982285062472026\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016806304997868008\n",
      "Average test loss: 0.001916170377180808\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01675039085663027\n",
      "Average test loss: 0.0019209510210073657\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01677217425405979\n",
      "Average test loss: 0.001924756775713629\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01674399595376518\n",
      "Average test loss: 0.001933951725769374\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01675906527870231\n",
      "Average test loss: 0.0019482192624774245\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01677518894771735\n",
      "Average test loss: 0.00196032557491627\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01674312710762024\n",
      "Average test loss: 0.0019292511579891045\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01672282560500834\n",
      "Average test loss: 0.027392296705808904\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016716010653310354\n",
      "Average test loss: 0.0018937401624603404\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01673849990632799\n",
      "Average test loss: 0.0024259825606519976\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01668638931711515\n",
      "Average test loss: 0.0019153858396328159\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016684032168653277\n",
      "Average test loss: 0.0021387893576174976\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016709303996629184\n",
      "Average test loss: 0.0019595605704105564\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016658433395955297\n",
      "Average test loss: 0.001891147755086422\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01665867187579473\n",
      "Average test loss: 0.0019243387335704432\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016654930197530324\n",
      "Average test loss: 0.002057213999537958\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016658395714230008\n",
      "Average test loss: 0.0019484486393630505\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01662581011156241\n",
      "Average test loss: 0.0018888099092162318\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016625766307115555\n",
      "Average test loss: 0.001936014614223192\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016663665619161393\n",
      "Average test loss: 0.0019030713415187266\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016606781903240418\n",
      "Average test loss: 0.001902727263669173\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016599080799354446\n",
      "Average test loss: 0.001998329539472858\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01658484987831778\n",
      "Average test loss: 0.0020097856906553108\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016564558824731245\n",
      "Average test loss: 0.0018955445151983036\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016531823746860026\n",
      "Average test loss: 0.0019023202093732027\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01655094045566188\n",
      "Average test loss: 0.0020132100531417463\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016560710499684016\n",
      "Average test loss: 0.0019832943913837275\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016542863665355578\n",
      "Average test loss: 0.0019418145792765749\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016538144510653285\n",
      "Average test loss: 0.0025062825087871815\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016559892404410574\n",
      "Average test loss: 0.0018996566850692034\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01653075069685777\n",
      "Average test loss: 0.001928041426671876\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016540040507912634\n",
      "Average test loss: 0.001957875619538956\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016483987260195943\n",
      "Average test loss: 0.0019080507268922197\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016505579052699938\n",
      "Average test loss: 0.0020022847683479387\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016514315757486554\n",
      "Average test loss: 0.0018945940108969807\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016481132884820303\n",
      "Average test loss: 0.001957159544962148\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016498109194967484\n",
      "Average test loss: 0.002014759281857146\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016476238712668418\n",
      "Average test loss: 0.0019530439846631553\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01648850199414624\n",
      "Average test loss: 0.0019102677812592851\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016479056848420038\n",
      "Average test loss: 0.001911818134287993\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01644058595680528\n",
      "Average test loss: 0.0019344488569638795\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016443952914741305\n",
      "Average test loss: 0.002806234679495295\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016454439716206658\n",
      "Average test loss: 0.002029974734203683\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016417631602949567\n",
      "Average test loss: 0.002000841147369809\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01644821409963899\n",
      "Average test loss: 0.0019342970550060273\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01638760797182719\n",
      "Average test loss: 0.0019906466426327825\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016414803324474227\n",
      "Average test loss: 0.0019642067708902887\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016418803950150808\n",
      "Average test loss: 0.0019363441891554329\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01638520351714558\n",
      "Average test loss: 0.0019409088939945731\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016409230771991943\n",
      "Average test loss: 0.0019539287129624024\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive_Depth10-.025/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.35\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.20\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.39\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.02\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.15\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.24\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.41\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 32.951022449069555\n",
      "Average test loss: 0.007526970128218333\n",
      "Epoch 2/300\n",
      "Average training loss: 16.002623407999675\n",
      "Average test loss: 0.005512813715885083\n",
      "Epoch 3/300\n",
      "Average training loss: 12.553012383355036\n",
      "Average test loss: 0.151429182285236\n",
      "Epoch 4/300\n",
      "Average training loss: 9.751518324957953\n",
      "Average test loss: 0.008843458876013757\n",
      "Epoch 5/300\n",
      "Average training loss: 8.335578926086425\n",
      "Average test loss: 0.9092359509401852\n",
      "Epoch 6/300\n",
      "Average training loss: 6.032805817074246\n",
      "Average test loss: 478.3390896566576\n",
      "Epoch 7/300\n",
      "Average training loss: 4.958695643107096\n",
      "Average test loss: 0.23822252231008476\n",
      "Epoch 8/300\n",
      "Average training loss: 4.229195634841919\n",
      "Average test loss: 0.005379078603867027\n",
      "Epoch 9/300\n",
      "Average training loss: 3.5607689442104764\n",
      "Average test loss: 0.007609495194007953\n",
      "Epoch 10/300\n",
      "Average training loss: 3.2380026660495336\n",
      "Average test loss: 0.022673201911151408\n",
      "Epoch 11/300\n",
      "Average training loss: 2.695148953967624\n",
      "Average test loss: 0.00439195151709848\n",
      "Epoch 12/300\n",
      "Average training loss: 2.5070664382510714\n",
      "Average test loss: 0.004277079736606943\n",
      "Epoch 13/300\n",
      "Average training loss: 2.1226995328267417\n",
      "Average test loss: 0.5017498235263759\n",
      "Epoch 14/300\n",
      "Average training loss: 1.7585314727359347\n",
      "Average test loss: 0.060279434179680215\n",
      "Epoch 15/300\n",
      "Average training loss: 1.4681148329840765\n",
      "Average test loss: 0.0060631173344122035\n",
      "Epoch 16/300\n",
      "Average training loss: 1.2810978447596233\n",
      "Average test loss: 0.00425098968980213\n",
      "Epoch 17/300\n",
      "Average training loss: 1.104658184475369\n",
      "Average test loss: 0.004205009361935986\n",
      "Epoch 18/300\n",
      "Average training loss: 0.9073817333645291\n",
      "Average test loss: 0.007314231663114495\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7392265339427524\n",
      "Average test loss: 0.004166288900292582\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6563760879834493\n",
      "Average test loss: 0.004147984059320556\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5553038813802931\n",
      "Average test loss: 0.004139085327585538\n",
      "Epoch 22/300\n",
      "Average training loss: 0.4061602939499749\n",
      "Average test loss: 0.004136836454065309\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3544961722426944\n",
      "Average test loss: 0.004608065151919921\n",
      "Epoch 25/300\n",
      "Average training loss: 0.30831566243701514\n",
      "Average test loss: 0.004077828395697806\n",
      "Epoch 26/300\n",
      "Average training loss: 0.24819438229666815\n",
      "Average test loss: 0.00429239373612735\n",
      "Epoch 28/300\n",
      "Average training loss: 0.20754342612955307\n",
      "Average test loss: 0.004669794119480583\n",
      "Epoch 30/300\n",
      "Average training loss: 0.17945834979746078\n",
      "Average test loss: 0.004027807593345642\n",
      "Epoch 32/300\n",
      "Average training loss: 0.16946266570356158\n",
      "Average test loss: 0.004037081100046635\n",
      "Epoch 33/300\n",
      "Average training loss: 0.16081888462437524\n",
      "Average test loss: 0.004016837209049199\n",
      "Epoch 34/300\n",
      "Average training loss: 0.15382285263140996\n",
      "Average test loss: 0.00401269474832548\n",
      "Epoch 35/300\n",
      "Average training loss: 0.14798332760069105\n",
      "Average test loss: 0.00401759282209807\n",
      "Epoch 36/300\n",
      "Average training loss: 0.14335133935345543\n",
      "Average test loss: 0.0042727047014567586\n",
      "Epoch 37/300\n",
      "Average training loss: 0.13985170233249664\n",
      "Average test loss: 0.003990578278278311\n",
      "Epoch 38/300\n",
      "Average training loss: 0.13662925844722323\n",
      "Average test loss: 0.00399917307909992\n",
      "Epoch 39/300\n",
      "Average training loss: 0.13342783578899173\n",
      "Average test loss: 0.00402548006673654\n",
      "Epoch 40/300\n",
      "Average training loss: 0.13122593393590715\n",
      "Average test loss: 0.003974977906379434\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12933930556641685\n",
      "Average test loss: 0.003991031820989317\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12785891384548612\n",
      "Average test loss: 0.004000784269223611\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1265977176758978\n",
      "Average test loss: 0.003968530010018084\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12552904833687675\n",
      "Average test loss: 0.003988266791320509\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12478747125466665\n",
      "Average test loss: 0.004046596061852243\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12372989571756787\n",
      "Average test loss: 0.0040110870252052944\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12312248083617952\n",
      "Average test loss: 0.003978120219376352\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12200511624415715\n",
      "Average test loss: 0.004008621893202265\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12140222154723274\n",
      "Average test loss: 0.003999564489349723\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12092162428299585\n",
      "Average test loss: 0.0039683030491901765\n",
      "Epoch 52/300\n",
      "Average training loss: 0.12038860107792748\n",
      "Average test loss: 0.003977583124612769\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11996532336208555\n",
      "Average test loss: 0.0040617998861190345\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1194902811580234\n",
      "Average test loss: 0.004004703902949889\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1187826455367936\n",
      "Average test loss: 0.003979236792152127\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11855885110961066\n",
      "Average test loss: 0.004001479514150156\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11790253788232803\n",
      "Average test loss: 0.003992339475287331\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11764912390708923\n",
      "Average test loss: 0.0040406565457168555\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11734894663095474\n",
      "Average test loss: 0.004043406409314937\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11674666602081722\n",
      "Average test loss: 0.004056481706392434\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11611819745434655\n",
      "Average test loss: 0.004041159897628758\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11583307327826818\n",
      "Average test loss: 0.003962505779332584\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11543523236778047\n",
      "Average test loss: 0.004020770717412234\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11501167930497064\n",
      "Average test loss: 0.004012967439368367\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11451528804832034\n",
      "Average test loss: 0.004060525000095367\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11385753975311914\n",
      "Average test loss: 0.004058650664778219\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11348392187224494\n",
      "Average test loss: 0.0039886069481985436\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11330828707747989\n",
      "Average test loss: 0.003991228794886006\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1129632909099261\n",
      "Average test loss: 0.004103674495385753\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11253576958179475\n",
      "Average test loss: 0.004065069300019079\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11174244740936491\n",
      "Average test loss: 0.004069054558045335\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11174281380573908\n",
      "Average test loss: 0.004177097484469414\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11095765542984008\n",
      "Average test loss: 0.004058145056996081\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11067231237226062\n",
      "Average test loss: 0.004093176854360434\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11051407228575813\n",
      "Average test loss: 0.004019249612465501\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10979846539762285\n",
      "Average test loss: 0.004164905346102185\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10926418366034826\n",
      "Average test loss: 0.003994686897016234\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10894486945205265\n",
      "Average test loss: 0.00410278325362338\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10851060886515511\n",
      "Average test loss: 0.004221064609164993\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10830023902654648\n",
      "Average test loss: 0.004084902685549524\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10791063745154275\n",
      "Average test loss: 0.004112758986237976\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10775786460108228\n",
      "Average test loss: 0.004035941338373555\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10754435447189543\n",
      "Average test loss: 0.004211841716948482\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10678056097692913\n",
      "Average test loss: 0.004145410747577747\n",
      "Epoch 90/300\n",
      "Average training loss: 0.1063248028225369\n",
      "Average test loss: 0.00409435924473736\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10604984764258067\n",
      "Average test loss: 0.004122845117002725\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10572131746345097\n",
      "Average test loss: 0.00409363482085367\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10554103807608287\n",
      "Average test loss: 0.004203087604915103\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10508882423241933\n",
      "Average test loss: 0.004112862356834941\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10441780361202029\n",
      "Average test loss: 0.004089555447921157\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10415990526808633\n",
      "Average test loss: 0.00419961558158199\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10397245543532901\n",
      "Average test loss: 0.004115703547580374\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10341363609499402\n",
      "Average test loss: 0.004128818684981929\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10345119604137208\n",
      "Average test loss: 0.004164983597273628\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10308148170179791\n",
      "Average test loss: 0.004175296449619862\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10255053317546844\n",
      "Average training loss: 0.10221373728911082\n",
      "Average test loss: 0.004133106373664406\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10209756499528885\n",
      "Average training loss: 0.10174735995795992\n",
      "Average test loss: 0.0042150779730743835\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10150666577286191\n",
      "Average test loss: 0.004175846457481384\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10147290951013566\n",
      "Average test loss: 0.004402922256539266\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10097515210840437\n",
      "Average test loss: 0.004129977956414223\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10053480465544595\n",
      "Average test loss: 0.004186221855382125\n",
      "Epoch 110/300\n",
      "Average training loss: 0.1001685035692321\n",
      "Average test loss: 0.004312429911560482\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09998701768451267\n",
      "Average test loss: 0.004249155284216007\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09989115426275465\n",
      "Average test loss: 0.004258005228307512\n",
      "Epoch 113/300\n",
      "Average test loss: 0.00425415794013275\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0993382147649924\n",
      "Average test loss: 0.004323401557074653\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09905821557177438\n",
      "Average test loss: 0.004158344006579783\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09866129912932714\n",
      "Average test loss: 0.004313310161232948\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09843565534883075\n",
      "Average test loss: 0.004208835711909665\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09856216837962468\n",
      "Average test loss: 0.004247429322451353\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09759959593746398\n",
      "Average test loss: 0.004311522700513402\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09751185735729005\n",
      "Average test loss: 0.004195468524263965\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09720809823936886\n",
      "Average test loss: 0.004156614700539244\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09695142732063929\n",
      "Average test loss: 0.004243672634992334\n",
      "Epoch 124/300\n",
      "Average training loss: 0.09671239261494742\n",
      "Average test loss: 0.004360986047734817\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09652986911270353\n",
      "Average test loss: 0.004183664047469696\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09598597106668684\n",
      "Average test loss: 0.004246594329675038\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09577931827969022\n",
      "Average test loss: 0.004400582272766365\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09545941079325146\n",
      "Average test loss: 0.004404214391691817\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09549307596683503\n",
      "Average test loss: 0.004332200268076526\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09517182754808003\n",
      "Average test loss: 0.004217965256008837\n",
      "Epoch 132/300\n",
      "Average training loss: 0.094734439863099\n",
      "Average test loss: 0.004296198202917973\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09429609560304218\n",
      "Average test loss: 0.004172430114613639\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09428807181782192\n",
      "Average test loss: 0.004442705387249589\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09422888816396395\n",
      "Average test loss: 0.004408574131420917\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09405023968882031\n",
      "Average test loss: 0.004369617996116479\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09383556777238845\n",
      "Average test loss: 0.004354394187943803\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09323654701974657\n",
      "Average test loss: 0.004400938568015893\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0932597952749994\n",
      "Average test loss: 0.004263386397726006\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09307701989677217\n",
      "Average test loss: 0.0043024408041189115\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09286296856734488\n",
      "Average test loss: 0.0043221985263129075\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09246717728508844\n",
      "Average test loss: 0.0043416436821636225\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09240010906590355\n",
      "Average test loss: 0.0042953185161782636\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09206166797876358\n",
      "Average test loss: 0.004183602356248432\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09184328525596194\n",
      "Average test loss: 0.0043192089032381776\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0915978097319603\n",
      "Average test loss: 0.004250862500733799\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09137272944715288\n",
      "Average test loss: 0.0042698373819390935\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0911200933655103\n",
      "Average test loss: 0.0042844210635456775\n",
      "Epoch 151/300\n",
      "Average test loss: 0.004215890462199847\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09107910998000039\n",
      "Average test loss: 0.004335704933438036\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09059676264392005\n",
      "Average test loss: 0.004290475294407871\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09048453792598513\n",
      "Average test loss: 0.0043166334409680636\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0902540065712399\n",
      "Average test loss: 0.0042455468736588954\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09014023987121052\n",
      "Average test loss: 0.004298225119296047\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09007600680324766\n",
      "Average test loss: 0.004351019603096777\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0895272905561659\n",
      "Average test loss: 0.004283624220846428\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0892665606074863\n",
      "Average test loss: 0.004240237940102816\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08918798049953248\n",
      "Average test loss: 0.0043139320378088285\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08906748925977283\n",
      "Average test loss: 0.004365065229228801\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08894545198811425\n",
      "Average test loss: 0.0043124494780268936\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08852486879958046\n",
      "Average test loss: 0.004245639938654171\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08847697342766656\n",
      "Average test loss: 0.004322680590467321\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08836561397711436\n",
      "Average test loss: 0.004412278214676512\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08805118474033144\n",
      "Average test loss: 0.0044287080818580255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0879285215139389\n",
      "Average test loss: 0.00462064015534189\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08779780872662862\n",
      "Average test loss: 0.004255557179037067\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08786894982390933\n",
      "Average test loss: 0.004531117080814309\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08741842506991493\n",
      "Average test loss: 0.004326906215813425\n",
      "Epoch 174/300\n",
      "Average test loss: 0.004245026363266839\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08715892904334598\n",
      "Average test loss: 0.0043890273539970315\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08695420817865265\n",
      "Average test loss: 0.0044470723672873445\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08663628498050902\n",
      "Average test loss: 0.004291073533602887\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08663441390461392\n",
      "Average test loss: 0.00448789354869061\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08649141220913993\n",
      "Average test loss: 0.0042183470692899495\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08622851416799757\n",
      "Average test loss: 0.004484006174736553\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08626413589715957\n",
      "Average test loss: 0.00435445385488371\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08613833218812943\n",
      "Average test loss: 0.004495435836414496\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08587016620900896\n",
      "Average test loss: 0.0044056870862841605\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08568358689877723\n",
      "Average test loss: 0.0043785479242602985\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08560933221711053\n",
      "Average test loss: 0.004330406432557437\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08561184047990375\n",
      "Average test loss: 0.004305444390616483\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08524309233824412\n",
      "Average test loss: 0.004347876077724828\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08511325664652719\n",
      "Average test loss: 0.0043219321606059865\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08503577298588223\n",
      "Average test loss: 0.00429503229384621\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08498153890503778\n",
      "Average test loss: 0.004252299589829312\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08477035421795315\n",
      "Average test loss: 0.004352553163137701\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08465094963709513\n",
      "Average test loss: 0.004338484755406777\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08487058703435792\n",
      "Average test loss: 0.004519750167512231\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08431816825270652\n",
      "Average test loss: 0.004428051382717159\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08432030095656713\n",
      "Average test loss: 0.004406887355984913\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08427688508232435\n",
      "Average test loss: 0.004357596362216605\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08388973523510827\n",
      "Average test loss: 0.004386835961706108\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08401384494039747\n",
      "Average test loss: 0.004459110368457106\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08369443943103154\n",
      "Average test loss: 0.004567648458398051\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08336053713825015\n",
      "Average test loss: 0.004321339029197892\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08358736564053429\n",
      "Average test loss: 0.00429561990333928\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08330438334080909\n",
      "Average test loss: 0.004335255832307868\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08308173506127463\n",
      "Average test loss: 0.004251744936323828\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08298216541608175\n",
      "Average test loss: 0.004433165124307076\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08304492988189062\n",
      "Average test loss: 0.004467615072925885\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08284887205229866\n",
      "Average test loss: 0.004387688645058208\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08256492143207127\n",
      "Average test loss: 0.00428192649078038\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08334910672240788\n",
      "Average test loss: 0.004242721391220888\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08242631721165446\n",
      "Average test loss: 0.004379786108930906\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08203657415840361\n",
      "Average test loss: 0.004497500223004156\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0820707062681516\n",
      "Average test loss: 0.004396694878737132\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08225417290793526\n",
      "Average test loss: 0.0043375478862888285\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08179983998669518\n",
      "Average test loss: 0.004319728640218576\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08160674663384755\n",
      "Average test loss: 0.004452694952074025\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08178890013032489\n",
      "Average test loss: 0.004317126750118203\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08137847346398565\n",
      "Average test loss: 0.0043949648133582535\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08128502808345689\n",
      "Average test loss: 0.00436568443559938\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08166158042351405\n",
      "Average test loss: 0.004443205006006691\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08115845158365037\n",
      "Average test loss: 0.004358592678482334\n",
      "Epoch 224/300\n",
      "Average test loss: 0.004461633612505264\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08095944372812906\n",
      "Average test loss: 0.004376553814030356\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08083469877971543\n",
      "Average test loss: 0.004319098928322395\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0807154476179017\n",
      "Average test loss: 0.00433772014412615\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08089696603682306\n",
      "Average test loss: 0.004484426153202852\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08065166803201039\n",
      "Average test loss: 0.004344876514333818\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08053148317337036\n",
      "Average test loss: 0.004436242865191566\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08047034335136413\n",
      "Average test loss: 0.004590803891006443\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0802577646209134\n",
      "Average test loss: 0.004386942467962702\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08022945398754544\n",
      "Average test loss: 0.004556220867567592\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0802333437833521\n",
      "Average test loss: 0.004340208147135045\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08006591803497738\n",
      "Average test loss: 0.004360253450564212\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08002464091115527\n",
      "Average test loss: 0.004307243049558666\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07972732779052523\n",
      "Average test loss: 0.004392821555750237\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07978969633579254\n",
      "Average test loss: 0.004526418597333961\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0796543317106035\n",
      "Average test loss: 0.004385704495426681\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07953771475288603\n",
      "Average test loss: 0.004255952082574368\n",
      "Epoch 241/300\n",
      "Average training loss: 0.079348347041342\n",
      "Average test loss: 0.0044320158155428035\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07940490043494436\n",
      "Average test loss: 0.004447640585609609\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07937881268395318\n",
      "Average test loss: 0.004306865598592493\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07936353106631173\n",
      "Average test loss: 0.004372808208896054\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07912001370721393\n",
      "Average test loss: 0.004429005447361204\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07890118330054813\n",
      "Average test loss: 0.0043502287310030726\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07898754598697026\n",
      "Average test loss: 0.004501422122741739\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07873974873622258\n",
      "Average test loss: 0.004361225754850441\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07887715480062697\n",
      "Average test loss: 0.0043768788377443945\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07874038380384445\n",
      "Average test loss: 0.004354161195456982\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07857721698946422\n",
      "Average test loss: 0.004388690967319741\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07877855492300458\n",
      "Average test loss: 0.004351334765139553\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0784029460284445\n",
      "Average test loss: 0.004434374573950966\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07853809548748864\n",
      "Average test loss: 0.004346075834913386\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07813884907298618\n",
      "Average test loss: 0.004390501109676229\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07828744606839286\n",
      "Average test loss: 0.00436282381373975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07811003658506606\n",
      "Average test loss: 0.004336950158493386\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0779469881190194\n",
      "Average test loss: 0.0043531935827599635\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07792961351076762\n",
      "Average test loss: 0.0044135084533029134\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07807516316572825\n",
      "Average test loss: 0.0043673193231225016\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07758247119850582\n",
      "Average test loss: 0.0043106581705311935\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07781190653642019\n",
      "Average test loss: 0.004506933444076114\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07774769785006841\n",
      "Average test loss: 0.004438910307983557\n",
      "Epoch 264/300\n",
      "Average training loss: 0.077758653819561\n",
      "Average test loss: 0.004389728447629346\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0774075210293134\n",
      "Average test loss: 0.004619455284956429\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0777678244445059\n",
      "Average test loss: 0.004416716345275442\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07734803607066472\n",
      "Average test loss: 0.004319721596522464\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07699083644151687\n",
      "Average test loss: 0.004311903635246886\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07709326942761739\n",
      "Average test loss: 0.004439929279188315\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07735359212756157\n",
      "Average test loss: 0.004524884801357984\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07703015542692608\n",
      "Average test loss: 0.004502901072303454\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07685596150822109\n",
      "Average test loss: 0.004369212658041053\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07667030605342653\n",
      "Average test loss: 0.004379211449788676\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07679501854711109\n",
      "Average test loss: 0.004334242977926301\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07661442806985644\n",
      "Average test loss: 0.004358480721298191\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07656633704900742\n",
      "Average test loss: 0.004565827081186904\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07648023420903417\n",
      "Average test loss: 0.004387021429008908\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07660748716857699\n",
      "Average test loss: 0.004636653106245729\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07639271722237269\n",
      "Average test loss: 0.0043164124194946555\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07628334202369054\n",
      "Average test loss: 0.004425102437535922\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07637512003713184\n",
      "Average test loss: 0.00436415621638298\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07638280251953337\n",
      "Average test loss: 0.004455780473434263\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07617098000976774\n",
      "Average test loss: 0.0044480538467566175\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07589891553587384\n",
      "Average test loss: 0.004473663808570968\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07644669986764589\n",
      "Average test loss: 0.004366692484666904\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07578452297714021\n",
      "Average test loss: 0.004308364786414637\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07579907910691368\n",
      "Average test loss: 0.004285406147440275\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07569112669759326\n",
      "Average test loss: 0.004532536207180884\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07575741215546926\n",
      "Average test loss: 0.004352559253573417\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07631422038210763\n",
      "Average test loss: 0.004369348193622297\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07545698311593797\n",
      "Average test loss: 0.004414194156312281\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07579159690936406\n",
      "Average test loss: 0.004572639720928338\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07624354218443234\n",
      "Average test loss: 0.0043361009195860886\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07518254517184364\n",
      "Average test loss: 0.004470144410514169\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07519176470571094\n",
      "Average test loss: 0.0042764039321078195\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07512928728924857\n",
      "Average test loss: 0.004471716472672091\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07519237854745653\n",
      "Average test loss: 0.004442310373609265\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07514219456248813\n",
      "Average test loss: 0.00438003471493721\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0750984207590421\n",
      "Average test loss: 0.004347499554148979\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07514242528875668\n",
      "Average test loss: 0.0044170021327833335\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 31.195666765001086\n",
      "Average test loss: 0.08181015889843304\n",
      "Epoch 2/300\n",
      "Average training loss: 19.393441516452366\n",
      "Average test loss: 0.005515544706748592\n",
      "Epoch 3/300\n",
      "Average training loss: 16.596447880215116\n",
      "Average test loss: 0.008626131890134678\n",
      "Epoch 4/300\n",
      "Average training loss: 11.803795215182834\n",
      "Average test loss: 0.0043653401426143115\n",
      "Epoch 5/300\n",
      "Average training loss: 9.196788585238986\n",
      "Average test loss: 0.005042137284245756\n",
      "Epoch 6/300\n",
      "Average training loss: 7.74456817457411\n",
      "Average test loss: 0.04811280774490701\n",
      "Epoch 7/300\n",
      "Average training loss: 6.684129178788927\n",
      "Average test loss: 0.07659213429854976\n",
      "Epoch 8/300\n",
      "Average training loss: 5.30247546810574\n",
      "Average test loss: 0.004053955882580744\n",
      "Epoch 9/300\n",
      "Average training loss: 4.650174534691705\n",
      "Average test loss: 0.008508427041272323\n",
      "Epoch 10/300\n",
      "Average training loss: 4.308298346201579\n",
      "Average test loss: 0.0038922186887098685\n",
      "Epoch 11/300\n",
      "Average training loss: 3.8291021563212078\n",
      "Average test loss: 0.0037265255159387987\n",
      "Epoch 12/300\n",
      "Average training loss: 3.297911333931817\n",
      "Average test loss: 0.0036733343369430966\n",
      "Epoch 13/300\n",
      "Average training loss: 3.027642708036635\n",
      "Average test loss: 1.4208424587349098\n",
      "Epoch 14/300\n",
      "Average training loss: 2.7891739726596407\n",
      "Average test loss: 0.12782418284068506\n",
      "Epoch 15/300\n",
      "Average training loss: 2.4306793140835232\n",
      "Average test loss: 0.003509905185757412\n",
      "Epoch 16/300\n",
      "Average training loss: 2.108362488640679\n",
      "Average test loss: 0.003469583188907968\n",
      "Epoch 17/300\n",
      "Average training loss: 1.7669419073528714\n",
      "Average test loss: 0.00374415466023816\n",
      "Epoch 18/300\n",
      "Average training loss: 1.4355814494027033\n",
      "Average test loss: 0.384235420136609\n",
      "Epoch 19/300\n",
      "Average training loss: 1.2957792439990574\n",
      "Average test loss: 7.9578566154208445\n",
      "Epoch 20/300\n",
      "Average training loss: 1.099773611916436\n",
      "Average test loss: 0.005892626834412416\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9273896861606175\n",
      "Average test loss: 0.003320476418774989\n",
      "Epoch 22/300\n",
      "Average training loss: 0.8004744617144267\n",
      "Average test loss: 0.008071799268739091\n",
      "Epoch 23/300\n",
      "Average training loss: 0.685337019443512\n",
      "Average test loss: 0.0032530960484097402\n",
      "Epoch 24/300\n",
      "Average training loss: 0.569338991218143\n",
      "Average test loss: 0.03160839672055509\n",
      "Epoch 25/300\n",
      "Average training loss: 0.4820567911465963\n",
      "Average test loss: 0.0033225527389181986\n",
      "Epoch 26/300\n",
      "Average training loss: 0.4175292496681213\n",
      "Average test loss: 0.32903146193756\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3658050463464525\n",
      "Average test loss: 0.003150011668395665\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3237485172483656\n",
      "Average test loss: 0.003136854978899161\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2886149538622962\n",
      "Average test loss: 0.008514974056846565\n",
      "Epoch 30/300\n",
      "Average training loss: 0.2595938800705804\n",
      "Average test loss: 0.003124398562229342\n",
      "Epoch 31/300\n",
      "Average training loss: 0.23391449623637728\n",
      "Average test loss: 0.0031105862551679213\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2127961890300115\n",
      "Average test loss: 0.0030839038941388327\n",
      "Epoch 33/300\n",
      "Average training loss: 0.19360794982645246\n",
      "Average test loss: 0.0030867926134831376\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1775649719503191\n",
      "Average test loss: 0.0030855160200347503\n",
      "Epoch 35/300\n",
      "Average training loss: 0.16360197593106163\n",
      "Average test loss: 0.0030652211494743824\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15212210513485802\n",
      "Average test loss: 0.0030652348388814266\n",
      "Epoch 37/300\n",
      "Average training loss: 0.14225165719456143\n",
      "Average test loss: 0.003070772980650266\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1341379036837154\n",
      "Average test loss: 0.003073991090266241\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12760750885804495\n",
      "Average test loss: 0.0030523296093775167\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12233455046017964\n",
      "Average test loss: 0.003088117909928163\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1175654392639796\n",
      "Average test loss: 0.003031841184116072\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11396308408180873\n",
      "Average test loss: 0.003083894799773892\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1107711961666743\n",
      "Average test loss: 0.003042180724028084\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10830997749831942\n",
      "Average test loss: 0.0031056517207374177\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10620771698819266\n",
      "Average test loss: 0.003091774040626155\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1042091745071941\n",
      "Average test loss: 0.003030463553758131\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10249594421850311\n",
      "Average test loss: 0.0030510541544192366\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10121715931097666\n",
      "Average test loss: 0.0030527114677760337\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09996748789151509\n",
      "Average test loss: 0.0030061790821039013\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09894768027464548\n",
      "Average test loss: 0.003052680005215936\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09777944257524278\n",
      "Average test loss: 0.003122899519279599\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09701010580857594\n",
      "Average test loss: 0.0030165091363920107\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09618354639079836\n",
      "Average test loss: 0.0031363801062107084\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09531834560632706\n",
      "Average test loss: 0.0031085280360033116\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09461455863714219\n",
      "Average test loss: 0.00301939122689267\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0937841569185257\n",
      "Average test loss: 0.0030067346607231434\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09283420756790373\n",
      "Average test loss: 0.0031747788099779025\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09241650726397832\n",
      "Average test loss: 0.0030126222071962224\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0916269063618448\n",
      "Average test loss: 0.0030274920811255773\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09084717741939756\n",
      "Average test loss: 0.0030386264419390097\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09032050103611416\n",
      "Average test loss: 0.0030666202114274102\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08991808298561309\n",
      "Average test loss: 0.002997950836809145\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08909762950075997\n",
      "Average test loss: 0.0032201689299609926\n",
      "Epoch 64/300\n",
      "Average training loss: 0.088677407529619\n",
      "Average test loss: 0.003133502084347937\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08792006384664111\n",
      "Average test loss: 0.0030598195511847733\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08753739294078615\n",
      "Average test loss: 0.003043953613481588\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08680045900742213\n",
      "Average test loss: 0.0030751906238082384\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08621182604299651\n",
      "Average test loss: 0.0031698532707782256\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0855644208656417\n",
      "Average test loss: 0.0030780084480841953\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08514650845527649\n",
      "Average test loss: 0.0031532052078594762\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08459476837846967\n",
      "Average test loss: 0.0030195053952435653\n",
      "Epoch 72/300\n",
      "Average training loss: 0.084042697277334\n",
      "Average test loss: 0.0030593937498827775\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08333437884847324\n",
      "Average test loss: 0.0031255698390305043\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08296787419584062\n",
      "Average test loss: 0.003088103589705295\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0829666322072347\n",
      "Average test loss: 0.0031336379926651717\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08271939555472797\n",
      "Average test loss: 0.003131118525026573\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08177092954516411\n",
      "Average test loss: 0.003094939526791374\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08110325161616007\n",
      "Average test loss: 0.0031754698521561093\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08059561285045412\n",
      "Average test loss: 0.00309403871693131\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08024101889795728\n",
      "Average test loss: 0.003063757413791286\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07975544583135181\n",
      "Average test loss: 0.003085365265193913\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0794059786465433\n",
      "Average test loss: 0.003102005682264765\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0789340248240365\n",
      "Average test loss: 0.0031348245485375327\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07857045730617311\n",
      "Average test loss: 0.003162296783592966\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07860000457697444\n",
      "Average test loss: 0.003125081800752216\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07877093544271258\n",
      "Average test loss: 0.0030765659250319003\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07735593472917875\n",
      "Average test loss: 0.0031183933665355048\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07758257948689991\n",
      "Average test loss: 0.0031850678416796858\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07668520710865656\n",
      "Average test loss: 0.003072320806586908\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07630567145678732\n",
      "Average test loss: 0.003159122500775589\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0759655444820722\n",
      "Average test loss: 0.0031329255741503505\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07567939058939616\n",
      "Average test loss: 0.003057353499242001\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07533927073743608\n",
      "Average test loss: 0.003092075885273516\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07516972572273678\n",
      "Average test loss: 0.0030865673513876066\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0750393669406573\n",
      "Average test loss: 0.0032034883155590958\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07439881010187997\n",
      "Average test loss: 0.0031588110658857557\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07430934590763516\n",
      "Average test loss: 0.003216988234470288\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07368852808740405\n",
      "Average test loss: 0.0031048138748026558\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07356786183847322\n",
      "Average test loss: 0.0031604217019759948\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07326115606890785\n",
      "Average test loss: 0.0031785237749831544\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0730111002193557\n",
      "Average test loss: 0.003178560151615077\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07276145441995727\n",
      "Average test loss: 0.0032388436405195132\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0725228692624304\n",
      "Average test loss: 0.0032238236562245424\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07216236164834765\n",
      "Average test loss: 0.003172587439417839\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0718863939974043\n",
      "Average test loss: 0.003235524794086814\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07172820472055011\n",
      "Average test loss: 0.003284171425840921\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0713048145307435\n",
      "Average test loss: 0.00316207142567469\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0710231985118654\n",
      "Average test loss: 0.0031563427549683386\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07098504465487268\n",
      "Average test loss: 0.003237383908074763\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07052025902271271\n",
      "Average test loss: 0.003137372759688232\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07036915378438102\n",
      "Average test loss: 0.0033435454635570446\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07017943782607715\n",
      "Average test loss: 0.0032801224461032285\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06991760571135415\n",
      "Average test loss: 0.0032575897768967682\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06967387668291727\n",
      "Average test loss: 0.003104957011114392\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06935275402002865\n",
      "Average test loss: 0.003242829085017244\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06962927053703202\n",
      "Average test loss: 0.003188485144327084\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06913103612263997\n",
      "Average test loss: 0.003304278007398049\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06901859070857366\n",
      "Average test loss: 0.0032354302685707807\n",
      "Epoch 119/300\n",
      "Average training loss: 0.068597683860196\n",
      "Average test loss: 0.0033765062501447066\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06830162896712622\n",
      "Average test loss: 0.003221258267760277\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06816885894205835\n",
      "Average test loss: 0.0032506286116937796\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06808199063936869\n",
      "Average test loss: 0.0032605802029785184\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06782026642229821\n",
      "Average test loss: 0.00329742441409164\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06764841706222958\n",
      "Average test loss: 0.0032113384577549167\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06754508744676908\n",
      "Average test loss: 0.0033041223757382895\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06728076405657663\n",
      "Average test loss: 0.003230133716017008\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06683923293815719\n",
      "Average test loss: 0.0032558450287001\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06669325783517625\n",
      "Average test loss: 0.003301927121149169\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06664165698488553\n",
      "Average test loss: 0.0033119372764809263\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06629328832361434\n",
      "Average test loss: 0.0033227706491533253\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0662667839328448\n",
      "Average test loss: 0.0032722343198127216\n",
      "Epoch 133/300\n",
      "Average training loss: 0.065900350689888\n",
      "Average test loss: 0.003201330423769024\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06560403663913408\n",
      "Average test loss: 0.00335523344286614\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06546368467145496\n",
      "Average test loss: 0.003265794038358662\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06545509204268456\n",
      "Average test loss: 0.0032947129437492953\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06517821137441529\n",
      "Average test loss: 0.0032261070604953502\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06513085838490063\n",
      "Average test loss: 0.0032770441782971225\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06479091468122271\n",
      "Average test loss: 0.003248709702657329\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06485563458667862\n",
      "Average test loss: 0.003233921724475092\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06453815411527952\n",
      "Average test loss: 0.003419227202940318\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06463517721494039\n",
      "Average test loss: 0.003318062094350656\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06442042410704825\n",
      "Average test loss: 0.0032598096968399154\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06423001823160383\n",
      "Average test loss: 0.0032792838004728157\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06414895925919215\n",
      "Average test loss: 0.0032351539871758885\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06385457998845312\n",
      "Average test loss: 0.0031252600234001875\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06407764094736841\n",
      "Average test loss: 0.00326598205903752\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06372710115379758\n",
      "Average test loss: 0.0032866492147247\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06367876119746102\n",
      "Average test loss: 0.0032399826852811705\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06338872420125538\n",
      "Average test loss: 0.0032618774767551157\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06323435048262278\n",
      "Average test loss: 0.003336973883211613\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06308739444944593\n",
      "Average test loss: 0.0033900787600626547\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0631471851170063\n",
      "Average test loss: 0.0032031173977173037\n",
      "Epoch 155/300\n",
      "Average training loss: 0.062876123699877\n",
      "Average test loss: 0.003303437633646859\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0627261814309491\n",
      "Average test loss: 0.003265326549195581\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0626676264140341\n",
      "Average test loss: 0.0032969578239652846\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06258730552925004\n",
      "Average test loss: 0.0033395264759245847\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06241737043195301\n",
      "Average test loss: 0.0032311708918876117\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06232142770290375\n",
      "Average test loss: 0.003268207421940234\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06208489472005102\n",
      "Average test loss: 0.0033956244827972517\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06213781445556217\n",
      "Average test loss: 0.0032783230259600614\n",
      "Epoch 163/300\n",
      "Average training loss: 0.062013573480976955\n",
      "Average test loss: 0.0031899136019249755\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06193842103746202\n",
      "Average test loss: 0.0032248921593030296\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0620145816107591\n",
      "Average test loss: 0.0032685580814464225\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0616503198908435\n",
      "Average test loss: 0.0032956509846780034\n",
      "Epoch 167/300\n",
      "Average training loss: 0.061486085921525956\n",
      "Average test loss: 0.0033634984874063066\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06144136697716183\n",
      "Average test loss: 0.0033904869386719334\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06147413746184773\n",
      "Average test loss: 0.003273246886001693\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06121697164244122\n",
      "Average test loss: 0.0032530801697737643\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06139946748812993\n",
      "Average test loss: 0.0032081854858746132\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06112469860580232\n",
      "Average test loss: 0.0032497688097258407\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06087183818552229\n",
      "Average test loss: 0.0033731530319071478\n",
      "Epoch 174/300\n",
      "Average training loss: 0.060718838565879396\n",
      "Average test loss: 0.003350293994156851\n",
      "Epoch 175/300\n",
      "Average training loss: 0.060722116265032024\n",
      "Average test loss: 0.003224699644578828\n",
      "Epoch 176/300\n",
      "Average training loss: 0.060494085126452976\n",
      "Average test loss: 0.0033905098976360426\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06041558723979526\n",
      "Average test loss: 0.003272511648428109\n",
      "Epoch 178/300\n",
      "Average training loss: 0.060289622214105394\n",
      "Average test loss: 0.0033064769990742205\n",
      "Epoch 179/300\n",
      "Average training loss: 0.060357348412275316\n",
      "Average test loss: 0.00325653796022137\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06034836586978701\n",
      "Average test loss: 0.003288948071913587\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06009926505221261\n",
      "Average test loss: 0.003277806064527896\n",
      "Epoch 182/300\n",
      "Average training loss: 0.060088275445832146\n",
      "Average test loss: 0.0034022920752565067\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05978740517960655\n",
      "Average test loss: 0.0032648922459532818\n",
      "Epoch 184/300\n",
      "Average training loss: 0.059832382023334504\n",
      "Average test loss: 0.0033048139450450736\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05980601447820663\n",
      "Average test loss: 0.003301544279274013\n",
      "Epoch 186/300\n",
      "Average training loss: 0.059613334092828964\n",
      "Average test loss: 0.003347739526381095\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05954793368776639\n",
      "Average test loss: 0.0033544410640994706\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05960025547610389\n",
      "Average test loss: 0.0032995042208996085\n",
      "Epoch 189/300\n",
      "Average training loss: 0.059738820718394384\n",
      "Average test loss: 0.0032703539228273763\n",
      "Epoch 190/300\n",
      "Average training loss: 0.059120445092519125\n",
      "Average test loss: 0.0034086637430720858\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05897983854015668\n",
      "Average test loss: 0.003318457008856866\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05909052033887969\n",
      "Average test loss: 0.003489108937482039\n",
      "Epoch 193/300\n",
      "Average training loss: 0.059048551936944325\n",
      "Average test loss: 0.0032175018213068444\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0590167704489496\n",
      "Average test loss: 0.003269503807235095\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05893470796942711\n",
      "Average test loss: 0.0033224434109611644\n",
      "Epoch 196/300\n",
      "Average training loss: 0.058955511278576324\n",
      "Average test loss: 0.0033029741489638883\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05885199631916152\n",
      "Average test loss: 0.0033349482168753943\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05858480455146896\n",
      "Average test loss: 0.003386704686615202\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05843758174114757\n",
      "Average test loss: 0.003249604439569844\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05841656092471546\n",
      "Average test loss: 0.0033946905119551555\n",
      "Epoch 201/300\n",
      "Average training loss: 0.058344479670127236\n",
      "Average test loss: 0.0033241120365758736\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05837071615457535\n",
      "Average test loss: 0.003319585188188487\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05828459647628996\n",
      "Average test loss: 0.003440362314176228\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05810800377527873\n",
      "Average test loss: 0.0033935581354631317\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05801401731703017\n",
      "Average test loss: 0.003375889281845755\n",
      "Epoch 206/300\n",
      "Average training loss: 0.058016334570116464\n",
      "Average test loss: 0.0033233504355367686\n",
      "Epoch 207/300\n",
      "Average training loss: 0.057953228483597435\n",
      "Average test loss: 0.003501592152648502\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05783554605642954\n",
      "Average test loss: 0.0032282493830555017\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05774191449085871\n",
      "Average test loss: 0.0033402724627198442\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0576936647494634\n",
      "Average test loss: 0.0033932900480512116\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05754881965782907\n",
      "Average test loss: 0.003282671607616875\n",
      "Epoch 212/300\n",
      "Average training loss: 0.057586731034848425\n",
      "Average test loss: 0.0032994686315457027\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0574411251909203\n",
      "Average test loss: 0.0033554224535408948\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05739125615358353\n",
      "Average test loss: 0.003337622224042813\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05735329703158802\n",
      "Average test loss: 0.0033863369238873323\n",
      "Epoch 216/300\n",
      "Average training loss: 0.057266191949446994\n",
      "Average test loss: 0.0033436402465320298\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05716514148314794\n",
      "Average test loss: 0.0034000938282244737\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05703022645579444\n",
      "Average test loss: 0.0033770798018409147\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05692641351620356\n",
      "Average test loss: 0.0036874596282011935\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0569978675643603\n",
      "Average test loss: 0.003347440362390545\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05707278710272577\n",
      "Average test loss: 0.0033869634125795627\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05697784676485591\n",
      "Average test loss: 0.0033814071327861813\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05686030765705639\n",
      "Average test loss: 0.0033114043921232223\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05669960230588913\n",
      "Average test loss: 0.0033503293991088865\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05679527322782411\n",
      "Average test loss: 0.0033915850760208237\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05652465679248174\n",
      "Average test loss: 0.0032672829404473305\n",
      "Epoch 227/300\n",
      "Average training loss: 0.056454296390215555\n",
      "Average test loss: 0.0032988873877459104\n",
      "Epoch 228/300\n",
      "Average training loss: 0.056410668128066596\n",
      "Average test loss: 0.0035377547037270334\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05652406866020627\n",
      "Average test loss: 0.0033373320268260107\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05633820445670022\n",
      "Average test loss: 0.003487182264940606\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05617141161031193\n",
      "Average test loss: 0.003317640236682362\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05625536366303762\n",
      "Average test loss: 0.003488948454459508\n",
      "Epoch 233/300\n",
      "Average training loss: 0.056128960589567824\n",
      "Average test loss: 0.0034261295329779387\n",
      "Epoch 234/300\n",
      "Average training loss: 0.056114394893248876\n",
      "Average test loss: 0.003305000266681115\n",
      "Epoch 235/300\n",
      "Average training loss: 0.056093452682097755\n",
      "Average test loss: 0.0033966473158862855\n",
      "Epoch 236/300\n",
      "Average training loss: 0.055984613670243155\n",
      "Average test loss: 0.0033773413019047844\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05581690413090918\n",
      "Average test loss: 0.0035803520555297533\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05592779948976305\n",
      "Average test loss: 0.003434420110864772\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05574494747651948\n",
      "Average test loss: 0.0033113845125254658\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05564253695474731\n",
      "Average test loss: 0.0034931737368719446\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05562193296684159\n",
      "Average test loss: 0.003441635030425257\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05564390043086476\n",
      "Average test loss: 0.0034620094750490455\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05554835215542051\n",
      "Average test loss: 0.0033670401753236852\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05555363646811909\n",
      "Average test loss: 0.003358147255041533\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05540208861562941\n",
      "Average test loss: 0.003365738530125883\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05532498136162758\n",
      "Average test loss: 0.0033815036709937784\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05525651425951057\n",
      "Average test loss: 0.003459986767007245\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05516148570179939\n",
      "Average test loss: 0.003342306929330031\n",
      "Epoch 249/300\n",
      "Average training loss: 0.055358360509077706\n",
      "Average test loss: 0.00343403016610278\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05528627027074496\n",
      "Average test loss: 0.0033611191351794532\n",
      "Epoch 251/300\n",
      "Average training loss: 0.055179449030094675\n",
      "Average test loss: 0.0033817933936499886\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05509517465035121\n",
      "Average test loss: 0.0033684832089477114\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05491077474090788\n",
      "Average test loss: 0.0034091800736884277\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05489914210306274\n",
      "Average test loss: 0.003421884135446615\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05495084575812022\n",
      "Average test loss: 0.003442684295276801\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05492575170927578\n",
      "Average test loss: 0.003316277877531118\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05477470778425535\n",
      "Average test loss: 0.0034223378580063584\n",
      "Epoch 258/300\n",
      "Average training loss: 0.054700384226110246\n",
      "Average test loss: 0.003515886852517724\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05468605945176548\n",
      "Average test loss: 0.0034417683302114407\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05457945765389337\n",
      "Average test loss: 0.003430947854701016\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05455339225464397\n",
      "Average test loss: 0.003422444482644399\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05451899570226669\n",
      "Average test loss: 0.0033782455902546644\n",
      "Epoch 263/300\n",
      "Average training loss: 0.054479133148988085\n",
      "Average test loss: 0.00335046015948885\n",
      "Epoch 264/300\n",
      "Average training loss: 0.054471155749426944\n",
      "Average test loss: 0.0034719664663490322\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05436116746730275\n",
      "Average test loss: 0.0034087722624341647\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05417166891031795\n",
      "Average test loss: 0.0034098359994176363\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05431193607052167\n",
      "Average test loss: 0.0034943641420039865\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05421783951918284\n",
      "Average test loss: 0.0033497625396897396\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05418755842579736\n",
      "Average test loss: 0.010843411477075683\n",
      "Epoch 270/300\n",
      "Average training loss: 0.054234155939684975\n",
      "Average test loss: 0.0033823002934869795\n",
      "Epoch 271/300\n",
      "Average training loss: 0.054181048548883864\n",
      "Average test loss: 0.003292614185147815\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05411441285742654\n",
      "Average test loss: 0.003359337650653389\n",
      "Epoch 273/300\n",
      "Average training loss: 0.053952232549587885\n",
      "Average test loss: 0.003388694239780307\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05406003734138277\n",
      "Average test loss: 0.0034131455059266754\n",
      "Epoch 276/300\n",
      "Average training loss: 0.053866806265380644\n",
      "Average test loss: 0.003578122397677766\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05386584664384524\n",
      "Average test loss: 0.0033235050352911156\n",
      "Epoch 278/300\n",
      "Average training loss: 0.053780820614761776\n",
      "Average test loss: 0.0034574337024241685\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05364640767375628\n",
      "Average test loss: 0.0033944645884136358\n",
      "Epoch 280/300\n",
      "Average training loss: 0.053535267482201256\n",
      "Average test loss: 0.003425331123173237\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05349718446864022\n",
      "Average test loss: 0.0033562229341930813\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05349731894996431\n",
      "Average test loss: 0.0037309751854174666\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05343621876835823\n",
      "Average training loss: 0.05343244631257322\n",
      "Average test loss: 0.0036591936515437233\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05333693379494879\n",
      "Average test loss: 0.0034837396705730095\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05331615306602584\n",
      "Average test loss: 0.0034466901653342776\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05324661221769121\n",
      "Average test loss: 0.0033937691859900953\n",
      "Epoch 290/300\n",
      "Average training loss: 0.053205870025687746\n",
      "Average test loss: 0.003368519350886345\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05312311361895667\n",
      "Average test loss: 0.0033632203133569824\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05318505545457204\n",
      "Average test loss: 0.003411634222086933\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05313499744402038\n",
      "Average test loss: 0.0033262828275975255\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05342734920316272\n",
      "Average test loss: 0.0033406269769701694\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05300603172514174\n",
      "Average test loss: 0.003351877073239949\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05309080228209496\n",
      "Average test loss: 0.0033539737947285175\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0528540558218956\n",
      "Average test loss: 0.0034175276690059236\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05279935123523077\n",
      "Average test loss: 0.003444629405521684\n",
      "Epoch 299/300\n",
      "Average training loss: 0.052779961678716875\n",
      "Average test loss: 0.003467694022382299\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05285702153378063\n",
      "Average test loss: 0.003343383751395676\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 23.035766186184354\n",
      "Average test loss: 0.05437422414579325\n",
      "Epoch 3/300\n",
      "Average training loss: 17.127253957960342\n",
      "Average test loss: 0.17785524201061992\n",
      "Epoch 4/300\n",
      "Average training loss: 13.08438205549452\n",
      "Average test loss: 0.0060360028545061745\n",
      "Epoch 5/300\n",
      "Average training loss: 10.023934695773654\n",
      "Average test loss: 0.007396967548049158\n",
      "Epoch 6/300\n",
      "Average training loss: 8.891296854654948\n",
      "Average test loss: 0.01518729480749203\n",
      "Epoch 7/300\n",
      "Average training loss: 8.154736293792725\n",
      "Average test loss: 0.017246620698728496\n",
      "Epoch 8/300\n",
      "Average training loss: 6.771046553717719\n",
      "Average test loss: 0.16298674423827064\n",
      "Epoch 10/300\n",
      "Average training loss: 5.8437476403978135\n",
      "Average test loss: 0.00837830240952058\n",
      "Epoch 11/300\n",
      "Average training loss: 5.186461945427789\n",
      "Average test loss: 0.137127303328779\n",
      "Epoch 12/300\n",
      "Average training loss: 4.602908492618137\n",
      "Average test loss: 0.0032264006415175066\n",
      "Epoch 13/300\n",
      "Average training loss: 3.968832883834839\n",
      "Average test loss: 0.0038946971295194493\n",
      "Epoch 14/300\n",
      "Average training loss: 3.3071232560475665\n",
      "Average test loss: 0.3564381514061242\n",
      "Epoch 15/300\n",
      "Average training loss: 2.777897432751126\n",
      "Average test loss: 0.0027926067397412327\n",
      "Epoch 16/300\n",
      "Average training loss: 2.5757334490882027\n",
      "Average test loss: 0.0043706698765357335\n",
      "Epoch 17/300\n",
      "Average training loss: 2.4306903739505343\n",
      "Average test loss: 47.46626618299219\n",
      "Epoch 18/300\n",
      "Average training loss: 2.024544804573059\n",
      "Average test loss: 0.1870064192575713\n",
      "Epoch 19/300\n",
      "Average training loss: 1.669649117787679\n",
      "Average test loss: 0.12475835241956844\n",
      "Epoch 20/300\n",
      "Average training loss: 1.3720891924964058\n",
      "Average test loss: 12.489176856877075\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9490285894076029\n",
      "Average test loss: 0.0026884158288853037\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7981132227049934\n",
      "Average test loss: 591.00572200897\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6819635904630025\n",
      "Average test loss: 0.11153252227273253\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5725171562300788\n",
      "Average test loss: 5.230376729896499\n",
      "Epoch 26/300\n",
      "Average training loss: 0.48860891641510856\n",
      "Average test loss: 0.0026558290500607756\n",
      "Epoch 27/300\n",
      "Average training loss: 0.41802536635928683\n",
      "Average test loss: 0.0024623633819735712\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3624054721461402\n",
      "Average test loss: 0.003169901604867644\n",
      "Epoch 29/300\n",
      "Average training loss: 0.31499086589283415\n",
      "Average test loss: 0.004859904805405272\n",
      "Epoch 30/300\n",
      "Average training loss: 0.27570881051487395\n",
      "Average test loss: 0.002370885777597626\n",
      "Epoch 31/300\n",
      "Average training loss: 0.24370452568266127\n",
      "Average test loss: 0.002407214538918601\n",
      "Epoch 32/300\n",
      "Average training loss: 0.21612078795168135\n",
      "Average test loss: 0.00234519156627357\n",
      "Epoch 33/300\n",
      "Average training loss: 0.19317951940165626\n",
      "Average test loss: 0.0023566229277186923\n",
      "Epoch 34/300\n",
      "Average training loss: 0.173543310046196\n",
      "Average training loss: 0.15754386032952203\n",
      "Average test loss: 0.002311601338080234\n",
      "Epoch 36/300\n",
      "Average training loss: 0.14402630502647823\n",
      "Average test loss: 0.002391448400190307\n",
      "Epoch 37/300\n",
      "Average training loss: 0.13272643839650683\n",
      "Average test loss: 0.0023311851672414277\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12339692152871026\n",
      "Average test loss: 0.0022926300337745083\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11570335155063205\n",
      "Average test loss: 0.0022776881203883225\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10920683185590638\n",
      "Average test loss: 0.002323914484017425\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0993805791007148\n",
      "Average test loss: 0.002319776696877347\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0923985398610433\n",
      "Average test loss: 0.0022549087109251156\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08972227912478976\n",
      "Average test loss: 0.002304524810984731\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08717235210206774\n",
      "Average test loss: 0.002263886692519817\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08534724811050627\n",
      "Average test loss: 0.00226934253041529\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08343098613950942\n",
      "Average test loss: 0.0022557320160170396\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08039090155230628\n",
      "Average test loss: 0.0022538244965382747\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07915297479761971\n",
      "Average test loss: 0.0022407006023244724\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07815464260843065\n",
      "Average test loss: 0.00229862009609739\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07720284104347229\n",
      "Average test loss: 0.0022323518532017866\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07596824283070035\n",
      "Average test loss: 0.002258948960031072\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07536837285757064\n",
      "Average test loss: 0.0022922252853297526\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0743591204782327\n",
      "Average test loss: 0.002277632204712265\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07342885241243574\n",
      "Average test loss: 0.0022491851998493076\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07291153942214118\n",
      "Average test loss: 0.002259010371234682\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07224536440107558\n",
      "Average test loss: 0.0022288006893876524\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07129652256766955\n",
      "Average test loss: 0.0023232687819335195\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0705846423374282\n",
      "Average test loss: 0.00232422449791597\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06969772219989034\n",
      "Average test loss: 0.002256694845855236\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06834827099906074\n",
      "Average test loss: 0.0022694147556192347\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0677471046646436\n",
      "Average test loss: 0.0023419970092881056\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06657134479946561\n",
      "Average test loss: 0.0022683259713360006\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06588875144720077\n",
      "Average test loss: 0.0023390940530225637\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06551477995846007\n",
      "Average test loss: 0.00229500647334175\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06482042980194092\n",
      "Average test loss: 0.0023610643823113705\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06423978263801998\n",
      "Average test loss: 0.0024039660518368087\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06374128472805023\n",
      "Average test loss: 0.00231782531655497\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06309358600113127\n",
      "Average test loss: 0.002255520096255673\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06258823728892539\n",
      "Average test loss: 0.0023009845986962318\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0625378302137057\n",
      "Average test loss: 0.0022820420757763914\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06169560366868973\n",
      "Average test loss: 0.002271405543718073\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06116502338316705\n",
      "Average test loss: 0.002326365234123336\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06065721564160453\n",
      "Average test loss: 0.0023336694070862397\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06032463692625364\n",
      "Average test loss: 0.002291896954385771\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05971356758144167\n",
      "Average test loss: 0.002273336928130852\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05953227933579021\n",
      "Average test loss: 0.0022988015808578996\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05890156156818072\n",
      "Average test loss: 0.002300589924885167\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05855215218000942\n",
      "Average test loss: 0.002735992305394676\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05818983108136389\n",
      "Average test loss: 0.0023030989430844785\n",
      "Epoch 85/300\n",
      "Average training loss: 0.057860371847947435\n",
      "Average test loss: 0.002319341077365809\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0575335895187325\n",
      "Average test loss: 0.0023428530347430043\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05689523745907678\n",
      "Average test loss: 0.0023804708365350964\n",
      "Epoch 89/300\n",
      "Average training loss: 0.055924265623092655\n",
      "Average test loss: 0.002298047594829566\n",
      "Epoch 91/300\n",
      "Average training loss: 0.055744655552837585\n",
      "Average test loss: 0.0024225456507669554\n",
      "Epoch 92/300\n",
      "Average training loss: 0.055504271974166235\n",
      "Average test loss: 0.00240204718336463\n",
      "Epoch 93/300\n",
      "Average training loss: 0.055157413128349514\n",
      "Average test loss: 0.0024526422681907813\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0549879929986265\n",
      "Average test loss: 0.0023999130073934795\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05454548039370113\n",
      "Average test loss: 0.002385274128884905\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0542504644493262\n",
      "Average test loss: 0.002351132297164036\n",
      "Epoch 97/300\n",
      "Average test loss: 0.002393551881528563\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05388156271311972\n",
      "Average test loss: 0.0023701882438941133\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05337844988703728\n",
      "Average test loss: 0.0023953484079490103\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05315576085117128\n",
      "Average test loss: 0.0023814763269490668\n",
      "Epoch 101/300\n",
      "Average training loss: 0.053167109075519775\n",
      "Average test loss: 0.0023657146990299226\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05273864962326156\n",
      "Average test loss: 0.0023423230926402742\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05255789597829183\n",
      "Average test loss: 0.002412716565653682\n",
      "Epoch 104/300\n",
      "Average training loss: 0.052392545968294145\n",
      "Average test loss: 0.002356208896367914\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0521157807442877\n",
      "Average test loss: 0.002345544823962781\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05183971208002832\n",
      "Average test loss: 0.0024052616856578323\n",
      "Epoch 107/300\n",
      "Average training loss: 0.051834058781464895\n",
      "Average test loss: 0.0023302097163266605\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05134818341996935\n",
      "Average test loss: 0.002396589895296428\n",
      "Epoch 109/300\n",
      "Average training loss: 0.051343059851063626\n",
      "Average test loss: 0.0023597190462880664\n",
      "Epoch 110/300\n",
      "Average training loss: 0.051016882217592666\n",
      "Average test loss: 0.0024106761159168348\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05081341368291113\n",
      "Average test loss: 0.002390712506448229\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05047389304969046\n",
      "Average test loss: 0.002408010788468851\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05027838397357199\n",
      "Average test loss: 0.002436531386234694\n",
      "Epoch 115/300\n",
      "Average training loss: 0.050100438584884006\n",
      "Average test loss: 0.0023828050970203345\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04997304754124748\n",
      "Average test loss: 0.002373792791739106\n",
      "Epoch 117/300\n",
      "Average training loss: 0.049850952198108034\n",
      "Average test loss: 0.002475679652247992\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0496565610534615\n",
      "Average test loss: 0.0023913150493883426\n",
      "Epoch 119/300\n",
      "Average training loss: 0.049364393322004214\n",
      "Average test loss: 0.002401594055816531\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0491651767525408\n",
      "Average test loss: 0.002390272656766077\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04898737203081449\n",
      "Average test loss: 0.002454717212667068\n",
      "Epoch 123/300\n",
      "Average training loss: 0.048854996227555805\n",
      "Average test loss: 0.002415378134490715\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04864449880189366\n",
      "Average test loss: 0.0024325246264537176\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04854971773756875\n",
      "Average test loss: 0.0023531267576747473\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04847037717368868\n",
      "Average test loss: 0.0024395482525643376\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0481421599553691\n",
      "Average test loss: 0.0024194497337771785\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04800965115096834\n",
      "Average test loss: 0.002454904997514354\n",
      "Epoch 130/300\n",
      "Average training loss: 0.047865394893619746\n",
      "Average test loss: 0.0027274089654286703\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04773177170753479\n",
      "Average test loss: 0.0024646361666835017\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04767373838689592\n",
      "Average test loss: 0.0026215071017957397\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0474846467408869\n",
      "Average test loss: 0.002414587944952978\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04724123137527042\n",
      "Average test loss: 0.002411409588220219\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04723080221480794\n",
      "Average test loss: 0.0024305620932330687\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04721582872668902\n",
      "Average test loss: 0.0025059029484788575\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04710263415177663\n",
      "Average test loss: 0.0024842122070905235\n",
      "Epoch 139/300\n",
      "Average training loss: 0.046726394454638165\n",
      "Average test loss: 0.0024791626381791302\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04667507255408499\n",
      "Average test loss: 0.0024605349676890508\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04646911240286297\n",
      "Average test loss: 0.0024516621014724176\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04647945551077525\n",
      "Average test loss: 0.0025039642432497606\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0462909957435396\n",
      "Average test loss: 0.002366895236488846\n",
      "Epoch 144/300\n",
      "Average training loss: 0.046215964280896714\n",
      "Average test loss: 0.002439443780730168\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04618763072623147\n",
      "Average test loss: 0.002458318155879776\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04604583138227463\n",
      "Average test loss: 0.0024709075130522253\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04586587744620111\n",
      "Average test loss: 0.0024784622462466358\n",
      "Epoch 149/300\n",
      "Average training loss: 0.045679573072327506\n",
      "Average test loss: 0.00251319209817383\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04568455829554134\n",
      "Average test loss: 0.0025631524893558687\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04547875536812676\n",
      "Average test loss: 0.0025432546397464143\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04535373862584432\n",
      "Average test loss: 0.002678274627774954\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04531523458328512\n",
      "Average test loss: 0.0024692897810083296\n",
      "Epoch 155/300\n",
      "Average training loss: 0.045108866675032507\n",
      "Average test loss: 0.0024233350773445435\n",
      "Epoch 156/300\n",
      "Average training loss: 0.045111656576395036\n",
      "Average test loss: 0.002464845414790842\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04483427782687876\n",
      "Average test loss: 0.002440099377805988\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04485281160308255\n",
      "Average test loss: 0.0024569074214539593\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04476002065837383\n",
      "Average test loss: 0.0025305644383447038\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04458845798671245\n",
      "Average test loss: 0.0025136515725817945\n",
      "Epoch 162/300\n",
      "Average training loss: 0.044536572419934806\n",
      "Average test loss: 0.0025377721527798307\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04453105234437519\n",
      "Average test loss: 0.0024604447920703225\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04443363006909688\n",
      "Average test loss: 0.0025104859069817595\n",
      "Epoch 165/300\n",
      "Average training loss: 0.044267802980211046\n",
      "Average test loss: 0.002525277110023631\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0443256451619996\n",
      "Average test loss: 0.00247712025915583\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04421201918522517\n",
      "Average test loss: 0.00249595359671447\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04414838457769818\n",
      "Average test loss: 0.002559866469974319\n",
      "Epoch 169/300\n",
      "Average training loss: 0.044108282655477525\n",
      "Average test loss: 0.00253049842806326\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04383218590418498\n",
      "Average test loss: 0.002531345096313291\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04389068301849895\n",
      "Average test loss: 0.003840835481054253\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04378342497017648\n",
      "Average test loss: 0.0026834646744860542\n",
      "Epoch 173/300\n",
      "Average training loss: 0.043792135977082786\n",
      "Average test loss: 0.002493666565666596\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0436545137696796\n",
      "Average test loss: 0.0024555779365408753\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04358838255869018\n",
      "Average test loss: 0.0024847121936165625\n",
      "Epoch 176/300\n",
      "Average training loss: 0.043425807704528176\n",
      "Average test loss: 0.0025509141130993763\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04346947254074945\n",
      "Average test loss: 0.0024545806985762386\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04343207316266166\n",
      "Average test loss: 0.002529047559739815\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04342714620629946\n",
      "Average test loss: 0.0024797141916222043\n",
      "Epoch 180/300\n",
      "Average training loss: 0.043216233058108226\n",
      "Average test loss: 0.0024615410914023716\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04319788110587332\n",
      "Average test loss: 0.002461879255870978\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04304460188746452\n",
      "Average test loss: 0.002494817796473702\n",
      "Epoch 183/300\n",
      "Average training loss: 0.043057111869255704\n",
      "Average test loss: 0.002511380650310053\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0430098082224528\n",
      "Average test loss: 0.0025030227226929534\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04297684650619825\n",
      "Average test loss: 0.002515572460471756\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0432876253525416\n",
      "Average test loss: 0.002428022861894634\n",
      "Epoch 187/300\n",
      "Average training loss: 0.042847740252812704\n",
      "Average test loss: 0.0024710748317754933\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04275239639812046\n",
      "Average test loss: 0.0024300856029407848\n",
      "Epoch 189/300\n",
      "Average training loss: 0.042507888158162435\n",
      "Average test loss: 0.0025030604168358775\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0425102981461419\n",
      "Average test loss: 0.0024817301728245287\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04265057149860594\n",
      "Average test loss: 0.0024556721918698814\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04259060801400079\n",
      "Average test loss: 0.002739288771525025\n",
      "Epoch 193/300\n",
      "Average training loss: 0.042467783980899385\n",
      "Average test loss: 0.002632802560304602\n",
      "Epoch 194/300\n",
      "Average training loss: 0.042258268528514435\n",
      "Average test loss: 0.0025217621798316638\n",
      "Epoch 195/300\n",
      "Average training loss: 0.042253957516617244\n",
      "Average test loss: 0.0024459398857628305\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04230893559257189\n",
      "Average test loss: 0.002487737650051713\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04214688136180242\n",
      "Average test loss: 0.002513694738555286\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04213598554001914\n",
      "Average test loss: 0.002526358184715112\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04203162231047948\n",
      "Average test loss: 0.0025898500811308623\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04199763630827268\n",
      "Average test loss: 0.002493086484571298\n",
      "Epoch 201/300\n",
      "Average training loss: 0.042212695274088115\n",
      "Average test loss: 0.0024443184340165723\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04192590392960442\n",
      "Average test loss: 0.002586251767559184\n",
      "Epoch 203/300\n",
      "Average training loss: 0.041829330030414794\n",
      "Average test loss: 0.0025444308295845984\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04187865132755703\n",
      "Average test loss: 0.002569097123419245\n",
      "Epoch 205/300\n",
      "Average training loss: 0.041821345683601165\n",
      "Average test loss: 0.0025197255447920825\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04168135949969292\n",
      "Average test loss: 0.002542010139570468\n",
      "Epoch 207/300\n",
      "Average training loss: 0.041669442471530704\n",
      "Average test loss: 0.0024903906049827734\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04152119299107128\n",
      "Average test loss: 0.0024705745118359726\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04152535317672623\n",
      "Average test loss: 0.0025101816903592812\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04146410494380527\n",
      "Average test loss: 0.00272748646305667\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04145290768146515\n",
      "Average test loss: 0.0027320016347285774\n",
      "Epoch 212/300\n",
      "Average training loss: 0.041453051156467864\n",
      "Average test loss: 0.0025199180791775386\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04140883655349414\n",
      "Average test loss: 0.0025457430626783105\n",
      "Epoch 214/300\n",
      "Average training loss: 0.041305074165264764\n",
      "Average test loss: 0.0024860280346539285\n",
      "Epoch 215/300\n",
      "Average training loss: 0.041179275436533824\n",
      "Average test loss: 0.0024781260252412824\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04119704200161828\n",
      "Average test loss: 0.002569468703948789\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04119471737742424\n",
      "Average test loss: 0.002494176291136278\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04111907523208194\n",
      "Average test loss: 0.0025389992536769975\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04103857169879807\n",
      "Average test loss: 0.002601558848387665\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04101491004228592\n",
      "Average test loss: 0.0025154253306488195\n",
      "Epoch 221/300\n",
      "Average training loss: 0.040926890323559444\n",
      "Average test loss: 0.0025196535376211006\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04090603012674385\n",
      "Average test loss: 0.002515355292707682\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04085942705141173\n",
      "Average test loss: 0.0025700094036550983\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04083557987213135\n",
      "Average test loss: 0.0025094416497482195\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04073680376675394\n",
      "Average test loss: 0.002599627744820383\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04073492865761121\n",
      "Average test loss: 0.0027032545026805664\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04069825065467093\n",
      "Average test loss: 0.0025285816500998206\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04061500511566798\n",
      "Average test loss: 0.0026325552008218235\n",
      "Epoch 229/300\n",
      "Average training loss: 0.040762933429744505\n",
      "Average test loss: 0.0029580120150413777\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04048656148049566\n",
      "Average test loss: 0.002528467017950283\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0404026263223754\n",
      "Average test loss: 0.002503313683076865\n",
      "Epoch 232/300\n",
      "Average training loss: 0.040488230579429206\n",
      "Average test loss: 0.002591368135685722\n",
      "Epoch 233/300\n",
      "Average training loss: 0.040463302764627666\n",
      "Average test loss: 0.0025860701211624674\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0405208557844162\n",
      "Average test loss: 0.0028190614281015263\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04035622211959627\n",
      "Average test loss: 0.0025724078520304628\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04032472840282652\n",
      "Average test loss: 0.0025373419736408527\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04029114183783531\n",
      "Average test loss: 0.003029549830282728\n",
      "Epoch 238/300\n",
      "Average training loss: 0.040224396670858066\n",
      "Average test loss: 0.0025525172110646963\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04027318966057566\n",
      "Average test loss: 0.002508932302809424\n",
      "Epoch 240/300\n",
      "Average training loss: 0.040180795901351506\n",
      "Average test loss: 0.0025226894136932162\n",
      "Epoch 241/300\n",
      "Average training loss: 0.040034070058001414\n",
      "Average test loss: 0.0025641901679337026\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04010398318039046\n",
      "Average test loss: 0.002593465212939514\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04010867905947897\n",
      "Average test loss: 0.002497203580621216\n",
      "Epoch 244/300\n",
      "Average training loss: 0.039924878276056716\n",
      "Average test loss: 0.00248000385488073\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03992001942131254\n",
      "Average test loss: 0.002639749171005355\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03992383989691734\n",
      "Average test loss: 0.00249727898887876\n",
      "Epoch 247/300\n",
      "Average training loss: 0.039854135973585975\n",
      "Average test loss: 0.0026078111401034725\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04004062558876144\n",
      "Average test loss: 0.0025899829901754857\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03989895522594452\n",
      "Average test loss: 0.002616123821793331\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03974112663004133\n",
      "Average test loss: 0.004431814499820272\n",
      "Epoch 251/300\n",
      "Average training loss: 0.039869231541951496\n",
      "Average test loss: 0.0025463686030771995\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03972338563534949\n",
      "Average test loss: 0.0027876700065616105\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0396772848798169\n",
      "Average test loss: 0.002590652664295501\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03966015888253848\n",
      "Average test loss: 0.002733171747583482\n",
      "Epoch 255/300\n",
      "Average training loss: 0.039559710956282086\n",
      "Average test loss: 0.002600524257040686\n",
      "Epoch 256/300\n",
      "Average training loss: 0.039538544565439225\n",
      "Average test loss: 0.002584144777721829\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03946449715561337\n",
      "Average test loss: 0.002739636353217065\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03954446096056038\n",
      "Average test loss: 0.0025583205343120627\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03948798291219605\n",
      "Average test loss: 0.002564983705058694\n",
      "Epoch 260/300\n",
      "Average training loss: 0.039396334648132324\n",
      "Average test loss: 0.0025424297984896435\n",
      "Epoch 261/300\n",
      "Average training loss: 0.039401729838715656\n",
      "Average test loss: 0.0025199658483680753\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03934782367282444\n",
      "Average test loss: 0.002739312971424725\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03930034288764\n",
      "Average test loss: 0.0025138805907012687\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03931484441955884\n",
      "Average test loss: 0.00263768948852602\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03930982469022274\n",
      "Average test loss: 0.0041886932243489555\n",
      "Epoch 266/300\n",
      "Average training loss: 0.039296354052093294\n",
      "Average test loss: 0.0025514114088275365\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03920784819788403\n",
      "Average test loss: 0.0025224886062658494\n",
      "Epoch 268/300\n",
      "Average training loss: 0.039215456144677266\n",
      "Average test loss: 0.0025312996030681665\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03921100043919351\n",
      "Average test loss: 0.0025337164281970925\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03899803482492765\n",
      "Average test loss: 0.002531735837459564\n",
      "Epoch 271/300\n",
      "Average training loss: 0.039049086279339264\n",
      "Average test loss: 0.002552823012901677\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03898274958464835\n",
      "Average test loss: 0.002563336552120745\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03895805146793525\n",
      "Average test loss: 0.002580630232993927\n",
      "Epoch 274/300\n",
      "Average training loss: 0.039007317114207476\n",
      "Average test loss: 0.0025402666322059102\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03895418227381176\n",
      "Average test loss: 0.002540407517510984\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03900378198424975\n",
      "Average test loss: 0.0025388262787212927\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03895446935461627\n",
      "Average test loss: 0.002532131275989943\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03879888321955999\n",
      "Average test loss: 0.0025128129890395535\n",
      "Epoch 279/300\n",
      "Average training loss: 0.038783352428012424\n",
      "Average test loss: 0.002557062466421889\n",
      "Epoch 280/300\n",
      "Average training loss: 0.038748593585358725\n",
      "Average test loss: 0.0028557380533052817\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03873160109255049\n",
      "Average test loss: 0.0025302352228512366\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03871419103940328\n",
      "Average test loss: 0.002566235875296924\n",
      "Epoch 284/300\n",
      "Average training loss: 0.038694214247994956\n",
      "Average test loss: 0.0026506433700107866\n",
      "Epoch 285/300\n",
      "Average training loss: 0.038669968730873534\n",
      "Average test loss: 0.0025429138474994234\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03860695134268867\n",
      "Average test loss: 0.0025378324313917095\n",
      "Epoch 288/300\n",
      "Average training loss: 0.038436401800976856\n",
      "Average test loss: 0.002598482652670807\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03854606847630607\n",
      "Average test loss: 0.0026485236460963885\n",
      "Epoch 291/300\n",
      "Average training loss: 0.038441955169041954\n",
      "Average test loss: 0.0025542388047195145\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03843781063291762\n",
      "Average test loss: 0.0027870384167051976\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0383684022343821\n",
      "Average test loss: 0.0025614045889427264\n",
      "Epoch 294/300\n",
      "Average training loss: 0.038352774179644056\n",
      "Average test loss: 0.002562172284970681\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03831324615412288\n",
      "Average test loss: 0.0026129889639301433\n",
      "Epoch 297/300\n",
      "Average training loss: 0.038322923037740916\n",
      "Average test loss: 0.0026233922011322445\n",
      "Epoch 298/300\n",
      "Average training loss: 0.038340160601668886\n",
      "Average test loss: 0.0026024966951873566\n",
      "Epoch 299/300\n",
      "Average training loss: 0.038171459724505745\n",
      "Average test loss: 0.0025628479866103993\n",
      "Epoch 300/300\n",
      "Average training loss: 0.038185971786578496\n",
      "Average test loss: 0.0026413169933689963\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 30.364443742540146\n",
      "Average test loss: 0.16683470613923337\n",
      "Epoch 2/300\n",
      "Average training loss: 12.789806532118055\n",
      "Average test loss: 737.8452613751665\n",
      "Epoch 4/300\n",
      "Average training loss: 9.359107933044434\n",
      "Average test loss: 0.183408434127147\n",
      "Epoch 5/300\n",
      "Average training loss: 7.262591933356391\n",
      "Average test loss: 1.1006049836451808\n",
      "Epoch 6/300\n",
      "Average training loss: 5.5947527465820315\n",
      "Average test loss: 2.6769791672387058\n",
      "Epoch 8/300\n",
      "Average training loss: 5.046268006218805\n",
      "Average test loss: 0.9113670525157617\n",
      "Epoch 9/300\n",
      "Average training loss: 4.3188878686692975\n",
      "Average test loss: 1.617291943811708\n",
      "Epoch 10/300\n",
      "Average training loss: 3.0263470187717014\n",
      "Average test loss: 0.5509152131477991\n",
      "Epoch 11/300\n",
      "Average training loss: 2.8630363953908287\n",
      "Average test loss: 200.8725284584893\n",
      "Epoch 12/300\n",
      "Average training loss: 2.5025809162987604\n",
      "Average test loss: 0.1639321001302451\n",
      "Epoch 13/300\n",
      "Average training loss: 2.1244217156304255\n",
      "Average test loss: 284.1382715741942\n",
      "Epoch 14/300\n",
      "Average training loss: 1.8386920776367188\n",
      "Average test loss: 7.0597267564863175\n",
      "Epoch 15/300\n",
      "Average training loss: 1.6157078292634752\n",
      "Average test loss: 10543.914404467767\n",
      "Epoch 16/300\n",
      "Average training loss: 1.3712356118096245\n",
      "Average test loss: 65.06249351656602\n",
      "Epoch 17/300\n",
      "Average training loss: 1.190652261628045\n",
      "Average test loss: 736.9717723969618\n",
      "Epoch 18/300\n",
      "Average training loss: 1.1136678145726522\n",
      "Average test loss: 8.33647643641217\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7669643903838264\n",
      "Average test loss: 0.005059981353373991\n",
      "Epoch 21/300\n",
      "Average training loss: 0.6784866081873576\n",
      "Average test loss: 0.002985278116746081\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5952158095041911\n",
      "Average test loss: 0.003096756110071308\n",
      "Epoch 23/300\n",
      "Average training loss: 0.5267383113967048\n",
      "Average test loss: 0.0018739882357832458\n",
      "Epoch 24/300\n",
      "Average training loss: 0.4619480690161387\n",
      "Average test loss: 0.001885715633423792\n",
      "Epoch 25/300\n",
      "Average training loss: 0.4029478379090627\n",
      "Average test loss: 0.007976150856456823\n",
      "Epoch 26/300\n",
      "Average training loss: 0.3496275010903676\n",
      "Average test loss: 0.009053966715931892\n",
      "Epoch 27/300\n",
      "Average training loss: 0.30380430012279086\n",
      "Average test loss: 0.00200986241373337\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2632190185652839\n",
      "Average test loss: 0.0018102380459507307\n",
      "Epoch 29/300\n",
      "Average training loss: 0.22839324373669095\n",
      "Average test loss: 0.0017649384726666742\n",
      "Epoch 30/300\n",
      "Average training loss: 0.19944979378912184\n",
      "Average test loss: 0.0021730704489681457\n",
      "Epoch 31/300\n",
      "Average training loss: 0.17459647068712447\n",
      "Average test loss: 0.0017323783242868052\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13764921895662943\n",
      "Average test loss: 0.0017290861574519012\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12378117409017352\n",
      "Average test loss: 0.001705560635568367\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11248541747199164\n",
      "Average test loss: 0.001700616762559447\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10284305527475145\n",
      "Average test loss: 0.0016922748983941144\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09527791031201681\n",
      "Average test loss: 0.0017168184832359354\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08888269440333048\n",
      "Average test loss: 0.0016907119107329182\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07904159008132088\n",
      "Average test loss: 0.0016990359297229184\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07565577483177185\n",
      "Average test loss: 0.0016774121248680684\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07248499264982011\n",
      "Average test loss: 0.001683635555828611\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06989824945396847\n",
      "Average test loss: 0.0016850379105243418\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06778268879320887\n",
      "Average test loss: 0.0016538473562751378\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06600734385516908\n",
      "Average test loss: 0.0016657946016639471\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06301769680447049\n",
      "Average test loss: 0.0016565109208847085\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06179227395190133\n",
      "Average test loss: 0.001660052815452218\n",
      "Epoch 49/300\n",
      "Average training loss: 0.060739681561787925\n",
      "Average test loss: 0.0016749913130576412\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05975176180733575\n",
      "Average test loss: 0.0016569209505493442\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05881319319208463\n",
      "Average test loss: 0.0016710435660142037\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05798074930906296\n",
      "Average test loss: 0.0017431229761698179\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05737525820069843\n",
      "Average test loss: 0.0016669445061642263\n",
      "Epoch 54/300\n",
      "Average training loss: 0.056538926280207104\n",
      "Average test loss: 0.0016690782219068044\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05575520752535926\n",
      "Average test loss: 0.0016766446354902452\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05512088653114107\n",
      "Average test loss: 0.0016551160417083237\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05456696000364092\n",
      "Average test loss: 0.0017144899480562243\n",
      "Epoch 58/300\n",
      "Average training loss: 0.053833151747783026\n",
      "Average test loss: 0.0016469365655341081\n",
      "Epoch 59/300\n",
      "Average training loss: 0.053309793161021336\n",
      "Average test loss: 0.0016976516846981314\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05271505809823672\n",
      "Average test loss: 0.0016935074302471347\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05183646941847271\n",
      "Average test loss: 0.0016640866262217363\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05133967488341861\n",
      "Average test loss: 0.0016599603873781032\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05071292871236801\n",
      "Average test loss: 0.0017016945482335158\n",
      "Epoch 64/300\n",
      "Average training loss: 0.050345436443885165\n",
      "Average test loss: 0.0016641133388297425\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04978685359987948\n",
      "Average test loss: 0.0017576761931284435\n",
      "Epoch 66/300\n",
      "Average training loss: 0.049169060054752564\n",
      "Average test loss: 0.0018679109516864022\n",
      "Epoch 67/300\n",
      "Average training loss: 0.048511429160833355\n",
      "Average test loss: 0.0017116032532519764\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04812739812334379\n",
      "Average test loss: 0.0017288660117321544\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04768069618774785\n",
      "Average test loss: 0.0017273553527063794\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0472299029495981\n",
      "Average test loss: 0.0016669984810675183\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0466339989900589\n",
      "Average test loss: 0.001724550469674998\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04645174917909834\n",
      "Average test loss: 0.0017646058462560178\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04604062192969852\n",
      "Average test loss: 0.0017019473462262088\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04542102395825916\n",
      "Average test loss: 0.001723133124721547\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04509653964969847\n",
      "Average test loss: 0.0017336147458602985\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04461253889070617\n",
      "Average test loss: 0.0017545082382857799\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04427038146720992\n",
      "Average test loss: 0.0016847854351831808\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04375434268514315\n",
      "Average test loss: 0.0017457567284711534\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0435087471736802\n",
      "Average test loss: 0.0019118126589390966\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04321744962533315\n",
      "Average test loss: 0.0017841420771761074\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04284420285291142\n",
      "Average test loss: 0.001713614275575512\n",
      "Epoch 82/300\n",
      "Average training loss: 0.042688022974464626\n",
      "Average test loss: 0.0017562400088128116\n",
      "Epoch 83/300\n",
      "Average training loss: 0.042431888633304174\n",
      "Average test loss: 0.0017116735246446397\n",
      "Epoch 84/300\n",
      "Average training loss: 0.041913936038812\n",
      "Average test loss: 0.001767199357557628\n",
      "Epoch 85/300\n",
      "Average training loss: 0.041741631656885145\n",
      "Average test loss: 0.002127505549643603\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04145869374937481\n",
      "Average test loss: 0.001701201096177101\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04107827947537104\n",
      "Average test loss: 0.0017174868579539987\n",
      "Epoch 88/300\n",
      "Average training loss: 0.040877746091948615\n",
      "Average test loss: 0.001703176982493864\n",
      "Epoch 89/300\n",
      "Average training loss: 0.040695048520962396\n",
      "Average test loss: 0.0016762392894468373\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04047827359040578\n",
      "Average test loss: 0.0038033035503079494\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04015553907222218\n",
      "Average test loss: 0.001731665052783986\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04005113027824296\n",
      "Average test loss: 0.0017453759675328103\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03980123010608885\n",
      "Average test loss: 0.001881348161958158\n",
      "Epoch 94/300\n",
      "Average training loss: 0.039588552342520816\n",
      "Average test loss: 0.001819326431593961\n",
      "Epoch 95/300\n",
      "Average training loss: 0.039244404953387046\n",
      "Average test loss: 0.0024372236718320185\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03911495006084442\n",
      "Average test loss: 0.0017752250714434517\n",
      "Epoch 97/300\n",
      "Average training loss: 0.038921583579646216\n",
      "Average test loss: 0.0018206073708004421\n",
      "Epoch 98/300\n",
      "Average training loss: 0.038946972709563044\n",
      "Average test loss: 0.0017471545243428813\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03861579516861174\n",
      "Average test loss: 0.0017400163982270493\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03836451676156786\n",
      "Average test loss: 0.0018041796297248868\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03814420931041241\n",
      "Average test loss: 0.001777404718928867\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03813178050683604\n",
      "Average test loss: 0.0018011435774258442\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03797036496798197\n",
      "Average test loss: 0.0017391225369647145\n",
      "Epoch 104/300\n",
      "Average training loss: 0.037772457498643136\n",
      "Average test loss: 0.0017587566108753284\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03752047989103529\n",
      "Average test loss: 0.0017958622016012668\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03744243912895521\n",
      "Average test loss: 0.0018697739999430875\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03729644102189276\n",
      "Average test loss: 0.0017764942581868833\n",
      "Epoch 108/300\n",
      "Average training loss: 0.037105293608374064\n",
      "Average test loss: 0.0017610811880893178\n",
      "Epoch 109/300\n",
      "Average training loss: 0.036981864558325875\n",
      "Average test loss: 0.0017970378552046088\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03693494881192843\n",
      "Average test loss: 0.001780538629533516\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03669224575824208\n",
      "Average test loss: 0.001766148244134254\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03657929802106486\n",
      "Average test loss: 0.0018479712966622578\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03653158670994971\n",
      "Average test loss: 0.0017653367492473787\n",
      "Epoch 114/300\n",
      "Average training loss: 0.036350546812017756\n",
      "Average test loss: 0.00193735562161439\n",
      "Epoch 115/300\n",
      "Average training loss: 0.036270415691865815\n",
      "Average test loss: 0.0018387147571063704\n",
      "Epoch 116/300\n",
      "Average training loss: 0.036140894965993035\n",
      "Average test loss: 0.00189209111345311\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03599101482828458\n",
      "Average test loss: 0.0018112080010275046\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03584740114708741\n",
      "Average test loss: 0.0017823720110787286\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03579972673787011\n",
      "Average test loss: 0.0017671552295279171\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03562729662325647\n",
      "Average test loss: 0.0019365581491341194\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03558110977212588\n",
      "Average test loss: 0.0017878872354825338\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03539737187657092\n",
      "Average test loss: 0.001836218228460186\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03534682209955321\n",
      "Average test loss: 0.0018297833165981704\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03503846771187252\n",
      "Average test loss: 0.0017840459833128584\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03512704892953237\n",
      "Average test loss: 0.0018147143330425025\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03499910625815392\n",
      "Average test loss: 0.002130476139899757\n",
      "Epoch 128/300\n",
      "Average training loss: 0.034798606816265315\n",
      "Average test loss: 0.0018368924640946918\n",
      "Epoch 129/300\n",
      "Average training loss: 0.034730940759181976\n",
      "Average test loss: 0.0017742149426291387\n",
      "Epoch 130/300\n",
      "Average training loss: 0.034555966006384956\n",
      "Average test loss: 0.0019298183087052571\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03452138392792808\n",
      "Average test loss: 0.0018788946337170071\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03440024079216851\n",
      "Average test loss: 0.001852720055832631\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03435715843902694\n",
      "Average test loss: 0.0018487747225703465\n",
      "Epoch 135/300\n",
      "Average training loss: 0.034233947058518725\n",
      "Average test loss: 0.0018377884866462814\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03418503173192342\n",
      "Average test loss: 0.0019378257232407728\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03394011124968529\n",
      "Average test loss: 0.0017799133242418369\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03390642583370209\n",
      "Average test loss: 0.0019200457284847896\n",
      "Epoch 140/300\n",
      "Average training loss: 0.033873663855923546\n",
      "Average test loss: 0.0018648900225655073\n",
      "Epoch 141/300\n",
      "Average training loss: 0.033861816104915406\n",
      "Average test loss: 0.0018281356607460313\n",
      "Epoch 142/300\n",
      "Average training loss: 0.033700738201538724\n",
      "Average test loss: 0.002078636408680015\n",
      "Epoch 143/300\n",
      "Average training loss: 0.033595501664612025\n",
      "Average test loss: 0.03036738765695029\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03343374666902754\n",
      "Average test loss: 0.0023711263516710862\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03376599224408468\n",
      "Average test loss: 0.0017887896838494474\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03326811598075761\n",
      "Average test loss: 0.001845259301778343\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03313942113684283\n",
      "Average test loss: 0.0018066020582078231\n",
      "Epoch 149/300\n",
      "Average training loss: 0.033195758952034846\n",
      "Average test loss: 0.0018480637922055191\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03323378141555521\n",
      "Average test loss: 0.0019377000358783536\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03295319000548787\n",
      "Average test loss: 0.00188216584858795\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03291386518544621\n",
      "Average test loss: 0.0018491535532391733\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03283485613597764\n",
      "Average test loss: 0.001919007151077191\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03270439538690779\n",
      "Average test loss: 0.0018693338399752974\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03275319427086247\n",
      "Average test loss: 0.0019535324744259317\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03274728693564733\n",
      "Average training loss: 0.03264991976486312\n",
      "Average test loss: 0.0018598331790417432\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03252640132771598\n",
      "Average test loss: 0.001913537854846153\n",
      "Epoch 160/300\n",
      "Average training loss: 0.032439646081791985\n",
      "Average test loss: 0.0018535540221879879\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03241014464033975\n",
      "Average test loss: 0.0018599093817174435\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03235003583298789\n",
      "Average test loss: 0.0017863163391335143\n",
      "Epoch 163/300\n",
      "Average training loss: 0.032315613023108904\n",
      "Average test loss: 0.002554728828370571\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03253687633574009\n",
      "Average test loss: 0.001815764094185498\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03209321759144465\n",
      "Average test loss: 0.0018206850634887813\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03200549853841464\n",
      "Average test loss: 0.0018392713063706955\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03202522722548909\n",
      "Average test loss: 0.0018874515257775784\n",
      "Epoch 170/300\n",
      "Average training loss: 0.031939242263635\n",
      "Average test loss: 0.0018332142146925132\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03186683097150591\n",
      "Average test loss: 0.0018501653638150956\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03202492259608375\n",
      "Average test loss: 0.001809213217554821\n",
      "Epoch 173/300\n",
      "Average training loss: 0.031863829708761636\n",
      "Average test loss: 0.002232535514359673\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03167993784944216\n",
      "Average test loss: 0.0018534349948167801\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03168462247980965\n",
      "Average test loss: 0.0018243955748362673\n",
      "Epoch 176/300\n",
      "Average training loss: 0.031628783116738\n",
      "Average test loss: 0.0018433638551375933\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03159291428493129\n",
      "Average test loss: 0.0018979617937778432\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03161696107519998\n",
      "Average test loss: 0.001878386363801029\n",
      "Epoch 179/300\n",
      "Average training loss: 0.031498554514514075\n",
      "Average test loss: 0.0019382791152844827\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0315116125676367\n",
      "Average test loss: 0.0018594557289034128\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03138899471031295\n",
      "Average test loss: 0.0018652459116031727\n",
      "Epoch 182/300\n",
      "Average training loss: 0.031361730045742456\n",
      "Average test loss: 0.0022852469879306027\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03129653596215778\n",
      "Average test loss: 0.0018648594489528074\n",
      "Epoch 184/300\n",
      "Average training loss: 0.031295435438553496\n",
      "Average test loss: 0.001866516576665971\n",
      "Epoch 185/300\n",
      "Average training loss: 0.031345543444156646\n",
      "Average test loss: 0.0018601862711624968\n",
      "Epoch 186/300\n",
      "Average training loss: 0.031171720994843378\n",
      "Average test loss: 0.0018403685140526958\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03114183321926329\n",
      "Average test loss: 0.0018202395712335903\n",
      "Epoch 188/300\n",
      "Average training loss: 0.031081369307306077\n",
      "Average test loss: 0.0019220598341069288\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03100486997432179\n",
      "Average test loss: 0.0029657326204081376\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031038375241888894\n",
      "Average test loss: 0.0019040203168988227\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030958591030703652\n",
      "Average test loss: 0.0018973401326479184\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030905297054184808\n",
      "Average test loss: 0.0019214592948555947\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030956167933013704\n",
      "Average test loss: 0.0020784725530280006\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03079090244571368\n",
      "Average test loss: 0.0018209963008347485\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03085767603251669\n",
      "Average test loss: 0.0038499706190907295\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030834405874212583\n",
      "Average test loss: 0.0018674544294675192\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03076752374238438\n",
      "Average test loss: 0.0018647997086453769\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030635920605725713\n",
      "Average test loss: 0.0018396751182153822\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03065906521015697\n",
      "Average test loss: 0.0019152614306658506\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03068227294418547\n",
      "Average test loss: 0.0018589524511868755\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03049952173564169\n",
      "Average test loss: 0.0018850984420213433\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030605182853009966\n",
      "Average test loss: 0.0019189945390034053\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030501626003119682\n",
      "Average test loss: 0.0019048092718132667\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030491310639513863\n",
      "Average test loss: 0.0021509672701358795\n",
      "Epoch 205/300\n",
      "Average training loss: 0.030518826044268078\n",
      "Average test loss: 0.0018657637846966584\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030421294768651328\n",
      "Average test loss: 0.002469406798274981\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030338134699397615\n",
      "Average test loss: 0.0018317373459123903\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030345667198300362\n",
      "Average test loss: 0.001879144807242685\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03027207959194978\n",
      "Average test loss: 0.0019324103872188263\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030274610078997084\n",
      "Average test loss: 0.001932902386618985\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030228949680924416\n",
      "Average test loss: 0.0018684623412167033\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030146638164917628\n",
      "Average test loss: 0.0019871552106406954\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030129187607102925\n",
      "Average test loss: 0.002008059371366269\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03007884897457229\n",
      "Average test loss: 0.0018433552457847529\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030027584320969053\n",
      "Average test loss: 0.0018403509049158958\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0300348867740896\n",
      "Average test loss: 0.001959014348582261\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03001437907252047\n",
      "Average test loss: 0.0019039672661779656\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03000557710727056\n",
      "Average test loss: 0.0019323888242555162\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029928860952456792\n",
      "Average test loss: 0.0019175289194617007\n",
      "Epoch 222/300\n",
      "Average training loss: 0.029897329355279605\n",
      "Average test loss: 0.0019225873382141194\n",
      "Epoch 223/300\n",
      "Average training loss: 0.029781234135230383\n",
      "Average test loss: 0.001867744791838858\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029871379001273048\n",
      "Average test loss: 0.0018726020813402203\n",
      "Epoch 225/300\n",
      "Average training loss: 0.029792762691775956\n",
      "Average test loss: 0.0019400824174905817\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02971384667356809\n",
      "Average test loss: 0.0019435360860079528\n",
      "Epoch 227/300\n",
      "Average training loss: 0.029722737638486756\n",
      "Average test loss: 0.0024256727709952327\n",
      "Epoch 228/300\n",
      "Average training loss: 0.029751284600959885\n",
      "Average test loss: 0.0019097042021652064\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029670662282241717\n",
      "Average test loss: 0.001869695150707331\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029599749348229832\n",
      "Average test loss: 0.0018799500178752675\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02957713696029451\n",
      "Average test loss: 0.00996107903536823\n",
      "Epoch 233/300\n",
      "Average training loss: 0.029526035613483853\n",
      "Average test loss: 0.001873814915617307\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029604803793960147\n",
      "Average test loss: 0.0019293118044734002\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0295249596486489\n",
      "Average test loss: 0.0018273642172829973\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02953054682413737\n",
      "Average test loss: 0.002256694267400437\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02943077605134911\n",
      "Average test loss: 0.0019166769087314605\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029394236044751273\n",
      "Average test loss: 0.0021162019264366894\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02935689217514462\n",
      "Average test loss: 0.0030924358784945477\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02927118118935161\n",
      "Average test loss: 0.0018725319420918821\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02933998371495141\n",
      "Average test loss: 0.0020770941478096775\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02928094025949637\n",
      "Average test loss: 0.0018893338013440371\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029310926838053596\n",
      "Average test loss: 0.0031809815697165\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029233390245172713\n",
      "Average test loss: 0.0019014737606048585\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02916214965283871\n",
      "Average test loss: 0.001953500130938159\n",
      "Epoch 247/300\n",
      "Average training loss: 0.029186768596371013\n",
      "Average test loss: 0.0019075004657109578\n",
      "Epoch 248/300\n",
      "Average training loss: 0.029165955401129193\n",
      "Average test loss: 0.001964247469479839\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02911999537050724\n",
      "Average test loss: 0.0019537729198733965\n",
      "Epoch 250/300\n",
      "Average training loss: 0.029034146693017748\n",
      "Average test loss: 0.0019407161608752277\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02913017450273037\n",
      "Average test loss: 0.0018764254904041688\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02905461140970389\n",
      "Average test loss: 0.0018975680079311133\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029032061939438185\n",
      "Average test loss: 0.002066991380519337\n",
      "Epoch 255/300\n",
      "Average training loss: 0.028965525729788673\n",
      "Average test loss: 0.002000459410250187\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02895623067352507\n",
      "Average test loss: 0.001977466620918777\n",
      "Epoch 257/300\n",
      "Average training loss: 0.028945903693636258\n",
      "Average test loss: 0.0019644604031410482\n",
      "Epoch 258/300\n",
      "Average training loss: 0.028872450028856595\n",
      "Average test loss: 0.0019189336033951904\n",
      "Epoch 259/300\n",
      "Average training loss: 0.028898096740245818\n",
      "Average test loss: 0.0019000451117753983\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02880928474995825\n",
      "Average test loss: 0.001931088806854354\n",
      "Epoch 261/300\n",
      "Average training loss: 0.028883171190818152\n",
      "Average test loss: 0.0019181295012434324\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02886818880505032\n",
      "Average test loss: 0.002049554492553903\n",
      "Epoch 263/300\n",
      "Average training loss: 0.028707456558942794\n",
      "Average test loss: 0.001955215171393421\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02873857961760627\n",
      "Average test loss: 0.002074321718265613\n",
      "Epoch 266/300\n",
      "Average training loss: 0.028723546539743742\n",
      "Average test loss: 0.0018882517639754548\n",
      "Epoch 267/300\n",
      "Average training loss: 0.028657182276248933\n",
      "Average test loss: 0.001874346352285809\n",
      "Epoch 268/300\n",
      "Average training loss: 0.028642775427964\n",
      "Average test loss: 0.0019502758257504966\n",
      "Epoch 269/300\n",
      "Average training loss: 0.028716675847768783\n",
      "Average test loss: 0.0019172901570176084\n",
      "Epoch 270/300\n",
      "Average training loss: 0.028638045880529615\n",
      "Average test loss: 0.001848863203285469\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0285804004934099\n",
      "Average test loss: 0.0019181988835334778\n",
      "Epoch 272/300\n",
      "Average test loss: 0.001870689822981755\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02851058919231097\n",
      "Average test loss: 0.0018642524104151461\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02854137906432152\n",
      "Average test loss: 0.0019244433522431386\n",
      "Epoch 275/300\n",
      "Average training loss: 0.028554612590206994\n",
      "Average test loss: 0.0018976375435789427\n",
      "Epoch 276/300\n",
      "Average training loss: 0.028475303398238287\n",
      "Average test loss: 0.0018742480153838793\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0284992482331064\n",
      "Average test loss: 0.001901091206818819\n",
      "Epoch 278/300\n",
      "Average training loss: 0.028593827169802452\n",
      "Average test loss: 0.0018975592152112061\n",
      "Epoch 280/300\n",
      "Average training loss: 0.028394057244062423\n",
      "Average test loss: 0.001929789180556933\n",
      "Epoch 281/300\n",
      "Average training loss: 0.028350742803679573\n",
      "Average test loss: 0.0019114345230369105\n",
      "Epoch 282/300\n",
      "Average training loss: 0.028406256028347544\n",
      "Average test loss: 0.002012157863523397\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02832127250234286\n",
      "Average test loss: 0.002065577021903462\n",
      "Epoch 284/300\n",
      "Average training loss: 0.028361129782266088\n",
      "Average test loss: 0.0018981051110766\n",
      "Epoch 285/300\n",
      "Average training loss: 0.028269803478486007\n",
      "Average test loss: 0.003559186693901817\n",
      "Epoch 287/300\n",
      "Average training loss: 0.028283222476641338\n",
      "Average test loss: 0.0019107785378065375\n",
      "Epoch 288/300\n",
      "Average training loss: 0.028248239460918638\n",
      "Average test loss: 0.0019083542096325092\n",
      "Epoch 289/300\n",
      "Average training loss: 0.028322825777861806\n",
      "Average test loss: 0.0022478668871853086\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02820032065278954\n",
      "Average test loss: 0.001968339334345526\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02818527317709393\n",
      "Average test loss: 0.0019287377561752995\n",
      "Epoch 292/300\n",
      "Average training loss: 0.028105676248669623\n",
      "Average test loss: 0.0019056023276514478\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02819405531552103\n",
      "Average test loss: 0.001920149093079898\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02813307464453909\n",
      "Average test loss: 0.001949883549267219\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02813264514009158\n",
      "Average test loss: 0.0020991910200359094\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02811112823088964\n",
      "Average test loss: 0.0019177577832920683\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02812362544072999\n",
      "Average test loss: 0.0019101570167889199\n",
      "Epoch 298/300\n",
      "Average training loss: 0.028040899468792808\n",
      "Average test loss: 0.001961353463017278\n",
      "Epoch 299/300\n",
      "Average training loss: 0.028007214099168776\n",
      "Average test loss: 0.0019325400271142522\n",
      "Epoch 300/300\n",
      "Average training loss: 0.028008360905779732\n",
      "Average test loss: 0.0019469799689120716\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive_Depth10-.025/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.55\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.57\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.01\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.05\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.32\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.24\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.58\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.06\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.58\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.64\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.22\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.49\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.73\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.45\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
