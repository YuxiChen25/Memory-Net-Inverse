{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c43ff08-647d-46a5-809e-4cbe0b8d35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246d0088-2dcb-492c-8fa2-fc568a8f2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30fb6f0-4ed6-458b-bab0-dc107f992dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbdf0747-553d-4606-91c3-241f2045708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fcc8ff-61c4-4cd0-9def-feacfacd640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.1)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.1)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.1)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.1)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9940673-405f-4742-a59c-81ef0ae54fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640812a-b691-4d33-a542-4e1740fc4a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1630118755499522\n",
      "Average test loss: 0.012772286831504769\n",
      "Epoch 2/300\n",
      "Average training loss: 0.07020170906517241\n",
      "Average test loss: 0.011175842831118239\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06416703426175648\n",
      "Average test loss: 0.012012088408900631\n",
      "Epoch 4/300\n",
      "Average training loss: 0.061121091345945996\n",
      "Average test loss: 0.010726644723779625\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05915222175916036\n",
      "Average test loss: 0.010211161591112614\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0567703849044111\n",
      "Average test loss: 0.01051109422205223\n",
      "Epoch 7/300\n",
      "Average training loss: 0.055361488256189556\n",
      "Average test loss: 0.010114245128300454\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05421122270491388\n",
      "Average test loss: 0.01010492153879669\n",
      "Epoch 9/300\n",
      "Average training loss: 0.053403982102870944\n",
      "Average test loss: 0.009589981280267239\n",
      "Epoch 10/300\n",
      "Average training loss: 0.052655006928576366\n",
      "Average test loss: 0.00994272872640027\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05213246548506949\n",
      "Average test loss: 0.009288268036312526\n",
      "Epoch 12/300\n",
      "Average training loss: 0.051510929061306845\n",
      "Average test loss: 0.009285616863105032\n",
      "Epoch 13/300\n",
      "Average training loss: 0.050903924190335806\n",
      "Average test loss: 0.00927476467192173\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05037543450461494\n",
      "Average test loss: 0.009455196058998505\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0499636854396926\n",
      "Average test loss: 0.00997926866925425\n",
      "Epoch 16/300\n",
      "Average training loss: 0.049748222589492797\n",
      "Average test loss: 0.009464929439127445\n",
      "Epoch 17/300\n",
      "Average training loss: 0.049461830394135584\n",
      "Average test loss: 0.009420321536560854\n",
      "Epoch 18/300\n",
      "Average training loss: 0.049032833615938826\n",
      "Average test loss: 0.008887265452080302\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04873618300424682\n",
      "Average test loss: 0.008989907069752614\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04840127717455228\n",
      "Average test loss: 0.009168332854078875\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04818289805783166\n",
      "Average test loss: 0.009115869145426485\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04796520864632395\n",
      "Average test loss: 0.008825952904919783\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04781693053576681\n",
      "Average test loss: 0.011353950454129113\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04752743069330851\n",
      "Average test loss: 0.008717179599321551\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0473116265171104\n",
      "Average test loss: 0.008966325609220399\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04726771010292901\n",
      "Average test loss: 0.00872935679472155\n",
      "Epoch 27/300\n",
      "Average training loss: 0.047007315542962815\n",
      "Average test loss: 0.008748885387347805\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04670846597353617\n",
      "Average test loss: 0.00881625523418188\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04672272433506118\n",
      "Average test loss: 0.008807709015491936\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04671160862512059\n",
      "Average test loss: 0.013618146484096845\n",
      "Epoch 31/300\n",
      "Average training loss: 0.046464937537908556\n",
      "Average test loss: 0.008821059451335007\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04627120657430755\n",
      "Average test loss: 0.008692007252739536\n",
      "Epoch 33/300\n",
      "Average training loss: 0.046133432040611905\n",
      "Average test loss: 0.008606962600515949\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04607124748163753\n",
      "Average test loss: 0.008686608254081673\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0459150983856784\n",
      "Average test loss: 0.008530232631497913\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04584869815905889\n",
      "Average test loss: 0.008871442544377512\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04573444089624617\n",
      "Average test loss: 0.008549541337622537\n",
      "Epoch 38/300\n",
      "Average training loss: 0.045693065315485\n",
      "Average test loss: 0.008563502933416102\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0454791404157877\n",
      "Average test loss: 0.008511875227093697\n",
      "Epoch 40/300\n",
      "Average training loss: 0.045424247354269025\n",
      "Average test loss: 0.012891260196765264\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04542296754320463\n",
      "Average test loss: 0.008658762796885438\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04524160549044609\n",
      "Average test loss: 0.008541715032524533\n",
      "Epoch 43/300\n",
      "Average training loss: 0.045249426576826304\n",
      "Average test loss: 0.008585389975044462\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04514754929476314\n",
      "Average test loss: 0.00873547658820947\n",
      "Epoch 45/300\n",
      "Average training loss: 0.044983416659964456\n",
      "Average test loss: 0.008557282131579188\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04495427626702521\n",
      "Average test loss: 0.008487474147644308\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04499765656557348\n",
      "Average test loss: 0.008806145318680339\n",
      "Epoch 48/300\n",
      "Average training loss: 0.044860919601387446\n",
      "Average test loss: 0.008600800897512171\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04471885167558988\n",
      "Average test loss: 0.008514816943969992\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0446607457432482\n",
      "Average test loss: 0.00942467438760731\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04460294638077418\n",
      "Average test loss: 0.00861934132501483\n",
      "Epoch 52/300\n",
      "Average training loss: 0.044582623293002444\n",
      "Average test loss: 0.008585941968278752\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0445095232874155\n",
      "Average test loss: 0.008528197707815302\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04433133058415519\n",
      "Average test loss: 0.00848894531197018\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04433715945813391\n",
      "Average test loss: 0.008672519205345048\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04436059270633592\n",
      "Average test loss: 0.008631291131592458\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04418394315573904\n",
      "Average test loss: 0.008494812903304895\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04421976841820611\n",
      "Average test loss: 0.00990741333241264\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04403490256269773\n",
      "Average test loss: 0.008584603982253207\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04402688650621308\n",
      "Average test loss: 0.00880292822006676\n",
      "Epoch 61/300\n",
      "Average training loss: 0.043970417694913014\n",
      "Average test loss: 0.008458692061818308\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04393326942125956\n",
      "Average test loss: 0.009629102749129136\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04394598609871334\n",
      "Average test loss: 0.008739716665612327\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04382876087228457\n",
      "Average test loss: 0.009096535512142711\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04380679304732217\n",
      "Average test loss: 0.21874814963340758\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04409287799729241\n",
      "Average test loss: 0.00858176310857137\n",
      "Epoch 67/300\n",
      "Average training loss: 0.043591583927472435\n",
      "Average test loss: 0.008703922224541505\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04361570331785414\n",
      "Average test loss: 0.008525975229839484\n",
      "Epoch 69/300\n",
      "Average training loss: 0.043585988144079844\n",
      "Average test loss: 0.008736360621949038\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04355043151643541\n",
      "Average test loss: 0.008421337324712012\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04343821864989069\n",
      "Average test loss: 0.008462967995140288\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04347473553816478\n",
      "Average test loss: 0.008519563440647391\n",
      "Epoch 73/300\n",
      "Average training loss: 0.043427111705144246\n",
      "Average test loss: 0.008684602688584063\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04333098712563515\n",
      "Average test loss: 0.008490842765404119\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04332330041130384\n",
      "Average test loss: 0.008460864504178364\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04324867882993486\n",
      "Average test loss: 0.008437826900432507\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04317538208100531\n",
      "Average test loss: 0.008644357297983435\n",
      "Epoch 78/300\n",
      "Average training loss: 0.043206676181819705\n",
      "Average test loss: 0.008627137261546321\n",
      "Epoch 79/300\n",
      "Average training loss: 0.043054915560616386\n",
      "Average test loss: 0.008488922866268291\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04305700374477439\n",
      "Average test loss: 0.01020877907011244\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04306993580526776\n",
      "Average test loss: 0.00894679674671756\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0430107988913854\n",
      "Average test loss: 0.009327152661979199\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04292596463693513\n",
      "Average test loss: 0.008853241781393687\n",
      "Epoch 84/300\n",
      "Average training loss: 0.042934389528301024\n",
      "Average test loss: 0.008545157564183076\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04286033567123943\n",
      "Average test loss: 0.008576194250749217\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04278755002551608\n",
      "Average test loss: 0.008521551916582716\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04271905264589522\n",
      "Average test loss: 0.009094456900325086\n",
      "Epoch 88/300\n",
      "Average training loss: 0.042701348377598654\n",
      "Average test loss: 0.008670830253097747\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04271407824092441\n",
      "Average test loss: 0.009053501696222358\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04267835341890653\n",
      "Average test loss: 0.008455299395653936\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04262903032037947\n",
      "Average test loss: 0.008576593559649256\n",
      "Epoch 92/300\n",
      "Average training loss: 0.042518668158186806\n",
      "Average test loss: 0.008569652190638913\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04260258660713832\n",
      "Average test loss: 0.008460418880813652\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0425232034722964\n",
      "Average test loss: 0.008627469528052541\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04241203060746193\n",
      "Average test loss: 0.00857162846542067\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04245294305019909\n",
      "Average test loss: 0.008882785990834237\n",
      "Epoch 97/300\n",
      "Average training loss: 0.042396820694208144\n",
      "Average test loss: 0.00860468558097879\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04232239942749341\n",
      "Average test loss: 0.008467902524603738\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04231240439414978\n",
      "Average test loss: 0.008595368762397105\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04228924110035102\n",
      "Average test loss: 0.008527642236815559\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04224723659621345\n",
      "Average test loss: 0.008743938771386942\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04223130915231175\n",
      "Average test loss: 0.008521068120168314\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04217370285259353\n",
      "Average test loss: 0.0085294715480672\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04211159579952558\n",
      "Average test loss: 0.008570748023688794\n",
      "Epoch 105/300\n",
      "Average training loss: 0.042127457989586725\n",
      "Average test loss: 0.008614071764879756\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04207411098811362\n",
      "Average test loss: 0.008850343931880262\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04203743531306585\n",
      "Average test loss: 0.008707904647621844\n",
      "Epoch 108/300\n",
      "Average training loss: 0.042047894514269296\n",
      "Average test loss: 0.00877271317028337\n",
      "Epoch 109/300\n",
      "Average training loss: 0.042006574829419455\n",
      "Average test loss: 0.008724534864226977\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0419701231221358\n",
      "Average test loss: 0.00863512650049395\n",
      "Epoch 111/300\n",
      "Average training loss: 0.041917352457841235\n",
      "Average test loss: 0.00890187751998504\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04187214261955685\n",
      "Average test loss: 0.008693939153518942\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04188172646032439\n",
      "Average test loss: 0.012947219374279182\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04176407596634494\n",
      "Average test loss: 0.008723260341419114\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04181422196494208\n",
      "Average test loss: 0.008569267136769162\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04175883608725336\n",
      "Average test loss: 0.008727570647166834\n",
      "Epoch 117/300\n",
      "Average training loss: 0.041706290243400466\n",
      "Average test loss: 0.008892590396106244\n",
      "Epoch 118/300\n",
      "Average training loss: 0.041713672015402055\n",
      "Average test loss: 0.008744056148661507\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04161174540056123\n",
      "Average test loss: 0.008759020458492969\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04165691077046924\n",
      "Average test loss: 0.008547439124849107\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0415970330271456\n",
      "Average test loss: 0.00906919532848729\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04155255928635597\n",
      "Average test loss: 0.008640736266970634\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04164801922771666\n",
      "Average test loss: 0.008607305473337571\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04152639784084426\n",
      "Average test loss: 0.008750592386556996\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04147990581062105\n",
      "Average test loss: 0.00863504266159402\n",
      "Epoch 126/300\n",
      "Average training loss: 0.041413261423508325\n",
      "Average test loss: 0.008621300138947036\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0414025275376108\n",
      "Average test loss: 0.008664198827412393\n",
      "Epoch 128/300\n",
      "Average training loss: 0.041404212239715785\n",
      "Average test loss: 0.008646143039067586\n",
      "Epoch 129/300\n",
      "Average training loss: 0.041293643885188636\n",
      "Average test loss: 0.008813677790264289\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04137384764022297\n",
      "Average test loss: 0.008599660827053917\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04139644844995605\n",
      "Average test loss: 0.008553686586519083\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04123657504055235\n",
      "Average test loss: 0.008962580288449923\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04119348926676644\n",
      "Average test loss: 0.008723509146935409\n",
      "Epoch 134/300\n",
      "Average training loss: 0.041260305838452443\n",
      "Average test loss: 0.00870094951449169\n",
      "Epoch 135/300\n",
      "Average training loss: 0.041153338630994164\n",
      "Average test loss: 0.00877481446415186\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0411475348174572\n",
      "Average test loss: 0.008796433017899593\n",
      "Epoch 137/300\n",
      "Average training loss: 0.041137691602110865\n",
      "Average test loss: 0.00891878897903694\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04115093849102656\n",
      "Average test loss: 0.00859967475839787\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04107850448952781\n",
      "Average test loss: 0.008664488178988298\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04101671531465319\n",
      "Average test loss: 0.008669895312438409\n",
      "Epoch 141/300\n",
      "Average training loss: 0.041092644611994425\n",
      "Average test loss: 0.009237813684675428\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0413826904296875\n",
      "Average test loss: 0.008628830969333649\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04099616165624725\n",
      "Average test loss: 0.008580266897463136\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04087670566638311\n",
      "Average test loss: 0.008791624058451918\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04090956840746932\n",
      "Average test loss: 0.008768872208893299\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04086692433224784\n",
      "Average test loss: 0.008882032451116377\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04085375616285536\n",
      "Average test loss: 0.008750374315513504\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0408670413725906\n",
      "Average test loss: 0.00866364640245835\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04084038505620426\n",
      "Average test loss: 0.008580223825656706\n",
      "Epoch 150/300\n",
      "Average training loss: 0.040843512078126275\n",
      "Average test loss: 0.008777065957585971\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04080009330312411\n",
      "Average test loss: 0.009168276147709953\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04078378035955959\n",
      "Average test loss: 0.008766092975934347\n",
      "Epoch 153/300\n",
      "Average training loss: 0.040671865057614116\n",
      "Average test loss: 0.009076089783675141\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04073797967036565\n",
      "Average test loss: 0.008866435372995005\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04063324336873161\n",
      "Average test loss: 0.008680332486828169\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04062167393830087\n",
      "Average test loss: 0.008772974362389909\n",
      "Epoch 157/300\n",
      "Average training loss: 0.040691270142793654\n",
      "Average test loss: 0.00878018350029985\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04059579920106464\n",
      "Average test loss: 0.009150217527316677\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04053356269664234\n",
      "Average test loss: 0.008715464777830573\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04073757273289892\n",
      "Average test loss: 0.011931644183893999\n",
      "Epoch 161/300\n",
      "Average training loss: 0.040589231782489354\n",
      "Average test loss: 0.008717957933743794\n",
      "Epoch 162/300\n",
      "Average training loss: 0.040433653036753336\n",
      "Average test loss: 0.009013003474308385\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04050141242477629\n",
      "Average test loss: 0.008757161017921235\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04040042170219951\n",
      "Average test loss: 0.008874631887094843\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04044541914264361\n",
      "Average test loss: 0.008758947598437468\n",
      "Epoch 166/300\n",
      "Average training loss: 0.040364550083875655\n",
      "Average test loss: 0.008825823534280061\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04035525725616349\n",
      "Average test loss: 0.008863627360098892\n",
      "Epoch 168/300\n",
      "Average training loss: 0.040382302711407345\n",
      "Average test loss: 0.008906913162933456\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04034541579749849\n",
      "Average test loss: 0.009084082464377086\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04030433515376515\n",
      "Average test loss: 0.008763853847980499\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04030719387862417\n",
      "Average test loss: 0.00871266775743829\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04036509102582932\n",
      "Average test loss: 0.008886759909490744\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04025377066598998\n",
      "Average test loss: 0.008787543370491929\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04019921936591466\n",
      "Average test loss: 0.008890353228482936\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0401880561709404\n",
      "Average test loss: 0.008951535095771153\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04018084300557773\n",
      "Average test loss: 0.01732808904680941\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04019735882017347\n",
      "Average test loss: 0.009926349478463332\n",
      "Epoch 178/300\n",
      "Average training loss: 0.040136202494303386\n",
      "Average test loss: 0.012045438563658131\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04015507154332267\n",
      "Average test loss: 0.00873729295283556\n",
      "Epoch 180/300\n",
      "Average training loss: 0.040140681425730386\n",
      "Average test loss: 0.008831740900874139\n",
      "Epoch 181/300\n",
      "Average training loss: 0.040002273350954055\n",
      "Average test loss: 0.008818681896560722\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04003337859445148\n",
      "Average test loss: 0.008867375124659804\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04006273612711165\n",
      "Average test loss: 0.008812054608845049\n",
      "Epoch 184/300\n",
      "Average training loss: 0.040051742023891876\n",
      "Average test loss: 0.009399198759761121\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04010075985060798\n",
      "Average test loss: 0.009042885416911709\n",
      "Epoch 186/300\n",
      "Average training loss: 0.039937670022249225\n",
      "Average test loss: 0.00880341340104739\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03994836198621326\n",
      "Average test loss: 0.009201365572710831\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03995019485884243\n",
      "Average test loss: 0.00889738440182474\n",
      "Epoch 189/300\n",
      "Average training loss: 0.039911987715297276\n",
      "Average test loss: 0.008948605752653545\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0399305112891727\n",
      "Average test loss: 0.008905856774085098\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03981572608484162\n",
      "Average test loss: 0.008804145009153419\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03993841968642341\n",
      "Average test loss: 0.009024097320106294\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03981720526350869\n",
      "Average test loss: 0.009023469825585682\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03985418815414111\n",
      "Average test loss: 0.009283809281885625\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03978321656253603\n",
      "Average test loss: 0.008803859396527211\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03980582738916079\n",
      "Average test loss: 0.009404166551927726\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03975150941146745\n",
      "Average test loss: 0.008963341323037942\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03969946352309651\n",
      "Average test loss: 0.00909751005553537\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03974080478482776\n",
      "Average test loss: 0.009051028097255363\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03975074121687147\n",
      "Average test loss: 0.021215324390265678\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03970846427149243\n",
      "Average test loss: 0.009034540596935484\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03960599277748002\n",
      "Average test loss: 0.008907016659362449\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03969508989320861\n",
      "Average test loss: 0.008905491203069688\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03961033806204796\n",
      "Average test loss: 0.00953133202592532\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03961140266391966\n",
      "Average test loss: 0.009300780312882529\n",
      "Epoch 206/300\n",
      "Average training loss: 0.039563062575128344\n",
      "Average test loss: 0.008949115859551562\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03959746369719505\n",
      "Average test loss: 0.008888505458003945\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03956709032257398\n",
      "Average test loss: 0.00905712999486261\n",
      "Epoch 209/300\n",
      "Average training loss: 0.039527657902903024\n",
      "Average test loss: 0.008906520497467783\n",
      "Epoch 210/300\n",
      "Average training loss: 0.039529199451208114\n",
      "Average test loss: 0.009319562277032269\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03949214811126391\n",
      "Average test loss: 0.00898419179353449\n",
      "Epoch 212/300\n",
      "Average training loss: 0.039475748389959334\n",
      "Average test loss: 0.008939028556976054\n",
      "Epoch 213/300\n",
      "Average training loss: 0.039422176125976774\n",
      "Average test loss: 0.009184473344849215\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03946200708217091\n",
      "Average test loss: 0.009130203225546413\n",
      "Epoch 215/300\n",
      "Average training loss: 0.039448687669303685\n",
      "Average test loss: 0.009423817213210796\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03946615458197064\n",
      "Average test loss: 0.00924826728966501\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0393486537999577\n",
      "Average test loss: 0.008961516201910045\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03942182963755396\n",
      "Average test loss: 0.009243069910340839\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03936079294482867\n",
      "Average test loss: 0.008990954382552041\n",
      "Epoch 220/300\n",
      "Average training loss: 0.039461314138438966\n",
      "Average test loss: 0.008987366127057209\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03937517351905505\n",
      "Average test loss: 0.010305908943216006\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03926583921578195\n",
      "Average test loss: 0.00948818425171905\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03926383970843421\n",
      "Average test loss: 0.009058395771516695\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0393675077855587\n",
      "Average test loss: 0.009244871204511987\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03927157128850619\n",
      "Average test loss: 0.0091387564630972\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03928238696522183\n",
      "Average test loss: 0.00984737549473842\n",
      "Epoch 227/300\n",
      "Average training loss: 0.039243961963388654\n",
      "Average test loss: 0.009577839307487011\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03925644490785069\n",
      "Average test loss: 0.011920224879350927\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03918245256940524\n",
      "Average test loss: 0.00892768296185467\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03915880700614717\n",
      "Average test loss: 0.009151436338408125\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03921601321299871\n",
      "Average test loss: 0.009230913870036602\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0391637359129058\n",
      "Average test loss: 0.009093447619842158\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03916879154907332\n",
      "Average test loss: 0.009138578049838543\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03915962272220188\n",
      "Average test loss: 0.009032827912933297\n",
      "Epoch 235/300\n",
      "Average training loss: 0.039174965014060335\n",
      "Average test loss: 0.009034983586106035\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03911100083589554\n",
      "Average test loss: 0.00884507205337286\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03911685472395685\n",
      "Average test loss: 0.009433014464875062\n",
      "Epoch 238/300\n",
      "Average training loss: 0.039159024586280185\n",
      "Average test loss: 0.009085335863961114\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03909972403115696\n",
      "Average test loss: 0.00934999206331041\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03899174698525005\n",
      "Average test loss: 0.009041324213147163\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03905384750498666\n",
      "Average test loss: 0.009076067018012206\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03904333660668797\n",
      "Average test loss: 0.008998832088791662\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03897285741567612\n",
      "Average test loss: 0.009984784449968073\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03900567467345132\n",
      "Average test loss: 0.009047097239643335\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03900213047530916\n",
      "Average test loss: 0.0091945774157842\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03895518732898765\n",
      "Average test loss: 0.008976610629094972\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03901917361550861\n",
      "Average test loss: 0.009148449265294605\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03893153460820516\n",
      "Average test loss: 0.009001549418601726\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0389891362256474\n",
      "Average test loss: 0.008920728947139448\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03894119439191288\n",
      "Average test loss: 0.009427299172514015\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03892794907755322\n",
      "Average test loss: 0.00935534792765975\n",
      "Epoch 252/300\n",
      "Average training loss: 0.038927679316865076\n",
      "Average test loss: 0.009314333215355873\n",
      "Epoch 253/300\n",
      "Average training loss: 0.038868257449732886\n",
      "Average test loss: 0.009082552424735494\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03887771177954144\n",
      "Average test loss: 0.00896955134636826\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03882561240924729\n",
      "Average test loss: 0.008981562005976836\n",
      "Epoch 256/300\n",
      "Average training loss: 0.038820491664939456\n",
      "Average test loss: 0.009291603833436966\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03885978236132198\n",
      "Average test loss: 0.009445070954660574\n",
      "Epoch 258/300\n",
      "Average training loss: 0.038870822774039374\n",
      "Average test loss: 0.009489796366128657\n",
      "Epoch 259/300\n",
      "Average training loss: 0.038802496641874316\n",
      "Average test loss: 0.009518672448065545\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03883681303593847\n",
      "Average test loss: 0.009242413620154063\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0387994049290816\n",
      "Average test loss: 0.008945918425917625\n",
      "Epoch 262/300\n",
      "Average training loss: 0.038902851389514076\n",
      "Average test loss: 0.009782976795401839\n",
      "Epoch 263/300\n",
      "Average training loss: 0.038760085384051006\n",
      "Average test loss: 0.009224100421700212\n",
      "Epoch 264/300\n",
      "Average training loss: 0.038772600644164615\n",
      "Average test loss: 0.009160524190300041\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03884538724356228\n",
      "Average test loss: 0.009193211908969615\n",
      "Epoch 266/300\n",
      "Average training loss: 0.038735876831743454\n",
      "Average test loss: 0.009190849412646558\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0387027997440762\n",
      "Average test loss: 0.009090690996083948\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03868662560979525\n",
      "Average test loss: 0.009031161703169345\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03868144894970788\n",
      "Average test loss: 0.009502529811527993\n",
      "Epoch 270/300\n",
      "Average training loss: 0.038674743917253285\n",
      "Average test loss: 0.00954208129644394\n",
      "Epoch 271/300\n",
      "Average training loss: 0.038623644577132334\n",
      "Average test loss: 0.00913331405652894\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0386925735672315\n",
      "Average test loss: 0.009307284770740403\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03861910641524527\n",
      "Average test loss: 0.009316648080117172\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03868409158455001\n",
      "Average test loss: 0.008984315587414636\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03863135919802719\n",
      "Average test loss: 0.009831007048487663\n",
      "Epoch 276/300\n",
      "Average training loss: 0.038629007077879376\n",
      "Average test loss: 0.009372606377634738\n",
      "Epoch 277/300\n",
      "Average training loss: 0.038580179856883154\n",
      "Average test loss: 0.009604755265845193\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03855560131205453\n",
      "Average test loss: 0.009072858211066988\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03866671217812432\n",
      "Average test loss: 0.009447775586611695\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03854685911867354\n",
      "Average test loss: 0.009037202417436573\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0385661416053772\n",
      "Average test loss: 0.009059311788943078\n",
      "Epoch 282/300\n",
      "Average training loss: 0.038539842247962953\n",
      "Average test loss: 0.009409465325375397\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03859937094317542\n",
      "Average test loss: 0.0092018682104018\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03849987372093731\n",
      "Average test loss: 0.009375580324894852\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03855018081267675\n",
      "Average test loss: 0.0092604416815771\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03848678796158896\n",
      "Average test loss: 0.00946082181400723\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03856212067272928\n",
      "Average test loss: 0.009342192800508605\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03857782878147231\n",
      "Average test loss: 0.009230810506476296\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03846597272157669\n",
      "Average test loss: 0.009411043597178327\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03847582642237345\n",
      "Average test loss: 0.009069413594901562\n",
      "Epoch 291/300\n",
      "Average training loss: 0.038422336873081\n",
      "Average test loss: 0.009388746103478802\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03845757055282593\n",
      "Average test loss: 0.009199835699879462\n",
      "Epoch 293/300\n",
      "Average training loss: 0.038476874344878724\n",
      "Average test loss: 0.010232984972496827\n",
      "Epoch 294/300\n",
      "Average training loss: 0.038422114564312826\n",
      "Average test loss: 0.0094525861557987\n",
      "Epoch 295/300\n",
      "Average training loss: 0.038556009040938484\n",
      "Average test loss: 0.009287088197138575\n",
      "Epoch 296/300\n",
      "Average training loss: 0.038409015049537025\n",
      "Average test loss: 0.01043132295128372\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03841483141978582\n",
      "Average test loss: 0.009104030459291404\n",
      "Epoch 298/300\n",
      "Average training loss: 0.038401624974277286\n",
      "Average test loss: 0.009173791074918376\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03834043516384231\n",
      "Average test loss: 0.009278261004222764\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03839527113570107\n",
      "Average test loss: 0.009105485156592395\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14123454539643393\n",
      "Average test loss: 0.009738808266818524\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05692092580596606\n",
      "Average test loss: 0.01393800110866626\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05188214402728611\n",
      "Average test loss: 0.008672635239859422\n",
      "Epoch 4/300\n",
      "Average training loss: 0.048683630744616194\n",
      "Average test loss: 0.0075917605281704\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04648492734299766\n",
      "Average test loss: 0.007499326249377595\n",
      "Epoch 6/300\n",
      "Average training loss: 0.044747359100315305\n",
      "Average test loss: 0.008277389112446043\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04309692051344448\n",
      "Average test loss: 0.0073263641504777805\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04183712053961224\n",
      "Average test loss: 0.007522259102927314\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04114723727107048\n",
      "Average test loss: 0.0068058235889507665\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04035311555862427\n",
      "Average test loss: 0.00688918271743589\n",
      "Epoch 11/300\n",
      "Average training loss: 0.039574280013640724\n",
      "Average test loss: 0.006687555968347523\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03909510324398677\n",
      "Average test loss: 0.0066937050595879555\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03851031625270843\n",
      "Average test loss: 0.006522193365212944\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03822338872485691\n",
      "Average test loss: 0.006626901079383162\n",
      "Epoch 15/300\n",
      "Average training loss: 0.037878419217136174\n",
      "Average test loss: 0.006607954324119621\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03743520715336005\n",
      "Average test loss: 0.006440372226966752\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03708537767661942\n",
      "Average test loss: 0.006507268445773257\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03681681434810161\n",
      "Average test loss: 0.00635685399423043\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03668170531259643\n",
      "Average test loss: 0.007174708177646001\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03634385082787937\n",
      "Average test loss: 0.006322935727735361\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03624821308751901\n",
      "Average test loss: 0.006315675100104676\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03604678053326077\n",
      "Average test loss: 0.006240015166087283\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03578781996501817\n",
      "Average test loss: 0.006214017706405785\n",
      "Epoch 24/300\n",
      "Average training loss: 0.035681988952888384\n",
      "Average test loss: 0.006227006567021211\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03552997947732608\n",
      "Average test loss: 0.006253227869255675\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03538701050811344\n",
      "Average test loss: 0.006184539554019769\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03525358550747235\n",
      "Average test loss: 0.006197642312281661\n",
      "Epoch 28/300\n",
      "Average training loss: 0.035174698813094034\n",
      "Average test loss: 0.006668188110407856\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03501845171385341\n",
      "Average test loss: 0.006217502129988538\n",
      "Epoch 30/300\n",
      "Average training loss: 0.034975895679659315\n",
      "Average test loss: 0.006278141822252009\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03484965665804015\n",
      "Average test loss: 0.00616433447247578\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03476191041204665\n",
      "Average test loss: 0.006128671417633693\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03460154431230492\n",
      "Average test loss: 0.006165284665508403\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03459076303905911\n",
      "Average test loss: 0.006303873899910185\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03452980583575037\n",
      "Average test loss: 0.006274381310161617\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03441143320335282\n",
      "Average test loss: 0.006293437918441163\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03430916803247399\n",
      "Average test loss: 0.006017850559618738\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03429067940513293\n",
      "Average test loss: 0.006350693703111675\n",
      "Epoch 39/300\n",
      "Average training loss: 0.034186440249284106\n",
      "Average test loss: 0.0060773437288072375\n",
      "Epoch 40/300\n",
      "Average training loss: 0.034141916549868055\n",
      "Average test loss: 0.0061616165977385305\n",
      "Epoch 41/300\n",
      "Average training loss: 0.034083936406506435\n",
      "Average test loss: 0.006056035049259663\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03409390492571725\n",
      "Average test loss: 0.006267197609775596\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03391731471982267\n",
      "Average test loss: 0.006221044765164455\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03390889853570196\n",
      "Average test loss: 0.006160068913466401\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03388217685123285\n",
      "Average test loss: 0.006137097828504112\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03375612589385774\n",
      "Average test loss: 0.006002580846349398\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03377640748023987\n",
      "Average test loss: 0.006142795367787282\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03364767803748449\n",
      "Average test loss: 0.006155200565026866\n",
      "Epoch 49/300\n",
      "Average training loss: 0.033644712383548415\n",
      "Average test loss: 0.006002413114325868\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0336305361158318\n",
      "Average test loss: 0.006032159799089034\n",
      "Epoch 51/300\n",
      "Average training loss: 0.033508793267938826\n",
      "Average test loss: 0.006058157349626224\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03347407354248894\n",
      "Average test loss: 0.006094922451095448\n",
      "Epoch 53/300\n",
      "Average training loss: 0.033467839896678925\n",
      "Average test loss: 0.0059646775846680005\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03342513827648428\n",
      "Average test loss: 0.006145796346167723\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03338096588022179\n",
      "Average test loss: 0.006207988211264213\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03333232238226467\n",
      "Average test loss: 0.0060786809614963005\n",
      "Epoch 57/300\n",
      "Average training loss: 0.033316581865151726\n",
      "Average test loss: 0.006075410765078333\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03323738335238563\n",
      "Average test loss: 0.006029863687439098\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03324004598789745\n",
      "Average test loss: 0.005962591517716646\n",
      "Epoch 60/300\n",
      "Average training loss: 0.033149846778975596\n",
      "Average test loss: 0.006091098232401742\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03312339241968261\n",
      "Average test loss: 0.005977604842020406\n",
      "Epoch 62/300\n",
      "Average training loss: 0.033133579346868725\n",
      "Average test loss: 0.005979219505356417\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03306557520892885\n",
      "Average test loss: 0.00613733630006512\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03303480264047782\n",
      "Average test loss: 0.005972793723560041\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03299339176548852\n",
      "Average test loss: 0.006303548857569695\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0329700238638454\n",
      "Average test loss: 0.005964040943731865\n",
      "Epoch 67/300\n",
      "Average training loss: 0.032944241394599276\n",
      "Average test loss: 0.006229414412544833\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03287880156437556\n",
      "Average test loss: 0.0061732492856681346\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03288611050446828\n",
      "Average test loss: 0.006505159433517191\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03278163938389884\n",
      "Average test loss: 0.0062351705866555375\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03281775159637133\n",
      "Average test loss: 0.005970234048863252\n",
      "Epoch 72/300\n",
      "Average training loss: 0.032828224354320104\n",
      "Average test loss: 0.005963410898215241\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0327239778108067\n",
      "Average test loss: 0.005993997374756469\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03271006858679983\n",
      "Average test loss: 0.006228343148198393\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03264950645301077\n",
      "Average test loss: 0.005981454457259841\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03269219197829564\n",
      "Average test loss: 0.0065030638612806795\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03262988845838441\n",
      "Average test loss: 0.006611893348395824\n",
      "Epoch 78/300\n",
      "Average training loss: 0.032583097603585985\n",
      "Average test loss: 0.005991514936089516\n",
      "Epoch 79/300\n",
      "Average training loss: 0.032593466685877905\n",
      "Average test loss: 0.005980259569154846\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03284298212991821\n",
      "Average test loss: 0.0060912268583973245\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03249376995364825\n",
      "Average test loss: 0.0062528936221367785\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03243182775709364\n",
      "Average test loss: 0.006139257212479909\n",
      "Epoch 83/300\n",
      "Average training loss: 0.032524776352776424\n",
      "Average test loss: 0.005949197858985927\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03238774551782343\n",
      "Average test loss: 0.005950795698910952\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03238793073429002\n",
      "Average test loss: 0.005983229300214185\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03240029194951057\n",
      "Average test loss: 0.006059495613392856\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03236827888091405\n",
      "Average test loss: 0.006036713131186035\n",
      "Epoch 88/300\n",
      "Average training loss: 0.032362804308533666\n",
      "Average test loss: 0.006010229913310872\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03230386309987969\n",
      "Average test loss: 0.005991882801055908\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03229007245269087\n",
      "Average test loss: 0.005960357267823484\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0322489404214753\n",
      "Average test loss: 0.006115061007026169\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03224181490474277\n",
      "Average test loss: 0.005974383970929517\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03223986845215162\n",
      "Average test loss: 0.006040732690443596\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03219519126084116\n",
      "Average test loss: 0.007969746290395657\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03214700870050324\n",
      "Average test loss: 0.005946128527323405\n",
      "Epoch 96/300\n",
      "Average training loss: 0.032121058579948215\n",
      "Average test loss: 0.006169995783517758\n",
      "Epoch 97/300\n",
      "Average training loss: 0.032131651080316966\n",
      "Average test loss: 0.005990745317604807\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03209695353772905\n",
      "Average test loss: 0.006204074962271584\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03213544891277949\n",
      "Average test loss: 0.005960860072324673\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03203135023183293\n",
      "Average test loss: 0.006339722690069013\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03203716764847438\n",
      "Average test loss: 0.0060749933711356586\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03202281825741132\n",
      "Average test loss: 0.005950810716797908\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03198301454385122\n",
      "Average test loss: 0.005993481521391206\n",
      "Epoch 104/300\n",
      "Average training loss: 0.031976014415423075\n",
      "Average test loss: 0.006071093383762572\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03195777144200272\n",
      "Average test loss: 0.006082716683546702\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03199281152751711\n",
      "Average test loss: 0.006161174151632521\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03187929829955101\n",
      "Average test loss: 0.006006189733743668\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03194049281875293\n",
      "Average test loss: 0.0059897559384504955\n",
      "Epoch 109/300\n",
      "Average training loss: 0.031841969629128775\n",
      "Average test loss: 0.006496781756066614\n",
      "Epoch 110/300\n",
      "Average training loss: 0.031793727848264904\n",
      "Average test loss: 0.005961358113835255\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0318460056980451\n",
      "Average test loss: 0.0068380307958771785\n",
      "Epoch 112/300\n",
      "Average training loss: 0.031761609448326955\n",
      "Average test loss: 0.006445113219734695\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03179987071288957\n",
      "Average test loss: 0.008180419881310727\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03175062111020088\n",
      "Average test loss: 0.006287780767927567\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03176252725389268\n",
      "Average test loss: 0.005982759405341413\n",
      "Epoch 116/300\n",
      "Average training loss: 0.031738274974955454\n",
      "Average test loss: 0.006117192592885759\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03173401387863689\n",
      "Average test loss: 0.006079721312969923\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03165608645810021\n",
      "Average test loss: 0.006250920988619327\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03165244424674246\n",
      "Average test loss: 0.00641724482882354\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03167938473820686\n",
      "Average test loss: 0.006212078411546018\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03163056990338697\n",
      "Average test loss: 0.006053207114752796\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03158423149089019\n",
      "Average test loss: 0.0060527545457912816\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03159969642427232\n",
      "Average test loss: 0.006040650351593892\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03154033575786484\n",
      "Average test loss: 0.005961152801083194\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03156379621227582\n",
      "Average test loss: 0.005978786290519767\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03155053103963534\n",
      "Average test loss: 0.006132447864032454\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03151832912200027\n",
      "Average test loss: 0.006249681327078077\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03149337929652797\n",
      "Average test loss: 0.006158015106701189\n",
      "Epoch 129/300\n",
      "Average training loss: 0.031470127577582996\n",
      "Average test loss: 0.006233306247740984\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03150450554821226\n",
      "Average test loss: 0.006129351502491368\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031445044696331026\n",
      "Average test loss: 0.006068651943571038\n",
      "Epoch 132/300\n",
      "Average training loss: 0.031410737650261984\n",
      "Average test loss: 0.006336193035874102\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03142923016349475\n",
      "Average test loss: 0.006256647925410006\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03139518607987298\n",
      "Average test loss: 0.006080033029533095\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03135522048340903\n",
      "Average test loss: 0.006130590540667375\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031358550671074126\n",
      "Average test loss: 0.00682222988580664\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0313216565185123\n",
      "Average test loss: 0.006055053854568137\n",
      "Epoch 138/300\n",
      "Average training loss: 0.031327246980534657\n",
      "Average test loss: 0.006074918263902267\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03129384405248695\n",
      "Average test loss: 0.006153950305862559\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03128857165906165\n",
      "Average test loss: 0.0076006405949592595\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03129610322746965\n",
      "Average test loss: 0.006265695745332374\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03127449776397811\n",
      "Average test loss: 0.015943342610365816\n",
      "Epoch 143/300\n",
      "Average training loss: 0.031276609314812555\n",
      "Average test loss: 0.006053118199110031\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03119677351580726\n",
      "Average test loss: 0.00619322332367301\n",
      "Epoch 145/300\n",
      "Average training loss: 0.031165258089701334\n",
      "Average test loss: 0.0062734702970418665\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03120563547478782\n",
      "Average test loss: 0.006103373049034013\n",
      "Epoch 147/300\n",
      "Average training loss: 0.031163212011257806\n",
      "Average test loss: 0.006074728813436296\n",
      "Epoch 148/300\n",
      "Average training loss: 0.031170760224262874\n",
      "Average test loss: 0.006525378847701682\n",
      "Epoch 149/300\n",
      "Average training loss: 0.031120342855652173\n",
      "Average test loss: 0.006082726672291755\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031098517522215843\n",
      "Average test loss: 0.0060838875621557235\n",
      "Epoch 151/300\n",
      "Average training loss: 0.031138127472665575\n",
      "Average test loss: 0.006029765165514416\n",
      "Epoch 152/300\n",
      "Average training loss: 0.031087546937995487\n",
      "Average test loss: 0.006441044409655862\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03110661101175679\n",
      "Average test loss: 0.006230668376717302\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031039164046446483\n",
      "Average test loss: 0.006115497346553538\n",
      "Epoch 155/300\n",
      "Average training loss: 0.031066136267450122\n",
      "Average test loss: 0.006079126465237803\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03102799788945251\n",
      "Average test loss: 0.006704907561341921\n",
      "Epoch 157/300\n",
      "Average training loss: 0.031055793043639923\n",
      "Average test loss: 0.006190029513090849\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030963690631919435\n",
      "Average test loss: 0.006173150039795372\n",
      "Epoch 159/300\n",
      "Average training loss: 0.031052673276927737\n",
      "Average test loss: 0.006016149815999799\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03097000886996587\n",
      "Average test loss: 0.007541035485350424\n",
      "Epoch 161/300\n",
      "Average training loss: 0.030967540817128287\n",
      "Average test loss: 0.006118876073095534\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030908924377626844\n",
      "Average test loss: 0.0060625485794411765\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030938573938277033\n",
      "Average test loss: 0.006025315017749866\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030910570482412972\n",
      "Average test loss: 0.006042290002521542\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030939376569456525\n",
      "Average test loss: 0.006991093871494134\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03090003327363067\n",
      "Average test loss: 0.006240369913064771\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03088036592801412\n",
      "Average test loss: 0.006140013698488474\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03087608916229672\n",
      "Average test loss: 0.006161212336685922\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03082172765665584\n",
      "Average test loss: 0.006179525408479903\n",
      "Epoch 170/300\n",
      "Average training loss: 0.030839861868156328\n",
      "Average test loss: 0.006117578984548649\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03083998430106375\n",
      "Average test loss: 0.0062291477036972845\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030790923333830305\n",
      "Average test loss: 0.006155416094180611\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030835108551714157\n",
      "Average test loss: 0.006133132258223163\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03078638431429863\n",
      "Average test loss: 0.007210720390081406\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030811714407470492\n",
      "Average test loss: 0.006175418253574106\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030683701588047875\n",
      "Average test loss: 0.0062900385782122615\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030716527538167105\n",
      "Average test loss: 0.0060646155120597945\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03073052371210522\n",
      "Average test loss: 0.006195779687000646\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030710377688209216\n",
      "Average test loss: 0.0061149815581738945\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030699895608756276\n",
      "Average test loss: 0.006079597954534822\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030671616416838434\n",
      "Average test loss: 0.006179565936326981\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03064571210079723\n",
      "Average test loss: 0.0070084109587801825\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03065454614162445\n",
      "Average test loss: 0.006313286263909605\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030641973215672705\n",
      "Average test loss: 0.006147965159681109\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03062511012951533\n",
      "Average test loss: 0.006311795343127515\n",
      "Epoch 186/300\n",
      "Average training loss: 0.030642451610830096\n",
      "Average test loss: 0.006193246014830139\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0305972757074568\n",
      "Average test loss: 0.006209782546593083\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03058607285304202\n",
      "Average test loss: 0.006401975466973252\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030590382958451907\n",
      "Average test loss: 0.006575271873010529\n",
      "Epoch 190/300\n",
      "Average training loss: 0.030555633544921874\n",
      "Average test loss: 0.0061026474440263375\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03055422525935703\n",
      "Average test loss: 0.006206335701876216\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030566897574398254\n",
      "Average test loss: 0.0062897000809510545\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0305219147718615\n",
      "Average test loss: 0.006252554716335403\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03055730739235878\n",
      "Average test loss: 0.00612169643988212\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030490424633026123\n",
      "Average test loss: 0.006371327533904049\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030508639112114905\n",
      "Average test loss: 0.006273625799351268\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030505546503596836\n",
      "Average test loss: 0.006274838498483102\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030459200806087917\n",
      "Average test loss: 0.009986553613510397\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03049311516020033\n",
      "Average test loss: 0.0061086678658094674\n",
      "Epoch 200/300\n",
      "Average training loss: 0.030415634761253994\n",
      "Average test loss: 0.006160985995497969\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03041877597901556\n",
      "Average test loss: 0.00606374679836962\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030430868744850157\n",
      "Average test loss: 0.0064278637969659436\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030391582760545943\n",
      "Average test loss: 0.006158389263682895\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030401019781827927\n",
      "Average test loss: 0.006157193748487367\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0303634468946192\n",
      "Average test loss: 0.006329548753798008\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030397312892807855\n",
      "Average test loss: 0.006844191470493873\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030354062845309577\n",
      "Average test loss: 0.006183328747749329\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030357210415932867\n",
      "Average test loss: 0.10570106340779198\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0303730385336611\n",
      "Average test loss: 0.006149955532203118\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03032595138086213\n",
      "Average test loss: 0.006282522710661094\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030337875326474507\n",
      "Average test loss: 0.006169130271093713\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030298172033495375\n",
      "Average test loss: 0.0067365704423023596\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030338099989626142\n",
      "Average test loss: 0.006160800967365503\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030305136303106942\n",
      "Average test loss: 0.006210658292803499\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03027949388490783\n",
      "Average test loss: 0.006576014879677031\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030307045461403\n",
      "Average test loss: 0.0062441188080443275\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030249734787477386\n",
      "Average test loss: 0.006292295480974847\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030274868708517817\n",
      "Average test loss: 0.006204158143036895\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030235342966185675\n",
      "Average test loss: 0.006207370377249188\n",
      "Epoch 220/300\n",
      "Average training loss: 0.030233392632669873\n",
      "Average test loss: 0.006215567937327756\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030221194012297524\n",
      "Average test loss: 0.006097981582913134\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03024812097681893\n",
      "Average test loss: 0.006454379296551148\n",
      "Epoch 223/300\n",
      "Average training loss: 0.030173535947998365\n",
      "Average test loss: 0.006221130176550812\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03019086820549435\n",
      "Average test loss: 0.006243443700174491\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030175474362240898\n",
      "Average test loss: 0.006171333045181301\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03019993605878618\n",
      "Average test loss: 0.006222998194396496\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030158279481861325\n",
      "Average test loss: 0.006365913586484061\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03018813756108284\n",
      "Average test loss: 0.006410323865711689\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03018854225344128\n",
      "Average test loss: 0.006198191136949592\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03010994577407837\n",
      "Average test loss: 0.006239562426590258\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03016101794772678\n",
      "Average test loss: 0.006322749382919736\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030109647408127785\n",
      "Average test loss: 0.006429840709186263\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030110317417316967\n",
      "Average test loss: 0.006562539546853966\n",
      "Epoch 234/300\n",
      "Average training loss: 0.030122797952757942\n",
      "Average test loss: 0.006253352470695973\n",
      "Epoch 235/300\n",
      "Average training loss: 0.030091929892698925\n",
      "Average test loss: 0.006219849240448739\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030038646688063938\n",
      "Average test loss: 0.006822281191332473\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030077881508403355\n",
      "Average test loss: 0.006274745790080891\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03001571469174491\n",
      "Average test loss: 0.006215633405993382\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030052536732620663\n",
      "Average test loss: 0.00638849478132195\n",
      "Epoch 240/300\n",
      "Average training loss: 0.030112057460678947\n",
      "Average test loss: 0.006264041831923856\n",
      "Epoch 241/300\n",
      "Average training loss: 0.030035587492916318\n",
      "Average test loss: 0.006342961451245679\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03001956215997537\n",
      "Average test loss: 0.006231622670259741\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0299903943406211\n",
      "Average test loss: 0.006136499052246411\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03006800240278244\n",
      "Average test loss: 0.006698827036139038\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030023250020212597\n",
      "Average test loss: 0.006452706306344933\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03000187537405226\n",
      "Average test loss: 0.006172672791199552\n",
      "Epoch 247/300\n",
      "Average training loss: 0.029948686947425206\n",
      "Average test loss: 0.0062968480793966185\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02994155998693572\n",
      "Average test loss: 0.006291216685126225\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02997031306558185\n",
      "Average test loss: 0.006437326510333353\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030015625617570346\n",
      "Average test loss: 0.006162086806777451\n",
      "Epoch 251/300\n",
      "Average training loss: 0.029952083306180106\n",
      "Average test loss: 0.007231878168880939\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029899428957038454\n",
      "Average test loss: 0.006548882914500104\n",
      "Epoch 253/300\n",
      "Average training loss: 0.029902429322401683\n",
      "Average test loss: 0.006347744336558713\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029903642108043034\n",
      "Average test loss: 0.0063437969386577605\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029936935623486836\n",
      "Average test loss: 0.006960042418705093\n",
      "Epoch 256/300\n",
      "Average training loss: 0.029900487865010896\n",
      "Average test loss: 0.007080555747780535\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02991518666015731\n",
      "Average test loss: 0.0062303658578958775\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02989266898896959\n",
      "Average test loss: 0.006249582384195593\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02986360889673233\n",
      "Average test loss: 0.006353152549101247\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029885810289118023\n",
      "Average test loss: 0.006308660690155294\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02988769177099069\n",
      "Average test loss: 0.0063520434598128\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029812254700395797\n",
      "Average test loss: 0.0077397123591767415\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02984523644712236\n",
      "Average test loss: 0.006359888536234697\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029815686335166295\n",
      "Average test loss: 0.006213616099622514\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029853541437122558\n",
      "Average test loss: 0.006206441137111849\n",
      "Epoch 266/300\n",
      "Average training loss: 0.029805245131254196\n",
      "Average test loss: 0.006299969612724251\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02980666912595431\n",
      "Average test loss: 0.006316131540884574\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029822708898120456\n",
      "Average test loss: 0.00631573259168201\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02981272100408872\n",
      "Average test loss: 0.007594490649385585\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029791740417480467\n",
      "Average test loss: 0.00644517299408714\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029824371804793675\n",
      "Average test loss: 0.0069028917199207675\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0297664315700531\n",
      "Average test loss: 0.0062999527425401745\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029726501948303646\n",
      "Average test loss: 0.006280009815262424\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029730178005165524\n",
      "Average test loss: 0.006332759888635742\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02981068088610967\n",
      "Average test loss: 0.006217693168256018\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02973207792805301\n",
      "Average test loss: 0.006302131497611602\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029761705367101564\n",
      "Average test loss: 0.006327075598140558\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02973195810450448\n",
      "Average test loss: 0.006715582617868979\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02971899660097228\n",
      "Average test loss: 0.006319378844151894\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0297127777437369\n",
      "Average test loss: 0.007045777907801999\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029706691382659807\n",
      "Average test loss: 0.010558264564308856\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029726618399222693\n",
      "Average test loss: 0.007366630062460899\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02968329216374291\n",
      "Average test loss: 0.0063920557457539775\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029721355011065802\n",
      "Average test loss: 0.0062965280848244825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02967204294105371\n",
      "Average test loss: 0.006460259512894684\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029698637310001587\n",
      "Average test loss: 0.006395744874659512\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029645691951115927\n",
      "Average test loss: 0.014973305182324516\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029709196713235644\n",
      "Average test loss: 0.006195320965929164\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029632340613338682\n",
      "Average test loss: 0.007651027183979749\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02964981473154492\n",
      "Average test loss: 0.007683619383722543\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02962322058611446\n",
      "Average test loss: 0.006405385193725427\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02965992592109574\n",
      "Average test loss: 0.006605335500091314\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02962051545580228\n",
      "Average test loss: 0.006559496403154399\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02961109073128965\n",
      "Average test loss: 0.006840570801248153\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02961275992459721\n",
      "Average test loss: 0.006653533426423867\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029622842710879112\n",
      "Average test loss: 0.006560343057331112\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02960745734307501\n",
      "Average test loss: 0.006476255178865459\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029598479870292876\n",
      "Average test loss: 0.006357281518065267\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029555372135506734\n",
      "Average test loss: 0.006605568007048633\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02960270110103819\n",
      "Average test loss: 0.006337448144124614\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13012820627954272\n",
      "Average test loss: 0.008023200365404288\n",
      "Epoch 2/300\n",
      "Average training loss: 0.049381809075673425\n",
      "Average test loss: 0.006807978717403279\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04439140243993865\n",
      "Average test loss: 0.006523694827324815\n",
      "Epoch 4/300\n",
      "Average training loss: 0.041617730581098134\n",
      "Average test loss: 0.006338810796125067\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03959481127725707\n",
      "Average test loss: 0.006544979404658079\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03815850580400891\n",
      "Average test loss: 0.005756339273932908\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03662124195363786\n",
      "Average test loss: 0.005825687675633364\n",
      "Epoch 8/300\n",
      "Average training loss: 0.035518765052159625\n",
      "Average test loss: 0.005978796848406394\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03477819953693284\n",
      "Average test loss: 0.005610446141825782\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03395238222016229\n",
      "Average test loss: 0.005779965139511558\n",
      "Epoch 11/300\n",
      "Average training loss: 0.033314513410131134\n",
      "Average test loss: 0.005848728594680627\n",
      "Epoch 12/300\n",
      "Average training loss: 0.032793777250581316\n",
      "Average test loss: 0.005526292882445786\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03235069951083925\n",
      "Average test loss: 0.0054688998303479616\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03199498586191071\n",
      "Average test loss: 0.0052540374650723405\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03161882799863815\n",
      "Average test loss: 0.006639027731700076\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03139499032828543\n",
      "Average test loss: 0.005512534812092781\n",
      "Epoch 17/300\n",
      "Average training loss: 0.031005352175898022\n",
      "Average test loss: 0.005159563612606791\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030752556886937885\n",
      "Average test loss: 0.005364928152412176\n",
      "Epoch 19/300\n",
      "Average training loss: 0.030534885303841696\n",
      "Average test loss: 0.005153635404176182\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03034171693854862\n",
      "Average test loss: 0.005085489810754855\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03016090401013692\n",
      "Average test loss: 0.0050304206605586745\n",
      "Epoch 22/300\n",
      "Average training loss: 0.030036963827080196\n",
      "Average test loss: 0.0053331545309887995\n",
      "Epoch 23/300\n",
      "Average training loss: 0.029908030622535282\n",
      "Average test loss: 0.00497843445589145\n",
      "Epoch 24/300\n",
      "Average training loss: 0.029720862835645675\n",
      "Average test loss: 0.0049761217844982945\n",
      "Epoch 25/300\n",
      "Average training loss: 0.029548624073465667\n",
      "Average test loss: 0.00495442938266529\n",
      "Epoch 26/300\n",
      "Average training loss: 0.029469101250171662\n",
      "Average test loss: 0.004960025506301059\n",
      "Epoch 27/300\n",
      "Average training loss: 0.029413295971022713\n",
      "Average test loss: 0.004950841249691115\n",
      "Epoch 28/300\n",
      "Average training loss: 0.029257457971572875\n",
      "Average test loss: 0.004896163701183266\n",
      "Epoch 29/300\n",
      "Average training loss: 0.029204388936360678\n",
      "Average test loss: 0.005055354942878087\n",
      "Epoch 30/300\n",
      "Average training loss: 0.029114758683575524\n",
      "Average test loss: 0.0050586786439849275\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02897349465886752\n",
      "Average test loss: 0.00488318684986896\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02887162774801254\n",
      "Average test loss: 0.005039813390208615\n",
      "Epoch 33/300\n",
      "Average training loss: 0.028799493003222678\n",
      "Average test loss: 0.00485238902664019\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02877421878443824\n",
      "Average test loss: 0.00518110521344675\n",
      "Epoch 35/300\n",
      "Average training loss: 0.028666519280936983\n",
      "Average test loss: 0.004862145159393549\n",
      "Epoch 36/300\n",
      "Average training loss: 0.028593929077188173\n",
      "Average test loss: 0.004921874264876048\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02852965924806065\n",
      "Average test loss: 0.004908507857885626\n",
      "Epoch 38/300\n",
      "Average training loss: 0.028484499063756732\n",
      "Average test loss: 0.004857832678904136\n",
      "Epoch 39/300\n",
      "Average training loss: 0.028410449349217944\n",
      "Average test loss: 0.004869048056917058\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02836926439570056\n",
      "Average test loss: 0.004836991428501076\n",
      "Epoch 41/300\n",
      "Average training loss: 0.028330426808860568\n",
      "Average test loss: 0.004810009287049373\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02825578729973899\n",
      "Average test loss: 0.00494841556954715\n",
      "Epoch 43/300\n",
      "Average training loss: 0.028200768445928892\n",
      "Average test loss: 0.004826605656080776\n",
      "Epoch 44/300\n",
      "Average training loss: 0.028202023973067602\n",
      "Average test loss: 0.0048696011395918\n",
      "Epoch 45/300\n",
      "Average training loss: 0.028100688078337245\n",
      "Average test loss: 0.005122335205061568\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02806238336695565\n",
      "Average test loss: 0.004805080411748754\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02802364793419838\n",
      "Average test loss: 0.004845611084252596\n",
      "Epoch 48/300\n",
      "Average training loss: 0.027964122970898946\n",
      "Average test loss: 0.004805329703622394\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02791649654838774\n",
      "Average test loss: 0.004784379213220544\n",
      "Epoch 50/300\n",
      "Average training loss: 0.027952165732781094\n",
      "Average test loss: 0.004904170766058895\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02786175427171919\n",
      "Average test loss: 0.0048317879490140415\n",
      "Epoch 52/300\n",
      "Average training loss: 0.027848777876959906\n",
      "Average test loss: 0.0047891922398573825\n",
      "Epoch 53/300\n",
      "Average training loss: 0.027770244704352486\n",
      "Average test loss: 0.00477745487416784\n",
      "Epoch 54/300\n",
      "Average training loss: 0.027724170085456638\n",
      "Average test loss: 0.0048278586928629215\n",
      "Epoch 55/300\n",
      "Average training loss: 0.027714523682991665\n",
      "Average test loss: 0.0051510366317298675\n",
      "Epoch 56/300\n",
      "Average training loss: 0.027674115159445338\n",
      "Average test loss: 0.004925239870738652\n",
      "Epoch 57/300\n",
      "Average training loss: 0.027703299274047216\n",
      "Average test loss: 0.00492524892422888\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02758950820234087\n",
      "Average test loss: 0.004780782390592827\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02755977411899302\n",
      "Average test loss: 0.0049806166903840175\n",
      "Epoch 60/300\n",
      "Average training loss: 0.027526470333337785\n",
      "Average test loss: 0.004851726315915584\n",
      "Epoch 61/300\n",
      "Average training loss: 0.027514927476644517\n",
      "Average test loss: 0.004772028473516305\n",
      "Epoch 62/300\n",
      "Average training loss: 0.027478518260849846\n",
      "Average test loss: 0.004774001021352079\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02747104540301694\n",
      "Average test loss: 0.005437365423474047\n",
      "Epoch 64/300\n",
      "Average training loss: 0.027404989790585307\n",
      "Average test loss: 0.005642674105448856\n",
      "Epoch 65/300\n",
      "Average training loss: 0.027379063998659452\n",
      "Average test loss: 0.0047841240415970486\n",
      "Epoch 66/300\n",
      "Average training loss: 0.027330370811952484\n",
      "Average test loss: 0.004884962533083227\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02734626814391878\n",
      "Average test loss: 0.0048168701682653695\n",
      "Epoch 68/300\n",
      "Average training loss: 0.027302035894658832\n",
      "Average test loss: 0.004874233508689536\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02728652696477042\n",
      "Average test loss: 0.004829057876434591\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02727249211404059\n",
      "Average test loss: 0.004754347891857227\n",
      "Epoch 71/300\n",
      "Average training loss: 0.027256309845381313\n",
      "Average test loss: 0.004733349265323745\n",
      "Epoch 72/300\n",
      "Average training loss: 0.027223952861295805\n",
      "Average test loss: 0.004883938962386714\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02716870330605242\n",
      "Average test loss: 0.019626689364512763\n",
      "Epoch 74/300\n",
      "Average training loss: 0.027174087792634964\n",
      "Average test loss: 0.0047592906115783585\n",
      "Epoch 75/300\n",
      "Average training loss: 0.027136200793915325\n",
      "Average test loss: 0.004809089466101594\n",
      "Epoch 76/300\n",
      "Average training loss: 0.027096011037627855\n",
      "Average test loss: 0.00477498658042815\n",
      "Epoch 77/300\n",
      "Average training loss: 0.027118675647510422\n",
      "Average test loss: 0.005048401638865471\n",
      "Epoch 78/300\n",
      "Average training loss: 0.027046826019883157\n",
      "Average test loss: 0.005453321420484119\n",
      "Epoch 79/300\n",
      "Average training loss: 0.027057968775431315\n",
      "Average test loss: 0.005096706765807337\n",
      "Epoch 80/300\n",
      "Average training loss: 0.027032904124922222\n",
      "Average test loss: 0.005093467368433873\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02700349923306041\n",
      "Average test loss: 0.005475465598205725\n",
      "Epoch 82/300\n",
      "Average training loss: 0.026980809087554614\n",
      "Average test loss: 0.004792051261497869\n",
      "Epoch 83/300\n",
      "Average training loss: 0.026949464407232074\n",
      "Average test loss: 0.0050530347819957465\n",
      "Epoch 84/300\n",
      "Average training loss: 0.026935260343882772\n",
      "Average test loss: 0.004742902849283483\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02692896463804775\n",
      "Average test loss: 0.004846554309957557\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02688814109398259\n",
      "Average test loss: 0.005062813048975335\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0268849259449376\n",
      "Average test loss: 0.004845262652056085\n",
      "Epoch 88/300\n",
      "Average training loss: 0.026868627437286907\n",
      "Average test loss: 0.00475642194184992\n",
      "Epoch 89/300\n",
      "Average training loss: 0.026823648717668323\n",
      "Average test loss: 0.00491269743649496\n",
      "Epoch 90/300\n",
      "Average training loss: 0.026800315462880666\n",
      "Average test loss: 0.005420550677097506\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0267709950308005\n",
      "Average test loss: 0.004757640473130677\n",
      "Epoch 92/300\n",
      "Average training loss: 0.026754939410421584\n",
      "Average test loss: 0.004806408073753119\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02676171066529221\n",
      "Average test loss: 0.004747215278239714\n",
      "Epoch 94/300\n",
      "Average training loss: 0.026768622861968145\n",
      "Average test loss: 0.004768066338987814\n",
      "Epoch 95/300\n",
      "Average training loss: 0.026738218168417614\n",
      "Average test loss: 0.005024937392936813\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02667530059483316\n",
      "Average test loss: 0.00476759047102597\n",
      "Epoch 97/300\n",
      "Average training loss: 0.026681335270404817\n",
      "Average test loss: 0.004799629434115357\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026649196599920592\n",
      "Average test loss: 0.004813113111174769\n",
      "Epoch 99/300\n",
      "Average training loss: 0.026623427046669854\n",
      "Average test loss: 0.004730409269531568\n",
      "Epoch 100/300\n",
      "Average training loss: 0.026627286617954573\n",
      "Average test loss: 0.004773241085310777\n",
      "Epoch 101/300\n",
      "Average training loss: 0.026622107156448895\n",
      "Average test loss: 0.0048439219422224495\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02662601022422314\n",
      "Average test loss: 0.004839174629292555\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02658022890985012\n",
      "Average test loss: 0.004787479658714599\n",
      "Epoch 104/300\n",
      "Average training loss: 0.026558628486262427\n",
      "Average test loss: 0.004860397549139129\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026552404479848014\n",
      "Average test loss: 0.004780232272628281\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02654182533091969\n",
      "Average test loss: 0.004885853672607078\n",
      "Epoch 107/300\n",
      "Average training loss: 0.026512444529268476\n",
      "Average test loss: 0.004852124655826224\n",
      "Epoch 108/300\n",
      "Average training loss: 0.026488367915153504\n",
      "Average test loss: 0.004759525294105212\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02648497712612152\n",
      "Average test loss: 0.004915981154061026\n",
      "Epoch 110/300\n",
      "Average training loss: 0.026490841388702394\n",
      "Average test loss: 0.0047719362249804866\n",
      "Epoch 111/300\n",
      "Average training loss: 0.026413670168982613\n",
      "Average test loss: 0.004765243207414945\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0264165091233121\n",
      "Average test loss: 0.0049369616525040735\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02646337851881981\n",
      "Average test loss: 0.00546116857479016\n",
      "Epoch 114/300\n",
      "Average training loss: 0.026405926211012735\n",
      "Average test loss: 0.005215489845722914\n",
      "Epoch 115/300\n",
      "Average training loss: 0.026403651783863703\n",
      "Average test loss: 0.005002310356953078\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02636746495631006\n",
      "Average test loss: 0.004755891778816779\n",
      "Epoch 117/300\n",
      "Average training loss: 0.026367995927731196\n",
      "Average test loss: 0.005175999381475978\n",
      "Epoch 118/300\n",
      "Average training loss: 0.026368137367897563\n",
      "Average test loss: 0.004928984338003728\n",
      "Epoch 119/300\n",
      "Average training loss: 0.026361431252625255\n",
      "Average test loss: 0.004807446130861839\n",
      "Epoch 120/300\n",
      "Average training loss: 0.026288141407900388\n",
      "Average test loss: 0.004753205597400665\n",
      "Epoch 121/300\n",
      "Average training loss: 0.026303193502955968\n",
      "Average test loss: 0.004790506264401807\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02627445615331332\n",
      "Average test loss: 0.004901970809118615\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02627373020350933\n",
      "Average test loss: 0.004831683884685238\n",
      "Epoch 124/300\n",
      "Average training loss: 0.026247268312507205\n",
      "Average test loss: 0.004752518870350387\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02623952325516277\n",
      "Average test loss: 0.004779377756640315\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02624867081311014\n",
      "Average test loss: 0.004815531981488069\n",
      "Epoch 127/300\n",
      "Average training loss: 0.026221533685922623\n",
      "Average test loss: 0.004779421580334504\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02621061668296655\n",
      "Average test loss: 0.004801591191026899\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02626070653233263\n",
      "Average test loss: 0.005171603072434664\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0261673284471035\n",
      "Average test loss: 0.040978599215547244\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02618138457172447\n",
      "Average test loss: 0.004924105188498894\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02617333486676216\n",
      "Average test loss: 0.0048033740412857795\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0261246097418997\n",
      "Average test loss: 0.007415668449882004\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02612748547560639\n",
      "Average test loss: 0.004769959527585242\n",
      "Epoch 135/300\n",
      "Average training loss: 0.026114487040374015\n",
      "Average test loss: 0.004887303573803769\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02610976822508706\n",
      "Average test loss: 0.00484909975528717\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0260963660362694\n",
      "Average test loss: 0.004810884374297327\n",
      "Epoch 138/300\n",
      "Average training loss: 0.026074822414252494\n",
      "Average test loss: 0.0047858457039627765\n",
      "Epoch 139/300\n",
      "Average training loss: 0.026046024686760372\n",
      "Average test loss: 0.0048211546987295155\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02607936439745956\n",
      "Average test loss: 0.004800892427563667\n",
      "Epoch 141/300\n",
      "Average training loss: 0.026052493224541347\n",
      "Average test loss: 0.005150638535204861\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02602099349266953\n",
      "Average test loss: 0.00480778173233072\n",
      "Epoch 143/300\n",
      "Average training loss: 0.025987994641065597\n",
      "Average test loss: 0.005087270043790341\n",
      "Epoch 144/300\n",
      "Average training loss: 0.025993432947331006\n",
      "Average test loss: 0.004805992101215654\n",
      "Epoch 145/300\n",
      "Average training loss: 0.026006576399008434\n",
      "Average test loss: 0.005278031906319989\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02594965153104729\n",
      "Average test loss: 0.004843142122030258\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02600843601425489\n",
      "Average test loss: 0.004880142415563266\n",
      "Epoch 148/300\n",
      "Average training loss: 0.025955423209402296\n",
      "Average test loss: 0.004999467697408464\n",
      "Epoch 149/300\n",
      "Average training loss: 0.025985897827479575\n",
      "Average test loss: 0.005016858065293895\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02592560331688987\n",
      "Average test loss: 0.00483128916968902\n",
      "Epoch 151/300\n",
      "Average training loss: 0.025926572903990745\n",
      "Average test loss: 0.004819657024410036\n",
      "Epoch 152/300\n",
      "Average training loss: 0.025915320686168142\n",
      "Average test loss: 0.004937093865954214\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0258826096256574\n",
      "Average test loss: 0.005565575015213754\n",
      "Epoch 154/300\n",
      "Average training loss: 0.025911878291103575\n",
      "Average test loss: 0.004827284830311934\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02587234148879846\n",
      "Average test loss: 0.004806027246846093\n",
      "Epoch 156/300\n",
      "Average training loss: 0.025870485696527692\n",
      "Average test loss: 0.004757280576974153\n",
      "Epoch 157/300\n",
      "Average training loss: 0.025828174960282113\n",
      "Average test loss: 0.004831576459523704\n",
      "Epoch 158/300\n",
      "Average training loss: 0.025821076406372916\n",
      "Average test loss: 0.004866798605769873\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02586489549941487\n",
      "Average test loss: 0.0048098662429385715\n",
      "Epoch 160/300\n",
      "Average training loss: 0.025811803685294257\n",
      "Average test loss: 0.004848891214778026\n",
      "Epoch 161/300\n",
      "Average training loss: 0.025836837394369972\n",
      "Average test loss: 0.004795818681104316\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02580575327409638\n",
      "Average test loss: 0.0047806624985403485\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02576903866065873\n",
      "Average test loss: 0.006537485914511813\n",
      "Epoch 164/300\n",
      "Average training loss: 0.025763763639662002\n",
      "Average test loss: 0.004822377793076965\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02580498065882259\n",
      "Average test loss: 0.004802381513847245\n",
      "Epoch 166/300\n",
      "Average training loss: 0.025752334371209145\n",
      "Average test loss: 0.004809878952387306\n",
      "Epoch 167/300\n",
      "Average training loss: 0.025716163312395415\n",
      "Average test loss: 0.00483094753138721\n",
      "Epoch 168/300\n",
      "Average training loss: 0.025718220189213752\n",
      "Average test loss: 0.004906899555689759\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02571747044722239\n",
      "Average test loss: 0.005030124528540505\n",
      "Epoch 170/300\n",
      "Average training loss: 0.025726905832688014\n",
      "Average test loss: 0.004911918793908424\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02570794936021169\n",
      "Average test loss: 0.004829879579444726\n",
      "Epoch 172/300\n",
      "Average training loss: 0.025700684587160745\n",
      "Average test loss: 0.004895540553662512\n",
      "Epoch 173/300\n",
      "Average training loss: 0.025679132600625355\n",
      "Average test loss: 0.004944694959868988\n",
      "Epoch 174/300\n",
      "Average training loss: 0.025729906557334793\n",
      "Average test loss: 0.004966993470779724\n",
      "Epoch 175/300\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive-.1/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0ab28-8a41-4871-9fbe-fa321c520661",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222a5dd-cfec-45d0-acc7-1b083efa972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive-.1/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153fd41-6c7d-41db-ba5b-df59bc7ea557",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce61d8-bc6f-4624-9cce-57f62d126863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive-.1/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd46e4-ff9f-4da2-a90a-6287ea65d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
