{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.025)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.025)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.025)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.025)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.060433595447076695\n",
      "Average test loss: 0.005108636341161198\n",
      "Epoch 2/300\n",
      "Average training loss: 0.023992376729846002\n",
      "Average test loss: 0.0046805844782955115\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022902614570326274\n",
      "Average test loss: 0.004517647260593043\n",
      "Epoch 4/300\n",
      "Average training loss: 0.022414540673295655\n",
      "Average test loss: 0.004515911019303732\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02213797199394968\n",
      "Average test loss: 0.004542382898430029\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02193121287557814\n",
      "Average test loss: 0.004392952525367339\n",
      "Epoch 7/300\n",
      "Average training loss: 0.021783601290649837\n",
      "Average test loss: 0.004368152441663875\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02166708174182309\n",
      "Average test loss: 0.004357085736261474\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02155657383468416\n",
      "Average test loss: 0.004320474741773473\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021468567435940108\n",
      "Average test loss: 0.00434366403416627\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02139972946047783\n",
      "Average test loss: 0.004306866116821766\n",
      "Epoch 12/300\n",
      "Average training loss: 0.021318581473496225\n",
      "Average test loss: 0.00427201506950789\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021263984363940027\n",
      "Average test loss: 0.004262207522160477\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02120405209892326\n",
      "Average test loss: 0.004273146211273141\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021148418984479375\n",
      "Average test loss: 0.004241231959727075\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021110528707504274\n",
      "Average test loss: 0.0042289310410204865\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021044065036707454\n",
      "Average test loss: 0.004216987239817779\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02100372292763657\n",
      "Average test loss: 0.004198803261336353\n",
      "Epoch 19/300\n",
      "Average training loss: 0.020939317658543587\n",
      "Average test loss: 0.004229442383680079\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020910691978202927\n",
      "Average test loss: 0.00420646556880739\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020872646633121703\n",
      "Average test loss: 0.004203620144890415\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020835140788720712\n",
      "Average test loss: 0.0041872829285760724\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020783794571955998\n",
      "Average test loss: 0.004188683837859167\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020763853806588385\n",
      "Average test loss: 0.004213328983013829\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02073049224830336\n",
      "Average test loss: 0.004154166596631209\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020703089151117536\n",
      "Average test loss: 0.004152677548428377\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020675113538901013\n",
      "Average test loss: 0.004147184691495365\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020635869926876493\n",
      "Average test loss: 0.004135326056430737\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02061279083126121\n",
      "Average test loss: 0.004134655572887924\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02059318205051952\n",
      "Average test loss: 0.004220075775351789\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020573003254002995\n",
      "Average test loss: 0.004131346169031329\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020543331265449525\n",
      "Average test loss: 0.004118694172551235\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020530106117328008\n",
      "Average test loss: 0.004127753843863805\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02050142300128937\n",
      "Average test loss: 0.004115528569039371\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020484818432066176\n",
      "Average test loss: 0.004114556956208415\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020464446018139522\n",
      "Average test loss: 0.004105466480470366\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020465783594383134\n",
      "Average test loss: 0.004126625415558616\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020438743342955906\n",
      "Average test loss: 0.00410166927261485\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020417429861095217\n",
      "Average test loss: 0.004133376143044896\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020396800705128247\n",
      "Average test loss: 0.0040931094673772655\n",
      "Epoch 41/300\n",
      "Average training loss: 0.020385039812988705\n",
      "Average test loss: 0.004107488242288431\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020375897179047267\n",
      "Average test loss: 0.004092933143592543\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020360985957913927\n",
      "Average test loss: 0.004083813277383645\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02034855246875021\n",
      "Average test loss: 0.004083201175348626\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020341402605175973\n",
      "Average test loss: 0.004089211853014098\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020320502766304545\n",
      "Average test loss: 0.004079114619228575\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02030830653011799\n",
      "Average test loss: 0.004088606436426441\n",
      "Epoch 48/300\n",
      "Average training loss: 0.020297415665454333\n",
      "Average test loss: 0.004076451282327374\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020278161712818675\n",
      "Average test loss: 0.004076233071999417\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020267949328654344\n",
      "Average test loss: 0.004075683067242305\n",
      "Epoch 51/300\n",
      "Average training loss: 0.020266956692768467\n",
      "Average test loss: 0.004071028735074732\n",
      "Epoch 52/300\n",
      "Average training loss: 0.020256514860524072\n",
      "Average test loss: 0.004077768397414022\n",
      "Epoch 53/300\n",
      "Average training loss: 0.020244323891070153\n",
      "Average test loss: 0.0040729621292816265\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02022613857024246\n",
      "Average test loss: 0.0040645505690740215\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02021906796594461\n",
      "Average test loss: 0.004072216191225581\n",
      "Epoch 56/300\n",
      "Average training loss: 0.020209501289659076\n",
      "Average test loss: 0.004073241887821092\n",
      "Epoch 57/300\n",
      "Average training loss: 0.020198198553588655\n",
      "Average test loss: 0.00407235165954464\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020196033221152095\n",
      "Average test loss: 0.004065204880717728\n",
      "Epoch 59/300\n",
      "Average training loss: 0.020183367924557792\n",
      "Average test loss: 0.004066693900773923\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02017096699608697\n",
      "Average test loss: 0.004073826401390963\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02016353212462531\n",
      "Average test loss: 0.0040649912148300145\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020154050229324236\n",
      "Average test loss: 0.0040650787891613116\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0201400246752633\n",
      "Average test loss: 0.004055233359750774\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02013894958794117\n",
      "Average test loss: 0.004083446089178324\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020131540064182548\n",
      "Average test loss: 0.00406781259799997\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020121407036979993\n",
      "Average test loss: 0.004069902522696389\n",
      "Epoch 67/300\n",
      "Average training loss: 0.020110385749075147\n",
      "Average test loss: 0.00405133497963349\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020104518156912592\n",
      "Average test loss: 0.004052597334401475\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020103196036484507\n",
      "Average test loss: 0.004077699636419614\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02009043572180801\n",
      "Average test loss: 0.004050540930695004\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020080075348416965\n",
      "Average test loss: 0.004063728615641594\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020066861051652167\n",
      "Average test loss: 0.004054851940315631\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02006546600162983\n",
      "Average test loss: 0.004047746349953943\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020058531608846453\n",
      "Average test loss: 0.004047922696711288\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020046310979459022\n",
      "Average test loss: 0.00405045934787227\n",
      "Epoch 76/300\n",
      "Average training loss: 0.020044436608751614\n",
      "Average test loss: 0.004057843351115783\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020035606369376183\n",
      "Average test loss: 0.004049711359457837\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0200272975564003\n",
      "Average test loss: 0.004048931740845243\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02001577680144045\n",
      "Average test loss: 0.00404895326640043\n",
      "Epoch 80/300\n",
      "Average training loss: 0.020012654799554084\n",
      "Average test loss: 0.0040419622816973266\n",
      "Epoch 81/300\n",
      "Average training loss: 0.020007000707917742\n",
      "Average test loss: 0.004053550914550821\n",
      "Epoch 82/300\n",
      "Average training loss: 0.020002472596863906\n",
      "Average test loss: 0.004042401597731643\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019991446553005113\n",
      "Average test loss: 0.004045542122175296\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01998893581330776\n",
      "Average test loss: 0.004062919396079249\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01997655261721876\n",
      "Average test loss: 0.0040441348792778125\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019971713739136854\n",
      "Average test loss: 0.00404705178985993\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019963361111780008\n",
      "Average test loss: 0.004045526465194093\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019961765113804077\n",
      "Average test loss: 0.004043202037612597\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01995822875532839\n",
      "Average test loss: 0.004050660953960485\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019945094605286918\n",
      "Average test loss: 0.00404285749565396\n",
      "Epoch 91/300\n",
      "Average training loss: 0.019928743936949305\n",
      "Average test loss: 0.004050522017396158\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019933608811762597\n",
      "Average test loss: 0.004054822584407197\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019921822658843465\n",
      "Average test loss: 0.004038232783890433\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01992208576036824\n",
      "Average test loss: 0.004077602592193418\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0199187700814671\n",
      "Average test loss: 0.004044648228834073\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019904160435001057\n",
      "Average test loss: 0.00404811004238824\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01990004921787315\n",
      "Average test loss: 0.004044793314817879\n",
      "Epoch 98/300\n",
      "Average training loss: 0.019894086394045087\n",
      "Average test loss: 0.004057009717035625\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019889568878544702\n",
      "Average test loss: 0.004056193034682009\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019882502923409144\n",
      "Average test loss: 0.0040419072719911735\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019872015888492266\n",
      "Average test loss: 0.00404189407101108\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019861680993603335\n",
      "Average test loss: 0.004043327283321156\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019858555205994183\n",
      "Average test loss: 0.00406939595606592\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01985215553641319\n",
      "Average test loss: 0.004040076376249393\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019853420413202708\n",
      "Average test loss: 0.004040303854892651\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019839882174299824\n",
      "Average test loss: 0.0040440748116622365\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01983176495300399\n",
      "Average test loss: 0.004062949516293076\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019825522230731116\n",
      "Average test loss: 0.00406060539972451\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019825391319890816\n",
      "Average test loss: 0.004048727449650566\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01981738852130042\n",
      "Average test loss: 0.004036728396597836\n",
      "Epoch 111/300\n",
      "Average training loss: 0.019812679259313477\n",
      "Average test loss: 0.0040557024892833495\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01980868401295609\n",
      "Average test loss: 0.004035652953303523\n",
      "Epoch 113/300\n",
      "Average training loss: 0.019798990026116372\n",
      "Average test loss: 0.004059941899031401\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019789313311378162\n",
      "Average test loss: 0.004101603092004856\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01978397578994433\n",
      "Average test loss: 0.004057532501924369\n",
      "Epoch 116/300\n",
      "Average training loss: 0.019788223715292083\n",
      "Average test loss: 0.004049252943239278\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019783151262336308\n",
      "Average test loss: 0.00404256785247061\n",
      "Epoch 118/300\n",
      "Average training loss: 0.019771577255593406\n",
      "Average test loss: 0.0040685958593255946\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0197670566936334\n",
      "Average test loss: 0.004045773604263862\n",
      "Epoch 120/300\n",
      "Average training loss: 0.019763246739904084\n",
      "Average test loss: 0.004044477416823308\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01975462318625715\n",
      "Average test loss: 0.004055806121892399\n",
      "Epoch 122/300\n",
      "Average training loss: 0.019747661456465723\n",
      "Average test loss: 0.0040592822226592235\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01973724825017982\n",
      "Average test loss: 0.0040615939282708695\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019740119227104717\n",
      "Average test loss: 0.00409200215112004\n",
      "Epoch 125/300\n",
      "Average training loss: 0.019734378574623002\n",
      "Average test loss: 0.004095329821109771\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01972766754693455\n",
      "Average test loss: 0.004064060803295837\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019716880016028882\n",
      "Average test loss: 0.004043668094608519\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019717519611120223\n",
      "Average test loss: 0.004048283701969518\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019699710016449292\n",
      "Average test loss: 0.004075605047659741\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01970487569603655\n",
      "Average test loss: 0.004049040177630053\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01969569417834282\n",
      "Average test loss: 0.00403414623460008\n",
      "Epoch 132/300\n",
      "Average training loss: 0.019687962263822554\n",
      "Average test loss: 0.00403929306483931\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01968312338987986\n",
      "Average test loss: 0.004045219787706931\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01968361634016037\n",
      "Average test loss: 0.004046979739848111\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019681727583209675\n",
      "Average test loss: 0.004069514459619919\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019674284368753434\n",
      "Average test loss: 0.00404451018716726\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01965967775715722\n",
      "Average test loss: 0.0040329070517586335\n",
      "Epoch 138/300\n",
      "Average training loss: 0.019655573329991766\n",
      "Average test loss: 0.004059123538641466\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019652768125136694\n",
      "Average test loss: 0.00407011023743285\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01964893490738339\n",
      "Average test loss: 0.0040817580881218115\n",
      "Epoch 141/300\n",
      "Average training loss: 0.019637845574153795\n",
      "Average test loss: 0.004062146251814233\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0196322159162826\n",
      "Average test loss: 0.004066567256425818\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01963000774052408\n",
      "Average test loss: 0.004065858909860253\n",
      "Epoch 144/300\n",
      "Average training loss: 0.019630859340230625\n",
      "Average test loss: 0.004048688252353006\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01960904243754016\n",
      "Average test loss: 0.004061799093253083\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0196232460025284\n",
      "Average test loss: 0.004041445127791829\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01960740979678101\n",
      "Average test loss: 0.004047140525446998\n",
      "Epoch 148/300\n",
      "Average training loss: 0.019610164384875034\n",
      "Average test loss: 0.0040379062253567905\n",
      "Epoch 149/300\n",
      "Average training loss: 0.019590022698872618\n",
      "Average test loss: 0.004067814044654369\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01958889451622963\n",
      "Average test loss: 0.004064103735403882\n",
      "Epoch 151/300\n",
      "Average training loss: 0.019590540889236664\n",
      "Average test loss: 0.004058673372699155\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019581644675797885\n",
      "Average test loss: 0.004043724824984868\n",
      "Epoch 153/300\n",
      "Average training loss: 0.019571933744682204\n",
      "Average test loss: 0.0040598387279444275\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01957311800784535\n",
      "Average test loss: 0.004074113875420557\n",
      "Epoch 155/300\n",
      "Average training loss: 0.019564303886559276\n",
      "Average test loss: 0.004066861016261909\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019560310145219167\n",
      "Average test loss: 0.004049715027833978\n",
      "Epoch 157/300\n",
      "Average training loss: 0.019556111805968814\n",
      "Average test loss: 0.00414674449712038\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019554740228586727\n",
      "Average test loss: 0.004062129418883059\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019542414918541907\n",
      "Average test loss: 0.004067865043878556\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019530974096722074\n",
      "Average test loss: 0.004048896031247245\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019530654831065072\n",
      "Average test loss: 0.004113039174013668\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019536378141906525\n",
      "Average test loss: 0.0040515761007037425\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01952745229171382\n",
      "Average test loss: 0.004087104769630564\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019515607094599142\n",
      "Average test loss: 0.0040519544486370355\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019514170123471154\n",
      "Average test loss: 0.0042185305924051335\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01950201041665342\n",
      "Average test loss: 0.004054320503026247\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019501790437433456\n",
      "Average test loss: 0.004072484298712677\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019491708043548796\n",
      "Average test loss: 0.004050332106028994\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019491922184824942\n",
      "Average test loss: 0.0040570904949886934\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01948357873161634\n",
      "Average test loss: 0.004063564180913899\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019475910734799172\n",
      "Average test loss: 0.004104633394628764\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019476306718256738\n",
      "Average test loss: 0.004068186302565866\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01947482109732098\n",
      "Average test loss: 0.004067274906569057\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019467942415012252\n",
      "Average test loss: 0.004065061533616649\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019454691759414142\n",
      "Average test loss: 0.004046833819813199\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019457801463703316\n",
      "Average test loss: 0.004069398102247053\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0194477638469802\n",
      "Average test loss: 0.0040791976037952635\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0194484527160724\n",
      "Average test loss: 0.004065798003226519\n",
      "Epoch 179/300\n",
      "Average training loss: 0.019435665971703\n",
      "Average test loss: 0.0040628131102356645\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01943736764126354\n",
      "Average test loss: 0.004077094124216173\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019426867066986032\n",
      "Average test loss: 0.00406220491665105\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0194268895768457\n",
      "Average test loss: 0.004078594184170167\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019417336151003838\n",
      "Average test loss: 0.004100657514192992\n",
      "Epoch 184/300\n",
      "Average training loss: 0.019415672849449846\n",
      "Average test loss: 0.004149256505486038\n",
      "Epoch 185/300\n",
      "Average training loss: 0.019400212061074045\n",
      "Average test loss: 0.004090769533481863\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019404298643271128\n",
      "Average test loss: 0.004089292205249269\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019403871716724502\n",
      "Average test loss: 0.004055669856982099\n",
      "Epoch 188/300\n",
      "Average training loss: 0.019394360648261177\n",
      "Average test loss: 0.00408611756687363\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01939344732628928\n",
      "Average test loss: 0.004084574122395781\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019388411712315347\n",
      "Average test loss: 0.004079951750321521\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019375323386655912\n",
      "Average test loss: 0.0040818899716768\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0193732281393475\n",
      "Average test loss: 0.004104870757708947\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019367543588081997\n",
      "Average test loss: 0.004103697175367011\n",
      "Epoch 194/300\n",
      "Average training loss: 0.019360263859232268\n",
      "Average test loss: 0.0041055862189580995\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019350067290994855\n",
      "Average test loss: 0.004072872795164585\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019351959897412195\n",
      "Average test loss: 0.004092546980828047\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01935260222024388\n",
      "Average test loss: 0.004078252200451162\n",
      "Epoch 198/300\n",
      "Average training loss: 0.019338578085104623\n",
      "Average test loss: 0.0040801299075699515\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01933404533068339\n",
      "Average test loss: 0.004074753041482634\n",
      "Epoch 200/300\n",
      "Average training loss: 0.019336993731558325\n",
      "Average test loss: 0.0040997246919820705\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019327777011526954\n",
      "Average test loss: 0.004090899446979165\n",
      "Epoch 202/300\n",
      "Average training loss: 0.019330358018477756\n",
      "Average test loss: 0.004101445610324541\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019315555413564046\n",
      "Average test loss: 0.004085378759023216\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01931034520930714\n",
      "Average test loss: 0.004109623049282365\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019309061727590032\n",
      "Average test loss: 0.004110723780675066\n",
      "Epoch 206/300\n",
      "Average training loss: 0.019308371439576148\n",
      "Average test loss: 0.004084325829727782\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019301359962258072\n",
      "Average test loss: 0.004105570012703538\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019300321910116408\n",
      "Average test loss: 0.0041167209868629775\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019289526981612046\n",
      "Average test loss: 0.004162394139501783\n",
      "Epoch 210/300\n",
      "Average training loss: 0.019284416216943\n",
      "Average test loss: 0.004094170472274224\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01928181270758311\n",
      "Average test loss: 0.004115240332980951\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01927985165682104\n",
      "Average test loss: 0.004078585296455357\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01927269999186198\n",
      "Average test loss: 0.004127415720787313\n",
      "Epoch 214/300\n",
      "Average training loss: 0.019282685357663365\n",
      "Average test loss: 0.004100234667460124\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019261853663457763\n",
      "Average test loss: 0.004119764903767241\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01926160614606407\n",
      "Average test loss: 0.004109928593246473\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01924955008758439\n",
      "Average test loss: 0.004205648712192972\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019249758140908346\n",
      "Average test loss: 0.0041393704215685525\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01923868525111013\n",
      "Average test loss: 0.0041236289168397585\n",
      "Epoch 220/300\n",
      "Average training loss: 0.019233930149012143\n",
      "Average test loss: 0.004131106426318486\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019237648003631168\n",
      "Average test loss: 0.004115514665842057\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019229181526435747\n",
      "Average test loss: 0.004131336591309972\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019223782485557925\n",
      "Average test loss: 0.004138800805020664\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019219781458377837\n",
      "Average test loss: 0.004115972591150138\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019220986252029736\n",
      "Average test loss: 0.004118994082427687\n",
      "Epoch 226/300\n",
      "Average training loss: 0.019211615175008773\n",
      "Average test loss: 0.004139007731444306\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019199441126651236\n",
      "Average test loss: 0.00415192237496376\n",
      "Epoch 228/300\n",
      "Average training loss: 0.019205455301536455\n",
      "Average test loss: 0.004097967852734857\n",
      "Epoch 229/300\n",
      "Average training loss: 0.019201597097847196\n",
      "Average test loss: 0.004140293681373199\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019193351114789644\n",
      "Average test loss: 0.004090206837074624\n",
      "Epoch 231/300\n",
      "Average training loss: 0.019189943609966172\n",
      "Average test loss: 0.004106381291316615\n",
      "Epoch 232/300\n",
      "Average training loss: 0.019184409843550788\n",
      "Average test loss: 0.004127981177220742\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019179785268174276\n",
      "Average test loss: 0.004120456974539492\n",
      "Epoch 234/300\n",
      "Average training loss: 0.019173782931433785\n",
      "Average test loss: 0.004116977616523703\n",
      "Epoch 235/300\n",
      "Average training loss: 0.019174839759038553\n",
      "Average test loss: 0.00408829765021801\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019161028906289075\n",
      "Average test loss: 0.0042119794641104005\n",
      "Epoch 237/300\n",
      "Average training loss: 0.019157467388444475\n",
      "Average test loss: 0.004108122854597039\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019164728813701206\n",
      "Average test loss: 0.004106340185221698\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01915248203608725\n",
      "Average test loss: 0.004150899630453852\n",
      "Epoch 240/300\n",
      "Average training loss: 0.019150468417339854\n",
      "Average test loss: 0.004117725713592437\n",
      "Epoch 241/300\n",
      "Average training loss: 0.019155017861061625\n",
      "Average test loss: 0.004116725728743606\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019139020212822492\n",
      "Average test loss: 0.004138279926445749\n",
      "Epoch 243/300\n",
      "Average training loss: 0.019143904791937935\n",
      "Average test loss: 0.004085276751054658\n",
      "Epoch 244/300\n",
      "Average training loss: 0.019134839458598032\n",
      "Average test loss: 0.004093124895253116\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01913444075898992\n",
      "Average test loss: 0.004154091153293848\n",
      "Epoch 246/300\n",
      "Average training loss: 0.019126379153794712\n",
      "Average test loss: 0.004123298384663131\n",
      "Epoch 247/300\n",
      "Average training loss: 0.019122119419276714\n",
      "Average test loss: 0.004110115996251503\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01912124484280745\n",
      "Average test loss: 0.004120777258028586\n",
      "Epoch 249/300\n",
      "Average training loss: 0.019115195305811034\n",
      "Average test loss: 0.0041556218150589205\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01911262467586332\n",
      "Average test loss: 0.004119995834098922\n",
      "Epoch 251/300\n",
      "Average training loss: 0.019107568588521747\n",
      "Average test loss: 0.0041256750815858445\n",
      "Epoch 252/300\n",
      "Average training loss: 0.019104164603683683\n",
      "Average test loss: 0.004076831008204156\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01909827860693137\n",
      "Average test loss: 0.004145833431018723\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019098393749859596\n",
      "Average test loss: 0.004091187560723888\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019086645851532618\n",
      "Average test loss: 0.004119696567248967\n",
      "Epoch 256/300\n",
      "Average training loss: 0.019086471632122994\n",
      "Average test loss: 0.004128808365513881\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019080892571144634\n",
      "Average test loss: 0.004138509481317467\n",
      "Epoch 258/300\n",
      "Average training loss: 0.019077403278814423\n",
      "Average test loss: 0.004145552266803053\n",
      "Epoch 259/300\n",
      "Average training loss: 0.019075040926535926\n",
      "Average test loss: 0.00415831977998217\n",
      "Epoch 260/300\n",
      "Average training loss: 0.019065417321191894\n",
      "Average test loss: 0.0041356986738327476\n",
      "Epoch 261/300\n",
      "Average training loss: 0.019062095681826274\n",
      "Average test loss: 0.004184317589633994\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01906194811893834\n",
      "Average test loss: 0.0041594293274813225\n",
      "Epoch 263/300\n",
      "Average training loss: 0.019056272086169983\n",
      "Average test loss: 0.0041917060990300445\n",
      "Epoch 264/300\n",
      "Average training loss: 0.019058685158689817\n",
      "Average test loss: 0.0041701577173339\n",
      "Epoch 265/300\n",
      "Average training loss: 0.019043432071805002\n",
      "Average test loss: 0.004131775754607386\n",
      "Epoch 266/300\n",
      "Average training loss: 0.019040932416915894\n",
      "Average test loss: 0.004210871970074044\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01904231495161851\n",
      "Average test loss: 0.004145897932350636\n",
      "Epoch 268/300\n",
      "Average training loss: 0.019039658826258448\n",
      "Average test loss: 0.004198140909274419\n",
      "Epoch 269/300\n",
      "Average training loss: 0.019026828333735465\n",
      "Average test loss: 0.004132943877950311\n",
      "Epoch 270/300\n",
      "Average training loss: 0.019042707938286992\n",
      "Average test loss: 0.004122548379210962\n",
      "Epoch 271/300\n",
      "Average training loss: 0.019022586474816004\n",
      "Average test loss: 0.004172248404473066\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01901412087678909\n",
      "Average test loss: 0.004159204937310683\n",
      "Epoch 273/300\n",
      "Average training loss: 0.019021042146616512\n",
      "Average test loss: 0.004173418587280644\n",
      "Epoch 274/300\n",
      "Average training loss: 0.019013310693204402\n",
      "Average test loss: 0.00414759996947315\n",
      "Epoch 275/300\n",
      "Average training loss: 0.019011725670761533\n",
      "Average test loss: 0.004218405806356006\n",
      "Epoch 276/300\n",
      "Average training loss: 0.019004975414938396\n",
      "Average test loss: 0.0041491120826039046\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01900111345284515\n",
      "Average test loss: 0.004193762287911441\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01900868792583545\n",
      "Average test loss: 0.004283830382964677\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018996358070108625\n",
      "Average test loss: 0.00415447402621309\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018989631937609778\n",
      "Average test loss: 0.0041539213413165675\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018995619506471686\n",
      "Average test loss: 0.004318586156393091\n",
      "Epoch 282/300\n",
      "Average training loss: 0.018995820477604865\n",
      "Average test loss: 0.004138619582479199\n",
      "Epoch 283/300\n",
      "Average training loss: 0.018982221537166173\n",
      "Average test loss: 0.004183045129395193\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018971883349948458\n",
      "Average test loss: 0.004200142393095626\n",
      "Epoch 285/300\n",
      "Average training loss: 0.018979964913593397\n",
      "Average test loss: 0.0041684380662110115\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018969884456031854\n",
      "Average test loss: 0.004179528570009602\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018962873659200137\n",
      "Average test loss: 0.004163296538094679\n",
      "Epoch 288/300\n",
      "Average training loss: 0.018962435021996498\n",
      "Average test loss: 0.004207133438438177\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0189678085040715\n",
      "Average test loss: 0.004150682968397935\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01895576165864865\n",
      "Average test loss: 0.004163898549560043\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0189597012143996\n",
      "Average test loss: 0.004197370686050918\n",
      "Epoch 292/300\n",
      "Average training loss: 0.018960687526398234\n",
      "Average test loss: 0.00419169965883096\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01894492892920971\n",
      "Average test loss: 0.004179404582828283\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018937682072321573\n",
      "Average test loss: 0.004159228086471557\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01894162831372685\n",
      "Average test loss: 0.004157279433475601\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01893993946082062\n",
      "Average test loss: 0.004133701124332017\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018934319362044334\n",
      "Average test loss: 0.004321339037683275\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018935588219099575\n",
      "Average test loss: 0.0042111344995598\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018920166259010635\n",
      "Average test loss: 0.004317945089191198\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018922798133558696\n",
      "Average test loss: 0.004168891621960534\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05939376336832841\n",
      "Average test loss: 0.004602816665752066\n",
      "Epoch 2/300\n",
      "Average training loss: 0.021546666350629594\n",
      "Average test loss: 0.004211083055784305\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020291540746887524\n",
      "Average test loss: 0.003978073519965013\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019675323069095613\n",
      "Average test loss: 0.0039009088637928166\n",
      "Epoch 5/300\n",
      "Average training loss: 0.019262884790698688\n",
      "Average test loss: 0.0038169063720852135\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018958614433805147\n",
      "Average test loss: 0.0037623972271879514\n",
      "Epoch 7/300\n",
      "Average training loss: 0.018712855714890692\n",
      "Average test loss: 0.00374704530586799\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01851502677301566\n",
      "Average test loss: 0.00368045290828579\n",
      "Epoch 9/300\n",
      "Average training loss: 0.018335433405306602\n",
      "Average test loss: 0.003639148784801364\n",
      "Epoch 10/300\n",
      "Average training loss: 0.018190350676576296\n",
      "Average test loss: 0.0036466182325449255\n",
      "Epoch 11/300\n",
      "Average training loss: 0.018059117512570486\n",
      "Average test loss: 0.003622716545437773\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017946897002557912\n",
      "Average test loss: 0.0035828142271687588\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0178312826719549\n",
      "Average test loss: 0.0035358141358527873\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01773959946218464\n",
      "Average test loss: 0.003532227925128407\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01764181150496006\n",
      "Average test loss: 0.0035275262041638295\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01755198862320847\n",
      "Average test loss: 0.003479897883410255\n",
      "Epoch 17/300\n",
      "Average training loss: 0.017459357619285583\n",
      "Average test loss: 0.0034377762654589283\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017372489897741213\n",
      "Average test loss: 0.0034286520725323095\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01730258227719201\n",
      "Average test loss: 0.0034207910450382364\n",
      "Epoch 20/300\n",
      "Average training loss: 0.017213361779020893\n",
      "Average test loss: 0.003410151512672504\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017149804025888442\n",
      "Average test loss: 0.003388224284268088\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01708473815686173\n",
      "Average test loss: 0.0033761839005682203\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017013604374395478\n",
      "Average test loss: 0.003398713807264964\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01695480577647686\n",
      "Average test loss: 0.0033519638085530864\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01689595220486323\n",
      "Average test loss: 0.0033399017747077677\n",
      "Epoch 26/300\n",
      "Average training loss: 0.016846063103940753\n",
      "Average test loss: 0.0033203817999197375\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01680383581088649\n",
      "Average test loss: 0.0032876505388153925\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016762289797266326\n",
      "Average test loss: 0.0033054809988372854\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016710809653004012\n",
      "Average test loss: 0.0032849970534443856\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016681389580998157\n",
      "Average test loss: 0.0032931090924474926\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016632778767082426\n",
      "Average test loss: 0.003277936389048894\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01660696523553795\n",
      "Average test loss: 0.0032937511197394793\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0165731844479839\n",
      "Average test loss: 0.003253019386695491\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01653132678816716\n",
      "Average test loss: 0.003252893716096878\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01651009440339274\n",
      "Average test loss: 0.0032407087513970003\n",
      "Epoch 36/300\n",
      "Average training loss: 0.016479187197983265\n",
      "Average test loss: 0.003233476546075609\n",
      "Epoch 37/300\n",
      "Average training loss: 0.016452551203469435\n",
      "Average test loss: 0.003235002235405975\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01642223391433557\n",
      "Average test loss: 0.003226761621216105\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01639712454709742\n",
      "Average test loss: 0.0032213013971017466\n",
      "Epoch 40/300\n",
      "Average training loss: 0.016379056725237106\n",
      "Average test loss: 0.0032202095076855684\n",
      "Epoch 41/300\n",
      "Average training loss: 0.016361835518644917\n",
      "Average test loss: 0.003213294656119413\n",
      "Epoch 42/300\n",
      "Average training loss: 0.016326169956889416\n",
      "Average test loss: 0.0032053872982247007\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0163101186628143\n",
      "Average test loss: 0.0032103949460304446\n",
      "Epoch 44/300\n",
      "Average training loss: 0.016288769958747756\n",
      "Average test loss: 0.0032149123582575056\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016260852330260808\n",
      "Average test loss: 0.0032132637076493767\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016253731701109143\n",
      "Average test loss: 0.003189940075493521\n",
      "Epoch 47/300\n",
      "Average training loss: 0.016229893777105542\n",
      "Average test loss: 0.003197996323307355\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016205770334435834\n",
      "Average test loss: 0.003185937233062254\n",
      "Epoch 49/300\n",
      "Average training loss: 0.016182881289058263\n",
      "Average test loss: 0.00320451016475757\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01617269943488969\n",
      "Average test loss: 0.00319719588653081\n",
      "Epoch 51/300\n",
      "Average training loss: 0.016152971079779997\n",
      "Average test loss: 0.003228387232042021\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01613864850997925\n",
      "Average test loss: 0.0031913978457450867\n",
      "Epoch 53/300\n",
      "Average training loss: 0.016115948317779436\n",
      "Average test loss: 0.0031750858076330687\n",
      "Epoch 54/300\n",
      "Average training loss: 0.016104855401648417\n",
      "Average test loss: 0.003205057451501489\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01609530983865261\n",
      "Average test loss: 0.003186508029906286\n",
      "Epoch 56/300\n",
      "Average training loss: 0.016073002502322197\n",
      "Average test loss: 0.0031742522824141713\n",
      "Epoch 57/300\n",
      "Average training loss: 0.016054218464427523\n",
      "Average test loss: 0.003191428617263834\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01603646355619033\n",
      "Average test loss: 0.0031984098371532228\n",
      "Epoch 59/300\n",
      "Average training loss: 0.016027575499481625\n",
      "Average training loss: 0.015975786153641012\n",
      "Average test loss: 0.003166842226766878\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01595741642349296\n",
      "Average test loss: 0.003156406575710409\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015940615561273363\n",
      "Average test loss: 0.003148066406034761\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015933658759627076\n",
      "Average test loss: 0.006218187555256817\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01594199845608738\n",
      "Average test loss: 0.0032137897302293114\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01590806439270576\n",
      "Average test loss: 0.003187695574636261\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015889674078259204\n",
      "Average test loss: 0.0031622152618236013\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015881439596414568\n",
      "Average test loss: 0.0031560406637274555\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015857730018595854\n",
      "Average test loss: 0.0031557693843626312\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015860754181113507\n",
      "Average test loss: 0.0031543643693957065\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01583943312449588\n",
      "Average test loss: 0.003147916884471973\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015830408879452283\n",
      "Average test loss: 0.0031464086771011354\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015818300638761787\n",
      "Average test loss: 0.0031603483628067706\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01569149409234524\n",
      "Average test loss: 0.003142161691768302\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015686956059601572\n",
      "Average test loss: 0.003282369943956534\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015673524423605867\n",
      "Average test loss: 0.0031458456023699707\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015660164794988104\n",
      "Average test loss: 0.003147589033469558\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015658324806226625\n",
      "Average test loss: 0.0031469051254292327\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015632527878301012\n",
      "Average test loss: 0.0031551019445889526\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015633193345533477\n",
      "Average test loss: 0.0031645008439405097\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015620979490379492\n",
      "Average test loss: 0.0031420935781465635\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015615780159003204\n",
      "Average test loss: 0.0031503924566010635\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015603129591378901\n",
      "Average test loss: 0.0031521136872470377\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01559812572515673\n",
      "Average test loss: 0.0031678934051758713\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015579786779152022\n",
      "Average test loss: 0.0031576137849026256\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01557272812972466\n",
      "Average test loss: 0.003210194554593828\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015571711307598485\n",
      "Average test loss: 0.003155862625895275\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015563591171056032\n",
      "Average test loss: 0.003173106714669201\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015546731251809331\n",
      "Average test loss: 0.003154828782710764\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015546509811447727\n",
      "Average test loss: 0.0031675008601612514\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015540016004608737\n",
      "Average test loss: 0.003271336197439167\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01552696737067567\n",
      "Average test loss: 0.003152876698308521\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015523787493507068\n",
      "Average test loss: 0.003172145870824655\n",
      "Epoch 107/300\n",
      "Average training loss: 0.015511884541975128\n",
      "Average test loss: 0.003166663284310036\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015496337931189272\n",
      "Average test loss: 0.0031783645281361207\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015461967585815323\n",
      "Average test loss: 0.003169641150782506\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015459376439452171\n",
      "Average test loss: 0.0031462316657933924\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015450576446122593\n",
      "Average test loss: 0.003177309517852134\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01545931892345349\n",
      "Average test loss: 0.0031797604635357855\n",
      "Epoch 116/300\n",
      "Average training loss: 0.015435167415274514\n",
      "Average test loss: 0.0031606607304679023\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015426087309088971\n",
      "Average test loss: 0.0031555744651705028\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01541691097120444\n",
      "Average test loss: 0.0031594916958775786\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015411058637830946\n",
      "Average test loss: 0.003151082739647892\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015405058494044674\n",
      "Average test loss: 0.0032728460170328615\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01539081272482872\n",
      "Average test loss: 0.003151730716849367\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015392235192987654\n",
      "Average test loss: 0.0031506149005144837\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01530487485892243\n",
      "Average test loss: 0.0031753796483907433\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015302460417151451\n",
      "Average test loss: 0.0031570516503933404\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015297657707499133\n",
      "Average test loss: 0.003263847897036208\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01528418062792884\n",
      "Average test loss: 0.0031722904623796544\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015282342442207867\n",
      "Average test loss: 0.003174428763695889\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015270229626033041\n",
      "Average test loss: 0.0032143288364426957\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01526398187627395\n",
      "Average test loss: 0.0032438195447127022\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015258816417720584\n",
      "Average test loss: 0.003188873377525144\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015253850945168072\n",
      "Average test loss: 0.0032550026890304355\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015243564475741652\n",
      "Average test loss: 0.0031476337537169456\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015233624041080475\n",
      "Average test loss: 0.0031844261727399297\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015237747612098854\n",
      "Average test loss: 0.0031771289054304363\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015227585309909449\n",
      "Average test loss: 0.0031604177463385795\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01522698184599479\n",
      "Average test loss: 0.003151656373300486\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015213179087473286\n",
      "Average test loss: 0.003191585482408603\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015207162761025959\n",
      "Average test loss: 0.003363301971720325\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015206490774949392\n",
      "Average test loss: 0.0031828810891343486\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015193491234547562\n",
      "Average test loss: 0.0031673540625100334\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01518808934920364\n",
      "Average test loss: 0.0031928417161107064\n",
      "Epoch 152/300\n",
      "Average training loss: 0.015182475550307169\n",
      "Average test loss: 0.003167190229934123\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015177554245624277\n",
      "Average test loss: 0.0031703371664302217\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015171544151173697\n",
      "Average test loss: 0.0034377469797101288\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015171885335610973\n",
      "Average test loss: 0.0031391232110973863\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015155684457884894\n",
      "Average test loss: 0.0031623845224579175\n",
      "Epoch 157/300\n",
      "Average training loss: 0.015155531838536263\n",
      "Average test loss: 0.003217495248342554\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015152151010930539\n",
      "Average test loss: 0.0031670360503097375\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015144281988342603\n",
      "Average test loss: 0.003214261262988051\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015138898294832972\n",
      "Average test loss: 0.003192816441257795\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015130232787794537\n",
      "Average test loss: 0.0032699636680384478\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015124971327682335\n",
      "Average test loss: 0.003205823412992888\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015132148454586665\n",
      "Average test loss: 0.0031628526300191877\n",
      "Epoch 164/300\n",
      "Average training loss: 0.015114407353931002\n",
      "Average test loss: 0.0031732095989088216\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015110390979382727\n",
      "Average test loss: 0.0031705294631214605\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015108162844346629\n",
      "Average test loss: 0.0032239856709622675\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015092092875805166\n",
      "Average test loss: 0.0031742278343687455\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015103188711735938\n",
      "Average test loss: 0.0032093277701901067\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01508720628834433\n",
      "Average test loss: 0.003172927557180325\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0150783250456055\n",
      "Average test loss: 0.0031666073362446494\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015084564603037304\n",
      "Average test loss: 0.0031853093836042615\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015070284862485198\n",
      "Average test loss: 0.0034947690491875013\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015068088245060708\n",
      "Average test loss: 0.003219323322176933\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015060523932178816\n",
      "Average test loss: 0.003215169954631064\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015048407652311855\n",
      "Average test loss: 0.0032271133924110067\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015050131265487935\n",
      "Average test loss: 0.0032343617814282574\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015044909045100212\n",
      "Average test loss: 0.0032274388873742687\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015037192386885483\n",
      "Average test loss: 0.0032036934074842266\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015038135218951438\n",
      "Average test loss: 0.0032115545632938544\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015030842659374079\n",
      "Average test loss: 0.0031896604516853887\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015024320942660173\n",
      "Average test loss: 0.003194807986004485\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015017171213196383\n",
      "Average test loss: 0.003198918701873885\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015015295262965892\n",
      "Average test loss: 0.0031881852940552763\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015007225165764491\n",
      "Average test loss: 0.0031874165460467336\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01500830741888947\n",
      "Average test loss: 0.003200865529477596\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014995200401379003\n",
      "Average test loss: 0.0031947305833713874\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01499752113968134\n",
      "Average test loss: 0.003190623528427548\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014988044620388084\n",
      "Average test loss: 0.003185070503503084\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014983296816547712\n",
      "Average test loss: 0.003254303614091542\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014984723425573773\n",
      "Average test loss: 0.003247632960478465\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014982442921234502\n",
      "Average test loss: 0.0032385275579161115\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014964939433667395\n",
      "Average test loss: 0.0032374647880593936\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014958025968737073\n",
      "Average test loss: 0.0032156936286224258\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014960511539545324\n",
      "Average test loss: 0.0032393751074042587\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014955366721583737\n",
      "Average test loss: 0.0032530635291089616\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014960345904860232\n",
      "Average test loss: 0.003262438745755288\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01494990594436725\n",
      "Average test loss: 0.003199112588332759\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01494841500123342\n",
      "Average test loss: 0.003202037134932147\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014942894203795328\n",
      "Average test loss: 0.0032075430897788868\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014934094530012872\n",
      "Average test loss: 0.0032018560705085594\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014937202366689842\n",
      "Average test loss: 0.0032327071867055365\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014921717065076034\n",
      "Average test loss: 0.0032030318758140006\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01491752659695016\n",
      "Average test loss: 0.003199303977398409\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014909618945585356\n",
      "Average test loss: 0.003217216974745194\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014912248378826513\n",
      "Average test loss: 0.003210585062702497\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014919714322520627\n",
      "Average test loss: 0.0032041450996572775\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014906777761048741\n",
      "Average test loss: 0.003196592941880226\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014900149297383097\n",
      "Average test loss: 0.0031876329843782717\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014894348735610644\n",
      "Average test loss: 0.0032032444638510544\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014893798003594081\n",
      "Average test loss: 0.0032165916769040957\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014883077426089181\n",
      "Average test loss: 0.0031869617785430615\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014880150945650207\n",
      "Average test loss: 0.0032203489277097914\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014885663824776808\n",
      "Average test loss: 0.0032804307498865656\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014870094213220808\n",
      "Average test loss: 0.0032521130355695886\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014866964540547794\n",
      "Average test loss: 0.0032298135811256036\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014859591358237797\n",
      "Average test loss: 0.0032091751168999406\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01486081954009003\n",
      "Average test loss: 0.0032168441779083677\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014863164151708285\n",
      "Average test loss: 0.003230680332933035\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014861734776033296\n",
      "Average test loss: 0.0032126253292792373\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014851833223468728\n",
      "Average test loss: 0.003172845868600739\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014844780671099822\n",
      "Average test loss: 0.0032910349521165093\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014808379963040352\n",
      "Average test loss: 0.003324796011464463\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014809698979887697\n",
      "Average test loss: 0.00329069293414553\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014792763018773662\n",
      "Average test loss: 0.003281816146026055\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014797924550871054\n",
      "Average test loss: 0.0033038761297033894\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014796834928294023\n",
      "Average test loss: 0.0031988971353405053\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014784289600948493\n",
      "Average test loss: 0.0032191532287332747\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014776252473394076\n",
      "Average test loss: 0.0032128310315310956\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014778965961602\n",
      "Average test loss: 0.0032296418729755613\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01477423475517167\n",
      "Average test loss: 0.0032082941873619953\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014772798686391777\n",
      "Average test loss: 0.0032286929736534756\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014777761283020179\n",
      "Average test loss: 0.0033151780147519377\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014767846938636567\n",
      "Average test loss: 0.003229810091149476\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014751766724718942\n",
      "Average test loss: 0.0032004841876526672\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014756341189973885\n",
      "Average test loss: 0.0032693940926757123\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014746985078685814\n",
      "Average test loss: 0.003275087314968308\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014751975530551539\n",
      "Average test loss: 0.0033317835353728798\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014748639772335688\n",
      "Average test loss: 0.0032781630728600753\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01473739038573371\n",
      "Average test loss: 0.0032202646583318712\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014744646264447107\n",
      "Average test loss: 0.003228859364779459\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014734513156116008\n",
      "Average test loss: 0.003244725120978223\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014739304420848687\n",
      "Average test loss: 0.003223608657096823\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014732590475844012\n",
      "Average test loss: 0.0033360176409284272\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014734352778229449\n",
      "Average test loss: 0.003421009319110049\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014729927152395248\n",
      "Average test loss: 0.003190500144329336\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014724019505083561\n",
      "Average test loss: 0.0032488443719016183\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014716362707316875\n",
      "Average test loss: 0.003370619433725046\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014729976911511686\n",
      "Average test loss: 0.003248966871657305\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014710034378700786\n",
      "Average test loss: 0.0032143853766222795\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014717424328128496\n",
      "Average test loss: 0.0032078745688001314\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014706886424786514\n",
      "Average test loss: 0.003277990861485402\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014698641435967552\n",
      "Average test loss: 0.003231389396927423\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014695052138633199\n",
      "Average test loss: 0.003249925511578719\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014710263492332565\n",
      "Average test loss: 0.00324864257644448\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014688226787580385\n",
      "Average test loss: 0.0033108115788135265\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014692948621180323\n",
      "Average test loss: 0.003262568041475283\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014692405878669686\n",
      "Average test loss: 0.0033696234273827737\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01469911863654852\n",
      "Average test loss: 0.0033631452723509735\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014694721370107598\n",
      "Average test loss: 0.0032455094100700484\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014681882720854546\n",
      "Average test loss: 0.0032589071641365686\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01468318535718653\n",
      "Average test loss: 0.003260563837985198\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014673118375241757\n",
      "Average test loss: 0.003234180935141113\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014671637756129105\n",
      "Average test loss: 0.0031978067679123744\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014665645247532262\n",
      "Average test loss: 0.0033130188803705903\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014677920994659265\n",
      "Average test loss: 0.0032847726448542542\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014653968250585927\n",
      "Average test loss: 0.0032724531363281938\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014662584414084752\n",
      "Average test loss: 0.003243294059402413\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014655477858251996\n",
      "Average test loss: 0.0032888797552635273\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014660138727062278\n",
      "Average test loss: 0.0032990011647343635\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01464558272394869\n",
      "Average test loss: 0.0032419387466377683\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014646728366613388\n",
      "Average test loss: 0.0032283036878539458\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014642034642398357\n",
      "Average test loss: 0.0033100652967890105\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014642921041283343\n",
      "Average test loss: 0.003238979201970829\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014633964840736653\n",
      "Average test loss: 0.0033104753676388\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01463961552331845\n",
      "Average test loss: 0.0032588554786311254\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014637105157805815\n",
      "Average test loss: 0.003253953313248025\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014639838092029095\n",
      "Average test loss: 0.00320767274664508\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014624062384168307\n",
      "Average test loss: 0.0032574064197639623\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014626269132726723\n",
      "Average test loss: 0.003227679884268178\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01461080267528693\n",
      "Average test loss: 0.00322484102493359\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014627986558609538\n",
      "Average test loss: 0.003212152543581194\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014632231281863318\n",
      "Average test loss: 0.00326209250671996\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014617544300854206\n",
      "Average test loss: 0.0032436657380312682\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014608891963130899\n",
      "Average test loss: 0.0032618475190053383\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014613082755770949\n",
      "Average test loss: 0.0032998948027897212\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014616933120621576\n",
      "Average test loss: 0.0034010979142040016\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014596048773990736\n",
      "Average test loss: 0.003242642314069801\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014605287282003297\n",
      "Average test loss: 0.003238125729064147\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014612247469524543\n",
      "Average test loss: 0.0032438708771434094\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05644012023177412\n",
      "Average test loss: 0.004118119492299027\n",
      "Epoch 2/300\n",
      "Average training loss: 0.019071824941370222\n",
      "Average test loss: 0.0037279570458663832\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017678549274802207\n",
      "Average test loss: 0.0034992304858234196\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01697008021755351\n",
      "Average test loss: 0.0033358585931774643\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01645690342866712\n",
      "Average test loss: 0.0032804623504893646\n",
      "Epoch 6/300\n",
      "Average training loss: 0.016061642539997895\n",
      "Average test loss: 0.003149824949602286\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01574736364599731\n",
      "Average test loss: 0.003083948903199699\n",
      "Epoch 8/300\n",
      "Average training loss: 0.015494844226373566\n",
      "Average test loss: 0.00301300528479947\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015269951259096463\n",
      "Average test loss: 0.002970355993550685\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01510118083904187\n",
      "Average test loss: 0.0029368319569362533\n",
      "Epoch 11/300\n",
      "Average training loss: 0.014933088997171984\n",
      "Average test loss: 0.002896410352653927\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014780867823296124\n",
      "Average test loss: 0.0028832367745538555\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014642939791083337\n",
      "Average test loss: 0.0028253669378658134\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014522351578705841\n",
      "Average test loss: 0.002800636353385117\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014395166215797265\n",
      "Average test loss: 0.0027692732695076203\n",
      "Epoch 16/300\n",
      "Average training loss: 0.014280924373202854\n",
      "Average test loss: 0.0027643606449580856\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01418852014011807\n",
      "Average test loss: 0.002727633324348264\n",
      "Epoch 18/300\n",
      "Average training loss: 0.014099971672727002\n",
      "Average test loss: 0.002723625391928686\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01399247176448504\n",
      "Average test loss: 0.002680286544892523\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013916354650424586\n",
      "Average test loss: 0.0026499188956287173\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0138397983668579\n",
      "Average test loss: 0.0026616014319782457\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013759817245933744\n",
      "Average test loss: 0.002615275086835027\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01369012995229827\n",
      "Average test loss: 0.002599473693097631\n",
      "Epoch 24/300\n",
      "Average training loss: 0.013642952130072647\n",
      "Average test loss: 0.0025937575101852416\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01358459598902199\n",
      "Average test loss: 0.002591434799341692\n",
      "Epoch 26/300\n",
      "Average training loss: 0.013526517989734808\n",
      "Average test loss: 0.0025750875543389057\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01347842403418488\n",
      "Average test loss: 0.0025781003988037507\n",
      "Epoch 28/300\n",
      "Average training loss: 0.013441846427818139\n",
      "Average test loss: 0.002562261884411176\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013397029809653759\n",
      "Average test loss: 0.0025372981587424874\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013354734997782442\n",
      "Average test loss: 0.002548097019394239\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013303277485900456\n",
      "Average test loss: 0.0025315383503006564\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013274969013200865\n",
      "Average test loss: 0.0025211707196301884\n",
      "Epoch 33/300\n",
      "Average training loss: 0.013243893511593342\n",
      "Average test loss: 0.0025143483862694765\n",
      "Epoch 34/300\n",
      "Average training loss: 0.013210678812530305\n",
      "Average test loss: 0.0025195924256824786\n",
      "Epoch 35/300\n",
      "Average training loss: 0.013185274849335352\n",
      "Average test loss: 0.0025292146004115544\n",
      "Epoch 36/300\n",
      "Average training loss: 0.013148407105770376\n",
      "Average test loss: 0.0024955261902262767\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013124812517729071\n",
      "Average test loss: 0.0025105570135638116\n",
      "Epoch 38/300\n",
      "Average training loss: 0.013093313366174699\n",
      "Average test loss: 0.0024843194575773346\n",
      "Epoch 39/300\n",
      "Average training loss: 0.013069444369938638\n",
      "Average test loss: 0.0024975009864817064\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01304893491251601\n",
      "Average test loss: 0.0024802530128508805\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013020094198485216\n",
      "Average test loss: 0.002481325350701809\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012993457804123561\n",
      "Average test loss: 0.0024674294065270158\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012983664315607813\n",
      "Average test loss: 0.002459291912615299\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01296341709461477\n",
      "Average test loss: 0.0024694996012581717\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01293393420179685\n",
      "Average test loss: 0.002475559532021483\n",
      "Epoch 46/300\n",
      "Average training loss: 0.012920981001522806\n",
      "Average test loss: 0.002463567534254657\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012898534999125534\n",
      "Average test loss: 0.002479491938733392\n",
      "Epoch 48/300\n",
      "Average training loss: 0.012874745419455899\n",
      "Average test loss: 0.002456548457965255\n",
      "Epoch 49/300\n",
      "Average training loss: 0.012866382614605957\n",
      "Average test loss: 0.0024553780804077783\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01284476006279389\n",
      "Average test loss: 0.0024473523563808865\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012826353332234754\n",
      "Average test loss: 0.0024448571412099733\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012811606153017945\n",
      "Average test loss: 0.0024517706458767256\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012794372123148707\n",
      "Average test loss: 0.00245215191733506\n",
      "Epoch 54/300\n",
      "Average training loss: 0.012782215721905231\n",
      "Average test loss: 0.002445394002625512\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012760356103380522\n",
      "Average test loss: 0.002432444595835275\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012737837942938009\n",
      "Average test loss: 0.0024434936951018044\n",
      "Epoch 57/300\n",
      "Average training loss: 0.012729336565567387\n",
      "Average test loss: 0.0024355856518571578\n",
      "Epoch 58/300\n",
      "Average training loss: 0.012715047031641006\n",
      "Average test loss: 0.0024296695799049402\n",
      "Epoch 59/300\n",
      "Average training loss: 0.012698300576872296\n",
      "Average test loss: 0.002437624751383232\n",
      "Epoch 60/300\n",
      "Average training loss: 0.012699024185538293\n",
      "Average test loss: 0.002432880972615547\n",
      "Epoch 61/300\n",
      "Average training loss: 0.012678302130765385\n",
      "Average test loss: 0.002436598720649878\n",
      "Epoch 62/300\n",
      "Average training loss: 0.012652514031363858\n",
      "Average test loss: 0.0024559913254860377\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012650411566098531\n",
      "Average test loss: 0.0024560929087715016\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012632966845399803\n",
      "Average test loss: 0.002426489865614308\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012621789433889918\n",
      "Average test loss: 0.0024262755231724844\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012611258433924782\n",
      "Average test loss: 0.002441452621689273\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01260406250341071\n",
      "Average test loss: 0.0024174797100325425\n",
      "Epoch 68/300\n",
      "Average training loss: 0.012591948561370372\n",
      "Average test loss: 0.0024242094780007996\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01257593348539538\n",
      "Average test loss: 0.002405935382263528\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012568035054537985\n",
      "Average test loss: 0.0024090527738961907\n",
      "Epoch 71/300\n",
      "Average training loss: 0.012559168806506527\n",
      "Average test loss: 0.0024770059935334656\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01253436516970396\n",
      "Average test loss: 0.002416568353979124\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012530628864963849\n",
      "Average test loss: 0.002412748740158147\n",
      "Epoch 74/300\n",
      "Average training loss: 0.012518267774747478\n",
      "Average test loss: 0.002402686545211408\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012501284836067094\n",
      "Average test loss: 0.002426703455340531\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01250035068144401\n",
      "Average test loss: 0.002406333012506366\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012494736629227798\n",
      "Average test loss: 0.002400920973883735\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012477396902938684\n",
      "Average test loss: 0.002399668599582381\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012467822842299938\n",
      "Average test loss: 0.0024002866035120357\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012456698928442266\n",
      "Average test loss: 0.0024104277338418694\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012449480247166422\n",
      "Average test loss: 0.0024230637095040745\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012437005614240965\n",
      "Average test loss: 0.0024076073211100365\n",
      "Epoch 83/300\n",
      "Average training loss: 0.012428674991759989\n",
      "Average test loss: 0.0025068307049158547\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012421502222617467\n",
      "Average test loss: 0.0024150574730916158\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012420861887435119\n",
      "Average test loss: 0.0024215689566400318\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012396595188313061\n",
      "Average test loss: 0.002421597571215696\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012396063109238943\n",
      "Average test loss: 0.002414053168354763\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012371208258801036\n",
      "Average test loss: 0.0023967032935470344\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012378777819375197\n",
      "Average test loss: 0.0024015376650624804\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012355224640005165\n",
      "Average test loss: 0.0023945808278189764\n",
      "Epoch 91/300\n",
      "Average training loss: 0.012354209898246659\n",
      "Average test loss: 0.0023920087485263743\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012349833420581288\n",
      "Average test loss: 0.0024137756298813554\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012338351228998767\n",
      "Average test loss: 0.002404396493194832\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01232951806485653\n",
      "Average test loss: 0.0024205450638093884\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012330458123650816\n",
      "Average test loss: 0.0023869678042829036\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012314761121239927\n",
      "Average test loss: 0.0024247467757927046\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012305069601370228\n",
      "Average test loss: 0.0023944250852283504\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012295552713175615\n",
      "Average test loss: 0.0024098468215929137\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012290616687801148\n",
      "Average test loss: 0.0024414361069599786\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012283543050289154\n",
      "Average test loss: 0.002398054220403234\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012275383950107627\n",
      "Average test loss: 0.0023871643216245705\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012267959260278278\n",
      "Average test loss: 0.002418072083344062\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012261621882518132\n",
      "Average test loss: 0.00240867974029647\n",
      "Epoch 104/300\n",
      "Average training loss: 0.012255196568866571\n",
      "Average test loss: 0.0024118703376087877\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012246270940535599\n",
      "Average test loss: 0.0023942430574032995\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012239124040636751\n",
      "Average test loss: 0.002405973037911786\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012232862734132342\n",
      "Average test loss: 0.0023987311257256403\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012225881677534845\n",
      "Average test loss: 0.002389335492418872\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012210378993716504\n",
      "Average test loss: 0.002402371161306898\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012209186486485932\n",
      "Average test loss: 0.0024160535823967723\n",
      "Epoch 111/300\n",
      "Average training loss: 0.012200300159553687\n",
      "Average test loss: 0.002409161210474041\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012195573331581222\n",
      "Average test loss: 0.0023866527250243556\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012184736455480258\n",
      "Average test loss: 0.0025699693306038777\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01218063004894389\n",
      "Average test loss: 0.00242964126707779\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012172029817269909\n",
      "Average test loss: 0.002391832306981087\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012169549426270855\n",
      "Average test loss: 0.0024059514748967356\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01216675472839011\n",
      "Average test loss: 0.002408782270840473\n",
      "Epoch 118/300\n",
      "Average training loss: 0.012154870679808987\n",
      "Average test loss: 0.002391966388457351\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012147259281741249\n",
      "Average test loss: 0.0023920330485949913\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01214283546557029\n",
      "Average test loss: 0.0024141527270484303\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012137724558512369\n",
      "Average test loss: 0.002382753344356186\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012129496379858917\n",
      "Average test loss: 0.002434252714531289\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012124134328630236\n",
      "Average test loss: 0.0023932576349212065\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012123730233973926\n",
      "Average test loss: 0.0023864627760938474\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012120134544041422\n",
      "Average test loss: 0.002396739316690299\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012102220401995711\n",
      "Average test loss: 0.0024011062377442918\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012096484378808075\n",
      "Average test loss: 0.002422979000127978\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01209167771662275\n",
      "Average test loss: 0.00240818472734342\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01208293371150891\n",
      "Average test loss: 0.002383694830764499\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012082232432232963\n",
      "Average test loss: 0.0024043546402826906\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012072248268458579\n",
      "Average test loss: 0.0023930084365937445\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012074447166588571\n",
      "Average test loss: 0.002401758031298717\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012069027235938444\n",
      "Average test loss: 0.0023986569444338483\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012066900899840726\n",
      "Average test loss: 0.0023885128531191085\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012048966319196754\n",
      "Average test loss: 0.0024131157462381654\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012043215971853998\n",
      "Average test loss: 0.002402167648697893\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012036638915538788\n",
      "Average test loss: 0.002399514806146423\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012042547999156846\n",
      "Average test loss: 0.0024191759259750444\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012025322015914651\n",
      "Average test loss: 0.00240022740037077\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012025964930653572\n",
      "Average test loss: 0.0024200637524740562\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012018417139848074\n",
      "Average test loss: 0.002435077529814508\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012018823908434974\n",
      "Average test loss: 0.002409401859260268\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012014824460777972\n",
      "Average test loss: 0.0023895374157776434\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012006470541159312\n",
      "Average test loss: 0.0024051487530685135\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01200151464011934\n",
      "Average test loss: 0.0023933881535712215\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011993709802627563\n",
      "Average test loss: 0.0024028166379365654\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011990700150529544\n",
      "Average test loss: 0.0023965092970886165\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011984418239858415\n",
      "Average test loss: 0.0023914842458648815\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011977917407949765\n",
      "Average test loss: 0.002390310507060753\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011970730212827524\n",
      "Average test loss: 0.0024033705298271446\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011963622570037842\n",
      "Average test loss: 0.002410514587432974\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011968980932401287\n",
      "Average test loss: 0.0024177331624345645\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011961531072027154\n",
      "Average test loss: 0.0023976113978359435\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011950834180745813\n",
      "Average test loss: 0.002407122583128512\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011939147676858637\n",
      "Average test loss: 0.0024011846139199203\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011937405263384182\n",
      "Average test loss: 0.0024219996535943616\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011944877559112178\n",
      "Average test loss: 0.0024255996834900645\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011932551393906275\n",
      "Average test loss: 0.0023918850030750036\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011937267283598582\n",
      "Average test loss: 0.0024202613638093073\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011926264008714093\n",
      "Average test loss: 0.0024302233447217277\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011911307562556532\n",
      "Average test loss: 0.0023857146681596835\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011914407268994384\n",
      "Average test loss: 0.0023920821969707806\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01191052051136891\n",
      "Average test loss: 0.0024064407541106144\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011902730846570599\n",
      "Average test loss: 0.0024253371831857496\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011907618958916929\n",
      "Average test loss: 0.0024013020665281347\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011896153710782528\n",
      "Average test loss: 0.002392436145287421\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011890594616532326\n",
      "Average test loss: 0.002460659494623542\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011882814795606666\n",
      "Average test loss: 0.002423681567216085\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01189046027428574\n",
      "Average test loss: 0.002416516444749302\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011873159488870038\n",
      "Average test loss: 0.0024153485120170645\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011868029557168484\n",
      "Average test loss: 0.002393642627944549\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01187781775991122\n",
      "Average test loss: 0.0024030875148665573\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011869183332555823\n",
      "Average test loss: 0.002393679939210415\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011858398254546855\n",
      "Average test loss: 0.0024093949240114954\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011852627554701435\n",
      "Average test loss: 0.0024192423907419044\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0118524308030804\n",
      "Average test loss: 0.002402793840608663\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01184658452288972\n",
      "Average test loss: 0.0024149518876026075\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011847727712657716\n",
      "Average test loss: 0.002404390210078822\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01184150006290939\n",
      "Average test loss: 0.002477189927465386\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011834521743986342\n",
      "Average test loss: 0.0024098649776230258\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011830654790831936\n",
      "Average test loss: 0.0025320786933104196\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011831033336619536\n",
      "Average test loss: 0.0024193268029226197\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011826073560449813\n",
      "Average test loss: 0.0024051745175901387\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011827591481308143\n",
      "Average test loss: 0.002421226484183636\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011817129803200562\n",
      "Average test loss: 0.0024195643193605874\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011816164427333409\n",
      "Average test loss: 0.0024029270948635207\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011823927529156208\n",
      "Average test loss: 0.002417864876695805\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011806380317442947\n",
      "Average test loss: 0.0023947014918343887\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011808221557901965\n",
      "Average test loss: 0.0024866870695518122\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011800306024650733\n",
      "Average test loss: 0.002400029333929221\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011797080294539531\n",
      "Average test loss: 0.0024550090519090493\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011788287819259696\n",
      "Average test loss: 0.002418605801856352\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011793556467526488\n",
      "Average test loss: 0.002417515299179488\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01178174512916141\n",
      "Average test loss: 0.0024232253928979236\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011780874577661355\n",
      "Average test loss: 0.002413702491256926\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011786396599478192\n",
      "Average test loss: 0.0024342881372819342\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011771202623844146\n",
      "Average test loss: 0.002410991100387441\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011775623558296098\n",
      "Average test loss: 0.0024406649702125127\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011765490068329705\n",
      "Average test loss: 0.0024224122621946866\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011763656538393763\n",
      "Average test loss: 0.002401901974239283\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011758266661730077\n",
      "Average test loss: 0.0024210545239556165\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011758413512673644\n",
      "Average test loss: 0.0024131342882497443\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011749369706544612\n",
      "Average test loss: 0.0024266622070637013\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011739957013891803\n",
      "Average test loss: 0.0024218212864879104\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011746974418560664\n",
      "Average test loss: 0.0024095370373171236\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011743031688862376\n",
      "Average test loss: 0.002439156199288037\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011751390542421076\n",
      "Average test loss: 0.0024782889051776793\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011732370520631473\n",
      "Average test loss: 0.002418390713011225\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011737721471322907\n",
      "Average test loss: 0.0024207387063652276\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011724228397011757\n",
      "Average test loss: 0.0024130739422721995\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011715207176903884\n",
      "Average test loss: 0.002450817850107948\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011721997939050198\n",
      "Average test loss: 0.0024496207578728595\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011716498893168238\n",
      "Average test loss: 0.002408740785076386\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011718971412214969\n",
      "Average test loss: 0.002432622498832643\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011706901840865612\n",
      "Average test loss: 0.0024364226151050794\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011709720203446017\n",
      "Average test loss: 0.002418404795643356\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011706777883900536\n",
      "Average test loss: 0.0024474248081031774\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011707486644387245\n",
      "Average test loss: 0.0024504220944104923\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011693957121835815\n",
      "Average test loss: 0.002420010578301218\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01169529052823782\n",
      "Average test loss: 0.002529055998971065\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0116959637420045\n",
      "Average test loss: 0.002490871054948204\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011692117354108227\n",
      "Average test loss: 0.002500098686147895\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011689314270599022\n",
      "Average test loss: 0.0024436053004529743\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011686876574324236\n",
      "Average test loss: 0.0024511180623537966\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011691399956742923\n",
      "Average test loss: 0.002446686447184119\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01167434247252014\n",
      "Average test loss: 0.0024528757201300725\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011666404087510374\n",
      "Average test loss: 0.0024279299132112\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011675166479415364\n",
      "Average test loss: 0.002443122875566284\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01166566174228986\n",
      "Average test loss: 0.002438059916305873\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01165908972422282\n",
      "Average test loss: 0.0025816997482130924\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011657888725399971\n",
      "Average test loss: 0.002434723388817575\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01165462861292892\n",
      "Average test loss: 0.0024810811871041854\n",
      "Epoch 233/300\n",
      "Average training loss: 0.011660117571552595\n",
      "Average test loss: 0.002411778553078572\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011658231125937567\n",
      "Average test loss: 0.0025234021010498205\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011638799881769551\n",
      "Average test loss: 0.0024503477740411956\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011660366398592789\n",
      "Average test loss: 0.0030331069793966083\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011673958370255099\n",
      "Average test loss: 0.002461154172817866\n",
      "Epoch 238/300\n",
      "Average training loss: 0.011634486702581246\n",
      "Average test loss: 0.002446916297905975\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011639200642704964\n",
      "Average test loss: 0.002469534081303411\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011641465215219391\n",
      "Average test loss: 0.002520486581656668\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011628398860494295\n",
      "Average test loss: 0.002438929929708441\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011632010055912866\n",
      "Average test loss: 0.0024310528753946226\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01162394711209668\n",
      "Average test loss: 0.0024192742705345153\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011632100869384076\n",
      "Average test loss: 0.0024827518065770465\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011630979132321145\n",
      "Average test loss: 0.002434303178348475\n",
      "Epoch 246/300\n",
      "Average training loss: 0.011631789603994953\n",
      "Average test loss: 0.0024634864922198985\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011624342535932858\n",
      "Average test loss: 0.002474941610876057\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011616687638892068\n",
      "Average test loss: 0.0024353695335901445\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011608863645957576\n",
      "Average test loss: 0.0024605268246183794\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011612793830947744\n",
      "Average test loss: 0.0024874628016518224\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011603132681714165\n",
      "Average test loss: 0.0024351518705694212\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011611395409537687\n",
      "Average test loss: 0.002438193715694878\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01160767435034116\n",
      "Average test loss: 0.0024418929540034797\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01159947417014175\n",
      "Average test loss: 0.0024452043045312165\n",
      "Epoch 255/300\n",
      "Average training loss: 0.011603648727138836\n",
      "Average test loss: 0.0024365671054563586\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011603266881985796\n",
      "Average test loss: 0.0024350203037675883\n",
      "Epoch 257/300\n",
      "Average training loss: 0.011589444682002067\n",
      "Average test loss: 0.002442030160687864\n",
      "Epoch 258/300\n",
      "Average training loss: 0.011598133698105813\n",
      "Average test loss: 0.002441400640954574\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011587369701100721\n",
      "Average test loss: 0.0024814249490284256\n",
      "Epoch 260/300\n",
      "Average training loss: 0.011593690815071264\n",
      "Average test loss: 0.0024436091180476877\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01158381276494927\n",
      "Average test loss: 0.0024228463630295463\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011574281191660298\n",
      "Average test loss: 0.0024351039967603154\n",
      "Epoch 263/300\n",
      "Average training loss: 0.011583042340974013\n",
      "Average test loss: 0.002502463925215933\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011577436024116145\n",
      "Average test loss: 0.002457860125021802\n",
      "Epoch 265/300\n",
      "Average training loss: 0.011573195922705863\n",
      "Average test loss: 0.0024550313454949194\n",
      "Epoch 266/300\n",
      "Average training loss: 0.011591853165792095\n",
      "Average test loss: 0.002424851389394866\n",
      "Epoch 267/300\n",
      "Average training loss: 0.011577733053929276\n",
      "Average test loss: 0.0024469230664480065\n",
      "Epoch 268/300\n",
      "Average training loss: 0.011564206138253211\n",
      "Average test loss: 0.0024575451415859992\n",
      "Epoch 269/300\n",
      "Average training loss: 0.011560645187894503\n",
      "Average test loss: 0.002479552663448784\n",
      "Epoch 270/300\n",
      "Average training loss: 0.011566708453413514\n",
      "Average test loss: 0.0024816442891541454\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01156436858408981\n",
      "Average test loss: 0.002433861124432749\n",
      "Epoch 272/300\n",
      "Average training loss: 0.011561997639636199\n",
      "Average test loss: 0.0024226093041814036\n",
      "Epoch 273/300\n",
      "Average training loss: 0.011560971689720949\n",
      "Average test loss: 0.0024934340819923414\n",
      "Epoch 274/300\n",
      "Average training loss: 0.011551896097759405\n",
      "Average test loss: 0.0024663724017639954\n",
      "Epoch 275/300\n",
      "Average training loss: 0.011554532379739814\n",
      "Average test loss: 0.00243375078547332\n",
      "Epoch 276/300\n",
      "Average training loss: 0.011553069476038218\n",
      "Average test loss: 0.0024964565649214718\n",
      "Epoch 277/300\n",
      "Average training loss: 0.011542773502568405\n",
      "Average test loss: 0.0025126062714391282\n",
      "Epoch 278/300\n",
      "Average training loss: 0.011547772944801384\n",
      "Average test loss: 0.00244653837237921\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011539629026419586\n",
      "Average test loss: 0.002437215572843949\n",
      "Epoch 280/300\n",
      "Average training loss: 0.011538608354826767\n",
      "Average test loss: 0.002454553524342676\n",
      "Epoch 281/300\n",
      "Average training loss: 0.011546555543939272\n",
      "Average test loss: 0.002451767879848679\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01153574214875698\n",
      "Average test loss: 0.0024537483879054586\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01153914984398418\n",
      "Average test loss: 0.002482211250811815\n",
      "Epoch 284/300\n",
      "Average training loss: 0.011527802377111382\n",
      "Average test loss: 0.0024681646620026893\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011524524046315087\n",
      "Average test loss: 0.0024490116360700795\n",
      "Epoch 286/300\n",
      "Average training loss: 0.011536199755966663\n",
      "Average test loss: 0.0024691306189116506\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011522638975746102\n",
      "Average test loss: 0.0024551638797339465\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011526899149020513\n",
      "Average test loss: 0.0024412159170541497\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011525624539289209\n",
      "Average test loss: 0.002462632163738211\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011518902510404588\n",
      "Average test loss: 0.0024460026024737294\n",
      "Epoch 291/300\n",
      "Average training loss: 0.011521169522570238\n",
      "Average test loss: 0.0024657888079269064\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011517074807650513\n",
      "Average test loss: 0.0024556321662126315\n",
      "Epoch 293/300\n",
      "Average training loss: 0.011516690319610966\n",
      "Average test loss: 0.0025167891485616566\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01151481154974964\n",
      "Average test loss: 0.002485691780431403\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011503340293135909\n",
      "Average test loss: 0.0024918231192148396\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011505794064866171\n",
      "Average test loss: 0.0024525430709537535\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01149886593553755\n",
      "Average test loss: 0.002438618991937902\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01150123906880617\n",
      "Average test loss: 0.002527817452740338\n",
      "Epoch 299/300\n",
      "Average training loss: 0.011494145239392916\n",
      "Average test loss: 0.0025737040475424795\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011496081620454788\n",
      "Average test loss: 0.0024757488073988095\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05202349154320028\n",
      "Average test loss: 0.0034642114270892407\n",
      "Epoch 2/300\n",
      "Average training loss: 0.016130210048622556\n",
      "Average test loss: 0.0031358917914330957\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014744552216596074\n",
      "Average test loss: 0.002851893023070362\n",
      "Epoch 4/300\n",
      "Average training loss: 0.014015430032379098\n",
      "Average test loss: 0.002777009093720052\n",
      "Epoch 5/300\n",
      "Average training loss: 0.013493267028696007\n",
      "Average test loss: 0.0026111554031570754\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013063855119049549\n",
      "Average test loss: 0.0025368891776435904\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012728646602067682\n",
      "Average test loss: 0.002430456170398328\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01244680792093277\n",
      "Average test loss: 0.002433711712869505\n",
      "Epoch 9/300\n",
      "Average training loss: 0.012182386101947891\n",
      "Average test loss: 0.002313167664843301\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01197168070409033\n",
      "Average test loss: 0.0022842399577299756\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011790306277573109\n",
      "Average test loss: 0.0022555530348585713\n",
      "Epoch 12/300\n",
      "Average training loss: 0.011630949834982553\n",
      "Average test loss: 0.0021994080611815055\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011497915239797698\n",
      "Average test loss: 0.0021751871845788425\n",
      "Epoch 14/300\n",
      "Average training loss: 0.011386840644809935\n",
      "Average test loss: 0.0021423048006577626\n",
      "Epoch 15/300\n",
      "Average training loss: 0.011261143492327796\n",
      "Average test loss: 0.0022170247089945607\n",
      "Epoch 16/300\n",
      "Average training loss: 0.011177513068748845\n",
      "Average test loss: 0.002096649476430482\n",
      "Epoch 17/300\n",
      "Average training loss: 0.011075223653680749\n",
      "Average test loss: 0.0022076193743074935\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010984629025889767\n",
      "Average test loss: 0.0020853860545903442\n",
      "Epoch 19/300\n",
      "Average training loss: 0.010905905504193571\n",
      "Average test loss: 0.0020450877492419546\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010823127034223742\n",
      "Average test loss: 0.002053353727484743\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010752818191217052\n",
      "Average test loss: 0.002014046165885197\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010696237410936092\n",
      "Average test loss: 0.0019951594370520775\n",
      "Epoch 23/300\n",
      "Average training loss: 0.010632241023083527\n",
      "Average test loss: 0.001980120279412303\n",
      "Epoch 24/300\n",
      "Average training loss: 0.010588869040211042\n",
      "Average test loss: 0.001976232052884168\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010521289468639427\n",
      "Average test loss: 0.001951093932096329\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010483175040947066\n",
      "Average test loss: 0.001945532561176353\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010448593803577953\n",
      "Average test loss: 0.0019240787936788467\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010394029370612567\n",
      "Average test loss: 0.0019192813736283118\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01037295334206687\n",
      "Average test loss: 0.0019221853061268726\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010326700322329997\n",
      "Average test loss: 0.0019256011900595494\n",
      "Epoch 31/300\n",
      "Average training loss: 0.010312267922692828\n",
      "Average test loss: 0.0019384738492468994\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01027259454710616\n",
      "Average test loss: 0.0019479021495208144\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010236489351010986\n",
      "Average test loss: 0.0018989274783266915\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010211999135712782\n",
      "Average test loss: 0.0018847204751024644\n",
      "Epoch 35/300\n",
      "Average training loss: 0.010186418068905672\n",
      "Average test loss: 0.0018912119385268954\n",
      "Epoch 36/300\n",
      "Average training loss: 0.010163186547656853\n",
      "Average test loss: 0.0018692425152079927\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010144346432553398\n",
      "Average test loss: 0.0018718240688451462\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010111067306664254\n",
      "Average test loss: 0.0018949706703424453\n",
      "Epoch 39/300\n",
      "Average training loss: 0.010096866252521673\n",
      "Average test loss: 0.0018591736455758413\n",
      "Epoch 40/300\n",
      "Average training loss: 0.010076576078103649\n",
      "Average test loss: 0.0018540594946179125\n",
      "Epoch 41/300\n",
      "Average training loss: 0.010058141973283556\n",
      "Average test loss: 0.0018640276874519056\n",
      "Epoch 42/300\n",
      "Average training loss: 0.010037410514222252\n",
      "Average test loss: 0.0018590807360079553\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010022778785891003\n",
      "Average test loss: 0.0019387061109559403\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009992891252454785\n",
      "Average test loss: 0.001855508704773254\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009978836158083544\n",
      "Average test loss: 0.001848222360221876\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009975218416916\n",
      "Average test loss: 0.0018480862792995241\n",
      "Epoch 47/300\n",
      "Average training loss: 0.009952515838046868\n",
      "Average test loss: 0.0018392066504392358\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009932919512606329\n",
      "Average test loss: 0.0018382158972736862\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009921989091568523\n",
      "Average test loss: 0.0018329634813384877\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009903306093066932\n",
      "Average test loss: 0.001845631462832292\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009890929002728727\n",
      "Average test loss: 0.0018351588482554589\n",
      "Epoch 52/300\n",
      "Average training loss: 0.009876371915969583\n",
      "Average test loss: 0.001829793587533964\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009859973140888744\n",
      "Average test loss: 0.0018427701109192437\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009847198474738333\n",
      "Average test loss: 0.0018159047007146809\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00983554793149233\n",
      "Average test loss: 0.0018347133272844883\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009823608203480641\n",
      "Average test loss: 0.0018174424606064955\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00980885644753774\n",
      "Average test loss: 0.0018226518920726245\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009804316478470961\n",
      "Average test loss: 0.001824556556944218\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009793931596808964\n",
      "Average test loss: 0.0018155248719784948\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009781127016577455\n",
      "Average test loss: 0.0018086523465398285\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009763209361996915\n",
      "Average test loss: 0.0018141035832361214\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009757902295225196\n",
      "Average test loss: 0.001832323178441988\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009751582208606932\n",
      "Average test loss: 0.0018058368511911895\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009731095663375326\n",
      "Average test loss: 0.0018074968685913417\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009733990882005956\n",
      "Average test loss: 0.0018084318749606609\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009711390649278959\n",
      "Average test loss: 0.0018007595491492086\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009708411561946075\n",
      "Average test loss: 0.001802493047176136\n",
      "Epoch 68/300\n",
      "Average training loss: 0.00969925659812159\n",
      "Average test loss: 0.0018072322618423236\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009683582425117493\n",
      "Average test loss: 0.0018010914833802316\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009679270640015602\n",
      "Average test loss: 0.001812691406864259\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009668508656322955\n",
      "Average test loss: 0.0018039140991556147\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00966201546953784\n",
      "Average test loss: 0.0017941068675782945\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009649926720394029\n",
      "Average test loss: 0.001789661874787675\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00964028941343228\n",
      "Average test loss: 0.0017920447130584055\n",
      "Epoch 75/300\n",
      "Average training loss: 0.00963218527701166\n",
      "Average test loss: 0.0017901738036630883\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009630132828321722\n",
      "Average test loss: 0.001793357286705739\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009609251182112429\n",
      "Average test loss: 0.0017880161899245448\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009611303061246872\n",
      "Average test loss: 0.0019024540244912107\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009597753783067068\n",
      "Average test loss: 0.0017864340400944154\n",
      "Epoch 80/300\n",
      "Average training loss: 0.009588191515455643\n",
      "Average test loss: 0.0018078606168015136\n",
      "Epoch 81/300\n",
      "Average training loss: 0.009585556223988533\n",
      "Average test loss: 0.0017892481261450384\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009568714348806275\n",
      "Average test loss: 0.0017880455910538634\n",
      "Epoch 83/300\n",
      "Average training loss: 0.009562029429607922\n",
      "Average test loss: 0.0017854093592613936\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009559395160112116\n",
      "Average test loss: 0.0017882555491394467\n",
      "Epoch 85/300\n",
      "Average training loss: 0.009552075965536965\n",
      "Average test loss: 0.001788884842561351\n",
      "Epoch 86/300\n",
      "Average training loss: 0.009544149791200955\n",
      "Average test loss: 0.0017805318679246637\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00953894912575682\n",
      "Average test loss: 0.0018347545568313864\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00953007114181916\n",
      "Average test loss: 0.0018155762652556102\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009525195042292278\n",
      "Average test loss: 0.0017844033210227886\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009518080150087675\n",
      "Average test loss: 0.0017824771487050587\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009510572983986802\n",
      "Average test loss: 0.0017993879643165402\n",
      "Epoch 92/300\n",
      "Average training loss: 0.009504949043194454\n",
      "Average test loss: 0.0017896980470460322\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00950336753990915\n",
      "Average test loss: 0.0017731901362745299\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009490953102707863\n",
      "Average test loss: 0.0017952643944364455\n",
      "Epoch 95/300\n",
      "Average training loss: 0.009485241906096538\n",
      "Average test loss: 0.001789474894396133\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009476521888954773\n",
      "Average test loss: 0.001783179172159483\n",
      "Epoch 97/300\n",
      "Average training loss: 0.009474350489262078\n",
      "Average test loss: 0.0017976164331452713\n",
      "Epoch 98/300\n",
      "Average training loss: 0.009467903924485047\n",
      "Average test loss: 0.0017765738389765222\n",
      "Epoch 99/300\n",
      "Average training loss: 0.009458228760295444\n",
      "Average test loss: 0.001794318502768874\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00945630775557624\n",
      "Average test loss: 0.0018069425241814718\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009452776895629036\n",
      "Average test loss: 0.0017793733226135373\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009442570762501823\n",
      "Average test loss: 0.0017936373245384957\n",
      "Epoch 103/300\n",
      "Average training loss: 0.009438851606928639\n",
      "Average test loss: 0.0018005003211502399\n",
      "Epoch 104/300\n",
      "Average training loss: 0.009437137310703595\n",
      "Average test loss: 0.0017780238469648692\n",
      "Epoch 105/300\n",
      "Average training loss: 0.00942426131086217\n",
      "Average test loss: 0.0019974573788543544\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009416698403656483\n",
      "Average test loss: 0.0017818539089833696\n",
      "Epoch 107/300\n",
      "Average training loss: 0.009426919250024689\n",
      "Average test loss: 0.0017861000867560506\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00940981262177229\n",
      "Average test loss: 0.0017683359535100559\n",
      "Epoch 109/300\n",
      "Average training loss: 0.009401572929488289\n",
      "Average test loss: 0.001782646657485101\n",
      "Epoch 110/300\n",
      "Average training loss: 0.00939437888811032\n",
      "Average test loss: 0.001780333968396816\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00939278200434314\n",
      "Average test loss: 0.0017667156123659677\n",
      "Epoch 112/300\n",
      "Average training loss: 0.009383916780352592\n",
      "Average test loss: 0.0017793597582106788\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009387056321733528\n",
      "Average test loss: 0.0017728828273506627\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00937345409558879\n",
      "Average test loss: 0.0017789822382231553\n",
      "Epoch 115/300\n",
      "Average training loss: 0.009371240745815966\n",
      "Average test loss: 0.0017783099834082854\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0093723357303275\n",
      "Average test loss: 0.0017905720040823022\n",
      "Epoch 117/300\n",
      "Average training loss: 0.009368753446473015\n",
      "Average test loss: 0.0017763802092522383\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00935873108688328\n",
      "Average test loss: 0.001777218806453877\n",
      "Epoch 119/300\n",
      "Average training loss: 0.009351225213044219\n",
      "Average test loss: 0.0017794985784631637\n",
      "Epoch 120/300\n",
      "Average training loss: 0.009342272422379918\n",
      "Average test loss: 0.0017780049024149775\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009338523861020803\n",
      "Average test loss: 0.0017970531398637428\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009332294619745678\n",
      "Average test loss: 0.001798705939617422\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009331917905145221\n",
      "Average test loss: 0.0017666288325563072\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009330853013528718\n",
      "Average test loss: 0.001774242689108683\n",
      "Epoch 125/300\n",
      "Average training loss: 0.009328249039749305\n",
      "Average test loss: 0.002111431579726438\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009312053595980009\n",
      "Average test loss: 0.0017806649601293935\n",
      "Epoch 127/300\n",
      "Average training loss: 0.009318403422832489\n",
      "Average test loss: 0.0017848617773916985\n",
      "Epoch 128/300\n",
      "Average training loss: 0.00930755543212096\n",
      "Average test loss: 0.0018090693846137987\n",
      "Epoch 129/300\n",
      "Average training loss: 0.009303250138958295\n",
      "Average test loss: 0.001779110965422458\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009300357848405837\n",
      "Average test loss: 0.001771981749890579\n",
      "Epoch 131/300\n",
      "Average training loss: 0.009293611260751884\n",
      "Average test loss: 0.001798401421142949\n",
      "Epoch 132/300\n",
      "Average training loss: 0.009291882644924853\n",
      "Average test loss: 0.0017660055928346184\n",
      "Epoch 133/300\n",
      "Average training loss: 0.009286522552371026\n",
      "Average test loss: 0.0017898514635033079\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009281480370296372\n",
      "Average test loss: 0.0017727760484235154\n",
      "Epoch 135/300\n",
      "Average training loss: 0.00927376024921735\n",
      "Average test loss: 0.0017916713272117905\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009273740253514713\n",
      "Average test loss: 0.001775655208207253\n",
      "Epoch 137/300\n",
      "Average training loss: 0.009263790866567029\n",
      "Average test loss: 0.0017770104646268818\n",
      "Epoch 138/300\n",
      "Average training loss: 0.009267470276190175\n",
      "Average test loss: 0.001789706068734328\n",
      "Epoch 139/300\n",
      "Average training loss: 0.009267204677893055\n",
      "Average test loss: 0.0017772708502080705\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00925242807964484\n",
      "Average test loss: 0.0017989323514824111\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00925324541123377\n",
      "Average test loss: 0.0017882472150441673\n",
      "Epoch 142/300\n",
      "Average training loss: 0.009247215539216995\n",
      "Average test loss: 0.0017897964631103807\n",
      "Epoch 143/300\n",
      "Average training loss: 0.009249578171720108\n",
      "Average test loss: 0.0017911455384973024\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009237167014843888\n",
      "Average test loss: 0.0017852511931624676\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009234865431570344\n",
      "Average test loss: 0.0017756013700531588\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009234077242513497\n",
      "Average test loss: 0.0017714916259671252\n",
      "Epoch 147/300\n",
      "Average training loss: 0.009232522010803223\n",
      "Average test loss: 0.0017816988420155313\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009225577895012167\n",
      "Average test loss: 0.0018252547096668018\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009221705310046673\n",
      "Average test loss: 0.001776028140137593\n",
      "Epoch 150/300\n",
      "Average training loss: 0.009220414134777255\n",
      "Average test loss: 0.0017795273944114646\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009211744464519951\n",
      "Average test loss: 0.0018104218972019024\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009213531498279836\n",
      "Average test loss: 0.0017879407250632842\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009212283066991303\n",
      "Average test loss: 0.0017894564785270227\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009204063832759858\n",
      "Average test loss: 0.001774496876841618\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009211320071584648\n",
      "Average test loss: 0.0017906605552674996\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009198228365017307\n",
      "Average test loss: 0.0017801382334695923\n",
      "Epoch 157/300\n",
      "Average training loss: 0.009189497675332758\n",
      "Average test loss: 0.001777647871731056\n",
      "Epoch 158/300\n",
      "Average training loss: 0.009188823697467645\n",
      "Average test loss: 0.001806016161520448\n",
      "Epoch 159/300\n",
      "Average training loss: 0.009188086888856358\n",
      "Average test loss: 0.0017719725786397855\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009185119899610678\n",
      "Average test loss: 0.0017841853075143365\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009174451654983892\n",
      "Average test loss: 0.0017776903469736377\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009172771211299632\n",
      "Average test loss: 0.0017843473670590255\n",
      "Epoch 163/300\n",
      "Average training loss: 0.009170935076971849\n",
      "Average test loss: 0.0017851029658276173\n",
      "Epoch 164/300\n",
      "Average training loss: 0.009172916268308958\n",
      "Average test loss: 0.001771943564630217\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009169346941014131\n",
      "Average test loss: 0.0017898439077867403\n",
      "Epoch 166/300\n",
      "Average training loss: 0.009169845030539565\n",
      "Average test loss: 0.0017789651198933522\n",
      "Epoch 167/300\n",
      "Average training loss: 0.009153316544161903\n",
      "Average test loss: 0.0017781196417700914\n",
      "Epoch 168/300\n",
      "Average training loss: 0.00916852307981915\n",
      "Average test loss: 0.001799032289120886\n",
      "Epoch 169/300\n",
      "Average training loss: 0.009155437293979857\n",
      "Average test loss: 0.0017913010513616932\n",
      "Epoch 170/300\n",
      "Average training loss: 0.009147086405919658\n",
      "Average test loss: 0.0018004236788385444\n",
      "Epoch 171/300\n",
      "Average training loss: 0.009142116392652194\n",
      "Average test loss: 0.0017940582384665808\n",
      "Epoch 172/300\n",
      "Average training loss: 0.009148109461698268\n",
      "Average test loss: 0.0017786167215348946\n",
      "Epoch 173/300\n",
      "Average training loss: 0.009145900013546149\n",
      "Average test loss: 0.0017904077316634357\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009135146966411007\n",
      "Average test loss: 0.00177968749207341\n",
      "Epoch 175/300\n",
      "Average training loss: 0.009138345884366168\n",
      "Average test loss: 0.0017821770346620016\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009130299643095996\n",
      "Average test loss: 0.0017846456325302522\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009128717746171687\n",
      "Average test loss: 0.0017872112182279428\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009123905195130242\n",
      "Average test loss: 0.0017813992403033707\n",
      "Epoch 179/300\n",
      "Average training loss: 0.00912222406309512\n",
      "Average test loss: 0.0017793868134419123\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009126687539948358\n",
      "Average test loss: 0.0017848824589616723\n",
      "Epoch 181/300\n",
      "Average training loss: 0.009119553806053268\n",
      "Average test loss: 0.0017854830901035004\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009113986545138888\n",
      "Average test loss: 0.0017777257160180144\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009117561899125576\n",
      "Average test loss: 0.0017792855902678437\n",
      "Epoch 184/300\n",
      "Average training loss: 0.009103912173046007\n",
      "Average test loss: 0.0017727030913035075\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009101624709036615\n",
      "Average test loss: 0.001788678937074211\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009100663272043069\n",
      "Average test loss: 0.0017691641895928316\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009113346709973283\n",
      "Average test loss: 0.0017805927416516675\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009098720887882841\n",
      "Average test loss: 0.0018030251547073324\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009097277931041188\n",
      "Average test loss: 0.0018532782114214367\n",
      "Epoch 190/300\n",
      "Average training loss: 0.009096071689079206\n",
      "Average test loss: 0.001804276500414643\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009089520086430842\n",
      "Average test loss: 0.0017735916029454934\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009094741842399041\n",
      "Average test loss: 0.001782936807307932\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00907914266652531\n",
      "Average test loss: 0.0017788213545249568\n",
      "Epoch 194/300\n",
      "Average training loss: 0.009081498144401444\n",
      "Average test loss: 0.0017976773121497697\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009092207233938906\n",
      "Average test loss: 0.0017761055467029412\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0090854098258747\n",
      "Average test loss: 0.0018008188529767923\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009074939879692262\n",
      "Average test loss: 0.0017920983450280296\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009079063207738929\n",
      "Average test loss: 0.0018088567662570211\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009063671219680044\n",
      "Average test loss: 0.0017964628987635176\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009069208630257182\n",
      "Average test loss: 0.001782315651886165\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00906594647715489\n",
      "Average test loss: 0.0017760750593410598\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00906916659987635\n",
      "Average test loss: 0.0017833231604761548\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009059576341675387\n",
      "Average test loss: 0.0017839208895133601\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009049636480708917\n",
      "Average test loss: 0.0017804807656341128\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009061322391861014\n",
      "Average test loss: 0.0017879325557086203\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00906150737322039\n",
      "Average test loss: 0.0018096820602400436\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009047772431125244\n",
      "Average test loss: 0.0018004744345736172\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009043489752544298\n",
      "Average test loss: 0.001788328645336959\n",
      "Epoch 209/300\n",
      "Average training loss: 0.009050268773403432\n",
      "Average test loss: 0.0019156980503143536\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009044815301895142\n",
      "Average test loss: 0.0017819513444685274\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009045199813942114\n",
      "Average test loss: 0.0017907372609608702\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0090333725967341\n",
      "Average test loss: 0.0018149570090075333\n",
      "Epoch 213/300\n",
      "Average training loss: 0.009036264683637354\n",
      "Average test loss: 0.001810982110703157\n",
      "Epoch 214/300\n",
      "Average training loss: 0.009035266098876794\n",
      "Average test loss: 0.00183238451814072\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00903591349803739\n",
      "Average test loss: 0.0017906576509897907\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009027926961580912\n",
      "Average test loss: 0.002035709999915626\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009023910116818216\n",
      "Average test loss: 0.0017877330879370371\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009027329839766025\n",
      "Average test loss: 0.001798002428168224\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009021601028326484\n",
      "Average test loss: 0.0018022423025427594\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009025811673866378\n",
      "Average test loss: 0.0017958113908146819\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009019902556306785\n",
      "Average test loss: 0.0017860512603074313\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009025089977516068\n",
      "Average test loss: 0.0018254168298509387\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009010772328409883\n",
      "Average test loss: 0.001783234378012518\n",
      "Epoch 224/300\n",
      "Average training loss: 0.009006344760457675\n",
      "Average test loss: 0.0017934112534340886\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009007995655553208\n",
      "Average test loss: 0.0017895518672756024\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009009304129415089\n",
      "Average test loss: 0.0017803688784026436\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009001778438687325\n",
      "Average test loss: 0.001814155309771498\n",
      "Epoch 228/300\n",
      "Average training loss: 0.009002686454190148\n",
      "Average test loss: 0.001785125236854785\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009007308710780409\n",
      "Average test loss: 0.001813562537750436\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009002774097439315\n",
      "Average test loss: 0.0017897015677558051\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008998989256719749\n",
      "Average test loss: 0.001799264685354299\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008993830264442497\n",
      "Average test loss: 0.0018909297773821486\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008995914058138927\n",
      "Average test loss: 0.001794275035046869\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008984902917510933\n",
      "Average test loss: 0.001786497199235277\n",
      "Epoch 235/300\n",
      "Average training loss: 0.008989015181031491\n",
      "Average test loss: 0.0018114829141025741\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008996361419558525\n",
      "Average test loss: 0.0017981305055113303\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008993505926595794\n",
      "Average test loss: 0.0018223481617040105\n",
      "Epoch 238/300\n",
      "Average training loss: 0.008981752971808116\n",
      "Average test loss: 0.0017902992653350035\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00898481997102499\n",
      "Average test loss: 0.001809227683270971\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008987660873267385\n",
      "Average test loss: 0.0018037778176367282\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008976210489869118\n",
      "Average test loss: 0.0018025073785748746\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008975947566330434\n",
      "Average test loss: 0.0017917185214658578\n",
      "Epoch 243/300\n",
      "Average training loss: 0.008973295054501958\n",
      "Average test loss: 0.0017855101759648986\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008970530952844355\n",
      "Average test loss: 0.0017931065798426668\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008966907545096344\n",
      "Average test loss: 0.0018025312423706054\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008969018697738647\n",
      "Average test loss: 0.0017930644065555599\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008973205169455873\n",
      "Average test loss: 0.001793056205742889\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008971882345775763\n",
      "Average test loss: 0.0019526496757235792\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008963094335463311\n",
      "Average test loss: 0.001785845494725638\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008947957900663217\n",
      "Average test loss: 0.0018042950794721643\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008960598122742441\n",
      "Average test loss: 0.0018010136704477999\n",
      "Epoch 252/300\n",
      "Average training loss: 0.00896252275755008\n",
      "Average test loss: 0.0017983708824548456\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008957418149544132\n",
      "Average test loss: 0.0018084536240332656\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008954407847589916\n",
      "Average test loss: 0.001829403733006782\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008955820800529585\n",
      "Average test loss: 0.001791470788936648\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008948324386444356\n",
      "Average test loss: 0.0018093626802373264\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008945276061693827\n",
      "Average test loss: 0.0017935661948803399\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008952246715625127\n",
      "Average test loss: 0.0017926929922153553\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008952523588306375\n",
      "Average test loss: 0.0018117792658093902\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008939503885805607\n",
      "Average test loss: 0.0018401327598839999\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008938242334458562\n",
      "Average test loss: 0.0017924625896331336\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008944001578622395\n",
      "Average test loss: 0.0018153993123107485\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008934559162292216\n",
      "Average test loss: 0.0017939734505489468\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008936091244220734\n",
      "Average test loss: 0.001787282427565919\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008932588019304806\n",
      "Average test loss: 0.0018174756958873736\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008931229283412298\n",
      "Average test loss: 0.0018025374168323146\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008939862028592163\n",
      "Average test loss: 0.0017885139685952002\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008925504938595825\n",
      "Average test loss: 0.001822819117249714\n",
      "Epoch 269/300\n",
      "Average training loss: 0.00892724881403976\n",
      "Average test loss: 0.0018018747955146762\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008925174164275329\n",
      "Average test loss: 0.0017950572293872634\n",
      "Epoch 271/300\n",
      "Average training loss: 0.00893064686573214\n",
      "Average test loss: 0.0018268477546258106\n",
      "Epoch 272/300\n",
      "Average training loss: 0.00893108484438724\n",
      "Average test loss: 0.0017938134518141549\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008918923070447312\n",
      "Average test loss: 0.0018185812578433089\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008920537603398164\n",
      "Average test loss: 0.0017952169065053264\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008916750992337862\n",
      "Average test loss: 0.001807876678597596\n",
      "Epoch 276/300\n",
      "Average training loss: 0.00891171939795216\n",
      "Average test loss: 0.001804675209749904\n",
      "Epoch 277/300\n",
      "Average training loss: 0.00890965849492285\n",
      "Average test loss: 0.0017908813472216328\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008911231376644638\n",
      "Average test loss: 0.001801968088787463\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008911882973379559\n",
      "Average test loss: 0.0018059442631072468\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008919634499483638\n",
      "Average test loss: 0.0017953311260789633\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008908676671485106\n",
      "Average test loss: 0.0018056493322882387\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00890593833476305\n",
      "Average test loss: 0.0018395278841877977\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008906514995627933\n",
      "Average test loss: 0.0017919144796000586\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008905003647009531\n",
      "Average test loss: 0.001797496544714603\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00889643578024374\n",
      "Average test loss: 0.0018041502026220162\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008896483369999462\n",
      "Average test loss: 0.001797018069980873\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008899363263613649\n",
      "Average test loss: 0.0018292252571425504\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008896018292340968\n",
      "Average test loss: 0.0018080211263149977\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008899013770951165\n",
      "Average test loss: 0.0018105297554284335\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008891318870915307\n",
      "Average test loss: 0.0017964756381180552\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008900074957145584\n",
      "Average test loss: 0.001832219323143363\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008898196068902811\n",
      "Average test loss: 0.0018143693332870801\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008890087715867493\n",
      "Average test loss: 0.0018153415282981264\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008887340424375402\n",
      "Average test loss: 0.0018211812817802032\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008885174946652518\n",
      "Average test loss: 0.0018159772606773509\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008891026876866817\n",
      "Average test loss: 0.001793029471921424\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008884545183844036\n",
      "Average test loss: 0.0017928048815164302\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008883510501848327\n",
      "Average test loss: 0.0018110502566107446\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008881744282941023\n",
      "Average test loss: 0.0018134273543126053\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008880620314843124\n",
      "Average test loss: 0.0018102632049057218\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive_Depth3-.025/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.22\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.09\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7472301466597451\n",
      "Average test loss: 0.005642343551748329\n",
      "Epoch 2/300\n",
      "Average training loss: 0.1162312316497167\n",
      "Average test loss: 0.005135427174882756\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08467493845356835\n",
      "Average test loss: 0.004760612784160508\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07674572073088752\n",
      "Average test loss: 0.004645885417444838\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07325693128506343\n",
      "Average test loss: 0.004562300014413065\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07109279965029823\n",
      "Average test loss: 0.00454732714055313\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06957077228029569\n",
      "Average test loss: 0.004453626770940092\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06840863718920284\n",
      "Average test loss: 0.004443738923304611\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06744307521979014\n",
      "Average test loss: 0.00448401766974065\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0666972025235494\n",
      "Average test loss: 0.004358811146683163\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06611265350712671\n",
      "Average test loss: 0.00434938456159499\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0655945113201936\n",
      "Average test loss: 0.004336477340509494\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06513590301407708\n",
      "Average test loss: 0.0043176128187527255\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0647513949473699\n",
      "Average test loss: 0.00432343121452464\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06440134446488487\n",
      "Average test loss: 0.004292180128809478\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06416027675734626\n",
      "Average test loss: 0.004255637844403585\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0638518610563543\n",
      "Average test loss: 0.004259561286618312\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06360060950782565\n",
      "Average test loss: 0.004285259519393245\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06335345095396042\n",
      "Average test loss: 0.0042225968121654455\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06312172513537936\n",
      "Average test loss: 0.004208120369455881\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06301302542620235\n",
      "Average test loss: 0.004197240178783735\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0628188360300329\n",
      "Average test loss: 0.004280001996085048\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06264154837528864\n",
      "Average test loss: 0.0041892683733668595\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0624253498547607\n",
      "Average test loss: 0.0041711418537629975\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06228847122192383\n",
      "Average test loss: 0.004190121059616407\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06214902585413721\n",
      "Average test loss: 0.0041564261606997915\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06199188972181744\n",
      "Average test loss: 0.004151637910968727\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06185468602842755\n",
      "Average test loss: 0.004141847421725591\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0617271761364407\n",
      "Average test loss: 0.00412630827517973\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06158353983693653\n",
      "Average test loss: 0.004126927921755446\n",
      "Epoch 31/300\n",
      "Average training loss: 0.061478583176930744\n",
      "Average test loss: 0.0041412719918621915\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0613669708304935\n",
      "Average test loss: 0.00410838219937351\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06125834061702092\n",
      "Average test loss: 0.004107127582033476\n",
      "Epoch 34/300\n",
      "Average training loss: 0.061176936520470515\n",
      "Average test loss: 0.004147947336650557\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06104667013221317\n",
      "Average test loss: 0.004084788351837132\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06099563673138619\n",
      "Average test loss: 0.004075996471361982\n",
      "Epoch 37/300\n",
      "Average training loss: 0.060889571699831224\n",
      "Average test loss: 0.004085892927729421\n",
      "Epoch 38/300\n",
      "Average training loss: 0.060800701595014994\n",
      "Average test loss: 0.004073807619305121\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06073048155506452\n",
      "Average test loss: 0.004076156844488449\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06068060872952143\n",
      "Average test loss: 0.0040695003755390645\n",
      "Epoch 41/300\n",
      "Average training loss: 0.060559829569525186\n",
      "Average test loss: 0.004072445093550616\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06052424963977602\n",
      "Average test loss: 0.004075581325011121\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06044108200404379\n",
      "Average test loss: 0.004061597248332368\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0603817149731848\n",
      "Average test loss: 0.004046307436294026\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06031993759009573\n",
      "Average test loss: 0.004057989403605461\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06028009935551219\n",
      "Average test loss: 0.004068423834732837\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0602213279041979\n",
      "Average test loss: 0.004049805340667565\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06015622149242295\n",
      "Average test loss: 0.004042498550688227\n",
      "Epoch 49/300\n",
      "Average training loss: 0.060084079633156456\n",
      "Average test loss: 0.004043883512831396\n",
      "Epoch 50/300\n",
      "Average training loss: 0.060052670813269086\n",
      "Average test loss: 0.004051640790369775\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06000069889260663\n",
      "Average test loss: 0.004031398286836015\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05992392328712675\n",
      "Average test loss: 0.004043229992190997\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05990509951776928\n",
      "Average test loss: 0.004029405005690124\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05984619290298886\n",
      "Average test loss: 0.0040478254177918034\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05978898173239496\n",
      "Average test loss: 0.004037846626299951\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05977901574969292\n",
      "Average test loss: 0.004014681244062053\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05969584749142329\n",
      "Average test loss: 0.004022850496280524\n",
      "Epoch 58/300\n",
      "Average training loss: 0.059682561549875474\n",
      "Average test loss: 0.004031365818447537\n",
      "Epoch 59/300\n",
      "Average training loss: 0.059609953549173145\n",
      "Average test loss: 0.0040381836162673105\n",
      "Epoch 60/300\n",
      "Average training loss: 0.059569365998109185\n",
      "Average test loss: 0.004027157868362136\n",
      "Epoch 61/300\n",
      "Average training loss: 0.059557623975806766\n",
      "Average test loss: 0.004029242878572808\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05948260191414091\n",
      "Average test loss: 0.0040300895625518425\n",
      "Epoch 63/300\n",
      "Average training loss: 0.059444421860906815\n",
      "Average test loss: 0.004021462553077274\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05945618024799559\n",
      "Average test loss: 0.004012150058936742\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05936469383206632\n",
      "Average test loss: 0.0040672098091906975\n",
      "Epoch 66/300\n",
      "Average training loss: 0.059363379677136736\n",
      "Average test loss: 0.004022535085264179\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05931108108494017\n",
      "Average test loss: 0.004014112189412117\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05923847010069423\n",
      "Average test loss: 0.004024253100363744\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05921075589789285\n",
      "Average test loss: 0.0040071593345039425\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05919777815540632\n",
      "Average test loss: 0.004008158871490094\n",
      "Epoch 71/300\n",
      "Average training loss: 0.059168394595384595\n",
      "Average test loss: 0.004009140811860561\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05912378614809778\n",
      "Average test loss: 0.004031129685540994\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05908662928475274\n",
      "Average test loss: 0.004018827576811115\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05903873149222798\n",
      "Average test loss: 0.004028406277712848\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05899821619192759\n",
      "Average test loss: 0.004022276887049278\n",
      "Epoch 76/300\n",
      "Average training loss: 0.058987437291277776\n",
      "Average test loss: 0.00401510200690892\n",
      "Epoch 77/300\n",
      "Average training loss: 0.058924500859446\n",
      "Average test loss: 0.004040578887694411\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05887367423044311\n",
      "Average test loss: 0.004014370724144909\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05887000007430712\n",
      "Average test loss: 0.004008587470485104\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05885032557778888\n",
      "Average test loss: 0.00400939563040932\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05876295108265347\n",
      "Average test loss: 0.004004165801737044\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05877824935979313\n",
      "Average test loss: 0.004023094753010405\n",
      "Epoch 83/300\n",
      "Average training loss: 0.058739155650138854\n",
      "Average test loss: 0.004018075899117523\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05870131666461627\n",
      "Average test loss: 0.004241583945022689\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05864056767688857\n",
      "Average test loss: 0.0040284787511659995\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05862965405980746\n",
      "Average test loss: 0.003996989080889357\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0585480573144224\n",
      "Average test loss: 0.004031179940534963\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05853093454572889\n",
      "Average test loss: 0.004050491912911336\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05852556543548902\n",
      "Average test loss: 0.004019407369196415\n",
      "Epoch 90/300\n",
      "Average training loss: 0.058472941199938455\n",
      "Average test loss: 0.004018287543207407\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05841783338785172\n",
      "Average test loss: 0.004015612881216738\n",
      "Epoch 92/300\n",
      "Average training loss: 0.058393114742305546\n",
      "Average test loss: 0.00400689824587769\n",
      "Epoch 93/300\n",
      "Average training loss: 0.058358117358552085\n",
      "Average test loss: 0.004015983120848735\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05834694679578145\n",
      "Average test loss: 0.004000527447710435\n",
      "Epoch 95/300\n",
      "Average training loss: 0.058255638043085735\n",
      "Average test loss: 0.004017032905792197\n",
      "Epoch 96/300\n",
      "Average training loss: 0.058265990525484083\n",
      "Average test loss: 0.004002020576761829\n",
      "Epoch 97/300\n",
      "Average training loss: 0.058234717736641566\n",
      "Average test loss: 0.00402605217984981\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05819822822345628\n",
      "Average test loss: 0.004033401581148306\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05814671044548352\n",
      "Average test loss: 0.004013696624586979\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05812182296315829\n",
      "Average test loss: 0.00400815918834673\n",
      "Epoch 101/300\n",
      "Average training loss: 0.058141623503632014\n",
      "Average test loss: 0.004013542126864195\n",
      "Epoch 102/300\n",
      "Average training loss: 0.058050112436215086\n",
      "Average test loss: 0.0040141806757698455\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0580101618303193\n",
      "Average test loss: 0.004016446235279242\n",
      "Epoch 104/300\n",
      "Average training loss: 0.057991715603404576\n",
      "Average test loss: 0.004052043536677956\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05792574124534925\n",
      "Average test loss: 0.004030678341372145\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05788609798087014\n",
      "Average test loss: 0.004004406308755279\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05786274601022402\n",
      "Average test loss: 0.0040304295909073615\n",
      "Epoch 108/300\n",
      "Average training loss: 0.057856441656748456\n",
      "Average test loss: 0.004018422256741259\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05780722041924795\n",
      "Average test loss: 0.004021969303902652\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05777837127778265\n",
      "Average test loss: 0.0040091469842526645\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05772404412759675\n",
      "Average test loss: 0.004029763473818699\n",
      "Epoch 112/300\n",
      "Average training loss: 0.057710243069463305\n",
      "Average test loss: 0.004019786185481482\n",
      "Epoch 113/300\n",
      "Average training loss: 0.057646949211756386\n",
      "Average test loss: 0.004008647643857532\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05767251232266426\n",
      "Average test loss: 0.004009556509140465\n",
      "Epoch 115/300\n",
      "Average training loss: 0.057590010315179824\n",
      "Average test loss: 0.004027146751061082\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05755498277478748\n",
      "Average test loss: 0.004155388928121991\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05754370746678776\n",
      "Average test loss: 0.003999120950284932\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05734874753157298\n",
      "Average test loss: 0.004037103741119305\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05735921583573023\n",
      "Average test loss: 0.004048947134779559\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05732347505953577\n",
      "Average test loss: 0.004010505948215723\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05725641595655018\n",
      "Average test loss: 0.004046985894855526\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05723418346709675\n",
      "Average test loss: 0.004015873186704186\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05723743162883652\n",
      "Average test loss: 0.004039682727307081\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05716289177205827\n",
      "Average test loss: 0.00404893764687909\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05713552660081122\n",
      "Average test loss: 0.004027182558758391\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05708697252803378\n",
      "Average test loss: 0.004071713670260376\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05700601038005617\n",
      "Average test loss: 0.004060470910121997\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05701740650335948\n",
      "Average test loss: 0.004043449505749676\n",
      "Epoch 132/300\n",
      "Average training loss: 0.056962500366899704\n",
      "Average test loss: 0.004031395969291528\n",
      "Epoch 133/300\n",
      "Average training loss: 0.056938151823149784\n",
      "Average test loss: 0.004043981065766679\n",
      "Epoch 134/300\n",
      "Average training loss: 0.056916920519537395\n",
      "Average test loss: 0.004042109271718396\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05685624883572261\n",
      "Average test loss: 0.004124228972113795\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05680649267302619\n",
      "Average test loss: 0.004090692276755969\n",
      "Epoch 137/300\n",
      "Average training loss: 0.056811282641357846\n",
      "Average test loss: 0.004084836435814698\n",
      "Epoch 138/300\n",
      "Average training loss: 0.056722019536627664\n",
      "Average test loss: 0.0040697382709218395\n",
      "Epoch 139/300\n",
      "Average training loss: 0.056709451056189006\n",
      "Average test loss: 0.004038381792604923\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05670052866141002\n",
      "Average test loss: 0.004116786484089163\n",
      "Epoch 141/300\n",
      "Average training loss: 0.056630594140953484\n",
      "Average test loss: 0.004036765438401037\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05663171668516265\n",
      "Average test loss: 0.004071175363535682\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05664488489760293\n",
      "Average test loss: 0.004120478451251984\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05654367522067494\n",
      "Average test loss: 0.004038175723825892\n",
      "Epoch 145/300\n",
      "Average training loss: 0.056461076034439936\n",
      "Average test loss: 0.004065020336873002\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05627723453773392\n",
      "Average test loss: 0.004083586026810937\n",
      "Epoch 152/300\n",
      "Average training loss: 0.056259975814157065\n",
      "Average test loss: 0.004076853526135286\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05622944189442529\n",
      "Average test loss: 0.00418292751059764\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05615872926513354\n",
      "Average test loss: 0.0040977154125769934\n",
      "Epoch 155/300\n",
      "Average training loss: 0.056130777762995826\n",
      "Average test loss: 0.004055909566581249\n",
      "Epoch 156/300\n",
      "Average training loss: 0.056119280252191754\n",
      "Average test loss: 0.0041682349248892735\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05613234293791983\n",
      "Average test loss: 0.004123714137408469\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05605043746696578\n",
      "Average test loss: 0.004112437320666181\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05601694020628929\n",
      "Average test loss: 0.004099269619625476\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05598296820455127\n",
      "Average test loss: 0.004120600692513916\n",
      "Epoch 161/300\n",
      "Average training loss: 0.055964094516303804\n",
      "Average test loss: 0.004146241903098093\n",
      "Epoch 162/300\n",
      "Average training loss: 0.055877922157446545\n",
      "Average test loss: 0.004084024138955606\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05583910348680284\n",
      "Average test loss: 0.004099133656360209\n",
      "Epoch 164/300\n",
      "Average training loss: 0.055864847126934264\n",
      "Average test loss: 0.004115858443081379\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05580762793620427\n",
      "Average test loss: 0.004133231878901521\n",
      "Epoch 166/300\n",
      "Average training loss: 0.055775126523441736\n",
      "Average test loss: 0.004055549130671554\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05574563711219364\n",
      "Average test loss: 0.004164008932602074\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05570120693246523\n",
      "Average test loss: 0.004505860276106331\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05574671459197998\n",
      "Average test loss: 0.004104330391933521\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05560441785057386\n",
      "Average test loss: 0.0041179685749941405\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0556521499256293\n",
      "Average test loss: 0.004120550288094414\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05555814950995975\n",
      "Average test loss: 0.004105789451135529\n",
      "Epoch 173/300\n",
      "Average training loss: 0.055536374082167946\n",
      "Average test loss: 0.004127709977328777\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05548579095469581\n",
      "Average test loss: 0.0041448993388977316\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05543603445755111\n",
      "Average test loss: 0.004136959831333823\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05547426991992527\n",
      "Average test loss: 0.004203599022908343\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05535096158915096\n",
      "Average test loss: 0.004148740437709623\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05536502471400632\n",
      "Average test loss: 0.0041015907105886274\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05533125032981237\n",
      "Average test loss: 0.004109946747620901\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05535154530737135\n",
      "Average test loss: 0.004138562035643392\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05530987721681595\n",
      "Average test loss: 0.004150828686025408\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05523057957490285\n",
      "Average test loss: 0.004107901292335656\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05518430653545592\n",
      "Average test loss: 0.004195166550576687\n",
      "Epoch 184/300\n",
      "Average training loss: 0.055207521382305356\n",
      "Average test loss: 0.004177980440565282\n",
      "Epoch 185/300\n",
      "Average training loss: 0.055175068798992366\n",
      "Average test loss: 0.004245473875767655\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05512478502591451\n",
      "Average test loss: 0.004127909977402952\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05514446708228853\n",
      "Average test loss: 0.004098125637612409\n",
      "Epoch 188/300\n",
      "Average training loss: 0.055114342719316485\n",
      "Average test loss: 0.004091954690300756\n",
      "Epoch 189/300\n",
      "Average training loss: 0.055017174747255114\n",
      "Average test loss: 0.004123319070165356\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0549813927312692\n",
      "Average test loss: 0.004105387900024652\n",
      "Epoch 191/300\n",
      "Average training loss: 0.054951101104418434\n",
      "Average test loss: 0.004148090829037957\n",
      "Epoch 192/300\n",
      "Average training loss: 0.054983636720312964\n",
      "Average test loss: 0.004107109059890111\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05493065704239739\n",
      "Average test loss: 0.0041533517916169434\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0548866391380628\n",
      "Average test loss: 0.00412327266484499\n",
      "Epoch 195/300\n",
      "Average training loss: 0.054840578423606026\n",
      "Average test loss: 0.004134709370426006\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05489762948287858\n",
      "Average test loss: 0.00415672345749206\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05480046591824955\n",
      "Average test loss: 0.004132424392633968\n",
      "Epoch 198/300\n",
      "Average training loss: 0.054725302729341715\n",
      "Average test loss: 0.004170925320850478\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05465322091182073\n",
      "Average test loss: 0.004146569641100036\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05464728539850977\n",
      "Average test loss: 0.004134258293443256\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05464339700010088\n",
      "Average test loss: 0.00421899745427072\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05462785498632325\n",
      "Average test loss: 0.004164306675808297\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05457316672801971\n",
      "Average test loss: 0.0041624170814951265\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05447352852423986\n",
      "Average test loss: 0.004240784357405371\n",
      "Epoch 208/300\n",
      "Average training loss: 0.054585349430640535\n",
      "Average test loss: 0.004142302670205633\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05444146137436231\n",
      "Average test loss: 0.004145404200587008\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05449849296609561\n",
      "Average test loss: 0.0041027893568906515\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05444368794560432\n",
      "Average test loss: 0.0041739866060929165\n",
      "Epoch 212/300\n",
      "Average training loss: 0.054435038056638504\n",
      "Average test loss: 0.004165206170744366\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05438472134537167\n",
      "Average test loss: 0.004179165156351195\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05433545200030009\n",
      "Average test loss: 0.004180876999265618\n",
      "Epoch 215/300\n",
      "Average training loss: 0.054379072742329705\n",
      "Average test loss: 0.004228925533799661\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05429937673277325\n",
      "Average test loss: 0.004201161325805717\n",
      "Epoch 217/300\n",
      "Average training loss: 0.054237847404347526\n",
      "Average test loss: 0.004157929503669342\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05421110155185064\n",
      "Average test loss: 0.004183547734386391\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05421120474735896\n",
      "Average test loss: 0.004138914718810055\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05417791821228133\n",
      "Average test loss: 0.004234571918017334\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05420292058255937\n",
      "Average test loss: 0.004146349515765905\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05414203898111979\n",
      "Average test loss: 0.004134511254313919\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05405644459525744\n",
      "Average test loss: 0.004172252542028824\n",
      "Epoch 227/300\n",
      "Average training loss: 0.054009657131301035\n",
      "Average test loss: 0.004162885046667523\n",
      "Epoch 228/300\n",
      "Average training loss: 0.054051861180199515\n",
      "Average test loss: 0.004227552199115356\n",
      "Epoch 229/300\n",
      "Average training loss: 0.053953634907801945\n",
      "Average test loss: 0.004196111499849293\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05397942777143584\n",
      "Average test loss: 0.0041674851338482565\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05396967230240504\n",
      "Average test loss: 0.00418612706164519\n",
      "Epoch 232/300\n",
      "Average training loss: 0.053883983297480474\n",
      "Average test loss: 0.004347261649452977\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05392592938409911\n",
      "Average test loss: 0.00414890778851178\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05387874566515287\n",
      "Average test loss: 0.004142149713511269\n",
      "Epoch 235/300\n",
      "Average training loss: 0.053837795635064446\n",
      "Average test loss: 0.004197644405791329\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05380480599403381\n",
      "Average test loss: 0.004187406059768465\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05383629916773902\n",
      "Average test loss: 0.004219451926648617\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05378111967113283\n",
      "Average test loss: 0.004170205992129114\n",
      "Epoch 239/300\n",
      "Average training loss: 0.053737584306134116\n",
      "Average test loss: 0.004205060072657135\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0537506374253167\n",
      "Average test loss: 0.004222120562775268\n",
      "Epoch 241/300\n",
      "Average training loss: 0.053726248055696486\n",
      "Average test loss: 0.0042026292971438835\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05374129781126976\n",
      "Average test loss: 0.004242842427765329\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0536866490940253\n",
      "Average test loss: 0.004203857428497738\n",
      "Epoch 244/300\n",
      "Average training loss: 0.053654712090889616\n",
      "Average test loss: 0.004338044977850384\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05358690964844492\n",
      "Average test loss: 0.004232488996452756\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05356809642910958\n",
      "Average test loss: 0.004292023784170548\n",
      "Epoch 247/300\n",
      "Average training loss: 0.053653794272078406\n",
      "Average test loss: 0.004216109601366851\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05354292358623611\n",
      "Average test loss: 0.004232937939051125\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05354325427611669\n",
      "Average test loss: 0.004326962149184611\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05356016790535715\n",
      "Average test loss: 0.004188847535393304\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05360473842753304\n",
      "Average test loss: 0.004145037967711687\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05344190105464723\n",
      "Average test loss: 0.004190465330249734\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05344144457909796\n",
      "Average test loss: 0.0042708123632603226\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05346577107244068\n",
      "Average test loss: 0.00842603845314847\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05337674430012703\n",
      "Average test loss: 0.00434440309761299\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05331897795862622\n",
      "Average test loss: 0.004396181917852826\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0533930940495597\n",
      "Average test loss: 0.0042701902993851236\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05328760765327348\n",
      "Average test loss: 0.00424696775401632\n",
      "Epoch 262/300\n",
      "Average training loss: 0.053237342089414594\n",
      "Average test loss: 0.00417423660101162\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05327682955728637\n",
      "Average test loss: 0.004231423907809787\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05323012663258447\n",
      "Average test loss: 0.004232516521381007\n",
      "Epoch 265/300\n",
      "Average training loss: 0.053251237836149004\n",
      "Average test loss: 0.004249743808888727\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05318394355972608\n",
      "Average test loss: 0.004309098188247946\n",
      "Epoch 267/300\n",
      "Average training loss: 0.053173073437478804\n",
      "Average test loss: 0.0041953215005083215\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05319171452853415\n",
      "Average test loss: 0.004265086422363917\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05319833549526003\n",
      "Average test loss: 0.004364661038335826\n",
      "Epoch 270/300\n",
      "Average training loss: 0.053140864663653906\n",
      "Average test loss: 0.0042696466582516825\n",
      "Epoch 271/300\n",
      "Average training loss: 0.053108695199092226\n",
      "Average test loss: 0.004177850012563997\n",
      "Epoch 272/300\n",
      "Average training loss: 0.053100548654794695\n",
      "Average test loss: 0.004271227871378263\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05309523082441754\n",
      "Average test loss: 0.004282895529849662\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05307775633202659\n",
      "Average test loss: 0.004184616920227806\n",
      "Epoch 275/300\n",
      "Average training loss: 0.053019308927986356\n",
      "Average test loss: 0.004299275948355595\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05310778452621566\n",
      "Average test loss: 0.004241421619844105\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05294135836760203\n",
      "Average test loss: 0.004210027210000488\n",
      "Epoch 278/300\n",
      "Average training loss: 0.052988199750582374\n",
      "Average test loss: 0.004284623427937428\n",
      "Epoch 279/300\n",
      "Average training loss: 0.052949929744005204\n",
      "Average test loss: 0.004280487728615602\n",
      "Epoch 280/300\n",
      "Average training loss: 0.052998315665456985\n",
      "Average test loss: 0.0042482570289737644\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05297168730033769\n",
      "Average test loss: 0.004267279836038748\n",
      "Epoch 282/300\n",
      "Average training loss: 0.052928708099656635\n",
      "Average test loss: 0.004236758414242003\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05294690266913838\n",
      "Average test loss: 0.004320853184494707\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05285822347137663\n",
      "Average test loss: 0.004261136080241866\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05290801841351721\n",
      "Average test loss: 0.004301574531528685\n",
      "Epoch 286/300\n",
      "Average training loss: 0.052896075950728524\n",
      "Average test loss: 0.004241146258181996\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0528578068183528\n",
      "Average test loss: 0.004274201326486137\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0528029164340761\n",
      "Average test loss: 0.004167814819763104\n",
      "Epoch 289/300\n",
      "Average training loss: 0.052749325772126515\n",
      "Average test loss: 0.004227181399861972\n",
      "Epoch 290/300\n",
      "Average training loss: 0.052801667398876616\n",
      "Average test loss: 0.004185172963887453\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05280307420757082\n",
      "Average test loss: 0.004245583122803105\n",
      "Epoch 292/300\n",
      "Average training loss: 0.052795598031746015\n",
      "Average test loss: 0.004281852443599039\n",
      "Epoch 293/300\n",
      "Average training loss: 0.052738309605254066\n",
      "Average test loss: 0.0043730005810244214\n",
      "Epoch 294/300\n",
      "Average training loss: 0.052752721326218714\n",
      "Average test loss: 0.0042515595298674375\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05268421816163593\n",
      "Average test loss: 0.004220647934410307\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0527190425031715\n",
      "Average test loss: 0.004255819071912103\n",
      "Epoch 297/300\n",
      "Average training loss: 0.052696688410308626\n",
      "Average test loss: 0.00430469983122829\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05273061346345478\n",
      "Average test loss: 0.0043053048075073295\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05260867505272229\n",
      "Average test loss: 0.00434783149510622\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05263106034199397\n",
      "Average test loss: 0.004322562751463718\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7556928395827611\n",
      "Average test loss: 0.005401549513555235\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0980924537314309\n",
      "Average test loss: 0.004696319704047508\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07911790755391121\n",
      "Average test loss: 0.004498863954097033\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07195686453580856\n",
      "Average test loss: 0.004292634461075068\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06772589536507924\n",
      "Average test loss: 0.004190434830263257\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0649489913781484\n",
      "Average test loss: 0.00411427266150713\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0628913755748007\n",
      "Average test loss: 0.004096627771854401\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06131060990360048\n",
      "Average test loss: 0.003913682571301857\n",
      "Epoch 9/300\n",
      "Average training loss: 0.060007977700895734\n",
      "Average test loss: 0.003889382290136483\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05886369453205003\n",
      "Average test loss: 0.003802229620102379\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05786676120095783\n",
      "Average test loss: 0.003753715094592836\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05697353061702516\n",
      "Average test loss: 0.0036898874276214176\n",
      "Epoch 13/300\n",
      "Average training loss: 0.056202531645695365\n",
      "Average test loss: 0.003655701780070861\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05549741366505623\n",
      "Average test loss: 0.0036186986337933273\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05486553272273805\n",
      "Average test loss: 0.0036261507583161194\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05418259854449166\n",
      "Average test loss: 0.0035713501810613604\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05367762115266588\n",
      "Average test loss: 0.0034863555762502884\n",
      "Epoch 18/300\n",
      "Average training loss: 0.053245050681961906\n",
      "Average test loss: 0.0034691480276071362\n",
      "Epoch 19/300\n",
      "Average training loss: 0.052724341683917576\n",
      "Average test loss: 0.003476750978579124\n",
      "Epoch 20/300\n",
      "Average training loss: 0.052327063467767505\n",
      "Average test loss: 0.003404507629780306\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05198578613996506\n",
      "Average test loss: 0.0034108363435500196\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05160874723229143\n",
      "Average test loss: 0.0034397849833799734\n",
      "Epoch 23/300\n",
      "Average training loss: 0.051235142320394514\n",
      "Average test loss: 0.0033565038070082665\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05092919279800521\n",
      "Average test loss: 0.0033581228939195476\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05066340866684914\n",
      "Average test loss: 0.0033233919005013174\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05038683441778024\n",
      "Average test loss: 0.0033558628720541796\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05004711196488804\n",
      "Average test loss: 0.003266702397416035\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04982603942685657\n",
      "Average test loss: 0.0032807592884120016\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0495928259326352\n",
      "Average test loss: 0.0032856116897116107\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04941264507174492\n",
      "Average test loss: 0.0032526147827092143\n",
      "Epoch 31/300\n",
      "Average training loss: 0.049167315310902064\n",
      "Average test loss: 0.00329761020363205\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04900593990087509\n",
      "Average test loss: 0.0032161847804155616\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04879408559203148\n",
      "Average test loss: 0.003220517733030849\n",
      "Epoch 34/300\n",
      "Average training loss: 0.048617681281434164\n",
      "Average test loss: 0.003197004344728258\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04845358849896325\n",
      "Average test loss: 0.0032056730731079975\n",
      "Epoch 36/300\n",
      "Average training loss: 0.048273442424005934\n",
      "Average test loss: 0.0032148716946442924\n",
      "Epoch 37/300\n",
      "Average training loss: 0.048132384184334014\n",
      "Average test loss: 0.0031680441945791246\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04800401985314157\n",
      "Average test loss: 0.0031851607708053456\n",
      "Epoch 39/300\n",
      "Average training loss: 0.047892197920216456\n",
      "Average test loss: 0.003173362417767445\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04772925401065085\n",
      "Average test loss: 0.0031739598334663446\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04760636950863732\n",
      "Average test loss: 0.003172790595640739\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04753192981084188\n",
      "Average test loss: 0.0031470105070620777\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04739222776889801\n",
      "Average test loss: 0.003140320291949643\n",
      "Epoch 44/300\n",
      "Average training loss: 0.047285180181264874\n",
      "Average test loss: 0.003174990076157782\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04716325130065282\n",
      "Average test loss: 0.0031344143133610488\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04704183144701852\n",
      "Average test loss: 0.003128741840728455\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04696277348531617\n",
      "Average test loss: 0.003138265450588531\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04685925638344553\n",
      "Average test loss: 0.0031214386050899823\n",
      "Epoch 49/300\n",
      "Average training loss: 0.046774908655219605\n",
      "Average test loss: 0.003113831288077765\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04666632928450902\n",
      "Average test loss: 0.003104254508597983\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04658423364493582\n",
      "Average test loss: 0.0031041024227937064\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0465101122011741\n",
      "Average test loss: 0.0031322604258441264\n",
      "Epoch 53/300\n",
      "Average training loss: 0.046416424324115114\n",
      "Average test loss: 0.0031047352488256163\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0463581361439493\n",
      "Average test loss: 0.003093637384577758\n",
      "Epoch 55/300\n",
      "Average training loss: 0.046244915458891124\n",
      "Average test loss: 0.003154167792863316\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04618977631794082\n",
      "Average test loss: 0.003104018321674731\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04606528683172332\n",
      "Average test loss: 0.0031217926827569803\n",
      "Epoch 58/300\n",
      "Average training loss: 0.046042142644524574\n",
      "Average test loss: 0.003105951177784138\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04594115122490459\n",
      "Average test loss: 0.003103452546728982\n",
      "Epoch 60/300\n",
      "Average training loss: 0.045861023273732926\n",
      "Average test loss: 0.0031260903291404248\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04578023707535532\n",
      "Average test loss: 0.0030887681163019602\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04571783739659521\n",
      "Average test loss: 0.0030892307572066785\n",
      "Epoch 63/300\n",
      "Average training loss: 0.045631211366918355\n",
      "Average test loss: 0.0031281813765979475\n",
      "Epoch 64/300\n",
      "Average training loss: 0.045564630346165765\n",
      "Average test loss: 0.0030900159391264123\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04547791102859709\n",
      "Average test loss: 0.003076583530132969\n",
      "Epoch 66/300\n",
      "Average training loss: 0.045378019799788796\n",
      "Average test loss: 0.003071081283605761\n",
      "Epoch 67/300\n",
      "Average training loss: 0.045354333692126804\n",
      "Average test loss: 0.0030988194031847847\n",
      "Epoch 68/300\n",
      "Average training loss: 0.045296918673647775\n",
      "Average test loss: 0.0031227571556551588\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04522643885016441\n",
      "Average test loss: 0.003096434214255876\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04517513813243972\n",
      "Average test loss: 0.0030967343902836243\n",
      "Epoch 71/300\n",
      "Average training loss: 0.045056102567248874\n",
      "Average test loss: 0.003093837584265404\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04501893594529894\n",
      "Average test loss: 0.0030838004228555494\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04498091620206833\n",
      "Average test loss: 0.003081231873689426\n",
      "Epoch 74/300\n",
      "Average training loss: 0.044876609538992245\n",
      "Average test loss: 0.003086868003838592\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04478881605797344\n",
      "Average test loss: 0.0030809676740318538\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04478672460714976\n",
      "Average test loss: 0.0031120902732428577\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04471169023381339\n",
      "Average test loss: 0.0030855223019089963\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04462297180626128\n",
      "Average test loss: 0.003100029875938263\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04457572905057006\n",
      "Average test loss: 0.0031157045805205903\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0445364713801278\n",
      "Average test loss: 0.003110351374165879\n",
      "Epoch 81/300\n",
      "Average training loss: 0.044424227812223965\n",
      "Average test loss: 0.0030808076216942734\n",
      "Epoch 82/300\n",
      "Average training loss: 0.044444148401419324\n",
      "Average test loss: 0.0030921125908692677\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04432020210888651\n",
      "Average test loss: 0.0030943889065335193\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04427149213022656\n",
      "Average test loss: 0.003062808796349499\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04418296969268057\n",
      "Average test loss: 0.0030906654538379774\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04410458696550793\n",
      "Average test loss: 0.0030849082318858967\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04412896936138471\n",
      "Average test loss: 0.0030750584097372162\n",
      "Epoch 88/300\n",
      "Average training loss: 0.044026385423209934\n",
      "Average test loss: 0.003081653474105729\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04396152722173267\n",
      "Average test loss: 0.0030970947607937786\n",
      "Epoch 90/300\n",
      "Average training loss: 0.043902789917257094\n",
      "Average test loss: 0.0030776914612700543\n",
      "Epoch 91/300\n",
      "Average training loss: 0.043901890857352154\n",
      "Average test loss: 0.0030905332114133568\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04381411124931441\n",
      "Average test loss: 0.00313613810390234\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04375365041030778\n",
      "Average test loss: 0.003069033453447951\n",
      "Epoch 94/300\n",
      "Average training loss: 0.043691569119691846\n",
      "Average test loss: 0.003159952582584487\n",
      "Epoch 95/300\n",
      "Average training loss: 0.043626123004489474\n",
      "Average test loss: 0.0031118260230869055\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04359497810072369\n",
      "Average test loss: 0.0030752235288835235\n",
      "Epoch 97/300\n",
      "Average training loss: 0.043531753016842735\n",
      "Average test loss: 0.0031207925652464233\n",
      "Epoch 98/300\n",
      "Average training loss: 0.043462864107555815\n",
      "Average test loss: 0.003094269714835617\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04340088380707635\n",
      "Average test loss: 0.003085742328109013\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04335914218425751\n",
      "Average test loss: 0.003085008066975408\n",
      "Epoch 101/300\n",
      "Average training loss: 0.043305867357386486\n",
      "Average test loss: 0.0030838950930370227\n",
      "Epoch 102/300\n",
      "Average training loss: 0.043236987892124386\n",
      "Average test loss: 0.0031022630306995575\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04317169449064467\n",
      "Average test loss: 0.0030759750503218837\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04312939524319437\n",
      "Average test loss: 0.0030983420444859397\n",
      "Epoch 105/300\n",
      "Average training loss: 0.043144715441597835\n",
      "Average test loss: 0.0031186003784338633\n",
      "Epoch 106/300\n",
      "Average training loss: 0.043067476133505506\n",
      "Average test loss: 0.003128695690797435\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04297589619954427\n",
      "Average test loss: 0.003101981887800826\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04290852588746283\n",
      "Average test loss: 0.003118877795421415\n",
      "Epoch 109/300\n",
      "Average training loss: 0.042889724181758036\n",
      "Average test loss: 0.0031643944612393775\n",
      "Epoch 110/300\n",
      "Average training loss: 0.042866788234975604\n",
      "Average test loss: 0.003095060701171557\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04277943851881557\n",
      "Average test loss: 0.003236210154990355\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04273729777004984\n",
      "Average test loss: 0.003126306071670519\n",
      "Epoch 113/300\n",
      "Average training loss: 0.042709742557671335\n",
      "Average test loss: 0.003119932123356395\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04268088110619121\n",
      "Average test loss: 0.0031139229287703834\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04262358749243948\n",
      "Average test loss: 0.003093046339849631\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04253149473667145\n",
      "Average test loss: 0.003128293116473489\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04249564956128597\n",
      "Average test loss: 0.003186314373794529\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04244225850039058\n",
      "Average test loss: 0.0031284585744142533\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04243312364154392\n",
      "Average test loss: 0.0031158021775384744\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04241781431602107\n",
      "Average test loss: 0.0031344090660827026\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04232865937219726\n",
      "Average test loss: 0.003197742232845889\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04227037161257532\n",
      "Average test loss: 0.0031320128976884817\n",
      "Epoch 123/300\n",
      "Average training loss: 0.042219341923793154\n",
      "Average test loss: 0.003199405264523294\n",
      "Epoch 124/300\n",
      "Average training loss: 0.042203933589988286\n",
      "Average test loss: 0.0031311583921520246\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04215090505282084\n",
      "Average test loss: 0.0032389724751313528\n",
      "Epoch 126/300\n",
      "Average training loss: 0.042121433215008844\n",
      "Average test loss: 0.003135966150297059\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04205954687131776\n",
      "Average test loss: 0.0031238775158094036\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04203945167859395\n",
      "Average test loss: 0.0031596929240557885\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04199918741981189\n",
      "Average test loss: 0.003132190292080243\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04194044681058989\n",
      "Average test loss: 0.0031160089259760246\n",
      "Epoch 131/300\n",
      "Average training loss: 0.041923963824907935\n",
      "Average test loss: 0.003154703647726112\n",
      "Epoch 132/300\n",
      "Average training loss: 0.041862943927447\n",
      "Average test loss: 0.003127891690366798\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04183390435576439\n",
      "Average test loss: 0.0032175458154330653\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04178813566433059\n",
      "Average test loss: 0.0031398626265840396\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04176254619492425\n",
      "Average test loss: 0.003133092429695858\n",
      "Epoch 136/300\n",
      "Average training loss: 0.041683164187603525\n",
      "Average test loss: 0.003148493064670927\n",
      "Epoch 137/300\n",
      "Average training loss: 0.041678722119993634\n",
      "Average test loss: 0.0031538740562068093\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0416113232837783\n",
      "Average test loss: 0.003170267504743404\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04159262842105495\n",
      "Average test loss: 0.0032063300607519016\n",
      "Epoch 140/300\n",
      "Average training loss: 0.041549479772647224\n",
      "Average test loss: 0.0031393418750829166\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04152351929081811\n",
      "Average test loss: 0.003117217118334439\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04147026091482904\n",
      "Average test loss: 0.0032159148938953877\n",
      "Epoch 143/300\n",
      "Average training loss: 0.041445545305808386\n",
      "Average test loss: 0.003187199701037672\n",
      "Epoch 144/300\n",
      "Average training loss: 0.041392217189073564\n",
      "Average test loss: 0.0033233576984041265\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04137291114860111\n",
      "Average test loss: 0.003218483689137631\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04133114727338155\n",
      "Average test loss: 0.0031705981995910405\n",
      "Epoch 147/300\n",
      "Average training loss: 0.041329788371920584\n",
      "Average test loss: 0.0032688228275833857\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04127789834472868\n",
      "Average test loss: 0.0031527031221323543\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0412359618710147\n",
      "Average test loss: 0.003136527669512563\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04116196113328139\n",
      "Average test loss: 0.003209062616030375\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04114079910847876\n",
      "Average test loss: 0.0032426448760347233\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04113986113667488\n",
      "Average test loss: 0.0031467694669134088\n",
      "Epoch 153/300\n",
      "Average training loss: 0.041090737985240086\n",
      "Average test loss: 0.0032331955811629692\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04107047828866376\n",
      "Average test loss: 0.0031724366537398763\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04104472494456503\n",
      "Average test loss: 0.0031648311633616687\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0410370936161942\n",
      "Average test loss: 0.0032575326566067005\n",
      "Epoch 157/300\n",
      "Average training loss: 0.040968608922428554\n",
      "Average test loss: 0.0032589630815717907\n",
      "Epoch 158/300\n",
      "Average training loss: 0.040927432063553065\n",
      "Average test loss: 0.00317870677386721\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04096423603759872\n",
      "Average test loss: 0.0031777090109470817\n",
      "Epoch 160/300\n",
      "Average training loss: 0.040862173696359\n",
      "Average test loss: 0.003193073865233196\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04086469721794128\n",
      "Average test loss: 0.003203418704577618\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04084433458248774\n",
      "Average test loss: 0.003227255666007598\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04076798092822234\n",
      "Average test loss: 0.003227904125013285\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04079806081453959\n",
      "Average test loss: 0.0031846635792818333\n",
      "Epoch 165/300\n",
      "Average training loss: 0.040730240619844864\n",
      "Average test loss: 0.0032147995572951106\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04070491306649314\n",
      "Average test loss: 0.0032103459727432994\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04068668267461989\n",
      "Average test loss: 0.003187154268225034\n",
      "Epoch 168/300\n",
      "Average training loss: 0.040639906817012364\n",
      "Average test loss: 0.003272676508873701\n",
      "Epoch 169/300\n",
      "Average training loss: 0.040633769325084154\n",
      "Average test loss: 0.0032346398604826796\n",
      "Epoch 170/300\n",
      "Average training loss: 0.040620734300878314\n",
      "Average test loss: 0.003217822675489717\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04052235645386908\n",
      "Average test loss: 0.003170898903161287\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04048459811343087\n",
      "Average test loss: 0.003223466694768932\n",
      "Epoch 173/300\n",
      "Average training loss: 0.040518239673640995\n",
      "Average test loss: 0.003205755594703886\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04047505801916122\n",
      "Average test loss: 0.003186541615674893\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04045878259672059\n",
      "Average test loss: 0.00318272088550859\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04040762115187115\n",
      "Average test loss: 0.003236862256295151\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04042991521292263\n",
      "Average test loss: 0.0032018646229472424\n",
      "Epoch 178/300\n",
      "Average training loss: 0.040365238782432346\n",
      "Average test loss: 0.0031747769460909897\n",
      "Epoch 179/300\n",
      "Average training loss: 0.040365564826462\n",
      "Average test loss: 0.0032251183587229913\n",
      "Epoch 180/300\n",
      "Average training loss: 0.040334306369225185\n",
      "Average test loss: 0.003215852982468075\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0403099943863021\n",
      "Average test loss: 0.003252725649625063\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04027387649483151\n",
      "Average test loss: 0.003173868656779329\n",
      "Epoch 183/300\n",
      "Average training loss: 0.040212399287356274\n",
      "Average test loss: 0.0032118428146673574\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04029784972965717\n",
      "Average test loss: 0.0032049094583425255\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04021143932309416\n",
      "Average test loss: 0.003217807005263037\n",
      "Epoch 186/300\n",
      "Average training loss: 0.040167728904220795\n",
      "Average test loss: 0.0032228155196126966\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04013927891353766\n",
      "Average test loss: 0.00337811380583379\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0401356281903055\n",
      "Average test loss: 0.003192212036293414\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04011736119786898\n",
      "Average test loss: 0.003231768087380462\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04006353171500895\n",
      "Average test loss: 0.003194759205397632\n",
      "Epoch 191/300\n",
      "Average training loss: 0.040025329649448396\n",
      "Average test loss: 0.003239272032967872\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04004911516441239\n",
      "Average test loss: 0.00325887436626686\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03999235406849119\n",
      "Average test loss: 0.003252796898285548\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04000558291872342\n",
      "Average test loss: 0.003238143005097906\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03998584486047427\n",
      "Average test loss: 0.0032470478746626113\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03993909678194258\n",
      "Average test loss: 0.003220476820237107\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03993163289626439\n",
      "Average test loss: 0.0032554770782589914\n",
      "Epoch 198/300\n",
      "Average training loss: 0.039917390561766096\n",
      "Average test loss: 0.003221533457852072\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03989650008413527\n",
      "Average test loss: 0.0032490716110914948\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03981278731425603\n",
      "Average test loss: 0.0032262956992619566\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03989306037624677\n",
      "Average test loss: 0.0032535115000274445\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03983675479888916\n",
      "Average test loss: 0.003209023226156003\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03978532935182254\n",
      "Average test loss: 0.0032437456684807934\n",
      "Epoch 204/300\n",
      "Average training loss: 0.039724466608630285\n",
      "Average test loss: 0.0032369163185358048\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03975289596451653\n",
      "Average test loss: 0.0032917546549191077\n",
      "Epoch 206/300\n",
      "Average training loss: 0.039724512070417405\n",
      "Average test loss: 0.003262676252052188\n",
      "Epoch 207/300\n",
      "Average training loss: 0.039712791283925374\n",
      "Average test loss: 0.0032458669017586443\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03966056145230929\n",
      "Average test loss: 0.0032061218791123894\n",
      "Epoch 209/300\n",
      "Average training loss: 0.039699416296349634\n",
      "Average test loss: 0.0032383398664080436\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03968940679894553\n",
      "Average test loss: 0.0032471647831714815\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03964793473978837\n",
      "Average test loss: 0.003255641284295254\n",
      "Epoch 212/300\n",
      "Average training loss: 0.039720638904306625\n",
      "Average test loss: 0.0032288447088665434\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03980976003739569\n",
      "Average test loss: 0.003182308817903201\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03951069243086709\n",
      "Average test loss: 0.00326267965303527\n",
      "Epoch 215/300\n",
      "Average training loss: 0.039557335360182654\n",
      "Average test loss: 0.003278621198816432\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03956217866142591\n",
      "Average test loss: 0.0034424806545592015\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03956201141410404\n",
      "Average test loss: 0.0032868534440381658\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0394976628224055\n",
      "Average test loss: 0.0032147116565869912\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03945408643947707\n",
      "Average test loss: 0.003221783219733172\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03946608499354786\n",
      "Average test loss: 0.003203987841390901\n",
      "Epoch 221/300\n",
      "Average training loss: 0.039438505482342505\n",
      "Average test loss: 0.0033002611752599477\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03942520312302642\n",
      "Average test loss: 0.0032450035562117895\n",
      "Epoch 223/300\n",
      "Average training loss: 0.039408253646559185\n",
      "Average test loss: 0.0035435904411392077\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03937623667385843\n",
      "Average test loss: 0.00345046645651261\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03942514371871948\n",
      "Average test loss: 0.003290064312517643\n",
      "Epoch 226/300\n",
      "Average training loss: 0.039375394738382766\n",
      "Average test loss: 0.003374746659149726\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03941443255212572\n",
      "Average test loss: 0.00324681453489595\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03930419653322961\n",
      "Average test loss: 0.003206910096729795\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03938826851877901\n",
      "Average test loss: 0.0035342742606169647\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03928761660721567\n",
      "Average test loss: 0.0032260573988573417\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03926465920938386\n",
      "Average test loss: 0.0032343961687551605\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0392364712787999\n",
      "Average test loss: 0.0032214560370064445\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03926294886238045\n",
      "Average test loss: 0.0033179829622515373\n",
      "Epoch 234/300\n",
      "Average training loss: 0.039287504227624996\n",
      "Average test loss: 0.0033039875212642882\n",
      "Epoch 235/300\n",
      "Average training loss: 0.039235504964987435\n",
      "Average test loss: 0.00328372589374582\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03921510561638408\n",
      "Average test loss: 0.0032829111435761053\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03918607054485215\n",
      "Average test loss: 0.0032062482490307757\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0392226834528976\n",
      "Average test loss: 0.003259490690918432\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03915546167559094\n",
      "Average test loss: 0.0032821039536760914\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03912721085217264\n",
      "Average test loss: 0.0032580328447123367\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03909781954023573\n",
      "Average test loss: 0.003276979158942898\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03913950859175788\n",
      "Average test loss: 0.0032544915535383753\n",
      "Epoch 243/300\n",
      "Average training loss: 0.039109571470154654\n",
      "Average test loss: 0.0032471780557599334\n",
      "Epoch 244/300\n",
      "Average training loss: 0.039044585787587696\n",
      "Average test loss: 0.0033512555596729118\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03907397010922432\n",
      "Average test loss: 0.003212371068696181\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03903363609645102\n",
      "Average test loss: 0.0033274000878963208\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03900028464860386\n",
      "Average test loss: 0.003258518835115764\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03904345628288057\n",
      "Average test loss: 0.0032145867159383166\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03893739345504178\n",
      "Average test loss: 0.003368533210001058\n",
      "Epoch 250/300\n",
      "Average training loss: 0.038979274668627314\n",
      "Average test loss: 0.003238790531332294\n",
      "Epoch 251/300\n",
      "Average training loss: 0.038977175341712106\n",
      "Average test loss: 0.0032418110033290252\n",
      "Epoch 252/300\n",
      "Average training loss: 0.038942114257150225\n",
      "Average test loss: 0.0032447978361613223\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03894559430744913\n",
      "Average test loss: 0.003385126190673974\n",
      "Epoch 254/300\n",
      "Average training loss: 0.038932616823249395\n",
      "Average test loss: 0.0031968805115256045\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03888180704580413\n",
      "Average test loss: 0.0032299456790917448\n",
      "Epoch 256/300\n",
      "Average training loss: 0.038881206524040965\n",
      "Average test loss: 0.003219904922362831\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03887429337700208\n",
      "Average test loss: 0.003255522184073925\n",
      "Epoch 258/300\n",
      "Average training loss: 0.038905407610866756\n",
      "Average test loss: 0.003273405036371615\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03881921908259392\n",
      "Average test loss: 0.0033113165468805364\n",
      "Epoch 260/300\n",
      "Average training loss: 0.038827236437135274\n",
      "Average test loss: 0.0032838923533757528\n",
      "Epoch 261/300\n",
      "Average training loss: 0.038847696893744996\n",
      "Average test loss: 0.0032807471601085532\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03879988042347961\n",
      "Average test loss: 0.003362428094777796\n",
      "Epoch 263/300\n",
      "Average training loss: 0.038819737484057745\n",
      "Average test loss: 0.003272356253531244\n",
      "Epoch 264/300\n",
      "Average training loss: 0.038779033405913245\n",
      "Average test loss: 0.003319900899918543\n",
      "Epoch 265/300\n",
      "Average training loss: 0.038789503206809364\n",
      "Average test loss: 0.0033175882155903513\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03876379322508971\n",
      "Average test loss: 0.0034268076793394155\n",
      "Epoch 267/300\n",
      "Average training loss: 0.038708379732237924\n",
      "Average test loss: 0.003264749318982164\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03875714726746082\n",
      "Average test loss: 0.0032797005573908487\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0387311170829667\n",
      "Average test loss: 0.003308431196336945\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03872957929637697\n",
      "Average test loss: 0.0032633600530938968\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03866868717802895\n",
      "Average test loss: 0.003266211752883262\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03866203317377302\n",
      "Average test loss: 0.0032717766076740293\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03864144473936822\n",
      "Average test loss: 0.0032914214885483187\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03862860377132893\n",
      "Average test loss: 0.003334021843762861\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03863478048145771\n",
      "Average test loss: 0.003328911051568058\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03857948898275693\n",
      "Average test loss: 0.0033524974688059752\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03866803665955861\n",
      "Average test loss: 0.0032687217585949434\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0386394078930219\n",
      "Average test loss: 0.003299851988752683\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03859952069653405\n",
      "Average test loss: 0.0033076777040130564\n",
      "Epoch 280/300\n",
      "Average training loss: 0.038582980105446446\n",
      "Average test loss: 0.003305790029466152\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03857204533947839\n",
      "Average test loss: 0.003309234172105789\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03856682897607486\n",
      "Average test loss: 0.0032841738561789195\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03850357430842188\n",
      "Average test loss: 0.0032551521921737325\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03858386156625218\n",
      "Average test loss: 0.0032771307066496876\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03854417280024952\n",
      "Average test loss: 0.003270160005738338\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03851535648769803\n",
      "Average test loss: 0.003532901666023665\n",
      "Epoch 287/300\n",
      "Average training loss: 0.038556082487106326\n",
      "Average test loss: 0.0033578862545804843\n",
      "Epoch 288/300\n",
      "Average training loss: 0.038509133832322226\n",
      "Average test loss: 0.0033008973991705314\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03842749917507172\n",
      "Average test loss: 0.003293455954641104\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03847935214969847\n",
      "Average test loss: 0.0032722458692474496\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03845576367113325\n",
      "Average test loss: 0.0033100162513761055\n",
      "Epoch 292/300\n",
      "Average training loss: 0.038439977063073054\n",
      "Average test loss: 0.0033649489171802996\n",
      "Epoch 293/300\n",
      "Average training loss: 0.038438614302211335\n",
      "Average test loss: 0.0033077457927995256\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03841896245545811\n",
      "Average test loss: 0.003247114446428087\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03837569432788425\n",
      "Average test loss: 0.003332238176630603\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03846010072363747\n",
      "Average test loss: 0.0032996102256907358\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03846519113249249\n",
      "Average test loss: 0.003344545279112127\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03840420592493481\n",
      "Average test loss: 0.0033127046310239367\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03834340017702845\n",
      "Average test loss: 0.0033697069057573876\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03836080416540305\n",
      "Average test loss: 0.0032867236772759095\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7316983779138989\n",
      "Average test loss: 0.005018729978965388\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09235060422950321\n",
      "Average test loss: 0.0042612257446679805\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07288646464215384\n",
      "Average test loss: 0.004145985761450396\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06506307151582506\n",
      "Average test loss: 0.0037633425912095442\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0604314377506574\n",
      "Average test loss: 0.0037742649552722773\n",
      "Epoch 6/300\n",
      "Average training loss: 0.057318108350038526\n",
      "Average test loss: 0.0035282220269242924\n",
      "Epoch 7/300\n",
      "Average training loss: 0.054984116524457935\n",
      "Average test loss: 0.0035472270148909756\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05311147726409965\n",
      "Average test loss: 0.0034067088971949286\n",
      "Epoch 9/300\n",
      "Average training loss: 0.051591475311252806\n",
      "Average test loss: 0.0032616454381495714\n",
      "Epoch 10/300\n",
      "Average training loss: 0.050192722996075946\n",
      "Average test loss: 0.0031798252363999685\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04892287947071923\n",
      "Average test loss: 0.0031016593819691075\n",
      "Epoch 12/300\n",
      "Average training loss: 0.047828447742594614\n",
      "Average test loss: 0.0031534580847041474\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04686821219490634\n",
      "Average test loss: 0.003038939167641931\n",
      "Epoch 14/300\n",
      "Average training loss: 0.045930336465438204\n",
      "Average test loss: 0.0029063312939057746\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04512006569902102\n",
      "Average test loss: 0.0028560682011561263\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04432638119326698\n",
      "Average test loss: 0.0028173975841038756\n",
      "Epoch 17/300\n",
      "Average training loss: 0.043627370577719475\n",
      "Average test loss: 0.0028223717564509975\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04304039900832706\n",
      "Average test loss: 0.0027849633871681163\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04244589473141564\n",
      "Average test loss: 0.002709442794116007\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04200301725665728\n",
      "Average test loss: 0.0026442922260612248\n",
      "Epoch 21/300\n",
      "Average training loss: 0.041490263733598924\n",
      "Average test loss: 0.002631043634066979\n",
      "Epoch 22/300\n",
      "Average training loss: 0.041023348096344205\n",
      "Average test loss: 0.0026128009516331887\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04065853483478228\n",
      "Average test loss: 0.002604805001264645\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0403191024404433\n",
      "Average test loss: 0.0025802310030493473\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03996137108074294\n",
      "Average test loss: 0.002552842524109615\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03979616200758351\n",
      "Average test loss: 0.0025560351045181354\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03940946278141604\n",
      "Average test loss: 0.0025178751941356396\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03915365363823043\n",
      "Average test loss: 0.00251788406405184\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0389250597887569\n",
      "Average test loss: 0.002491030415209631\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0387030757036474\n",
      "Average test loss: 0.0025351733999947708\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0385737637910578\n",
      "Average test loss: 0.002541871311143041\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03837166779571109\n",
      "Average test loss: 0.0024745110352006223\n",
      "Epoch 33/300\n",
      "Average training loss: 0.038136649363570745\n",
      "Average test loss: 0.002476562056897415\n",
      "Epoch 34/300\n",
      "Average training loss: 0.037954213460286455\n",
      "Average test loss: 0.002448088875454333\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03781051739719179\n",
      "Average test loss: 0.0024511020754774413\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0376823453389936\n",
      "Average test loss: 0.0024420784850501353\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03748516155448225\n",
      "Average test loss: 0.0024120767013066343\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03738786381814215\n",
      "Average test loss: 0.002412575157566203\n",
      "Epoch 39/300\n",
      "Average training loss: 0.037249133798811174\n",
      "Average test loss: 0.0024368968388686577\n",
      "Epoch 40/300\n",
      "Average training loss: 0.037145120796230106\n",
      "Average test loss: 0.0024453913730879623\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03704195637173123\n",
      "Average test loss: 0.002393884631494681\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03690091713931826\n",
      "Average test loss: 0.0024105799816962746\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03677684645354748\n",
      "Average test loss: 0.0024305721410653658\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03670520218544536\n",
      "Average test loss: 0.002402917341846559\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03659453118013011\n",
      "Average test loss: 0.0024033912256773977\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03648282649781969\n",
      "Average test loss: 0.002359769329118232\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03639903954333729\n",
      "Average test loss: 0.0023910994584568674\n",
      "Epoch 48/300\n",
      "Average training loss: 0.036289323439200716\n",
      "Average test loss: 0.0023633121334844165\n",
      "Epoch 49/300\n",
      "Average training loss: 0.036172145812047855\n",
      "Average test loss: 0.0023712240242295796\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03610537048512035\n",
      "Average test loss: 0.0023638797977732287\n",
      "Epoch 51/300\n",
      "Average training loss: 0.036070260299576655\n",
      "Average test loss: 0.0023567464455134337\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03589537293877867\n",
      "Average test loss: 0.0023945822295629317\n",
      "Epoch 53/300\n",
      "Average training loss: 0.035891425606277255\n",
      "Average test loss: 0.00236371994846397\n",
      "Epoch 54/300\n",
      "Average training loss: 0.035789156085915035\n",
      "Average test loss: 0.0023459106898970072\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0357272369629807\n",
      "Average test loss: 0.0023463395852595568\n",
      "Epoch 56/300\n",
      "Average training loss: 0.035641597911715506\n",
      "Average test loss: 0.00236649007577863\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03557812217209074\n",
      "Average test loss: 0.0023475575850655634\n",
      "Epoch 58/300\n",
      "Average training loss: 0.035447521939873695\n",
      "Average test loss: 0.0023396063819527625\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03539155064357652\n",
      "Average test loss: 0.0023453228790313004\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03537461386786567\n",
      "Average test loss: 0.0023436032192160686\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03526563268237644\n",
      "Average test loss: 0.0023403777624997828\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03523958418104384\n",
      "Average test loss: 0.002338916301313374\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03513620143135389\n",
      "Average test loss: 0.002345133203599188\n",
      "Epoch 64/300\n",
      "Average training loss: 0.035071334371964136\n",
      "Average test loss: 0.0023376270066946744\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03502574321627617\n",
      "Average test loss: 0.0023346985958309636\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03493332093457381\n",
      "Average test loss: 0.0023287834243641958\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03494373302989536\n",
      "Average test loss: 0.002329171411279175\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03483735009034475\n",
      "Average test loss: 0.0023375040404498577\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03476857579913404\n",
      "Average test loss: 0.0023301969376496143\n",
      "Epoch 70/300\n",
      "Average training loss: 0.034728788293070266\n",
      "Average test loss: 0.0023136642165482043\n",
      "Epoch 71/300\n",
      "Average training loss: 0.034672595211201244\n",
      "Average test loss: 0.0023253712869352764\n",
      "Epoch 72/300\n",
      "Average training loss: 0.034572574105527665\n",
      "Average test loss: 0.0023255258448835877\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03452363607949681\n",
      "Average test loss: 0.002332493614198433\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03444961892565091\n",
      "Average test loss: 0.0023698728604035243\n",
      "Epoch 75/300\n",
      "Average training loss: 0.034405556082725525\n",
      "Average test loss: 0.002320916046284967\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03438137405779627\n",
      "Average test loss: 0.002328178603822986\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03433339651591248\n",
      "Average test loss: 0.0023706145040276977\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03424675742288431\n",
      "Average test loss: 0.0023373211651212637\n",
      "Epoch 79/300\n",
      "Average training loss: 0.034146816917591624\n",
      "Average test loss: 0.0023370719965961242\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03418352439337306\n",
      "Average test loss: 0.0023209595864431727\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03408351209263007\n",
      "Average test loss: 0.0023302806235022014\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03401978201336331\n",
      "Average test loss: 0.0023639640231720277\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03402007801334063\n",
      "Average test loss: 0.0023229717831644747\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03394132894939846\n",
      "Average test loss: 0.002330216216130389\n",
      "Epoch 85/300\n",
      "Average training loss: 0.033923624550302824\n",
      "Average test loss: 0.002392128011625674\n",
      "Epoch 86/300\n",
      "Average training loss: 0.033819956014553704\n",
      "Average test loss: 0.0023467265869387324\n",
      "Epoch 87/300\n",
      "Average training loss: 0.033749791501296894\n",
      "Average test loss: 0.0023363098802251948\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03372380286620723\n",
      "Average test loss: 0.0023294180461929906\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03361960961421331\n",
      "Average test loss: 0.002323270638576812\n",
      "Epoch 90/300\n",
      "Average training loss: 0.033621031555864545\n",
      "Average test loss: 0.002328944001139866\n",
      "Epoch 91/300\n",
      "Average training loss: 0.033605428765217465\n",
      "Average test loss: 0.002323877919672264\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03354036005006896\n",
      "Average test loss: 0.002331604679011636\n",
      "Epoch 93/300\n",
      "Average training loss: 0.033480563630660376\n",
      "Average test loss: 0.002345587479778462\n",
      "Epoch 94/300\n",
      "Average training loss: 0.033485364496707914\n",
      "Average test loss: 0.002328014457391368\n",
      "Epoch 95/300\n",
      "Average training loss: 0.033390333308113945\n",
      "Average test loss: 0.0023339615751885707\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0333522828022639\n",
      "Average test loss: 0.0023578635671486457\n",
      "Epoch 97/300\n",
      "Average training loss: 0.033286995795038014\n",
      "Average test loss: 0.002324472225064205\n",
      "Epoch 98/300\n",
      "Average training loss: 0.033240545362234114\n",
      "Average test loss: 0.0023687855017681917\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03320559247831503\n",
      "Average test loss: 0.0023322113340513576\n",
      "Epoch 100/300\n",
      "Average training loss: 0.033198065001103616\n",
      "Average test loss: 0.0023491551661863922\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03312270871632629\n",
      "Average test loss: 0.002344831117946241\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03304591225584348\n",
      "Average test loss: 0.0023603327240174017\n",
      "Epoch 103/300\n",
      "Average training loss: 0.033064067547520004\n",
      "Average test loss: 0.002345794582946433\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03298168287674586\n",
      "Average test loss: 0.002334914908226993\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03295704886317253\n",
      "Average test loss: 0.0023691538596111866\n",
      "Epoch 106/300\n",
      "Average training loss: 0.032912922269768184\n",
      "Average test loss: 0.0023666883290020956\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03286157330870628\n",
      "Average test loss: 0.002365684683331185\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03280064345399539\n",
      "Average test loss: 0.002367409322721263\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0327712696161535\n",
      "Average test loss: 0.0023409850247618227\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03275233922070927\n",
      "Average test loss: 0.0023494085340450208\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03267856910493639\n",
      "Average test loss: 0.0023295272648748426\n",
      "Epoch 112/300\n",
      "Average training loss: 0.032659362574418385\n",
      "Average test loss: 0.00234242320785092\n",
      "Epoch 113/300\n",
      "Average training loss: 0.032612293680508934\n",
      "Average test loss: 0.0023558164499700067\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03256349464919832\n",
      "Average test loss: 0.002334453510741393\n",
      "Epoch 115/300\n",
      "Average training loss: 0.032535500953594844\n",
      "Average test loss: 0.0023315296994729172\n",
      "Epoch 116/300\n",
      "Average training loss: 0.032498492118385104\n",
      "Average test loss: 0.002358716893113322\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03245687588055929\n",
      "Average test loss: 0.002352142066591316\n",
      "Epoch 118/300\n",
      "Average training loss: 0.032409770622849464\n",
      "Average test loss: 0.002334520158254438\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03237540695567926\n",
      "Average test loss: 0.0023488040773404968\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032375055836306676\n",
      "Average test loss: 0.002384609023316039\n",
      "Epoch 121/300\n",
      "Average training loss: 0.032282269928190443\n",
      "Average test loss: 0.0023694013235055736\n",
      "Epoch 122/300\n",
      "Average training loss: 0.032292633313271736\n",
      "Average test loss: 0.0023704135846346615\n",
      "Epoch 123/300\n",
      "Average training loss: 0.032252186722225615\n",
      "Average test loss: 0.002349615496686763\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03218855564130677\n",
      "Average test loss: 0.002388863259098596\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03218659299777614\n",
      "Average test loss: 0.002368606793176797\n",
      "Epoch 126/300\n",
      "Average training loss: 0.032116329189803866\n",
      "Average test loss: 0.0023717125385171837\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03207146222558287\n",
      "Average test loss: 0.002369120453070435\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03204723880026076\n",
      "Average test loss: 0.0023697607558634546\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03199228994382752\n",
      "Average test loss: 0.0024049342907965184\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03197373936076959\n",
      "Average test loss: 0.0024818993856509527\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031955953785114816\n",
      "Average test loss: 0.002409012119803164\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03193559498091539\n",
      "Average test loss: 0.002384222708952924\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03190193267497751\n",
      "Average test loss: 0.002383416218890084\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03189223924941487\n",
      "Average test loss: 0.002353467351446549\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03181064076887237\n",
      "Average test loss: 0.002476284357201722\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031775525035129655\n",
      "Average test loss: 0.002377322830673721\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03174128914872805\n",
      "Average test loss: 0.002373550587437219\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0317016873492135\n",
      "Average test loss: 0.0023800254077133206\n",
      "Epoch 139/300\n",
      "Average training loss: 0.031670327881971994\n",
      "Average test loss: 0.002398453207893504\n",
      "Epoch 140/300\n",
      "Average training loss: 0.031688461310333677\n",
      "Average test loss: 0.0023666712902486323\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03165551805330647\n",
      "Average test loss: 0.002435483869165182\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03160048640436596\n",
      "Average test loss: 0.0023521533619819414\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03155214897129271\n",
      "Average test loss: 0.0023674336850850117\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03153195804523097\n",
      "Average test loss: 0.002363332445733249\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03149497461650107\n",
      "Average test loss: 0.0023898959217800034\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03147816602885723\n",
      "Average test loss: 0.0023949454695814188\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03139172604348924\n",
      "Average test loss: 0.0023654947560280562\n",
      "Epoch 148/300\n",
      "Average training loss: 0.031417878746986386\n",
      "Average test loss: 0.0023757194831139513\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03136563585201899\n",
      "Average test loss: 0.002370451314581765\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031369260821077556\n",
      "Average test loss: 0.0024007267941617305\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03130484385788441\n",
      "Average test loss: 0.0023957528916911945\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03128933903078238\n",
      "Average test loss: 0.0023642994308223325\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03128154289722443\n",
      "Average test loss: 0.0023954722562597856\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031290082448058656\n",
      "Average test loss: 0.0024126222216420703\n",
      "Epoch 155/300\n",
      "Average training loss: 0.031254341853989495\n",
      "Average test loss: 0.0023986620584295856\n",
      "Epoch 156/300\n",
      "Average training loss: 0.031202366918325425\n",
      "Average test loss: 0.0023687741442893944\n",
      "Epoch 157/300\n",
      "Average training loss: 0.031177739727828238\n",
      "Average test loss: 0.002395601399656799\n",
      "Epoch 158/300\n",
      "Average training loss: 0.031130831682019765\n",
      "Average test loss: 0.002402851880217592\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03110300609966119\n",
      "Average test loss: 0.002405586424801085\n",
      "Epoch 160/300\n",
      "Average training loss: 0.031085762331883114\n",
      "Average test loss: 0.002380286713027292\n",
      "Epoch 161/300\n",
      "Average training loss: 0.031072865585486095\n",
      "Average test loss: 0.002410820787358615\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03105463955965307\n",
      "Average test loss: 0.0024003118471139008\n",
      "Epoch 163/300\n",
      "Average training loss: 0.031042807438307337\n",
      "Average test loss: 0.0023861645691924623\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030991276275780466\n",
      "Average test loss: 0.002378283691695995\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030975694570276473\n",
      "Average test loss: 0.00239799596555531\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03094027510119809\n",
      "Average test loss: 0.00245600840387245\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030921353605058457\n",
      "Average test loss: 0.0024499341955201495\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030879861215750377\n",
      "Average test loss: 0.002378446711641219\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03087882998585701\n",
      "Average test loss: 0.0023742777736236653\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03085294239885277\n",
      "Average test loss: 0.0024245989231599704\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030964251639114485\n",
      "Average test loss: 0.0024848315889636676\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03086003276043468\n",
      "Average test loss: 0.002435282245174878\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030747684005233977\n",
      "Average test loss: 0.0024006788122157255\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030744751792814995\n",
      "Average test loss: 0.0024166094205445715\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03076702528860834\n",
      "Average test loss: 0.002413399155769083\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030727194844020738\n",
      "Average test loss: 0.002430859701294038\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030720307565397686\n",
      "Average test loss: 0.0024226811269505155\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030698449924588204\n",
      "Average test loss: 0.0024379953796871833\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030652125340369013\n",
      "Average test loss: 0.0025077571891662146\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030611878428194256\n",
      "Average test loss: 0.002473321779113677\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03061163060201539\n",
      "Average test loss: 0.002466346424486902\n",
      "Epoch 182/300\n",
      "Average training loss: 0.030552891353766123\n",
      "Average test loss: 0.0024329273756593466\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03070029445985953\n",
      "Average test loss: 0.0023990195675028694\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030585359222359127\n",
      "Average test loss: 0.0023886625698457164\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03053913502064016\n",
      "Average test loss: 0.002410106799668736\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03049757845534219\n",
      "Average test loss: 0.002391051395278838\n",
      "Epoch 187/300\n",
      "Average training loss: 0.030469659601648647\n",
      "Average test loss: 0.0024704398210677837\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030577863067388536\n",
      "Average test loss: 0.002430824698880315\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030456613934702344\n",
      "Average test loss: 0.0024335430901911525\n",
      "Epoch 190/300\n",
      "Average training loss: 0.030434618655178283\n",
      "Average test loss: 0.002388657528079218\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030409840350349743\n",
      "Average test loss: 0.0024621082904438176\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030409118212759496\n",
      "Average test loss: 0.0024624297767877577\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0303772943980164\n",
      "Average test loss: 0.00242099115687112\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03038262609806326\n",
      "Average test loss: 0.002447514057159424\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03042617413898309\n",
      "Average test loss: 0.0024330184461755886\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030281952043374378\n",
      "Average test loss: 0.0024343729820102454\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030293407198455597\n",
      "Average test loss: 0.0024040278502636485\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03024588640199767\n",
      "Average test loss: 0.002438062367029488\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030254412121242948\n",
      "Average test loss: 0.0024155070677192676\n",
      "Epoch 200/300\n",
      "Average training loss: 0.030240891910261577\n",
      "Average test loss: 0.002429773898381326\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030299573454591965\n",
      "Average test loss: 0.002488385229474968\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0302667442874776\n",
      "Average test loss: 0.0024125193890391127\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030203910743196807\n",
      "Average test loss: 0.0024658059374325805\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03017309728761514\n",
      "Average test loss: 0.0024533841382298207\n",
      "Epoch 205/300\n",
      "Average training loss: 0.030162547864847712\n",
      "Average test loss: 0.0024267356884148388\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030125384486383862\n",
      "Average test loss: 0.0024119449131604696\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030140071481466295\n",
      "Average test loss: 0.002464047403385242\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030119473511974016\n",
      "Average test loss: 0.002435763164009485\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030126208985845247\n",
      "Average test loss: 0.002420020893216133\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03013027530411879\n",
      "Average test loss: 0.002437555343326595\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03009393267167939\n",
      "Average test loss: 0.002451690462935302\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030039615102940134\n",
      "Average test loss: 0.0024159152480877108\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03005093417233891\n",
      "Average test loss: 0.0024782951929503017\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030031651553180484\n",
      "Average test loss: 0.0024283479808105365\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03000984835293558\n",
      "Average test loss: 0.002477985680517223\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030047990350259676\n",
      "Average test loss: 0.0024520001568728023\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030012570023536682\n",
      "Average test loss: 0.0024324405131240685\n",
      "Epoch 218/300\n",
      "Average training loss: 0.029971030761798224\n",
      "Average test loss: 0.0024743471292571887\n",
      "Epoch 219/300\n",
      "Average training loss: 0.029935272827744483\n",
      "Average test loss: 0.00242847303683973\n",
      "Epoch 220/300\n",
      "Average training loss: 0.029958256027764744\n",
      "Average test loss: 0.0024633864899062446\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029996712629993757\n",
      "Average test loss: 0.002454717216288878\n",
      "Epoch 222/300\n",
      "Average training loss: 0.029944819127519926\n",
      "Average test loss: 0.0024383853802250493\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02990292628440592\n",
      "Average test loss: 0.0024724787461260955\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029923876985907556\n",
      "Average test loss: 0.002453969533244769\n",
      "Epoch 225/300\n",
      "Average training loss: 0.029861773673031066\n",
      "Average test loss: 0.002758845469603936\n",
      "Epoch 226/300\n",
      "Average training loss: 0.029914899642268816\n",
      "Average test loss: 0.0024803525706132254\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02986097385817104\n",
      "Average test loss: 0.002445544364551703\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02986089688539505\n",
      "Average test loss: 0.0024406630962880123\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029788638290431765\n",
      "Average test loss: 0.0025265187834286025\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029810202701224222\n",
      "Average test loss: 0.00247804823372927\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029784478086564276\n",
      "Average test loss: 0.0024378614177306495\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029755454992254574\n",
      "Average test loss: 0.0024769946007678905\n",
      "Epoch 233/300\n",
      "Average training loss: 0.029773338188727697\n",
      "Average test loss: 0.0024496595222089027\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029770686272117826\n",
      "Average test loss: 0.002476722613390949\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029723643196953666\n",
      "Average test loss: 0.002458133353334334\n",
      "Epoch 236/300\n",
      "Average training loss: 0.029771477356553076\n",
      "Average test loss: 0.002560938584100869\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02971291060414579\n",
      "Average test loss: 0.0024325250346834463\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02968845086130831\n",
      "Average test loss: 0.0024810398897777003\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029715686135821873\n",
      "Average test loss: 0.002570428058091137\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029664784216218525\n",
      "Average test loss: 0.0025008380764370993\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029692074111766286\n",
      "Average test loss: 0.0024514972012903953\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02965384349061383\n",
      "Average test loss: 0.002463755066299604\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02966561200718085\n",
      "Average test loss: 0.002407426744699478\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029590024145113098\n",
      "Average test loss: 0.0024577994770887826\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029638764914539126\n",
      "Average test loss: 0.0024501499481913115\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02961205936471621\n",
      "Average test loss: 0.0024296590137398905\n",
      "Epoch 247/300\n",
      "Average training loss: 0.029566008844309384\n",
      "Average test loss: 0.002461769942401184\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02952947788271639\n",
      "Average test loss: 0.002429179100630184\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029594916303952536\n",
      "Average test loss: 0.0024606227593289482\n",
      "Epoch 250/300\n",
      "Average training loss: 0.029537923902273177\n",
      "Average test loss: 0.0024893849870810908\n",
      "Epoch 251/300\n",
      "Average training loss: 0.029564497341712317\n",
      "Average test loss: 0.0024762225072416993\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029510035584370296\n",
      "Average test loss: 0.0026151104552878273\n",
      "Epoch 253/300\n",
      "Average training loss: 0.029530896968311734\n",
      "Average test loss: 0.0024827824564029773\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02949042848414845\n",
      "Average test loss: 0.00252265417875929\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02950440347691377\n",
      "Average test loss: 0.0024766705605304903\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02949207882086436\n",
      "Average test loss: 0.002484046880259282\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029454428194297683\n",
      "Average test loss: 0.0024707476079670917\n",
      "Epoch 258/300\n",
      "Average training loss: 0.029444920003414154\n",
      "Average test loss: 0.002444028483910693\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02947442451781697\n",
      "Average test loss: 0.0024352587763633994\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029406100251608426\n",
      "Average test loss: 0.002482110749930143\n",
      "Epoch 261/300\n",
      "Average training loss: 0.029406391211681895\n",
      "Average test loss: 0.002473223854891128\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02946431266930368\n",
      "Average test loss: 0.002422474194318056\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02940900821818246\n",
      "Average test loss: 0.002457664467394352\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029409411018093426\n",
      "Average test loss: 0.002483064427350958\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02938548801508215\n",
      "Average test loss: 0.0025058666848474078\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02941695048577256\n",
      "Average test loss: 0.0024026416138642364\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02932725872264968\n",
      "Average test loss: 0.0024904623847040864\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029357046461767622\n",
      "Average test loss: 0.002457354567738043\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02935181362595823\n",
      "Average test loss: 0.002538232777681616\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029356591963105732\n",
      "Average test loss: 0.002474207267475625\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029350285877784092\n",
      "Average test loss: 0.002511172259433402\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029337450115217104\n",
      "Average test loss: 0.0024598168841459683\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02929874361058076\n",
      "Average test loss: 0.002507492889339725\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029353416833612655\n",
      "Average test loss: 0.002474388214863009\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029262779146432875\n",
      "Average test loss: 0.0024507326568580334\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02930644331706895\n",
      "Average test loss: 0.002542964505756067\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029272023091713586\n",
      "Average test loss: 0.00252450091826419\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029284877204232746\n",
      "Average test loss: 0.0025230322300146026\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029289299498001735\n",
      "Average test loss: 0.002502717801059286\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029234837704234653\n",
      "Average test loss: 0.002718800324532721\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029239367653926214\n",
      "Average test loss: 0.002487841928584708\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02922718700269858\n",
      "Average test loss: 0.0024843373635990754\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029229210691319573\n",
      "Average test loss: 0.0024716790864864986\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029202674710088306\n",
      "Average test loss: 0.0024807366625302367\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0292025545189778\n",
      "Average test loss: 0.0024634735511822833\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02920501941939195\n",
      "Average test loss: 0.002561215231815974\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029177243065502907\n",
      "Average test loss: 0.0024743701331317427\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029185298694504633\n",
      "Average test loss: 0.0024617217866082985\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02915073868466748\n",
      "Average test loss: 0.0024649330768734216\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029158813224898446\n",
      "Average test loss: 0.0025123531266839968\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02912158045834965\n",
      "Average test loss: 0.0025039826375949713\n",
      "Epoch 292/300\n",
      "Average training loss: 0.029123903203341697\n",
      "Average test loss: 0.002510194093402889\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02915836489200592\n",
      "Average test loss: 0.0025247707246906226\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029123681583338314\n",
      "Average test loss: 0.002575041689806514\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029087251908249324\n",
      "Average test loss: 0.0024681306245426336\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029113592573338086\n",
      "Average test loss: 0.0024585867050207324\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02905055300394694\n",
      "Average test loss: 0.002478817411077519\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02907633282740911\n",
      "Average test loss: 0.002475672988221049\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02906322076751126\n",
      "Average test loss: 0.0024492674879729747\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029042959402004877\n",
      "Average test loss: 0.00249572063361605\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.6890235216153993\n",
      "Average test loss: 0.004601847757895788\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08895391036404504\n",
      "Average test loss: 0.003782054958244165\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06544513386156824\n",
      "Average test loss: 0.0034636588705082736\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05673568442132738\n",
      "Average test loss: 0.0035863668198386827\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05168416498767005\n",
      "Average test loss: 0.003127404871914122\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04843323913547728\n",
      "Average test loss: 0.0029512383768128024\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04604426390926043\n",
      "Average test loss: 0.0028820711721976597\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04412779405713081\n",
      "Average test loss: 0.002833437058246798\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04254725259211328\n",
      "Average test loss: 0.002701911392725176\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04104991122086843\n",
      "Average test loss: 0.0026002684368027582\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03971424295504888\n",
      "Average test loss: 0.0026184431196500857\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03855803030232589\n",
      "Average test loss: 0.0024528435410724747\n",
      "Epoch 13/300\n",
      "Average training loss: 0.037421789821651244\n",
      "Average test loss: 0.002399679948989716\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0364130463997523\n",
      "Average test loss: 0.002278464156513413\n",
      "Epoch 15/300\n",
      "Average training loss: 0.035490066501829357\n",
      "Average test loss: 0.002235294153292974\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03464596909284592\n",
      "Average test loss: 0.0021748677137204343\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03390863195392821\n",
      "Average test loss: 0.0021655377327568\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03325531771116787\n",
      "Average test loss: 0.002083051189366314\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03265677864683999\n",
      "Average test loss: 0.002047533333508505\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03220541996591621\n",
      "Average test loss: 0.0020122797376372747\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03180856942302651\n",
      "Average test loss: 0.001999843453988433\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03142601130074925\n",
      "Average test loss: 0.0019741755856408014\n",
      "Epoch 23/300\n",
      "Average training loss: 0.031082016590568754\n",
      "Average test loss: 0.0019468332827091218\n",
      "Epoch 24/300\n",
      "Average training loss: 0.030744662140806515\n",
      "Average test loss: 0.0019375358317047357\n",
      "Epoch 25/300\n",
      "Average training loss: 0.030485299083921644\n",
      "Average test loss: 0.001968505660899811\n",
      "Epoch 26/300\n",
      "Average training loss: 0.030200049304299884\n",
      "Average test loss: 0.0019110990298084087\n",
      "Epoch 27/300\n",
      "Average training loss: 0.029976990956399177\n",
      "Average test loss: 0.0019280149508267641\n",
      "Epoch 28/300\n",
      "Average training loss: 0.029834273230698375\n",
      "Average test loss: 0.0018808859328014983\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02959430131647322\n",
      "Average test loss: 0.0018632337778496245\n",
      "Epoch 30/300\n",
      "Average training loss: 0.029403837636113168\n",
      "Average test loss: 0.0018635926071761382\n",
      "Epoch 31/300\n",
      "Average training loss: 0.029223998319771554\n",
      "Average test loss: 0.0018547256499942806\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02905722788969676\n",
      "Average test loss: 0.0018534929648869568\n",
      "Epoch 33/300\n",
      "Average training loss: 0.028904274331198798\n",
      "Average test loss: 0.0018442973368283774\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02877089217222399\n",
      "Average test loss: 0.001839047911318226\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02860816149579154\n",
      "Average test loss: 0.001834775883704424\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02852321167290211\n",
      "Average test loss: 0.0018315672596606115\n",
      "Epoch 37/300\n",
      "Average training loss: 0.028387784077061547\n",
      "Average test loss: 0.0018127401216576496\n",
      "Epoch 38/300\n",
      "Average training loss: 0.028268592965271736\n",
      "Average test loss: 0.0018057726851354043\n",
      "Epoch 39/300\n",
      "Average training loss: 0.028128114188710848\n",
      "Average test loss: 0.0018063189778476953\n",
      "Epoch 40/300\n",
      "Average training loss: 0.028098329855336085\n",
      "Average test loss: 0.001802319317435225\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0279239737805393\n",
      "Average test loss: 0.0017988247750844392\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02786343983312448\n",
      "Average test loss: 0.0017832854518459902\n",
      "Epoch 43/300\n",
      "Average training loss: 0.027774811435076926\n",
      "Average test loss: 0.001786153408802218\n",
      "Epoch 44/300\n",
      "Average training loss: 0.027739671260118483\n",
      "Average test loss: 0.0017757864324375987\n",
      "Epoch 45/300\n",
      "Average training loss: 0.027598065071635775\n",
      "Average test loss: 0.0017751293387264014\n",
      "Epoch 46/300\n",
      "Average training loss: 0.027503777912921374\n",
      "Average test loss: 0.0017697420459654597\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02743007855448458\n",
      "Average test loss: 0.001790131463876201\n",
      "Epoch 48/300\n",
      "Average training loss: 0.027344085708260538\n",
      "Average test loss: 0.0017624636023408837\n",
      "Epoch 49/300\n",
      "Average training loss: 0.027287827301356527\n",
      "Average test loss: 0.001771604891659485\n",
      "Epoch 50/300\n",
      "Average training loss: 0.027244974239004984\n",
      "Average test loss: 0.0017661449518054724\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02715111587776078\n",
      "Average test loss: 0.0017568417570243279\n",
      "Epoch 52/300\n",
      "Average training loss: 0.027112816284100213\n",
      "Average test loss: 0.0017722259492923816\n",
      "Epoch 53/300\n",
      "Average training loss: 0.027027734185258546\n",
      "Average test loss: 0.0017551985972871383\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02699412097202407\n",
      "Average test loss: 0.0017709295532355705\n",
      "Epoch 55/300\n",
      "Average training loss: 0.026917091415988074\n",
      "Average test loss: 0.001743780906829569\n",
      "Epoch 56/300\n",
      "Average training loss: 0.026859693742460673\n",
      "Average test loss: 0.001747971066377229\n",
      "Epoch 57/300\n",
      "Average training loss: 0.026769690399368603\n",
      "Average test loss: 0.001751427942679988\n",
      "Epoch 58/300\n",
      "Average training loss: 0.026709576013187568\n",
      "Average test loss: 0.0017401410268826616\n",
      "Epoch 59/300\n",
      "Average training loss: 0.026672985707720122\n",
      "Average test loss: 0.0017967267567291856\n",
      "Epoch 60/300\n",
      "Average training loss: 0.026667416863971286\n",
      "Average test loss: 0.0017346988843960894\n",
      "Epoch 61/300\n",
      "Average training loss: 0.026579200628730985\n",
      "Average test loss: 0.001749638211602966\n",
      "Epoch 62/300\n",
      "Average training loss: 0.026489134268628225\n",
      "Average test loss: 0.0017349290721532372\n",
      "Epoch 63/300\n",
      "Average training loss: 0.026473147831029362\n",
      "Average test loss: 0.0017497936762455436\n",
      "Epoch 64/300\n",
      "Average training loss: 0.026449487431181803\n",
      "Average test loss: 0.0017332820107953417\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02636779529021846\n",
      "Average test loss: 0.001744995620722572\n",
      "Epoch 66/300\n",
      "Average training loss: 0.026309284643994436\n",
      "Average test loss: 0.0017675498879204195\n",
      "Epoch 67/300\n",
      "Average training loss: 0.026271870164407626\n",
      "Average test loss: 0.0017466348163369628\n",
      "Epoch 68/300\n",
      "Average training loss: 0.026232063750425975\n",
      "Average test loss: 0.00172977841210862\n",
      "Epoch 69/300\n",
      "Average training loss: 0.026163307675056986\n",
      "Average test loss: 0.0017339054201212195\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02612725053065353\n",
      "Average test loss: 0.0017320034303185013\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02605421594944265\n",
      "Average test loss: 0.0017277051754709747\n",
      "Epoch 72/300\n",
      "Average training loss: 0.026038281194037862\n",
      "Average test loss: 0.001729194906540215\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02597572393053108\n",
      "Average test loss: 0.0017244645837280485\n",
      "Epoch 74/300\n",
      "Average training loss: 0.025908463913533422\n",
      "Average test loss: 0.0017411252450611856\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02590783187084728\n",
      "Average test loss: 0.0017278084704238506\n",
      "Epoch 76/300\n",
      "Average training loss: 0.025830148743258584\n",
      "Average test loss: 0.0017854599507732523\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02580189074575901\n",
      "Average test loss: 0.001731262933048937\n",
      "Epoch 78/300\n",
      "Average training loss: 0.025750712941090267\n",
      "Average test loss: 0.0017362054987914031\n",
      "Epoch 79/300\n",
      "Average training loss: 0.025697765979501935\n",
      "Average test loss: 0.001727681598212156\n",
      "Epoch 80/300\n",
      "Average training loss: 0.025666169045699967\n",
      "Average test loss: 0.0017449260559967822\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02564140863219897\n",
      "Average test loss: 0.0017324412777605983\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025613852525750797\n",
      "Average test loss: 0.0017445546144412623\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025553199210100703\n",
      "Average test loss: 0.0017371946620858378\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02551009233792623\n",
      "Average test loss: 0.0017313466404254238\n",
      "Epoch 85/300\n",
      "Average training loss: 0.025463612879316012\n",
      "Average test loss: 0.0017266698928756845\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0254412659621901\n",
      "Average test loss: 0.0017187000060867933\n",
      "Epoch 87/300\n",
      "Average training loss: 0.025423254859116343\n",
      "Average test loss: 0.0017221749253157111\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025371033474802972\n",
      "Average test loss: 0.0017329088161802955\n",
      "Epoch 89/300\n",
      "Average training loss: 0.025324474894338183\n",
      "Average test loss: 0.0017348274563749632\n",
      "Epoch 90/300\n",
      "Average training loss: 0.025278692808416155\n",
      "Average test loss: 0.0017309920829203394\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02525495594408777\n",
      "Average test loss: 0.0017170091534240378\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0252087023175425\n",
      "Average test loss: 0.001726656349789765\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02517666605446074\n",
      "Average test loss: 0.001736947325265242\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025150825937589008\n",
      "Average test loss: 0.001734732653428283\n",
      "Epoch 95/300\n",
      "Average training loss: 0.025083380012048614\n",
      "Average test loss: 0.0017442052600284417\n",
      "Epoch 96/300\n",
      "Average training loss: 0.025099597776929536\n",
      "Average test loss: 0.0017334857787936926\n",
      "Epoch 97/300\n",
      "Average training loss: 0.025030815756983227\n",
      "Average test loss: 0.0017893219230075677\n",
      "Epoch 98/300\n",
      "Average training loss: 0.025008102276259\n",
      "Average test loss: 0.0017305646760182249\n",
      "Epoch 99/300\n",
      "Average training loss: 0.024977134890026516\n",
      "Average test loss: 0.0017390479046023553\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02496815416879124\n",
      "Average test loss: 0.0017532764763260882\n",
      "Epoch 101/300\n",
      "Average training loss: 0.024892648049526744\n",
      "Average test loss: 0.0017395757045596837\n",
      "Epoch 102/300\n",
      "Average training loss: 0.024858832132485176\n",
      "Average test loss: 0.0017272626554800405\n",
      "Epoch 103/300\n",
      "Average training loss: 0.024818440539969337\n",
      "Average test loss: 0.0017345764608018928\n",
      "Epoch 104/300\n",
      "Average training loss: 0.024808216384715503\n",
      "Average test loss: 0.0017332274713036087\n",
      "Epoch 105/300\n",
      "Average training loss: 0.024751350498861735\n",
      "Average test loss: 0.001732662548414535\n",
      "Epoch 106/300\n",
      "Average training loss: 0.024708231934242778\n",
      "Average test loss: 0.0017442042163780167\n",
      "Epoch 107/300\n",
      "Average training loss: 0.024700012306372324\n",
      "Average test loss: 0.001744538169975082\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024662462330526776\n",
      "Average test loss: 0.0017654387443843814\n",
      "Epoch 109/300\n",
      "Average training loss: 0.024640123118956882\n",
      "Average test loss: 0.0017368096736156279\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02464376031772958\n",
      "Average test loss: 0.0017489727047375508\n",
      "Epoch 111/300\n",
      "Average training loss: 0.024598793682124878\n",
      "Average test loss: 0.001757174646274911\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02457244955499967\n",
      "Average test loss: 0.0017333575196357236\n",
      "Epoch 113/300\n",
      "Average training loss: 0.024530589953064917\n",
      "Average test loss: 0.0017430164955763354\n",
      "Epoch 114/300\n",
      "Average training loss: 0.024502716915475\n",
      "Average test loss: 0.001752976047496001\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02445150426361296\n",
      "Average test loss: 0.0017383444245076842\n",
      "Epoch 116/300\n",
      "Average training loss: 0.024424913139806853\n",
      "Average test loss: 0.0017383916055162749\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02439384935796261\n",
      "Average test loss: 0.001777780298764507\n",
      "Epoch 118/300\n",
      "Average training loss: 0.024384649286667507\n",
      "Average test loss: 0.0017598209898504946\n",
      "Epoch 119/300\n",
      "Average training loss: 0.024350007785691154\n",
      "Average test loss: 0.0017587090232926937\n",
      "Epoch 120/300\n",
      "Average training loss: 0.024305427718493675\n",
      "Average test loss: 0.0017339791793169247\n",
      "Epoch 121/300\n",
      "Average training loss: 0.024271939996216033\n",
      "Average test loss: 0.0017500736389516127\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02426694205403328\n",
      "Average test loss: 0.0017351403799321916\n",
      "Epoch 123/300\n",
      "Average training loss: 0.024227921365035904\n",
      "Average test loss: 0.0017430829796940088\n",
      "Epoch 124/300\n",
      "Average training loss: 0.024190301799111896\n",
      "Average test loss: 0.0017659775345689722\n",
      "Epoch 125/300\n",
      "Average training loss: 0.024179754795299636\n",
      "Average test loss: 0.0017447428296630582\n",
      "Epoch 126/300\n",
      "Average training loss: 0.024137749847438602\n",
      "Average test loss: 0.0017507667158626848\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024120681105388536\n",
      "Average test loss: 0.001745795784621603\n",
      "Epoch 128/300\n",
      "Average training loss: 0.024111227308710417\n",
      "Average test loss: 0.0018275905717164277\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02411050142844518\n",
      "Average test loss: 0.0017484600649525723\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024106731823749013\n",
      "Average test loss: 0.001789237396998538\n",
      "Epoch 131/300\n",
      "Average training loss: 0.024002534361349213\n",
      "Average test loss: 0.0017442067641143997\n",
      "Epoch 132/300\n",
      "Average training loss: 0.023997138985329205\n",
      "Average test loss: 0.0017508708834648133\n",
      "Epoch 133/300\n",
      "Average training loss: 0.023968474820256234\n",
      "Average test loss: 0.0017584889072717893\n",
      "Epoch 134/300\n",
      "Average training loss: 0.023944381033380827\n",
      "Average test loss: 0.0017402765990959274\n",
      "Epoch 135/300\n",
      "Average training loss: 0.023924045624832312\n",
      "Average test loss: 0.0017439575685809056\n",
      "Epoch 136/300\n",
      "Average training loss: 0.023891598795851073\n",
      "Average test loss: 0.0017536624703142379\n",
      "Epoch 137/300\n",
      "Average training loss: 0.023859406714638074\n",
      "Average test loss: 0.0017541237498323124\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02383605504863792\n",
      "Average test loss: 0.0017662709748579397\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02388632874025239\n",
      "Average test loss: 0.0017610472772891324\n",
      "Epoch 140/300\n",
      "Average training loss: 0.023788694095280435\n",
      "Average test loss: 0.0017682734084212117\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02379063467350271\n",
      "Average test loss: 0.0017659848727699782\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0237581197478705\n",
      "Average test loss: 0.0017647777485350767\n",
      "Epoch 143/300\n",
      "Average training loss: 0.023741424191329213\n",
      "Average test loss: 0.0017868352168136173\n",
      "Epoch 144/300\n",
      "Average training loss: 0.023729841384622787\n",
      "Average test loss: 0.0017818504998253451\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023714206198851267\n",
      "Average test loss: 0.001778584277153843\n",
      "Epoch 146/300\n",
      "Average training loss: 0.023680607312255436\n",
      "Average test loss: 0.0017594960696167416\n",
      "Epoch 147/300\n",
      "Average training loss: 0.023656670686271454\n",
      "Average test loss: 0.0017591710247927242\n",
      "Epoch 148/300\n",
      "Average training loss: 0.023637312716907926\n",
      "Average test loss: 0.0017701075891446735\n",
      "Epoch 149/300\n",
      "Average training loss: 0.023585067469212743\n",
      "Average test loss: 0.0017761640469026235\n",
      "Epoch 150/300\n",
      "Average training loss: 0.023581341475248336\n",
      "Average test loss: 0.0023626521254579225\n",
      "Epoch 151/300\n",
      "Average training loss: 0.023592481411165662\n",
      "Average test loss: 0.0019342486652442151\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02355147787100739\n",
      "Average test loss: 0.001749503471992082\n",
      "Epoch 153/300\n",
      "Average training loss: 0.023532351872987217\n",
      "Average test loss: 0.0018101868416286178\n",
      "Epoch 154/300\n",
      "Average training loss: 0.023516442405680814\n",
      "Average test loss: 0.0017728725692464245\n",
      "Epoch 155/300\n",
      "Average training loss: 0.023486526181300482\n",
      "Average test loss: 0.0017738532809954551\n",
      "Epoch 156/300\n",
      "Average training loss: 0.023467430702514117\n",
      "Average test loss: 0.0017797185745504168\n",
      "Epoch 157/300\n",
      "Average training loss: 0.023435892282260787\n",
      "Average test loss: 0.0018086925127233067\n",
      "Epoch 158/300\n",
      "Average training loss: 0.023400432117283345\n",
      "Average test loss: 0.0017822415832844044\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02340851821170913\n",
      "Average test loss: 0.001768626905977726\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02338691606124242\n",
      "Average test loss: 0.0017569228917774227\n",
      "Epoch 161/300\n",
      "Average training loss: 0.023378601728214158\n",
      "Average test loss: 0.0017863675458356738\n",
      "Epoch 162/300\n",
      "Average training loss: 0.023373230054974554\n",
      "Average test loss: 0.0017915027942508458\n",
      "Epoch 163/300\n",
      "Average training loss: 0.023368544252382384\n",
      "Average test loss: 0.0017657308698528344\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02332687878774272\n",
      "Average test loss: 0.0017816448023335802\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023281082310610348\n",
      "Average test loss: 0.001799638014804158\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02328516133957439\n",
      "Average test loss: 0.0017568617696977324\n",
      "Epoch 167/300\n",
      "Average training loss: 0.023254235281712478\n",
      "Average test loss: 0.0018035139692947268\n",
      "Epoch 168/300\n",
      "Average training loss: 0.023261415771312185\n",
      "Average test loss: 0.001779685376263741\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02324481231553687\n",
      "Average test loss: 0.001791326012255417\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023220099475648667\n",
      "Average test loss: 0.0017991127942999204\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0231980559527874\n",
      "Average test loss: 0.0017785712931719091\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02316320520804988\n",
      "Average test loss: 0.0017667938942710559\n",
      "Epoch 173/300\n",
      "Average training loss: 0.023176311133636367\n",
      "Average test loss: 0.001784107571053836\n",
      "Epoch 174/300\n",
      "Average training loss: 0.023156969817148314\n",
      "Average test loss: 0.0017762839801402557\n",
      "Epoch 175/300\n",
      "Average training loss: 0.023115378911296527\n",
      "Average test loss: 0.0017725241461044384\n",
      "Epoch 176/300\n",
      "Average training loss: 0.023105217509799533\n",
      "Average test loss: 0.0018122843752304712\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023087459595666993\n",
      "Average test loss: 0.0018130285297003057\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023098976092206108\n",
      "Average test loss: 0.0017773405802953574\n",
      "Epoch 179/300\n",
      "Average training loss: 0.023062366815076934\n",
      "Average test loss: 0.001783457924508386\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023069087584813436\n",
      "Average test loss: 0.0017982548042717907\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0230175983607769\n",
      "Average test loss: 0.0018042401217131152\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023028556330336465\n",
      "Average test loss: 0.0020364486689989766\n",
      "Epoch 183/300\n",
      "Average training loss: 0.023053979297478995\n",
      "Average test loss: 0.001776115238873495\n",
      "Epoch 184/300\n",
      "Average training loss: 0.023027556417716873\n",
      "Average test loss: 0.0018140135527484947\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02296753839403391\n",
      "Average test loss: 0.0018111541142894162\n",
      "Epoch 186/300\n",
      "Average training loss: 0.022986922946241165\n",
      "Average test loss: 0.0018170280635563863\n",
      "Epoch 187/300\n",
      "Average training loss: 0.022960775408479902\n",
      "Average test loss: 0.0017846444534758727\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02292098170518875\n",
      "Average test loss: 0.0018141907174140214\n",
      "Epoch 189/300\n",
      "Average training loss: 0.022891919453938803\n",
      "Average test loss: 0.0018321041893213988\n",
      "Epoch 190/300\n",
      "Average training loss: 0.022916123398476176\n",
      "Average test loss: 0.0018020856476699313\n",
      "Epoch 191/300\n",
      "Average training loss: 0.022905114049712818\n",
      "Average test loss: 0.001805704595728053\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02288864305615425\n",
      "Average test loss: 0.0018242468141640225\n",
      "Epoch 193/300\n",
      "Average training loss: 0.022883607341183557\n",
      "Average test loss: 0.0018310573922677173\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022863561699787777\n",
      "Average test loss: 0.0017950701302745277\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0228519514426589\n",
      "Average test loss: 0.0019148078409747945\n",
      "Epoch 196/300\n",
      "Average training loss: 0.022807039082050323\n",
      "Average test loss: 0.001798843647663792\n",
      "Epoch 197/300\n",
      "Average training loss: 0.022825796883967187\n",
      "Average test loss: 0.0017935912588404285\n",
      "Epoch 198/300\n",
      "Average training loss: 0.022783908947474427\n",
      "Average test loss: 0.0018030977884514465\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02279101063973374\n",
      "Average test loss: 0.0018139938144013286\n",
      "Epoch 200/300\n",
      "Average training loss: 0.022759813691178958\n",
      "Average test loss: 0.0017961158543411228\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022783655607038072\n",
      "Average test loss: 0.0017841190534333387\n",
      "Epoch 202/300\n",
      "Average training loss: 0.022755756758981283\n",
      "Average test loss: 0.0018310237389264836\n",
      "Epoch 203/300\n",
      "Average training loss: 0.022744629088375302\n",
      "Average test loss: 0.001807459336395065\n",
      "Epoch 204/300\n",
      "Average training loss: 0.022710244231753877\n",
      "Average test loss: 0.0018430306042234103\n",
      "Epoch 205/300\n",
      "Average training loss: 0.022698105317023064\n",
      "Average test loss: 0.0018271389990631077\n",
      "Epoch 206/300\n",
      "Average training loss: 0.022708943693174256\n",
      "Average test loss: 0.0017936861370172765\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022659505978226663\n",
      "Average test loss: 0.0017930198419425223\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022674837057789166\n",
      "Average test loss: 0.0017992709618475703\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02267003464533223\n",
      "Average test loss: 0.0018056194171723393\n",
      "Epoch 210/300\n",
      "Average training loss: 0.022619764450523588\n",
      "Average test loss: 0.0018137870319187642\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02260642783674929\n",
      "Average test loss: 0.0018172904186778598\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02264158571428723\n",
      "Average test loss: 0.0018867473628164993\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022626248605549334\n",
      "Average test loss: 0.0018495914377272128\n",
      "Epoch 214/300\n",
      "Average training loss: 0.022609306810630694\n",
      "Average test loss: 0.0018075039894837471\n",
      "Epoch 215/300\n",
      "Average training loss: 0.022592605135507055\n",
      "Average test loss: 0.0018173097885317274\n",
      "Epoch 216/300\n",
      "Average training loss: 0.022583024059732754\n",
      "Average test loss: 0.0018542299858397909\n",
      "Epoch 217/300\n",
      "Average training loss: 0.022551501386695438\n",
      "Average test loss: 0.0017950152861368326\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02255698945124944\n",
      "Average test loss: 0.0018251729992528757\n",
      "Epoch 219/300\n",
      "Average training loss: 0.022556721791625024\n",
      "Average test loss: 0.0018050118688907888\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02252202615307437\n",
      "Average test loss: 0.0018025243056731092\n",
      "Epoch 221/300\n",
      "Average training loss: 0.022496285590860577\n",
      "Average test loss: 0.0017987392825177975\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022547055752740967\n",
      "Average test loss: 0.00183468406368047\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022534316834476258\n",
      "Average test loss: 0.0018213921927122607\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02245457301206059\n",
      "Average test loss: 0.0018368274779576395\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022460072442889212\n",
      "Average test loss: 0.001794204692563249\n",
      "Epoch 226/300\n",
      "Average training loss: 0.022464199774795107\n",
      "Average test loss: 0.0018126423669358094\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022482723351981905\n",
      "Average test loss: 0.0018287145934171147\n",
      "Epoch 228/300\n",
      "Average training loss: 0.022427925502260526\n",
      "Average test loss: 0.0018363641892663307\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02243319711751408\n",
      "Average test loss: 0.0018158809956577088\n",
      "Epoch 230/300\n",
      "Average training loss: 0.022415875496135818\n",
      "Average test loss: 0.0018250187369477418\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022413928028610017\n",
      "Average test loss: 0.0018613789528608322\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022402341404722796\n",
      "Average test loss: 0.0018477075732209617\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022382375253571403\n",
      "Average test loss: 0.001807658155552215\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022395009747809835\n",
      "Average test loss: 0.0018323677302234703\n",
      "Epoch 235/300\n",
      "Average training loss: 0.022360349719723064\n",
      "Average test loss: 0.0018276019782966209\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02235698099931081\n",
      "Average test loss: 0.0017965037247372998\n",
      "Epoch 237/300\n",
      "Average training loss: 0.022361644248167674\n",
      "Average test loss: 0.0018226537000801829\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022333572392662367\n",
      "Average test loss: 0.0018594037949418028\n",
      "Epoch 239/300\n",
      "Average training loss: 0.022347855592767397\n",
      "Average test loss: 0.0018585902945035034\n",
      "Epoch 240/300\n",
      "Average training loss: 0.022378893736335965\n",
      "Average test loss: 0.0018190588015649053\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0223132796353764\n",
      "Average test loss: 0.0017979023356197608\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02229876953860124\n",
      "Average test loss: 0.0018417211851725975\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02229432549989886\n",
      "Average test loss: 0.0018262559857426417\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02231090457075172\n",
      "Average test loss: 0.0018011215119105245\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022259840569562383\n",
      "Average test loss: 0.0018793674610141252\n",
      "Epoch 246/300\n",
      "Average training loss: 0.022268476103742916\n",
      "Average test loss: 0.0018264048719364735\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02223818452987406\n",
      "Average test loss: 0.001840114889976879\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02227204728457663\n",
      "Average test loss: 0.0018111784280174308\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02223850898941358\n",
      "Average test loss: 0.0018064931809074348\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022222917987240687\n",
      "Average test loss: 0.0018629588146383564\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02225023775961664\n",
      "Average test loss: 0.0018488487750291825\n",
      "Epoch 252/300\n",
      "Average training loss: 0.022230738477574456\n",
      "Average test loss: 0.0018046279706888729\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022219118840992452\n",
      "Average test loss: 0.001868487016401357\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022178850513365534\n",
      "Average test loss: 0.001828371437318209\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02216436031791899\n",
      "Average test loss: 0.0018621292375028133\n",
      "Epoch 256/300\n",
      "Average training loss: 0.022171565196580357\n",
      "Average test loss: 0.0018308808939117524\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022164632187949288\n",
      "Average test loss: 0.0018392720245238808\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02217244018945429\n",
      "Average test loss: 0.001842682441800005\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022138086789184146\n",
      "Average test loss: 0.0018554143655217356\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02213425875372357\n",
      "Average test loss: 0.0018467201808881428\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022157205430997744\n",
      "Average test loss: 0.0018004300902701087\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022136519599292012\n",
      "Average test loss: 0.0018243640567072563\n",
      "Epoch 263/300\n",
      "Average training loss: 0.022116674595408968\n",
      "Average test loss: 0.0018297495141418444\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02211462513771322\n",
      "Average test loss: 0.0018178190653108889\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022109049547049734\n",
      "Average test loss: 0.0018755549775022599\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022084729231066174\n",
      "Average test loss: 0.001862809789625721\n",
      "Epoch 267/300\n",
      "Average training loss: 0.022109575264983708\n",
      "Average test loss: 0.0018794238933672507\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02205647729833921\n",
      "Average test loss: 0.0018630319711648755\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02207368172208468\n",
      "Average test loss: 0.0018822187227714392\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022069990439547434\n",
      "Average test loss: 0.0018230563342157338\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022075723816951116\n",
      "Average test loss: 0.001820145545527339\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022039027070005733\n",
      "Average test loss: 0.0018427943483822875\n",
      "Epoch 273/300\n",
      "Average training loss: 0.022047838797171912\n",
      "Average test loss: 0.0018598450579577022\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02203470128112369\n",
      "Average test loss: 0.0018652583643173177\n",
      "Epoch 275/300\n",
      "Average training loss: 0.022035793855786325\n",
      "Average test loss: 0.0018442770511739783\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022067583315902285\n",
      "Average test loss: 0.0018316444340679382\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021972908331288232\n",
      "Average test loss: 0.00196094876072473\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02199797359440062\n",
      "Average test loss: 0.00183794062005149\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02198599298298359\n",
      "Average test loss: 0.0018825862162436047\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02200478293001652\n",
      "Average test loss: 0.0018198123182066612\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021981776505708695\n",
      "Average test loss: 0.0018392577146490415\n",
      "Epoch 282/300\n",
      "Average training loss: 0.021973647938834297\n",
      "Average test loss: 0.0023022830637378826\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02199448629220327\n",
      "Average test loss: 0.0018538731891247961\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021977130639884206\n",
      "Average test loss: 0.0018379121218911475\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02195613571173615\n",
      "Average test loss: 0.0018467529302255974\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021960917570524745\n",
      "Average test loss: 0.0018264722360504999\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02194543362657229\n",
      "Average test loss: 0.0018591699906521373\n",
      "Epoch 288/300\n",
      "Average training loss: 0.021962421317895255\n",
      "Average test loss: 0.0018353620840029583\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02192555028034581\n",
      "Average test loss: 0.0018647920794578063\n",
      "Epoch 290/300\n",
      "Average training loss: 0.021900346209605535\n",
      "Average test loss: 0.0019255869806640677\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02191583073635896\n",
      "Average test loss: 0.06003980563994911\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02202693983250194\n",
      "Average test loss: 0.0018569026643203365\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02193149457540777\n",
      "Average test loss: 0.0018201936611698733\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021876842247115242\n",
      "Average test loss: 0.0018256826989559664\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02191768197549714\n",
      "Average test loss: 0.0018369445285449425\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021889386114146973\n",
      "Average test loss: 0.0018476154158512752\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021876197397708894\n",
      "Average test loss: 0.0018896387502965\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021854954729477565\n",
      "Average test loss: 0.0018436796119850542\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021854664254519676\n",
      "Average test loss: 0.0018330135351667801\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02184849348333147\n",
      "Average test loss: 0.0018386316966886322\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive_Depth3-.025/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.60\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.91\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.78\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.94\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.01\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.03\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.43\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.14\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.79\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.47\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.64\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.11\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.11\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.19\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.19\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.204811606407166\n",
      "Average test loss: 0.005923012406461769\n",
      "Epoch 2/300\n",
      "Average training loss: 1.167172887749142\n",
      "Average test loss: 0.005072437677946355\n",
      "Epoch 3/300\n",
      "Average training loss: 0.6052171907954745\n",
      "Average test loss: 0.004888723781953255\n",
      "Epoch 4/300\n",
      "Average training loss: 0.4142967560821109\n",
      "Average training loss: 0.25884343394968246\n",
      "Average test loss: 0.004634785628567139\n",
      "Epoch 7/300\n",
      "Average training loss: 0.21659329651461706\n",
      "Average test loss: 0.004517973952823215\n",
      "Epoch 8/300\n",
      "Average training loss: 0.18776813639534845\n",
      "Average test loss: 0.004504997529503372\n",
      "Epoch 9/300\n",
      "Average training loss: 0.17095572292804717\n",
      "Average test loss: 0.004468836416800817\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1611996421284146\n",
      "Average test loss: 0.004459587679555019\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1454706076118681\n",
      "Average test loss: 0.004532197427004576\n",
      "Epoch 14/300\n",
      "Average training loss: 0.14230840020709568\n",
      "Average test loss: 0.004364068377349112\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1397289104130533\n",
      "Average test loss: 0.004341873950014512\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13764271337456174\n",
      "Average test loss: 0.004290303166127867\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13577505991193983\n",
      "Average test loss: 0.0042688493782447445\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13433013886875578\n",
      "Average test loss: 0.004250808541145589\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1328659719824791\n",
      "Average test loss: 0.0042362733487453725\n",
      "Epoch 20/300\n",
      "Average training loss: 0.13184681602319082\n",
      "Average test loss: 0.004284872870892287\n",
      "Epoch 21/300\n",
      "Average training loss: 0.13078523887528312\n",
      "Average test loss: 0.00422717939151658\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12992070937156677\n",
      "Average test loss: 0.004231258131149742\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12917577328284582\n",
      "Average test loss: 0.004236572043970227\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12855221878819995\n",
      "Average test loss: 0.004188843660056591\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12785814236270057\n",
      "Average test loss: 0.004189911656081676\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12730787679221894\n",
      "Average test loss: 0.004211364986374974\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12681589869658152\n",
      "Average test loss: 0.004168281013559964\n",
      "Epoch 28/300\n",
      "Average training loss: 0.12597192098697027\n",
      "Average test loss: 0.004139856503655513\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12558118940061994\n",
      "Average test loss: 0.004184148095341192\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12517321797211964\n",
      "Average test loss: 0.00413205883362227\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12482518321606847\n",
      "Average test loss: 0.004113208770751953\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12451807311839527\n",
      "Average test loss: 0.004124681919606196\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12414011663198471\n",
      "Average test loss: 0.004177836005679435\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12379961374733184\n",
      "Average test loss: 0.004105713258186976\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12355360290739272\n",
      "Average test loss: 0.004101475686662727\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1232474614183108\n",
      "Average test loss: 0.004107924163548483\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12306905629237493\n",
      "Average test loss: 0.004107956493066417\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12278232347302967\n",
      "Average test loss: 0.0040933213670634565\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12262267362409168\n",
      "Average test loss: 0.004088901914656162\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12235324872202344\n",
      "Average test loss: 0.004084070933361848\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12209828158881929\n",
      "Average test loss: 0.004057230230834749\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12183186507225037\n",
      "Average test loss: 0.00409121143238412\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12168815246555541\n",
      "Average test loss: 0.004087880067320334\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12152251607179641\n",
      "Average test loss: 0.004106635995209217\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12129494188229242\n",
      "Average test loss: 0.00407284424909287\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12109921145439148\n",
      "Average test loss: 0.0040540580290059245\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12091416925854154\n",
      "Average test loss: 0.00404090599488053\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12072725656959746\n",
      "Average test loss: 0.004036201433382101\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12052342258559333\n",
      "Average test loss: 0.004048401503306296\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12039004715945986\n",
      "Average test loss: 0.004038250662800338\n",
      "Epoch 52/300\n",
      "Average training loss: 0.12021329755915536\n",
      "Average test loss: 0.004037462153575487\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12013434379630619\n",
      "Average test loss: 0.0040703027976883785\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11997817001740138\n",
      "Average test loss: 0.004043120602766672\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11991627323627473\n",
      "Average test loss: 0.004034246447599596\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11973188954591751\n",
      "Average test loss: 0.004112281458659305\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1196492465602027\n",
      "Average test loss: 0.0040480201870409975\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1194758778280682\n",
      "Average test loss: 0.00409845303143892\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11939522279633416\n",
      "Average test loss: 0.004013905362122589\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11922557536098692\n",
      "Average test loss: 0.004022914745948381\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11913679032855563\n",
      "Average test loss: 0.004016585891859399\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11907225949896706\n",
      "Average test loss: 0.004015590182609028\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1188417303495937\n",
      "Average test loss: 0.0040094890404078695\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11891332032283147\n",
      "Average test loss: 0.004020529631318317\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11873537366920048\n",
      "Average test loss: 0.004014834029806985\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1185510718623797\n",
      "Average test loss: 0.003997309048556619\n",
      "Epoch 67/300\n",
      "Average training loss: 0.118498126314746\n",
      "Average test loss: 0.004020732258756955\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11841437836488088\n",
      "Average test loss: 0.003997945214932163\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11833139830165439\n",
      "Average test loss: 0.004001498338248995\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11819606982337104\n",
      "Average test loss: 0.004030526686873701\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11811788013246324\n",
      "Average test loss: 0.00400307587078876\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11797357946634293\n",
      "Average test loss: 0.004025149814784527\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1179210881723298\n",
      "Average test loss: 0.0040153566158066195\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11781541710429722\n",
      "Average test loss: 0.003994556676596403\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1177258645163642\n",
      "Average test loss: 0.00399319384371241\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11763786626524396\n",
      "Average test loss: 0.003995462966875898\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11754196894168854\n",
      "Average test loss: 0.004008309172673358\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11741890580124326\n",
      "Average test loss: 0.003995136269264751\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1173284835351838\n",
      "Average test loss: 0.004000121045236786\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11701867592334747\n",
      "Average test loss: 0.003997604039601154\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11694199666712018\n",
      "Average test loss: 0.003986546761459774\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11688322854042053\n",
      "Average test loss: 0.0039889540212849775\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1168008619679345\n",
      "Average test loss: 0.004000913659317626\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11672791471083958\n",
      "Average test loss: 0.004004435650176472\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11659532195329667\n",
      "Average test loss: 0.003991618068681823\n",
      "Epoch 88/300\n",
      "Average training loss: 0.1165386328763432\n",
      "Average test loss: 0.003987435986184411\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11638124618927638\n",
      "Average test loss: 0.0040148681882354945\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11628257093164655\n",
      "Average test loss: 0.0040377687762180965\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11622892365852992\n",
      "Average test loss: 0.003999981141545706\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11606139865848754\n",
      "Average test loss: 0.004015334923234251\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11610995090007782\n",
      "Average test loss: 0.0039897873368528155\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11593763171301948\n",
      "Average test loss: 0.003999720230698585\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11586865442328984\n",
      "Average test loss: 0.004006013605329726\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11576311130656136\n",
      "Average test loss: 0.0040141698233783244\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1156841552986039\n",
      "Average test loss: 0.003982660407407416\n",
      "Epoch 98/300\n",
      "Average training loss: 0.11559028279119067\n",
      "Average test loss: 0.004015334502690368\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11542253737317192\n",
      "Average test loss: 0.003993845254182816\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11540954627593358\n",
      "Average test loss: 0.003986028786541687\n",
      "Epoch 101/300\n",
      "Average training loss: 0.11510286255677542\n",
      "Average test loss: 0.004051763282881843\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11500327316257689\n",
      "Average test loss: 0.004005042073213391\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11491566833522585\n",
      "Average test loss: 0.003999851270889243\n",
      "Epoch 106/300\n",
      "Average training loss: 0.1148247057530615\n",
      "Average test loss: 0.004039813450434142\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11470201545953751\n",
      "Average test loss: 0.004024214652677377\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11467965739303165\n",
      "Average test loss: 0.003985063679516315\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11456202483177184\n",
      "Average test loss: 0.004055351787764165\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11439044915305244\n",
      "Average test loss: 0.004002738227446874\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11427982163429261\n",
      "Average test loss: 0.00401711007931994\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1142196760641204\n",
      "Average test loss: 0.0041276640219406946\n",
      "Epoch 113/300\n",
      "Average training loss: 0.11417529867092768\n",
      "Average test loss: 0.004005069539364841\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11406058651208878\n",
      "Average test loss: 0.003996718348314365\n",
      "Epoch 115/300\n",
      "Average training loss: 0.11387718664275276\n",
      "Average test loss: 0.004003151395668586\n",
      "Epoch 116/300\n",
      "Average training loss: 0.11383687061071396\n",
      "Average test loss: 0.005115973520196147\n",
      "Epoch 117/300\n",
      "Average training loss: 0.11345648309257296\n",
      "Average test loss: 0.004104072929670413\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11343735811445448\n",
      "Average test loss: 0.004011971923419171\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11344532004992167\n",
      "Average test loss: 0.003984494238677952\n",
      "Epoch 122/300\n",
      "Average training loss: 0.1132523393034935\n",
      "Average test loss: 0.003994548445774449\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11316016381978988\n",
      "Average test loss: 0.004078637505984969\n",
      "Epoch 124/300\n",
      "Average training loss: 0.1127943762143453\n",
      "Average test loss: 0.004097064148220751\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11264173310332828\n",
      "Average test loss: 0.00404056457699173\n",
      "Epoch 128/300\n",
      "Average training loss: 0.11267829157246484\n",
      "Average test loss: 0.0040169568591647675\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11240898374054167\n",
      "Average test loss: 0.004015353281464842\n",
      "Epoch 131/300\n",
      "Average training loss: 0.11222023732794656\n",
      "Average test loss: 0.0042185469898912645\n",
      "Epoch 132/300\n",
      "Average training loss: 0.11215494298934936\n",
      "Average test loss: 0.004063420512816972\n",
      "Epoch 133/300\n",
      "Average training loss: 0.11219558232360416\n",
      "Average test loss: 0.00405088322609663\n",
      "Epoch 134/300\n",
      "Average training loss: 0.11189266823397742\n",
      "Average test loss: 0.004076915947720409\n",
      "Epoch 135/300\n",
      "Average training loss: 0.11205614974101384\n",
      "Average test loss: 0.004027125060972241\n",
      "Epoch 136/300\n",
      "Average training loss: 0.1117845493555069\n",
      "Average test loss: 0.004024853695804874\n",
      "Epoch 137/300\n",
      "Average training loss: 0.11161202626095877\n",
      "Average test loss: 0.004104911740455362\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11155882561869092\n",
      "Average test loss: 0.004061755889198846\n",
      "Epoch 139/300\n",
      "Average training loss: 0.11129768695433935\n",
      "Average test loss: 0.004131627170989911\n",
      "Epoch 140/300\n",
      "Average training loss: 0.11126232333978017\n",
      "Average test loss: 0.004058197080675099\n",
      "Epoch 141/300\n",
      "Average training loss: 0.11126510744624668\n",
      "Average test loss: 0.004070090239039726\n",
      "Epoch 142/300\n",
      "Average training loss: 0.11112368451224433\n",
      "Average test loss: 0.004025215363957816\n",
      "Epoch 143/300\n",
      "Average training loss: 0.11098674821191364\n",
      "Average test loss: 0.004060275063332584\n",
      "Epoch 144/300\n",
      "Average training loss: 0.1109817887544632\n",
      "Average test loss: 0.004022289200789398\n",
      "Epoch 145/300\n",
      "Average training loss: 0.11095334620608224\n",
      "Average test loss: 0.004128318639058205\n",
      "Epoch 146/300\n",
      "Average training loss: 0.11070893073744244\n",
      "Average test loss: 0.004059379210902585\n",
      "Epoch 147/300\n",
      "Average training loss: 0.1105757024023268\n",
      "Average test loss: 0.004068179947220617\n",
      "Epoch 148/300\n",
      "Average training loss: 0.11053928922944599\n",
      "Average test loss: 0.004060850780250298\n",
      "Epoch 149/300\n",
      "Average training loss: 0.11031990758577982\n",
      "Average test loss: 0.004091710117956003\n",
      "Epoch 152/300\n",
      "Average training loss: 0.11002056287394629\n",
      "Average test loss: 0.004071547753281063\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10991821099652184\n",
      "Average test loss: 0.0040336222102244695\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10999707873662312\n",
      "Average test loss: 0.004157612797286775\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10981101607614094\n",
      "Average test loss: 0.004254928557823102\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10969433779848946\n",
      "Average test loss: 0.004057409584936169\n",
      "Epoch 157/300\n",
      "Average training loss: 0.1095242235660553\n",
      "Average test loss: 0.0040833980407979755\n",
      "Epoch 158/300\n",
      "Average training loss: 0.1095444734427664\n",
      "Average test loss: 0.004093576381189955\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10922368864880667\n",
      "Average test loss: 0.004046409259239833\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10933707249164581\n",
      "Average test loss: 0.004179598046052787\n",
      "Epoch 161/300\n",
      "Average training loss: 0.10904683895243539\n",
      "Average test loss: 0.004179315312455098\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10907360339164734\n",
      "Average test loss: 0.004163679614663124\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10903339831696617\n",
      "Average test loss: 0.004095369209845861\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10910159683889813\n",
      "Average test loss: 0.0041274344209167695\n",
      "Epoch 165/300\n",
      "Average training loss: 0.1087669920457734\n",
      "Average test loss: 0.004139288867099418\n",
      "Epoch 166/300\n",
      "Average training loss: 0.1087291577855746\n",
      "Average test loss: 0.004072674972936511\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10850207136074702\n",
      "Average test loss: 0.00419180250995689\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10852420013149579\n",
      "Average test loss: 0.004186403674383958\n",
      "Epoch 169/300\n",
      "Average training loss: 0.10827680473195182\n",
      "Average test loss: 0.004158472355041239\n",
      "Epoch 171/300\n",
      "Average training loss: 0.10834738567802642\n",
      "Average test loss: 0.004095651287585497\n",
      "Epoch 172/300\n",
      "Average training loss: 0.1080456073946423\n",
      "Average test loss: 0.004121381265007787\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10818387781911426\n",
      "Average test loss: 0.004098885007202626\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10784738110171424\n",
      "Average test loss: 0.004155947458412912\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10775373961528142\n",
      "Average test loss: 0.004153193906777435\n",
      "Epoch 176/300\n",
      "Average training loss: 0.10792465889453888\n",
      "Average test loss: 0.004088565902370545\n",
      "Epoch 177/300\n",
      "Average training loss: 0.10771316565407647\n",
      "Average test loss: 0.004180260651227501\n",
      "Epoch 178/300\n",
      "Average training loss: 0.10756429774231381\n",
      "Average test loss: 0.004211939313759406\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1074183385041025\n",
      "Average test loss: 0.004172694504261017\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10757241350412369\n",
      "Average test loss: 0.004092886804292599\n",
      "Epoch 181/300\n",
      "Average training loss: 0.10743358586231867\n",
      "Average test loss: 0.004146538646684753\n",
      "Epoch 182/300\n",
      "Average training loss: 0.1073678674697876\n",
      "Average test loss: 0.004102182992630534\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10716735008027818\n",
      "Average test loss: 0.004133302766415808\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10711747495333354\n",
      "Average test loss: 0.004108922040089965\n",
      "Epoch 185/300\n",
      "Average training loss: 0.10702584180567\n",
      "Average test loss: 0.004155269089879261\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10681883598036236\n",
      "Average test loss: 0.00416793443966243\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10678250269757376\n",
      "Average test loss: 0.004127065928859843\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10688162078791194\n",
      "Average test loss: 0.004195483264823755\n",
      "Epoch 189/300\n",
      "Average training loss: 0.1067508990102344\n",
      "Average test loss: 0.004101139900998937\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10665641819768482\n",
      "Average test loss: 0.00421302991381122\n",
      "Epoch 192/300\n",
      "Average training loss: 0.10638391138447656\n",
      "Average test loss: 0.004194678634405136\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1064097365339597\n",
      "Average test loss: 0.0042036587153044015\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10632757888237636\n",
      "Average test loss: 0.004138734510789315\n",
      "Epoch 195/300\n",
      "Average training loss: 0.10634119750393761\n",
      "Average test loss: 0.0042130840722885395\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10629649773571226\n",
      "Average test loss: 0.0041543966778036625\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10609449697865381\n",
      "Average test loss: 0.004300993621763256\n",
      "Epoch 198/300\n",
      "Average training loss: 0.1059859798087014\n",
      "Average test loss: 0.0042179018205238715\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10584964242246415\n",
      "Average test loss: 0.0042065479718148705\n",
      "Epoch 200/300\n",
      "Average training loss: 0.10573238792684343\n",
      "Average test loss: 0.004160804226580593\n",
      "Epoch 201/300\n",
      "Average training loss: 0.10584404896365272\n",
      "Average test loss: 0.004171507660920421\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10560735876692666\n",
      "Average test loss: 0.00420702127698395\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10551891201072269\n",
      "Average test loss: 0.004154894062214427\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10569044715828366\n",
      "Average test loss: 0.004167845140521725\n",
      "Epoch 205/300\n",
      "Average training loss: 0.10554740980598662\n",
      "Average test loss: 0.004203024048772123\n",
      "Epoch 206/300\n",
      "Average training loss: 0.10545275543133417\n",
      "Average test loss: 0.0041365552677048575\n",
      "Epoch 207/300\n",
      "Average training loss: 0.10534466776582929\n",
      "Average training loss: 0.1052592298719618\n",
      "Average test loss: 0.004301080386878715\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10500473493337631\n",
      "Average test loss: 0.004252621607648002\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10506510352426104\n",
      "Average test loss: 0.004406299764497412\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10498763530121909\n",
      "Average test loss: 0.004179323869032993\n",
      "Epoch 213/300\n",
      "Average training loss: 0.1048017373085022\n",
      "Average test loss: 0.004236198133064641\n",
      "Epoch 214/300\n",
      "Average training loss: 0.10489980078405804\n",
      "Average test loss: 0.004126902215803663\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10468698778417375\n",
      "Average test loss: 0.004159034055968126\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10471727407640881\n",
      "Average test loss: 0.0042165495227608416\n",
      "Epoch 217/300\n",
      "Average training loss: 0.10470001213418113\n",
      "Average test loss: 0.004346357252034876\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10469636942942938\n",
      "Average test loss: 0.004686933386243052\n",
      "Epoch 219/300\n",
      "Average training loss: 0.10448013974560631\n",
      "Average test loss: 0.004285209796908829\n",
      "Epoch 220/300\n",
      "Average training loss: 0.10440354273716608\n",
      "Average test loss: 0.004254211641848088\n",
      "Epoch 221/300\n",
      "Average training loss: 0.10422736665937636\n",
      "Average test loss: 0.004227083332422707\n",
      "Epoch 222/300\n",
      "Average training loss: 0.1043794287774298\n",
      "Average test loss: 0.004234724616010984\n",
      "Epoch 223/300\n",
      "Average training loss: 0.10429615039957894\n",
      "Average test loss: 0.004203797941820489\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10431894622246424\n",
      "Average test loss: 0.00418297170702782\n",
      "Epoch 227/300\n",
      "Average training loss: 0.10401498153474596\n",
      "Average test loss: 0.004190996271454626\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10407159596681595\n",
      "Average test loss: 0.004197362711860074\n",
      "Epoch 229/300\n",
      "Average training loss: 0.10393493143055174\n",
      "Average test loss: 0.004256153209341897\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1037025480998887\n",
      "Average test loss: 0.004320167383386029\n",
      "Epoch 231/300\n",
      "Average training loss: 0.10375375923183229\n",
      "Average test loss: 0.0042071074830989045\n",
      "Epoch 232/300\n",
      "Average training loss: 0.10365467464923858\n",
      "Average test loss: 0.004175232032313943\n",
      "Epoch 233/300\n",
      "Average training loss: 0.10388489517238406\n",
      "Average test loss: 0.004116326594932212\n",
      "Epoch 234/300\n",
      "Average training loss: 0.10384417145782047\n",
      "Average test loss: 0.004227859042584896\n",
      "Epoch 235/300\n",
      "Average training loss: 0.10348683684402042\n",
      "Average test loss: 0.004206726872879598\n",
      "Epoch 236/300\n",
      "Average training loss: 0.10332444191641278\n",
      "Average test loss: 0.004203581736733516\n",
      "Epoch 237/300\n",
      "Average training loss: 0.10343365836805768\n",
      "Average test loss: 0.004217219409015443\n",
      "Epoch 238/300\n",
      "Average training loss: 0.10329869520664216\n",
      "Average test loss: 0.004260637034972509\n",
      "Epoch 239/300\n",
      "Average training loss: 0.1031798984474606\n",
      "Average test loss: 0.00417232113579909\n",
      "Epoch 240/300\n",
      "Average training loss: 0.10323172711663776\n",
      "Average test loss: 0.004221091172761387\n",
      "Epoch 241/300\n",
      "Average training loss: 0.1031707805858718\n",
      "Average test loss: 0.004346062507066462\n",
      "Epoch 242/300\n",
      "Average training loss: 0.10321797141101625\n",
      "Average test loss: 0.004225977924341957\n",
      "Epoch 243/300\n",
      "Average training loss: 0.10315050478114023\n",
      "Average test loss: 0.0041875248981846705\n",
      "Epoch 244/300\n",
      "Average training loss: 0.10285712721943856\n",
      "Average test loss: 0.004249240787078937\n",
      "Epoch 245/300\n",
      "Average training loss: 0.10293721600373586\n",
      "Average test loss: 0.004367271297507816\n",
      "Epoch 246/300\n",
      "Average training loss: 0.10313537834750282\n",
      "Average test loss: 0.0042375333114630645\n",
      "Epoch 247/300\n",
      "Average training loss: 0.10287034273147583\n",
      "Average test loss: 0.004371914474914472\n",
      "Epoch 248/300\n",
      "Average training loss: 0.10271541457043754\n",
      "Average test loss: 0.004189161862143212\n",
      "Epoch 251/300\n",
      "Average training loss: 0.1026085868941413\n",
      "Average test loss: 0.004226608044571346\n",
      "Epoch 252/300\n",
      "Average training loss: 0.10262905483113395\n",
      "Average test loss: 0.004258496755527126\n",
      "Epoch 253/300\n",
      "Average training loss: 0.10258797204494477\n",
      "Average test loss: 0.004295795650945769\n",
      "Epoch 254/300\n",
      "Average training loss: 0.10251670704947577\n",
      "Average test loss: 0.004244324230899413\n",
      "Epoch 255/300\n",
      "Average training loss: 0.10241517665651109\n",
      "Average test loss: 0.004262395221946968\n",
      "Epoch 256/300\n",
      "Average training loss: 0.1024296895398034\n",
      "Average test loss: 0.004172431942282452\n",
      "Epoch 257/300\n",
      "Average training loss: 0.10231012091371748\n",
      "Average test loss: 4.655239035129547\n",
      "Epoch 258/300\n",
      "Average training loss: 0.10249892129500707\n",
      "Average test loss: 0.004266196447941992\n",
      "Epoch 259/300\n",
      "Average training loss: 0.10224275381035275\n",
      "Average test loss: 0.004346339300274849\n",
      "Epoch 260/300\n",
      "Average training loss: 0.10232704305648804\n",
      "Average test loss: 0.004344144063691298\n",
      "Epoch 261/300\n",
      "Average training loss: 0.10212154400348664\n",
      "Average test loss: 0.004242139384564426\n",
      "Epoch 262/300\n",
      "Average training loss: 0.1019735111925337\n",
      "Average test loss: 0.004334401499480009\n",
      "Epoch 263/300\n",
      "Average training loss: 0.10195027112960815\n",
      "Average test loss: 0.004277120419674449\n",
      "Epoch 264/300\n",
      "Average training loss: 0.10187084723843469\n",
      "Average test loss: 0.0042781985242747594\n",
      "Epoch 265/300\n",
      "Average test loss: 0.004212943442372812\n",
      "Epoch 267/300\n",
      "Average training loss: 0.1020181613696946\n",
      "Average test loss: 0.004362843418286907\n",
      "Epoch 268/300\n",
      "Average training loss: 0.10160628635353512\n",
      "Average test loss: 0.004188500901477204\n",
      "Epoch 269/300\n",
      "Average training loss: 0.10177482897043229\n",
      "Average test loss: 0.004305212321587735\n",
      "Epoch 270/300\n",
      "Average training loss: 0.10172313176261054\n",
      "Average test loss: 0.00432158432321416\n",
      "Epoch 271/300\n",
      "Average training loss: 0.10161881960100598\n",
      "Average test loss: 0.004278914016567999\n",
      "Epoch 272/300\n",
      "Average training loss: 0.10233625902401076\n",
      "Average test loss: 0.004314065797461404\n",
      "Epoch 273/300\n",
      "Average training loss: 0.10155770467387305\n",
      "Average test loss: 0.004320249832338757\n",
      "Epoch 274/300\n",
      "Average training loss: 0.10164728274941444\n",
      "Average test loss: 0.004274911483956708\n",
      "Epoch 275/300\n",
      "Average training loss: 0.10138615040646659\n",
      "Average test loss: 0.004266374996552864\n",
      "Epoch 276/300\n",
      "Average training loss: 0.10145447696579828\n",
      "Average test loss: 0.0042486459840503\n",
      "Epoch 277/300\n",
      "Average training loss: 0.10134303291638692\n",
      "Average test loss: 0.004258484399567048\n",
      "Epoch 278/300\n",
      "Average training loss: 0.10126736108462016\n",
      "Average test loss: 0.004243678187744485\n",
      "Epoch 279/300\n",
      "Average training loss: 0.10127902417712742\n",
      "Average test loss: 0.004274491577926609\n",
      "Epoch 280/300\n",
      "Average training loss: 0.1013385751247406\n",
      "Average test loss: 0.004359015945759085\n",
      "Epoch 281/300\n",
      "Average training loss: 0.10155118799209595\n",
      "Average test loss: 0.004340774262117015\n",
      "Epoch 282/300\n",
      "Average training loss: 0.10122814730803172\n",
      "Average test loss: 0.0043384094792935585\n",
      "Epoch 283/300\n",
      "Average training loss: 0.10114113659991159\n",
      "Average test loss: 0.004174736760970619\n",
      "Epoch 284/300\n",
      "Average training loss: 0.10100018387370639\n",
      "Average test loss: 0.0042803105964428845\n",
      "Epoch 286/300\n",
      "Average training loss: 0.10099064948823717\n",
      "Average test loss: 0.004286945235398081\n",
      "Epoch 287/300\n",
      "Average training loss: 0.10114324091540443\n",
      "Average test loss: 0.004285614077208771\n",
      "Epoch 288/300\n",
      "Average training loss: 0.10094394186470243\n",
      "Average test loss: 0.004358440999976463\n",
      "Epoch 289/300\n",
      "Average training loss: 0.10097374662425783\n",
      "Average test loss: 0.004295890613976452\n",
      "Epoch 290/300\n",
      "Average training loss: 0.10079367738299899\n",
      "Average test loss: 0.004393051818013191\n",
      "Epoch 291/300\n",
      "Average training loss: 0.10070815151929856\n",
      "Average test loss: 0.004335899340195788\n",
      "Epoch 292/300\n",
      "Average training loss: 0.10074805239836375\n",
      "Average test loss: 0.004316310418562757\n",
      "Epoch 293/300\n",
      "Average training loss: 0.10088189572095871\n",
      "Average test loss: 0.004367659746358792\n",
      "Epoch 294/300\n",
      "Average training loss: 0.10053849573267831\n",
      "Average test loss: 0.004311447496629424\n",
      "Epoch 295/300\n",
      "Average training loss: 0.10076751181814406\n",
      "Average test loss: 0.004307986118520299\n",
      "Epoch 296/300\n",
      "Average training loss: 0.10047650267680486\n",
      "Average test loss: 0.004305264518492752\n",
      "Epoch 297/300\n",
      "Average training loss: 0.10057008357842763\n",
      "Average test loss: 0.00428194048980044\n",
      "Epoch 298/300\n",
      "Average training loss: 0.10041646297772726\n",
      "Average test loss: 0.0043217454240139985\n",
      "Epoch 299/300\n",
      "Average training loss: 0.10055285849173863\n",
      "Average test loss: 0.004188908650643296\n",
      "Epoch 300/300\n",
      "Average training loss: 0.10052693193488652\n",
      "Average test loss: 0.004308640689899524\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.527268340852525\n",
      "Average test loss: 0.006281082180225187\n",
      "Epoch 2/300\n",
      "Average training loss: 1.0475382840368483\n",
      "Average test loss: 0.00502003587782383\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5401809073289235\n",
      "Average test loss: 0.004693905149896939\n",
      "Epoch 4/300\n",
      "Average training loss: 0.35736061011420356\n",
      "Average test loss: 0.004481960107054975\n",
      "Epoch 5/300\n",
      "Average training loss: 0.26525611968835194\n",
      "Average test loss: 0.004398099650939306\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2132719968424903\n",
      "Average test loss: 0.00449217144648234\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1841328353352017\n",
      "Average test loss: 0.004312474568270975\n",
      "Epoch 8/300\n",
      "Average training loss: 0.16556961835755243\n",
      "Average training loss: 0.14509614847103755\n",
      "Average test loss: 0.003977626796811819\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13887396909793218\n",
      "Average test loss: 0.0038774573910567494\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13402005277739631\n",
      "Average test loss: 0.0038316325719157853\n",
      "Epoch 13/300\n",
      "Average training loss: 0.129852541618877\n",
      "Average test loss: 0.003802842722998725\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12639751736323038\n",
      "Average test loss: 0.0037197141129937435\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12329260956578784\n",
      "Average test loss: 0.003683425213313765\n",
      "Epoch 16/300\n",
      "Average training loss: 0.12073083529869716\n",
      "Average test loss: 0.0036792537752125\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1182969624069002\n",
      "Average test loss: 0.0035920310028725202\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11628005235062705\n",
      "Average test loss: 0.0035706896239684686\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11447575981087155\n",
      "Average test loss: 0.00359154208853013\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11284409903155433\n",
      "Average test loss: 0.0034960777556730643\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11125970290766822\n",
      "Average test loss: 0.0035134572063883146\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10999418936835395\n",
      "Average test loss: 0.003415133465702335\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10880348692999946\n",
      "Average test loss: 0.0034019338885943095\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10779071212477154\n",
      "Average test loss: 0.0033653510957956313\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10487411166561975\n",
      "Average test loss: 0.0032957145273685455\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10379672866397434\n",
      "Average test loss: 0.003293440242194467\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10305122654305564\n",
      "Average test loss: 0.003301998656036125\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10231956926981609\n",
      "Average test loss: 0.0032672805959979693\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10143823146157795\n",
      "Average test loss: 0.003231025692075491\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10080998514095942\n",
      "Average test loss: 0.003249841576649083\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10011722834242714\n",
      "Average test loss: 0.0032279638554900884\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09958036484320959\n",
      "Average test loss: 0.0032097955648269917\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0990006010333697\n",
      "Average test loss: 0.0032118643923766082\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09837661527262793\n",
      "Average test loss: 0.0031805362078464695\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09794001026948293\n",
      "Average test loss: 0.0031863908796674677\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09745844835705228\n",
      "Average test loss: 0.003274867327055997\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09705993046694332\n",
      "Average test loss: 0.0031946718870765633\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09662004894680447\n",
      "Average test loss: 0.003143893778945009\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09617572749323315\n",
      "Average test loss: 0.0031492339726537466\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09575423491663403\n",
      "Average test loss: 0.003229827164154914\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09534292678700552\n",
      "Average test loss: 0.0031321598390738167\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09504313481516308\n",
      "Average test loss: 0.003166393106596337\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0946956636706988\n",
      "Average test loss: 0.0031134184644454055\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09439241002003351\n",
      "Average test loss: 0.003116360674094823\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0940476098789109\n",
      "Average test loss: 0.0031002730516095957\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09364594242307875\n",
      "Average test loss: 0.0031484215383728344\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09345905968878004\n",
      "Average test loss: 0.0031000469759520557\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09314037220345603\n",
      "Average test loss: 0.003090564474463463\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09287568174468147\n",
      "Average test loss: 0.0031012196892665493\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0926479063961241\n",
      "Average test loss: 0.0030806483885066375\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09229686351617178\n",
      "Average test loss: 0.003130323105595178\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09213613472382227\n",
      "Average test loss: 0.0030861736644680303\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0919828622771634\n",
      "Average test loss: 0.003058199408153693\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09171451034810808\n",
      "Average test loss: 0.003079873806072606\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09151995253562928\n",
      "Average test loss: 0.0030764529450486105\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09123202022910118\n",
      "Average test loss: 0.0030955895086129505\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09109523400333193\n",
      "Average test loss: 0.0030473573170602323\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09079308425717883\n",
      "Average test loss: 0.0030709574263956814\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09061412070526018\n",
      "Average test loss: 0.003050589528348711\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09041827305820253\n",
      "Average test loss: 0.0030580125511106516\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0901247923705313\n",
      "Average test loss: 0.003042290788350834\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08996090784337786\n",
      "Average test loss: 0.0030359471417549582\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08973548596103986\n",
      "Average test loss: 0.00307048767267002\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08953805043299994\n",
      "Average test loss: 0.00304356199523641\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08937783238954014\n",
      "Average test loss: 0.0030769799357901017\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08920513253741794\n",
      "Average test loss: 0.0030454291614393394\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08889201063579984\n",
      "Average test loss: 0.003053630785085261\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08875764056709078\n",
      "Average test loss: 0.003038584440946579\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08856439569261339\n",
      "Average test loss: 0.0030653693597349854\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08844668538040586\n",
      "Average test loss: 0.003054568104032013\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08819286440478431\n",
      "Average test loss: 0.003060646296892729\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08796542483899328\n",
      "Average test loss: 0.0030411288761016397\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08783696244160334\n",
      "Average test loss: 0.0030628220782511766\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08767021658685473\n",
      "Average test loss: 0.0030369800062229235\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08739365787307421\n",
      "Average test loss: 0.003051301964455181\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0872496340142356\n",
      "Average test loss: 0.0030812298792103924\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08714898623691665\n",
      "Average test loss: 0.003056954803566138\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08684930166933272\n",
      "Average test loss: 0.0030374648771766158\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08666705135504404\n",
      "Average test loss: 0.003054395906523698\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08656766215960185\n",
      "Average test loss: 0.0030477709993720053\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08640853509306907\n",
      "Average test loss: 0.0030690643654929265\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08619462531805039\n",
      "Average test loss: 0.003052192201630937\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08597656169202593\n",
      "Average test loss: 0.003070669784521063\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08589516103267669\n",
      "Average test loss: 0.0030751379910442566\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08559974136617449\n",
      "Average test loss: 0.003055581790705522\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08552651974889967\n",
      "Average test loss: 0.003204835150597824\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08543022911085023\n",
      "Average test loss: 0.003116165958137976\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08518774967723422\n",
      "Average test loss: 0.0030793790138430067\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08488838957415687\n",
      "Average test loss: 0.0030442715539700455\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08483758645587497\n",
      "Average test loss: 0.0030421142652630804\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08454813035991457\n",
      "Average test loss: 0.003055920826064216\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08437578033738666\n",
      "Average test loss: 0.0031253744148545797\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08425006901555591\n",
      "Average test loss: 0.0030601235955125755\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08411079882913165\n",
      "Average test loss: 0.003065224354258842\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08397469715277354\n",
      "Average test loss: 0.0030700293588969444\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08390665897395876\n",
      "Average test loss: 0.003071662750095129\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08372739138205847\n",
      "Average test loss: 0.0030604887975172862\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08352561236752404\n",
      "Average test loss: 0.003077640018943283\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08337960141234928\n",
      "Average test loss: 0.0030875833926515446\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08311678928799099\n",
      "Average test loss: 0.0031688388927529256\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08317275909582773\n",
      "Average test loss: 0.0030829464942216875\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08293181091215876\n",
      "Average test loss: 0.003090239925723937\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08263254447778066\n",
      "Average test loss: 0.0032316619391656583\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08263010691934161\n",
      "Average test loss: 0.0030618771796839103\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08242634060647752\n",
      "Average test loss: 0.0030686418045726086\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08223405278391309\n",
      "Average test loss: 0.0031626488527076112\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0821396644976404\n",
      "Average test loss: 0.0031196427525331575\n",
      "Epoch 110/300\n",
      "Average training loss: 0.08193889651033613\n",
      "Average test loss: 0.0031266998087780343\n",
      "Epoch 111/300\n",
      "Average training loss: 0.08185721238454183\n",
      "Average test loss: 0.0030587238559706344\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08169981011417177\n",
      "Average test loss: 0.0030774197205901146\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08149741019142999\n",
      "Average test loss: 0.0031443393631941743\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08140011501312255\n",
      "Average test loss: 0.003095812204397387\n",
      "Epoch 115/300\n",
      "Average training loss: 0.08136323983470599\n",
      "Average test loss: 0.0031019695293572212\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08140729807151688\n",
      "Average test loss: 0.0031232284802115624\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08104167089859644\n",
      "Average test loss: 0.003131774306918184\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08090196545256509\n",
      "Average test loss: 0.003103822161340051\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08060916007227367\n",
      "Average test loss: 0.003102219032951527\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08070280731055472\n",
      "Average test loss: 0.0031305097180108227\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08064884471231036\n",
      "Average test loss: 0.003107720782566402\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08038014542394215\n",
      "Average test loss: 0.0031181599601275392\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08018584660689036\n",
      "Average test loss: 0.0030981329650514654\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0800482418735822\n",
      "Average test loss: 0.003107991004983584\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08008857440948486\n",
      "Average test loss: 0.0031319662543634573\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07991066115763452\n",
      "Average test loss: 0.0031783961283249986\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07973029287656148\n",
      "Average test loss: 0.003155918085533712\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07977442798018455\n",
      "Average test loss: 0.003172142560283343\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07961938532193502\n",
      "Average test loss: 0.0032138649303880005\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07951985955238343\n",
      "Average test loss: 0.003149371630201737\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07919980528619554\n",
      "Average test loss: 0.003125499218702316\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07910081974996461\n",
      "Average test loss: 0.00315451424382627\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07914611173338361\n",
      "Average test loss: 0.0032737356401566003\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07902665347523159\n",
      "Average test loss: 0.003154265920528107\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0789072317481041\n",
      "Average test loss: 0.0031440434564525884\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07876993714438545\n",
      "Average test loss: 0.0031023823236011796\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07858495715591643\n",
      "Average test loss: 0.003160980485172735\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07867274093296793\n",
      "Average test loss: 0.003173170273295707\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07839125219318602\n",
      "Average test loss: 0.0031664690979652933\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07830326037274467\n",
      "Average test loss: 0.003348620776914888\n",
      "Epoch 141/300\n",
      "Average training loss: 0.078350011838807\n",
      "Average test loss: 0.003181294084423118\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07821391331156095\n",
      "Average test loss: 0.003215821094914443\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0781200816962454\n",
      "Average test loss: 0.0031649643845028347\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07813029311100642\n",
      "Average test loss: 0.0031881092741257613\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07779006848070356\n",
      "Average test loss: 0.0031957952013860146\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07797568237119251\n",
      "Average test loss: 0.003162797202873561\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07766184937953949\n",
      "Average test loss: 0.0031708989344123337\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07770371815231111\n",
      "Average test loss: 0.0031766796687410937\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07758013554414113\n",
      "Average test loss: 0.0031693645767453646\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07744374288784132\n",
      "Average test loss: 0.00316880950000551\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07731586465570661\n",
      "Average test loss: 0.0032429915169874825\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07734889122181468\n",
      "Average test loss: 0.003161851215486725\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07714202019241122\n",
      "Average test loss: 0.0032274683751165865\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07694943335983488\n",
      "Average test loss: 0.003220296521567636\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07699160665273666\n",
      "Average test loss: 0.003187264876647128\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07697253003385332\n",
      "Average test loss: 0.003208742254724105\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07687413897779252\n",
      "Average test loss: 0.003168142371293571\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07683926594919628\n",
      "Average test loss: 0.003138214405419098\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07663546834389369\n",
      "Average test loss: 0.0032023431447645028\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07654342526859707\n",
      "Average test loss: 0.003128328500315547\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07638082220819262\n",
      "Average test loss: 0.003194121149265104\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07643073081970214\n",
      "Average test loss: 0.0031937826366888154\n",
      "Epoch 163/300\n",
      "Average training loss: 0.076373884803719\n",
      "Average test loss: 0.0031561880927118992\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07628985426823298\n",
      "Average test loss: 0.003182255476092299\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07615967026021746\n",
      "Average test loss: 0.0031579388638751375\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07600840446021821\n",
      "Average test loss: 0.0032093114484515457\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07612042323085996\n",
      "Average test loss: 0.0031586946795384087\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07605896438161532\n",
      "Average test loss: 0.003235859078872535\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07590787018007703\n",
      "Average test loss: 0.003138603851613071\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07588295078277588\n",
      "Average test loss: 0.003218630960625079\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07567508531941308\n",
      "Average test loss: 0.003177841967592637\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07587021201848984\n",
      "Average test loss: 0.003220255339311229\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07552306288149621\n",
      "Average test loss: 0.0034726030367116132\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07555589061313206\n",
      "Average test loss: 0.003257954624378019\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07555889625019498\n",
      "Average test loss: 0.003212385077857309\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0754821282128493\n",
      "Average test loss: 0.003243069891921348\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07533799017137952\n",
      "Average test loss: 0.0032325300394246974\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07532918211486604\n",
      "Average test loss: 0.003192959348567658\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0753156750202179\n",
      "Average test loss: 0.0032097792083190546\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07524978023105197\n",
      "Average test loss: 0.003252867898179425\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0750219003111124\n",
      "Average test loss: 0.0032170374910864565\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07503697925143772\n",
      "Average test loss: 0.0032019798000239665\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07496445640590456\n",
      "Average test loss: 0.0032273347586807277\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0750164792339007\n",
      "Average test loss: 0.0031994647172590095\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07481229891710811\n",
      "Average test loss: 0.003254383778406514\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07487244911326302\n",
      "Average test loss: 0.003172984680781762\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07472871169779036\n",
      "Average test loss: 0.003180222265008423\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07467998684777154\n",
      "Average test loss: 0.0032299885275877183\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07476741831832462\n",
      "Average test loss: 0.0032197881131950353\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0746171609626876\n",
      "Average test loss: 0.0032267814059224395\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07444738782776726\n",
      "Average test loss: 0.003202050152545174\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07427490743001303\n",
      "Average test loss: 0.003212014939222071\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07425745297140546\n",
      "Average test loss: 0.00322888353963693\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07436529453595479\n",
      "Average test loss: 0.003223325758344597\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07431719530953301\n",
      "Average test loss: 0.00327922498434782\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07444572044081157\n",
      "Average test loss: 0.0034734905424217383\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07415475411547555\n",
      "Average test loss: 0.0032447635705272356\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07408728673391872\n",
      "Average test loss: 0.00322490064572129\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07406091977159182\n",
      "Average test loss: 0.0031985244138373267\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07398297633065118\n",
      "Average test loss: 0.0032570662361880143\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07405553986959987\n",
      "Average test loss: 0.0032265328781472314\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07399754986498092\n",
      "Average test loss: 0.003265759112106429\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0737723914153046\n",
      "Average test loss: 0.0032238734935720764\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07373843036095301\n",
      "Average test loss: 0.0032908692167450985\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07374741908576754\n",
      "Average test loss: 0.003241526091057393\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07361287719011307\n",
      "Average test loss: 0.0032025790607763662\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07375604676869181\n",
      "Average test loss: 0.0032384340941078132\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07373910703923967\n",
      "Average test loss: 0.003231676510638661\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07345209685961405\n",
      "Average test loss: 0.003256881991194354\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07356494024064805\n",
      "Average test loss: 0.003265813075419929\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07333765793508953\n",
      "Average test loss: 0.0032760195109165376\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07333946398231718\n",
      "Average test loss: 0.003255105251239406\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07337190052535798\n",
      "Average test loss: 0.0032809446973519193\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07331021824147967\n",
      "Average test loss: 0.00327991751064029\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07358347658978569\n",
      "Average test loss: 0.0033065292386131153\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07310989570617676\n",
      "Average test loss: 0.0032538902627097235\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07309193573395412\n",
      "Average test loss: 0.0032777991570118402\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07305982002947066\n",
      "Average test loss: 0.003254471923535069\n",
      "Epoch 219/300\n",
      "Average training loss: 0.072966869380739\n",
      "Average test loss: 0.003240534302882022\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07294909588164754\n",
      "Average test loss: 0.00327506081511577\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0729225648774041\n",
      "Average test loss: 0.0033029466855029263\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07276231172018581\n",
      "Average test loss: 0.003261375171235866\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07293014164765677\n",
      "Average test loss: 0.0032415431648906735\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07283684760994381\n",
      "Average test loss: 0.003296081595536735\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07265359203020731\n",
      "Average test loss: 0.003272941859645976\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07280522438552645\n",
      "Average test loss: 0.0032194981966167688\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07283098565207588\n",
      "Average test loss: 0.0032373402155935764\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07263333394130071\n",
      "Average test loss: 0.0032349605382316644\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07267669636673398\n",
      "Average test loss: 0.003193600359890196\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07242830404970381\n",
      "Average test loss: 0.003284425742510292\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07253395067652066\n",
      "Average test loss: 0.0032440308938837714\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07245864035685858\n",
      "Average test loss: 0.003311533096867303\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07234698453214433\n",
      "Average test loss: 0.0032799343021793497\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07241789213154051\n",
      "Average test loss: 0.003381640064633555\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07241200057665508\n",
      "Average test loss: 0.003276085761893127\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07222959095901912\n",
      "Average test loss: 0.003227999418766962\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07217748724752002\n",
      "Average test loss: 0.0032735599072443115\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07211983658207788\n",
      "Average test loss: 0.0032893477355440456\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07231830469104979\n",
      "Average test loss: 0.0032017459608614444\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07212478701935875\n",
      "Average test loss: 0.003278237459146314\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0722450244029363\n",
      "Average test loss: 0.003281933138353957\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07219707264502843\n",
      "Average test loss: 0.003293666002651056\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07204062690999773\n",
      "Average test loss: 0.003227502951812413\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07195548143982887\n",
      "Average test loss: 0.003271681388012237\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07176313307550218\n",
      "Average test loss: 0.0033107414591229626\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07198947534296248\n",
      "Average test loss: 0.00327812788883845\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0720961004032029\n",
      "Average test loss: 0.00327385039606856\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07173854407336977\n",
      "Average test loss: 0.0033136117396255333\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07168927860922283\n",
      "Average test loss: 0.003361808815039694\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0717781622144911\n",
      "Average test loss: 0.0032480904774533376\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07167860694064035\n",
      "Average test loss: 0.0033610082351499134\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07159349373314115\n",
      "Average test loss: 0.0032714140634569856\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07169104270802604\n",
      "Average test loss: 0.0033243107861942716\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0715586050219006\n",
      "Average test loss: 0.0035667644679132434\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07144082234303156\n",
      "Average test loss: 0.003316172164761358\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07158206530412038\n",
      "Average test loss: 0.0032930658554865253\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07140645706984732\n",
      "Average test loss: 0.003386611421075132\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07145853719777531\n",
      "Average test loss: 0.003345750062415997\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07136402151319715\n",
      "Average test loss: 0.0033187081215696203\n",
      "Epoch 261/300\n",
      "Average training loss: 0.071483954972691\n",
      "Average test loss: 0.003256663110314144\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07127074514163866\n",
      "Average test loss: 0.003377979120860497\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07141637949479951\n",
      "Average test loss: 0.0032684001016120116\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07132471613420381\n",
      "Average test loss: 0.00327315049038993\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0712458670006858\n",
      "Average test loss: 0.0032757971164666944\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07115368381473754\n",
      "Average test loss: 0.003332160578419765\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07119539674785402\n",
      "Average test loss: 0.0033149431325081324\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0712487296793196\n",
      "Average test loss: 0.003419839290695058\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07129963291684786\n",
      "Average test loss: 0.0032491087946626873\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07118237947424252\n",
      "Average test loss: 0.0033018960224257573\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07111146252685123\n",
      "Average test loss: 0.0033428241465654636\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07121137520339754\n",
      "Average test loss: 0.0032784795405136216\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07112519604629941\n",
      "Average test loss: 0.0032795389362921317\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07107551038927502\n",
      "Average test loss: 0.0032704295830594168\n",
      "Epoch 275/300\n",
      "Average training loss: 0.070857177734375\n",
      "Average test loss: 0.0033171733770933414\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07081889308161206\n",
      "Average test loss: 0.003335091617786222\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07095660723249117\n",
      "Average test loss: 0.0033218677499228053\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07085503387120035\n",
      "Average test loss: 0.003397832286854585\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07071663845909966\n",
      "Average test loss: 0.003291511020312707\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0708029636806912\n",
      "Average test loss: 0.0032925053396158747\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07085350188944074\n",
      "Average test loss: 0.0033257670572234524\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07073728374640147\n",
      "Average test loss: 0.003304763032330407\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07072275571028391\n",
      "Average test loss: 0.0032755097757197088\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07054216853777567\n",
      "Average test loss: 0.0033422455375807154\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07070654623707136\n",
      "Average test loss: 0.0033305128510627484\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07061096923881106\n",
      "Average test loss: 0.0033132368953277666\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07057490171326532\n",
      "Average test loss: 0.003336736345042785\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07051851297087139\n",
      "Average test loss: 0.0033185582293404473\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07041730201906628\n",
      "Average test loss: 0.003326557732704613\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07036111990610759\n",
      "Average test loss: 0.0032245131203283866\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0704005006187492\n",
      "Average test loss: 0.003360862543599473\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07049690920114517\n",
      "Average test loss: 0.003285637897749742\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07049446442061\n",
      "Average test loss: 0.003291691969666216\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07043849324848916\n",
      "Average test loss: 0.003274581374393569\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07036823837624656\n",
      "Average test loss: 0.0032822451572865246\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07027708234058486\n",
      "Average test loss: 0.003343479516191615\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07032620251178741\n",
      "Average test loss: 0.0032722869981080295\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0702920366856787\n",
      "Average test loss: 0.003239463942539361\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0701672530637847\n",
      "Average test loss: 0.0033252428281638356\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07016551473405626\n",
      "Average test loss: 0.003362191929585404\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.625767037815518\n",
      "Average test loss: 0.0061718796388142635\n",
      "Epoch 2/300\n",
      "Average training loss: 0.8486790174908109\n",
      "Average test loss: 0.004934451011733876\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4171821862326728\n",
      "Average test loss: 0.004423469771941503\n",
      "Epoch 4/300\n",
      "Average training loss: 0.27935700575510664\n",
      "Average test loss: 0.004067048366285033\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2135372586912579\n",
      "Average test loss: 0.0038754227314558294\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17811923967467413\n",
      "Average test loss: 0.0037406358967224758\n",
      "Epoch 7/300\n",
      "Average training loss: 0.15723712892002528\n",
      "Average test loss: 0.004233090623178416\n",
      "Epoch 8/300\n",
      "Average training loss: 0.14368055787351397\n",
      "Average test loss: 0.0035529540959331725\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1338311455051104\n",
      "Average test loss: 0.0034958902458763785\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12634098122517268\n",
      "Average test loss: 0.003419066686597135\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12041383755869335\n",
      "Average test loss: 0.0032852986606044902\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1155069057808982\n",
      "Average test loss: 0.003254656144314342\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1111856957409117\n",
      "Average test loss: 0.0032314521233654686\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10758935987287098\n",
      "Average test loss: 0.0030647968074513808\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10430055940813489\n",
      "Average test loss: 0.0030124511876040033\n",
      "Epoch 16/300\n",
      "Average training loss: 0.10139490896463395\n",
      "Average test loss: 0.0029795670728716586\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0988273085951805\n",
      "Average test loss: 0.0028872098575035733\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09654733332660463\n",
      "Average test loss: 0.002817570493867\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09434201220671336\n",
      "Average test loss: 0.002784088019488586\n",
      "Epoch 20/300\n",
      "Average training loss: 0.09251295918557378\n",
      "Average test loss: 0.0027075576030959688\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09073645190066762\n",
      "Average test loss: 0.002693393997434113\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08912275870641073\n",
      "Average test loss: 0.0026391819609949984\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08777713509400685\n",
      "Average test loss: 0.002620388226894041\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08652752851115332\n",
      "Average test loss: 0.002606100142002106\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08520372609628571\n",
      "Average test loss: 0.002590598838817742\n",
      "Epoch 26/300\n",
      "Average training loss: 0.08424831930134032\n",
      "Average test loss: 0.0025553499905185567\n",
      "Epoch 27/300\n",
      "Average training loss: 0.08307680016093784\n",
      "Average test loss: 0.0025341006068305837\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08207163093487421\n",
      "Average test loss: 0.002540667151514855\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08117933923006057\n",
      "Average test loss: 0.002542825264028377\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08041323163774279\n",
      "Average test loss: 0.002571620476328664\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07960735750860638\n",
      "Average test loss: 0.002481074934100939\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07898682783047359\n",
      "Average test loss: 0.0024725039090133377\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07834060184823143\n",
      "Average test loss: 0.0024639485606716737\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07785586663749483\n",
      "Average test loss: 0.002461775179952383\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07714644582072894\n",
      "Average test loss: 0.0024772187074025473\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0766874573926131\n",
      "Average test loss: 0.0024870033127566177\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07617977493339115\n",
      "Average test loss: 0.002423014927551978\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07572505880395572\n",
      "Average test loss: 0.0024244268967045677\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07516748423708809\n",
      "Average test loss: 0.002399886916908953\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07482219495375951\n",
      "Average test loss: 0.0023990603188673654\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07433994222680727\n",
      "Average test loss: 0.0024058367481662167\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07401012816694048\n",
      "Average test loss: 0.002387592406115598\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07374090535773171\n",
      "Average test loss: 0.00238949526908497\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07342975046899583\n",
      "Average test loss: 0.0023619641477449073\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0729606868227323\n",
      "Average test loss: 0.002344892671331763\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07268220598830118\n",
      "Average test loss: 0.0023849110709917216\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07227837405602137\n",
      "Average test loss: 0.002362407153679265\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07220584024985631\n",
      "Average test loss: 0.0023349007376366193\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07179533565706676\n",
      "Average test loss: 0.002339241143419511\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07154621626271142\n",
      "Average test loss: 0.002335602432489395\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07130062048302756\n",
      "Average test loss: 0.0023438500499145852\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07112755606571833\n",
      "Average test loss: 0.0023119915243652134\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07074533476432164\n",
      "Average test loss: 0.0023191522204627593\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07060019254022175\n",
      "Average test loss: 0.00234118325687531\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07025407291120953\n",
      "Average test loss: 0.002346918834476835\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07006006540854771\n",
      "Average test loss: 0.0023091717436909674\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0698842166894012\n",
      "Average test loss: 0.002307199707047807\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06956547317902247\n",
      "Average test loss: 0.0022959048912550013\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06958548716041776\n",
      "Average test loss: 0.0023492597873426145\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06925003184874852\n",
      "Average test loss: 0.002305747817373938\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06907002237439155\n",
      "Average test loss: 0.002310428789299395\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06889962954984771\n",
      "Average test loss: 0.0023263093238282536\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06864140552944607\n",
      "Average test loss: 0.002290067713500725\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06843923646211623\n",
      "Average test loss: 0.002298570797778666\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06820104534427325\n",
      "Average test loss: 0.0023110262993723154\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06805118854178323\n",
      "Average test loss: 0.002296628705950247\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06789652682344119\n",
      "Average test loss: 0.002323999226714174\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06774259571565522\n",
      "Average test loss: 0.002282704017849432\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06757259524530834\n",
      "Average test loss: 0.0023130689687612983\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0674688686364227\n",
      "Average test loss: 0.002369020075019863\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06716635378864076\n",
      "Average test loss: 0.00232283041274382\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06699947037630612\n",
      "Average test loss: 0.002285904871713784\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06680418208738167\n",
      "Average test loss: 0.0022945158732020194\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06665338797039456\n",
      "Average test loss: 0.0022872497133082812\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06650519473022885\n",
      "Average test loss: 0.0022978166771224803\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06626936489012507\n",
      "Average test loss: 0.002295560296521419\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06627991588579284\n",
      "Average test loss: 0.002320568814977176\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06586959197786119\n",
      "Average test loss: 0.0022854000157159237\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06580915070904626\n",
      "Average test loss: 0.0022917776031212674\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06564688951108191\n",
      "Average test loss: 0.002301664808040692\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06546783607535892\n",
      "Average test loss: 0.002290995398743285\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06530226471689013\n",
      "Average test loss: 0.0022855423750976723\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0650832515988085\n",
      "Average test loss: 0.002280885249376297\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06495895774496926\n",
      "Average test loss: 0.002291825170111325\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06481022095017963\n",
      "Average test loss: 0.0023075020081467098\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06478256654408243\n",
      "Average test loss: 0.0023299920890066357\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06453246545460489\n",
      "Average test loss: 0.0023326141581767137\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06430066728923056\n",
      "Average test loss: 0.002288411523422433\n",
      "Epoch 89/300\n",
      "Average training loss: 0.064198102199369\n",
      "Average test loss: 0.002347330575601922\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0640365331172943\n",
      "Average test loss: 0.002303485099329717\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06379937130212784\n",
      "Average test loss: 0.0023039766783929535\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06372253012657166\n",
      "Average test loss: 0.0023356601347525913\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06354420430461566\n",
      "Average test loss: 0.0023015824368016586\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06339288031392627\n",
      "Average test loss: 0.00229357414226979\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06331094193458557\n",
      "Average test loss: 0.0022933572580416998\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06323853588104247\n",
      "Average test loss: 0.0023276803276191154\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06304352671570249\n",
      "Average test loss: 0.002311886118311021\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06289017552137374\n",
      "Average test loss: 0.0023426945683442885\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06276253815491994\n",
      "Average test loss: 0.002316130190777282\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0625400420361095\n",
      "Average test loss: 0.0023205398259063563\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0624633501999908\n",
      "Average test loss: 0.0022992175564997726\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06231840451889568\n",
      "Average test loss: 0.0023315166108724143\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06212669731842147\n",
      "Average test loss: 0.002339570926191906\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06203781415025393\n",
      "Average test loss: 0.0023424435721503363\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06193089410000377\n",
      "Average test loss: 0.0023460089289065863\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06181491888893975\n",
      "Average test loss: 0.002328987454374631\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06169818631145689\n",
      "Average test loss: 0.0023942640295459166\n",
      "Epoch 108/300\n",
      "Average training loss: 0.061556252178218626\n",
      "Average test loss: 0.0023270262598784433\n",
      "Epoch 109/300\n",
      "Average training loss: 0.061396882133351434\n",
      "Average test loss: 0.002314821027012335\n",
      "Epoch 110/300\n",
      "Average training loss: 0.061353882723384434\n",
      "Average test loss: 0.0023468696041963996\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06115974955426322\n",
      "Average test loss: 0.0023552324197565516\n",
      "Epoch 112/300\n",
      "Average training loss: 0.061009700891044405\n",
      "Average test loss: 0.002316501259803772\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06107546112603612\n",
      "Average test loss: 0.0023326916317972872\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06086712233887778\n",
      "Average test loss: 0.00232744316280716\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06069480781422721\n",
      "Average test loss: 0.0023513653990295197\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06047587207621998\n",
      "Average test loss: 0.0023623854611068964\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0605276074078348\n",
      "Average test loss: 0.002337734820321202\n",
      "Epoch 118/300\n",
      "Average training loss: 0.060305700179603364\n",
      "Average test loss: 0.0023612803236270946\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0602840239405632\n",
      "Average test loss: 0.0023637830989642274\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06009853444827928\n",
      "Average test loss: 0.002333250927221444\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05996028342843056\n",
      "Average test loss: 0.002368657851177785\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05992002343469196\n",
      "Average test loss: 0.0023651329353451727\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05996832122736507\n",
      "Average test loss: 0.0023723410871914694\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05969303434093793\n",
      "Average test loss: 0.0024461885711385143\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05960594111349848\n",
      "Average test loss: 0.0023843919390605555\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05957522612478998\n",
      "Average test loss: 0.00238235463326176\n",
      "Epoch 127/300\n",
      "Average training loss: 0.059508760968844096\n",
      "Average test loss: 0.002352841517681049\n",
      "Epoch 128/300\n",
      "Average training loss: 0.059232293503152\n",
      "Average test loss: 0.0023758270976444087\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05925091538826625\n",
      "Average test loss: 0.0024077806000908216\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05930360635618369\n",
      "Average test loss: 0.002384855660713381\n",
      "Epoch 131/300\n",
      "Average training loss: 0.058956516918208864\n",
      "Average test loss: 0.0024091911086191733\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05908502642313639\n",
      "Average test loss: 0.0023957835940851104\n",
      "Epoch 133/300\n",
      "Average training loss: 0.058929336229960126\n",
      "Average test loss: 0.0023549709264189004\n",
      "Epoch 134/300\n",
      "Average training loss: 0.058668598996268376\n",
      "Average test loss: 0.002362796288811498\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05856615160571204\n",
      "Average test loss: 0.0023660506061795685\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05859480189283689\n",
      "Average test loss: 0.0023624841548088526\n",
      "Epoch 137/300\n",
      "Average training loss: 0.058463149474726785\n",
      "Average test loss: 0.002362148082090749\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05838697706990772\n",
      "Average test loss: 0.0024148525438374945\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05844703225294749\n",
      "Average test loss: 0.002460202995480763\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05813643602199025\n",
      "Average test loss: 0.002344058773583836\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0581226022111045\n",
      "Average test loss: 0.0024000382464792994\n",
      "Epoch 142/300\n",
      "Average training loss: 0.058224339611000486\n",
      "Average test loss: 0.002368731027365559\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05800426039099693\n",
      "Average test loss: 0.002414052792307403\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05791004204750061\n",
      "Average test loss: 0.002379191072140303\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0578270960715082\n",
      "Average test loss: 0.002388133222858111\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05778389459848404\n",
      "Average test loss: 0.0023906530073533458\n",
      "Epoch 147/300\n",
      "Average training loss: 0.057666988859574\n",
      "Average test loss: 0.0023919608193553156\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05752625198496713\n",
      "Average test loss: 0.0023724922771669096\n",
      "Epoch 149/300\n",
      "Average training loss: 0.057545910898182125\n",
      "Average test loss: 0.0023832502015348938\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05740970647335052\n",
      "Average test loss: 0.002390908564751347\n",
      "Epoch 151/300\n",
      "Average training loss: 0.057397307819790314\n",
      "Average test loss: 0.002407450863884555\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05730965329209964\n",
      "Average test loss: 0.0023961858461714453\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05723230706983142\n",
      "Average test loss: 0.002398805940937665\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05707513422436184\n",
      "Average test loss: 0.0023859949093312026\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05709655946493149\n",
      "Average test loss: 0.0023750673259298007\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05707336932751868\n",
      "Average test loss: 0.0026983092710789708\n",
      "Epoch 157/300\n",
      "Average training loss: 0.057034504536125394\n",
      "Average test loss: 0.0024484543311927053\n",
      "Epoch 158/300\n",
      "Average training loss: 0.056834398051102956\n",
      "Average test loss: 0.0024249291240962013\n",
      "Epoch 159/300\n",
      "Average training loss: 0.056774009542332755\n",
      "Average test loss: 0.002385459384570519\n",
      "Epoch 160/300\n",
      "Average training loss: 0.056849101536803774\n",
      "Average test loss: 0.0024306333520346217\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05675017684698105\n",
      "Average test loss: 0.002472173575311899\n",
      "Epoch 162/300\n",
      "Average training loss: 0.056629692216714225\n",
      "Average test loss: 0.0024206787299157844\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05667530753546291\n",
      "Average test loss: 0.002366774366547664\n",
      "Epoch 164/300\n",
      "Average training loss: 0.056590124981270896\n",
      "Average test loss: 0.0024166215491584608\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05643944158156713\n",
      "Average test loss: 0.0024209229312837123\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0563469339840942\n",
      "Average test loss: 0.002418479587468836\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05640629640221596\n",
      "Average test loss: 0.002416346866637468\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05624328613612387\n",
      "Average test loss: 0.0024241233797123035\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05617424715558688\n",
      "Average test loss: 0.002447727520018816\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05613304256399473\n",
      "Average test loss: 0.00242837363988575\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05602699719369411\n",
      "Average test loss: 0.002532772156306439\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05608471459481451\n",
      "Average test loss: 0.0024320085923083953\n",
      "Epoch 173/300\n",
      "Average training loss: 0.055976602415243784\n",
      "Average test loss: 0.0024364240320606362\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0559071764714188\n",
      "Average test loss: 0.002427497368512882\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05601378904117478\n",
      "Average test loss: 0.002446600893098447\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05569509190983243\n",
      "Average test loss: 0.002399168665831288\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05571610436836878\n",
      "Average test loss: 0.0024640719791253407\n",
      "Epoch 178/300\n",
      "Average training loss: 0.055695543169975284\n",
      "Average test loss: 0.0024178096742058794\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05572695020503468\n",
      "Average test loss: 0.0024257742046482032\n",
      "Epoch 180/300\n",
      "Average training loss: 0.055572837508387034\n",
      "Average test loss: 0.002454136415074269\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05548486557602882\n",
      "Average test loss: 0.0024884051070031194\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05558130935827891\n",
      "Average test loss: 0.0024375147920929724\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05538724923464987\n",
      "Average test loss: 0.0026483436986390087\n",
      "Epoch 184/300\n",
      "Average training loss: 0.055388544188605414\n",
      "Average test loss: 0.0024232378819336493\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05534530894292725\n",
      "Average test loss: 0.00246677865450167\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05526393609907892\n",
      "Average test loss: 0.0023851447805969253\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05526088865598043\n",
      "Average test loss: 0.0024123686742451456\n",
      "Epoch 188/300\n",
      "Average training loss: 0.055227156105968685\n",
      "Average test loss: 0.002962011375567979\n",
      "Epoch 189/300\n",
      "Average training loss: 0.055288087328275046\n",
      "Average test loss: 0.002476766695992814\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05505524476700359\n",
      "Average test loss: 0.0024257895075198675\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05513186670011944\n",
      "Average test loss: 0.0025541834535284173\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05505942280424966\n",
      "Average test loss: 0.0024190035928040744\n",
      "Epoch 193/300\n",
      "Average training loss: 0.054955109463797676\n",
      "Average test loss: 0.0024725633596794474\n",
      "Epoch 194/300\n",
      "Average training loss: 0.054919742557737565\n",
      "Average test loss: 0.002428826949559152\n",
      "Epoch 195/300\n",
      "Average training loss: 0.054911789665619534\n",
      "Average test loss: 0.0024150393398271667\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05482213631603453\n",
      "Average test loss: 0.0024407698661088944\n",
      "Epoch 197/300\n",
      "Average training loss: 0.054795406139559216\n",
      "Average test loss: 0.0024174506585631105\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05477037259274059\n",
      "Average test loss: 0.002475887976379858\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05474437398049566\n",
      "Average test loss: 0.0024620344104866188\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05464619469973776\n",
      "Average test loss: 0.0024350273973411983\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05451234637035264\n",
      "Average test loss: 0.0024866068953027327\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05465439592467414\n",
      "Average test loss: 0.002496931108025213\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05468524113628599\n",
      "Average test loss: 0.0024627248453390265\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05453761008050707\n",
      "Average test loss: 0.002435148982124196\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05443565320306354\n",
      "Average test loss: 0.0024810264982903997\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05440785407688883\n",
      "Average test loss: 0.0025425572407742343\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05438291601671113\n",
      "Average test loss: 0.0024555787121256193\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05441604769561026\n",
      "Average test loss: 0.0024514859676775004\n",
      "Epoch 209/300\n",
      "Average training loss: 0.054320482081837124\n",
      "Average test loss: 0.002538905986895164\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05418663294116656\n",
      "Average test loss: 0.0024584737908509044\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05419373257954915\n",
      "Average test loss: 0.0024476604805224473\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0542962883412838\n",
      "Average test loss: 0.002461445150482986\n",
      "Epoch 213/300\n",
      "Average training loss: 0.054356443613767624\n",
      "Average test loss: 0.0024531242292788295\n",
      "Epoch 214/300\n",
      "Average training loss: 0.054048113329543006\n",
      "Average test loss: 0.002438629002413816\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05415025424626139\n",
      "Average test loss: 0.0024739810327688854\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05410411180059115\n",
      "Average test loss: 0.002461699378987153\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0540133309132523\n",
      "Average test loss: 0.0024767499775108365\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0539756464196576\n",
      "Average test loss: 0.0025152769301914507\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05388017698460155\n",
      "Average test loss: 0.0025117571542246473\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05387689072224829\n",
      "Average test loss: 0.002483654199168086\n",
      "Epoch 221/300\n",
      "Average training loss: 0.053857924746142496\n",
      "Average test loss: 0.0025719942175265816\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05382895000444518\n",
      "Average test loss: 0.0024763709420545232\n",
      "Epoch 223/300\n",
      "Average training loss: 0.053735461155573525\n",
      "Average test loss: 0.0024978159837838678\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05377471337715785\n",
      "Average test loss: 0.002439208119486769\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05375648480653763\n",
      "Average test loss: 0.0024861747237543267\n",
      "Epoch 226/300\n",
      "Average training loss: 0.053740236424737506\n",
      "Average test loss: 0.0025060148590968713\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0538018643591139\n",
      "Average test loss: 0.002460527471991049\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05362143523163266\n",
      "Average test loss: 0.002637505872382058\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05362744742631912\n",
      "Average test loss: 0.002482947654194302\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05359961309035619\n",
      "Average test loss: 0.002502724995629655\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05357635871238179\n",
      "Average test loss: 0.0025837234906438324\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05347903472847409\n",
      "Average test loss: 0.002478171666463216\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05338698304361767\n",
      "Average test loss: 0.0024617462156133517\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05333690311842495\n",
      "Average test loss: 0.0025010626245703963\n",
      "Epoch 235/300\n",
      "Average training loss: 0.053441476076841354\n",
      "Average test loss: 0.002503309956855244\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05333907344937325\n",
      "Average test loss: 0.0025216578633214036\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0533656475742658\n",
      "Average test loss: 0.002462631128004028\n",
      "Epoch 238/300\n",
      "Average training loss: 0.053331225117047625\n",
      "Average test loss: 0.0025074372006994154\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05329401415917608\n",
      "Average test loss: 0.0025079146170367797\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05323461033900579\n",
      "Average test loss: 0.0025739765396962564\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05335746238629023\n",
      "Average test loss: 0.0025144542606754436\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05307477555010054\n",
      "Average test loss: 0.0024897020910349156\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05325199140442742\n",
      "Average test loss: 0.002494413948721356\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05321003195974562\n",
      "Average test loss: 0.0024640012164082793\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05306946168343226\n",
      "Average test loss: 0.002472161737167173\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05303871778978242\n",
      "Average test loss: 0.0025141208509190213\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05293804779979918\n",
      "Average test loss: 0.0024869519819815953\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05313788047764036\n",
      "Average test loss: 0.0024777116255006856\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05293417336543401\n",
      "Average test loss: 0.0024840684158520566\n",
      "Epoch 250/300\n",
      "Average training loss: 0.053019139571322337\n",
      "Average test loss: 0.002498807801761561\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05288774646321932\n",
      "Average test loss: 0.0024840579752085936\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05290340418285794\n",
      "Average test loss: 0.0025319774815191825\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05285286585158772\n",
      "Average test loss: 0.00252448663694991\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05294997589124573\n",
      "Average test loss: 0.0024611675935900875\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05277979917989837\n",
      "Average test loss: 0.00247434142873519\n",
      "Epoch 256/300\n",
      "Average training loss: 0.052802588476075066\n",
      "Average test loss: 0.002557923138141632\n",
      "Epoch 257/300\n",
      "Average training loss: 0.052818323642015455\n",
      "Average test loss: 0.0025202602419174378\n",
      "Epoch 258/300\n",
      "Average training loss: 0.052943654855092365\n",
      "Average test loss: 0.0025129562055485116\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05263756531145838\n",
      "Average test loss: 0.0025014276320321693\n",
      "Epoch 260/300\n",
      "Average training loss: 0.052678807963927585\n",
      "Average test loss: 0.0024410267455710305\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05273781819144885\n",
      "Average test loss: 0.002523954896359808\n",
      "Epoch 262/300\n",
      "Average training loss: 0.052535302930408055\n",
      "Average test loss: 0.0026002118231521713\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05262056359317568\n",
      "Average test loss: 0.0024730496381719907\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05256324455473158\n",
      "Average test loss: 0.0025169339842266505\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05258852772249116\n",
      "Average test loss: 0.0025621594584857425\n",
      "Epoch 266/300\n",
      "Average training loss: 0.052542918390697906\n",
      "Average test loss: 0.002487441804053055\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05248321956396103\n",
      "Average test loss: 0.002457603110000491\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05250414070155886\n",
      "Average test loss: 0.0025291712495187917\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05243992572360569\n",
      "Average test loss: 0.002511172923155957\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05240537180999915\n",
      "Average test loss: 0.002518946128897369\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05242021065122551\n",
      "Average test loss: 0.0025361196320090027\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05240737455089887\n",
      "Average test loss: 0.0024959870133962896\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05243106276128027\n",
      "Average test loss: 0.002569330279611879\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05227868106464545\n",
      "Average test loss: 0.0025514430664479735\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05226474762625164\n",
      "Average test loss: 0.0025124837319470115\n",
      "Epoch 276/300\n",
      "Average training loss: 0.052329103175136775\n",
      "Average test loss: 0.0024727359923223655\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05224088894327482\n",
      "Average test loss: 0.0024889793729202616\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05222301578190591\n",
      "Average test loss: 0.0024622403285983535\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05225265442331632\n",
      "Average test loss: 0.0025201299053927264\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05224840281738175\n",
      "Average test loss: 0.00261954303085804\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05215677017635769\n",
      "Average test loss: 0.0025083217873341506\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0520685288409392\n",
      "Average test loss: 0.0025580176644855075\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05215747787555059\n",
      "Average test loss: 0.0025038659810605974\n",
      "Epoch 284/300\n",
      "Average training loss: 0.052140054388178717\n",
      "Average test loss: 0.002515223746912347\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05205063800348176\n",
      "Average test loss: 0.0024950602715834973\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05217978387739923\n",
      "Average test loss: 0.0026422673132684496\n",
      "Epoch 287/300\n",
      "Average training loss: 0.051991476787461174\n",
      "Average test loss: 0.0025130498547934824\n",
      "Epoch 288/300\n",
      "Average training loss: 0.051958704832527375\n",
      "Average test loss: 0.0025203537742296855\n",
      "Epoch 289/300\n",
      "Average training loss: 0.052052032000488704\n",
      "Average test loss: 0.0025189072131696676\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05197976768679089\n",
      "Average test loss: 0.0024687451064172718\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05197736483812332\n",
      "Average test loss: 0.0025588471481783523\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05199925820032755\n",
      "Average test loss: 0.002519607055518362\n",
      "Epoch 293/300\n",
      "Average training loss: 0.051914813995361325\n",
      "Average test loss: 0.0025884421685089666\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05187541968954934\n",
      "Average test loss: 0.0025470550805330276\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05181366905238893\n",
      "Average test loss: 0.0025578963624106514\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05200924153460397\n",
      "Average test loss: 0.0025252448214838903\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05187632414036327\n",
      "Average test loss: 0.003375786038219101\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05177483150362969\n",
      "Average test loss: 0.0025235446352097723\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05180276183949577\n",
      "Average test loss: 0.002491898716944787\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05192220121953223\n",
      "Average test loss: 0.0025202228689773214\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.442425491227044\n",
      "Average test loss: 0.006598492495301697\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7962285101413726\n",
      "Average test loss: 0.004335727198670308\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3842069065570831\n",
      "Average test loss: 0.0038410474962244433\n",
      "Epoch 4/300\n",
      "Average training loss: 0.25410520177417334\n",
      "Average test loss: 0.003693251500113143\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1930379803445604\n",
      "Average test loss: 0.003356299591457678\n",
      "Epoch 6/300\n",
      "Average training loss: 0.16018697261810302\n",
      "Average test loss: 0.0032420054247809782\n",
      "Epoch 7/300\n",
      "Average training loss: 0.13988641973336538\n",
      "Average test loss: 0.003174044433153338\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1264881188604567\n",
      "Average test loss: 0.002994622904393408\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11617257605658637\n",
      "Average test loss: 0.0029240761181960504\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10852730066246456\n",
      "Average test loss: 0.0029036151555677254\n",
      "Epoch 11/300\n",
      "Average training loss: 0.10223527427514394\n",
      "Average test loss: 0.0029076879639178513\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09709490669435925\n",
      "Average test loss: 0.0026352311653188533\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09254764093293084\n",
      "Average test loss: 0.002531685357913375\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08873132475217184\n",
      "Average test loss: 0.0024711350121845803\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0851820962064796\n",
      "Average test loss: 0.002409508707622687\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08222113240427441\n",
      "Average test loss: 0.0023006212578879463\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07949665327204598\n",
      "Average test loss: 0.0022455104223142066\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07711621132824156\n",
      "Average test loss: 0.0021656465826349128\n",
      "Epoch 19/300\n",
      "Average training loss: 0.074934077779452\n",
      "Average test loss: 0.0021314323192669284\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07302927778164546\n",
      "Average test loss: 0.0020790342874825\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07116716707083914\n",
      "Average test loss: 0.0020694979888697465\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06958791316217847\n",
      "Average test loss: 0.0020000546344866353\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06804626605245802\n",
      "Average test loss: 0.0019871642705467014\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06686226990818978\n",
      "Average test loss: 0.001982702963468101\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06566874167323113\n",
      "Average test loss: 0.0019814205353872643\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06449782423840629\n",
      "Average test loss: 0.0019084250144660473\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06361221104860305\n",
      "Average test loss: 0.0019005450839176775\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06262480699353748\n",
      "Average test loss: 0.0018960031918767426\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06165547771917449\n",
      "Average test loss: 0.0018977615314846237\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06110571205615997\n",
      "Average test loss: 0.0018778467072794835\n",
      "Epoch 31/300\n",
      "Average training loss: 0.060513730605443315\n",
      "Average test loss: 0.0018642540842087733\n",
      "Epoch 32/300\n",
      "Average training loss: 0.059685679200622774\n",
      "Average test loss: 0.0018493921682238579\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05908878846963247\n",
      "Average test loss: 0.001841222340034114\n",
      "Epoch 34/300\n",
      "Average training loss: 0.058598760224050944\n",
      "Average test loss: 0.001845015200268891\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0581092577709092\n",
      "Average test loss: 0.001868535806528396\n",
      "Epoch 36/300\n",
      "Average training loss: 0.057846075114276675\n",
      "Average test loss: 0.0018386311957405673\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05735839096373982\n",
      "Average test loss: 0.0018117735892948176\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05693422005905045\n",
      "Average test loss: 0.0017941309546844826\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05647231673200925\n",
      "Average test loss: 0.00179999651801255\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05612098576956325\n",
      "Average test loss: 0.0017722487006750372\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05579659590787358\n",
      "Average test loss: 0.0017666187652179764\n",
      "Epoch 42/300\n",
      "Average training loss: 0.055625743872589534\n",
      "Average test loss: 0.001776768011144466\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05528745960526996\n",
      "Average test loss: 0.0017695118378226956\n",
      "Epoch 44/300\n",
      "Average training loss: 0.054959622068537606\n",
      "Average test loss: 0.0017734011886641383\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05478483419948154\n",
      "Average test loss: 0.0017752006113943126\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05443415846427282\n",
      "Average test loss: 0.0017768667319582568\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05418865381346809\n",
      "Average test loss: 0.001756320129045182\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05408503161205186\n",
      "Average test loss: 0.0017390142666796843\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05381610314382447\n",
      "Average test loss: 0.0017352122167746226\n",
      "Epoch 50/300\n",
      "Average training loss: 0.053607428474558724\n",
      "Average test loss: 0.0017321935287780232\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05340017915434307\n",
      "Average test loss: 0.0017335696760565043\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05310398292210367\n",
      "Average test loss: 0.0017441007335566813\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05300193296207322\n",
      "Average test loss: 0.0017370870566616457\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05273937601513333\n",
      "Average test loss: 0.0017235624535630147\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05271941713160939\n",
      "Average test loss: 0.0017123724492266773\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05243477935261197\n",
      "Average test loss: 0.0017314532149790062\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05224050692054961\n",
      "Average test loss: 0.0017223310021476613\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05208668586942885\n",
      "Average test loss: 0.0017285649707126948\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05192265770998266\n",
      "Average test loss: 0.0017101731271379525\n",
      "Epoch 60/300\n",
      "Average training loss: 0.051684215482738285\n",
      "Average test loss: 0.0017051055550368296\n",
      "Epoch 61/300\n",
      "Average training loss: 0.051654355297485986\n",
      "Average test loss: 0.001708840478832523\n",
      "Epoch 62/300\n",
      "Average training loss: 0.051541246430741414\n",
      "Average test loss: 0.0017167606887718042\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05133497579230203\n",
      "Average test loss: 0.0017083348816571136\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05114660548501544\n",
      "Average test loss: 0.0017201634947624472\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05096295827296045\n",
      "Average test loss: 0.0017014868508817421\n",
      "Epoch 66/300\n",
      "Average training loss: 0.050921932025088205\n",
      "Average test loss: 0.0017150892689824105\n",
      "Epoch 67/300\n",
      "Average training loss: 0.050692584382163154\n",
      "Average test loss: 0.0017122358045437269\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05058163045181169\n",
      "Average test loss: 0.001722024396682779\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05061056079798275\n",
      "Average test loss: 0.0019802888881208167\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05031461059053739\n",
      "Average test loss: 0.00171529035963532\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05013765134745174\n",
      "Average test loss: 0.001700820070070525\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04992997373143832\n",
      "Average test loss: 0.0016971661117341783\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04995200242929988\n",
      "Average test loss: 0.001711430407439669\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04974322634273105\n",
      "Average test loss: 0.0016976664164620968\n",
      "Epoch 75/300\n",
      "Average training loss: 0.049592875238921905\n",
      "Average test loss: 0.0016925685180144178\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04969208565354347\n",
      "Average test loss: 0.0016900940734065242\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04950019337733587\n",
      "Average test loss: 0.0017052261179002623\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04915421392851406\n",
      "Average test loss: 0.0017050698352977633\n",
      "Epoch 79/300\n",
      "Average training loss: 0.049100214802556566\n",
      "Average test loss: 0.001700751047167513\n",
      "Epoch 80/300\n",
      "Average training loss: 0.048926058404975464\n",
      "Average test loss: 0.0016963598076254129\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04881236750218603\n",
      "Average test loss: 0.0017012728359550237\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04862882423069742\n",
      "Average test loss: 0.0016926328134205606\n",
      "Epoch 83/300\n",
      "Average training loss: 0.048621042625771625\n",
      "Average test loss: 0.0016894585189099114\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04850500480002827\n",
      "Average training loss: 0.04831794792082575\n",
      "Average test loss: 0.0017000885393677486\n",
      "Epoch 86/300\n",
      "Average training loss: 0.048226975901259315\n",
      "Average test loss: 0.0017103605301429829\n",
      "Epoch 87/300\n",
      "Average training loss: 0.047975765039523444\n",
      "Average test loss: 0.0017112868525501754\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04785900113648838\n",
      "Average test loss: 0.001719053431517548\n",
      "Epoch 90/300\n",
      "Average training loss: 0.047746359874804814\n",
      "Average test loss: 0.0017043776632183127\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04758370900650819\n",
      "Average test loss: 0.0016982269622385503\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04752766250901752\n",
      "Average test loss: 0.0017022827971312736\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04741660173734029\n",
      "Average test loss: 0.0017068501287657353\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04722652454508675\n",
      "Average test loss: 0.001721682351289524\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04721246734758218\n",
      "Average test loss: 0.0017107503006441726\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04706458021203677\n",
      "Average test loss: 0.0017201781978623736\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04697523204485576\n",
      "Average test loss: 0.0017189755029976368\n",
      "Epoch 98/300\n",
      "Average training loss: 0.046857694771554735\n",
      "Average test loss: 0.001721639765219556\n",
      "Epoch 99/300\n",
      "Average training loss: 0.046817570057180194\n",
      "Average test loss: 0.0017094474168908266\n",
      "Epoch 100/300\n",
      "Average training loss: 0.046688065542115104\n",
      "Average test loss: 0.0017023762272049983\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04655200374788708\n",
      "Average test loss: 0.0017078716373071074\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04651726466417313\n",
      "Average test loss: 0.001710944108147588\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04637394790848096\n",
      "Average test loss: 0.002614138950283329\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04598382382260428\n",
      "Average test loss: 0.0017289016381320027\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0459826504819923\n",
      "Average test loss: 0.0017285553151741625\n",
      "Epoch 108/300\n",
      "Average training loss: 0.045750103407435946\n",
      "Average test loss: 0.0017671906091272832\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04586182083355056\n",
      "Average test loss: 0.0017407210634814369\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04564627264936765\n",
      "Average test loss: 0.0017195081578360663\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04558293621407615\n",
      "Average test loss: 0.0017580357358480494\n",
      "Epoch 112/300\n",
      "Average training loss: 0.045487918029228844\n",
      "Average test loss: 0.001719154038776954\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04540987585319413\n",
      "Average test loss: 0.0017236340466058916\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04525092185205883\n",
      "Average test loss: 0.0017559389814527499\n",
      "Epoch 115/300\n",
      "Average training loss: 0.045183103068007366\n",
      "Average test loss: 0.0017532451819214556\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04512445628311899\n",
      "Average test loss: 0.0017375186650703351\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04501127033432325\n",
      "Average test loss: 0.001723332413782676\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0449542628162437\n",
      "Average test loss: 0.0017517341535745396\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04490199836757448\n",
      "Average test loss: 0.0017259106026548479\n",
      "Epoch 120/300\n",
      "Average training loss: 0.044581525829103255\n",
      "Average test loss: 0.0017627902978824244\n",
      "Epoch 123/300\n",
      "Average training loss: 0.044533347894748054\n",
      "Average test loss: 0.0017417166641809875\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04444245458311505\n",
      "Average test loss: 0.0017708806991577149\n",
      "Epoch 125/300\n",
      "Average training loss: 0.044294503483507365\n",
      "Average test loss: 0.0017164016501564118\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04426948090063201\n",
      "Average test loss: 0.0017397261936631467\n",
      "Epoch 127/300\n",
      "Average training loss: 0.044122539205683604\n",
      "Average test loss: 0.0017311963828073608\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04411382856302791\n",
      "Average test loss: 0.0017449054186128908\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04402918830845091\n",
      "Average test loss: 0.0017861750018265512\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04406503558821148\n",
      "Average test loss: 0.0017732677449368768\n",
      "Epoch 131/300\n",
      "Average training loss: 0.043931031548314625\n",
      "Average test loss: 0.0017574099380936888\n",
      "Epoch 132/300\n",
      "Average training loss: 0.043897551741864944\n",
      "Average test loss: 0.0017774782611264122\n",
      "Epoch 133/300\n",
      "Average training loss: 0.043744802829292086\n",
      "Average test loss: 0.0017423766419912379\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04355700965722402\n",
      "Average test loss: 0.001767640960092346\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04344228623641862\n",
      "Average test loss: 0.0017719466808355517\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04347235522005293\n",
      "Average test loss: 0.0018383283244652882\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04337774174080955\n",
      "Average test loss: 0.00407304251111216\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04334553494387203\n",
      "Average test loss: 0.0017655294318166044\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04317375808291965\n",
      "Average test loss: 0.00174547300218708\n",
      "Epoch 142/300\n",
      "Average training loss: 0.043115266548262705\n",
      "Average test loss: 0.0017966029287005464\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04311473448077838\n",
      "Average test loss: 0.001795297273227738\n",
      "Epoch 144/300\n",
      "Average training loss: 0.043060149288839764\n",
      "Average test loss: 0.0017911684805941251\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04321137603786256\n",
      "Average test loss: 0.0017664684468052454\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04290449583530426\n",
      "Average test loss: 0.0017446155620531904\n",
      "Epoch 147/300\n",
      "Average training loss: 0.042812232659922705\n",
      "Average test loss: 0.0017679590006462402\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04282714129156537\n",
      "Average test loss: 0.0018847905099391936\n",
      "Epoch 149/300\n",
      "Average training loss: 0.042678955137729645\n",
      "Average test loss: 0.0017913302948905363\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04265773615903325\n",
      "Average test loss: 0.0017605920956573554\n",
      "Epoch 151/300\n",
      "Average training loss: 0.042552177240451176\n",
      "Average test loss: 0.001759052864379353\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04264433073004087\n",
      "Average test loss: 0.0017607009232872063\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04233826386928558\n",
      "Average test loss: 0.0017841583813230197\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04234325216213862\n",
      "Average test loss: 0.001792940442967746\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04230007612374094\n",
      "Average test loss: 0.0017912695010503133\n",
      "Epoch 158/300\n",
      "Average training loss: 0.042188925617271\n",
      "Average test loss: 0.0018546844823285938\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04215908563302623\n",
      "Average test loss: 0.0017692373591061267\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04216646644141939\n",
      "Average test loss: 0.0017709368904017739\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04211316244138612\n",
      "Average test loss: 0.0019243923490867018\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04198666387796402\n",
      "Average test loss: 0.0017870169327490859\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04202406066987249\n",
      "Average test loss: 0.0017787848375737667\n",
      "Epoch 164/300\n",
      "Average training loss: 0.041939904719591144\n",
      "Average test loss: 0.00179034919767744\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04186600487762027\n",
      "Average test loss: 0.0017581463862313992\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04183337611622281\n",
      "Average test loss: 0.0017804106956140864\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04181407495670848\n",
      "Average test loss: 0.0017956635020673275\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04175577320655187\n",
      "Average test loss: 0.0018155695419344636\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0416706751551893\n",
      "Average test loss: 0.0018220363030624059\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04169351686371697\n",
      "Average test loss: 0.0018417012292063898\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0415016103602118\n",
      "Average test loss: 0.0017969594742688868\n",
      "Epoch 173/300\n",
      "Average training loss: 0.041497040761841665\n",
      "Average test loss: 0.0018478177109112343\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04146352321240637\n",
      "Average test loss: 0.0017958297794684768\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04148800475398699\n",
      "Average test loss: 0.0018242716290470626\n",
      "Epoch 176/300\n",
      "Average training loss: 0.041376370675034\n",
      "Average test loss: 0.0017847722774992387\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04129992092318005\n",
      "Average test loss: 0.0018125897948630153\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04134259688523081\n",
      "Average test loss: 0.0018191134314984084\n",
      "Epoch 179/300\n",
      "Average training loss: 0.041217870278490915\n",
      "Average test loss: 0.0018198299894316328\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04117318461669816\n",
      "Average test loss: 0.0017924946129529013\n",
      "Epoch 181/300\n",
      "Average training loss: 0.041295720313986144\n",
      "Average test loss: 0.001811375181708071\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04119428500533104\n",
      "Average test loss: 0.00182715940165023\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04107535587085618\n",
      "Average test loss: 0.0018137014124335513\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04112994402315882\n",
      "Average test loss: 0.001819035158596105\n",
      "Epoch 185/300\n",
      "Average training loss: 0.040925768968131805\n",
      "Average test loss: 0.0018579943151109748\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04097014257642958\n",
      "Average test loss: 0.0018032723361005385\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04093650727470716\n",
      "Average test loss: 0.001816940549761057\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04089490150743061\n",
      "Average test loss: 0.0018207431618745128\n",
      "Epoch 190/300\n",
      "Average training loss: 0.040857156816456054\n",
      "Average test loss: 0.0017799465516582132\n",
      "Epoch 191/300\n",
      "Average training loss: 0.040808101137479146\n",
      "Average test loss: 0.001785545359676083\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04075790223148134\n",
      "Average test loss: 0.0018551163646496004\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04080333061350717\n",
      "Average test loss: 0.0018269190941419867\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04095567188329167\n",
      "Average test loss: 0.001807095092617803\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04069837827483813\n",
      "Average test loss: 0.0018376247574471765\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04074702652957704\n",
      "Average test loss: 0.0018124920940026641\n",
      "Epoch 197/300\n",
      "Average training loss: 0.040612504601478576\n",
      "Average test loss: 0.0018804410555296473\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04053240416944027\n",
      "Average test loss: 0.0018403500149854356\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04049674548043145\n",
      "Average test loss: 0.0018536387400494682\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04052667407029205\n",
      "Average test loss: 0.001812798632722762\n",
      "Epoch 201/300\n",
      "Average training loss: 0.040410713252094055\n",
      "Average test loss: 0.0018398172344184585\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04037449851632118\n",
      "Average test loss: 0.0018330887257018023\n",
      "Epoch 203/300\n",
      "Average training loss: 0.040442812320258884\n",
      "Average test loss: 0.0018690334200445148\n",
      "Epoch 204/300\n",
      "Average test loss: 0.0018068567940758334\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04031672053204642\n",
      "Average test loss: 0.0018291400777589943\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04055836924579408\n",
      "Average test loss: 0.0018086947704561882\n",
      "Epoch 208/300\n",
      "Average training loss: 0.040390627374251684\n",
      "Average test loss: 0.0018065871795018514\n",
      "Epoch 209/300\n",
      "Average training loss: 0.040224862817260953\n",
      "Average test loss: 0.0018391115431570344\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0401433893971973\n",
      "Average test loss: 0.0021187925769223107\n",
      "Epoch 211/300\n",
      "Average training loss: 0.040158431033293404\n",
      "Average test loss: 0.0018141602704094515\n",
      "Epoch 212/300\n",
      "Average training loss: 0.040076798198951616\n",
      "Average test loss: 0.0018308084959992104\n",
      "Epoch 213/300\n",
      "Average training loss: 0.040143132872051665\n",
      "Average test loss: 0.0018571811092810498\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04011817032761044\n",
      "Average test loss: 0.0018267049379646779\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04001539542277654\n",
      "Average test loss: 0.0018408458623000318\n",
      "Epoch 216/300\n",
      "Average training loss: 0.040010881854428186\n",
      "Average test loss: 0.001906518724747002\n",
      "Epoch 217/300\n",
      "Average training loss: 0.039937659346395066\n",
      "Average test loss: 0.0018286982596748406\n",
      "Epoch 218/300\n",
      "Average training loss: 0.039900226407580903\n",
      "Average test loss: 0.0018620398450228902\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0399493715663751\n",
      "Average test loss: 0.0018331650853570965\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03988959875702858\n",
      "Average test loss: 0.004259113166481257\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03980518174668153\n",
      "Average test loss: 0.0018419176375286447\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03982920136385494\n",
      "Average test loss: 0.0018193340990692378\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03977792171968354\n",
      "Average test loss: 0.0018341119739537439\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03979180873764886\n",
      "Average test loss: 0.0018419730574306514\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03970344050725301\n",
      "Average test loss: 0.0018253700820108254\n",
      "Epoch 226/300\n",
      "Average training loss: 0.039732963429556956\n",
      "Average test loss: 0.0018469737600535154\n",
      "Epoch 227/300\n",
      "Average training loss: 0.039739944107002684\n",
      "Average test loss: 0.0019292987554023664\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03976667187942399\n",
      "Average test loss: 0.001829568722906212\n",
      "Epoch 229/300\n",
      "Average training loss: 0.039667262391911616\n",
      "Average test loss: 0.0018250247928210432\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03959696574012438\n",
      "Average test loss: 0.0018369499920970864\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03958353324731191\n",
      "Average test loss: 0.0018052297830581665\n",
      "Epoch 232/300\n",
      "Average training loss: 0.039597496969832315\n",
      "Average training loss: 0.039507735613319606\n",
      "Average test loss: 0.001893287112729417\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03941768722401725\n",
      "Average test loss: 0.0018431705260235401\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03954067889518208\n",
      "Average test loss: 0.0018530464853263563\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03946468387213018\n",
      "Average test loss: 0.0018853516548665034\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03958919004599253\n",
      "Average test loss: 0.001841596246490048\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03938357768456141\n",
      "Average test loss: 0.001843708307016641\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03930239465170436\n",
      "Average test loss: 0.0018848213704509868\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03937251097295019\n",
      "Average test loss: 0.001838675660184688\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03939254982935058\n",
      "Average test loss: 0.0018558745897478527\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03922680594854885\n",
      "Average test loss: 0.0018598982201268276\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03921985330681006\n",
      "Average test loss: 0.0018562917382352882\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03928958075907495\n",
      "Average test loss: 0.0018927594649915894\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03921815026799838\n",
      "Average test loss: 0.0018304240605276493\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03929644390443961\n",
      "Average test loss: 0.0019033516463306216\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0392029597957929\n",
      "Average test loss: 0.0018605425633076164\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03920440029601256\n",
      "Average test loss: 0.0020959044011930623\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03917407837675677\n",
      "Average test loss: 0.0019266022546216845\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03902618054217762\n",
      "Average test loss: 0.0018698916526304352\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03905761449535688\n",
      "Average test loss: 0.0018410798834843768\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03905886891484261\n",
      "Average test loss: 0.0018459054231643676\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03904691870510578\n",
      "Average test loss: 0.0018595768505086502\n",
      "Epoch 257/300\n",
      "Average training loss: 0.039064185927311576\n",
      "Average test loss: 0.0018541913349181414\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03902270289262136\n",
      "Average test loss: 0.0018593013981978098\n",
      "Epoch 259/300\n",
      "Average training loss: 0.038922380617923205\n",
      "Average test loss: 0.0018674603454354737\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03895164210597674\n",
      "Average test loss: 0.001823484032932255\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03894248957766427\n",
      "Average test loss: 0.001846036243148976\n",
      "Epoch 262/300\n",
      "Average training loss: 0.038963277323378454\n",
      "Average test loss: 0.0018458974193781615\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03890622006853422\n",
      "Average test loss: 0.0018774202073820762\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03884166951146391\n",
      "Average test loss: 0.0019257006175402139\n",
      "Epoch 265/300\n",
      "Average training loss: 0.038840959827105205\n",
      "Average test loss: 0.0018643828877765271\n",
      "Epoch 266/300\n",
      "Average training loss: 0.038761327111058765\n",
      "Average test loss: 0.0018521679076883527\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03877651776538955\n",
      "Average test loss: 0.0018393774457896748\n",
      "Epoch 270/300\n",
      "Average training loss: 0.038773599876297846\n",
      "Average test loss: 0.0018404621914443042\n",
      "Epoch 271/300\n",
      "Average training loss: 0.038784968886110514\n",
      "Average test loss: 0.001909638181535734\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03872308649950557\n",
      "Average test loss: 0.0018567906334582302\n",
      "Epoch 273/300\n",
      "Average training loss: 0.038621222532457776\n",
      "Average test loss: 0.0018604573185245195\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03869651792777909\n",
      "Average test loss: 0.0018472904432564974\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03868543256322543\n",
      "Average test loss: 0.0018273238968104124\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03873175737924046\n",
      "Average test loss: 0.001849139476298458\n",
      "Epoch 277/300\n",
      "Average training loss: 0.038652736748258275\n",
      "Average test loss: 0.0018875589383145173\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03853795780075921\n",
      "Average test loss: 0.001866396671264536\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03860766030682458\n",
      "Average test loss: 0.0018825281347251601\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03861823187602891\n",
      "Average test loss: 0.0018604889884591103\n",
      "Epoch 281/300\n",
      "Average training loss: 0.038678097787830566\n",
      "Average test loss: 0.0018355274833738804\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03857534185714192\n",
      "Average test loss: 0.001848243596446183\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03855112201637692\n",
      "Average training loss: 0.03855159188972579\n",
      "Average test loss: 0.0018952437036981185\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03843741377856996\n",
      "Average test loss: 0.0019480910446080897\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03846392040782504\n",
      "Average test loss: 0.0018622175622731447\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03852983043260044\n",
      "Average test loss: 0.002025957792169518\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03838015953037474\n",
      "Average test loss: 0.001902774644187755\n",
      "Epoch 290/300\n",
      "Average training loss: 0.038438309490680694\n",
      "Average test loss: 0.001866222772333357\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03842218513621224\n",
      "Average test loss: 0.0018817116575729515\n",
      "Epoch 292/300\n",
      "Average training loss: 0.038401630106899474\n",
      "Average test loss: 0.0019712207530521685\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03848797949817446\n",
      "Average test loss: 0.0019349413509998057\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03827508037288984\n",
      "Average test loss: 0.0018743403270426724\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03824337433775266\n",
      "Average test loss: 0.0018917956919305855\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03828900921675894\n",
      "Average test loss: 0.001894252002446188\n",
      "Epoch 297/300\n",
      "Average training loss: 0.038332903938161\n",
      "Average test loss: 0.0018539661984476779\n",
      "Epoch 298/300\n",
      "Average training loss: 0.038289457907279334\n",
      "Average test loss: 0.0018537300812701385\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03826962889234225\n",
      "Average test loss: 0.0019024852226591773\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03819162983695666\n",
      "Average test loss: 0.001898256188051568\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive_Depth3-.025/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.91\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.82\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.01\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.05\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.08\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
