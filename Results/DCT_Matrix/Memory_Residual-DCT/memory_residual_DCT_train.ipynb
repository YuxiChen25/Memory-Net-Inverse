{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.ImageDataset import ImageDataset\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.060518795907497404\n",
      "Average test loss: 0.00489865992238952\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02350796465906832\n",
      "Average test loss: 0.004518384981693493\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02239646481639809\n",
      "Average test loss: 0.004418895064542691\n",
      "Epoch 4/300\n",
      "Average training loss: 0.021948022193378873\n",
      "Average test loss: 0.004344916076502866\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02168579330543677\n",
      "Average test loss: 0.0043069913325210415\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021484562228123345\n",
      "Average test loss: 0.004293905946736534\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02131906799475352\n",
      "Average test loss: 0.004241920863588651\n",
      "Epoch 8/300\n",
      "Average training loss: 0.021168390318751337\n",
      "Average test loss: 0.004226766019231744\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021064067408442497\n",
      "Average test loss: 0.004241270441975859\n",
      "Epoch 10/300\n",
      "Average training loss: 0.020966637955771552\n",
      "Average test loss: 0.004175386407309108\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02086125941740142\n",
      "Average test loss: 0.004161798477172852\n",
      "Epoch 12/300\n",
      "Average training loss: 0.020783105288942654\n",
      "Average test loss: 0.004151765881520179\n",
      "Epoch 13/300\n",
      "Average training loss: 0.020702471833262178\n",
      "Average test loss: 0.004134282555431128\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02061466675168938\n",
      "Average test loss: 0.004132006167951558\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020557017154163783\n",
      "Average test loss: 0.004107511417733299\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02048068389793237\n",
      "Average test loss: 0.004104279874927468\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02043534769448969\n",
      "Average test loss: 0.004090178430494335\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020379920505815084\n",
      "Average test loss: 0.004106807142082188\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02032183612883091\n",
      "Average test loss: 0.004083888180760874\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020277750382820765\n",
      "Average test loss: 0.004067446257919073\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02022404062913524\n",
      "Average test loss: 0.004073422031270133\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020182584111889203\n",
      "Average test loss: 0.004041601218076216\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02013466555542416\n",
      "Average test loss: 0.004028880054752032\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020091436773538588\n",
      "Average test loss: 0.004036343108862638\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020065752701626884\n",
      "Average test loss: 0.004030688077832262\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020020862148867714\n",
      "Average test loss: 0.00402958496246073\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01998553975423177\n",
      "Average test loss: 0.0040141771288795605\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01994745948414008\n",
      "Average test loss: 0.003998919484102064\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01991200692454974\n",
      "Average test loss: 0.004015399639391237\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01988451630373796\n",
      "Average test loss: 0.004029551186702318\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019861229025655323\n",
      "Average test loss: 0.003994843741982348\n",
      "Epoch 32/300\n",
      "Average training loss: 0.019827749896380635\n",
      "Average test loss: 0.003989235584520631\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019803581630190213\n",
      "Average test loss: 0.003982140666494767\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019777190576824876\n",
      "Average test loss: 0.003997257909013166\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019756054843465487\n",
      "Average test loss: 0.003986273532526361\n",
      "Epoch 36/300\n",
      "Average training loss: 0.019733241346147325\n",
      "Average test loss: 0.003992137202579114\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019701504120396243\n",
      "Average test loss: 0.003973647154867649\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019674896185596785\n",
      "Average test loss: 0.003967293865978718\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019664350934326648\n",
      "Average test loss: 0.003984913224975268\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019635824077659184\n",
      "Average test loss: 0.004014552864763472\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019616556821597946\n",
      "Average test loss: 0.003961714683721463\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019600382829705873\n",
      "Average test loss: 0.003959922240012222\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019578538816836147\n",
      "Average test loss: 0.003967050214194589\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019556006751954555\n",
      "Average test loss: 0.003965059423612223\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019544416084057756\n",
      "Average test loss: 0.003952958700143628\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019530559624234837\n",
      "Average test loss: 0.003961684834005105\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019508008184532324\n",
      "Average test loss: 0.003963397303182218\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0194838064354327\n",
      "Average test loss: 0.0039791116101874245\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019470304207669365\n",
      "Average test loss: 0.0039602032800515495\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019453273607624902\n",
      "Average test loss: 0.003973983185365796\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0194317125082016\n",
      "Average test loss: 0.0039611446847104366\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01942289874785476\n",
      "Average test loss: 0.003982806974814998\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01940269900692834\n",
      "Average test loss: 0.003976867724416984\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019376260257429546\n",
      "Average test loss: 0.003961812094267872\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019365754021538627\n",
      "Average test loss: 0.003957320771697494\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019354969650506974\n",
      "Average test loss: 0.0039429779390080105\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019336876665552456\n",
      "Average test loss: 0.003957730260160234\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01931832731763522\n",
      "Average test loss: 0.003968277675617072\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019303538274433877\n",
      "Average test loss: 0.003934489489015606\n",
      "Epoch 60/300\n",
      "Average training loss: 0.019283416522873773\n",
      "Average test loss: 0.003963566042482853\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019273944540984102\n",
      "Average test loss: 0.003958052138073577\n",
      "Epoch 62/300\n",
      "Average training loss: 0.019253770808378857\n",
      "Average test loss: 0.003974909767922428\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019246997613873745\n",
      "Average test loss: 0.003969544509011838\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01922810427347819\n",
      "Average test loss: 0.003946116129350331\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019219531065887877\n",
      "Average test loss: 0.004047070058683554\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01920883001718256\n",
      "Average test loss: 0.003980081364719404\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019185157106982337\n",
      "Average test loss: 0.003964719904379712\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019158713478181096\n",
      "Average test loss: 0.003946384115351571\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019156925012667973\n",
      "Average test loss: 0.003948214947349495\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019139728721645145\n",
      "Average test loss: 0.003988509931084182\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019122062399983406\n",
      "Average test loss: 0.00396473708955778\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019107498094439505\n",
      "Average test loss: 0.003966636095196009\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01910659360388915\n",
      "Average test loss: 0.0039609829472998775\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01907625990609328\n",
      "Average test loss: 0.003964987232349813\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019054934271507794\n",
      "Average test loss: 0.003954365809758505\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019039968720740742\n",
      "Average test loss: 0.003942898361840182\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019046311299006143\n",
      "Average test loss: 0.003951179461139771\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01902015692078405\n",
      "Average test loss: 0.003947402353812423\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019011983803576892\n",
      "Average test loss: 0.003993720502282182\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01898310923245218\n",
      "Average test loss: 0.003977812143249644\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018965760469436644\n",
      "Average test loss: 0.00397844416461885\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018961403569413556\n",
      "Average test loss: 0.004068725808213155\n",
      "Epoch 83/300\n",
      "Average training loss: 0.018945154084099664\n",
      "Average test loss: 0.003941536636609171\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018932921290397645\n",
      "Average test loss: 0.003964405481186178\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018909081233872308\n",
      "Average test loss: 0.003955461057523886\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01889233965675036\n",
      "Average test loss: 0.003963574079589712\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018886139166023996\n",
      "Average test loss: 0.003959718957543373\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018863284120957058\n",
      "Average test loss: 0.003974096375207107\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018860718642671904\n",
      "Average test loss: 0.0039867923087957835\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018830618646409777\n",
      "Average test loss: 0.003988927768129442\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018842028553287187\n",
      "Average test loss: 0.004031102874626716\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018816799461841584\n",
      "Average test loss: 0.0039733403503066965\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01878563830256462\n",
      "Average test loss: 0.0040141216619975035\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018784353547626073\n",
      "Average test loss: 0.0039824420875973175\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01876484265840716\n",
      "Average test loss: 0.003995304780287875\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018754466580020057\n",
      "Average test loss: 0.003998879647917218\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018732695745097266\n",
      "Average test loss: 0.003977826437188519\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01872478218211068\n",
      "Average test loss: 0.003986877585450808\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01869411397476991\n",
      "Average test loss: 0.0040494851681093375\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01869836946328481\n",
      "Average test loss: 0.004021903780599435\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018692338825099998\n",
      "Average test loss: 0.003988307739297549\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018673088437981076\n",
      "Average test loss: 0.004001889655573501\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018650643527507782\n",
      "Average test loss: 0.004003663100923101\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01863741521868441\n",
      "Average test loss: 0.004044390394869778\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01862548808587922\n",
      "Average test loss: 0.00396306124350263\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018609585949116283\n",
      "Average test loss: 0.004127861356569661\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018591698287261857\n",
      "Average test loss: 0.004076208087305228\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018587530955672265\n",
      "Average test loss: 0.004043755845891105\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018578185063269404\n",
      "Average test loss: 0.004124113912383715\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01856364929676056\n",
      "Average test loss: 0.004022476149515973\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01854794360531701\n",
      "Average test loss: 0.004036084017198947\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01852583107848962\n",
      "Average test loss: 0.003999539382755757\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018517501963509455\n",
      "Average test loss: 0.004045407385047939\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01851484335379468\n",
      "Average test loss: 0.0040959579334076904\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018481013614270424\n",
      "Average test loss: 0.004004061850822634\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018493722756703693\n",
      "Average test loss: 0.004048636614862416\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01846488475220071\n",
      "Average test loss: 0.004044923600844211\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018460492146511874\n",
      "Average test loss: 0.00408943100232217\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018441571480698055\n",
      "Average test loss: 0.004055980033344692\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0184256280048026\n",
      "Average test loss: 0.0040997257687979275\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01842028300629722\n",
      "Average test loss: 0.004051457896828652\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01838516378071573\n",
      "Average test loss: 0.004014148311068614\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01839061713880963\n",
      "Average test loss: 0.004002245785875453\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018382082184983623\n",
      "Average test loss: 0.004048510479016437\n",
      "Epoch 125/300\n",
      "Average training loss: 0.018372675196992025\n",
      "Average test loss: 0.0041772632660965125\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01835229615204864\n",
      "Average test loss: 0.004027885928129156\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01834547653959857\n",
      "Average test loss: 0.004058408453646634\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018317565995785923\n",
      "Average test loss: 0.004059338113086091\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01832153698305289\n",
      "Average test loss: 0.0040687458891835475\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018317627373668884\n",
      "Average test loss: 0.003998152673244476\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018300378837519223\n",
      "Average test loss: 0.004071022475345267\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018278054603272014\n",
      "Average test loss: 0.004046573207196262\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018267211241854563\n",
      "Average test loss: 0.0040183713506493305\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018258190835515657\n",
      "Average test loss: 0.004037683809383048\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018249420479767853\n",
      "Average test loss: 0.00405802648431725\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01823603255715635\n",
      "Average test loss: 0.004114939625685414\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018234548735949728\n",
      "Average test loss: 0.004210587843424744\n",
      "Epoch 138/300\n",
      "Average training loss: 0.018205266339911354\n",
      "Average test loss: 0.004038291009142995\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01820244075357914\n",
      "Average test loss: 0.00408446636216508\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01819560680290063\n",
      "Average test loss: 0.004185480740749174\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018180901338656744\n",
      "Average test loss: 0.004131758702711927\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018173668411042954\n",
      "Average test loss: 0.004063648213528924\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018156298879120084\n",
      "Average test loss: 0.004089907692331407\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018158613002134692\n",
      "Average test loss: 0.004183089188403553\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018133157547977237\n",
      "Average test loss: 0.004111500037213166\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018126054238114093\n",
      "Average test loss: 0.004083581595578128\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018114026236865255\n",
      "Average test loss: 0.004038081037294534\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018120222015513315\n",
      "Average test loss: 0.00413379893783066\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01808975649211142\n",
      "Average test loss: 0.004032982554079758\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01808326185743014\n",
      "Average test loss: 0.004103228113096621\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018075948536396026\n",
      "Average test loss: 0.00415965266360177\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01806886302596993\n",
      "Average test loss: 0.0041598716258174845\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018070597482224306\n",
      "Average test loss: 0.004128517077200943\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01804799576269256\n",
      "Average test loss: 0.004087522102933791\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01803209764096472\n",
      "Average test loss: 0.00414537073754602\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018016542808877097\n",
      "Average test loss: 0.004069698520625631\n",
      "Epoch 157/300\n",
      "Average training loss: 0.018017010953691272\n",
      "Average test loss: 0.004092340568080544\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018015177598430052\n",
      "Average test loss: 0.00411713169556525\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018000259457363024\n",
      "Average test loss: 0.004115547560983234\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01799770407627026\n",
      "Average test loss: 0.004115888899813095\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01798075636393494\n",
      "Average test loss: 0.004122376165870164\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01797086599055264\n",
      "Average test loss: 0.0040532019072108795\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017957590070863564\n",
      "Average test loss: 0.004081877140535249\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01796767985324065\n",
      "Average test loss: 0.004149985405512982\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017947099446422525\n",
      "Average test loss: 0.004052334921641482\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017931277124418152\n",
      "Average test loss: 0.004130732123843498\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01792768866320451\n",
      "Average test loss: 0.004203377458370394\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017943227324220868\n",
      "Average test loss: 0.00418993398340212\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017911291252407764\n",
      "Average test loss: 0.004092900661544667\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017902886549631753\n",
      "Average test loss: 0.004116207347975837\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017892839449975224\n",
      "Average test loss: 0.004111591790699296\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017902657123903435\n",
      "Average test loss: 0.004087235791608692\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017875340018835333\n",
      "Average test loss: 0.004174346547159884\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017879321771363418\n",
      "Average test loss: 0.004226748633922802\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017863465731342634\n",
      "Average test loss: 0.004107561139596833\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017847999236649936\n",
      "Average test loss: 0.0043366864919662474\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017840628066824543\n",
      "Average test loss: 0.004193628812001811\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01783742179307673\n",
      "Average test loss: 0.004139860150093834\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0178251474549373\n",
      "Average test loss: 0.004061283716311057\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017826528244548372\n",
      "Average test loss: 0.004176130934928854\n",
      "Epoch 181/300\n",
      "Average training loss: 0.017818644199106427\n",
      "Average test loss: 0.0042104485157049365\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01780508694383833\n",
      "Average test loss: 0.004127421864515378\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01780545833044582\n",
      "Average test loss: 0.004099655790047513\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017797392116652596\n",
      "Average test loss: 0.004214838963001967\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01779976757698589\n",
      "Average test loss: 0.004124359636671013\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01777907327397002\n",
      "Average test loss: 0.004225777310629686\n",
      "Epoch 187/300\n",
      "Average training loss: 0.017766223159929116\n",
      "Average test loss: 0.0041723611332062215\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017765371201766862\n",
      "Average test loss: 0.0041105227915363175\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017758702504965994\n",
      "Average test loss: 0.004152517702016565\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017749359567960102\n",
      "Average test loss: 0.004216743406943149\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017731814561618698\n",
      "Average test loss: 0.004100873188840018\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017735252941648166\n",
      "Average test loss: 0.004194068259456092\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017719191444416842\n",
      "Average test loss: 0.0042067609044412774\n",
      "Epoch 194/300\n",
      "Average training loss: 0.017720315728750494\n",
      "Average test loss: 0.004193134814500809\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017708965621888638\n",
      "Average test loss: 0.004217451786622405\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017717040714290408\n",
      "Average test loss: 0.004161969241375725\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017702196743753222\n",
      "Average test loss: 0.004107732383327352\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017695980961124104\n",
      "Average test loss: 0.004211997910092274\n",
      "Epoch 199/300\n",
      "Average training loss: 0.017689559042453767\n",
      "Average test loss: 0.004224191131898099\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017688922671808136\n",
      "Average test loss: 0.004180351960990164\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017672556617193753\n",
      "Average test loss: 0.004146548075808419\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017662684833010037\n",
      "Average test loss: 0.004230877901117007\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017674254783325724\n",
      "Average test loss: 0.004165062235254381\n",
      "Epoch 204/300\n",
      "Average training loss: 0.017662330383227932\n",
      "Average test loss: 0.004135122996237543\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017635790701541636\n",
      "Average test loss: 0.004221986511929168\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017645570650696753\n",
      "Average test loss: 0.004166882244042224\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0176280364708768\n",
      "Average test loss: 0.004116811292866866\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01762491325289011\n",
      "Average test loss: 0.004171632347628474\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017619068438808123\n",
      "Average test loss: 0.004150814577730166\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017629667434427475\n",
      "Average test loss: 0.004228228706452582\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01760236650208632\n",
      "Average test loss: 0.004289196934551\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01759419337908427\n",
      "Average test loss: 0.0043636727689041034\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017592253865881098\n",
      "Average test loss: 0.004237434078007937\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017589855459001328\n",
      "Average test loss: 0.00409840991658469\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017573420310186015\n",
      "Average test loss: 0.004153929182638724\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01758830086555746\n",
      "Average test loss: 0.004348366044668687\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017565178210536637\n",
      "Average test loss: 0.0042369123937355145\n",
      "Epoch 218/300\n",
      "Average training loss: 0.017560557082295418\n",
      "Average test loss: 0.004211623369819588\n",
      "Epoch 219/300\n",
      "Average training loss: 0.017568151018685765\n",
      "Average test loss: 0.0041286398662875095\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017548001628783016\n",
      "Average test loss: 0.004286408938674463\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017550213499201668\n",
      "Average test loss: 0.004125104776687092\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017541717040869924\n",
      "Average test loss: 0.004282746466290619\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017545796521008013\n",
      "Average test loss: 0.004215108571988013\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017534156906108062\n",
      "Average test loss: 0.004213528053214153\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01754284696115388\n",
      "Average test loss: 0.0042212885674089195\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01751803748144044\n",
      "Average test loss: 0.004165203367256456\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01751894406725963\n",
      "Average test loss: 0.004212754954894384\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017506433897548253\n",
      "Average test loss: 0.004372737085653676\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017504174446894064\n",
      "Average test loss: 0.004116188693791628\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017493089237146906\n",
      "Average test loss: 0.004230982177373436\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017497748296293948\n",
      "Average test loss: 0.004214435974549916\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01749441865666045\n",
      "Average test loss: 0.0041947337140639624\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01749062532517645\n",
      "Average test loss: 0.004261190291494131\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017478569895029068\n",
      "Average test loss: 0.004250631637250384\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01747437737385432\n",
      "Average test loss: 0.0041468734025127355\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017465948985682593\n",
      "Average test loss: 0.004116289358999994\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017471152126789093\n",
      "Average test loss: 0.004237188615732723\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01745206682384014\n",
      "Average test loss: 0.0042781239412724974\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01746327130496502\n",
      "Average test loss: 0.004142166001929177\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017452073388629488\n",
      "Average test loss: 0.004226548996236589\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0174240859531694\n",
      "Average test loss: 0.004137637406173679\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01743281563040283\n",
      "Average test loss: 0.0041025592815130945\n",
      "Epoch 243/300\n",
      "Average training loss: 0.017423868865602547\n",
      "Average test loss: 0.0042517872959789305\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01743339294112391\n",
      "Average test loss: 0.00415101989896761\n",
      "Epoch 245/300\n",
      "Average training loss: 0.017417614524563155\n",
      "Average test loss: 0.004178535988968279\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017413530160155562\n",
      "Average test loss: 0.004253228106846412\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017416258689430024\n",
      "Average test loss: 0.004175300482660532\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017393635976645682\n",
      "Average test loss: 0.004213289608144098\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01740261194523838\n",
      "Average test loss: 0.004220476136853298\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017405565748612087\n",
      "Average test loss: 0.004250645061333974\n",
      "Epoch 251/300\n",
      "Average training loss: 0.017401521826369896\n",
      "Average test loss: 0.0043019704719384514\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01737981162634161\n",
      "Average test loss: 0.004311223849654198\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017377048775553704\n",
      "Average test loss: 0.0042058458835300475\n",
      "Epoch 254/300\n",
      "Average training loss: 0.017385926586058406\n",
      "Average test loss: 0.004121869319015079\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01737338417520126\n",
      "Average test loss: 0.004228559203652872\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01735800531672107\n",
      "Average test loss: 0.0042815276210506755\n",
      "Epoch 257/300\n",
      "Average training loss: 0.017363644626405505\n",
      "Average test loss: 0.004275188674943315\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017372504967782233\n",
      "Average test loss: 0.004213016469859414\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017363133640752898\n",
      "Average test loss: 0.004432440269738436\n",
      "Epoch 260/300\n",
      "Average training loss: 0.017349709192911785\n",
      "Average test loss: 0.004222959968778822\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01735191456311279\n",
      "Average test loss: 0.004212410396378902\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017352670672039192\n",
      "Average test loss: 0.004303452728109228\n",
      "Epoch 263/300\n",
      "Average training loss: 0.017324909205238024\n",
      "Average test loss: 0.004240214210003614\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017339268065161176\n",
      "Average test loss: 0.004352974774936835\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01733307066063086\n",
      "Average test loss: 0.004299531297344301\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01731533294916153\n",
      "Average test loss: 0.004238081593480375\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017328349272410074\n",
      "Average test loss: 0.004209615110523171\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017319742729266483\n",
      "Average test loss: 0.004231799767663081\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017314334885941612\n",
      "Average test loss: 0.004263768553319905\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01731052913185623\n",
      "Average test loss: 0.004229529227026635\n",
      "Epoch 271/300\n",
      "Average training loss: 0.017312830868694517\n",
      "Average test loss: 0.004278012846079138\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01730716297361586\n",
      "Average test loss: 0.00432415625349515\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01729157504397962\n",
      "Average test loss: 0.004232089259558254\n",
      "Epoch 274/300\n",
      "Average training loss: 0.017281681996252803\n",
      "Average test loss: 0.004146100365039375\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017294497749871678\n",
      "Average test loss: 0.004400504890415403\n",
      "Epoch 276/300\n",
      "Average training loss: 0.017273384909662935\n",
      "Average test loss: 0.00418425007040302\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017285617935988637\n",
      "Average test loss: 0.004227775080750386\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01726299927300877\n",
      "Average test loss: 0.004137362325150106\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017273924994799825\n",
      "Average test loss: 0.004454022374004126\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01728258707953824\n",
      "Average test loss: 0.004118058680660195\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01725457407699691\n",
      "Average test loss: 0.004277454139457809\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017254256068004504\n",
      "Average test loss: 0.004255898899502225\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017249121877882214\n",
      "Average test loss: 0.004214071436681681\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01724641279545095\n",
      "Average test loss: 0.004246613983892732\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017236971739265652\n",
      "Average test loss: 0.004131354955749379\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017231736780868635\n",
      "Average test loss: 0.004287927913789948\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017239290416240692\n",
      "Average test loss: 0.004384418554604054\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01722941275851594\n",
      "Average test loss: 0.004354756142737137\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017232718791398736\n",
      "Average test loss: 0.00420782213865055\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017220428043769467\n",
      "Average test loss: 0.004199952493111292\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017212645524077946\n",
      "Average test loss: 0.004300426693012317\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01722383122642835\n",
      "Average test loss: 0.0043000268482913575\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017225034976998965\n",
      "Average test loss: 0.004335405450728204\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017207703593704434\n",
      "Average test loss: 0.004304498215102487\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017225795044667192\n",
      "Average test loss: 0.0042368472243348755\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017199872026840847\n",
      "Average test loss: 0.004352411956422859\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017207003499070804\n",
      "Average test loss: 0.004218903678158919\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017184188370903332\n",
      "Average test loss: 0.004126360030016965\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017191710250245202\n",
      "Average test loss: 0.0042484824663649\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017192695094479456\n",
      "Average test loss: 0.004255347014715275\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.053145335117975874\n",
      "Average test loss: 0.004196182490636905\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02053924065993892\n",
      "Average test loss: 0.004155879570378197\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019516073594490688\n",
      "Average test loss: 0.003842131998389959\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019006363873680432\n",
      "Average test loss: 0.0037404932555639083\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018624025757114093\n",
      "Average test loss: 0.003647646074493726\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018306231000357203\n",
      "Average test loss: 0.0035914388141698307\n",
      "Epoch 7/300\n",
      "Average training loss: 0.018032811533245777\n",
      "Average test loss: 0.0035861690926055115\n",
      "Epoch 8/300\n",
      "Average training loss: 0.017772628654208446\n",
      "Average test loss: 0.0035378778415421645\n",
      "Epoch 9/300\n",
      "Average training loss: 0.017561794677542316\n",
      "Average test loss: 0.0034717176697320407\n",
      "Epoch 10/300\n",
      "Average training loss: 0.017362225722935463\n",
      "Average test loss: 0.0034076274540275333\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01717409527964062\n",
      "Average test loss: 0.00336823427842723\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017001197903520532\n",
      "Average test loss: 0.003370065794636806\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016860472170015177\n",
      "Average test loss: 0.003298448466592365\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01671582213209735\n",
      "Average test loss: 0.0032720848462647863\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016600326735112404\n",
      "Average test loss: 0.003243512896200021\n",
      "Epoch 16/300\n",
      "Average training loss: 0.016480033514400323\n",
      "Average test loss: 0.0032148308563563557\n",
      "Epoch 17/300\n",
      "Average training loss: 0.016360162063605254\n",
      "Average test loss: 0.0031938710659742357\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01627471252448029\n",
      "Average test loss: 0.0031926414534035655\n",
      "Epoch 19/300\n",
      "Average training loss: 0.016160940592073732\n",
      "Average test loss: 0.003275337732914421\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01608086580865913\n",
      "Average test loss: 0.0031602688637665578\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01599165810727411\n",
      "Average test loss: 0.003116238347358174\n",
      "Epoch 22/300\n",
      "Average training loss: 0.015923950057062836\n",
      "Average test loss: 0.0030934149966471724\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01583391197025776\n",
      "Average test loss: 0.0031229310721779865\n",
      "Epoch 24/300\n",
      "Average training loss: 0.015769785961343184\n",
      "Average test loss: 0.0030908624002089105\n",
      "Epoch 25/300\n",
      "Average training loss: 0.015718101049462953\n",
      "Average test loss: 0.003096466706858741\n",
      "Epoch 26/300\n",
      "Average training loss: 0.015653413702216415\n",
      "Average test loss: 0.003048317406533493\n",
      "Epoch 27/300\n",
      "Average training loss: 0.015594510074290965\n",
      "Average test loss: 0.0030667768464320237\n",
      "Epoch 28/300\n",
      "Average training loss: 0.015540753230452538\n",
      "Average test loss: 0.00305768178941475\n",
      "Epoch 29/300\n",
      "Average training loss: 0.015485324025154113\n",
      "Average test loss: 0.0030406197166691225\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015441378915475474\n",
      "Average test loss: 0.0030297498202158345\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015395570847723219\n",
      "Average test loss: 0.0030151395637335048\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015353583875629637\n",
      "Average test loss: 0.0030114522489408653\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015318340005146133\n",
      "Average test loss: 0.003025359190793501\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01527891740616825\n",
      "Average test loss: 0.003052367137124141\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01523637293279171\n",
      "Average test loss: 0.0030046078419933716\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015200792113939921\n",
      "Average test loss: 0.003073323601235946\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015166250468128258\n",
      "Average test loss: 0.0030044324124852816\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015120802285770575\n",
      "Average test loss: 0.0029926233288521567\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015095538552436564\n",
      "Average test loss: 0.002982489359461599\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015061097716291745\n",
      "Average test loss: 0.003006897044264608\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01503359808276097\n",
      "Average test loss: 0.002967180797416303\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01500017203638951\n",
      "Average test loss: 0.0029632311620646054\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014967738094429176\n",
      "Average test loss: 0.0029803970294694105\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014945567933221658\n",
      "Average test loss: 0.002987315520333747\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014919092935820421\n",
      "Average test loss: 0.0030012878467225368\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01487842156406906\n",
      "Average test loss: 0.002955338205728266\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01485812620487478\n",
      "Average test loss: 0.002953976435172889\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014828276336193085\n",
      "Average test loss: 0.0029921337755190003\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014802925672796037\n",
      "Average test loss: 0.002987344817361898\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014782838962144322\n",
      "Average test loss: 0.002960404897729556\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014746630952590042\n",
      "Average test loss: 0.00297230868289868\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014721074504984749\n",
      "Average test loss: 0.0029494551660286055\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014692201253440645\n",
      "Average test loss: 0.0029550377641701037\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014675826497375965\n",
      "Average test loss: 0.0029773372017468016\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014651630360219214\n",
      "Average test loss: 0.002981880683865812\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014635029684338306\n",
      "Average test loss: 0.0029994607273903157\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014619539429744085\n",
      "Average test loss: 0.003024173009519776\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014559816040098667\n",
      "Average test loss: 0.0029639732049157224\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014551961776283052\n",
      "Average test loss: 0.0029682397784458266\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014525481821762191\n",
      "Average test loss: 0.002970945438577069\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014511160631974537\n",
      "Average test loss: 0.002944175072842174\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014491944702135192\n",
      "Average test loss: 0.002963363463472989\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014466775353584024\n",
      "Average test loss: 0.003022590261987514\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01445195404274596\n",
      "Average test loss: 0.003028584097408586\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014415468374888102\n",
      "Average test loss: 0.0029460599828097555\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014395097826917965\n",
      "Average test loss: 0.003002286025426454\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014374334669775434\n",
      "Average test loss: 0.0030202639570666686\n",
      "Epoch 68/300\n",
      "Average training loss: 0.014354536106189092\n",
      "Average test loss: 0.0029661698353787265\n",
      "Epoch 69/300\n",
      "Average training loss: 0.014352029879060056\n",
      "Average test loss: 0.003003091541636321\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014320446464750503\n",
      "Average test loss: 0.0029996664776570266\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014288199700415134\n",
      "Average test loss: 0.002965418794709775\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014279837579362922\n",
      "Average test loss: 0.0030022328798141743\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014250635772115654\n",
      "Average test loss: 0.0029838809677296216\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014232756589021948\n",
      "Average test loss: 0.003008450720459223\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014208791395856275\n",
      "Average test loss: 0.0029570394052813452\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014207925855285592\n",
      "Average test loss: 0.0029669736789332494\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014171659597920046\n",
      "Average test loss: 0.003000282705347571\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014168311887317233\n",
      "Average test loss: 0.0029699940938088627\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014155880472726291\n",
      "Average test loss: 0.003039002601471212\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014120519671589137\n",
      "Average test loss: 0.0030277879842453533\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014097954374220636\n",
      "Average test loss: 0.0030002856188350254\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014083914408253299\n",
      "Average test loss: 0.003116201768732733\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014072874056796233\n",
      "Average test loss: 0.002966865703773995\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014051269022954834\n",
      "Average test loss: 0.0029929982676274247\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014032376101447476\n",
      "Average test loss: 0.003011336997151375\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014024121431840791\n",
      "Average test loss: 0.002988537333181335\n",
      "Epoch 87/300\n",
      "Average training loss: 0.013998132970598008\n",
      "Average test loss: 0.003006703040873011\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01398735863218705\n",
      "Average test loss: 0.003112432392521037\n",
      "Epoch 89/300\n",
      "Average training loss: 0.013964328760074245\n",
      "Average test loss: 0.003112005912595325\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013944883479840225\n",
      "Average test loss: 0.003036950470879674\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013949100658297539\n",
      "Average test loss: 0.0029964595125574205\n",
      "Epoch 92/300\n",
      "Average training loss: 0.013919229677981801\n",
      "Average test loss: 0.002993544957290093\n",
      "Epoch 93/300\n",
      "Average training loss: 0.013904648127655189\n",
      "Average test loss: 0.003117340701735682\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013892984348866674\n",
      "Average test loss: 0.0030136023687405717\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013892356635795699\n",
      "Average test loss: 0.0030106859761807655\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013852472275495529\n",
      "Average test loss: 0.003034854597929451\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013844432344867123\n",
      "Average test loss: 0.0029888844071990915\n",
      "Epoch 98/300\n",
      "Average training loss: 0.013824418984353543\n",
      "Average test loss: 0.003000011575925681\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013804603982302878\n",
      "Average test loss: 0.0030666786366038853\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013812172647151682\n",
      "Average test loss: 0.003034857277654939\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013786467394067182\n",
      "Average test loss: 0.0030366820379470784\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01378049540023009\n",
      "Average test loss: 0.003057699150716265\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01375995662311713\n",
      "Average test loss: 0.0029668507120675512\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013744463584489292\n",
      "Average test loss: 0.003006834648342596\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013736032982667287\n",
      "Average test loss: 0.0030769949942413303\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013716815878947576\n",
      "Average test loss: 0.00299675672998031\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01369384440448549\n",
      "Average test loss: 0.003103143828610579\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013697050293286642\n",
      "Average test loss: 0.0030524959224793647\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013677747592329979\n",
      "Average test loss: 0.003011499362273349\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013656723405751917\n",
      "Average test loss: 0.0030260595439208877\n",
      "Epoch 111/300\n",
      "Average training loss: 0.013659139786329535\n",
      "Average test loss: 0.0030920000105268425\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01363223693271478\n",
      "Average test loss: 0.00302707172350751\n",
      "Epoch 113/300\n",
      "Average training loss: 0.013629848776592148\n",
      "Average test loss: 0.0030881940780414477\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013620805509388446\n",
      "Average test loss: 0.0029978481990595657\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013615450540350543\n",
      "Average test loss: 0.00310184485051367\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013596411703361405\n",
      "Average test loss: 0.003058799995109439\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013584994901385572\n",
      "Average test loss: 0.0030988048699994884\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013561031310094727\n",
      "Average test loss: 0.003078913711425331\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013558242686920696\n",
      "Average test loss: 0.0032033089022669528\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013555299922823906\n",
      "Average test loss: 0.0030618477803137567\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013533111390968165\n",
      "Average test loss: 0.003156948603068789\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01351738332460324\n",
      "Average test loss: 0.003101826459583309\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013515698318266206\n",
      "Average test loss: 0.0032068699192669657\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013500604727201992\n",
      "Average test loss: 0.0030548344552516937\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013508454486727715\n",
      "Average test loss: 0.0030438322805696064\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013474010272986359\n",
      "Average test loss: 0.003059507632214162\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013473152688807912\n",
      "Average test loss: 0.0031315429436249865\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01345467132412725\n",
      "Average test loss: 0.0030874630701210764\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013456392318010331\n",
      "Average test loss: 0.0031070933101905716\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01344050298879544\n",
      "Average test loss: 0.0030691553304592768\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013415546580321258\n",
      "Average test loss: 0.0031014728997316625\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013423770546085305\n",
      "Average test loss: 0.0032051836852398184\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013434973138901922\n",
      "Average test loss: 0.0032096706744697357\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013396164024041759\n",
      "Average test loss: 0.00315512194380992\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013406346919635931\n",
      "Average test loss: 0.0030658329023669165\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013379389882087707\n",
      "Average test loss: 0.003084571131815513\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013372685873674022\n",
      "Average test loss: 0.003056190900504589\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01335971451881859\n",
      "Average test loss: 0.0031425445748286115\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013359294712543487\n",
      "Average test loss: 0.0032194594660152993\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013339674230251047\n",
      "Average test loss: 0.003387993874442246\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013328159550825754\n",
      "Average test loss: 0.0031723977165917555\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013328341165350543\n",
      "Average test loss: 0.003426819576778346\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013317534863948823\n",
      "Average test loss: 0.003090702936053276\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013302614691356818\n",
      "Average test loss: 0.003033285609756907\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013303005129098893\n",
      "Average test loss: 0.003183407609247499\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013297559894621372\n",
      "Average test loss: 0.003098177001501123\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013288095066944759\n",
      "Average test loss: 0.0031150376411775748\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013298551385601362\n",
      "Average test loss: 0.0030992589272144768\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0132604515204827\n",
      "Average test loss: 0.0031453775314407217\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013248836571971576\n",
      "Average test loss: 0.003259871670148439\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013262145823074712\n",
      "Average test loss: 0.003078770815084378\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013241454280912876\n",
      "Average test loss: 0.0031031336298005447\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013248268652293418\n",
      "Average test loss: 0.0030821698517021208\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013234306250181464\n",
      "Average test loss: 0.0031909580886777906\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013223879362973902\n",
      "Average test loss: 0.003112252545853456\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013227406951288382\n",
      "Average test loss: 0.0032457528087413974\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013217771113746696\n",
      "Average test loss: 0.0031038314580089515\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013200563640230232\n",
      "Average test loss: 0.0031484119382997355\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013190722731252512\n",
      "Average test loss: 0.0031076066735097104\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013181310041911072\n",
      "Average test loss: 0.0030996912378403875\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013180207567910353\n",
      "Average test loss: 0.003152929385089212\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01314743352929751\n",
      "Average test loss: 0.003067374133194486\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013158367738127709\n",
      "Average test loss: 0.003198068629950285\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01316200169424216\n",
      "Average test loss: 0.003107255940635999\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01316168786999252\n",
      "Average test loss: 0.003192685446805424\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013135046415858798\n",
      "Average test loss: 0.003145264970759551\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013139802064332698\n",
      "Average test loss: 0.003149277480526103\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013138313158518738\n",
      "Average test loss: 0.0031779652802894514\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013120365047620403\n",
      "Average test loss: 0.003158118691502346\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01311114081988732\n",
      "Average test loss: 0.003058484229569634\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013102169060044818\n",
      "Average test loss: 0.003108445897594922\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01311156172139777\n",
      "Average test loss: 0.0032463071124835146\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013099361922178003\n",
      "Average test loss: 0.003206460907848345\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013094934326906999\n",
      "Average test loss: 0.0031104796417057512\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013082282895843188\n",
      "Average test loss: 0.0031319605039639604\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013092105359666878\n",
      "Average test loss: 0.0031893238365236255\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013060724352796873\n",
      "Average test loss: 0.003142604240940677\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01306259655704101\n",
      "Average test loss: 0.0032180304762182963\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013068176018695036\n",
      "Average test loss: 0.003170931439846754\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013051272878216372\n",
      "Average test loss: 0.0031062091949085394\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01304109006954564\n",
      "Average test loss: 0.0031999119677477415\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013038558142052757\n",
      "Average test loss: 0.0031218545488599274\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013059442967176437\n",
      "Average test loss: 0.003149486442406972\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013026723283860418\n",
      "Average test loss: 0.0031838153950456117\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013009042865700192\n",
      "Average test loss: 0.003081569872589575\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013051022829280959\n",
      "Average test loss: 0.003276069115433428\n",
      "Epoch 187/300\n",
      "Average training loss: 0.012995169565909438\n",
      "Average test loss: 0.003130061576763789\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013009470471905338\n",
      "Average test loss: 0.003214647557793392\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01299264858580298\n",
      "Average test loss: 0.003143831302308374\n",
      "Epoch 190/300\n",
      "Average training loss: 0.012972449790272449\n",
      "Average test loss: 0.0031418580011361177\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012984395016398694\n",
      "Average test loss: 0.003129806275582976\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012989759613242413\n",
      "Average test loss: 0.003117483940389421\n",
      "Epoch 193/300\n",
      "Average training loss: 0.012997390584813223\n",
      "Average test loss: 0.0033007332206600242\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012960456137028006\n",
      "Average test loss: 0.0032065273709595203\n",
      "Epoch 195/300\n",
      "Average training loss: 0.012970923187004196\n",
      "Average test loss: 0.0031162846266395514\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012966155926386516\n",
      "Average test loss: 0.0031919826832082538\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012945828648077118\n",
      "Average test loss: 0.0034222643793457084\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012971899318198364\n",
      "Average test loss: 0.0031542744864192273\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01294382871604628\n",
      "Average test loss: 0.0032012443190647497\n",
      "Epoch 200/300\n",
      "Average training loss: 0.012919941497345766\n",
      "Average test loss: 0.0031881141470124323\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012935098104178906\n",
      "Average test loss: 0.0031678496247364413\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01292282735887501\n",
      "Average test loss: 0.003144643417456084\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012922319902728002\n",
      "Average test loss: 0.0031823957328581147\n",
      "Epoch 204/300\n",
      "Average training loss: 0.012916063082714875\n",
      "Average test loss: 0.003262603481196695\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012927411945329773\n",
      "Average test loss: 0.003121537648141384\n",
      "Epoch 206/300\n",
      "Average training loss: 0.012935328303111924\n",
      "Average test loss: 0.0031287948874135813\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012907463427219126\n",
      "Average test loss: 0.0031221010624948474\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012890471767220232\n",
      "Average test loss: 0.0034391884768588676\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012891354500833485\n",
      "Average test loss: 0.0031632149310575593\n",
      "Epoch 210/300\n",
      "Average training loss: 0.012911501195695665\n",
      "Average test loss: 0.003274154675089651\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012884663707680172\n",
      "Average test loss: 0.0030713190908233327\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012879169108139145\n",
      "Average test loss: 0.00310226248535845\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012868763444324334\n",
      "Average test loss: 0.003192666412020723\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012870951218737497\n",
      "Average test loss: 0.0031922731469902726\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012876471286846531\n",
      "Average test loss: 0.0033219518727726407\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012884113649941154\n",
      "Average test loss: 0.0033259298596531152\n",
      "Epoch 217/300\n",
      "Average training loss: 0.012876778010692862\n",
      "Average test loss: 0.003199609168908662\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012878942161798477\n",
      "Average test loss: 0.0033432371368010837\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012848812355763382\n",
      "Average test loss: 0.0032293018543471893\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01283159105396933\n",
      "Average test loss: 0.003079585725027654\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012834821129010783\n",
      "Average test loss: 0.0031063186021314726\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012822866469621658\n",
      "Average test loss: 0.003233214140559236\n",
      "Epoch 223/300\n",
      "Average training loss: 0.012819832978149256\n",
      "Average test loss: 0.0031731497939262126\n",
      "Epoch 224/300\n",
      "Average training loss: 0.012840124617020289\n",
      "Average test loss: 0.0031717392808447284\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012827465965516037\n",
      "Average test loss: 0.003114869359259804\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012817547995183204\n",
      "Average test loss: 0.003242618139419291\n",
      "Epoch 227/300\n",
      "Average training loss: 0.012806154179076353\n",
      "Average test loss: 0.003151405941488014\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012820284974243906\n",
      "Average test loss: 0.0032113522787888846\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012804850353962845\n",
      "Average test loss: 0.0031455838433984253\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01280656277967824\n",
      "Average test loss: 0.0031380956324024334\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012804419717854924\n",
      "Average test loss: 0.0032732042264607217\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012797307450738218\n",
      "Average test loss: 0.0031467314335621066\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012795046586957242\n",
      "Average test loss: 0.003230625580168433\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012795406637920274\n",
      "Average test loss: 0.0031583704149557483\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01277304747617907\n",
      "Average test loss: 0.0031689245349003207\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012773635985950629\n",
      "Average test loss: 0.0032936106990608905\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012772573756674925\n",
      "Average test loss: 0.003250947987039884\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01277576105379396\n",
      "Average test loss: 0.0032316448653323783\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012769092566437192\n",
      "Average test loss: 0.0031734123122360972\n",
      "Epoch 240/300\n",
      "Average training loss: 0.012772018257114623\n",
      "Average test loss: 0.003147059430885646\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012771968714065022\n",
      "Average test loss: 0.0032295292764902114\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012768355114592447\n",
      "Average test loss: 0.0032039638691478307\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012758023068308831\n",
      "Average test loss: 0.003114938761625025\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012760702670448357\n",
      "Average test loss: 0.0031686778459697963\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01274834372351567\n",
      "Average test loss: 0.0032159030955905715\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01274700537075599\n",
      "Average test loss: 0.003136862480185098\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012728736510707273\n",
      "Average test loss: 0.0031907649081614285\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012742409555448427\n",
      "Average test loss: 0.0032274633571505547\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012721906033655007\n",
      "Average test loss: 0.0032412611324754027\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012730692606005404\n",
      "Average test loss: 0.003216763075441122\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01272566377537118\n",
      "Average test loss: 0.003087178635928366\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012721315977474053\n",
      "Average test loss: 0.003319290615204308\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012712808356516891\n",
      "Average test loss: 0.0032092853461702666\n",
      "Epoch 254/300\n",
      "Average training loss: 0.012716601400739616\n",
      "Average test loss: 0.0031926829496191607\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01270009573466248\n",
      "Average test loss: 0.003231590747833252\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012715496694048246\n",
      "Average test loss: 0.0033252939995792176\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012695363014936447\n",
      "Average test loss: 0.003186086254194379\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012700268318255742\n",
      "Average test loss: 0.003272728462600046\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012698558420356778\n",
      "Average test loss: 0.0032390982082320585\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012696412483851114\n",
      "Average test loss: 0.003296504316230615\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012684090367207924\n",
      "Average test loss: 0.003306676712921924\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012685334260265033\n",
      "Average test loss: 0.0033323843522618216\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012692514912121826\n",
      "Average test loss: 0.0031290914132777185\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012665862289567788\n",
      "Average test loss: 0.003240161329921749\n",
      "Epoch 265/300\n",
      "Average training loss: 0.012680715863075521\n",
      "Average test loss: 0.003235580375418067\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012665032064749135\n",
      "Average test loss: 0.0031387530768083203\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012674662484890884\n",
      "Average test loss: 0.0032105263171510563\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012686778933637672\n",
      "Average test loss: 0.003282121713169747\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012661765741805235\n",
      "Average test loss: 0.0032844071282694736\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012649771405590905\n",
      "Average test loss: 0.0033135632783588436\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012659924675193097\n",
      "Average test loss: 0.0031774141169670553\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01266807602180375\n",
      "Average test loss: 0.0033323536678734754\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012650624675055344\n",
      "Average test loss: 0.0033864019380675423\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012650886612633864\n",
      "Average test loss: 0.003157766616385844\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01266114633778731\n",
      "Average test loss: 0.0031237029786118204\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012645117613176505\n",
      "Average test loss: 0.0033757579206592507\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012623755356503858\n",
      "Average test loss: 0.003199212059378624\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012631378362576167\n",
      "Average test loss: 0.003223726040373246\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012636271339323785\n",
      "Average test loss: 0.0032134530088967746\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012635926525625918\n",
      "Average test loss: 0.0031843321517937714\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012630480195085208\n",
      "Average test loss: 0.0032049610066331095\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012620866988268163\n",
      "Average test loss: 0.0031921557552284665\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012626707765791152\n",
      "Average test loss: 0.003312184715229604\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012622724211050405\n",
      "Average test loss: 0.003233826399056448\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012622645990716087\n",
      "Average test loss: 0.0032367606597642103\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012605593086944686\n",
      "Average test loss: 0.0032302676151610083\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01260995944092671\n",
      "Average test loss: 0.0032206731672502226\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01260840074883567\n",
      "Average test loss: 0.003186823978399237\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012601163071890673\n",
      "Average test loss: 0.003298543468945556\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012603305653565459\n",
      "Average test loss: 0.0031889397294984924\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012600164401034513\n",
      "Average test loss: 0.003215138141065836\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012588854773177042\n",
      "Average test loss: 0.0033739188330041037\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012591483232875665\n",
      "Average test loss: 0.003197241119419535\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01258929258502192\n",
      "Average test loss: 0.0032451384206198986\n",
      "Epoch 295/300\n",
      "Average training loss: 0.012585688003235392\n",
      "Average test loss: 0.0032790620966504018\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01259416149970558\n",
      "Average test loss: 0.0032535139607886474\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012572188614971109\n",
      "Average test loss: 0.003208603276560704\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012593446232378482\n",
      "Average test loss: 0.003229079651749796\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012581408924526638\n",
      "Average test loss: 0.003207591973038183\n",
      "Epoch 300/300\n",
      "Average training loss: 0.012569575786590577\n",
      "Average test loss: 0.0031902793262981706\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04914511560897032\n",
      "Average test loss: 0.003585632367266549\n",
      "Epoch 2/300\n",
      "Average training loss: 0.017658227766553562\n",
      "Average test loss: 0.003362758612467183\n",
      "Epoch 3/300\n",
      "Average training loss: 0.016618642495738134\n",
      "Average test loss: 0.003159674797621038\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01602363410923216\n",
      "Average test loss: 0.0030338797906620633\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015548735927376482\n",
      "Average test loss: 0.002999473866282238\n",
      "Epoch 6/300\n",
      "Average training loss: 0.015132853378852208\n",
      "Average test loss: 0.0028520915021912918\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01476445631020599\n",
      "Average test loss: 0.0027954555683665804\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014457959543499682\n",
      "Average test loss: 0.00273651926095287\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014180869728326797\n",
      "Average test loss: 0.0027508187865217526\n",
      "Epoch 10/300\n",
      "Average training loss: 0.013937811759610971\n",
      "Average test loss: 0.002697861599839396\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01372068050172594\n",
      "Average test loss: 0.002652042219415307\n",
      "Epoch 12/300\n",
      "Average training loss: 0.013522396290467845\n",
      "Average test loss: 0.0026899555385526686\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013358896973232429\n",
      "Average test loss: 0.0024950622326383987\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01318616700420777\n",
      "Average test loss: 0.0024786241109379463\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013036780423588223\n",
      "Average test loss: 0.002580625446306335\n",
      "Epoch 16/300\n",
      "Average training loss: 0.012904920639263259\n",
      "Average test loss: 0.0024327067486527893\n",
      "Epoch 17/300\n",
      "Average training loss: 0.012791823570926984\n",
      "Average test loss: 0.0023920504533582266\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012689421811865435\n",
      "Average test loss: 0.0023779129445966746\n",
      "Epoch 19/300\n",
      "Average training loss: 0.012578956757982571\n",
      "Average test loss: 0.002339615487183134\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01247718362013499\n",
      "Average test loss: 0.0023406139908151496\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012417809635400772\n",
      "Average test loss: 0.0023061720637811553\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012325762627025446\n",
      "Average test loss: 0.0023238590421775975\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012265060555603769\n",
      "Average test loss: 0.0022914248181300032\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012188310438560114\n",
      "Average test loss: 0.002303600729753574\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012145736613621314\n",
      "Average test loss: 0.0022702327722476587\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012077260361777411\n",
      "Average test loss: 0.002272533272910449\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012019256315297551\n",
      "Average test loss: 0.002248907466315561\n",
      "Epoch 28/300\n",
      "Average training loss: 0.011983479506439633\n",
      "Average test loss: 0.0022341813497866194\n",
      "Epoch 29/300\n",
      "Average training loss: 0.011918800916108821\n",
      "Average test loss: 0.002252434390493565\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011873310969935522\n",
      "Average test loss: 0.0022236379343602394\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011832134524153338\n",
      "Average test loss: 0.0022035941208402316\n",
      "Epoch 32/300\n",
      "Average training loss: 0.011799662869423628\n",
      "Average test loss: 0.0022260747855115267\n",
      "Epoch 33/300\n",
      "Average training loss: 0.011759986976782481\n",
      "Average test loss: 0.0022200086297881273\n",
      "Epoch 34/300\n",
      "Average training loss: 0.011706742415825526\n",
      "Average test loss: 0.0022089251294318173\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011682395021120708\n",
      "Average test loss: 0.0022093030562003453\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011654650890164904\n",
      "Average test loss: 0.002186320121296578\n",
      "Epoch 37/300\n",
      "Average training loss: 0.011605428981284301\n",
      "Average test loss: 0.0022085655017031565\n",
      "Epoch 38/300\n",
      "Average training loss: 0.011583769790000385\n",
      "Average test loss: 0.0021864617390351163\n",
      "Epoch 39/300\n",
      "Average training loss: 0.011550832717782922\n",
      "Average test loss: 0.0022039422108274366\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011534413343502416\n",
      "Average test loss: 0.0021663411659085087\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0114865900485052\n",
      "Average test loss: 0.002161016958662205\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011457069611383808\n",
      "Average test loss: 0.0021738644944917823\n",
      "Epoch 43/300\n",
      "Average training loss: 0.011442056466307905\n",
      "Average test loss: 0.002160839349238409\n",
      "Epoch 44/300\n",
      "Average training loss: 0.011402903020381928\n",
      "Average test loss: 0.0021780291063090164\n",
      "Epoch 45/300\n",
      "Average training loss: 0.011386688573078977\n",
      "Average test loss: 0.0022474148384191922\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01135686487538947\n",
      "Average test loss: 0.002176900893656744\n",
      "Epoch 47/300\n",
      "Average training loss: 0.011329416380988227\n",
      "Average test loss: 0.0021904227609435716\n",
      "Epoch 48/300\n",
      "Average training loss: 0.011315226448906793\n",
      "Average test loss: 0.0021461963823272124\n",
      "Epoch 49/300\n",
      "Average training loss: 0.011287295184201664\n",
      "Average test loss: 0.0021704749043823944\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011269345163471169\n",
      "Average test loss: 0.0022537508120553363\n",
      "Epoch 51/300\n",
      "Average training loss: 0.011245500542223454\n",
      "Average test loss: 0.0021564652140562733\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011224266757567724\n",
      "Average test loss: 0.0021687205367618137\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011206886646648248\n",
      "Average test loss: 0.0021429178412589764\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011175230585866504\n",
      "Average test loss: 0.0021736152838501667\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011158676855266093\n",
      "Average test loss: 0.002183165864812003\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011133920901351504\n",
      "Average test loss: 0.0021867782627749775\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011124739926308393\n",
      "Average test loss: 0.002170669661834836\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01109615635789103\n",
      "Average test loss: 0.0021565626326741443\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011079775700966518\n",
      "Average test loss: 0.002188279807360636\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011067311575015386\n",
      "Average test loss: 0.0021794050401076673\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011046853080391885\n",
      "Average test loss: 0.002148683875385258\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011020845560563935\n",
      "Average test loss: 0.0022031657012800375\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011006848257448939\n",
      "Average test loss: 0.0021440192561389673\n",
      "Epoch 64/300\n",
      "Average training loss: 0.010980830139584011\n",
      "Average test loss: 0.0021812222765551672\n",
      "Epoch 65/300\n",
      "Average training loss: 0.010968169003725051\n",
      "Average test loss: 0.0021758494615140888\n",
      "Epoch 66/300\n",
      "Average training loss: 0.010961932385961215\n",
      "Average test loss: 0.002171760202075044\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01093152364426189\n",
      "Average test loss: 0.0021624942363964185\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01092781582971414\n",
      "Average test loss: 0.002151873227622774\n",
      "Epoch 69/300\n",
      "Average training loss: 0.010908284115294615\n",
      "Average test loss: 0.0021411995069227285\n",
      "Epoch 70/300\n",
      "Average training loss: 0.010887534867558214\n",
      "Average test loss: 0.0021532626657022372\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01087599314832025\n",
      "Average test loss: 0.002158525525902708\n",
      "Epoch 72/300\n",
      "Average training loss: 0.010863252995742692\n",
      "Average test loss: 0.002194686585209436\n",
      "Epoch 73/300\n",
      "Average training loss: 0.010828218350807826\n",
      "Average test loss: 0.0022116808649152516\n",
      "Epoch 74/300\n",
      "Average training loss: 0.010834816075033612\n",
      "Average test loss: 0.002145910336325566\n",
      "Epoch 75/300\n",
      "Average training loss: 0.010812961946758959\n",
      "Average test loss: 0.0021522197935523258\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01078322068353494\n",
      "Average test loss: 0.0021700817634248072\n",
      "Epoch 77/300\n",
      "Average training loss: 0.010784049773381816\n",
      "Average test loss: 0.0021771541397190754\n",
      "Epoch 78/300\n",
      "Average training loss: 0.010759269086851014\n",
      "Average test loss: 0.002167027084363831\n",
      "Epoch 79/300\n",
      "Average training loss: 0.010749328144722514\n",
      "Average test loss: 0.0022123807235103514\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010736603706247277\n",
      "Average test loss: 0.0022206649496737453\n",
      "Epoch 81/300\n",
      "Average training loss: 0.010729269399411148\n",
      "Average test loss: 0.002192118005620109\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010698372323065997\n",
      "Average test loss: 0.002154505297438138\n",
      "Epoch 83/300\n",
      "Average training loss: 0.010698627739730808\n",
      "Average test loss: 0.002236431384459138\n",
      "Epoch 84/300\n",
      "Average training loss: 0.010681628453234832\n",
      "Average test loss: 0.002187808486115601\n",
      "Epoch 85/300\n",
      "Average training loss: 0.010666675435172187\n",
      "Average test loss: 0.0022504037235760027\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010649236814843284\n",
      "Average test loss: 0.0022214425317943097\n",
      "Epoch 87/300\n",
      "Average training loss: 0.010653679113421174\n",
      "Average test loss: 0.0021711125591148935\n",
      "Epoch 88/300\n",
      "Average training loss: 0.010621745775143306\n",
      "Average test loss: 0.002142170375833909\n",
      "Epoch 89/300\n",
      "Average training loss: 0.010614691966109806\n",
      "Average test loss: 0.002216217010592421\n",
      "Epoch 90/300\n",
      "Average training loss: 0.010608844716515806\n",
      "Average test loss: 0.0021999111655685638\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010592423944009674\n",
      "Average test loss: 0.0022015276414652668\n",
      "Epoch 92/300\n",
      "Average training loss: 0.010580547027289867\n",
      "Average test loss: 0.0021671653944585058\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010571162306600147\n",
      "Average test loss: 0.00219485478879263\n",
      "Epoch 94/300\n",
      "Average training loss: 0.010548848585950003\n",
      "Average test loss: 0.0021774877815817794\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01055017966694302\n",
      "Average test loss: 0.002243077716583179\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010528157277239693\n",
      "Average test loss: 0.0021694105751812456\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010525515055490865\n",
      "Average test loss: 0.0022093538695739374\n",
      "Epoch 98/300\n",
      "Average training loss: 0.010506256192922591\n",
      "Average test loss: 0.0022242443934082983\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010504778328869077\n",
      "Average test loss: 0.0021820113346394565\n",
      "Epoch 100/300\n",
      "Average training loss: 0.010491829483873314\n",
      "Average test loss: 0.0021778279636055233\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01046903815617164\n",
      "Average test loss: 0.0022083029916716945\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010458293561306264\n",
      "Average test loss: 0.002232165273692873\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01046437008678913\n",
      "Average test loss: 0.0021925826159616313\n",
      "Epoch 104/300\n",
      "Average training loss: 0.010442273386650614\n",
      "Average test loss: 0.002165272237319085\n",
      "Epoch 105/300\n",
      "Average training loss: 0.010434761664105787\n",
      "Average test loss: 0.0022315964338680107\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01042599722204937\n",
      "Average test loss: 0.002217255276731319\n",
      "Epoch 107/300\n",
      "Average training loss: 0.010420392086936368\n",
      "Average test loss: 0.0022044174131006004\n",
      "Epoch 108/300\n",
      "Average training loss: 0.010396986598769824\n",
      "Average test loss: 0.0022505686887436442\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0103950682423181\n",
      "Average test loss: 0.002178229231801298\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010383081077701516\n",
      "Average test loss: 0.002181378098308212\n",
      "Epoch 111/300\n",
      "Average training loss: 0.010374632467826208\n",
      "Average test loss: 0.002193262909228603\n",
      "Epoch 112/300\n",
      "Average training loss: 0.010360843964748912\n",
      "Average test loss: 0.0022525655267139275\n",
      "Epoch 113/300\n",
      "Average training loss: 0.010355502592192756\n",
      "Average test loss: 0.0022679849829938675\n",
      "Epoch 114/300\n",
      "Average training loss: 0.010341102480060524\n",
      "Average test loss: 0.0022350310118248065\n",
      "Epoch 115/300\n",
      "Average training loss: 0.010324387255642149\n",
      "Average test loss: 0.0022277271951445274\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0103328372190396\n",
      "Average test loss: 0.0022082323162919944\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01032560024658839\n",
      "Average test loss: 0.0022041795325155058\n",
      "Epoch 118/300\n",
      "Average training loss: 0.010338911440637377\n",
      "Average test loss: 0.00221774089595096\n",
      "Epoch 119/300\n",
      "Average training loss: 0.010312168416049745\n",
      "Average test loss: 0.002246697039860818\n",
      "Epoch 120/300\n",
      "Average training loss: 0.010295684864123663\n",
      "Average test loss: 0.002242003195608656\n",
      "Epoch 121/300\n",
      "Average training loss: 0.010288315206766128\n",
      "Average test loss: 0.0022552431612792943\n",
      "Epoch 122/300\n",
      "Average training loss: 0.010257422994408343\n",
      "Average test loss: 0.002334127788328462\n",
      "Epoch 123/300\n",
      "Average training loss: 0.010262336438728703\n",
      "Average test loss: 0.00223913440729181\n",
      "Epoch 124/300\n",
      "Average training loss: 0.010253539806438818\n",
      "Average test loss: 0.002174467068993383\n",
      "Epoch 125/300\n",
      "Average training loss: 0.010253883578711086\n",
      "Average test loss: 0.002170556677194933\n",
      "Epoch 126/300\n",
      "Average training loss: 0.010253969514535534\n",
      "Average test loss: 0.002181657049494485\n",
      "Epoch 127/300\n",
      "Average training loss: 0.010238746645549933\n",
      "Average test loss: 0.0021886936260594263\n",
      "Epoch 128/300\n",
      "Average training loss: 0.010227152170406447\n",
      "Average test loss: 0.0022490659699671797\n",
      "Epoch 129/300\n",
      "Average training loss: 0.010234891345103582\n",
      "Average test loss: 0.002268420577670137\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01019387097035845\n",
      "Average test loss: 0.0023273553970373337\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01018395004255904\n",
      "Average test loss: 0.002294069179851148\n",
      "Epoch 132/300\n",
      "Average training loss: 0.010185757014486525\n",
      "Average test loss: 0.0022229478628271156\n",
      "Epoch 133/300\n",
      "Average training loss: 0.010175201774471336\n",
      "Average test loss: 0.002359170072608524\n",
      "Epoch 134/300\n",
      "Average training loss: 0.010182583185533682\n",
      "Average test loss: 0.002225248262493147\n",
      "Epoch 135/300\n",
      "Average training loss: 0.010172848414629699\n",
      "Average test loss: 0.002310543652416931\n",
      "Epoch 136/300\n",
      "Average training loss: 0.010174419374101691\n",
      "Average test loss: 0.0024403070848849086\n",
      "Epoch 137/300\n",
      "Average training loss: 0.010146594256990486\n",
      "Average test loss: 0.0022155556368331115\n",
      "Epoch 138/300\n",
      "Average training loss: 0.010143765566249689\n",
      "Average test loss: 0.002246604536142614\n",
      "Epoch 139/300\n",
      "Average training loss: 0.010144109052088526\n",
      "Average test loss: 0.002285774820898142\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01013451619239317\n",
      "Average test loss: 0.002274080420947737\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01011584240446488\n",
      "Average test loss: 0.0022153363428595993\n",
      "Epoch 142/300\n",
      "Average training loss: 0.010117666981286473\n",
      "Average test loss: 0.002239565774384472\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01010053758819898\n",
      "Average test loss: 0.0022602698929193948\n",
      "Epoch 144/300\n",
      "Average training loss: 0.010117417969637447\n",
      "Average test loss: 0.0022638429684771434\n",
      "Epoch 145/300\n",
      "Average training loss: 0.010111293955395619\n",
      "Average test loss: 0.0021972848361151084\n",
      "Epoch 146/300\n",
      "Average training loss: 0.010091310121119022\n",
      "Average test loss: 0.0022802540856517025\n",
      "Epoch 147/300\n",
      "Average training loss: 0.010082172036998802\n",
      "Average test loss: 0.002245263173348374\n",
      "Epoch 148/300\n",
      "Average training loss: 0.010071146477841669\n",
      "Average test loss: 0.0022882251135177083\n",
      "Epoch 149/300\n",
      "Average training loss: 0.010069105498078797\n",
      "Average test loss: 0.002282882440007395\n",
      "Epoch 150/300\n",
      "Average training loss: 0.010072221161590683\n",
      "Average test loss: 0.00226516026382645\n",
      "Epoch 151/300\n",
      "Average training loss: 0.010062695687015852\n",
      "Average test loss: 0.002248756637589799\n",
      "Epoch 152/300\n",
      "Average training loss: 0.010061019553078546\n",
      "Average test loss: 0.0022155918947731456\n",
      "Epoch 153/300\n",
      "Average training loss: 0.010073014775911967\n",
      "Average test loss: 0.002303978621131844\n",
      "Epoch 154/300\n",
      "Average training loss: 0.010038549091253016\n",
      "Average test loss: 0.0022895248377074796\n",
      "Epoch 155/300\n",
      "Average training loss: 0.010026883756121\n",
      "Average test loss: 0.0022346796898378265\n",
      "Epoch 156/300\n",
      "Average training loss: 0.010025040011439059\n",
      "Average test loss: 0.0022830615423412785\n",
      "Epoch 157/300\n",
      "Average training loss: 0.010027788942058882\n",
      "Average test loss: 0.002230969104088015\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0100141059971518\n",
      "Average test loss: 0.00223817923427042\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01001347352978256\n",
      "Average test loss: 0.0022414276072134573\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009994455763863192\n",
      "Average test loss: 0.002326564459544089\n",
      "Epoch 161/300\n",
      "Average training loss: 0.010007545412414604\n",
      "Average test loss: 0.0023270715955230926\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009989257067855861\n",
      "Average test loss: 0.0022411527254929146\n",
      "Epoch 163/300\n",
      "Average training loss: 0.009996514176328977\n",
      "Average test loss: 0.0023217810793883273\n",
      "Epoch 164/300\n",
      "Average training loss: 0.009977132686310345\n",
      "Average test loss: 0.00226601660458578\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009983137416756816\n",
      "Average test loss: 0.002246274722015692\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0099756948840287\n",
      "Average test loss: 0.0022596015874296427\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00996473707507054\n",
      "Average test loss: 0.002264187293851541\n",
      "Epoch 168/300\n",
      "Average training loss: 0.009956621046695445\n",
      "Average test loss: 0.0024066778073708215\n",
      "Epoch 169/300\n",
      "Average training loss: 0.009963838906751739\n",
      "Average test loss: 0.002272851238027215\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00995596320182085\n",
      "Average test loss: 0.0024460231222005356\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0099516712832782\n",
      "Average test loss: 0.0022642482319432827\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0099349357129799\n",
      "Average test loss: 0.0022812001939035123\n",
      "Epoch 173/300\n",
      "Average training loss: 0.009932537275056044\n",
      "Average test loss: 0.0022695903552489146\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009958893332216475\n",
      "Average test loss: 0.002277681022572021\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0099212732286089\n",
      "Average test loss: 0.0023645594560851653\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009922155596315861\n",
      "Average test loss: 0.0022541468968201015\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009911367980142435\n",
      "Average test loss: 0.0022562759656252133\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009912762335605091\n",
      "Average test loss: 0.002282113604868452\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009903049931344058\n",
      "Average test loss: 0.0022829714622348545\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009891412952707873\n",
      "Average test loss: 0.002249537126057678\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00990538856966628\n",
      "Average test loss: 0.0023334123771637677\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009896893057558271\n",
      "Average test loss: 0.0023156728903866476\n",
      "Epoch 183/300\n",
      "Average training loss: 0.00987893002645837\n",
      "Average test loss: 0.0021920166452311807\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0098831672295928\n",
      "Average test loss: 0.002284694716023902\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0098785930039982\n",
      "Average test loss: 0.0022852361920393176\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009870613282753361\n",
      "Average test loss: 0.0022568861372354956\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009876324954132239\n",
      "Average test loss: 0.002264179756037063\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009871856792105569\n",
      "Average test loss: 0.002285672035896116\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0098447311706841\n",
      "Average test loss: 0.0023260957796333564\n",
      "Epoch 190/300\n",
      "Average training loss: 0.009857522168093257\n",
      "Average test loss: 0.0022861440442502497\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009858604950209458\n",
      "Average test loss: 0.002323234462696645\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009857542735834916\n",
      "Average test loss: 0.0022929587265890505\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009830518619053893\n",
      "Average test loss: 0.002265267581782407\n",
      "Epoch 194/300\n",
      "Average training loss: 0.009826666963597139\n",
      "Average test loss: 0.0022762060422036384\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009852494894630378\n",
      "Average test loss: 0.002232144478915466\n",
      "Epoch 196/300\n",
      "Average training loss: 0.009821008313033315\n",
      "Average test loss: 0.002266782787318031\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009803816385567187\n",
      "Average test loss: 0.0022626839354634286\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009827389757252402\n",
      "Average test loss: 0.002300478046759963\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009809492676622338\n",
      "Average test loss: 0.0024691508354412186\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00981057440986236\n",
      "Average test loss: 0.002327498837891552\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009803136381838056\n",
      "Average test loss: 0.002304048179131415\n",
      "Epoch 202/300\n",
      "Average training loss: 0.009800502600769203\n",
      "Average test loss: 0.002357260541576478\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009815200121866333\n",
      "Average test loss: 0.0023104912344780234\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009796333935525682\n",
      "Average test loss: 0.0023447647870828707\n",
      "Epoch 205/300\n",
      "Average training loss: 0.00980226448095507\n",
      "Average test loss: 0.00225793253744228\n",
      "Epoch 206/300\n",
      "Average training loss: 0.009775646956430541\n",
      "Average test loss: 0.002263827102155321\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009797132601340611\n",
      "Average test loss: 0.0022869459465146066\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009787818814317385\n",
      "Average test loss: 0.002300679835387402\n",
      "Epoch 209/300\n",
      "Average training loss: 0.009776265018516117\n",
      "Average test loss: 0.002265178884896967\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00977503700968292\n",
      "Average test loss: 0.002300042230842842\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009772054822080664\n",
      "Average test loss: 0.0023641858785930607\n",
      "Epoch 212/300\n",
      "Average training loss: 0.009766377014418444\n",
      "Average test loss: 0.0022826925019423165\n",
      "Epoch 213/300\n",
      "Average training loss: 0.009762857744263278\n",
      "Average test loss: 0.0023755237725045946\n",
      "Epoch 214/300\n",
      "Average training loss: 0.009759798136850198\n",
      "Average test loss: 0.0024242501676910453\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009749260509179698\n",
      "Average test loss: 0.0022609875241501464\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009744499208198653\n",
      "Average test loss: 0.002301259353446464\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009752538966635863\n",
      "Average test loss: 0.0023140617590397595\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009751113294727273\n",
      "Average test loss: 0.0023226415514945985\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009735856570717362\n",
      "Average test loss: 0.002301677050275935\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009764507299496067\n",
      "Average test loss: 0.002259519953073727\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009747174623939725\n",
      "Average test loss: 0.0024304848896960418\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009728413141022126\n",
      "Average test loss: 0.002425708225928247\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009722884717086952\n",
      "Average test loss: 0.002257984052515692\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00972769749040405\n",
      "Average test loss: 0.0023396552552779515\n",
      "Epoch 225/300\n",
      "Average training loss: 0.00971586231307851\n",
      "Average test loss: 0.002339334140635199\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009719943867789375\n",
      "Average test loss: 0.002380084445286128\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00972216215564145\n",
      "Average test loss: 0.002305755036158694\n",
      "Epoch 228/300\n",
      "Average training loss: 0.009721603917578855\n",
      "Average test loss: 0.002339189022158583\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009703401849501662\n",
      "Average test loss: 0.002365293443823854\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009709035388297505\n",
      "Average test loss: 0.0022688180595222445\n",
      "Epoch 231/300\n",
      "Average training loss: 0.009705367669463158\n",
      "Average test loss: 0.002298837851319048\n",
      "Epoch 232/300\n",
      "Average training loss: 0.009700963378780418\n",
      "Average test loss: 0.002246995464174284\n",
      "Epoch 233/300\n",
      "Average training loss: 0.009701151270005438\n",
      "Average test loss: 0.0024162507760855886\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009684010799560282\n",
      "Average test loss: 0.0023580611137052378\n",
      "Epoch 235/300\n",
      "Average training loss: 0.009692393710215887\n",
      "Average test loss: 0.0023201032667938206\n",
      "Epoch 236/300\n",
      "Average training loss: 0.009677476602296035\n",
      "Average test loss: 0.002307814318479763\n",
      "Epoch 237/300\n",
      "Average training loss: 0.009684333023925622\n",
      "Average test loss: 0.0023639404639187785\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00968895981212457\n",
      "Average test loss: 0.0024195698752171465\n",
      "Epoch 239/300\n",
      "Average training loss: 0.009678490693370501\n",
      "Average test loss: 0.002369035963486466\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009669228802538581\n",
      "Average test loss: 0.002344386846965386\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009668999065127637\n",
      "Average test loss: 0.002255452274655302\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009669995899415678\n",
      "Average test loss: 0.0022950295260589985\n",
      "Epoch 243/300\n",
      "Average training loss: 0.009664778288039896\n",
      "Average test loss: 0.0023125063286473352\n",
      "Epoch 244/300\n",
      "Average training loss: 0.00965956414780683\n",
      "Average test loss: 0.0023699451817406547\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009669807778464424\n",
      "Average test loss: 0.0023107151893071005\n",
      "Epoch 246/300\n",
      "Average training loss: 0.009654261426379283\n",
      "Average test loss: 0.002331612595460481\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009650213934481145\n",
      "Average test loss: 0.0023532024293930996\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009638878960576322\n",
      "Average test loss: 0.0023375166839816504\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009641955030875074\n",
      "Average test loss: 0.002279004728421569\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009653907155825031\n",
      "Average test loss: 0.002364553392140402\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009647960028714604\n",
      "Average test loss: 0.0022827558281521003\n",
      "Epoch 252/300\n",
      "Average training loss: 0.009642869762248462\n",
      "Average test loss: 0.002320153933639328\n",
      "Epoch 253/300\n",
      "Average training loss: 0.009636355519294738\n",
      "Average test loss: 0.0023387261640487444\n",
      "Epoch 254/300\n",
      "Average training loss: 0.009646209710174136\n",
      "Average test loss: 0.0023249160951624315\n",
      "Epoch 255/300\n",
      "Average training loss: 0.009640571566091643\n",
      "Average test loss: 0.0023170519178319308\n",
      "Epoch 256/300\n",
      "Average training loss: 0.009630382829242282\n",
      "Average test loss: 0.002268919619421164\n",
      "Epoch 257/300\n",
      "Average training loss: 0.009623286506781976\n",
      "Average test loss: 0.002262267975550559\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0096232718800505\n",
      "Average test loss: 0.0022886442026744287\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009613144105507268\n",
      "Average test loss: 0.0022726410577694573\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009616455802487002\n",
      "Average test loss: 0.0023125849270986187\n",
      "Epoch 261/300\n",
      "Average training loss: 0.009615986629492706\n",
      "Average test loss: 0.0023328461630476847\n",
      "Epoch 262/300\n",
      "Average training loss: 0.009618439690934287\n",
      "Average test loss: 0.0023498966472430363\n",
      "Epoch 263/300\n",
      "Average training loss: 0.009615922530492147\n",
      "Average test loss: 0.002316856482687096\n",
      "Epoch 264/300\n",
      "Average training loss: 0.009613153639766905\n",
      "Average test loss: 0.0023212071840340892\n",
      "Epoch 265/300\n",
      "Average training loss: 0.009606177154514525\n",
      "Average test loss: 0.0022947792353936367\n",
      "Epoch 266/300\n",
      "Average training loss: 0.009603732283744547\n",
      "Average test loss: 0.0023275192135738003\n",
      "Epoch 267/300\n",
      "Average training loss: 0.009601980774352948\n",
      "Average test loss: 0.0023527400280452435\n",
      "Epoch 268/300\n",
      "Average training loss: 0.009604140882690747\n",
      "Average test loss: 0.0023096286449581383\n",
      "Epoch 269/300\n",
      "Average training loss: 0.00959376517103778\n",
      "Average test loss: 0.0023867573369708325\n",
      "Epoch 270/300\n",
      "Average training loss: 0.009587714487893714\n",
      "Average test loss: 0.0023199511023445263\n",
      "Epoch 271/300\n",
      "Average training loss: 0.009599895738893085\n",
      "Average test loss: 0.0023719990607351067\n",
      "Epoch 272/300\n",
      "Average training loss: 0.009586502165430121\n",
      "Average test loss: 0.00231994317813466\n",
      "Epoch 273/300\n",
      "Average training loss: 0.009581745824052228\n",
      "Average test loss: 0.0023236749085287253\n",
      "Epoch 274/300\n",
      "Average training loss: 0.009579947715004285\n",
      "Average test loss: 0.0023750169213033385\n",
      "Epoch 275/300\n",
      "Average training loss: 0.009586680164767636\n",
      "Average test loss: 0.0023127869099585545\n",
      "Epoch 276/300\n",
      "Average training loss: 0.00958175971565975\n",
      "Average test loss: 0.0023453625256402624\n",
      "Epoch 277/300\n",
      "Average training loss: 0.009574630974895424\n",
      "Average test loss: 0.0023496141557892165\n",
      "Epoch 278/300\n",
      "Average training loss: 0.009581564439667596\n",
      "Average test loss: 0.00237415164647003\n",
      "Epoch 279/300\n",
      "Average training loss: 0.009579553103281391\n",
      "Average test loss: 0.0023496545998172628\n",
      "Epoch 280/300\n",
      "Average training loss: 0.009565375024245844\n",
      "Average test loss: 0.0022897729496988983\n",
      "Epoch 281/300\n",
      "Average training loss: 0.009559685178928905\n",
      "Average test loss: 0.0023562253256224925\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00956980649381876\n",
      "Average test loss: 0.002384628844550914\n",
      "Epoch 283/300\n",
      "Average training loss: 0.009557197468976179\n",
      "Average test loss: 0.0024454334483792386\n",
      "Epoch 284/300\n",
      "Average training loss: 0.009557840379575888\n",
      "Average test loss: 0.0022886618176061247\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00955182392978006\n",
      "Average test loss: 0.0022853286696804895\n",
      "Epoch 286/300\n",
      "Average training loss: 0.009563296100331678\n",
      "Average test loss: 0.0023387534831547075\n",
      "Epoch 287/300\n",
      "Average training loss: 0.009549803053339322\n",
      "Average test loss: 0.002347616838084327\n",
      "Epoch 288/300\n",
      "Average training loss: 0.009548532091495063\n",
      "Average test loss: 0.0023297664456897313\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00957466955896881\n",
      "Average test loss: 0.002366227771052056\n",
      "Epoch 290/300\n",
      "Average training loss: 0.009554736506607797\n",
      "Average test loss: 0.0023455555147180954\n",
      "Epoch 291/300\n",
      "Average training loss: 0.009547523673209879\n",
      "Average test loss: 0.0023487986971934636\n",
      "Epoch 292/300\n",
      "Average training loss: 0.009529991874264346\n",
      "Average test loss: 0.0023297932827845216\n",
      "Epoch 293/300\n",
      "Average training loss: 0.009530611558092966\n",
      "Average test loss: 0.002360002328124311\n",
      "Epoch 294/300\n",
      "Average training loss: 0.009533685845633348\n",
      "Average test loss: 0.002460483361242546\n",
      "Epoch 295/300\n",
      "Average training loss: 0.009533715825113986\n",
      "Average test loss: 0.0023814406622615124\n",
      "Epoch 296/300\n",
      "Average training loss: 0.009533493085039986\n",
      "Average test loss: 0.002382165026747518\n",
      "Epoch 297/300\n",
      "Average training loss: 0.009542095094091362\n",
      "Average test loss: 0.002453130165942841\n",
      "Epoch 298/300\n",
      "Average training loss: 0.009535056461062697\n",
      "Average test loss: 0.00227808256374879\n",
      "Epoch 299/300\n",
      "Average training loss: 0.009525597487058904\n",
      "Average test loss: 0.0024386342478295167\n",
      "Epoch 300/300\n",
      "Average training loss: 0.009524330926438172\n",
      "Average test loss: 0.002317973872129288\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.044706915113661025\n",
      "Average test loss: 0.003233515292613043\n",
      "Epoch 2/300\n",
      "Average training loss: 0.014584996812873417\n",
      "Average test loss: 0.0027626706113417945\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01349720836099651\n",
      "Average test loss: 0.00269764573365036\n",
      "Epoch 4/300\n",
      "Average training loss: 0.012848453275859355\n",
      "Average test loss: 0.002437906059126059\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012354249931044049\n",
      "Average test loss: 0.002289915355750256\n",
      "Epoch 6/300\n",
      "Average training loss: 0.011927773651149538\n",
      "Average test loss: 0.0022078516359130542\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011579594598048262\n",
      "Average test loss: 0.0021317564647437798\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011259134203195572\n",
      "Average test loss: 0.002067420375533402\n",
      "Epoch 9/300\n",
      "Average training loss: 0.010982238860593902\n",
      "Average test loss: 0.0020115878915207253\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010731139472789234\n",
      "Average test loss: 0.0019493405390530825\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010514855206840567\n",
      "Average test loss: 0.001900655255239043\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01031774460275968\n",
      "Average test loss: 0.0019578587210012806\n",
      "Epoch 13/300\n",
      "Average training loss: 0.010145460680127144\n",
      "Average test loss: 0.0018298417681621181\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009999096433321635\n",
      "Average test loss: 0.0018203615713864565\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009865834090444777\n",
      "Average test loss: 0.0017865244444045755\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009724471577339702\n",
      "Average test loss: 0.0017403496163379815\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009620322118202846\n",
      "Average test loss: 0.0017352453778601356\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00951756535222133\n",
      "Average test loss: 0.0017013153415173293\n",
      "Epoch 19/300\n",
      "Average training loss: 0.009432211856047312\n",
      "Average test loss: 0.001714019387546513\n",
      "Epoch 20/300\n",
      "Average training loss: 0.009350587003760867\n",
      "Average test loss: 0.00166484906245023\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0092733640174071\n",
      "Average test loss: 0.0016536411607844961\n",
      "Epoch 22/300\n",
      "Average training loss: 0.009208993659251267\n",
      "Average test loss: 0.0016549040021167861\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00914340980640716\n",
      "Average test loss: 0.0016316512184631493\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009088932212856081\n",
      "Average test loss: 0.001658793236232466\n",
      "Epoch 25/300\n",
      "Average training loss: 0.009031273966034254\n",
      "Average test loss: 0.0016391089518244068\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008990117741955652\n",
      "Average test loss: 0.0016090505187296206\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00893764682693614\n",
      "Average test loss: 0.001606292571251591\n",
      "Epoch 28/300\n",
      "Average training loss: 0.008901103614932961\n",
      "Average test loss: 0.0016187674291431903\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00885444168953432\n",
      "Average test loss: 0.001600795383979049\n",
      "Epoch 30/300\n",
      "Average training loss: 0.008825347078343232\n",
      "Average test loss: 0.0015755103228406774\n",
      "Epoch 31/300\n",
      "Average training loss: 0.008784444174418846\n",
      "Average test loss: 0.0015550398595838082\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008746201615366671\n",
      "Average test loss: 0.0015769525808799598\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008722468099246422\n",
      "Average test loss: 0.0015746215245582991\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008697403088625934\n",
      "Average test loss: 0.001568525534433623\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008669817470841938\n",
      "Average test loss: 0.001600931405193276\n",
      "Epoch 36/300\n",
      "Average training loss: 0.008629054311662913\n",
      "Average test loss: 0.0015489583634254005\n",
      "Epoch 37/300\n",
      "Average training loss: 0.008602758444845676\n",
      "Average test loss: 0.0015596722615882754\n",
      "Epoch 38/300\n",
      "Average training loss: 0.008584908499485917\n",
      "Average test loss: 0.0015314733747185932\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008558994736108515\n",
      "Average test loss: 0.0015458874606216948\n",
      "Epoch 40/300\n",
      "Average training loss: 0.008531608101394441\n",
      "Average test loss: 0.0015387220738662614\n",
      "Epoch 41/300\n",
      "Average training loss: 0.008515874066286616\n",
      "Average test loss: 0.001525156007769207\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008487031041747995\n",
      "Average test loss: 0.0015425959138406647\n",
      "Epoch 43/300\n",
      "Average training loss: 0.008474748083286816\n",
      "Average test loss: 0.0015227464303995172\n",
      "Epoch 44/300\n",
      "Average training loss: 0.008442086961120367\n",
      "Average test loss: 0.0015272089852434066\n",
      "Epoch 45/300\n",
      "Average training loss: 0.008428375883648793\n",
      "Average test loss: 0.0015447421015964615\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00841657305177715\n",
      "Average test loss: 0.0015534976141320335\n",
      "Epoch 47/300\n",
      "Average training loss: 0.008386592751575841\n",
      "Average test loss: 0.0015207727385891808\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008373676459408468\n",
      "Average test loss: 0.0015187264113790458\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008357575724108352\n",
      "Average test loss: 0.0015168265431291527\n",
      "Epoch 50/300\n",
      "Average training loss: 0.008343177345063952\n",
      "Average test loss: 0.00151238660255654\n",
      "Epoch 51/300\n",
      "Average training loss: 0.008318025635762347\n",
      "Average test loss: 0.001523148646992114\n",
      "Epoch 52/300\n",
      "Average training loss: 0.008293190533502234\n",
      "Average test loss: 0.001523535803374317\n",
      "Epoch 53/300\n",
      "Average training loss: 0.008290856042255958\n",
      "Average test loss: 0.0014987871665507555\n",
      "Epoch 54/300\n",
      "Average training loss: 0.008262322215984264\n",
      "Average test loss: 0.0016055241503442328\n",
      "Epoch 55/300\n",
      "Average training loss: 0.008257390568239821\n",
      "Average test loss: 0.0016034565530717373\n",
      "Epoch 56/300\n",
      "Average training loss: 0.00823898518209656\n",
      "Average test loss: 0.0015172036043885682\n",
      "Epoch 57/300\n",
      "Average training loss: 0.008219667750928137\n",
      "Average test loss: 0.0015006775020414756\n",
      "Epoch 58/300\n",
      "Average training loss: 0.008206066354695294\n",
      "Average test loss: 0.001523845754046407\n",
      "Epoch 59/300\n",
      "Average training loss: 0.00819578902506166\n",
      "Average test loss: 0.001514371980343842\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008191801561663548\n",
      "Average test loss: 0.0015582288333939182\n",
      "Epoch 61/300\n",
      "Average training loss: 0.008162981275469064\n",
      "Average test loss: 0.0015100885840753713\n",
      "Epoch 62/300\n",
      "Average training loss: 0.008150336258941227\n",
      "Average test loss: 0.0016195377597792281\n",
      "Epoch 63/300\n",
      "Average training loss: 0.00813540772224466\n",
      "Average test loss: 0.0014914591619227496\n",
      "Epoch 64/300\n",
      "Average training loss: 0.008117004634605513\n",
      "Average test loss: 0.0015252096073495017\n",
      "Epoch 65/300\n",
      "Average training loss: 0.00810358828595943\n",
      "Average test loss: 0.0015050985646537609\n",
      "Epoch 66/300\n",
      "Average training loss: 0.008099831221004328\n",
      "Average test loss: 0.001496302041121655\n",
      "Epoch 67/300\n",
      "Average training loss: 0.008078302051872014\n",
      "Average test loss: 0.001537168256731497\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008071904072331057\n",
      "Average test loss: 0.0015078764899323384\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008050733666867017\n",
      "Average test loss: 0.0015206991885271337\n",
      "Epoch 70/300\n",
      "Average training loss: 0.008049349002540111\n",
      "Average test loss: 0.0015070150085828371\n",
      "Epoch 71/300\n",
      "Average training loss: 0.00802942457381222\n",
      "Average test loss: 0.0014986711861565709\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008025051950580543\n",
      "Average test loss: 0.0015317805159526566\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008006499187813865\n",
      "Average test loss: 0.0015045921839256253\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00800011419546273\n",
      "Average test loss: 0.0015782945146784186\n",
      "Epoch 75/300\n",
      "Average training loss: 0.007986384733683533\n",
      "Average test loss: 0.0015315261464565993\n",
      "Epoch 76/300\n",
      "Average training loss: 0.007973921521670289\n",
      "Average test loss: 0.001511329873258041\n",
      "Epoch 77/300\n",
      "Average training loss: 0.007975162011053827\n",
      "Average test loss: 0.0015223282802229127\n",
      "Epoch 78/300\n",
      "Average training loss: 0.007953236678822173\n",
      "Average test loss: 0.001517317229674922\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007942898305753867\n",
      "Average test loss: 0.0015136700626462698\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007928480569687155\n",
      "Average test loss: 0.0015655802213069465\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00791935095232394\n",
      "Average test loss: 0.0016231617354270484\n",
      "Epoch 82/300\n",
      "Average training loss: 0.007908007423082987\n",
      "Average test loss: 0.0014981425367295742\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007926028720620606\n",
      "Average test loss: 0.001527680794811911\n",
      "Epoch 84/300\n",
      "Average training loss: 0.007890967867854568\n",
      "Average test loss: 0.0015330348923388456\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00788508460422357\n",
      "Average test loss: 0.00149492699570126\n",
      "Epoch 86/300\n",
      "Average training loss: 0.007868930141545005\n",
      "Average test loss: 0.0015090713461654054\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007861394042563107\n",
      "Average test loss: 0.001640561188157234\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007846504458950625\n",
      "Average test loss: 0.0014983434052103096\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007849139341463645\n",
      "Average test loss: 0.0015723277787781423\n",
      "Epoch 90/300\n",
      "Average training loss: 0.007829748643769159\n",
      "Average test loss: 0.0016648042839434412\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007827124581982692\n",
      "Average test loss: 0.001524095858964655\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007818830122550328\n",
      "Average test loss: 0.0015128805881573094\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007806072620881928\n",
      "Average test loss: 0.0015107985583858357\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007792079635792308\n",
      "Average test loss: 0.0015181593669371472\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007799696883807579\n",
      "Average test loss: 0.0015504735519902575\n",
      "Epoch 96/300\n",
      "Average training loss: 0.007780860448463095\n",
      "Average test loss: 0.0015469181209595668\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007776692730685075\n",
      "Average test loss: 0.001540561243891716\n",
      "Epoch 98/300\n",
      "Average training loss: 0.007759831379685137\n",
      "Average test loss: 0.0015547073120251297\n",
      "Epoch 99/300\n",
      "Average training loss: 0.007751324899494648\n",
      "Average test loss: 0.0016066574906516407\n",
      "Epoch 100/300\n",
      "Average training loss: 0.007738215362239215\n",
      "Average test loss: 0.0015348949808006486\n",
      "Epoch 101/300\n",
      "Average training loss: 0.007743645663062732\n",
      "Average test loss: 0.0015323033007379207\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007730872513519393\n",
      "Average test loss: 0.0015633828868675563\n",
      "Epoch 103/300\n",
      "Average training loss: 0.007727634572734435\n",
      "Average test loss: 0.00152758487376074\n",
      "Epoch 104/300\n",
      "Average training loss: 0.007716345664941602\n",
      "Average test loss: 0.0015322871841490268\n",
      "Epoch 105/300\n",
      "Average training loss: 0.007710632773737113\n",
      "Average test loss: 0.0015078221185960703\n",
      "Epoch 106/300\n",
      "Average training loss: 0.007701117171181573\n",
      "Average test loss: 0.0015379303280885022\n",
      "Epoch 107/300\n",
      "Average training loss: 0.007697156481030915\n",
      "Average test loss: 0.0015137615870270465\n",
      "Epoch 108/300\n",
      "Average training loss: 0.007685212798416614\n",
      "Average test loss: 0.0015440582618531254\n",
      "Epoch 109/300\n",
      "Average training loss: 0.007676989662564463\n",
      "Average test loss: 0.0015209261919889186\n",
      "Epoch 110/300\n",
      "Average training loss: 0.007670845423721605\n",
      "Average test loss: 0.0015699959424220853\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00766994804268082\n",
      "Average test loss: 0.0016559705907065007\n",
      "Epoch 112/300\n",
      "Average training loss: 0.007651178280926413\n",
      "Average test loss: 0.001558550572850638\n",
      "Epoch 113/300\n",
      "Average training loss: 0.007648357324302197\n",
      "Average test loss: 0.0015208970843296912\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00763527696331342\n",
      "Average test loss: 0.0015152443600818515\n",
      "Epoch 115/300\n",
      "Average training loss: 0.007642232697870996\n",
      "Average test loss: 0.001556882543489337\n",
      "Epoch 116/300\n",
      "Average training loss: 0.007635221958574322\n",
      "Average test loss: 0.00154884802032676\n",
      "Epoch 117/300\n",
      "Average training loss: 0.007617482338928514\n",
      "Average test loss: 0.0015678454400557611\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007616814369542731\n",
      "Average test loss: 0.0015587128817828165\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0076116839229232736\n",
      "Average test loss: 0.001556220043450594\n",
      "Epoch 120/300\n",
      "Average training loss: 0.007606695031540261\n",
      "Average test loss: 0.001521127696045571\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007598220927019914\n",
      "Average test loss: 0.001558774365267406\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00759212414547801\n",
      "Average test loss: 0.011726839462295175\n",
      "Epoch 123/300\n",
      "Average training loss: 0.007596617648584975\n",
      "Average test loss: 0.001554023326581551\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0075810645976000365\n",
      "Average test loss: 0.0015284747729698816\n",
      "Epoch 125/300\n",
      "Average training loss: 0.007569938189453549\n",
      "Average test loss: 0.001535160212467114\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0075720737448169125\n",
      "Average test loss: 0.0015461900995837317\n",
      "Epoch 127/300\n",
      "Average training loss: 0.007561996638774872\n",
      "Average test loss: 0.0015433159787207843\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007549236357212066\n",
      "Average test loss: 0.0015364907338904837\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007549946796149016\n",
      "Average test loss: 0.0015327634933508104\n",
      "Epoch 130/300\n",
      "Average training loss: 0.007544392237646712\n",
      "Average test loss: 0.001547782676294446\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007530105281621218\n",
      "Average test loss: 0.0015260511089323296\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007529079835448\n",
      "Average test loss: 0.001537733910812272\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007517746386842595\n",
      "Average test loss: 0.0015309954144888454\n",
      "Epoch 134/300\n",
      "Average training loss: 0.007527418276088106\n",
      "Average test loss: 0.0015701167131256725\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007516315375351243\n",
      "Average test loss: 0.0016001726547256111\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007519376238187154\n",
      "Average test loss: 0.0015603117944879665\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00750321047877272\n",
      "Average test loss: 0.0015549950564487114\n",
      "Epoch 138/300\n",
      "Average training loss: 0.007485721919271681\n",
      "Average test loss: 0.0015461605959054498\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007484326442082723\n",
      "Average test loss: 0.0015946343328493338\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0074838628369486996\n",
      "Average test loss: 0.0015584690851780277\n",
      "Epoch 141/300\n",
      "Average training loss: 0.007482311322871181\n",
      "Average test loss: 0.001567865064367652\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00747860875311825\n",
      "Average test loss: 0.0015539733574001327\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00746646241678132\n",
      "Average test loss: 0.0015485611520707607\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00746645789510674\n",
      "Average test loss: 0.0015183568711703021\n",
      "Epoch 145/300\n",
      "Average training loss: 0.007463647389991416\n",
      "Average test loss: 0.001569620650779042\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0074505344798995385\n",
      "Average test loss: 0.001598357513339983\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007451394595619705\n",
      "Average test loss: 0.0015464526798783078\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007456258004738225\n",
      "Average test loss: 0.001542514231469896\n",
      "Epoch 149/300\n",
      "Average training loss: 0.007437124464660883\n",
      "Average test loss: 0.0015569200302577681\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007435755583147208\n",
      "Average test loss: 0.001527333876118064\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007432086171375381\n",
      "Average test loss: 0.0015461429323380192\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007422767919798692\n",
      "Average test loss: 0.0015332313836552204\n",
      "Epoch 153/300\n",
      "Average training loss: 0.007421996754904588\n",
      "Average test loss: 0.0015782083210845788\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007416106486486064\n",
      "Average test loss: 0.0015285247734023466\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007414767029798693\n",
      "Average test loss: 0.0016180432364344597\n",
      "Epoch 156/300\n",
      "Average training loss: 0.007410552739683125\n",
      "Average test loss: 0.001571309899704324\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0074066179191900625\n",
      "Average test loss: 0.0015742019075486394\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0073960482834113965\n",
      "Average test loss: 0.0015796354580670596\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0073917218185961244\n",
      "Average test loss: 0.0015870650552420153\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0073889401588175035\n",
      "Average test loss: 0.0015717812391618888\n",
      "Epoch 161/300\n",
      "Average training loss: 0.007393853918959697\n",
      "Average test loss: 0.0015432052385682862\n",
      "Epoch 162/300\n",
      "Average training loss: 0.007386436752561066\n",
      "Average test loss: 0.0015484280557268196\n",
      "Epoch 163/300\n",
      "Average training loss: 0.007383750426272551\n",
      "Average test loss: 0.0015410910685443216\n",
      "Epoch 164/300\n",
      "Average training loss: 0.007383039148317443\n",
      "Average test loss: 0.0015548514220346179\n",
      "Epoch 165/300\n",
      "Average training loss: 0.007362401602168878\n",
      "Average test loss: 0.001554543568028344\n",
      "Epoch 166/300\n",
      "Average training loss: 0.007370425090193748\n",
      "Average test loss: 0.0015789553361634414\n",
      "Epoch 167/300\n",
      "Average training loss: 0.007362474827302827\n",
      "Average test loss: 0.0015493981698527933\n",
      "Epoch 168/300\n",
      "Average training loss: 0.007362163556946648\n",
      "Average test loss: 0.001580766271903283\n",
      "Epoch 169/300\n",
      "Average training loss: 0.007357096834729115\n",
      "Average test loss: 0.001565350350147734\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007353654178066386\n",
      "Average test loss: 0.0015617286990293197\n",
      "Epoch 171/300\n",
      "Average training loss: 0.007348184228771263\n",
      "Average test loss: 0.0015528135583736003\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0073370540932648715\n",
      "Average test loss: 0.0016277207229286432\n",
      "Epoch 173/300\n",
      "Average training loss: 0.007342173604087697\n",
      "Average test loss: 0.0015765446412066618\n",
      "Epoch 174/300\n",
      "Average training loss: 0.007340912651684549\n",
      "Average test loss: 0.001585550790031751\n",
      "Epoch 175/300\n",
      "Average training loss: 0.007326572017951144\n",
      "Average test loss: 0.0015863199403716459\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007329835610257254\n",
      "Average test loss: 0.001537700403895643\n",
      "Epoch 177/300\n",
      "Average training loss: 0.007327979462842147\n",
      "Average test loss: 0.0015539721087035206\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007323433646311363\n",
      "Average test loss: 0.0016109581313406428\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0073218646922873125\n",
      "Average test loss: 0.0015945513221538728\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007317848939448595\n",
      "Average test loss: 0.0015755260275780326\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007314172749304109\n",
      "Average test loss: 0.001567764094306363\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007309506833553314\n",
      "Average test loss: 0.00155422839253313\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007309454107450115\n",
      "Average test loss: 0.0015642704504231612\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0073009225792355\n",
      "Average test loss: 0.0016044691813488802\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007294588608046373\n",
      "Average test loss: 0.001668559078954988\n",
      "Epoch 186/300\n",
      "Average training loss: 0.007304259832120604\n",
      "Average test loss: 0.0016204149319479864\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007294208358973265\n",
      "Average test loss: 0.0015667834667902854\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007287692131267654\n",
      "Average test loss: 0.001579692579081489\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007286498432358106\n",
      "Average test loss: 0.001582525567048126\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007277998692045609\n",
      "Average test loss: 0.0016208037722648845\n",
      "Epoch 191/300\n",
      "Average training loss: 0.007282370856238736\n",
      "Average test loss: 0.0015869387137807077\n",
      "Epoch 192/300\n",
      "Average training loss: 0.007265094933410486\n",
      "Average test loss: 0.001597302548173401\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00726991771451301\n",
      "Average test loss: 0.001577357064725624\n",
      "Epoch 194/300\n",
      "Average training loss: 0.007266214434057474\n",
      "Average test loss: 0.001625686777755618\n",
      "Epoch 195/300\n",
      "Average training loss: 0.00726140244015389\n",
      "Average test loss: 0.0016049696280517512\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00725605486623115\n",
      "Average test loss: 0.0015833009719952113\n",
      "Epoch 197/300\n",
      "Average training loss: 0.007261320516881015\n",
      "Average test loss: 0.001594359768088907\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007249996417512497\n",
      "Average test loss: 0.0016117779901251197\n",
      "Epoch 199/300\n",
      "Average training loss: 0.007262519589728779\n",
      "Average test loss: 0.001786355417739186\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007244516842895084\n",
      "Average test loss: 0.001612620072491053\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007249632026586268\n",
      "Average test loss: 0.0015940374362075494\n",
      "Epoch 202/300\n",
      "Average training loss: 0.007238086864352226\n",
      "Average test loss: 0.0015810136211415133\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007240703646093607\n",
      "Average test loss: 0.0015780207512693272\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007234651236898369\n",
      "Average test loss: 0.00158422774148898\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007234869753734933\n",
      "Average test loss: 0.0016100676883425977\n",
      "Epoch 206/300\n",
      "Average training loss: 0.007233345551623239\n",
      "Average test loss: 0.0015951105896383523\n",
      "Epoch 207/300\n",
      "Average training loss: 0.007248416281822655\n",
      "Average test loss: 0.0016123204009814394\n",
      "Epoch 208/300\n",
      "Average training loss: 0.00722368715910448\n",
      "Average test loss: 0.0015988297320695386\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007229579616338014\n",
      "Average test loss: 0.0015771171785891056\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007223188742581341\n",
      "Average test loss: 0.0016165241238971552\n",
      "Epoch 211/300\n",
      "Average training loss: 0.007226206293122636\n",
      "Average test loss: 0.0016416739322659042\n",
      "Epoch 212/300\n",
      "Average training loss: 0.007213650571803252\n",
      "Average test loss: 0.001585450073497163\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007217477972308795\n",
      "Average test loss: 0.0016128427121374343\n",
      "Epoch 214/300\n",
      "Average training loss: 0.007212876158456008\n",
      "Average test loss: 0.0015604500390278796\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007212124118788374\n",
      "Average test loss: 0.0015925000636941857\n",
      "Epoch 216/300\n",
      "Average training loss: 0.007206421133958631\n",
      "Average test loss: 0.0016488405625439352\n",
      "Epoch 217/300\n",
      "Average training loss: 0.007196265521562762\n",
      "Average test loss: 0.0016278419733668367\n",
      "Epoch 218/300\n",
      "Average training loss: 0.007201688562002447\n",
      "Average test loss: 0.001576806124041064\n",
      "Epoch 219/300\n",
      "Average training loss: 0.007197302223079734\n",
      "Average test loss: 0.0016176498393631643\n",
      "Epoch 220/300\n",
      "Average training loss: 0.00719350473003255\n",
      "Average test loss: 0.0016387524574788081\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007194294234116872\n",
      "Average test loss: 0.0016034861692330903\n",
      "Epoch 222/300\n",
      "Average training loss: 0.007192061204877164\n",
      "Average test loss: 0.0016012209169566631\n",
      "Epoch 223/300\n",
      "Average training loss: 0.007184697236865759\n",
      "Average test loss: 0.0015929824533975788\n",
      "Epoch 224/300\n",
      "Average training loss: 0.007187141403141949\n",
      "Average test loss: 0.0016377993999049068\n",
      "Epoch 225/300\n",
      "Average training loss: 0.007187185999833875\n",
      "Average test loss: 0.0016119960792776611\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0071786209046840665\n",
      "Average test loss: 0.001591668686312106\n",
      "Epoch 227/300\n",
      "Average training loss: 0.007179491397821241\n",
      "Average test loss: 0.001655703765133189\n",
      "Epoch 228/300\n",
      "Average training loss: 0.007183976819531785\n",
      "Average test loss: 0.0016055697109550239\n",
      "Epoch 229/300\n",
      "Average training loss: 0.007174562653319703\n",
      "Average test loss: 0.0015658867394344675\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007173247157699532\n",
      "Average test loss: 0.0016232738987439208\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007175929751661089\n",
      "Average test loss: 0.00159509870948063\n",
      "Epoch 232/300\n",
      "Average training loss: 0.007162555508729484\n",
      "Average test loss: 0.0016453998820442292\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007164631903999382\n",
      "Average test loss: 0.0016011477425280545\n",
      "Epoch 234/300\n",
      "Average training loss: 0.00716468768980768\n",
      "Average test loss: 0.0016238782115073668\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007167188248700566\n",
      "Average test loss: 0.001623934486673938\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0071534817967977785\n",
      "Average test loss: 0.0016285065042061938\n",
      "Epoch 237/300\n",
      "Average training loss: 0.007161339346319437\n",
      "Average test loss: 0.0016008951577047507\n",
      "Epoch 238/300\n",
      "Average training loss: 0.007150247380137443\n",
      "Average test loss: 0.001604679197486904\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007144526190227932\n",
      "Average test loss: 0.001601798975115849\n",
      "Epoch 240/300\n",
      "Average training loss: 0.007157145754330688\n",
      "Average test loss: 0.001596832413226366\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007147713308946954\n",
      "Average test loss: 0.0016521550810688899\n",
      "Epoch 242/300\n",
      "Average training loss: 0.007141965684791406\n",
      "Average test loss: 0.0016802561608039671\n",
      "Epoch 243/300\n",
      "Average training loss: 0.007149903500245677\n",
      "Average test loss: 0.001595109146191842\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007136767485696409\n",
      "Average test loss: 0.001631936831192838\n",
      "Epoch 245/300\n",
      "Average training loss: 0.007138783855156766\n",
      "Average test loss: 0.0015992208388116624\n",
      "Epoch 246/300\n",
      "Average training loss: 0.007136067936817805\n",
      "Average test loss: 0.0016750498374717103\n",
      "Epoch 247/300\n",
      "Average training loss: 0.007142005027582248\n",
      "Average test loss: 0.0016240328348552187\n",
      "Epoch 248/300\n",
      "Average training loss: 0.007127075034711096\n",
      "Average test loss: 0.0016279470703254144\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0071323188278410166\n",
      "Average test loss: 0.0015764287308168909\n",
      "Epoch 250/300\n",
      "Average training loss: 0.007131222325066726\n",
      "Average test loss: 0.001657272796612233\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007130740337901645\n",
      "Average test loss: 0.0016143402126100328\n",
      "Epoch 252/300\n",
      "Average training loss: 0.007128196025474204\n",
      "Average test loss: 0.001621857585126741\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007113286314739121\n",
      "Average test loss: 0.0016450070572189158\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0071219126101997165\n",
      "Average test loss: 0.0016123322499915957\n",
      "Epoch 255/300\n",
      "Average training loss: 0.007117708695017629\n",
      "Average test loss: 0.0016089089674254258\n",
      "Epoch 256/300\n",
      "Average training loss: 0.007120474518587192\n",
      "Average test loss: 0.0016197754733471407\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007115019050737222\n",
      "Average test loss: 0.0016551799600323042\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007109378149525987\n",
      "Average test loss: 0.0016425018111864726\n",
      "Epoch 259/300\n",
      "Average training loss: 0.007104672667053011\n",
      "Average test loss: 0.0016289521008729935\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007109845551765627\n",
      "Average test loss: 0.0016232625001834499\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00711153026835786\n",
      "Average test loss: 0.0016313299894746807\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00710269111684627\n",
      "Average test loss: 0.0016311697269168993\n",
      "Epoch 263/300\n",
      "Average training loss: 0.007103588999559482\n",
      "Average test loss: 0.0015900319477336275\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007100864726222224\n",
      "Average test loss: 0.0016311301685248813\n",
      "Epoch 265/300\n",
      "Average training loss: 0.00710069291272925\n",
      "Average test loss: 0.0016516010011028912\n",
      "Epoch 266/300\n",
      "Average training loss: 0.007099596449898349\n",
      "Average test loss: 0.001611034262718426\n",
      "Epoch 267/300\n",
      "Average training loss: 0.007100798934698105\n",
      "Average test loss: 0.0016608684400303496\n",
      "Epoch 268/300\n",
      "Average training loss: 0.007094550068179766\n",
      "Average test loss: 0.0016792449062276217\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0070889585523141755\n",
      "Average test loss: 0.0015981436465970344\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0070900239621599515\n",
      "Average test loss: 0.0015820785752601093\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007088744868834813\n",
      "Average test loss: 0.0016051805825490091\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007094426418758101\n",
      "Average test loss: 0.0016004850226971838\n",
      "Epoch 273/300\n",
      "Average training loss: 0.007088131985316674\n",
      "Average test loss: 0.0015927128466880984\n",
      "Epoch 274/300\n",
      "Average training loss: 0.00707760440516803\n",
      "Average test loss: 0.0016182398640861113\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0070724126646916075\n",
      "Average test loss: 0.0016184244652589163\n",
      "Epoch 276/300\n",
      "Average training loss: 0.007082689180970192\n",
      "Average test loss: 0.0019160291459411382\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007084656409505341\n",
      "Average test loss: 0.001653767899506622\n",
      "Epoch 278/300\n",
      "Average training loss: 0.007070984781616264\n",
      "Average test loss: 0.0016055994460152255\n",
      "Epoch 279/300\n",
      "Average training loss: 0.007070848973260985\n",
      "Average test loss: 0.0016048024494407906\n",
      "Epoch 280/300\n",
      "Average training loss: 0.00707516181220611\n",
      "Average test loss: 0.0016105383517634538\n",
      "Epoch 281/300\n",
      "Average training loss: 0.007075734875682327\n",
      "Average test loss: 0.0017400427097454666\n",
      "Epoch 282/300\n",
      "Average training loss: 0.007075079573111402\n",
      "Average test loss: 0.0019135542530566453\n",
      "Epoch 283/300\n",
      "Average training loss: 0.007061982865962717\n",
      "Average test loss: 0.0016700522115247118\n",
      "Epoch 284/300\n",
      "Average training loss: 0.007068923939433363\n",
      "Average test loss: 0.001624095929838303\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00706681004870269\n",
      "Average test loss: 0.0016218806160613894\n",
      "Epoch 286/300\n",
      "Average training loss: 0.007052634841038121\n",
      "Average test loss: 0.0016106610891098777\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0070647765716744795\n",
      "Average test loss: 0.0016149424728420046\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007057947394334608\n",
      "Average test loss: 0.0016628450910664268\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007059469318224324\n",
      "Average test loss: 0.0016124047228238648\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007063833444896672\n",
      "Average test loss: 0.001650330627233618\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0070573194726473755\n",
      "Average test loss: 0.0015847663726243709\n",
      "Epoch 292/300\n",
      "Average training loss: 0.007045894944419463\n",
      "Average test loss: 0.0016588676527349486\n",
      "Epoch 293/300\n",
      "Average training loss: 0.007060171195616325\n",
      "Average test loss: 0.0016618156474497584\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00704280814776818\n",
      "Average test loss: 0.0016353293411019776\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007046454892390304\n",
      "Average test loss: 0.00160334601936241\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007052438312934505\n",
      "Average test loss: 0.0016602797477195661\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007040865973052052\n",
      "Average test loss: 0.00163857641008993\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0070438266462749905\n",
      "Average test loss: 0.0016852609714907076\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00704673507478502\n",
      "Average test loss: 0.001670049920367698\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007041855281425847\n",
      "Average test loss: 0.0015877929410586754\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.38\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.71\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.50\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.21\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9045730032920838\n",
      "Average test loss: 0.010761704752014743\n",
      "Epoch 2/300\n",
      "Average training loss: 0.20055147471692827\n",
      "Average test loss: 0.004790673549804423\n",
      "Epoch 3/300\n",
      "Average training loss: 0.13712545228004455\n",
      "Average test loss: 0.004560668053312434\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11084359600808885\n",
      "Average test loss: 0.004472964861326747\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09589953323205312\n",
      "Average test loss: 0.00440911728847358\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08695599030123817\n",
      "Average test loss: 0.00437784195277426\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08085208438502417\n",
      "Average test loss: 0.004327765460229583\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07649962722592883\n",
      "Average test loss: 0.004295958323197233\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07312447319096989\n",
      "Average test loss: 0.004253982054276599\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07039444314108954\n",
      "Average test loss: 0.004222553390595648\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06813797370592753\n",
      "Average test loss: 0.004226171154942777\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06656367346644401\n",
      "Average test loss: 0.004194515757262707\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06548225175672107\n",
      "Average test loss: 0.004166541946844922\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06473316563831435\n",
      "Average test loss: 0.004143539733563861\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06410919876231087\n",
      "Average test loss: 0.0041335625586410366\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06358614089091619\n",
      "Average test loss: 0.004128686516442233\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06317130792140961\n",
      "Average test loss: 0.004123641790201266\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06281974772612253\n",
      "Average test loss: 0.00425032796834906\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06248947174681557\n",
      "Average test loss: 0.0041035593178951075\n",
      "Epoch 20/300\n",
      "Average training loss: 0.062190774516926874\n",
      "Average test loss: 0.004074575403498279\n",
      "Epoch 21/300\n",
      "Average training loss: 0.061929591645797095\n",
      "Average test loss: 0.004049451432294316\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06165837722354465\n",
      "Average test loss: 0.004046674347586102\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06142559707495902\n",
      "Average test loss: 0.004067299167315165\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06121903293993738\n",
      "Average test loss: 0.004056814056717688\n",
      "Epoch 25/300\n",
      "Average training loss: 0.061014940718809765\n",
      "Average test loss: 0.0040337684564292434\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06079193789097998\n",
      "Average test loss: 0.004013472765684128\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06062194244729148\n",
      "Average test loss: 0.004009640804181497\n",
      "Epoch 28/300\n",
      "Average training loss: 0.060408419483237795\n",
      "Average test loss: 0.00400938900311788\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06027466766701804\n",
      "Average test loss: 0.003988319759567579\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06004415035247803\n",
      "Average test loss: 0.004014218856684035\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05987170766128434\n",
      "Average test loss: 0.003975146825942728\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05974499901135762\n",
      "Average test loss: 0.003992287110537291\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05959142580959532\n",
      "Average test loss: 0.003964398068686327\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05944960680603981\n",
      "Average test loss: 0.0039480970354957715\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05930941723452674\n",
      "Average test loss: 0.003979021329846647\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05916774329543114\n",
      "Average test loss: 0.0039661603189176985\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05903520331448979\n",
      "Average test loss: 0.0039391086349884665\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05892058288719919\n",
      "Average test loss: 0.003930166797712445\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05880189746949408\n",
      "Average test loss: 0.003929526088552343\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05865241233176655\n",
      "Average test loss: 0.003945735602329175\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0585575206345982\n",
      "Average test loss: 0.003924485317120949\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05845879439512889\n",
      "Average test loss: 0.003921438192948699\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05833283617099126\n",
      "Average test loss: 0.003934315875172615\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05827378277315034\n",
      "Average test loss: 0.003946890454325411\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05813818655742539\n",
      "Average test loss: 0.003927959633577201\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05803782922691769\n",
      "Average test loss: 0.0039003333490755824\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05795433288150364\n",
      "Average test loss: 0.003918422551204761\n",
      "Epoch 48/300\n",
      "Average training loss: 0.057883750740024775\n",
      "Average test loss: 0.003924885734501812\n",
      "Epoch 49/300\n",
      "Average training loss: 0.057776248862346016\n",
      "Average test loss: 0.0039008732315980728\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05771279208858808\n",
      "Average test loss: 0.003926699386288722\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05763172624839677\n",
      "Average test loss: 0.0039037818221582306\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05754287557138337\n",
      "Average test loss: 0.003906407875112362\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05743669008546405\n",
      "Average test loss: 0.003921103230574065\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05742244564493497\n",
      "Average test loss: 0.0039067921468781104\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05729254322581821\n",
      "Average test loss: 0.003887534361746576\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05723361403743426\n",
      "Average test loss: 0.0039154058343006505\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05712566650244925\n",
      "Average test loss: 0.00389377154989375\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05701658766137229\n",
      "Average test loss: 0.0038949383300625616\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05695584997534752\n",
      "Average test loss: 0.0039027774091809986\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05688034944732984\n",
      "Average test loss: 0.003903553113134371\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05681126182277997\n",
      "Average test loss: 0.003898105996557408\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05673714342382219\n",
      "Average test loss: 0.00390951752766139\n",
      "Epoch 63/300\n",
      "Average training loss: 0.056661985251638625\n",
      "Average test loss: 0.003907574358913634\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05655723492966758\n",
      "Average test loss: 0.003906302623864677\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05645606551567713\n",
      "Average test loss: 0.0038931218491246302\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05642743057012558\n",
      "Average test loss: 0.0039208903380980095\n",
      "Epoch 67/300\n",
      "Average training loss: 0.056345769362317194\n",
      "Average test loss: 0.003986570586760839\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05627173461847835\n",
      "Average test loss: 0.0039004213387767474\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05620540316568481\n",
      "Average test loss: 0.003909281935335861\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05604312452342775\n",
      "Average test loss: 0.00391692241633104\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05602663422955407\n",
      "Average test loss: 0.0039320413863493334\n",
      "Epoch 72/300\n",
      "Average training loss: 0.055963335941235225\n",
      "Average test loss: 0.003911874569124646\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05585431627432505\n",
      "Average test loss: 0.00393046410733627\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05575560982690917\n",
      "Average test loss: 0.003935212237967385\n",
      "Epoch 75/300\n",
      "Average training loss: 0.055629978424972956\n",
      "Average test loss: 0.003904541729638974\n",
      "Epoch 76/300\n",
      "Average training loss: 0.055614814317888686\n",
      "Average test loss: 0.003901407190495067\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05550815434257189\n",
      "Average test loss: 0.003952224964482917\n",
      "Epoch 78/300\n",
      "Average training loss: 0.055436755779716705\n",
      "Average test loss: 0.0039124049349791475\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05534701853990555\n",
      "Average test loss: 0.003926289493011104\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05530522879295879\n",
      "Average test loss: 0.004007893744856119\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05513809069659975\n",
      "Average test loss: 0.00394069301088651\n",
      "Epoch 82/300\n",
      "Average training loss: 0.055143658912844125\n",
      "Average test loss: 0.003968830878949827\n",
      "Epoch 83/300\n",
      "Average training loss: 0.055038272930516136\n",
      "Average test loss: 0.003987150016758177\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05488712570733494\n",
      "Average test loss: 0.003936048448085785\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05488133268223869\n",
      "Average test loss: 0.003911063120183017\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05474875486228201\n",
      "Average test loss: 0.003988342617534929\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05467545029852126\n",
      "Average test loss: 0.003941496402439144\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05456299366884761\n",
      "Average test loss: 0.003995513543072674\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05457403680351045\n",
      "Average test loss: 0.003979496534913778\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05439112877514627\n",
      "Average test loss: 0.00395318386496769\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05437453480230437\n",
      "Average test loss: 0.00394001540976266\n",
      "Epoch 92/300\n",
      "Average training loss: 0.054300789864526855\n",
      "Average test loss: 0.003962200694200065\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05418510950273937\n",
      "Average test loss: 0.0039859261454807384\n",
      "Epoch 94/300\n",
      "Average training loss: 0.054053116026851866\n",
      "Average test loss: 0.003996157808022366\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05404440354307492\n",
      "Average test loss: 0.0039470973453587955\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05395859057704608\n",
      "Average test loss: 0.003975372906774283\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05393805209795634\n",
      "Average test loss: 0.003978328148523966\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05382571833994654\n",
      "Average test loss: 0.003964458329396115\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05375153488583035\n",
      "Average test loss: 0.004016364261094067\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05371253804200225\n",
      "Average test loss: 0.004005364444934659\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05363387731048796\n",
      "Average test loss: 0.00411623672561513\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05357039466996988\n",
      "Average test loss: 0.004076023098909192\n",
      "Epoch 103/300\n",
      "Average training loss: 0.053452310883336594\n",
      "Average test loss: 0.0040139909038941065\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05333917265468174\n",
      "Average test loss: 0.004016170309028692\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05329819797144996\n",
      "Average test loss: 0.004054271454198493\n",
      "Epoch 106/300\n",
      "Average training loss: 0.053264507522185645\n",
      "Average test loss: 0.004011990199486414\n",
      "Epoch 107/300\n",
      "Average training loss: 0.053164466708898545\n",
      "Average test loss: 0.003988047043896384\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05311362098654111\n",
      "Average test loss: 0.004085580395948555\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05302909474571546\n",
      "Average test loss: 0.004044726325405969\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05293898126151827\n",
      "Average test loss: 0.004023349593083063\n",
      "Epoch 111/300\n",
      "Average training loss: 0.052874957026706804\n",
      "Average test loss: 0.004107300782576203\n",
      "Epoch 112/300\n",
      "Average training loss: 0.052837715281380544\n",
      "Average test loss: 0.004006528194993734\n",
      "Epoch 113/300\n",
      "Average training loss: 0.052763330688079195\n",
      "Average test loss: 0.004113691015789906\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05271566112173928\n",
      "Average test loss: 0.004110960748667518\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05257976591918204\n",
      "Average test loss: 0.0041101441532373426\n",
      "Epoch 116/300\n",
      "Average training loss: 0.052591216305891676\n",
      "Average test loss: 0.0040936139168010815\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05250675869319174\n",
      "Average test loss: 0.0040165851033396194\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05239707006017367\n",
      "Average test loss: 0.004042716785230571\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05235537550515599\n",
      "Average test loss: 0.004055640045967367\n",
      "Epoch 120/300\n",
      "Average training loss: 0.052328655103842416\n",
      "Average test loss: 0.00407156382025116\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05223329597049289\n",
      "Average test loss: 0.004138821595658859\n",
      "Epoch 122/300\n",
      "Average training loss: 0.052200272705819874\n",
      "Average test loss: 0.004086094667514166\n",
      "Epoch 123/300\n",
      "Average training loss: 0.052151130143139095\n",
      "Average test loss: 0.004103725148157941\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05206234366695086\n",
      "Average test loss: 0.004102872163471248\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05192619175381131\n",
      "Average test loss: 0.004061622965666983\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05196553698513243\n",
      "Average test loss: 0.004106337267284592\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05198014494445589\n",
      "Average test loss: 0.004083925291068024\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05184071847134166\n",
      "Average test loss: 0.0041046628002077345\n",
      "Epoch 129/300\n",
      "Average training loss: 0.051834873328606285\n",
      "Average test loss: 0.004025386817339394\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05173006929953893\n",
      "Average test loss: 0.0041698090847995545\n",
      "Epoch 131/300\n",
      "Average training loss: 0.051634741948710546\n",
      "Average test loss: 0.0040675636120140555\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05162217860089408\n",
      "Average test loss: 0.00418366314491464\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05156220389240318\n",
      "Average test loss: 0.0040811203440858255\n",
      "Epoch 134/300\n",
      "Average training loss: 0.051557454201910234\n",
      "Average test loss: 0.004037952298091518\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05143802258372307\n",
      "Average test loss: 0.004113570912016763\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0514222393747833\n",
      "Average test loss: 0.0041309996466669776\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05130717315276464\n",
      "Average test loss: 0.004071562303437127\n",
      "Epoch 138/300\n",
      "Average training loss: 0.051310337202416526\n",
      "Average test loss: 0.004136338121982084\n",
      "Epoch 139/300\n",
      "Average training loss: 0.051227559960550735\n",
      "Average test loss: 0.004123257166809506\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05129202617208163\n",
      "Average test loss: 0.004031013699455394\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05117379398147265\n",
      "Average test loss: 0.004054761760764651\n",
      "Epoch 142/300\n",
      "Average training loss: 0.051096553626987666\n",
      "Average test loss: 0.0040895356680783965\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05107817037569152\n",
      "Average test loss: 0.004067371922855576\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05103656375408173\n",
      "Average test loss: 0.004077764338503281\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05101633422407839\n",
      "Average test loss: 0.004156824801738064\n",
      "Epoch 146/300\n",
      "Average training loss: 0.050930285771687824\n",
      "Average test loss: 0.0040710437571009\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05088459707299868\n",
      "Average test loss: 0.004127217589567105\n",
      "Epoch 148/300\n",
      "Average training loss: 0.050796573913759656\n",
      "Average test loss: 0.004050878671722279\n",
      "Epoch 149/300\n",
      "Average training loss: 0.050803175608317055\n",
      "Average test loss: 0.0040531810700065556\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05079481829537286\n",
      "Average test loss: 0.004150708059055938\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0507153349187639\n",
      "Average test loss: 0.004229433361233936\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05067423332068655\n",
      "Average test loss: 0.004135160517361429\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05054245013991992\n",
      "Average test loss: 0.0041637723156147536\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05053243573506673\n",
      "Average test loss: 0.004092791042394108\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05050305661890242\n",
      "Average test loss: 0.004076685976444019\n",
      "Epoch 156/300\n",
      "Average training loss: 0.050535295791096155\n",
      "Average test loss: 0.004218890993959374\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05044415308700668\n",
      "Average test loss: 0.0041945420126948095\n",
      "Epoch 158/300\n",
      "Average training loss: 0.050317000018225776\n",
      "Average test loss: 0.0041411465894844795\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0503444630338086\n",
      "Average test loss: 0.0041366740489999455\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0502787461578846\n",
      "Average test loss: 0.00417388597337736\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05029752724369367\n",
      "Average test loss: 0.004132828531579839\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05024176544613308\n",
      "Average test loss: 0.00416323807504442\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05018427133891318\n",
      "Average test loss: 0.004123116777588923\n",
      "Epoch 164/300\n",
      "Average training loss: 0.050156740493244596\n",
      "Average test loss: 0.004148551974652542\n",
      "Epoch 165/300\n",
      "Average training loss: 0.050114798979626764\n",
      "Average test loss: 0.0041787829175591465\n",
      "Epoch 166/300\n",
      "Average training loss: 0.050028490944041146\n",
      "Average test loss: 0.004221396686302291\n",
      "Epoch 167/300\n",
      "Average training loss: 0.050058626115322115\n",
      "Average test loss: 0.004136124088946316\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05001772197087606\n",
      "Average test loss: 0.004201490956669053\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04995807612935702\n",
      "Average test loss: 0.0041329122359553975\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04988656090034379\n",
      "Average test loss: 0.004170287830755115\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04992819210555818\n",
      "Average test loss: 0.004150790907856491\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04985792606737879\n",
      "Average test loss: 0.00408308428277572\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04982102826237678\n",
      "Average test loss: 0.004143410270619724\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04975295592347781\n",
      "Average test loss: 0.004164884825340576\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04973554249273406\n",
      "Average test loss: 0.004152455867785547\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04969551873869366\n",
      "Average test loss: 0.004148653653967712\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04967832029196951\n",
      "Average test loss: 0.004158104072842333\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04962979607780774\n",
      "Average test loss: 0.004229279380705621\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04956706313954459\n",
      "Average test loss: 0.004140892083446185\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04959342663486799\n",
      "Average test loss: 0.004184500724491146\n",
      "Epoch 181/300\n",
      "Average training loss: 0.049570932613478766\n",
      "Average test loss: 0.0041199070974770515\n",
      "Epoch 182/300\n",
      "Average training loss: 0.049536626332336\n",
      "Average test loss: 0.004113052887635099\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04940133023924298\n",
      "Average test loss: 0.004104962517817815\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04944070043663184\n",
      "Average test loss: 0.004189419414434168\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04940470782584614\n",
      "Average test loss: 0.004115220719120569\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04938014218873448\n",
      "Average test loss: 0.004112409874796868\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04928407534625795\n",
      "Average test loss: 0.004149804956797096\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04936413679851426\n",
      "Average test loss: 0.00411341018581556\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04929935203989347\n",
      "Average test loss: 0.0041535725158949694\n",
      "Epoch 190/300\n",
      "Average training loss: 0.049314625763230854\n",
      "Average test loss: 0.0041659788435531985\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04920632801122136\n",
      "Average test loss: 0.004110269395634532\n",
      "Epoch 192/300\n",
      "Average training loss: 0.049222005539470245\n",
      "Average test loss: 0.004261589816461007\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04912663198841943\n",
      "Average test loss: 0.00428377196399702\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04919649990068542\n",
      "Average test loss: 0.004234783251873321\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04910307761033376\n",
      "Average test loss: 0.004148333294110166\n",
      "Epoch 196/300\n",
      "Average training loss: 0.049046436144245996\n",
      "Average test loss: 0.004201501157134771\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04904122511876954\n",
      "Average test loss: 0.0043232712869842845\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04901471557882097\n",
      "Average test loss: 0.0042538827711509335\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04899045338564449\n",
      "Average test loss: 0.004148668796237972\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04892971176571316\n",
      "Average test loss: 0.004098006296075053\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04897850258482827\n",
      "Average test loss: 0.004267001053111421\n",
      "Epoch 202/300\n",
      "Average training loss: 0.048897209452258214\n",
      "Average test loss: 0.0042681192954381305\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04885207158492671\n",
      "Average test loss: 0.004215599978342652\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04880969477362103\n",
      "Average test loss: 0.004174806764142381\n",
      "Epoch 205/300\n",
      "Average training loss: 0.048765605794058905\n",
      "Average test loss: 0.004204183665621612\n",
      "Epoch 206/300\n",
      "Average training loss: 0.048771600236495335\n",
      "Average test loss: 0.004242540023186141\n",
      "Epoch 207/300\n",
      "Average training loss: 0.048778055644697615\n",
      "Average test loss: 0.004172991114772029\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04874360512693723\n",
      "Average test loss: 0.004251594519449605\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04870446915758981\n",
      "Average test loss: 0.004225734831765294\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04869569912221697\n",
      "Average test loss: 0.004189621988683939\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04866880128449864\n",
      "Average test loss: 0.004154495583019323\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04863758161995146\n",
      "Average test loss: 0.004260204186456071\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04853898515635067\n",
      "Average test loss: 0.004272732505367862\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04854995166924265\n",
      "Average test loss: 0.004150080333567328\n",
      "Epoch 215/300\n",
      "Average training loss: 0.048509181913402345\n",
      "Average test loss: 0.00420283501346906\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04852195377482308\n",
      "Average test loss: 0.0041286040362384585\n",
      "Epoch 217/300\n",
      "Average training loss: 0.048489613135655724\n",
      "Average test loss: 0.0041558814539263645\n",
      "Epoch 218/300\n",
      "Average training loss: 0.048441641979747346\n",
      "Average test loss: 0.004167069971768392\n",
      "Epoch 219/300\n",
      "Average training loss: 0.048446527351935706\n",
      "Average test loss: 0.004199118765691916\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04842889281113943\n",
      "Average test loss: 0.004115219251977073\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04847544571095043\n",
      "Average test loss: 0.004188731782138347\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04838453126947085\n",
      "Average test loss: 0.004204071859104766\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04833763879537582\n",
      "Average test loss: 0.004208911790409022\n",
      "Epoch 224/300\n",
      "Average training loss: 0.048298648807737564\n",
      "Average test loss: 0.004176901436514325\n",
      "Epoch 225/300\n",
      "Average training loss: 0.048286793463759956\n",
      "Average test loss: 0.004220075419379605\n",
      "Epoch 226/300\n",
      "Average training loss: 0.048233785543176866\n",
      "Average test loss: 0.0042910411689016555\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04824721679091454\n",
      "Average test loss: 0.004218873581952519\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0482323841618167\n",
      "Average test loss: 0.004209436552599073\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04822453473011653\n",
      "Average test loss: 0.004378928848438793\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04820632932914628\n",
      "Average test loss: 0.004125164702948597\n",
      "Epoch 231/300\n",
      "Average training loss: 0.048115238739384544\n",
      "Average test loss: 0.0042434326315091715\n",
      "Epoch 232/300\n",
      "Average training loss: 0.048142928186390134\n",
      "Average test loss: 0.00419334387820628\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04816635358002451\n",
      "Average test loss: 0.004337990132470926\n",
      "Epoch 234/300\n",
      "Average training loss: 0.048051961842510434\n",
      "Average test loss: 0.004168037038296461\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04807708559433619\n",
      "Average test loss: 0.004187839400437143\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04803989264700148\n",
      "Average test loss: 0.004301081624295976\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04793385215269195\n",
      "Average test loss: 0.0042162247583683995\n",
      "Epoch 238/300\n",
      "Average training loss: 0.048064281579520966\n",
      "Average test loss: 0.004165962311956617\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04800080578525861\n",
      "Average test loss: 0.004291005982293023\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04789476210872332\n",
      "Average test loss: 0.004260904630439149\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04791743779513571\n",
      "Average test loss: 0.004268226190573639\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04788045677542686\n",
      "Average test loss: 0.004240377650906642\n",
      "Epoch 243/300\n",
      "Average training loss: 0.047922794842057755\n",
      "Average test loss: 0.004310978264651365\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04781437864237362\n",
      "Average test loss: 0.004153657174979647\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04788771941926744\n",
      "Average test loss: 0.004208532969156901\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04786194325983524\n",
      "Average test loss: 0.004158720248275333\n",
      "Epoch 247/300\n",
      "Average training loss: 0.047822477608919146\n",
      "Average test loss: 0.00422624266313182\n",
      "Epoch 248/300\n",
      "Average training loss: 0.047818440232012004\n",
      "Average test loss: 0.004228585050337844\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04780288014147017\n",
      "Average test loss: 0.004113895873228709\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04774089907606443\n",
      "Average test loss: 0.004212548210596045\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04770126267936495\n",
      "Average test loss: 0.0042228805464175016\n",
      "Epoch 252/300\n",
      "Average training loss: 0.047723560790220894\n",
      "Average test loss: 0.004378002165920204\n",
      "Epoch 253/300\n",
      "Average training loss: 0.047699771914217204\n",
      "Average test loss: 0.004287742533617549\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04767053880294164\n",
      "Average test loss: 0.004254564453951186\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04767861473560333\n",
      "Average test loss: 0.004295997788094812\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04767019393377834\n",
      "Average test loss: 0.004129532265580362\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04760240796539519\n",
      "Average test loss: 0.004218768416593472\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04757263566056887\n",
      "Average test loss: 0.004226348039176729\n",
      "Epoch 259/300\n",
      "Average training loss: 0.047523340940475466\n",
      "Average test loss: 0.004185758450999856\n",
      "Epoch 260/300\n",
      "Average training loss: 0.047507637057039476\n",
      "Average test loss: 0.004264024884957406\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04752997762627072\n",
      "Average test loss: 0.004262273115830289\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04753230502208074\n",
      "Average test loss: 0.004286533490237263\n",
      "Epoch 263/300\n",
      "Average training loss: 0.047524463186661405\n",
      "Average test loss: 0.00435188805208438\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04741205546922154\n",
      "Average test loss: 0.004223982939289676\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04752297392818663\n",
      "Average test loss: 0.004271497546591693\n",
      "Epoch 266/300\n",
      "Average training loss: 0.047431333237224156\n",
      "Average test loss: 0.004188358479489882\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04744440249602\n",
      "Average test loss: 0.004324531489362319\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04743631429804696\n",
      "Average test loss: 0.004335041085672047\n",
      "Epoch 269/300\n",
      "Average training loss: 0.047411573509375256\n",
      "Average test loss: 0.004297110327002075\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0473616395857599\n",
      "Average test loss: 0.004327924219684468\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04737361459599601\n",
      "Average test loss: 0.004163428857301673\n",
      "Epoch 272/300\n",
      "Average training loss: 0.047321423447794386\n",
      "Average test loss: 0.004236393625537555\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04730286787615882\n",
      "Average test loss: 0.0043202389253096446\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04727508750889036\n",
      "Average test loss: 0.004286459790749682\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04730125010675854\n",
      "Average test loss: 0.004219578344581856\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04725515145560106\n",
      "Average test loss: 0.00437787954426474\n",
      "Epoch 277/300\n",
      "Average training loss: 0.047280147390233146\n",
      "Average test loss: 0.00420429431191749\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04722704552279578\n",
      "Average test loss: 0.004352615376313527\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04726843091348807\n",
      "Average test loss: 0.004216485108766291\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04716441461775038\n",
      "Average test loss: 0.004256827380508184\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04709145110514429\n",
      "Average test loss: 0.0042979440902256305\n",
      "Epoch 282/300\n",
      "Average training loss: 0.047169493003024\n",
      "Average test loss: 0.0042206416014168\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04715513200230068\n",
      "Average test loss: 0.004314163838823637\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04715086686942312\n",
      "Average test loss: 0.004613354441192415\n",
      "Epoch 285/300\n",
      "Average training loss: 0.047120570060279636\n",
      "Average test loss: 0.004204937636852264\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04702303952640957\n",
      "Average test loss: 0.004307085159752104\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04705225014355448\n",
      "Average test loss: 0.004250537538797491\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04698756906721327\n",
      "Average test loss: 0.004305773070082068\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04703560275170538\n",
      "Average test loss: 0.0043753462106817295\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04705355703168445\n",
      "Average test loss: 0.004318857898728715\n",
      "Epoch 291/300\n",
      "Average training loss: 0.047004262202315863\n",
      "Average test loss: 0.004200228779266278\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04698232342799505\n",
      "Average test loss: 0.004205040639473332\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04698055805762609\n",
      "Average test loss: 0.004189657479524612\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04694728514552116\n",
      "Average test loss: 0.0042302064336836335\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04691658166382048\n",
      "Average test loss: 0.00417543475723101\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04695318364103635\n",
      "Average test loss: 0.004326590133210023\n",
      "Epoch 297/300\n",
      "Average training loss: 0.046927317053079606\n",
      "Average test loss: 0.0043258974583198625\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0468764643073082\n",
      "Average test loss: 0.0041922575169139435\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04688888456755214\n",
      "Average test loss: 0.00416451603339778\n",
      "Epoch 300/300\n",
      "Average training loss: 0.046821198006471\n",
      "Average test loss: 0.004257515218108893\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9148518805238935\n",
      "Average test loss: 0.004788219563663006\n",
      "Epoch 2/300\n",
      "Average training loss: 0.20487001033624014\n",
      "Average test loss: 0.004314301737273733\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12680528499020471\n",
      "Average test loss: 0.00413050628412101\n",
      "Epoch 4/300\n",
      "Average training loss: 0.09843777004877727\n",
      "Average test loss: 0.004020712048643165\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08403498381376266\n",
      "Average test loss: 0.0038431001367668313\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07518540352582931\n",
      "Average test loss: 0.0037285439338948993\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0689243267443445\n",
      "Average test loss: 0.0036744058292566073\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06464487508601613\n",
      "Average test loss: 0.003595698555724488\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06170892584986157\n",
      "Average test loss: 0.0035605370816257265\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05959956612851885\n",
      "Average test loss: 0.0035255443687654204\n",
      "Epoch 11/300\n",
      "Average training loss: 0.058009660091665055\n",
      "Average test loss: 0.003438479956653383\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05670171054866579\n",
      "Average test loss: 0.003688829039533933\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05566939615872171\n",
      "Average test loss: 0.0033618917434165875\n",
      "Epoch 14/300\n",
      "Average training loss: 0.054758047779401145\n",
      "Average test loss: 0.0033775285345812637\n",
      "Epoch 15/300\n",
      "Average training loss: 0.053961162391636106\n",
      "Average test loss: 0.003327430778493484\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05326268347766664\n",
      "Average test loss: 0.003339095117524266\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05260381861527761\n",
      "Average test loss: 0.0032028102382189697\n",
      "Epoch 18/300\n",
      "Average training loss: 0.052040593951940536\n",
      "Average test loss: 0.0031624130134781203\n",
      "Epoch 19/300\n",
      "Average training loss: 0.051476560645633274\n",
      "Average test loss: 0.0031268649411698183\n",
      "Epoch 20/300\n",
      "Average training loss: 0.050897635569175086\n",
      "Average test loss: 0.0031158724303046864\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0504132805565993\n",
      "Average test loss: 0.0031112520984477466\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04989115549458398\n",
      "Average test loss: 0.0032113735537148184\n",
      "Epoch 23/300\n",
      "Average training loss: 0.049504309160841836\n",
      "Average test loss: 0.0030572451011588175\n",
      "Epoch 24/300\n",
      "Average training loss: 0.049000651756922406\n",
      "Average test loss: 0.0030628371230430073\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04855103809965981\n",
      "Average test loss: 0.003026395421475172\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04811863944762283\n",
      "Average test loss: 0.0030266046652363404\n",
      "Epoch 27/300\n",
      "Average training loss: 0.047816642466518615\n",
      "Average test loss: 0.002965715340649088\n",
      "Epoch 28/300\n",
      "Average training loss: 0.047391885684596166\n",
      "Average test loss: 0.0029653750368290478\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04706794352995025\n",
      "Average test loss: 0.002988978595369392\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04673326566484239\n",
      "Average test loss: 0.002951611804258492\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04639391016960144\n",
      "Average test loss: 0.0029357938905143076\n",
      "Epoch 32/300\n",
      "Average training loss: 0.046171953741047114\n",
      "Average test loss: 0.002930974062325226\n",
      "Epoch 33/300\n",
      "Average training loss: 0.045855489250686436\n",
      "Average test loss: 0.0029253901216304964\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04558987732728322\n",
      "Average test loss: 0.0029088276227315266\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04534809405273861\n",
      "Average test loss: 0.0029131895794222754\n",
      "Epoch 36/300\n",
      "Average training loss: 0.045081220550669564\n",
      "Average test loss: 0.0029171473176942933\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04482791649632984\n",
      "Average test loss: 0.002906240711195601\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04456800590289964\n",
      "Average test loss: 0.002889928191796773\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04440200682149993\n",
      "Average test loss: 0.0029425347873734105\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04418682811988724\n",
      "Average test loss: 0.0028702834240264363\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04404672885272238\n",
      "Average test loss: 0.002852149746691187\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04374527990818024\n",
      "Average test loss: 0.002868219162647923\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04358140633503596\n",
      "Average test loss: 0.002861612507245607\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04343354094028473\n",
      "Average test loss: 0.002852622249888049\n",
      "Epoch 45/300\n",
      "Average training loss: 0.043258808496925566\n",
      "Average test loss: 0.0028711413830104803\n",
      "Epoch 46/300\n",
      "Average training loss: 0.043046826405657664\n",
      "Average test loss: 0.002846535298973322\n",
      "Epoch 47/300\n",
      "Average training loss: 0.042947373090518846\n",
      "Average test loss: 0.002861632702251275\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04276607954833243\n",
      "Average test loss: 0.002852869553698434\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04256931909587648\n",
      "Average test loss: 0.002848327071302467\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0424319561868906\n",
      "Average test loss: 0.002883291299144427\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04229044796360864\n",
      "Average test loss: 0.002897438337405523\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04212481901380751\n",
      "Average test loss: 0.002846286864537332\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04198556994232867\n",
      "Average test loss: 0.002824381004398068\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04180768052405781\n",
      "Average test loss: 0.0028328662703020705\n",
      "Epoch 55/300\n",
      "Average training loss: 0.041667786727348964\n",
      "Average test loss: 0.002837088195193145\n",
      "Epoch 56/300\n",
      "Average training loss: 0.041547230670849485\n",
      "Average test loss: 0.002859673135810428\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04135512801011403\n",
      "Average test loss: 0.0029389176468054452\n",
      "Epoch 58/300\n",
      "Average training loss: 0.041248778879642484\n",
      "Average test loss: 0.0030321454827984175\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04113497588038444\n",
      "Average test loss: 0.002865052031353116\n",
      "Epoch 60/300\n",
      "Average training loss: 0.040965094251765144\n",
      "Average test loss: 0.002857437372000681\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04080184089806345\n",
      "Average test loss: 0.0028330643139779566\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04072137129969067\n",
      "Average test loss: 0.002840429197996855\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04055705824163225\n",
      "Average test loss: 0.0028406927494539153\n",
      "Epoch 64/300\n",
      "Average training loss: 0.040439836846457584\n",
      "Average test loss: 0.0028544676090694137\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04031603940327962\n",
      "Average test loss: 0.002862451459384627\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04018413138389587\n",
      "Average test loss: 0.0028633436707572804\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04001864916914039\n",
      "Average test loss: 0.0028504927843395205\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03993978949719005\n",
      "Average test loss: 0.0028685530660053095\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03980369689067205\n",
      "Average test loss: 0.0029506938552690876\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03969126730495029\n",
      "Average test loss: 0.002908081464469433\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03956931755774551\n",
      "Average test loss: 0.0028967012874782086\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03941387020382616\n",
      "Average test loss: 0.0028744100607517695\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03930848593016466\n",
      "Average test loss: 0.002972590449162655\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03924598968691296\n",
      "Average test loss: 0.002957481543223063\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03909049419230885\n",
      "Average test loss: 0.0028735980021042957\n",
      "Epoch 76/300\n",
      "Average training loss: 0.038985712869299784\n",
      "Average test loss: 0.00290503251635366\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0388618065516154\n",
      "Average test loss: 0.0029193666600104834\n",
      "Epoch 78/300\n",
      "Average training loss: 0.038790625830491386\n",
      "Average test loss: 0.002892856975396474\n",
      "Epoch 79/300\n",
      "Average training loss: 0.038689216524362566\n",
      "Average test loss: 0.002934150400881966\n",
      "Epoch 80/300\n",
      "Average training loss: 0.038557648317681416\n",
      "Average test loss: 0.002961740992135472\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03850346118211746\n",
      "Average test loss: 0.0030014795263608295\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03837554250823127\n",
      "Average test loss: 0.0028893840989718833\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03829225962029563\n",
      "Average test loss: 0.0029976534557839235\n",
      "Epoch 84/300\n",
      "Average training loss: 0.038216092596451444\n",
      "Average test loss: 0.0029390669289148515\n",
      "Epoch 85/300\n",
      "Average training loss: 0.038087378243605294\n",
      "Average test loss: 0.002966348092796074\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03801973499523269\n",
      "Average test loss: 0.002979214830841455\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03788949130309953\n",
      "Average test loss: 0.0029536334578361776\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0378782139238384\n",
      "Average test loss: 0.0029242297182273534\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03784191782606973\n",
      "Average test loss: 0.0030263790618628265\n",
      "Epoch 90/300\n",
      "Average training loss: 0.037662292235427434\n",
      "Average test loss: 0.0029078199641986028\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03756597711311446\n",
      "Average test loss: 0.0029459815743482776\n",
      "Epoch 92/300\n",
      "Average training loss: 0.037494509551260206\n",
      "Average test loss: 0.0029464752417471675\n",
      "Epoch 93/300\n",
      "Average training loss: 0.037481618374586106\n",
      "Average test loss: 0.0029397968765762115\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03735411997967296\n",
      "Average test loss: 0.003033490358541409\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03738253309826056\n",
      "Average test loss: 0.0029361731332416337\n",
      "Epoch 96/300\n",
      "Average training loss: 0.037172057459751766\n",
      "Average test loss: 0.0029346262622210713\n",
      "Epoch 97/300\n",
      "Average training loss: 0.037195231626431145\n",
      "Average test loss: 0.0030081248072286446\n",
      "Epoch 98/300\n",
      "Average training loss: 0.037077433324522444\n",
      "Average test loss: 0.0030129038801209795\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03698482366899649\n",
      "Average test loss: 0.0029615485813054773\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03693756597240766\n",
      "Average test loss: 0.0029794023885495132\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03689847974727551\n",
      "Average test loss: 0.0030406634509563446\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03675479543209076\n",
      "Average test loss: 0.0029716138667944406\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0368371584928698\n",
      "Average test loss: 0.0029685855380569893\n",
      "Epoch 104/300\n",
      "Average training loss: 0.036660121553474\n",
      "Average test loss: 0.0029467789398299324\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0366242448621326\n",
      "Average test loss: 0.002931820603294505\n",
      "Epoch 106/300\n",
      "Average training loss: 0.036568077723185224\n",
      "Average test loss: 0.003000912412069738\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03641289659010039\n",
      "Average test loss: 0.002906631336857875\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0364339701698886\n",
      "Average test loss: 0.003010080127252473\n",
      "Epoch 109/300\n",
      "Average training loss: 0.036407153743836615\n",
      "Average test loss: 0.0030204842848082386\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03626649326417181\n",
      "Average test loss: 0.002977554202907615\n",
      "Epoch 111/300\n",
      "Average training loss: 0.036249665634499655\n",
      "Average test loss: 0.0029711802707364163\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03616458115312788\n",
      "Average test loss: 0.0029811705843442015\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03614592948887083\n",
      "Average test loss: 0.002966591809358862\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03611644562913312\n",
      "Average test loss: 0.0030301863356596895\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03600761471523179\n",
      "Average test loss: 0.003122119785596927\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03596644701891475\n",
      "Average test loss: 0.0030492717685798804\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03597399909959899\n",
      "Average test loss: 0.0029589581071502633\n",
      "Epoch 118/300\n",
      "Average training loss: 0.035874018410841625\n",
      "Average test loss: 0.0030804878201128707\n",
      "Epoch 119/300\n",
      "Average training loss: 0.035869381441010366\n",
      "Average test loss: 0.002975592986163166\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03575872229867511\n",
      "Average test loss: 0.0030985554661601784\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03568807403908836\n",
      "Average test loss: 0.003097565779876378\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0357126753297117\n",
      "Average test loss: 0.0030664832176019747\n",
      "Epoch 123/300\n",
      "Average training loss: 0.035621370890074305\n",
      "Average test loss: 0.0029913017745647166\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03556975927286678\n",
      "Average test loss: 0.0030920617212024\n",
      "Epoch 125/300\n",
      "Average training loss: 0.035563567065530356\n",
      "Average test loss: 0.003000201584978236\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03551154291464223\n",
      "Average test loss: 0.0030411999583658243\n",
      "Epoch 127/300\n",
      "Average training loss: 0.035450973548822935\n",
      "Average test loss: 0.0029693634791506662\n",
      "Epoch 128/300\n",
      "Average training loss: 0.035487600998746024\n",
      "Average test loss: 0.0030283572288850942\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03536108711361885\n",
      "Average test loss: 0.0030357630136940215\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03529436899224917\n",
      "Average test loss: 0.0030239081982937122\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03530162403649754\n",
      "Average test loss: 0.0029901683570610154\n",
      "Epoch 132/300\n",
      "Average training loss: 0.035269463361965285\n",
      "Average test loss: 0.0029575775280180905\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03524077009989156\n",
      "Average test loss: 0.0029548874431186254\n",
      "Epoch 134/300\n",
      "Average training loss: 0.035183551930718954\n",
      "Average test loss: 0.0030718808617028926\n",
      "Epoch 135/300\n",
      "Average training loss: 0.035114239404598874\n",
      "Average test loss: 0.0030194238424301146\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03501225749982728\n",
      "Average test loss: 0.003140002780697412\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03504045232799318\n",
      "Average test loss: 0.002982955004605982\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03499313602844874\n",
      "Average test loss: 0.0030763241086776056\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03496776503655646\n",
      "Average test loss: 0.003066328127764993\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03491205800241894\n",
      "Average test loss: 0.003150935408555799\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03488785378469361\n",
      "Average test loss: 0.003098481701480018\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03484465575880474\n",
      "Average test loss: 0.0030058048186409805\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03483666704263952\n",
      "Average test loss: 0.0030501581664300627\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03477422603633669\n",
      "Average test loss: 0.0030346138702912464\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03476495644615756\n",
      "Average test loss: 0.003053248685681158\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03479026166929139\n",
      "Average test loss: 0.002986197541364365\n",
      "Epoch 147/300\n",
      "Average training loss: 0.034635729991727406\n",
      "Average test loss: 0.003018997369437582\n",
      "Epoch 148/300\n",
      "Average training loss: 0.034588152796030044\n",
      "Average test loss: 0.0031360491340359053\n",
      "Epoch 149/300\n",
      "Average training loss: 0.034584158781501985\n",
      "Average test loss: 0.0030493264579110673\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03462930955655045\n",
      "Average test loss: 0.0030326620971577035\n",
      "Epoch 151/300\n",
      "Average training loss: 0.034612193245026804\n",
      "Average test loss: 0.0031762293804850844\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03452149380246798\n",
      "Average test loss: 0.0030741596337821747\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03449561975399653\n",
      "Average test loss: 0.00304503940552887\n",
      "Epoch 154/300\n",
      "Average training loss: 0.034444538972444004\n",
      "Average test loss: 0.0030493481995330918\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03444243957599004\n",
      "Average test loss: 0.003065304410747356\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03440449389484194\n",
      "Average test loss: 0.0030664022678716313\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03434715408086777\n",
      "Average test loss: 0.003105713805390729\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03438899740907881\n",
      "Average test loss: 0.003146264963886804\n",
      "Epoch 159/300\n",
      "Average training loss: 0.034222975573605964\n",
      "Average test loss: 0.003030432487527529\n",
      "Epoch 160/300\n",
      "Average training loss: 0.034244582866628966\n",
      "Average test loss: 0.0030268795046334467\n",
      "Epoch 161/300\n",
      "Average training loss: 0.034238312638468216\n",
      "Average test loss: 0.003094522304315534\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03419396645161841\n",
      "Average test loss: 0.0030741738805340397\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03422338113685449\n",
      "Average test loss: 0.003092838521839844\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03413436017599371\n",
      "Average test loss: 0.0030337414008875688\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03408088577124808\n",
      "Average test loss: 0.00312053423995773\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03408037271433406\n",
      "Average test loss: 0.003059336771671143\n",
      "Epoch 167/300\n",
      "Average training loss: 0.034031634906927746\n",
      "Average test loss: 0.003013980503090554\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03408593418283595\n",
      "Average test loss: 0.0031994375857628053\n",
      "Epoch 169/300\n",
      "Average training loss: 0.034030848721663155\n",
      "Average test loss: 0.0030362307054715024\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03399757148159875\n",
      "Average test loss: 0.0031286516630401216\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03395959105756548\n",
      "Average test loss: 0.003057643324136734\n",
      "Epoch 172/300\n",
      "Average training loss: 0.033895984881454046\n",
      "Average test loss: 0.0031374955802328055\n",
      "Epoch 173/300\n",
      "Average training loss: 0.033863944914605884\n",
      "Average test loss: 0.003097799891399013\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03398349039753278\n",
      "Average test loss: 0.003107370144377152\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0337851780421204\n",
      "Average test loss: 0.003121545385155413\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03382457200355\n",
      "Average test loss: 0.0030069620952837997\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03376661506957478\n",
      "Average test loss: 0.00307139462708599\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03374553026424514\n",
      "Average test loss: 0.0031372507320096095\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03377461627622445\n",
      "Average test loss: 0.003125660804203815\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03376531987388929\n",
      "Average test loss: 0.0030704003202004565\n",
      "Epoch 181/300\n",
      "Average training loss: 0.033692009872860376\n",
      "Average test loss: 0.0030672415693600973\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03367107710242272\n",
      "Average test loss: 0.0030510389699290198\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03369482603503598\n",
      "Average test loss: 0.00309761853567842\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03365326905912823\n",
      "Average test loss: 0.0032028290109915865\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03364029026197063\n",
      "Average test loss: 0.0031544994161360794\n",
      "Epoch 186/300\n",
      "Average training loss: 0.033577953691283864\n",
      "Average test loss: 0.0030601786252939037\n",
      "Epoch 187/300\n",
      "Average training loss: 0.033600734018617205\n",
      "Average test loss: 0.003142687179562118\n",
      "Epoch 188/300\n",
      "Average training loss: 0.033532747065027554\n",
      "Average test loss: 0.0031236676453716226\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03353201394445366\n",
      "Average test loss: 0.0030652512523035207\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03346889911426438\n",
      "Average test loss: 0.0030549347913927504\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03345000005927351\n",
      "Average test loss: 0.003197816365501947\n",
      "Epoch 192/300\n",
      "Average training loss: 0.033460246180494625\n",
      "Average test loss: 0.0031005415469408035\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03342322566111883\n",
      "Average test loss: 0.0030667178987835845\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03342551968991756\n",
      "Average test loss: 0.0031097101887895\n",
      "Epoch 195/300\n",
      "Average training loss: 0.033353423333830304\n",
      "Average test loss: 0.003107245316521989\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03337358271082242\n",
      "Average test loss: 0.0031551935513400367\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03334597952001624\n",
      "Average test loss: 0.003076158952381876\n",
      "Epoch 198/300\n",
      "Average training loss: 0.033308028280735015\n",
      "Average test loss: 0.003099994456188546\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03330355050000879\n",
      "Average test loss: 0.003165401178929541\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03330361089772648\n",
      "Average test loss: 0.003057429889217019\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03323507443898254\n",
      "Average test loss: 0.003163225659686658\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03323545096649064\n",
      "Average test loss: 0.003036139655030436\n",
      "Epoch 203/300\n",
      "Average training loss: 0.033322302232186\n",
      "Average test loss: 0.0030892020873725416\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03323570454451773\n",
      "Average test loss: 0.003116897329584592\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03316268570224444\n",
      "Average test loss: 0.0030681476032154427\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03310883407791455\n",
      "Average test loss: 0.003114751723077562\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03311489007373651\n",
      "Average test loss: 0.0030884044526351823\n",
      "Epoch 208/300\n",
      "Average training loss: 0.033188647041718165\n",
      "Average test loss: 0.003127457767724991\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0330904036462307\n",
      "Average test loss: 0.0032018418365882505\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03305267314778434\n",
      "Average test loss: 0.0030879429773324066\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03299726153579023\n",
      "Average test loss: 0.003164715017709467\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03302109193139606\n",
      "Average test loss: 0.003125024358845419\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03308214514454206\n",
      "Average test loss: 0.0031128237213318547\n",
      "Epoch 214/300\n",
      "Average training loss: 0.033011122951904934\n",
      "Average test loss: 0.003349660348974996\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03295961867107285\n",
      "Average test loss: 0.003151691630896595\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0330047536459234\n",
      "Average test loss: 0.0031854051157004302\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03295768740442064\n",
      "Average test loss: 0.0031107691345322462\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03290995688736439\n",
      "Average test loss: 0.003107900008973148\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03295997282862663\n",
      "Average test loss: 0.00323215856610073\n",
      "Epoch 220/300\n",
      "Average training loss: 0.032860661034782725\n",
      "Average test loss: 0.003193197982178794\n",
      "Epoch 221/300\n",
      "Average training loss: 0.032952412135071225\n",
      "Average test loss: 0.0031461331540097795\n",
      "Epoch 222/300\n",
      "Average training loss: 0.032818980587853326\n",
      "Average test loss: 0.003117058615717623\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03287088712718752\n",
      "Average test loss: 0.003122322409517235\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03279967727429337\n",
      "Average test loss: 0.0031110572529335816\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03281541111071905\n",
      "Average test loss: 0.003153190537045399\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03281611953510179\n",
      "Average test loss: 0.003262408641891347\n",
      "Epoch 227/300\n",
      "Average training loss: 0.032764619557393924\n",
      "Average test loss: 0.003077557594411903\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03277993772096104\n",
      "Average test loss: 0.003282765493210819\n",
      "Epoch 229/300\n",
      "Average training loss: 0.032782495334744456\n",
      "Average test loss: 0.0032066381615069176\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0327799660232332\n",
      "Average test loss: 0.0031812640842464237\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03264811399247911\n",
      "Average test loss: 0.003145810038679176\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03271805321673552\n",
      "Average test loss: 0.0032220774771024785\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03265098161498706\n",
      "Average test loss: 0.003094255421310663\n",
      "Epoch 234/300\n",
      "Average training loss: 0.032690926122996546\n",
      "Average test loss: 0.003179156853713923\n",
      "Epoch 235/300\n",
      "Average training loss: 0.032651018364561926\n",
      "Average test loss: 0.0032543332820965183\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0326490640408463\n",
      "Average test loss: 0.003152868755161762\n",
      "Epoch 237/300\n",
      "Average test loss: 0.003149649485738741\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03261084147791068\n",
      "Average test loss: 0.003095892472813527\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03259471393956079\n",
      "Average test loss: 0.003146394273887078\n",
      "Epoch 240/300\n",
      "Average training loss: 0.032557508783208\n",
      "Average test loss: 0.003166351452262865\n",
      "Epoch 241/300\n",
      "Average training loss: 0.032553930590550104\n",
      "Average test loss: 0.0032171134549296566\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03255425516102049\n",
      "Average test loss: 0.0030677781299584444\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03253004471957684\n",
      "Average test loss: 0.0031675381482475333\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03249207099775473\n",
      "Average test loss: 0.0035683698538276883\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03254634791281488\n",
      "Average test loss: 0.0031139671568655306\n",
      "Epoch 246/300\n",
      "Average training loss: 0.032452737228737934\n",
      "Average test loss: 0.003170217934064567\n",
      "Epoch 247/300\n",
      "Average training loss: 0.032468003547853896\n",
      "Average test loss: 0.0031019783690571785\n",
      "Epoch 248/300\n",
      "Average training loss: 0.032469386234879494\n",
      "Average test loss: 0.003208880878571007\n",
      "Epoch 249/300\n",
      "Average training loss: 0.032386660754680635\n",
      "Average test loss: 0.003060149122443464\n",
      "Epoch 250/300\n",
      "Average training loss: 0.032409856950243315\n",
      "Average test loss: 0.0032309091730664172\n",
      "Epoch 251/300\n",
      "Average training loss: 0.032364475578069686\n",
      "Average test loss: 0.0033021574753026167\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03242477479908201\n",
      "Average test loss: 0.0032013403421474827\n",
      "Epoch 253/300\n",
      "Average training loss: 0.032361823752522466\n",
      "Average test loss: 0.0031544670313596724\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03237840992212296\n",
      "Average test loss: 0.003172506976251801\n",
      "Epoch 255/300\n",
      "Average training loss: 0.032368736770417954\n",
      "Average test loss: 0.0031812707657615344\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03239791138304604\n",
      "Average test loss: 0.0031946493693523936\n",
      "Epoch 257/300\n",
      "Average training loss: 0.032310217388802104\n",
      "Average test loss: 0.003195186894800928\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03227842121654086\n",
      "Average test loss: 0.003496182665642765\n",
      "Epoch 259/300\n",
      "Average training loss: 0.032375736705131004\n",
      "Average test loss: 0.0032308635101136233\n",
      "Epoch 260/300\n",
      "Average training loss: 0.032278569898671575\n",
      "Average test loss: 0.0033293963877691164\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03230566348963314\n",
      "Average test loss: 0.0031123242338912355\n",
      "Epoch 262/300\n",
      "Average training loss: 0.032300398763683104\n",
      "Average test loss: 0.0032406709713654384\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03223357813060284\n",
      "Average test loss: 0.003148264263032211\n",
      "Epoch 264/300\n",
      "Average training loss: 0.032321759591499966\n",
      "Average test loss: 0.003194254028300444\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03226778876947032\n",
      "Average test loss: 0.0031772681480894487\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03215267012185521\n",
      "Average test loss: 0.0032772844241311153\n",
      "Epoch 267/300\n",
      "Average training loss: 0.032255086380574435\n",
      "Average test loss: 0.0032232337540222538\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03216010624170303\n",
      "Average test loss: 0.0031653399633036717\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03218510955240991\n",
      "Average test loss: 0.0032028955991069475\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03214129102561209\n",
      "Average test loss: 0.003260805220860574\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03221888923810588\n",
      "Average test loss: 0.003132431294976009\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03212898293468687\n",
      "Average test loss: 0.0030845389626920224\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03209374415212207\n",
      "Average test loss: 0.0031897072332600753\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03210700719555219\n",
      "Average test loss: 0.003204744731593463\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03205847873952654\n",
      "Average test loss: 0.003251070979154772\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03202989215321011\n",
      "Average test loss: 0.003185114469172226\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03206767469975683\n",
      "Average test loss: 0.003146892452405559\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03210709729459551\n",
      "Average test loss: 0.003154432310205367\n",
      "Epoch 279/300\n",
      "Average training loss: 0.032032324779364796\n",
      "Average test loss: 0.00319017337821424\n",
      "Epoch 280/300\n",
      "Average training loss: 0.032106135081085896\n",
      "Average test loss: 0.0031646001148555015\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03204888971149921\n",
      "Average test loss: 0.0031748651816613143\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03200347658826245\n",
      "Average test loss: 0.003121748597671588\n",
      "Epoch 283/300\n",
      "Average training loss: 0.032053837429318166\n",
      "Average test loss: 0.003206588155279557\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03206423945228259\n",
      "Average test loss: 0.003171073305937979\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03199141372905837\n",
      "Average test loss: 0.0031630916490943896\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03197025581532054\n",
      "Average test loss: 0.003135193739914232\n",
      "Epoch 287/300\n",
      "Average training loss: 0.031947683443625766\n",
      "Average test loss: 0.0033169886840300426\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0319644308189551\n",
      "Average test loss: 0.0032177932964016992\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03197690088384681\n",
      "Average test loss: 0.0032361888916542132\n",
      "Epoch 290/300\n",
      "Average training loss: 0.031897537420193356\n",
      "Average test loss: 0.0030676823707504403\n",
      "Epoch 291/300\n",
      "Average training loss: 0.031859765622350905\n",
      "Average test loss: 0.0031232056721217103\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0319766169273191\n",
      "Average test loss: 0.0031764741775890193\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03192795328299205\n",
      "Average test loss: 0.0031833632584247324\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03184305184748438\n",
      "Average test loss: 0.0031005428414791823\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03182966963946819\n",
      "Average test loss: 0.0032084144556687936\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03183900436096721\n",
      "Average test loss: 0.0031616454283810323\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03182058536675241\n",
      "Average test loss: 0.003180970649752352\n",
      "Epoch 298/300\n",
      "Average training loss: 0.031851722275217376\n",
      "Average test loss: 0.003153638086799118\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03184452576769723\n",
      "Average test loss: 0.003183303830938207\n",
      "Epoch 300/300\n",
      "Average training loss: 0.031802390057179664\n",
      "Average test loss: 0.003174597557530635\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.8665252700646718\n",
      "Average test loss: 0.004218698337674141\n",
      "Epoch 2/300\n",
      "Average training loss: 0.19892830388413535\n",
      "Average test loss: 0.0037495681802845664\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11447797338167827\n",
      "Average test loss: 0.0034843897469755674\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08519194622172249\n",
      "Average test loss: 0.003316025138522188\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07059777800242106\n",
      "Average test loss: 0.003163486863589949\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0625963884194692\n",
      "Average test loss: 0.003058384723133511\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05766811683442857\n",
      "Average test loss: 0.0029662207956943245\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0544333207209905\n",
      "Average test loss: 0.0028871225108289054\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0519837346540557\n",
      "Average test loss: 0.0028635508873396448\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05007147441969977\n",
      "Average test loss: 0.002789047266046206\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04848834991786215\n",
      "Average test loss: 0.002684441820614868\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04715349911650022\n",
      "Average test loss: 0.0027362820893112156\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04599650903873973\n",
      "Average test loss: 0.0025670406555549966\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04497958771056599\n",
      "Average test loss: 0.002615822207182646\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04399739366769791\n",
      "Average test loss: 0.002553146853836046\n",
      "Epoch 16/300\n",
      "Average training loss: 0.043194951040877234\n",
      "Average test loss: 0.0024182329337216085\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04240898788554801\n",
      "Average test loss: 0.00242420196843644\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04168902233739694\n",
      "Average test loss: 0.0023504881956097155\n",
      "Epoch 19/300\n",
      "Average training loss: 0.040975569128990176\n",
      "Average test loss: 0.0023271158407959674\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04029577882422341\n",
      "Average test loss: 0.0023076861064053244\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03970976718266805\n",
      "Average test loss: 0.0023507014277080697\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03913336322373814\n",
      "Average test loss: 0.0022670951664654744\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03854149913787842\n",
      "Average test loss: 0.0022304662118355434\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03798765415946643\n",
      "Average test loss: 0.0022539956834581164\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03752283996012476\n",
      "Average test loss: 0.002268673668305079\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03701662562290827\n",
      "Average test loss: 0.0022346559678100876\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03663035653034846\n",
      "Average test loss: 0.0022302843627209465\n",
      "Epoch 28/300\n",
      "Average training loss: 0.036146213895744746\n",
      "Average test loss: 0.002197927869028515\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03577622401879894\n",
      "Average test loss: 0.0021616788297477694\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03543062504960431\n",
      "Average test loss: 0.0021590962072627414\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03504671080907186\n",
      "Average test loss: 0.002118933884219991\n",
      "Epoch 32/300\n",
      "Average training loss: 0.034718831441468666\n",
      "Average test loss: 0.0021255265850987698\n",
      "Epoch 33/300\n",
      "Average training loss: 0.034423358718554176\n",
      "Average test loss: 0.0021281952015641662\n",
      "Epoch 34/300\n",
      "Average training loss: 0.034164101877146295\n",
      "Average test loss: 0.002112198459398415\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03385257414314482\n",
      "Average test loss: 0.002108758917078376\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03365116403169102\n",
      "Average test loss: 0.0020942032150924206\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033380007925960756\n",
      "Average test loss: 0.0020818037096824913\n",
      "Epoch 38/300\n",
      "Average training loss: 0.033239661147197085\n",
      "Average test loss: 0.0021196201967282426\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03295074054764377\n",
      "Average test loss: 0.002068903461098671\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0327657957871755\n",
      "Average test loss: 0.002078784931761523\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03253970757126808\n",
      "Average test loss: 0.0020463029878834884\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03239027840395769\n",
      "Average test loss: 0.002048488851636648\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03217887807720237\n",
      "Average test loss: 0.0020730609653724563\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03205852331883378\n",
      "Average test loss: 0.0020341761608918506\n",
      "Epoch 45/300\n",
      "Average training loss: 0.031799410790205\n",
      "Average test loss: 0.002055702855396602\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03171681915554735\n",
      "Average test loss: 0.0020644845406835276\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03153797221514914\n",
      "Average test loss: 0.0020455705256511767\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03141290238996347\n",
      "Average test loss: 0.00202612475781805\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0313000033588873\n",
      "Average test loss: 0.0020941814199710884\n",
      "Epoch 50/300\n",
      "Average training loss: 0.031100435040063327\n",
      "Average test loss: 0.002025076392624113\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03103856633272436\n",
      "Average test loss: 0.0020474027215192717\n",
      "Epoch 52/300\n",
      "Average training loss: 0.030826613020565774\n",
      "Average test loss: 0.002059755896528562\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03074076907005575\n",
      "Average test loss: 0.002031031472918888\n",
      "Epoch 54/300\n",
      "Average training loss: 0.030566186032361456\n",
      "Average test loss: 0.0020385688493649167\n",
      "Epoch 55/300\n",
      "Average training loss: 0.030434712229503526\n",
      "Average test loss: 0.0020744211110803818\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03037643088731501\n",
      "Average test loss: 0.002071250533995529\n",
      "Epoch 57/300\n",
      "Average training loss: 0.030205861982372073\n",
      "Average test loss: 0.0020254106202887163\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03010340113606718\n",
      "Average test loss: 0.002059160179355078\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030021402888827854\n",
      "Average test loss: 0.0020260404060698217\n",
      "Epoch 60/300\n",
      "Average training loss: 0.029851002833909457\n",
      "Average test loss: 0.0020107131274417042\n",
      "Epoch 61/300\n",
      "Average training loss: 0.029740464626087083\n",
      "Average test loss: 0.0020536889967819054\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02958507881893052\n",
      "Average test loss: 0.002067380338907242\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02950305318666829\n",
      "Average test loss: 0.002036905883087052\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029400558026300535\n",
      "Average test loss: 0.0020388682333545554\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029288297227687304\n",
      "Average test loss: 0.002219592828717497\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02914909405840768\n",
      "Average test loss: 0.0020859432317730455\n",
      "Epoch 67/300\n",
      "Average training loss: 0.029152602265278497\n",
      "Average test loss: 0.0020937438582380613\n",
      "Epoch 68/300\n",
      "Average training loss: 0.028974876385596062\n",
      "Average test loss: 0.0020399854100412794\n",
      "Epoch 69/300\n",
      "Average training loss: 0.028885462986098395\n",
      "Average test loss: 0.0020852946332759327\n",
      "Epoch 70/300\n",
      "Average training loss: 0.028732876954807175\n",
      "Average test loss: 0.002038643000440465\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0286536337600814\n",
      "Average test loss: 0.0020719597027119663\n",
      "Epoch 72/300\n",
      "Average training loss: 0.028529739083515273\n",
      "Average test loss: 0.002087254294090801\n",
      "Epoch 73/300\n",
      "Average training loss: 0.028458503918515312\n",
      "Average test loss: 0.002104052222437329\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02838348697953754\n",
      "Average test loss: 0.0020870095839103063\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02830404267542892\n",
      "Average test loss: 0.002082921274213327\n",
      "Epoch 76/300\n",
      "Average training loss: 0.028258441499537892\n",
      "Average test loss: 0.0021058463686042363\n",
      "Epoch 77/300\n",
      "Average training loss: 0.028163552587231\n",
      "Average test loss: 0.0020857770163565876\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02802259750995371\n",
      "Average test loss: 0.0021373768923804164\n",
      "Epoch 79/300\n",
      "Average training loss: 0.027954487302237086\n",
      "Average test loss: 0.002075998408926858\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02791383069091373\n",
      "Average test loss: 0.002079741810448468\n",
      "Epoch 81/300\n",
      "Average training loss: 0.027771421261959604\n",
      "Average test loss: 0.0021393953765639\n",
      "Epoch 82/300\n",
      "Average training loss: 0.027646944193376436\n",
      "Average test loss: 0.002094246741384268\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02766988752782345\n",
      "Average test loss: 0.0021170076167003975\n",
      "Epoch 84/300\n",
      "Average training loss: 0.027522154837846757\n",
      "Average test loss: 0.0020973538692212767\n",
      "Epoch 85/300\n",
      "Average training loss: 0.027523192110988828\n",
      "Average test loss: 0.00206148804558648\n",
      "Epoch 86/300\n",
      "Average training loss: 0.027387218786610497\n",
      "Average test loss: 0.002072072842054897\n",
      "Epoch 87/300\n",
      "Average training loss: 0.027323708134392898\n",
      "Average test loss: 0.002086418185176121\n",
      "Epoch 88/300\n",
      "Average training loss: 0.027259214559362994\n",
      "Average test loss: 0.002173030479397211\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02714628700580862\n",
      "Average test loss: 0.0020956656102918917\n",
      "Epoch 90/300\n",
      "Average training loss: 0.027142075084149838\n",
      "Average test loss: 0.002163719001950489\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02707741591334343\n",
      "Average test loss: 0.00211266207570831\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02698607484002908\n",
      "Average test loss: 0.002079642499383125\n",
      "Epoch 93/300\n",
      "Average test loss: 0.004454882324569755\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02686135163075394\n",
      "Average test loss: 0.002097602568152878\n",
      "Epoch 95/300\n",
      "Average training loss: 0.026776609735356435\n",
      "Average test loss: 0.0020874235294759272\n",
      "Epoch 96/300\n",
      "Average training loss: 0.026804739442136554\n",
      "Average test loss: 0.002089653729564614\n",
      "Epoch 97/300\n",
      "Average training loss: 0.026707316544320847\n",
      "Average test loss: 0.0021321706662161483\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026604527652263642\n",
      "Average test loss: 0.0023234409493290715\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02655253337820371\n",
      "Average test loss: 0.0020781049482110473\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02650944083597925\n",
      "Average test loss: 0.0021064536404899427\n",
      "Epoch 101/300\n",
      "Average training loss: 0.026424227605263393\n",
      "Average test loss: 0.002160651886007852\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02640403335293134\n",
      "Average test loss: 0.0022043317813012337\n",
      "Epoch 103/300\n",
      "Average training loss: 0.026378536586960155\n",
      "Average test loss: 0.0021573113744250603\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02628372989429368\n",
      "Average test loss: 0.002097091250018113\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026207790467474196\n",
      "Average test loss: 0.00211434352873928\n",
      "Epoch 106/300\n",
      "Average training loss: 0.026198127244909605\n",
      "Average test loss: 0.0021894771907892494\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02617466609345542\n",
      "Average test loss: 0.0021672684610303905\n",
      "Epoch 108/300\n",
      "Average training loss: 0.026102305620908737\n",
      "Average test loss: 0.0021693254752705496\n",
      "Epoch 109/300\n",
      "Average training loss: 0.026040699907475047\n",
      "Average test loss: 0.002190195230146249\n",
      "Epoch 110/300\n",
      "Average training loss: 0.025983862302369543\n",
      "Average test loss: 0.002205363978735275\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02594715277519491\n",
      "Average test loss: 0.002112247013797363\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02593263413508733\n",
      "Average test loss: 0.0021337266080081463\n",
      "Epoch 113/300\n",
      "Average training loss: 0.025833529172672166\n",
      "Average test loss: 0.00215736846667197\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02587698324686951\n",
      "Average test loss: 0.002155205490378042\n",
      "Epoch 115/300\n",
      "Average training loss: 0.025779581881231733\n",
      "Average test loss: 0.002268462659791112\n",
      "Epoch 116/300\n",
      "Average training loss: 0.025781841283043224\n",
      "Average test loss: 0.002103651149198413\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02567374645339118\n",
      "Average test loss: 0.0021724522507025135\n",
      "Epoch 118/300\n",
      "Average training loss: 0.025615782169832124\n",
      "Average test loss: 0.002165505091763205\n",
      "Epoch 119/300\n",
      "Average training loss: 0.025590860709547997\n",
      "Average test loss: 0.0021640480855065914\n",
      "Epoch 120/300\n",
      "Average training loss: 0.025592150308191775\n",
      "Average test loss: 0.0021412454893191654\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025601179033517836\n",
      "Average test loss: 0.002205038859612412\n",
      "Epoch 122/300\n",
      "Average training loss: 0.025532086312770843\n",
      "Average test loss: 0.002255612866125173\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02545664976702796\n",
      "Average test loss: 0.002146343403702809\n",
      "Epoch 124/300\n",
      "Average training loss: 0.025450891993112033\n",
      "Average test loss: 0.002160756565630436\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0253985674712393\n",
      "Average test loss: 0.00217177577316761\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02534534811311298\n",
      "Average test loss: 0.0021729076808939376\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02535314463906818\n",
      "Average test loss: 0.0021746717892173265\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025283664360642433\n",
      "Average test loss: 0.002203456040678753\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02522546265191502\n",
      "Average test loss: 0.002140247133250038\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02522603481511275\n",
      "Average test loss: 0.0021723407736668984\n",
      "Epoch 131/300\n",
      "Average training loss: 0.025180676935447587\n",
      "Average test loss: 0.002158480485797756\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02516811887919903\n",
      "Average test loss: 0.0022677364018228317\n",
      "Epoch 133/300\n",
      "Average training loss: 0.025107492331001492\n",
      "Average test loss: 0.002173911054722137\n",
      "Epoch 134/300\n",
      "Average training loss: 0.025055097949173714\n",
      "Average test loss: 0.0022106824893918303\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02503815760380692\n",
      "Average test loss: 0.0022484475755029255\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0250066948764854\n",
      "Average test loss: 0.002204124459168977\n",
      "Epoch 137/300\n",
      "Average training loss: 0.024998506397008895\n",
      "Average test loss: 0.0022238699711031383\n",
      "Epoch 138/300\n",
      "Average training loss: 0.024956976651317544\n",
      "Average test loss: 0.0022563135665324\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024943076201611094\n",
      "Average test loss: 0.002298071917767326\n",
      "Epoch 140/300\n",
      "Average training loss: 0.024873426493671205\n",
      "Average test loss: 0.002229161196284824\n",
      "Epoch 141/300\n",
      "Average training loss: 0.024869962253504328\n",
      "Average test loss: 0.002185571977765196\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02484304370979468\n",
      "Average test loss: 0.0021722560399729343\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024797166970041064\n",
      "Average test loss: 0.002513041192872657\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024840091836121347\n",
      "Average test loss: 0.002346595040212075\n",
      "Epoch 145/300\n",
      "Average training loss: 0.024766247098644573\n",
      "Average test loss: 0.0024578196712666087\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02473986779153347\n",
      "Average test loss: 0.0022153696271901327\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024684657318724525\n",
      "Average test loss: 0.002228188360730807\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02466867631673813\n",
      "Average test loss: 0.0022261409872315\n",
      "Epoch 149/300\n",
      "Average training loss: 0.024696777241097555\n",
      "Average test loss: 0.0021892619849079183\n",
      "Epoch 150/300\n",
      "Average training loss: 0.024621984226836098\n",
      "Average test loss: 0.0021740894405585195\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024619025617837905\n",
      "Average test loss: 0.002183095282771521\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024616472961174116\n",
      "Average test loss: 0.0021845950504971877\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024569180354475976\n",
      "Average test loss: 0.002209622423681948\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024490941137075423\n",
      "Average test loss: 0.0021775410347100762\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0245106170790063\n",
      "Average test loss: 0.0022417561587774093\n",
      "Epoch 156/300\n",
      "Average training loss: 0.024473355089624723\n",
      "Average test loss: 0.002219030626739065\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02445630226035913\n",
      "Average test loss: 0.0022398530466275083\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02439310699701309\n",
      "Average test loss: 0.0021703195633987586\n",
      "Epoch 159/300\n",
      "Average training loss: 0.024387184442745315\n",
      "Average test loss: 0.002285792670523127\n",
      "Epoch 160/300\n",
      "Average training loss: 0.024418519326382213\n",
      "Average test loss: 0.002167776457241012\n",
      "Epoch 161/300\n",
      "Average training loss: 0.024315482396218512\n",
      "Average test loss: 0.0022869483104182614\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024321564414434962\n",
      "Average test loss: 0.0023004743715541232\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02432897386617131\n",
      "Average test loss: 0.0021795493627174032\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02431782442331314\n",
      "Average test loss: 0.002222016593855288\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024293364251653352\n",
      "Average test loss: 0.002288363392257856\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0242272220618195\n",
      "Average test loss: 0.0021530145336356426\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024235105054246055\n",
      "Average test loss: 0.0022189800618216396\n",
      "Epoch 168/300\n",
      "Average training loss: 0.024227793157100677\n",
      "Average test loss: 0.0021775878452592427\n",
      "Epoch 169/300\n",
      "Average training loss: 0.024145170321067173\n",
      "Average test loss: 0.0022096065524965527\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02415082572069433\n",
      "Average test loss: 0.002259361785836518\n",
      "Epoch 171/300\n",
      "Average training loss: 0.024225183705488842\n",
      "Average test loss: 0.002342517057019803\n",
      "Epoch 172/300\n",
      "Average training loss: 0.024140645580159294\n",
      "Average test loss: 0.002253535199910402\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02405679262181123\n",
      "Average test loss: 0.002230939250232445\n",
      "Epoch 174/300\n",
      "Average training loss: 0.024079608806305462\n",
      "Average test loss: 0.0022107651272995606\n",
      "Epoch 175/300\n",
      "Average training loss: 0.024065238010552194\n",
      "Average test loss: 0.0038062871924291054\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024020565499862034\n",
      "Average test loss: 0.002307212315706743\n",
      "Epoch 177/300\n",
      "Average training loss: 0.024082549000779788\n",
      "Average test loss: 0.002175875995722082\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02401414704322815\n",
      "Average test loss: 0.002292618983011279\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02400651306245062\n",
      "Average test loss: 0.002341902531269524\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02400288350217872\n",
      "Average test loss: 0.0024385178368538617\n",
      "Epoch 181/300\n",
      "Average training loss: 0.024007482873068916\n",
      "Average test loss: 0.002875814439625376\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023894313072164854\n",
      "Average test loss: 0.002652326282320751\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02391491158306599\n",
      "Average test loss: 0.00217563920840621\n",
      "Epoch 184/300\n",
      "Average training loss: 0.023918755854169527\n",
      "Average test loss: 0.0022321699389980898\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02390267826947901\n",
      "Average test loss: 0.0022330977035065493\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02385324677824974\n",
      "Average test loss: 0.0023103800351834958\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02386609731283453\n",
      "Average test loss: 0.0022468304849333235\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023817088135414652\n",
      "Average test loss: 0.0022339908906983006\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023826131049129697\n",
      "Average test loss: 0.004553623775434163\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023822924021217558\n",
      "Average test loss: 0.002247228545654151\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02375929690566328\n",
      "Average test loss: 0.002354751727440291\n",
      "Epoch 192/300\n",
      "Average training loss: 0.023775475430819724\n",
      "Average test loss: 0.0022324956375070746\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02374703020354112\n",
      "Average test loss: 0.0038723298735502693\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023715890102916293\n",
      "Average test loss: 0.0022405421442041793\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02370535826517476\n",
      "Average test loss: 0.0022843384053558113\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02374285286830531\n",
      "Average test loss: 0.0022212782880912225\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023682227139671642\n",
      "Average test loss: 0.0022245024769670433\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023599599957466126\n",
      "Average test loss: 0.0022654500674042437\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02363443002353112\n",
      "Average test loss: 0.002327408666205075\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02363872644967503\n",
      "Average test loss: 0.0022106997896399762\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023670238244864677\n",
      "Average test loss: 0.0022872289472983943\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023662759954730668\n",
      "Average test loss: 0.002249600596932901\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023612611211008494\n",
      "Average test loss: 0.0023765600834869676\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0235403293967247\n",
      "Average test loss: 0.0022611438325709768\n",
      "Epoch 205/300\n",
      "Average training loss: 0.023590314169724784\n",
      "Average test loss: 0.00225176408348812\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02360877286725574\n",
      "Average test loss: 0.002255638577975333\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023596172294682926\n",
      "Average test loss: 0.002273422937943704\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023500407396091355\n",
      "Average test loss: 0.0022349271927442817\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023481904743446243\n",
      "Average test loss: 0.0022575613353401423\n",
      "Epoch 210/300\n",
      "Average training loss: 0.023548578255706363\n",
      "Average test loss: 0.002630812086268432\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02346586466497845\n",
      "Average test loss: 0.0022194741384850606\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02350754234691461\n",
      "Average test loss: 0.0022373106599681905\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02345523701608181\n",
      "Average test loss: 0.0023188653484814696\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02342191661397616\n",
      "Average test loss: 0.0022907362128090527\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02340125780304273\n",
      "Average test loss: 0.0022845394194333088\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023395036361283726\n",
      "Average test loss: 0.002277902935114172\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023399085937274827\n",
      "Average test loss: 0.0022761121056973934\n",
      "Epoch 218/300\n",
      "Average training loss: 0.023416573300129837\n",
      "Average test loss: 0.0023136308228390084\n",
      "Epoch 219/300\n",
      "Average training loss: 0.023366729458173117\n",
      "Average test loss: 0.0022580347292953068\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02331776983042558\n",
      "Average test loss: 0.0022551932408370907\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02337329741484589\n",
      "Average test loss: 0.0022421751937104595\n",
      "Epoch 222/300\n",
      "Average training loss: 0.023310988338457215\n",
      "Average test loss: 0.0022448639893490408\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023326861921283935\n",
      "Average test loss: 0.0023012788947671652\n",
      "Epoch 224/300\n",
      "Average training loss: 0.023295982741647295\n",
      "Average test loss: 0.0022420002155833776\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023285798220170868\n",
      "Average test loss: 0.0023000498171895744\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023309376060962676\n",
      "Average test loss: 0.0022223308560334974\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023285862063368162\n",
      "Average test loss: 0.002267859802270929\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02323054588834445\n",
      "Average test loss: 0.002339237829049428\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023239344838592742\n",
      "Average test loss: 0.0022672320368389287\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023188192201985255\n",
      "Average test loss: 0.0022678966354578735\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02326387109193537\n",
      "Average test loss: 0.0022883174351106088\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02321051060822275\n",
      "Average test loss: 0.00237743705097172\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02319409180846479\n",
      "Average test loss: 0.002300588483301302\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023174966105156476\n",
      "Average test loss: 0.002219619163415498\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02314780718419287\n",
      "Average test loss: 0.0022711642127898005\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023170723744564588\n",
      "Average test loss: 0.002295533151055376\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023182134967711238\n",
      "Average test loss: 0.002418505454642905\n",
      "Epoch 238/300\n",
      "Average training loss: 0.023133544825845294\n",
      "Average test loss: 0.0022687597171299986\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02313764340678851\n",
      "Average test loss: 0.002273195342781643\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02313849454621474\n",
      "Average test loss: 0.002226127941161394\n",
      "Epoch 241/300\n",
      "Average training loss: 0.023102640009588664\n",
      "Average test loss: 0.0022917754530078836\n",
      "Epoch 242/300\n",
      "Average training loss: 0.023090489708715016\n",
      "Average test loss: 0.0023098609149456023\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02311721259024408\n",
      "Average test loss: 0.0022688502096571026\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023076789042188062\n",
      "Average test loss: 0.0022800129192570846\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02305213338798947\n",
      "Average test loss: 0.00227058347252508\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02302708672814899\n",
      "Average test loss: 0.0030386263045171897\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02302944855723116\n",
      "Average test loss: 0.00228643034491688\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023056905223263636\n",
      "Average test loss: 0.002295910343941715\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02307045347823037\n",
      "Average test loss: 0.002326265127501554\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02298337353931533\n",
      "Average test loss: 0.002258731842454937\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02298428973721133\n",
      "Average test loss: 0.002230350413049261\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023008389360374873\n",
      "Average test loss: 0.0023197585770653355\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022973151819573507\n",
      "Average test loss: 0.0022842995644443563\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02296406885319286\n",
      "Average test loss: 0.002400917180917329\n",
      "Epoch 255/300\n",
      "Average training loss: 0.022960936889052392\n",
      "Average test loss: 0.002305315655345718\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02293028251330058\n",
      "Average test loss: 0.0022820652356992164\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022908061221241952\n",
      "Average test loss: 0.0023440576851781873\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02294588571621312\n",
      "Average test loss: 0.002252659775006274\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022862777842415704\n",
      "Average test loss: 0.0022600476119874254\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022928040736251407\n",
      "Average test loss: 0.0023678155115081203\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02292183406982157\n",
      "Average test loss: 0.002239855650191506\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02284832657045788\n",
      "Average test loss: 0.002388924137689173\n",
      "Epoch 263/300\n",
      "Average training loss: 0.022858507421281603\n",
      "Average test loss: 0.0022508585080504417\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022902774934967358\n",
      "Average test loss: 0.0023700960577569073\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02288435310787625\n",
      "Average test loss: 0.002225931234140363\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022871217418048116\n",
      "Average test loss: 0.0023023507121122545\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02280489060944981\n",
      "Average test loss: 0.0022795238364487886\n",
      "Epoch 268/300\n",
      "Average training loss: 0.022842605786191093\n",
      "Average test loss: 0.0023154249685919948\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02285665531953176\n",
      "Average test loss: 0.0023243606926666364\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022802623230550025\n",
      "Average test loss: 0.0024174486175179482\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02279091240134504\n",
      "Average test loss: 0.002228656203589506\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022787486541602345\n",
      "Average test loss: 0.0023111786427390244\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02277107897069719\n",
      "Average test loss: 0.0026112929474976327\n",
      "Epoch 274/300\n",
      "Average training loss: 0.022829817886153857\n",
      "Average test loss: 0.002240554210729897\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02276689633561505\n",
      "Average test loss: 0.002328706509537167\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022787653769056\n",
      "Average test loss: 0.0023170712472250065\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022754091676738527\n",
      "Average test loss: 0.002327133903383381\n",
      "Epoch 278/300\n",
      "Average training loss: 0.022718724987573095\n",
      "Average test loss: 0.0022540934493558273\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022761941249171892\n",
      "Average test loss: 0.002261799671376745\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022786319977707335\n",
      "Average test loss: 0.002331448276837667\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022685273963544104\n",
      "Average test loss: 0.002260594159985582\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022707100310259395\n",
      "Average test loss: 0.002303457902123531\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02270545718073845\n",
      "Average test loss: 0.0022866647287996277\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022695727997355992\n",
      "Average test loss: 0.002241927307099104\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022681935381558205\n",
      "Average test loss: 0.0023393066819343304\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022641951978206634\n",
      "Average test loss: 0.0022354806665745046\n",
      "Epoch 287/300\n",
      "Average training loss: 0.022649917831023533\n",
      "Average test loss: 0.002282416256972485\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02264284101790852\n",
      "Average test loss: 0.0023418710995465517\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02269090219007598\n",
      "Average test loss: 0.002333889870904386\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022633943299452463\n",
      "Average test loss: 0.002349133756115205\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022604577627446918\n",
      "Average test loss: 0.0022645495561882853\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022623572215437888\n",
      "Average test loss: 0.002274768302631047\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02259887208872371\n",
      "Average test loss: 0.0022830121249167454\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022605721096197764\n",
      "Average test loss: 0.002285895431310766\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02262527802420987\n",
      "Average test loss: 0.0022773007994724643\n",
      "Epoch 296/300\n",
      "Average training loss: 0.022606248130400977\n",
      "Average test loss: 0.0023566714651468727\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02257836522658666\n",
      "Average test loss: 0.002370140290301707\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022624106728368336\n",
      "Average test loss: 0.0023223408795893193\n",
      "Epoch 299/300\n",
      "Average training loss: 0.022563896717296705\n",
      "Average test loss: 0.002306579346768558\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022580489221546384\n",
      "Average test loss: 0.0022788936911771695\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7551052270995247\n",
      "Average test loss: 0.003732405547466543\n",
      "Epoch 2/300\n",
      "Average training loss: 0.16164530102411906\n",
      "Average test loss: 0.00544701574742794\n",
      "Epoch 3/300\n",
      "Average training loss: 0.10323784051338832\n",
      "Average test loss: 0.002841706850255529\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07891739882363213\n",
      "Average test loss: 0.0026997979877309668\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06498071393039491\n",
      "Average test loss: 0.002517710881721642\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05595665768782298\n",
      "Average test loss: 0.002449099149141047\n",
      "Epoch 7/300\n",
      "Average training loss: 0.049591187139352164\n",
      "Average test loss: 0.002410177569836378\n",
      "Epoch 8/300\n",
      "Average training loss: 0.045305131087700526\n",
      "Average test loss: 0.002245717531070113\n",
      "Epoch 9/300\n",
      "Average training loss: 0.042367926253212826\n",
      "Average test loss: 0.002143471012926764\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04016776109072897\n",
      "Average test loss: 0.0022917679684857526\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03835608043604427\n",
      "Average test loss: 0.001990194998888506\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03688655932909912\n",
      "Average test loss: 0.0019453672792555557\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03569701812995805\n",
      "Average test loss: 0.0018826201572600338\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03461679823199908\n",
      "Average test loss: 0.0018657208984303806\n",
      "Epoch 15/300\n",
      "Average training loss: 0.033621942995323076\n",
      "Average test loss: 0.0017976875955031978\n",
      "Epoch 16/300\n",
      "Average training loss: 0.032819960570169816\n",
      "Average test loss: 0.00177220737375319\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03201152262091637\n",
      "Average test loss: 0.0017151991337951687\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03131778447992272\n",
      "Average test loss: 0.0018050059663752714\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03064206089741654\n",
      "Average test loss: 0.0016856677215546369\n",
      "Epoch 20/300\n",
      "Average training loss: 0.029995389719804127\n",
      "Average test loss: 0.0016291339403639237\n",
      "Epoch 21/300\n",
      "Average training loss: 0.029382785975933075\n",
      "Average test loss: 0.0017323941743622223\n",
      "Epoch 22/300\n",
      "Average training loss: 0.028820967132846514\n",
      "Average test loss: 0.001590932531696227\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02828277613222599\n",
      "Average test loss: 0.0015973609593800374\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02774437668753995\n",
      "Average test loss: 0.0015611473106675678\n",
      "Epoch 25/300\n",
      "Average training loss: 0.027281236610478824\n",
      "Average test loss: 0.0015388896861631008\n",
      "Epoch 26/300\n",
      "Average training loss: 0.026773607961005634\n",
      "Average test loss: 0.001539511260886987\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02643075278484159\n",
      "Average test loss: 0.001530938590462837\n",
      "Epoch 28/300\n",
      "Average training loss: 0.026018340453505515\n",
      "Average test loss: 0.0015198137240691318\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02568602518406179\n",
      "Average test loss: 0.0015048867245722147\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02533840860757563\n",
      "Average test loss: 0.0014781940556648704\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02511716349919637\n",
      "Average test loss: 0.0014799426906845635\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024806679477294286\n",
      "Average test loss: 0.0014638230921700597\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02458333935836951\n",
      "Average test loss: 0.001458314303929607\n",
      "Epoch 34/300\n",
      "Average training loss: 0.024394907259278828\n",
      "Average test loss: 0.0014698975815748176\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024265052951872347\n",
      "Average test loss: 0.0014615947704037858\n",
      "Epoch 36/300\n",
      "Average training loss: 0.023973597852720154\n",
      "Average test loss: 0.001444403546034462\n",
      "Epoch 37/300\n",
      "Average training loss: 0.023789019852876663\n",
      "Average test loss: 0.0014450310617685317\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02360762823952569\n",
      "Average test loss: 0.0014424933074042201\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02343308540681998\n",
      "Average test loss: 0.0014377130226542552\n",
      "Epoch 40/300\n",
      "Average training loss: 0.023280198004510668\n",
      "Average test loss: 0.001418960839509964\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02316216886209117\n",
      "Average test loss: 0.0014318013991125756\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02305537943376435\n",
      "Average test loss: 0.0014299102584934896\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02290681327548292\n",
      "Average test loss: 0.0014132141401577327\n",
      "Epoch 44/300\n",
      "Average training loss: 0.022732501751846736\n",
      "Average test loss: 0.0014216652252814837\n",
      "Epoch 45/300\n",
      "Average training loss: 0.022651663031842975\n",
      "Average test loss: 0.001411437164992094\n",
      "Epoch 46/300\n",
      "Average training loss: 0.022487262752321033\n",
      "Average test loss: 0.001418038065959182\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0223554175860352\n",
      "Average test loss: 0.0014168266448088818\n",
      "Epoch 48/300\n",
      "Average training loss: 0.022273226075702243\n",
      "Average test loss: 0.0014244117391709653\n",
      "Epoch 49/300\n",
      "Average training loss: 0.022169738822513156\n",
      "Average test loss: 0.0014842527368002468\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02202709512743685\n",
      "Average test loss: 0.0014488015731589662\n",
      "Epoch 51/300\n",
      "Average training loss: 0.021932874904738532\n",
      "Average test loss: 0.001410629557652606\n",
      "Epoch 52/300\n",
      "Average training loss: 0.021926703285839823\n",
      "Average test loss: 0.0014225170396061407\n",
      "Epoch 53/300\n",
      "Average training loss: 0.021738735338052113\n",
      "Average test loss: 0.0014070389206624694\n",
      "Epoch 54/300\n",
      "Average training loss: 0.021637711589535077\n",
      "Average test loss: 0.0014054800602090027\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021519235839446385\n",
      "Average test loss: 0.001408028210616774\n",
      "Epoch 56/300\n",
      "Average training loss: 0.021473890057868428\n",
      "Average test loss: 0.0014363805080453555\n",
      "Epoch 57/300\n",
      "Average training loss: 0.021365734581318168\n",
      "Average test loss: 0.001435321394043664\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02133694030178918\n",
      "Average test loss: 0.001414018740153147\n",
      "Epoch 59/300\n",
      "Average training loss: 0.021173630084428522\n",
      "Average test loss: 0.0014291363464047511\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021110391517480214\n",
      "Average test loss: 0.0014022569952325688\n",
      "Epoch 61/300\n",
      "Average training loss: 0.020990150718225374\n",
      "Average training loss: 0.020896053433418275\n",
      "Average test loss: 0.001413294743022157\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02082404494451152\n",
      "Average test loss: 0.001404836600749857\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02074950115217103\n",
      "Average test loss: 0.0014207312107707064\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020663322707017262\n",
      "Average test loss: 0.0014145607472293906\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02063984257479509\n",
      "Average test loss: 0.0014044588508291378\n",
      "Epoch 67/300\n",
      "Average training loss: 0.020527417550484338\n",
      "Average test loss: 0.001421951190671987\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02041671592162715\n",
      "Average test loss: 0.0014088625783721606\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020341597989201547\n",
      "Average test loss: 0.0014293677690956328\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020283378516634306\n",
      "Average test loss: 0.0014549901276412938\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020241429617007572\n",
      "Average test loss: 0.0014072566226952605\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02012106421424283\n",
      "Average test loss: 0.0014420413775369525\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020079592699805894\n",
      "Average test loss: 0.0014170415478034152\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020003839971290693\n",
      "Average test loss: 0.0014202825235616829\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019911209853986898\n",
      "Average test loss: 0.0014662103920968042\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019876548826694488\n",
      "Average test loss: 0.0014270708219458659\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01982769375708368\n",
      "Average test loss: 0.00145427240529615\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01971161270638307\n",
      "Average test loss: 0.0014197093471884728\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019740923383169705\n",
      "Average test loss: 0.0014409420546402949\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019613316295875444\n",
      "Average test loss: 0.0014269574804024565\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01956341360261043\n",
      "Average test loss: 0.0014238830901061495\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01949061055150297\n",
      "Average test loss: 0.0014471226093462771\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019434978360103237\n",
      "Average test loss: 0.001439090391103592\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019371354091498587\n",
      "Average test loss: 0.0014557139803138045\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01931511612650421\n",
      "Average test loss: 0.0014688544974972805\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01926939150525464\n",
      "Average test loss: 0.0014557209787890316\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01920884464432796\n",
      "Average test loss: 0.0014618109461540978\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019150797420905696\n",
      "Average test loss: 0.0014703021755235063\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019111979144314926\n",
      "Average test loss: 0.0014641504232875175\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019057547003030778\n",
      "Average test loss: 0.001456491724277536\n",
      "Epoch 91/300\n",
      "Average training loss: 0.019021099069052274\n",
      "Average test loss: 0.0014752791459775633\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018943458376659287\n",
      "Average test loss: 0.0014482396801726687\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01890540137887001\n",
      "Average test loss: 0.0014704345654075344\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018881238448123137\n",
      "Average test loss: 0.0015124138422931235\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0187940043186148\n",
      "Average test loss: 0.0014678995014789204\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018783724466545713\n",
      "Average test loss: 0.001502217099122289\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018704189911484717\n",
      "Average test loss: 0.0014649980749107069\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018655352478226025\n",
      "Average test loss: 0.0014498574301186535\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018673822714222802\n",
      "Average test loss: 0.0014551781855730546\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018604712134434116\n",
      "Average test loss: 0.0014659312992460199\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018531723545657262\n",
      "Average test loss: 0.0014388854700244135\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018513568277160327\n",
      "Average test loss: 0.0014825019780546427\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01848881451951133\n",
      "Average test loss: 0.0014590514390729367\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018433508762882817\n",
      "Average test loss: 0.0014831808556078209\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01839424061857992\n",
      "Average test loss: 0.0014931958148048984\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018370274583498637\n",
      "Average test loss: 0.0015171612504248817\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018317644469439984\n",
      "Average test loss: 0.001483880117846032\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01828221249828736\n",
      "Average test loss: 0.0015019235756869118\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018266472295754486\n",
      "Average test loss: 0.001531076489119894\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01821570334252384\n",
      "Average test loss: 0.0015285595877406498\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01817936119520002\n",
      "Average test loss: 0.0014838549681007861\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01812336490013533\n",
      "Average test loss: 0.001544637485096852\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018130311167074576\n",
      "Average test loss: 0.00152052690481974\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01812139454483986\n",
      "Average test loss: 0.001487361622094694\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018092291144861116\n",
      "Average test loss: 0.0015159965361882415\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01802390486498674\n",
      "Average test loss: 0.0014902065181069905\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018020365671979056\n",
      "Average test loss: 0.0014872172083705663\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017965337813728385\n",
      "Average test loss: 0.0015238351009579168\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017982795284854042\n",
      "Average test loss: 0.0014881562520232465\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017852421881424056\n",
      "Average test loss: 0.0015029258183721038\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01785190146995915\n",
      "Average test loss: 0.0014721278025665216\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01790645034445657\n",
      "Average test loss: 0.001539244415031539\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01784963097340531\n",
      "Average test loss: 0.00149985642110308\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017801686382128132\n",
      "Average test loss: 0.0015291800355124805\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017764994316630894\n",
      "Average test loss: 0.0015626107216295268\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017774188133577506\n",
      "Average test loss: 0.0015054318014946248\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01773733292106125\n",
      "Average test loss: 0.001560183394079407\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01768930779563056\n",
      "Average test loss: 0.0014860322564426397\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017668521056572597\n",
      "Average test loss: 0.001500338705877463\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017622883796691895\n",
      "Average test loss: 0.0015059743835073379\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01758978880279594\n",
      "Average test loss: 0.001585037700107528\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017580753901766406\n",
      "Average test loss: 0.0015364783907102213\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017583825869692697\n",
      "Average test loss: 0.0015009127396365834\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01754656877120336\n",
      "Average test loss: 0.001567217447484533\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017548032192720307\n",
      "Average test loss: 0.001533130164982544\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017472787299917802\n",
      "Average test loss: 0.0014962000871698061\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017513270624809795\n",
      "Average test loss: 0.0015099615768736435\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017459586340520118\n",
      "Average test loss: 0.0015439318111166358\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01742271204623911\n",
      "Average test loss: 0.0015197410814257132\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017462778880364366\n",
      "Average test loss: 0.0015058893834551175\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017461704432964326\n",
      "Average test loss: 0.0015132207041606306\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017369355531202422\n",
      "Average test loss: 0.0014987513412100574\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017316346456607182\n",
      "Average test loss: 0.001563140694051981\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017288059449030294\n",
      "Average test loss: 0.0014945856529391475\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017299913541310365\n",
      "Average test loss: 0.0015354014608181185\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01728698426981767\n",
      "Average test loss: 0.0014983103109730614\n",
      "Epoch 147/300\n",
      "Average training loss: 0.017243868955307537\n",
      "Average test loss: 0.0014966512067864338\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017232125838597616\n",
      "Average test loss: 0.0015263694426458742\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017258326343364185\n",
      "Average test loss: 0.0016012469584950142\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017222887450622188\n",
      "Average test loss: 0.0015926507423217925\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017169349749883017\n",
      "Average test loss: 0.0015368430083617568\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01715589330262608\n",
      "Average test loss: 0.0015236594999829928\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017180941065152486\n",
      "Average test loss: 0.001542193667859667\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017122645440200965\n",
      "Average test loss: 0.001512113092570669\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01710093194246292\n",
      "Average test loss: 0.0015460226022534901\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017101625929276148\n",
      "Average test loss: 0.0015226060687046912\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017087516960998375\n",
      "Average test loss: 0.0015751836225907836\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017109106063842774\n",
      "Average test loss: 0.0015554821849283246\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01703147255629301\n",
      "Average test loss: 0.0015439502139472298\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01704633807308144\n",
      "Average test loss: 0.0015429585700233778\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016974748858147197\n",
      "Average test loss: 0.0015516800441675717\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01700743181920714\n",
      "Average test loss: 0.0015370711546598209\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016949137890504465\n",
      "Average test loss: 0.001535267043237885\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01694860427412722\n",
      "Average test loss: 0.001525670236080057\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016932508066296578\n",
      "Average test loss: 0.0015317011367943553\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016950111120939255\n",
      "Average test loss: 0.0015716991065483955\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01691355857087506\n",
      "Average test loss: 0.001557400240459376\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016863104428682062\n",
      "Average test loss: 0.0015704826960961024\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016900071656538382\n",
      "Average test loss: 0.001566655467574795\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01685083951221572\n",
      "Average test loss: 0.0015548256587030159\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016865961926678816\n",
      "Average test loss: 0.00168513447439505\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016859050484167203\n",
      "Average test loss: 0.0015842960787316163\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016887306098308828\n",
      "Average training loss: 0.01684179949346516\n",
      "Average test loss: 0.001587234554398391\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016822225300802126\n",
      "Average test loss: 0.0015330092992840542\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01674279493921333\n",
      "Average test loss: 0.0015617982858998909\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0167635087851021\n",
      "Average test loss: 0.0015277657327759596\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016786513143115574\n",
      "Average test loss: 0.001614270730668472\n",
      "Epoch 179/300\n",
      "Average training loss: 0.016716515292723975\n",
      "Average test loss: 0.0015427759096233382\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01668418553471565\n",
      "Average test loss: 0.0015642912211931415\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016777850159340434\n",
      "Average test loss: 0.0015765176316102345\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016729445247186555\n",
      "Average test loss: 0.0015409986457477014\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016681174669000837\n",
      "Average test loss: 0.001593229251810246\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016706217899090714\n",
      "Average test loss: 0.0015467216822629174\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0166212335013681\n",
      "Average test loss: 0.001566064573927886\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016661555420193406\n",
      "Average test loss: 0.001595118780930837\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016656236887805993\n",
      "Average test loss: 0.0015959913196663062\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01663637586765819\n",
      "Average test loss: 0.0015353521919912762\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016647902109556727\n",
      "Average test loss: 0.00157271572533581\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016638091357217895\n",
      "Average test loss: 0.0015678709923393197\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016573490897814434\n",
      "Average test loss: 0.001551194421843522\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016582693984938994\n",
      "Average test loss: 0.00158597623070495\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01656000295612547\n",
      "Average test loss: 0.001553669386336373\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016544615449176896\n",
      "Average test loss: 0.0015954768953637944\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01652435699188047\n",
      "Average test loss: 0.001638315350955559\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016545206473105485\n",
      "Average test loss: 0.0016260471590277222\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01653657889448934\n",
      "Average test loss: 0.0016007016015549501\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01651453342123164\n",
      "Average test loss: 0.0015809141212246485\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016477199871506955\n",
      "Average test loss: 0.0015475875309978921\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01646736782292525\n",
      "Average test loss: 0.0015549442996788355\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016464385599725778\n",
      "Average test loss: 0.0015519622090376087\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016484535651902357\n",
      "Average test loss: 0.0015582988641949164\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01645461965352297\n",
      "Average test loss: 0.0015968007472240264\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016453942182163398\n",
      "Average test loss: 0.0015929476930242445\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01643116052945455\n",
      "Average test loss: 0.0015907248166493243\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016419053310321438\n",
      "Average test loss: 0.0015913952026102279\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016439972677164606\n",
      "Average test loss: 0.0015775518627423378\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01639467206845681\n",
      "Average test loss: 0.0016523947653153705\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0164023285864128\n",
      "Average test loss: 0.0016090357150468561\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016412036486797864\n",
      "Average test loss: 0.0015827682895792855\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016355144717627102\n",
      "Average test loss: 0.0016126368881927595\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016350360170834594\n",
      "Average test loss: 0.0015637559603071875\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01632189302229219\n",
      "Average test loss: 0.001589626687578857\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016363941561844612\n",
      "Average test loss: 0.0015589285047931804\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016338213997582594\n",
      "Average test loss: 0.0015993043584749103\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016311913611160384\n",
      "Average test loss: 0.001576111788292312\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016303750324580403\n",
      "Average test loss: 0.0016812250856310129\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016329591013491154\n",
      "Average test loss: 0.0016109863970842626\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016303063650925955\n",
      "Average test loss: 0.0016652122504181332\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016278854919804468\n",
      "Average test loss: 0.0015459582847025658\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01630903372582462\n",
      "Average test loss: 0.0015905577370689975\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016264741657508745\n",
      "Average test loss: 0.0015481096072536376\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01625104780495167\n",
      "Average test loss: 0.0016154388886772924\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016241316514710585\n",
      "Average test loss: 0.0016326472585399946\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016238276542888747\n",
      "Average test loss: 0.0015823750243418747\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016235415847765076\n",
      "Average test loss: 0.0015862594480729765\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016207878780033852\n",
      "Average test loss: 0.0015916874227114022\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01620380660891533\n",
      "Average test loss: 0.0016459321095090773\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01620851472268502\n",
      "Average test loss: 0.0015999125296043025\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016181222916477257\n",
      "Average test loss: 0.001632833818387654\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01619220783809821\n",
      "Average test loss: 0.0015396046100391281\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016173880147437255\n",
      "Average test loss: 0.0015930827477325996\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01618506795250707\n",
      "Average test loss: 0.0016129534085177713\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016164650638898215\n",
      "Average test loss: 0.0015816656817785567\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016132948029372426\n",
      "Average test loss: 0.0016059136632312503\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016162799364162816\n",
      "Average test loss: 0.0016469613311605321\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016125483772820897\n",
      "Average test loss: 0.0015934433665954404\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01609497527281443\n",
      "Average test loss: 0.0015712841533952289\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016109266055954828\n",
      "Average test loss: 0.0015609256523764796\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016131489081515207\n",
      "Average test loss: 0.0016315675758653218\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016088632736768988\n",
      "Average test loss: 0.001593618353932268\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016087494970609743\n",
      "Average test loss: 0.00165094294016146\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016124163314700127\n",
      "Average test loss: 0.0016314259272896582\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01607272443920374\n",
      "Average test loss: 0.0015761651060440475\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016075142756932313\n",
      "Average test loss: 0.0016007035047643716\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016070646542641853\n",
      "Average test loss: 0.001564701224470304\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016081600815885597\n",
      "Average test loss: 0.0016738212420087722\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016060287220610512\n",
      "Average test loss: 0.0016064559607249166\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01603364992307292\n",
      "Average test loss: 0.0015693272483638591\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01603813207977348\n",
      "Average test loss: 0.0015650122905563977\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0160173687612017\n",
      "Average test loss: 0.001630417434912589\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0160253463420603\n",
      "Average test loss: 0.0015821324815042317\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016032222980426416\n",
      "Average test loss: 0.001544783544416229\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01604531168440978\n",
      "Average test loss: 0.0016145911781738202\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015972310355967946\n",
      "Average test loss: 0.0015953372196397847\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015981986449824438\n",
      "Average test loss: 0.0016608642352124055\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015997647089262802\n",
      "Average test loss: 0.0016122631188482046\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015963691509432262\n",
      "Average test loss: 0.0015956223346810373\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015970421776175497\n",
      "Average test loss: 0.0016142499315448934\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015989433678487937\n",
      "Average test loss: 0.0015942068071001106\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015965194578799937\n",
      "Average test loss: 0.0016617266403304205\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015962473455402585\n",
      "Average test loss: 0.0016333736789723237\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015925440236098237\n",
      "Average test loss: 0.0015908898441120982\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01595146403213342\n",
      "Average test loss: 0.0015950676544258992\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015946157497664294\n",
      "Average test loss: 0.0016352786771021784\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015909546114504337\n",
      "Average test loss: 0.0016197595336577958\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01590433005326324\n",
      "Average test loss: 0.0015912789099125399\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0159026630802287\n",
      "Average test loss: 0.0016298193010605045\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015930208880868223\n",
      "Average test loss: 0.0016324456077482965\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01587305068141884\n",
      "Average test loss: 0.0015893286758412917\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015903172896967993\n",
      "Average test loss: 0.0016326403174963262\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015874574217531415\n",
      "Average test loss: 0.0016064452194712228\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015892133979333773\n",
      "Average test loss: 0.0016448635117461284\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015884981809390915\n",
      "Average test loss: 0.0015866343361429043\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015858338492612045\n",
      "Average test loss: 0.0016107396692451503\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015866978008713988\n",
      "Average test loss: 0.001623146230354905\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015882027256819937\n",
      "Average test loss: 0.001639109242707491\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015832760953240925\n",
      "Average test loss: 0.0016001977135116855\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01583189655840397\n",
      "Average test loss: 0.0016246495151685343\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01584055763317479\n",
      "Average test loss: 0.0016073520594379969\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015818408822019896\n",
      "Average test loss: 0.0016070461714019378\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015858870738910305\n",
      "Average test loss: 0.001623314240378224\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015828285041782592\n",
      "Average test loss: 0.0016096010162081156\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015775420937273237\n",
      "Average test loss: 0.0016428791449094812\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015788366094231607\n",
      "Average test loss: 0.0016149946708853046\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01581032667060693\n",
      "Average test loss: 0.0016035824513269796\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015789114188816813\n",
      "Average test loss: 0.0015723385520072448\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015792198077672057\n",
      "Average test loss: 0.0015753248853402006\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015787928111023374\n",
      "Average test loss: 0.0016323536856927806\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01574342425995403\n",
      "Average test loss: 0.0016865046287162437\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015768579630388153\n",
      "Average test loss: 0.0015860966408832206\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015805845581822926\n",
      "Average test loss: 0.00165318664867017\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0157740050719844\n",
      "Average test loss: 0.0016116835513255662\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015760459856854543\n",
      "Average test loss: 0.0016075137495580646\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015740299050178794\n",
      "Average test loss: 0.001605309360764093\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015719593000908694\n",
      "Average test loss: 0.0016248374401281278\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015737313986652426\n",
      "Average test loss: 0.0016411215811967849\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015777421967022948\n",
      "Average test loss: 0.0016100851810640759\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015727223066820038\n",
      "Average test loss: 0.0016203760142541595\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015732533062497774\n",
      "Average test loss: 0.0016444027745682332\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.09\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.23\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.98\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.23\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.32\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.63\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.70\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.89\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.12\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.15\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.68\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.53\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.65\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.89\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.38\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.67\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.74\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.82\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.90\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 9.78584620030721\n",
      "Average test loss: 0.005703495203413897\n",
      "Epoch 2/300\n",
      "Average training loss: 4.214982939826117\n",
      "Average test loss: 0.004902908619079325\n",
      "Epoch 3/300\n",
      "Average training loss: 2.795892479366726\n",
      "Average test loss: 0.004712874957670768\n",
      "Epoch 4/300\n",
      "Average training loss: 2.169863829930623\n",
      "Average test loss: 0.004602369986888435\n",
      "Epoch 5/300\n",
      "Average training loss: 1.9118886869218614\n",
      "Average test loss: 0.00450157437639104\n",
      "Epoch 6/300\n",
      "Average training loss: 1.629081131723192\n",
      "Average test loss: 0.004507127319359117\n",
      "Epoch 7/300\n",
      "Average training loss: 1.4141555118560791\n",
      "Average test loss: 0.004430469703343179\n",
      "Epoch 8/300\n",
      "Average training loss: 1.1622090047200522\n",
      "Average test loss: 0.039572212491805356\n",
      "Epoch 9/300\n",
      "Average training loss: 1.0571870122485691\n",
      "Average test loss: 0.0043621288121988375\n",
      "Epoch 10/300\n",
      "Average training loss: 0.8745498805046081\n",
      "Average test loss: 0.0044384103763020705\n",
      "Epoch 11/300\n",
      "Average training loss: 0.7436884509722391\n",
      "Average test loss: 0.005347873781083359\n",
      "Epoch 12/300\n",
      "Average training loss: 0.6270857215457493\n",
      "Average test loss: 0.004247797680190868\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5353712266286215\n",
      "Average test loss: 0.00835130812227726\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4502663639121585\n",
      "Average test loss: 0.0042429104844729105\n",
      "Epoch 15/300\n",
      "Average training loss: 0.38551926936043635\n",
      "Average test loss: 0.0060241440195176335\n",
      "Epoch 16/300\n",
      "Average training loss: 0.333792779551612\n",
      "Average test loss: 0.004150647026797136\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2916829099920061\n",
      "Average test loss: 0.004149303926361932\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2574312016963959\n",
      "Average test loss: 0.00413489787446128\n",
      "Epoch 19/300\n",
      "Average training loss: 0.22938638713624743\n",
      "Average test loss: 0.004116402181486289\n",
      "Epoch 20/300\n",
      "Average training loss: 0.20638221055931516\n",
      "Average test loss: 0.004096288675442338\n",
      "Epoch 21/300\n",
      "Average training loss: 0.18812667723496754\n",
      "Average test loss: 0.004070644167976247\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1741412280930413\n",
      "Average test loss: 0.0040651877582487134\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16345060500833725\n",
      "Average test loss: 0.004070784880262282\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15505027990871006\n",
      "Average test loss: 0.00405397525553902\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14787847130828433\n",
      "Average test loss: 0.004041896676023801\n",
      "Epoch 26/300\n",
      "Average training loss: 0.14223454787996082\n",
      "Average test loss: 0.004037758179422882\n",
      "Epoch 27/300\n",
      "Average training loss: 0.13783378785186343\n",
      "Average test loss: 0.004033415881089038\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13455302798085741\n",
      "Average test loss: 0.004033305301020543\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1320167602168189\n",
      "Average test loss: 0.004021871592435572\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13002334299352433\n",
      "Average test loss: 0.00400201318350931\n",
      "Epoch 31/300\n",
      "Average training loss: 0.128328267329269\n",
      "Average test loss: 0.0039925144267165\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1270541273355484\n",
      "Average test loss: 0.003986197700517045\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12595263455311456\n",
      "Average test loss: 0.003974597559620937\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12497727992799547\n",
      "Average test loss: 0.003986745317363077\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12410395557350583\n",
      "Average test loss: 0.003949801327039798\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12343498555156919\n",
      "Average test loss: 0.0039574520393378205\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12277014285326004\n",
      "Average test loss: 0.003945159856436981\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12223786150084602\n",
      "Average test loss: 0.003933161465865043\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1217571596370803\n",
      "Average test loss: 0.003933126855227682\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12133431085613039\n",
      "Average test loss: 0.0039453731390337145\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12087437948915694\n",
      "Average test loss: 0.003940631713097294\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12054040889607535\n",
      "Average test loss: 0.003920382040656275\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12001304219166438\n",
      "Average test loss: 0.003925445095946391\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11976726440588634\n",
      "Average test loss: 0.003997411022997565\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11944341325097614\n",
      "Average test loss: 0.003928699319147401\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11909745695193609\n",
      "Average test loss: 0.003919105196164714\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11885396686527464\n",
      "Average test loss: 0.003919574583984084\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11852919862005445\n",
      "Average test loss: 0.0039134276741080816\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11816736964384715\n",
      "Average test loss: 0.004034261911900507\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11805902122126685\n",
      "Average test loss: 0.003906482576909992\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11768099021249347\n",
      "Average test loss: 0.003912023166401519\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11743083908822802\n",
      "Average test loss: 0.003913689616239733\n",
      "Epoch 53/300\n",
      "Average training loss: 0.117079693933328\n",
      "Average test loss: 0.0038978112902906207\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11685554738177194\n",
      "Average test loss: 0.003881529868890842\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11672762122419145\n",
      "Average test loss: 0.0038955853660073547\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11638155664338005\n",
      "Average training loss: 0.11610402056905958\n",
      "Average test loss: 0.0038738730700893535\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11603411339388953\n",
      "Average test loss: 0.003894442981109023\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11574176175726784\n",
      "Average test loss: 0.003887987503574954\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11550491106510162\n",
      "Average test loss: 0.0038893401318540175\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11521434279282888\n",
      "Average test loss: 0.003884227447004782\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11500749499930275\n",
      "Average test loss: 0.003880525052547455\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11480341249704361\n",
      "Average test loss: 0.0038889562733885313\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11462375674645106\n",
      "Average test loss: 0.003881701290814413\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11447022086381912\n",
      "Average test loss: 0.0038974987113227447\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1143936658501625\n",
      "Average test loss: 0.003872829838345448\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1140955956114663\n",
      "Average test loss: 0.003878292134549055\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11390673249297671\n",
      "Average test loss: 0.003906746406729023\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1136149522860845\n",
      "Average test loss: 0.003872994100468026\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11342405383454429\n",
      "Average test loss: 0.003930594946154289\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11325266206264496\n",
      "Average test loss: 0.003887460058968928\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11313214356369443\n",
      "Average test loss: 0.003881752512107293\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11293457210726209\n",
      "Average test loss: 0.003933751334539718\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11261126036776437\n",
      "Average test loss: 0.003885065034031868\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11255279313855701\n",
      "Average test loss: 0.003873086908625232\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11234467748800914\n",
      "Average test loss: 0.0038885816329469283\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1121415584286054\n",
      "Average test loss: 0.00391186838514275\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11199641441636615\n",
      "Average test loss: 0.0038855529889050455\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11166966162125269\n",
      "Average test loss: 0.003919766897335648\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11146625251240201\n",
      "Average test loss: 0.0038994961527900563\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11132368624210358\n",
      "Average test loss: 0.0039247600949472855\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11120434856414795\n",
      "Average test loss: 0.003911286368552182\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11107249851359262\n",
      "Average test loss: 0.0039062959032340186\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11071962541341782\n",
      "Average test loss: 0.003920390499548779\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1104559986922476\n",
      "Average test loss: 0.003997613788892826\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11032105510764652\n",
      "Average test loss: 0.0039055839975674946\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11019673440853754\n",
      "Average test loss: 0.003920283569229974\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10995925605959363\n",
      "Average test loss: 0.003976509981271294\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10969510357247458\n",
      "Average test loss: 0.0038926784859763253\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10978118515676923\n",
      "Average test loss: 0.003934681109670136\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10930733905235926\n",
      "Average test loss: 0.003962263135446443\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10910702141125997\n",
      "Average test loss: 0.003911718665518695\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10882627152734332\n",
      "Average test loss: 0.003924711982202199\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10877213145626916\n",
      "Average test loss: 0.003896977858410941\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10852749694718256\n",
      "Average test loss: 0.003986001335291399\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10827599017487632\n",
      "Average test loss: 0.003936692584719923\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10812936443752713\n",
      "Average test loss: 0.0039059329599969916\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10782036993900934\n",
      "Average test loss: 0.003971595643088221\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10755343340502845\n",
      "Average test loss: 0.004028797997575667\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10738182326157887\n",
      "Average test loss: 0.0039268374964594845\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10719248133235508\n",
      "Average test loss: 0.003935573117807507\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10704895892408159\n",
      "Average test loss: 0.0039961444373346035\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10675865074661042\n",
      "Average test loss: 0.003981306806620624\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10663201183742947\n",
      "Average test loss: 0.00420945142250922\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10642482827107112\n",
      "Average test loss: 0.0039685086734178995\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10618038459618886\n",
      "Average test loss: 0.0040576708383030366\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10601907987727059\n",
      "Average test loss: 0.004028517923007409\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10587333714962005\n",
      "Average test loss: 0.003965581193566322\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10569572707679536\n",
      "Average test loss: 0.0040456987557311854\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10529654528035058\n",
      "Average test loss: 0.003994807802761595\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10524987188975016\n",
      "Average test loss: 0.00403518760866589\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10517330473330286\n",
      "Average test loss: 0.003998738099717431\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10472639355394575\n",
      "Average test loss: 0.0040512951815293895\n",
      "Epoch 114/300\n",
      "Average training loss: 0.1045860652923584\n",
      "Average test loss: 0.004131868184440665\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10441685928901037\n",
      "Average test loss: 0.0040124756557246045\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10419656500551436\n",
      "Average test loss: 0.0039730827160593535\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1041524062289132\n",
      "Average test loss: 0.004067684046717154\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10383089795377519\n",
      "Average test loss: 0.003994557104383905\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10366783892446094\n",
      "Average test loss: 0.004004572315451999\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10355392014980316\n",
      "Average test loss: 0.004100794946878321\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10353652107053332\n",
      "Average test loss: 0.0039791371114552025\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10303400542338689\n",
      "Average test loss: 0.004045079233745734\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10322124985191557\n",
      "Average test loss: 0.003955963555723429\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10304365377293692\n",
      "Average test loss: 0.004138080447912217\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10277009341451857\n",
      "Average test loss: 0.00404965729928679\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10243371358182696\n",
      "Average test loss: 0.0040561454674849905\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10240125529633629\n",
      "Average test loss: 0.004098230919076337\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10201196728812324\n",
      "Average test loss: 0.004130514309224155\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10211944843331973\n",
      "Average test loss: 0.004177807588337196\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10180299104584588\n",
      "Average test loss: 0.004068588015726871\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10164380561643177\n",
      "Average test loss: 0.00404736107463638\n",
      "Epoch 132/300\n",
      "Average training loss: 0.1014681843386756\n",
      "Average test loss: 0.004078596439419521\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10136132207843992\n",
      "Average test loss: 0.004065311649193366\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10117379115025202\n",
      "Average test loss: 0.004066806559347444\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10114426246616576\n",
      "Average test loss: 0.004112782723373837\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10087559305297004\n",
      "Average test loss: 0.004085527309940921\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10088271133767233\n",
      "Average test loss: 0.004114734729958905\n",
      "Epoch 138/300\n",
      "Average training loss: 0.100678371489048\n",
      "Average test loss: 0.004080504794294636\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10044250299533208\n",
      "Average test loss: 0.004083615768700838\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10035974762837092\n",
      "Average test loss: 0.004184421404782269\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1001993925637669\n",
      "Average test loss: 0.004104379744579395\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10004637311233415\n",
      "Average test loss: 0.0041180940510498155\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09998457295364804\n",
      "Average test loss: 0.004072246900449197\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0998592507176929\n",
      "Average test loss: 0.0042678037273387115\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09966848573419783\n",
      "Average test loss: 0.004069442469212744\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09949286094639036\n",
      "Average test loss: 0.0041499087303462955\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09932447334130605\n",
      "Average test loss: 0.004069249321396152\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09955214134852092\n",
      "Average test loss: 0.004156731262389157\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09898318668868807\n",
      "Average test loss: 0.004112255305465724\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0992412055267228\n",
      "Average test loss: 0.004060270188169347\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09907978474762705\n",
      "Average test loss: 0.004215711942563454\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0987305718395445\n",
      "Average test loss: 0.004131775592143337\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09860169533888499\n",
      "Average test loss: 0.004187673076573345\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09859619272417493\n",
      "Average test loss: 0.004144592021488481\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09842922637197707\n",
      "Average test loss: 0.00422677000073923\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0983409464293056\n",
      "Average test loss: 0.004081153038889169\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09800195588005914\n",
      "Average test loss: 0.004106057704322868\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0979942129155\n",
      "Average test loss: 0.004287531718611717\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09776680046319962\n",
      "Average test loss: 0.004173811412105957\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0978384446170595\n",
      "Average test loss: 0.00417292767845922\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0978105788760715\n",
      "Average test loss: 0.004301271638315585\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09753267415033447\n",
      "Average test loss: 0.004254500156475438\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09759052287207709\n",
      "Average test loss: 0.0041696950300700135\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09738605530394448\n",
      "Average test loss: 0.004140643817683061\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09721051654550765\n",
      "Average test loss: 0.004227193034564455\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0970817629231347\n",
      "Average test loss: 0.00425512258335948\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09713743031024932\n",
      "Average test loss: 0.004129987280401919\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09697365627686183\n",
      "Average test loss: 0.004242232932605677\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09682016526328192\n",
      "Average test loss: 0.004146675450727343\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09684730479452346\n",
      "Average test loss: 0.004128609673637483\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09664596338404549\n",
      "Average test loss: 0.004153991608570019\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09647747305366729\n",
      "Average test loss: 0.004190033934596512\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09653520367542903\n",
      "Average test loss: 0.004152691568765375\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09633603663576974\n",
      "Average test loss: 0.004174718847498298\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0960583730340004\n",
      "Average test loss: 0.004209231479093432\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09641880329449971\n",
      "Average test loss: 0.004128384082474642\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0959223098622428\n",
      "Average test loss: 0.0041980285073320075\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09617144560813903\n",
      "Average test loss: 0.004181037890828318\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0958429527481397\n",
      "Average test loss: 0.004082705320997371\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09597838147481283\n",
      "Average test loss: 0.004277939288980431\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0956883955862787\n",
      "Average test loss: 0.0041271089766588475\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09589200420180957\n",
      "Average test loss: 0.00421204168535769\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0954649875164032\n",
      "Average test loss: 0.0041679187251461875\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09538274361689886\n",
      "Average test loss: 0.004233515147740642\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09520646462837855\n",
      "Average test loss: 0.004271696708889471\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09519047533803517\n",
      "Average test loss: 0.00417768771491117\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09509291342231962\n",
      "Average test loss: 0.004189934917208221\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09509436524576612\n",
      "Average test loss: 0.004208437068594827\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09508558346827825\n",
      "Average test loss: 0.004304341562092304\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09519459744294484\n",
      "Average test loss: 0.004160526997927162\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09491915822691388\n",
      "Average test loss: 0.004109138916763995\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09453051662445068\n",
      "Average test loss: 0.004270793878576822\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09458109907971488\n",
      "Average test loss: 0.004178246857805385\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09457908750242656\n",
      "Average test loss: 0.0041758384570065475\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0944437631169955\n",
      "Average test loss: 0.004251024210204681\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09444580703311496\n",
      "Average test loss: 0.0042259283655633525\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09444934990008672\n",
      "Average test loss: 0.0041925282182378905\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09460771134164599\n",
      "Average test loss: 0.004190679676623808\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09399636231528388\n",
      "Average test loss: 0.004225162477542957\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0939669722252422\n",
      "Average test loss: 0.00425601125943164\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09415310660998026\n",
      "Average test loss: 0.004206274764612317\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09402061377300157\n",
      "Average test loss: 0.004214956115931272\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09390147343609068\n",
      "Average test loss: 0.0042404572067575325\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09375010993083319\n",
      "Average test loss: 0.004230855385876364\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0936270275314649\n",
      "Average test loss: 0.004431702822446823\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0939721302125189\n",
      "Average test loss: 0.004197104209413131\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09353592383530406\n",
      "Average test loss: 0.004513731864177519\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09349114971028434\n",
      "Average test loss: 0.004228481417728795\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09351841260327233\n",
      "Average test loss: 0.004130551807582378\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09326700330442853\n",
      "Average test loss: 0.004251445712728633\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09331035597456826\n",
      "Average test loss: 0.004160961855823795\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0932122333380911\n",
      "Average test loss: 0.004362737264070246\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09320720429552926\n",
      "Average test loss: 0.004306006855020921\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09308405347665151\n",
      "Average test loss: 0.004358262271309892\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09305786337455113\n",
      "Average test loss: 0.004236803598701954\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09286898965305752\n",
      "Average test loss: 0.004228584699332714\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09286377448505825\n",
      "Average test loss: 0.004303533323523071\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0928233594165908\n",
      "Average test loss: 0.004278666756633256\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0927546364598804\n",
      "Average test loss: 0.004121589459064934\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09263024723529815\n",
      "Average test loss: 0.004208551326145728\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09261248387230767\n",
      "Average test loss: 0.00425853910524812\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09254637209574382\n",
      "Average test loss: 0.0042293760782728595\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09245037688149346\n",
      "Average test loss: 0.004357325018280082\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09232033000389735\n",
      "Average test loss: 0.004235149705161651\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09226243215799332\n",
      "Average test loss: 0.0042193028873039615\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09216046682331297\n",
      "Average test loss: 0.004196847319809927\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09217974020375146\n",
      "Average test loss: 0.004173115727802118\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09303432923555374\n",
      "Average test loss: 0.004242546936704053\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09404739741484325\n",
      "Average test loss: 0.004193717496262656\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09201473191711637\n",
      "Average test loss: 0.004229388772199551\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09159170024924808\n",
      "Average test loss: 0.0042380040503210495\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09172262212965224\n",
      "Average test loss: 0.004189448845469289\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09155630307065116\n",
      "Average test loss: 0.004256699781037039\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09170979532930586\n",
      "Average test loss: 0.0041968192803776925\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09164921135372586\n",
      "Average test loss: 0.004282228568154905\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09176624076896243\n",
      "Average test loss: 0.004272429254319933\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09157120624515745\n",
      "Average test loss: 0.004160344747205575\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09177065931426154\n",
      "Average test loss: 0.004308051798078749\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09148469069931242\n",
      "Average test loss: 0.00432766528468993\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09144228914048937\n",
      "Average test loss: 0.004121554986470276\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09121435450845294\n",
      "Average test loss: 0.004251982728640238\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09127342355251312\n",
      "Average test loss: 0.004346514864721232\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09126422050926421\n",
      "Average test loss: 0.0042263778665413455\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09124629659785165\n",
      "Average test loss: 0.004270069641164607\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09111829620599747\n",
      "Average test loss: 0.004285022706207302\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09093358916044235\n",
      "Average test loss: 0.004286552893411782\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09089449605014589\n",
      "Average test loss: 0.004237483468527595\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09100030437442991\n",
      "Average test loss: 0.004235380788644155\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09088323291805056\n",
      "Average test loss: 0.004252780341853698\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09077274556292428\n",
      "Average test loss: 0.004393817999710639\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09095229027006362\n",
      "Average test loss: 0.0042724970815082395\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09078738188743592\n",
      "Average test loss: 0.004163514673295949\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0907078253560596\n",
      "Average test loss: 0.004223169261382686\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09064008685615328\n",
      "Average test loss: 0.004206531219184398\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09033870791064369\n",
      "Average test loss: 0.004240069595475991\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0904915890759892\n",
      "Average test loss: 0.00415921457277404\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09047495866484112\n",
      "Average test loss: 0.004149941260409024\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09055883518854776\n",
      "Average test loss: 0.004303293280096518\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09032905922995674\n",
      "Average test loss: 0.004186023021323813\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09024944897492726\n",
      "Average test loss: 0.004247508878923125\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09025053443511327\n",
      "Average test loss: 0.0042728446871042255\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09027297553751204\n",
      "Average test loss: 0.004280294583075576\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09009760899676217\n",
      "Average test loss: 0.0041649216512839\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09012436810135842\n",
      "Average test loss: 0.004198651018242041\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09057898218433062\n",
      "Average test loss: 0.004272776566031906\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08996458630429374\n",
      "Average test loss: 0.004273521659481857\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08980919022692574\n",
      "Average test loss: 0.004316511631839806\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08983883951769935\n",
      "Average test loss: 0.004182267345488071\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08983716100454331\n",
      "Average test loss: 0.004347463104666935\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0898288075129191\n",
      "Average test loss: 0.004216767234934701\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08977998677889507\n",
      "Average test loss: 0.004228952256548736\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08982006396849951\n",
      "Average test loss: 0.0041701471143298675\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08966469803121355\n",
      "Average test loss: 0.004242072929731674\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08955070092280706\n",
      "Average test loss: 0.0043860213744143645\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08956542713774575\n",
      "Average test loss: 0.004343293804261419\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08961997330851025\n",
      "Average test loss: 0.004300732525479463\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08948260895411174\n",
      "Average test loss: 0.004223027549477087\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08943231864770254\n",
      "Average test loss: 0.0042719661403033465\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0894306742085351\n",
      "Average test loss: 0.004316006738278601\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08946511157353719\n",
      "Average test loss: 0.004379403421034415\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08951264450947444\n",
      "Average test loss: 0.004208049510916074\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08936140358448029\n",
      "Average test loss: 0.004331134227001005\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08912319719129139\n",
      "Average test loss: 0.004314370725303889\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08914553866452642\n",
      "Average test loss: 0.004317187644955185\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08906830941306221\n",
      "Average test loss: 0.004277056832280424\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08912848698430591\n",
      "Average test loss: 0.004629258938547638\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08938959013753467\n",
      "Average test loss: 0.004294085359200835\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08888063159916136\n",
      "Average test loss: 0.004150278944108222\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08867223254508443\n",
      "Average test loss: 0.004380627105633418\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08897945286830267\n",
      "Average test loss: 0.004540462386276987\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08876591322819392\n",
      "Average test loss: 0.004271564273370637\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08898486267195807\n",
      "Average test loss: 0.004205030134568612\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08877907060252295\n",
      "Average test loss: 0.004292296674102545\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08875683351688915\n",
      "Average test loss: 0.004256697044604354\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08857354332341089\n",
      "Average test loss: 0.004300473917482628\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0885438606540362\n",
      "Average test loss: 0.004235443596003784\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08848438683483335\n",
      "Average test loss: 0.004269782666116953\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08860933915774027\n",
      "Average test loss: 0.004230458046206169\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08850985713137521\n",
      "Average test loss: 0.0042673807746420305\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08859041398101383\n",
      "Average test loss: 0.004279989149214493\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.161176109313965\n",
      "Average test loss: 0.10466704572074943\n",
      "Epoch 2/300\n",
      "Average training loss: 3.668481772528754\n",
      "Average test loss: 0.010313750647008419\n",
      "Epoch 3/300\n",
      "Average training loss: 2.7260861977471245\n",
      "Average test loss: 0.04038006058335304\n",
      "Epoch 4/300\n",
      "Average training loss: 2.056399145974053\n",
      "Average test loss: 0.2744462657939229\n",
      "Epoch 5/300\n",
      "Average training loss: 1.638465936978658\n",
      "Average test loss: 0.003928840573049254\n",
      "Epoch 6/300\n",
      "Average training loss: 1.286176105923123\n",
      "Average test loss: 0.003875439898421367\n",
      "Epoch 7/300\n",
      "Average training loss: 1.0202603816986084\n",
      "Average test loss: 0.003936438651548492\n",
      "Epoch 8/300\n",
      "Average training loss: 0.8082596556875441\n",
      "Average test loss: 0.003742946493749817\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6662096654574077\n",
      "Average test loss: 0.0036338987443596126\n",
      "Epoch 10/300\n",
      "Average training loss: 0.544753844473097\n",
      "Average test loss: 0.0037178578376770017\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4360325617790222\n",
      "Average test loss: 0.0036391744582603375\n",
      "Epoch 12/300\n",
      "Average training loss: 0.35558577410380043\n",
      "Average test loss: 0.0034758166186511516\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3003870578342014\n",
      "Average test loss: 0.003422651982969708\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2586356772184372\n",
      "Average test loss: 0.003419416272184915\n",
      "Epoch 15/300\n",
      "Average training loss: 0.22685873030291664\n",
      "Average test loss: 0.0033204725397129853\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2025135650899675\n",
      "Average test loss: 0.0033573725341508784\n",
      "Epoch 17/300\n",
      "Average training loss: 0.18309545018937853\n",
      "Average test loss: 0.0032940335041946834\n",
      "Epoch 18/300\n",
      "Average training loss: 0.16782679344548118\n",
      "Average test loss: 0.003213450514400999\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1557499038775762\n",
      "Average test loss: 0.0032753637644151847\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1458869756990009\n",
      "Average test loss: 0.0032030368890199395\n",
      "Epoch 21/300\n",
      "Average training loss: 0.13776409302155176\n",
      "Average test loss: 0.0031307050306349994\n",
      "Epoch 22/300\n",
      "Average training loss: 0.13093705826997756\n",
      "Average test loss: 0.0031480031119038663\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12496681793530783\n",
      "Average test loss: 0.003108420462244087\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12001738778087828\n",
      "Average test loss: 0.003065850301335255\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11599103176593781\n",
      "Average test loss: 0.0030293639393316374\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1125889102684127\n",
      "Average test loss: 0.0030467207920220163\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10957215217749278\n",
      "Average test loss: 0.003029518672161632\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10694088350401984\n",
      "Average test loss: 0.0030277499159177142\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10501094152530034\n",
      "Average test loss: 0.0029573976405792768\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10340957208474477\n",
      "Average test loss: 0.0029730525207188395\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1020370372864935\n",
      "Average test loss: 0.003011399995121691\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10076367760698\n",
      "Average test loss: 0.002956499335459537\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09970065566235119\n",
      "Average test loss: 0.002917588958930638\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09867394748661253\n",
      "Average test loss: 0.002918494815627734\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09774807245863809\n",
      "Average test loss: 0.0029070370395978293\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09679988213380178\n",
      "Average test loss: 0.002890545913328727\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09603816864887874\n",
      "Average test loss: 0.002899400411380662\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09505095836851332\n",
      "Average test loss: 0.0028749710170345175\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09438593711455663\n",
      "Average test loss: 0.002875307345141967\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09364511711729898\n",
      "Average test loss: 0.0029088012493318984\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09281655629475911\n",
      "Average test loss: 0.002903965815073914\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09223747915691799\n",
      "Average test loss: 0.0028807245966874892\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09151088399357266\n",
      "Average test loss: 0.0028739696233015923\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09091453977425894\n",
      "Average test loss: 0.0028857425487496787\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09047728529903624\n",
      "Average test loss: 0.002843527579266164\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08965798053476545\n",
      "Average test loss: 0.0028416949801353944\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08908428051074346\n",
      "Average test loss: 0.0028660327760088775\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08889563411474227\n",
      "Average test loss: 0.0028529314576751655\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08813783924778303\n",
      "Average test loss: 0.0028351855526367823\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08767255562543869\n",
      "Average test loss: 0.0028455334547907115\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08718223739994897\n",
      "Average test loss: 0.002820254243289431\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08680551811059316\n",
      "Average test loss: 0.0028316348255094555\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08627117286788093\n",
      "Average test loss: 0.0028204047137664425\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08572375697559781\n",
      "Average test loss: 0.0028531534348924955\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0854000222550498\n",
      "Average test loss: 0.0028275221491025552\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08489654421144062\n",
      "Average test loss: 0.0028407772398657267\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08456540763378144\n",
      "Average test loss: 0.0028132193440364465\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08413746638430489\n",
      "Average test loss: 0.0028141887235558693\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08385774646864998\n",
      "Average test loss: 0.00285215851974984\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0832186592022578\n",
      "Average test loss: 0.0028083748846418326\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08320057889488008\n",
      "Average test loss: 0.002808614698963033\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08249601827396287\n",
      "Average test loss: 0.0028668458339654737\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08209002572960324\n",
      "Average test loss: 0.0028015026849591068\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08160564937856463\n",
      "Average test loss: 0.002814445031185945\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08144597703880734\n",
      "Average test loss: 0.0028336433660652903\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08109193442927466\n",
      "Average test loss: 0.0029796000192355778\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0806907212999132\n",
      "Average test loss: 0.0028575115309407316\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08038707925213708\n",
      "Average test loss: 0.0028635326967471177\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07974148636394077\n",
      "Average test loss: 0.0028144603392316237\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07954854241344664\n",
      "Average test loss: 0.002811663175622622\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07928732803795073\n",
      "Average test loss: 0.002846555850779017\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07882343141900168\n",
      "Average test loss: 0.002823921207131611\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0784815296596951\n",
      "Average test loss: 0.0029007289268904264\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07813910892936919\n",
      "Average test loss: 0.002819798893191748\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07768612312608295\n",
      "Average test loss: 0.002844916264836987\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07748545583751466\n",
      "Average test loss: 0.0028898243156986105\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07713417740662892\n",
      "Average test loss: 0.002868375973362062\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0767673768930965\n",
      "Average test loss: 0.0029128616677804127\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07639566663238738\n",
      "Average test loss: 0.002897347836030854\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07614511370658875\n",
      "Average test loss: 0.002931424136066602\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0759226094716125\n",
      "Average test loss: 0.0028988358970317577\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0756550425224834\n",
      "Average test loss: 0.002849151120417648\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07522019286619293\n",
      "Average test loss: 0.0029551835747228727\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07505380608638128\n",
      "Average test loss: 0.002890949191008177\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07469251085652245\n",
      "Average test loss: 0.002941834657970402\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07453872411780887\n",
      "Average test loss: 0.0029851324854211674\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07414670303795073\n",
      "Average test loss: 0.002895592408358223\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07387733269731203\n",
      "Average test loss: 0.002869953436156114\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07362073741687669\n",
      "Average test loss: 0.002926528256593479\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07337087227900822\n",
      "Average test loss: 0.002898092505004671\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07308861317899493\n",
      "Average test loss: 0.002920265578561359\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07278113819493187\n",
      "Average test loss: 0.0029979617568767734\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07276712758011288\n",
      "Average test loss: 0.002916591418493125\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07241913432213995\n",
      "Average test loss: 0.0028927527517080306\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07213423786891832\n",
      "Average test loss: 0.002982770349830389\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07189804121520785\n",
      "Average test loss: 0.00302739196539753\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0715710609290335\n",
      "Average test loss: 0.0029060660493042735\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07158099056283633\n",
      "Average test loss: 0.0029283686305085817\n",
      "Epoch 99/300\n",
      "Average training loss: 0.071284568346209\n",
      "Average test loss: 0.002969898677741488\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07116558110713958\n",
      "Average test loss: 0.0029233777701026865\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07082560571697023\n",
      "Average test loss: 0.002894610684365034\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07091935177312957\n",
      "Average test loss: 0.0030208577871736553\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07066244237621626\n",
      "Average test loss: 0.0029395062124563586\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07026846965816286\n",
      "Average test loss: 0.0030033514731460146\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06999044669999016\n",
      "Average test loss: 0.002966584573603339\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06991105421715313\n",
      "Average test loss: 0.002970711203498973\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06976337583197488\n",
      "Average test loss: 0.002986704563928975\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06953709892100758\n",
      "Average test loss: 0.003019550636307233\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0695986518462499\n",
      "Average test loss: 0.002934336084458563\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06924102761016952\n",
      "Average test loss: 0.0030227024751818842\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06916038250260884\n",
      "Average test loss: 0.0029704644851800467\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06893744279278649\n",
      "Average test loss: 0.003023218911865519\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06871430500348409\n",
      "Average test loss: 0.003006254019629624\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06869597386651569\n",
      "Average test loss: 0.0029891818132665423\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06845023878746563\n",
      "Average test loss: 0.0030062702873514756\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06831519328885609\n",
      "Average test loss: 0.003025670233906971\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06838064839442572\n",
      "Average test loss: 0.002967417500913143\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06795303897062938\n",
      "Average test loss: 0.0030727606401261356\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0679964757959048\n",
      "Average test loss: 0.0030644574210875563\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06773240399360657\n",
      "Average test loss: 0.0029556724949636394\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06750154751870367\n",
      "Average test loss: 0.0029505348516007265\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06750818285346032\n",
      "Average test loss: 0.003038992675021291\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06730496990680694\n",
      "Average test loss: 0.002948600341462427\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06719736694958475\n",
      "Average test loss: 0.0029361364206092226\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06708786883619096\n",
      "Average test loss: 0.002993428060164054\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06711998135513729\n",
      "Average test loss: 0.0029629723555925818\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06676507520013385\n",
      "Average test loss: 0.002990126736358636\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06675257417559624\n",
      "Average test loss: 0.003032669441153606\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06663357099062867\n",
      "Average test loss: 0.0029994765698081916\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06646700104077657\n",
      "Average test loss: 0.0030843272308508553\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06686929897467295\n",
      "Average test loss: 0.0029856982653339702\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06609265749984318\n",
      "Average test loss: 0.0031260912577725118\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06599630589617624\n",
      "Average test loss: 0.003007697050770124\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06591533251603444\n",
      "Average test loss: 0.0030800882660680347\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06594777155915896\n",
      "Average test loss: 0.003007883980870247\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0657537730799781\n",
      "Average test loss: 0.0030715877635197507\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06561907991435793\n",
      "Average test loss: 0.0030174886646370095\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06572382842501004\n",
      "Average test loss: 0.002987115267250273\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06539695420530107\n",
      "Average test loss: 0.003062618551154931\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06529932129383087\n",
      "Average test loss: 0.003069347949077686\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06523203101423052\n",
      "Average test loss: 0.0030558759172757467\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06515134628117085\n",
      "Average test loss: 0.0030830701989018254\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06506065153744485\n",
      "Average test loss: 0.003031833165635665\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06499612350927458\n",
      "Average test loss: 0.00301537544797692\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06492901518940926\n",
      "Average test loss: 0.003042459780557288\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06476795868410004\n",
      "Average test loss: 0.003028956044672264\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06470004957252079\n",
      "Average test loss: 0.0030592113693969115\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06452447132269541\n",
      "Average test loss: 0.003108703443987502\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06457723752657572\n",
      "Average test loss: 0.003771037476551202\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06450458836886618\n",
      "Average test loss: 0.0031320354156196117\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06421680353747473\n",
      "Average test loss: 0.0032137392680678103\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06440260520246294\n",
      "Average test loss: 0.003168287315716346\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06420252357588874\n",
      "Average test loss: 0.003169754898175597\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06419593688514498\n",
      "Average test loss: 0.003006565582421091\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06400908687710762\n",
      "Average test loss: 0.0030755171382592784\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06395535136593712\n",
      "Average test loss: 0.0030739473619808754\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06373582079344325\n",
      "Average test loss: 0.0030477493510891996\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06368504920932981\n",
      "Average test loss: 0.003094516058348947\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06357187885377143\n",
      "Average test loss: 0.003045388317356507\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06365402569704585\n",
      "Average test loss: 0.00311783408621947\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06347039294573996\n",
      "Average test loss: 0.0033053243762503066\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06337063001592955\n",
      "Average test loss: 0.0029837214490398764\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06327505821651883\n",
      "Average test loss: 0.003043772017997172\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06324033117956586\n",
      "Average test loss: 0.0031278940772430765\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06337021696236399\n",
      "Average test loss: 0.003060005906555388\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0630141499042511\n",
      "Average test loss: 0.003093380545369453\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06305253897772894\n",
      "Average test loss: 0.003101450175046921\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06326052912407451\n",
      "Average test loss: 0.0030525571312755347\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0629591971569591\n",
      "Average test loss: 0.0031318187641186847\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06283925230635537\n",
      "Average test loss: 0.003086124426788754\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06270588071478737\n",
      "Average test loss: 0.0031762793304191696\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06258055716421869\n",
      "Average test loss: 0.003066434394982126\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06268272593286302\n",
      "Average test loss: 0.0030803702438457143\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06255484308136834\n",
      "Average test loss: 0.003148852134744326\n",
      "Epoch 175/300\n",
      "Average training loss: 0.062443999061981836\n",
      "Average test loss: 0.003091491466595067\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06231699652141995\n",
      "Average test loss: 0.002967767211712069\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06228627621796396\n",
      "Average test loss: 0.0030565679298920766\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06226063989599546\n",
      "Average test loss: 0.003127236498726739\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06209088499347369\n",
      "Average test loss: 0.0031149963852432037\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06225713884499338\n",
      "Average test loss: 0.0030993927975909575\n",
      "Epoch 181/300\n",
      "Average training loss: 0.062102893193562825\n",
      "Average test loss: 0.003124715691225396\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06204478842682309\n",
      "Average test loss: 0.0031296106127815114\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06193877700302336\n",
      "Average test loss: 0.0030526069671743448\n",
      "Epoch 184/300\n",
      "Average training loss: 0.061753118587864766\n",
      "Average test loss: 0.00314231341621942\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06191164333952798\n",
      "Average test loss: 0.003070794761594799\n",
      "Epoch 186/300\n",
      "Average training loss: 0.061817316157950296\n",
      "Average test loss: 0.0031004413382874595\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06173040360543463\n",
      "Average test loss: 0.003111719399690628\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06177032413085302\n",
      "Average test loss: 0.0030889172876874607\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06167530499564277\n",
      "Average test loss: 0.003003264048861133\n",
      "Epoch 190/300\n",
      "Average training loss: 0.061392976780732474\n",
      "Average test loss: 0.0031744611422634787\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06140029705564181\n",
      "Average test loss: 0.0030495324757777984\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06139126802484195\n",
      "Average test loss: 0.0030781434323224758\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06132990340060658\n",
      "Average test loss: 0.003060397211048338\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06136542199386491\n",
      "Average test loss: 0.003110607793347703\n",
      "Epoch 195/300\n",
      "Average training loss: 0.061152240753173825\n",
      "Average test loss: 0.003124145379289985\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06103275782863299\n",
      "Average test loss: 0.0030671802115523152\n",
      "Epoch 197/300\n",
      "Average training loss: 0.061196231149964864\n",
      "Average test loss: 0.0030950359619326064\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06091070479485724\n",
      "Average test loss: 0.003181480209239655\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06097768044802877\n",
      "Average test loss: 0.0030860935184690687\n",
      "Epoch 200/300\n",
      "Average training loss: 0.060928266164329316\n",
      "Average test loss: 0.003201611594814393\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06089400722251998\n",
      "Average test loss: 0.0031207322194758387\n",
      "Epoch 202/300\n",
      "Average training loss: 0.061038028379281364\n",
      "Average test loss: 0.0031220764902730783\n",
      "Epoch 203/300\n",
      "Average training loss: 0.060724002894428045\n",
      "Average test loss: 0.003092626383735074\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0606751571893692\n",
      "Average test loss: 0.003229579508304596\n",
      "Epoch 205/300\n",
      "Average training loss: 0.060705888874000975\n",
      "Average test loss: 0.0031225931040114825\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06057817123333613\n",
      "Average test loss: 0.003107038138848212\n",
      "Epoch 207/300\n",
      "Average training loss: 0.060668771482176254\n",
      "Average test loss: 0.0031287930438088046\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06068256742093298\n",
      "Average test loss: 0.0031812108016262453\n",
      "Epoch 209/300\n",
      "Average training loss: 0.060480711960130266\n",
      "Average test loss: 0.003072797552578979\n",
      "Epoch 210/300\n",
      "Average training loss: 0.060349052598079045\n",
      "Average test loss: 0.0031186235131074984\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06046594786975119\n",
      "Average test loss: 0.0031477022404885952\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06030999620755514\n",
      "Average test loss: 0.003090123938396573\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06026487962404887\n",
      "Average test loss: 0.0031004502619099285\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06022392640842332\n",
      "Average test loss: 0.003139390807598829\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06039130386047893\n",
      "Average test loss: 0.003114968292829063\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06024569057424863\n",
      "Average test loss: 0.003101174566687809\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06007858963476287\n",
      "Average test loss: 0.0030870550524236427\n",
      "Epoch 218/300\n",
      "Average training loss: 0.059977661503685845\n",
      "Average test loss: 0.0031150681169496643\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06022168345252673\n",
      "Average test loss: 0.003167651624729236\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05989670156273577\n",
      "Average test loss: 0.0031839050485028162\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05996153551008966\n",
      "Average test loss: 0.003187341322708461\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05986664423677657\n",
      "Average test loss: 0.0031323000345793036\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05987267927328745\n",
      "Average test loss: 0.003203768044296238\n",
      "Epoch 224/300\n",
      "Average training loss: 0.059749648074309034\n",
      "Average test loss: 0.0031812746860086917\n",
      "Epoch 225/300\n",
      "Average training loss: 0.059720352712604736\n",
      "Average test loss: 0.003096478772453136\n",
      "Epoch 226/300\n",
      "Average training loss: 0.059757280243767634\n",
      "Average test loss: 0.003133695569303301\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05963622790575027\n",
      "Average test loss: 0.0031984982438799407\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05954016565283139\n",
      "Average test loss: 0.003262003315612674\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0595120437906848\n",
      "Average test loss: 0.003089513673136632\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05961481321189139\n",
      "Average test loss: 0.003210335124284029\n",
      "Epoch 231/300\n",
      "Average training loss: 0.059457206904888156\n",
      "Average test loss: 0.0032135754936478203\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05946584168738789\n",
      "Average test loss: 0.003214157211387323\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05951418514715301\n",
      "Average test loss: 0.0031583925547699135\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05926960386170281\n",
      "Average test loss: 0.003090790296594302\n",
      "Epoch 235/300\n",
      "Average training loss: 0.059207824892467925\n",
      "Average test loss: 0.0030982748369375863\n",
      "Epoch 236/300\n",
      "Average training loss: 0.059206433571047254\n",
      "Average test loss: 0.003149255403214031\n",
      "Epoch 237/300\n",
      "Average training loss: 0.059238659848769505\n",
      "Average test loss: 0.0030473397914320233\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05914319754640261\n",
      "Average test loss: 0.0031030730572011735\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05917551206880146\n",
      "Average test loss: 0.0031196516460428635\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05911680696739091\n",
      "Average test loss: 0.003040517167912589\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05912160551548004\n",
      "Average test loss: 0.0031460054479539395\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05897500576575597\n",
      "Average test loss: 0.0031517834799985093\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05898443928029802\n",
      "Average test loss: 0.003142030597767896\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05901437803440624\n",
      "Average test loss: 0.003143062803687321\n",
      "Epoch 245/300\n",
      "Average training loss: 0.058824479556745954\n",
      "Average test loss: 0.0031378492530849244\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05889220565226343\n",
      "Average test loss: 0.0032077000704076554\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05884325478805436\n",
      "Average test loss: 0.0031530386573738523\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05871922617488437\n",
      "Average test loss: 0.0031227682917896243\n",
      "Epoch 249/300\n",
      "Average training loss: 0.058834917050268914\n",
      "Average test loss: 0.003190713848504755\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05871328785684374\n",
      "Average test loss: 0.003168908550093571\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05869959076245626\n",
      "Average test loss: 0.003126494251915978\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05855501264664862\n",
      "Average test loss: 0.003093837786051962\n",
      "Epoch 253/300\n",
      "Average training loss: 0.058440937966108324\n",
      "Average test loss: 0.003129028480086062\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05865962216920323\n",
      "Average test loss: 0.0031417810374663934\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05871472289827135\n",
      "Average test loss: 0.003144778037443757\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05847412919004758\n",
      "Average test loss: 0.0031402800826148855\n",
      "Epoch 257/300\n",
      "Average training loss: 0.058452681991789074\n",
      "Average test loss: 0.003169858168396685\n",
      "Epoch 258/300\n",
      "Average training loss: 0.058300628509786394\n",
      "Average test loss: 0.0031709071619229184\n",
      "Epoch 259/300\n",
      "Average training loss: 0.058441276136371824\n",
      "Average test loss: 0.0031260969682286185\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05861430291997062\n",
      "Average test loss: 0.0031840368594146435\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05836574094163047\n",
      "Average test loss: 0.003261258481691281\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05834030094080501\n",
      "Average test loss: 0.003216266692098644\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0582334594560994\n",
      "Average test loss: 0.003249221822867791\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05809770682122972\n",
      "Average test loss: 0.0031978993573122554\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05821877676910824\n",
      "Average test loss: 0.0031443323931760257\n",
      "Epoch 266/300\n",
      "Average training loss: 0.058161816318829856\n",
      "Average test loss: 0.0031212131747355063\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0581853348215421\n",
      "Average test loss: 0.003129291985183954\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0580611853202184\n",
      "Average test loss: 0.00307309214439657\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05797040891647339\n",
      "Average test loss: 0.0032478054066499076\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05795551398396492\n",
      "Average test loss: 0.0031403538973795043\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05801804722348849\n",
      "Average test loss: 0.0031742369828538764\n",
      "Epoch 272/300\n",
      "Average training loss: 0.057833540618419645\n",
      "Average test loss: 0.0031495187510218884\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05781133006016413\n",
      "Average test loss: 0.003211923541708125\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05778846298986011\n",
      "Average test loss: 0.0031587900473839707\n",
      "Epoch 275/300\n",
      "Average training loss: 0.057862673958142596\n",
      "Average test loss: 0.0031170103376110393\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05789951818188031\n",
      "Average test loss: 0.003150317267411285\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05775702511436409\n",
      "Average test loss: 0.003161069894830386\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05775470444891188\n",
      "Average test loss: 0.0031356105469167233\n",
      "Epoch 279/300\n",
      "Average training loss: 0.057845073766178555\n",
      "Average test loss: 0.003116409472086363\n",
      "Epoch 280/300\n",
      "Average training loss: 0.057597749998172125\n",
      "Average test loss: 0.003159766747512751\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05800568876663844\n",
      "Average test loss: 0.0031711352411657573\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05777374074525303\n",
      "Average test loss: 0.0032142527313489054\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05771851248211331\n",
      "Average test loss: 0.003126903517792622\n",
      "Epoch 284/300\n",
      "Average training loss: 0.057351844833956826\n",
      "Average test loss: 0.0032480914661039907\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05739551008741061\n",
      "Average test loss: 0.003204689465256201\n",
      "Epoch 286/300\n",
      "Average training loss: 0.057490110910601085\n",
      "Average test loss: 0.0031517331131423513\n",
      "Epoch 287/300\n",
      "Average training loss: 0.057385223862197667\n",
      "Average test loss: 0.003103759327489469\n",
      "Epoch 288/300\n",
      "Average training loss: 0.057464817341830995\n",
      "Average test loss: 0.003167653371890386\n",
      "Epoch 289/300\n",
      "Average training loss: 0.057424043022924\n",
      "Average test loss: 0.0032313477536663414\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05724223668045468\n",
      "Average test loss: 0.0031756787293901047\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05737154977851444\n",
      "Average test loss: 0.0031545569969134196\n",
      "Epoch 292/300\n",
      "Average training loss: 0.057365382466051315\n",
      "Average test loss: 0.0032190586996989116\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05736103683378961\n",
      "Average test loss: 0.0032071414977932968\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05725763316618072\n",
      "Average test loss: 0.0031019431865877576\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05713695955938763\n",
      "Average test loss: 0.0031391933088501293\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05708777450521787\n",
      "Average test loss: 0.003111798308375809\n",
      "Epoch 297/300\n",
      "Average training loss: 0.057249940097332\n",
      "Average test loss: 0.0031302802786231042\n",
      "Epoch 298/300\n",
      "Average training loss: 0.057174093132217725\n",
      "Average test loss: 0.003195549039997988\n",
      "Epoch 299/300\n",
      "Average training loss: 0.057094666242599484\n",
      "Average test loss: 0.0031902715617583856\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05706615381770664\n",
      "Average test loss: 0.003197923463665777\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 9.460614426930745\n",
      "Average test loss: 0.004764420729130506\n",
      "Epoch 2/300\n",
      "Average training loss: 4.018248633702596\n",
      "Average test loss: 0.007363665194975005\n",
      "Epoch 3/300\n",
      "Average training loss: 2.652854438993666\n",
      "Average test loss: 0.0036985574987613494\n",
      "Epoch 4/300\n",
      "Average training loss: 2.1919557656182183\n",
      "Average test loss: 0.0035771714996339545\n",
      "Epoch 5/300\n",
      "Average training loss: 1.8433666841718885\n",
      "Average test loss: 0.009019407032264604\n",
      "Epoch 6/300\n",
      "Average training loss: 1.5772424271901448\n",
      "Average test loss: 0.003331313149796592\n",
      "Epoch 7/300\n",
      "Average training loss: 1.2859056788550483\n",
      "Average test loss: 0.003372540422404806\n",
      "Epoch 8/300\n",
      "Average training loss: 1.0721375342475044\n",
      "Average test loss: 0.0030887062462667623\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8927610960536533\n",
      "Average test loss: 0.0030632018397251763\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7405176384184096\n",
      "Average test loss: 0.0028851545931150514\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6226889048682319\n",
      "Average test loss: 0.0028630078801264365\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5234177547030979\n",
      "Average test loss: 0.0028756867847922777\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4410587946044074\n",
      "Average test loss: 0.0028278909880254\n",
      "Epoch 14/300\n",
      "Average training loss: 0.37366519027286105\n",
      "Average test loss: 0.0026491770257966387\n",
      "Epoch 15/300\n",
      "Average training loss: 0.31865583647622003\n",
      "Average test loss: 0.0026231256112870245\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2744604833126068\n",
      "Average test loss: 0.0025092853746480413\n",
      "Epoch 17/300\n",
      "Average training loss: 0.23845295339160497\n",
      "Average test loss: 0.002558367050977217\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2087461506128311\n",
      "Average test loss: 0.00250313659860856\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18431041355927785\n",
      "Average test loss: 0.0023969132751226426\n",
      "Epoch 20/300\n",
      "Average training loss: 0.16541233407126532\n",
      "Average test loss: 0.002334061579985751\n",
      "Epoch 21/300\n",
      "Average training loss: 0.14961507398552365\n",
      "Average test loss: 0.002368508680413167\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1368064097099834\n",
      "Average test loss: 0.0022776417858484718\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1262706397804949\n",
      "Average test loss: 0.0022830582039637696\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11772495909531912\n",
      "Average test loss: 0.0023095093184254236\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1106609448062049\n",
      "Average test loss: 0.0022092952593747114\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10442893921666675\n",
      "Average test loss: 0.002196231894609001\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09908191421296861\n",
      "Average test loss: 0.002249003481430312\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09483811725510491\n",
      "Average test loss: 0.0021747143001606068\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09115670366419686\n",
      "Average test loss: 0.002151868110108707\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08797992919219864\n",
      "Average test loss: 0.0021949326859580145\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08522342220942179\n",
      "Average test loss: 0.0021799010986255276\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08301818521817525\n",
      "Average test loss: 0.0021342865676722595\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08107407487763299\n",
      "Average test loss: 0.002137795890888406\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07960904040601519\n",
      "Average test loss: 0.0021268490635686452\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07818426527579625\n",
      "Average test loss: 0.002073754366280304\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07692879954311582\n",
      "Average test loss: 0.0020925461933430697\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07567822054359648\n",
      "Average test loss: 0.0020873845777370863\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07462185770273208\n",
      "Average test loss: 0.002081807965102295\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07369613590174251\n",
      "Average test loss: 0.002071466200053692\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07260680831140942\n",
      "Average test loss: 0.002080659611771504\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07180908795197805\n",
      "Average test loss: 0.00206497136141277\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07081596952014499\n",
      "Average test loss: 0.0020638218571742374\n",
      "Epoch 43/300\n",
      "Average training loss: 0.070083086543613\n",
      "Average test loss: 0.0020335673482881652\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06930900059143702\n",
      "Average test loss: 0.002108522686900364\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06896114014254676\n",
      "Average test loss: 0.0020780467169566287\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06795750560363134\n",
      "Average test loss: 0.002016953117524584\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06728069638212522\n",
      "Average test loss: 0.0020095546423561043\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0667773548596435\n",
      "Average test loss: 0.0020312294179780617\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06621795294019911\n",
      "Average test loss: 0.0020034340319948062\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0654788207411766\n",
      "Average test loss: 0.0020655222475114794\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06507233077287675\n",
      "Average test loss: 0.0020312386467638945\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06442022729582257\n",
      "Average test loss: 0.002005753982812166\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06412686281402906\n",
      "Average test loss: 0.0020176201106773483\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06354839827948146\n",
      "Average test loss: 0.0020133963576000596\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06309900135133001\n",
      "Average test loss: 0.0020075017371111448\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06271996691823006\n",
      "Average test loss: 0.0020260226805177\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0622764287326071\n",
      "Average test loss: 0.002026207462574045\n",
      "Epoch 58/300\n",
      "Average training loss: 0.061864439679516686\n",
      "Average test loss: 0.001995224989536736\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06138879163066546\n",
      "Average test loss: 0.0019981650809446972\n",
      "Epoch 60/300\n",
      "Average training loss: 0.060839285999536516\n",
      "Average test loss: 0.0019995492899583444\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06060750715268983\n",
      "Average test loss: 0.002013862031615443\n",
      "Epoch 62/300\n",
      "Average training loss: 0.060216931240426166\n",
      "Average test loss: 0.001989626126156913\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05977069290479024\n",
      "Average test loss: 0.0020103136320701906\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05934745180937979\n",
      "Average test loss: 0.0020355929316331943\n",
      "Epoch 65/300\n",
      "Average training loss: 0.059038667301336924\n",
      "Average test loss: 0.002005466519130601\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05880588590436511\n",
      "Average test loss: 0.002023422814698683\n",
      "Epoch 67/300\n",
      "Average training loss: 0.058372004909647834\n",
      "Average test loss: 0.00200088671884603\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0579169049958388\n",
      "Average test loss: 0.0020120917322734994\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05766924058397611\n",
      "Average test loss: 0.002006249582187997\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05729391149514251\n",
      "Average test loss: 0.0020647988511870306\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05696020586623086\n",
      "Average test loss: 0.002023067509341571\n",
      "Epoch 72/300\n",
      "Average training loss: 0.056752030180560215\n",
      "Average test loss: 0.0020591528941359787\n",
      "Epoch 73/300\n",
      "Average training loss: 0.056315838601854115\n",
      "Average test loss: 0.001999037430104282\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05609489529993799\n",
      "Average test loss: 0.002040563194702069\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05576131243838204\n",
      "Average test loss: 0.00201444965083566\n",
      "Epoch 76/300\n",
      "Average training loss: 0.055478128055731456\n",
      "Average test loss: 0.0020676584541797637\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05512547352910042\n",
      "Average test loss: 0.00206310398504138\n",
      "Epoch 78/300\n",
      "Average training loss: 0.054988045665952896\n",
      "Average test loss: 0.0020403484997029105\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05473997158143255\n",
      "Average test loss: 0.002020181056215531\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05450250188178486\n",
      "Average test loss: 0.0020268547023750013\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05399170078833898\n",
      "Average test loss: 0.00204641325192319\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05366813702715768\n",
      "Average test loss: 0.002068856770897077\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05345588798655404\n",
      "Average test loss: 0.002027570334987508\n",
      "Epoch 84/300\n",
      "Average training loss: 0.053351622644397946\n",
      "Average test loss: 0.0020684036558700934\n",
      "Epoch 85/300\n",
      "Average training loss: 0.053062489797671634\n",
      "Average test loss: 0.0021826513819396495\n",
      "Epoch 86/300\n",
      "Average training loss: 0.052770650217930475\n",
      "Average test loss: 0.002127547898640235\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05263359647326999\n",
      "Average test loss: 0.0020894906351135835\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05230521276262071\n",
      "Average test loss: 0.002067142459977832\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05204583439893193\n",
      "Average test loss: 0.0020641689779650834\n",
      "Epoch 90/300\n",
      "Average training loss: 0.052025011277861063\n",
      "Average test loss: 0.002070605614636507\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05167148369881842\n",
      "Average test loss: 0.002549328975379467\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05158208376169205\n",
      "Average test loss: 0.0020669847140088677\n",
      "Epoch 93/300\n",
      "Average training loss: 0.051314285775025684\n",
      "Average test loss: 0.00217484077759501\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05090452835791641\n",
      "Average test loss: 0.0021180864069610832\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05089986060063044\n",
      "Average test loss: 0.0020954002317869003\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05059170064992375\n",
      "Average test loss: 0.0020916654726283418\n",
      "Epoch 97/300\n",
      "Average training loss: 0.050456524706549113\n",
      "Average test loss: 0.0022894589927875335\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05025600204202864\n",
      "Average test loss: 0.0020932135234276454\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04999964016675949\n",
      "Average test loss: 0.0021133211853189603\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04985239452454779\n",
      "Average test loss: 0.0021452758519185915\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04975843985544311\n",
      "Average test loss: 0.002125834639287657\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04951456336180369\n",
      "Average test loss: 0.002134327847096655\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04940596519245042\n",
      "Average test loss: 0.0021446892467016977\n",
      "Epoch 104/300\n",
      "Average training loss: 0.049225168701675204\n",
      "Average test loss: 0.0020896324132465656\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04910955766174528\n",
      "Average test loss: 0.0022880546864536074\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04884711746374766\n",
      "Average test loss: 0.0021585572825537787\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04890017988946703\n",
      "Average test loss: 0.0020798900787615112\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04852420932054519\n",
      "Average test loss: 0.0021003254835183422\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04843456537193722\n",
      "Average test loss: 0.0021336872146154443\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04833354140652551\n",
      "Average test loss: 0.0020925259739160537\n",
      "Epoch 111/300\n",
      "Average training loss: 0.048251009980837506\n",
      "Average test loss: 0.0021373043397648465\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04794951544867621\n",
      "Average test loss: 0.002153249091779192\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0479076986014843\n",
      "Average test loss: 0.0021232511889603405\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04784011042449209\n",
      "Average test loss: 0.0021339642552451956\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04760217657023006\n",
      "Average test loss: 0.0020904341236584714\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04746584230330255\n",
      "Average test loss: 0.0021474531212200723\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04738510374890433\n",
      "Average test loss: 0.0021017953774167433\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04721644934515158\n",
      "Average test loss: 0.002207566207067834\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04724230723910861\n",
      "Average test loss: 0.0021306975533564886\n",
      "Epoch 120/300\n",
      "Average training loss: 0.047097221102979446\n",
      "Average test loss: 0.002143383594850699\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0470116081668271\n",
      "Average test loss: 0.0021997550833556387\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04681692988342709\n",
      "Average test loss: 0.0021829069743139876\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04677556037571695\n",
      "Average test loss: 0.002203029124583635\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0465507423132658\n",
      "Average test loss: 0.002342060864178671\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04643098165260421\n",
      "Average test loss: 0.0021317046731710435\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04641189526849323\n",
      "Average test loss: 0.002175611273902986\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04624172729253769\n",
      "Average test loss: 0.002203670583044489\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04624441300829252\n",
      "Average test loss: 0.002169290780607197\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04607792102628284\n",
      "Average test loss: 0.002145787592459884\n",
      "Epoch 130/300\n",
      "Average training loss: 0.046012318028344046\n",
      "Average test loss: 0.0021947506287445626\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04583872940805223\n",
      "Average test loss: 0.00219921236899164\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04577623116970062\n",
      "Average test loss: 0.0021975048042626845\n",
      "Epoch 133/300\n",
      "Average training loss: 0.045717851274543336\n",
      "Average test loss: 0.0022064182724182806\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04567003044486046\n",
      "Average test loss: 0.0022205446072750623\n",
      "Epoch 135/300\n",
      "Average training loss: 0.045622741447554696\n",
      "Average test loss: 0.002175078605612119\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04537866153650814\n",
      "Average test loss: 0.002288322185476621\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04539578069580926\n",
      "Average test loss: 0.002229265283793211\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04525874561071396\n",
      "Average test loss: 0.005592538174241781\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04518222011129061\n",
      "Average test loss: 0.0021977923847734926\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04519519217477905\n",
      "Average test loss: 0.0022128400202426645\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04508020703991254\n",
      "Average test loss: 0.002184485030464\n",
      "Epoch 142/300\n",
      "Average training loss: 0.044959093670050305\n",
      "Average test loss: 0.002177988080928723\n",
      "Epoch 143/300\n",
      "Average training loss: 0.044820144967900384\n",
      "Average test loss: 0.002158057884209686\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04472445739308993\n",
      "Average test loss: 0.0022088432046067385\n",
      "Epoch 145/300\n",
      "Average training loss: 0.044780010177029504\n",
      "Average test loss: 0.002180545291552941\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04461767852306366\n",
      "Average test loss: 0.0022140203331493667\n",
      "Epoch 147/300\n",
      "Average training loss: 0.044461604522334205\n",
      "Average test loss: 0.0021980102149148782\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04451911955078443\n",
      "Average test loss: 0.0021959435859074195\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04448862603637907\n",
      "Average test loss: 0.0021977776958503658\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04440595729814635\n",
      "Average test loss: 0.002183048551488254\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04424773480494817\n",
      "Average test loss: 0.0022551964606261914\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04418018129799101\n",
      "Average test loss: 0.0021631698894004027\n",
      "Epoch 153/300\n",
      "Average training loss: 0.044126096450620225\n",
      "Average test loss: 0.002767936950756444\n",
      "Epoch 154/300\n",
      "Average training loss: 0.044183478262689375\n",
      "Average test loss: 0.0021934590213414694\n",
      "Epoch 155/300\n",
      "Average training loss: 0.043944472584459514\n",
      "Average test loss: 0.002227381106466055\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04388978871372011\n",
      "Average test loss: 0.0022031168565154074\n",
      "Epoch 157/300\n",
      "Average training loss: 0.043954325642850664\n",
      "Average test loss: 0.0021566006647836832\n",
      "Epoch 158/300\n",
      "Average training loss: 0.043805179923772815\n",
      "Average test loss: 0.002231175227401157\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04378794978062312\n",
      "Average test loss: 0.0021657846100214453\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04370380135046111\n",
      "Average test loss: 0.002246127192551891\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04351709721320205\n",
      "Average test loss: 0.0022750555670095814\n",
      "Epoch 162/300\n",
      "Average training loss: 0.043608820719851386\n",
      "Average test loss: 0.002233527501527634\n",
      "Epoch 163/300\n",
      "Average training loss: 0.043471097128258814\n",
      "Average test loss: 0.0025760057324336633\n",
      "Epoch 164/300\n",
      "Average training loss: 0.043365183480911784\n",
      "Average test loss: 0.0022971617790559927\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0433797709296147\n",
      "Average test loss: 0.004306290862564411\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04337574134104782\n",
      "Average test loss: 0.002246993628020088\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04329256755775875\n",
      "Average test loss: 0.002224177476018667\n",
      "Epoch 168/300\n",
      "Average training loss: 0.043270607706573275\n",
      "Average test loss: 0.002186501316519247\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04308596968650818\n",
      "Average test loss: 0.002247224640308155\n",
      "Epoch 170/300\n",
      "Average training loss: 0.043024180255002445\n",
      "Average test loss: 0.002235452420181698\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04294118238488833\n",
      "Average test loss: 0.002212505406182673\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04297746232151985\n",
      "Average test loss: 0.002192851522213055\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04295734667778015\n",
      "Average test loss: 0.0022417244737346965\n",
      "Epoch 174/300\n",
      "Average training loss: 0.042860711061292224\n",
      "Average test loss: 0.0021648098489062653\n",
      "Epoch 175/300\n",
      "Average training loss: 0.042889042297999065\n",
      "Average test loss: 0.00228127722048925\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04293772663672765\n",
      "Average test loss: 0.0022859573809223045\n",
      "Epoch 177/300\n",
      "Average training loss: 0.042777969658374784\n",
      "Average test loss: 0.0021640866787897214\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04263670572472943\n",
      "Average test loss: 0.002258385609421465\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04261728263232443\n",
      "Average test loss: 0.0022374492560823757\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04257655675543679\n",
      "Average test loss: 0.002276198050007224\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04251605005727874\n",
      "Average test loss: 0.002228175378094117\n",
      "Epoch 182/300\n",
      "Average training loss: 0.042555626276466585\n",
      "Average test loss: 0.002268905998931991\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04260889849397871\n",
      "Average test loss: 0.002222370143358906\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0424111989736557\n",
      "Average test loss: 0.005484620105061266\n",
      "Epoch 185/300\n",
      "Average training loss: 0.042369938413302104\n",
      "Average test loss: 0.00218179048359808\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04227398123012649\n",
      "Average test loss: 0.0022669270775384372\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0422741914457745\n",
      "Average test loss: 0.0027767318612378503\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04222074415948656\n",
      "Average test loss: 0.0022592169801808064\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04215962813297908\n",
      "Average test loss: 0.0022531339169169465\n",
      "Epoch 190/300\n",
      "Average training loss: 0.042215566323863134\n",
      "Average test loss: 0.002181522818075286\n",
      "Epoch 191/300\n",
      "Average training loss: 0.042114269249969055\n",
      "Average test loss: 0.0022195652719173165\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04203509863879946\n",
      "Average test loss: 0.0022441708797381983\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04209676992396514\n",
      "Average test loss: 0.0023261764128175047\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04189732583694988\n",
      "Average test loss: 0.002247391983970172\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04190026906132698\n",
      "Average test loss: 0.0022730906686435144\n",
      "Epoch 196/300\n",
      "Average training loss: 0.041887980318731734\n",
      "Average test loss: 0.0023325460902932616\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04181572574708197\n",
      "Average test loss: 0.002298646104418569\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04179147470990817\n",
      "Average test loss: 0.0062937167690445975\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04168391618794865\n",
      "Average test loss: 0.0022447588122967215\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04171643694572979\n",
      "Average test loss: 0.0022342348305715454\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04166847229335043\n",
      "Average test loss: 0.002264236980428298\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04159881744119856\n",
      "Average test loss: 0.0022661824052532514\n",
      "Epoch 203/300\n",
      "Average training loss: 0.041546609744429586\n",
      "Average test loss: 0.0023095631518711647\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0415073718296157\n",
      "Average test loss: 0.0022014777791789838\n",
      "Epoch 205/300\n",
      "Average training loss: 0.041418315453661814\n",
      "Average test loss: 0.002391286922411786\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04150698732667499\n",
      "Average test loss: 0.0022610175721347334\n",
      "Epoch 207/300\n",
      "Average training loss: 0.041341883344782726\n",
      "Average test loss: 0.002299717992544174\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04130022008220355\n",
      "Average test loss: 0.0023015035361879403\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04132706496781773\n",
      "Average test loss: 0.0022273200766907797\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0414026890380515\n",
      "Average test loss: 0.002252870979822344\n",
      "Epoch 211/300\n",
      "Average training loss: 0.041200592948330776\n",
      "Average test loss: 0.0022237323633291653\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04129588589403364\n",
      "Average test loss: 0.0022912899599307115\n",
      "Epoch 213/300\n",
      "Average training loss: 0.041250729138652485\n",
      "Average test loss: 0.0026817928596089284\n",
      "Epoch 214/300\n",
      "Average training loss: 0.041132693618536\n",
      "Average test loss: 0.002251355480609669\n",
      "Epoch 215/300\n",
      "Average training loss: 0.041130013840066065\n",
      "Average test loss: 0.0022790777074793976\n",
      "Epoch 216/300\n",
      "Average training loss: 0.041180535139309034\n",
      "Average test loss: 0.00245124577337669\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04110026734736231\n",
      "Average test loss: 0.0022338049491453502\n",
      "Epoch 218/300\n",
      "Average training loss: 0.041009196953641046\n",
      "Average test loss: 0.0022245172164920305\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04099826757113139\n",
      "Average test loss: 0.0022426392266319856\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04095798754029804\n",
      "Average test loss: 0.002257920536109143\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04091976984341939\n",
      "Average test loss: 0.0022578189737266965\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04088303148415354\n",
      "Average test loss: 0.0022599630704563524\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04078702082567745\n",
      "Average test loss: 0.0022078399169776176\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04089049991634157\n",
      "Average test loss: 0.002241823500643174\n",
      "Epoch 225/300\n",
      "Average training loss: 0.040751407381561064\n",
      "Average test loss: 0.002338533756426639\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0407204552590847\n",
      "Average test loss: 0.0022856292366567584\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04067982761396302\n",
      "Average test loss: 0.00226431530693339\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04090967316097684\n",
      "Average test loss: 0.0022605512583007414\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04071208596229553\n",
      "Average test loss: 0.0022431515174814397\n",
      "Epoch 230/300\n",
      "Average training loss: 0.040583430690897836\n",
      "Average test loss: 0.002268406339196695\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04058432870441013\n",
      "Average test loss: 0.0022532413318339323\n",
      "Epoch 232/300\n",
      "Average training loss: 0.040510320060782964\n",
      "Average test loss: 0.002273845156240794\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04048119869828224\n",
      "Average test loss: 0.0022628153564615383\n",
      "Epoch 234/300\n",
      "Average training loss: 0.040496781706809995\n",
      "Average test loss: 0.0022862777774118716\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04050959525505702\n",
      "Average test loss: 0.002237977193047603\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04048775324225426\n",
      "Average test loss: 0.004435887145706349\n",
      "Epoch 237/300\n",
      "Average training loss: 0.040436798827515705\n",
      "Average test loss: 0.0022155753124712243\n",
      "Epoch 238/300\n",
      "Average training loss: 0.040381693280405466\n",
      "Average test loss: 0.0022350862303541765\n",
      "Epoch 239/300\n",
      "Average training loss: 0.040365518778562544\n",
      "Average test loss: 0.002214168609637353\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04027026299966706\n",
      "Average test loss: 0.0022491002125251624\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04023229756289058\n",
      "Average test loss: 0.0022298535648733377\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04026284818516837\n",
      "Average test loss: 0.002278743231876029\n",
      "Epoch 243/300\n",
      "Average training loss: 0.040298696822590296\n",
      "Average test loss: 0.0023301026440329022\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04034748992655012\n",
      "Average test loss: 0.002230355173556341\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04020626654889849\n",
      "Average test loss: 0.0023432925066186323\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04010405810011758\n",
      "Average test loss: 0.002230760576410426\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04008131914668613\n",
      "Average test loss: 0.0022703758395380444\n",
      "Epoch 248/300\n",
      "Average training loss: 0.040234087033404246\n",
      "Average test loss: 0.0022646445681651435\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04008526251382298\n",
      "Average test loss: 0.00226413173487203\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04008889024456342\n",
      "Average test loss: 0.0022836005797402725\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04003396442863676\n",
      "Average test loss: 0.002258045949559245\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03999375871154997\n",
      "Average test loss: 0.002277214946121805\n",
      "Epoch 253/300\n",
      "Average training loss: 0.039991027171413104\n",
      "Average test loss: 0.0022778199558249777\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03990459598104159\n",
      "Average test loss: 0.0023098579249862168\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03991286886731783\n",
      "Average test loss: 0.002353783953314026\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03982126126686732\n",
      "Average test loss: 0.002320512582651443\n",
      "Epoch 257/300\n",
      "Average training loss: 0.039849212331904305\n",
      "Average test loss: 0.0023276378744178346\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03984052164024777\n",
      "Average test loss: 0.0022683643274423147\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03983975654178196\n",
      "Average test loss: 0.002333779813307855\n",
      "Epoch 260/300\n",
      "Average training loss: 0.039799774966306156\n",
      "Average test loss: 0.002281155091192987\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03974641115466754\n",
      "Average test loss: 0.0023802391833936173\n",
      "Epoch 262/300\n",
      "Average training loss: 0.039699867033296164\n",
      "Average test loss: 0.002291361813743909\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03961377986603313\n",
      "Average test loss: 0.002252393720257613\n",
      "Epoch 264/300\n",
      "Average training loss: 0.039667474918895294\n",
      "Average test loss: 0.002250719813216064\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03967545931869083\n",
      "Average test loss: 0.0023862442299723624\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03961408507823944\n",
      "Average test loss: 0.0022787015375991663\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03960109307037459\n",
      "Average test loss: 0.0022942942418158053\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03961635414759318\n",
      "Average test loss: 0.002265888474467728\n",
      "Epoch 269/300\n",
      "Average training loss: 0.039518885422084066\n",
      "Average test loss: 0.0023738390314910146\n",
      "Epoch 270/300\n",
      "Average training loss: 0.039578746206230586\n",
      "Average test loss: 0.0023132649480054777\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03951744732922978\n",
      "Average test loss: 0.0023640588503330948\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03944656641615762\n",
      "Average test loss: 0.0023063167581955594\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0394669411347972\n",
      "Average test loss: 0.0023012071117344828\n",
      "Epoch 274/300\n",
      "Average training loss: 0.039530594895283384\n",
      "Average test loss: 0.0022695114177962144\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03939191185765796\n",
      "Average test loss: 0.0027518388784180084\n",
      "Epoch 276/300\n",
      "Average training loss: 0.039402055230405596\n",
      "Average test loss: 0.002293519458216098\n",
      "Epoch 277/300\n",
      "Average training loss: 0.039411073207855224\n",
      "Average test loss: 0.002256975927390158\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03936274160775873\n",
      "Average test loss: 0.0023100799423538976\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0393592667463753\n",
      "Average test loss: 0.002319441221654415\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03927510218156709\n",
      "Average test loss: 0.0022999202298103934\n",
      "Epoch 281/300\n",
      "Average training loss: 0.039290298879146576\n",
      "Average test loss: 0.0023759814517365562\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03930978191230032\n",
      "Average test loss: 0.002320294148599108\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03924692334400283\n",
      "Average test loss: 0.002564822293404076\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03916765503750907\n",
      "Average test loss: 0.0022887156159720487\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03924206217130025\n",
      "Average test loss: 0.002263623068109155\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03924387711452113\n",
      "Average test loss: 0.00237466234424048\n",
      "Epoch 287/300\n",
      "Average training loss: 0.039214027313722505\n",
      "Average test loss: 0.0023432617801138096\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03908125378025903\n",
      "Average test loss: 0.002451212995996078\n",
      "Epoch 289/300\n",
      "Average training loss: 0.039139656431145135\n",
      "Average test loss: 0.0022635441896402173\n",
      "Epoch 290/300\n",
      "Average training loss: 0.039162546447581714\n",
      "Average test loss: 0.002296824957554539\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0391149798217747\n",
      "Average test loss: 0.0024202861682408386\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03895329493946499\n",
      "Average test loss: 0.002280667429996861\n",
      "Epoch 293/300\n",
      "Average training loss: 0.039015262954764894\n",
      "Average test loss: 0.002474286777070827\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03902209117677476\n",
      "Average test loss: 0.0022480870933375423\n",
      "Epoch 295/300\n",
      "Average training loss: 0.039057644188404086\n",
      "Average test loss: 0.0022621520234064924\n",
      "Epoch 296/300\n",
      "Average training loss: 0.038913681616385774\n",
      "Average test loss: 0.0022676414097141887\n",
      "Epoch 297/300\n",
      "Average training loss: 0.038861740983194774\n",
      "Average test loss: 0.0023109290848175685\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03893915262156063\n",
      "Average test loss: 0.002267598663560218\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0388517307639122\n",
      "Average test loss: 0.0022985224939054915\n",
      "Epoch 300/300\n",
      "Average training loss: 0.038865305489963954\n",
      "Average test loss: 0.002313207051509784\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7.256551407708062\n",
      "Average test loss: 0.06135031047463417\n",
      "Epoch 2/300\n",
      "Average training loss: 2.5196656019422745\n",
      "Average test loss: 0.003981031447649002\n",
      "Epoch 3/300\n",
      "Average training loss: 1.6037720416386922\n",
      "Average test loss: 0.002983763193504678\n",
      "Epoch 4/300\n",
      "Average training loss: 1.1522363204956054\n",
      "Average test loss: 0.0027952469314138093\n",
      "Epoch 5/300\n",
      "Average training loss: 0.8699288501739502\n",
      "Average test loss: 0.005040733544776837\n",
      "Epoch 6/300\n",
      "Average training loss: 0.6745581967565748\n",
      "Average test loss: 0.07054523259080532\n",
      "Epoch 7/300\n",
      "Average training loss: 0.5409783239364624\n",
      "Average test loss: 0.0024339325581159855\n",
      "Epoch 8/300\n",
      "Average training loss: 0.4405302050113678\n",
      "Average test loss: 0.0023906400928066836\n",
      "Epoch 9/300\n",
      "Average training loss: 0.36776229808065625\n",
      "Average test loss: 0.0032931651735885275\n",
      "Epoch 10/300\n",
      "Average training loss: 0.3092910576396518\n",
      "Average test loss: 0.0021677887394196457\n",
      "Epoch 11/300\n",
      "Average training loss: 0.2621732357078128\n",
      "Average test loss: 0.0021823973552220397\n",
      "Epoch 12/300\n",
      "Average training loss: 0.22420745299922096\n",
      "Average test loss: 0.002108180173155334\n",
      "Epoch 13/300\n",
      "Average training loss: 0.19510072810120052\n",
      "Average test loss: 0.0019934574063453408\n",
      "Epoch 14/300\n",
      "Average training loss: 0.171346732934316\n",
      "Average test loss: 4.316998079578082\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1514105068710115\n",
      "Average test loss: 0.0018496690893338786\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13570153248310088\n",
      "Average test loss: 0.002083184864268535\n",
      "Epoch 17/300\n",
      "Average training loss: 0.12240441879298952\n",
      "Average test loss: 0.001740886976218058\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11151835542917252\n",
      "Average test loss: 0.0017638762179348203\n",
      "Epoch 19/300\n",
      "Average training loss: 0.10255369098318948\n",
      "Average test loss: 0.001731004064799183\n",
      "Epoch 20/300\n",
      "Average training loss: 0.09501703562339146\n",
      "Average test loss: 0.0030368272085777586\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08880548848708471\n",
      "Average test loss: 0.0016428886279463769\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0834776168929206\n",
      "Average test loss: 0.001628398176903526\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0789110625717375\n",
      "Average test loss: 0.0016365215279575851\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07497774420844185\n",
      "Average test loss: 0.0015837779544914763\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07193708500928349\n",
      "Average test loss: 0.0015751431499504381\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06948285252518124\n",
      "Average test loss: 0.0015753753704743252\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06673089193966654\n",
      "Average test loss: 0.0015495918560255732\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06463437072436015\n",
      "Average test loss: 0.0015150691299802728\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06256147998571396\n",
      "Average test loss: 0.001524915308588081\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06071584757169088\n",
      "Average test loss: 0.0015051211625751522\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05918052831954426\n",
      "Average test loss: 0.001568168714746005\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05774434284369151\n",
      "Average test loss: 0.0015107745365757082\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05651233120759328\n",
      "Average test loss: 0.0014740871826393737\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05535403624508116\n",
      "Average test loss: 0.0014466085945669975\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05431486470500628\n",
      "Average test loss: 0.0014643054086094101\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0532939610713058\n",
      "Average test loss: 0.0014408198572281334\n",
      "Epoch 37/300\n",
      "Average training loss: 0.052437092555893794\n",
      "Average test loss: 0.0014372247158446246\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05193020720945464\n",
      "Average test loss: 0.0014585187246816025\n",
      "Epoch 39/300\n",
      "Average training loss: 0.050867635253402925\n",
      "Average test loss: 0.00143032371180339\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05012304659022225\n",
      "Average test loss: 0.001448179934691224\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0495410351637337\n",
      "Average test loss: 0.0014227144035200278\n",
      "Epoch 42/300\n",
      "Average training loss: 0.048955260710583794\n",
      "Average test loss: 0.0014899668416215315\n",
      "Epoch 43/300\n",
      "Average training loss: 0.048292186594671674\n",
      "Average test loss: 0.0014223399494464198\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04773066198329131\n",
      "Average test loss: 0.0013989430598707663\n",
      "Epoch 45/300\n",
      "Average training loss: 0.047255466166469785\n",
      "Average test loss: 0.0013979332176968454\n",
      "Epoch 46/300\n",
      "Average training loss: 0.046790438948406114\n",
      "Average test loss: 0.0014460840285238292\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04628741819328732\n",
      "Average test loss: 0.0014005545578483077\n",
      "Epoch 48/300\n",
      "Average training loss: 0.045850769913858835\n",
      "Average test loss: 0.0013804841159532468\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04523297534386317\n",
      "Average test loss: 0.0013895948990765546\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0450333291888237\n",
      "Average test loss: 0.0013961644193364515\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04453557137648265\n",
      "Average test loss: 0.0013766781237597267\n",
      "Epoch 52/300\n",
      "Average training loss: 0.044313160866498946\n",
      "Average test loss: 0.0014204887134385191\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04385354792409473\n",
      "Average test loss: 0.0014003633218300012\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04352887604965104\n",
      "Average test loss: 0.001385984471688668\n",
      "Epoch 55/300\n",
      "Average training loss: 0.043110204686721165\n",
      "Average test loss: 0.0014093851100963851\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04280857189827495\n",
      "Average test loss: 0.0013945767869138056\n",
      "Epoch 57/300\n",
      "Average training loss: 0.042579713794920176\n",
      "Average test loss: 0.001473036414012313\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04218673049244616\n",
      "Average test loss: 0.0013972586790720622\n",
      "Epoch 59/300\n",
      "Average training loss: 0.041892135745949216\n",
      "Average test loss: 0.001373927250970155\n",
      "Epoch 60/300\n",
      "Average training loss: 0.041701150291495855\n",
      "Average test loss: 0.0013618216442668604\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04138867293463813\n",
      "Average test loss: 0.0013868724631352558\n",
      "Epoch 62/300\n",
      "Average training loss: 0.041055053111579684\n",
      "Average test loss: 0.0013892961583203739\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04079048236873415\n",
      "Average test loss: 0.0013774661917446388\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04060843681957987\n",
      "Average test loss: 0.0013701732348029812\n",
      "Epoch 65/300\n",
      "Average training loss: 0.040198011972837976\n",
      "Average test loss: 0.001401326380049189\n",
      "Epoch 66/300\n",
      "Average training loss: 0.039955741324358515\n",
      "Average test loss: 0.0014025170992438992\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03974102096756299\n",
      "Average test loss: 0.0013867036126967932\n",
      "Epoch 68/300\n",
      "Average training loss: 0.039969681915309696\n",
      "Average test loss: 0.001403374758031633\n",
      "Epoch 69/300\n",
      "Average training loss: 0.039257778624693554\n",
      "Average test loss: 0.001383940741730233\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03885371799601449\n",
      "Average test loss: 0.0013837042198412948\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03869620584944884\n",
      "Average test loss: 0.0013977420100321372\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03841052874260478\n",
      "Average test loss: 0.0013830131600714392\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03819380122092035\n",
      "Average test loss: 0.0014043084715182582\n",
      "Epoch 74/300\n",
      "Average training loss: 0.038105189912849\n",
      "Average test loss: 0.0013923108887134326\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03772494494915009\n",
      "Average test loss: 0.0013988509761273032\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03752852605779966\n",
      "Average test loss: 0.0014287791399595637\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03750364743007554\n",
      "Average test loss: 0.001396348325949576\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03700070919758744\n",
      "Average test loss: 0.0014345979845772187\n",
      "Epoch 79/300\n",
      "Average training loss: 0.036883357961972556\n",
      "Average test loss: 0.0014185218525429566\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03670441189905008\n",
      "Average test loss: 0.001464231442246172\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0367847957611084\n",
      "Average test loss: 0.0013987219858811134\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03645586500896348\n",
      "Average test loss: 0.0014202131712809205\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03628131444089942\n",
      "Average test loss: 0.0014305814318358897\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0359914087984297\n",
      "Average test loss: 0.0014295335689352617\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03574776713715659\n",
      "Average test loss: 0.0014030768001038168\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03561433638963434\n",
      "Average test loss: 0.001432142801893254\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03538480309645335\n",
      "Average test loss: 0.0014161215031312571\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03527351925273736\n",
      "Average test loss: 0.001499870819143123\n",
      "Epoch 89/300\n",
      "Average training loss: 0.035162445768713954\n",
      "Average test loss: 0.0014422766342759132\n",
      "Epoch 90/300\n",
      "Average training loss: 0.034890939123100706\n",
      "Average test loss: 0.0014938650067067808\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03477605852484703\n",
      "Average test loss: 0.0014780110720441573\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03464946582251125\n",
      "Average test loss: 0.0014507811835760044\n",
      "Epoch 93/300\n",
      "Average training loss: 0.034501135928763284\n",
      "Average test loss: 0.0014372436485977635\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03433336044020123\n",
      "Average test loss: 0.0017730807889666821\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03426711397866408\n",
      "Average test loss: 0.0014199661287582583\n",
      "Epoch 96/300\n",
      "Average training loss: 0.034018432444996305\n",
      "Average test loss: 0.0015218587045868238\n",
      "Epoch 97/300\n",
      "Average training loss: 0.033833708624045056\n",
      "Average test loss: 0.0014474072034160296\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03378514018489255\n",
      "Average test loss: 0.0014710034880166252\n",
      "Epoch 99/300\n",
      "Average training loss: 0.033656727814012105\n",
      "Average test loss: 0.0017946001233326064\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03353546612792545\n",
      "Average test loss: 0.0014400620241132047\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03345953589677811\n",
      "Average test loss: 0.0014845499323370556\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03334520868791474\n",
      "Average test loss: 0.001482731249390377\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0331312271853288\n",
      "Average test loss: 0.0014726883878724442\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03306180855962965\n",
      "Average test loss: 0.00147241705785402\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0329287594176001\n",
      "Average test loss: 0.0017538910436754424\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03285983394583066\n",
      "Average test loss: 0.0014477660355882512\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03277092508474986\n",
      "Average test loss: 0.0014583429251280096\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03263838746315903\n",
      "Average test loss: 0.0014991190144792198\n",
      "Epoch 109/300\n",
      "Average training loss: 0.032848129254248404\n",
      "Average test loss: 0.0015053364370639125\n",
      "Epoch 110/300\n",
      "Average training loss: 0.032495551975237\n",
      "Average test loss: 0.001487817395064566\n",
      "Epoch 111/300\n",
      "Average training loss: 0.032326696642571026\n",
      "Average test loss: 0.0015337099896536933\n",
      "Epoch 112/300\n",
      "Average training loss: 0.032284070291452936\n",
      "Average test loss: 0.0014863272479010952\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03205778387520048\n",
      "Average test loss: 0.0014859961386149128\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03202282532387309\n",
      "Average test loss: 0.0014546088134456013\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0320012005319198\n",
      "Average test loss: 0.0014662163119970096\n",
      "Epoch 116/300\n",
      "Average training loss: 0.031944325652387404\n",
      "Average test loss: 0.0014814696866605017\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03173631413446532\n",
      "Average test loss: 0.0020541036433229846\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03169435041149457\n",
      "Average test loss: 0.0014925019598255554\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03159659238656362\n",
      "Average test loss: 0.00152491678049167\n",
      "Epoch 120/300\n",
      "Average training loss: 0.031525146861871085\n",
      "Average test loss: 0.0014909866498783232\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03144992159472571\n",
      "Average test loss: 0.0015739189444316758\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03140290283328957\n",
      "Average test loss: 0.0015686936066485941\n",
      "Epoch 123/300\n",
      "Average training loss: 0.031534411966800686\n",
      "Average test loss: 0.0016792692395134106\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03124047338300281\n",
      "Average test loss: 0.0018175999511861139\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03105308067633046\n",
      "Average test loss: 0.0015064971387489803\n",
      "Epoch 126/300\n",
      "Average training loss: 0.030977234547336897\n",
      "Average test loss: 0.0015051188330269523\n",
      "Epoch 127/300\n",
      "Average training loss: 0.031032195367746884\n",
      "Average test loss: 0.0014794398448947403\n",
      "Epoch 128/300\n",
      "Average training loss: 0.030959437850448822\n",
      "Average test loss: 0.0015198365294684967\n",
      "Epoch 129/300\n",
      "Average training loss: 0.030955034350355466\n",
      "Average test loss: 0.0015271480406324068\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03083505209452576\n",
      "Average test loss: 0.001561110394075513\n",
      "Epoch 131/300\n",
      "Average training loss: 0.030974599204129642\n",
      "Average test loss: 0.0015272101926513844\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03066929777794414\n",
      "Average test loss: 0.001479422314920359\n",
      "Epoch 133/300\n",
      "Average training loss: 0.030491836748189397\n",
      "Average test loss: 0.0016737181725394394\n",
      "Epoch 134/300\n",
      "Average training loss: 0.030529855224821302\n",
      "Average test loss: 0.0017268356586702996\n",
      "Epoch 135/300\n",
      "Average training loss: 0.030496429880460103\n",
      "Average test loss: 0.0015472467297998568\n",
      "Epoch 136/300\n",
      "Average training loss: 0.030402324418226878\n",
      "Average test loss: 0.001552408293924398\n",
      "Epoch 137/300\n",
      "Average training loss: 0.030339785253008206\n",
      "Average test loss: 0.001509963674781223\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03030767251882288\n",
      "Average test loss: 0.00155010163018273\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03027000290983253\n",
      "Average test loss: 0.0015175073138541646\n",
      "Epoch 140/300\n",
      "Average training loss: 0.030143494789799056\n",
      "Average test loss: 0.0015862640703303947\n",
      "Epoch 141/300\n",
      "Average training loss: 0.030138565674424172\n",
      "Average test loss: 0.001517421755525801\n",
      "Epoch 142/300\n",
      "Average training loss: 0.030005423789223034\n",
      "Average test loss: 0.001563867445000344\n",
      "Epoch 143/300\n",
      "Average training loss: 0.029979959492882093\n",
      "Average test loss: 0.001501916053549697\n",
      "Epoch 144/300\n",
      "Average training loss: 0.030015389401051733\n",
      "Average test loss: 0.0015059229945763946\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02995404251747661\n",
      "Average test loss: 0.0015223917336099678\n",
      "Epoch 146/300\n",
      "Average training loss: 0.029922750855485597\n",
      "Average test loss: 0.001751241275212831\n",
      "Epoch 147/300\n",
      "Average training loss: 0.029784767460491923\n",
      "Average test loss: 0.0015099829704397255\n",
      "Epoch 148/300\n",
      "Average training loss: 0.029831529981560176\n",
      "Average test loss: 0.001550707201162974\n",
      "Epoch 149/300\n",
      "Average training loss: 0.029662972112496695\n",
      "Average test loss: 0.0017410184771029485\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02967859838406245\n",
      "Average test loss: 0.001496473782385389\n",
      "Epoch 151/300\n",
      "Average training loss: 0.029567790215214093\n",
      "Average test loss: 0.0015594081910740999\n",
      "Epoch 152/300\n",
      "Average training loss: 0.029546612908442814\n",
      "Average test loss: 0.001494897458702326\n",
      "Epoch 153/300\n",
      "Average training loss: 0.029507916649182637\n",
      "Average test loss: 0.0015095719970348808\n",
      "Epoch 154/300\n",
      "Average training loss: 0.029633951963649856\n",
      "Average test loss: 0.001556010907722844\n",
      "Epoch 155/300\n",
      "Average training loss: 0.029414474268754324\n",
      "Average test loss: 0.0015642541605565283\n",
      "Epoch 156/300\n",
      "Average training loss: 0.029374497193429206\n",
      "Average test loss: 0.0015169735954453547\n",
      "Epoch 157/300\n",
      "Average training loss: 0.029357852649357585\n",
      "Average test loss: 0.0016804419350292948\n",
      "Epoch 158/300\n",
      "Average training loss: 0.029387772343224948\n",
      "Average test loss: 0.0021775700542040997\n",
      "Epoch 159/300\n",
      "Average training loss: 0.029288973463906184\n",
      "Average test loss: 0.0016236079760516683\n",
      "Epoch 160/300\n",
      "Average training loss: 0.029256920733385615\n",
      "Average test loss: 0.0022034896108218366\n",
      "Epoch 161/300\n",
      "Average training loss: 0.029083477564983896\n",
      "Average test loss: 0.0015452688443991873\n",
      "Epoch 162/300\n",
      "Average training loss: 0.029050581127405165\n",
      "Average test loss: 0.0015266278268665904\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029110386807057593\n",
      "Average test loss: 0.0015146709649513164\n",
      "Epoch 164/300\n",
      "Average training loss: 0.028978424398435485\n",
      "Average test loss: 0.0015945863048028614\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02901162813603878\n",
      "Average test loss: 0.0017208219005001915\n",
      "Epoch 166/300\n",
      "Average training loss: 0.028934700393014483\n",
      "Average test loss: 0.00157217961942984\n",
      "Epoch 167/300\n",
      "Average training loss: 0.028891967280043496\n",
      "Average test loss: 0.0015609273048531678\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02889040323595206\n",
      "Average test loss: 0.0015449401958742076\n",
      "Epoch 169/300\n",
      "Average training loss: 0.028888378747635418\n",
      "Average test loss: 0.0015659696037570636\n",
      "Epoch 170/300\n",
      "Average training loss: 0.028862203309933346\n",
      "Average test loss: 0.0015363986014078062\n",
      "Epoch 171/300\n",
      "Average training loss: 0.028736131638288497\n",
      "Average test loss: 0.0015808593820159633\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02866413956715001\n",
      "Average test loss: 0.0015585968132234283\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02871031980878777\n",
      "Average test loss: 0.0015305392222685946\n",
      "Epoch 174/300\n",
      "Average training loss: 0.028672438222501012\n",
      "Average test loss: 0.0015584412620713313\n",
      "Epoch 175/300\n",
      "Average training loss: 0.028599234180317984\n",
      "Average test loss: 0.0015586587545565432\n",
      "Epoch 176/300\n",
      "Average training loss: 0.028562101268106037\n",
      "Average test loss: 0.001557751494149367\n",
      "Epoch 177/300\n",
      "Average training loss: 0.028650361996557978\n",
      "Average test loss: 0.0015979977500521474\n",
      "Epoch 178/300\n",
      "Average training loss: 0.028487679784496626\n",
      "Average test loss: 0.0015333275670806566\n",
      "Epoch 179/300\n",
      "Average training loss: 0.028473702116145028\n",
      "Average test loss: 0.0015831696660154396\n",
      "Epoch 180/300\n",
      "Average training loss: 0.028451124207841027\n",
      "Average test loss: 0.0015411584640128745\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02841839801768462\n",
      "Average test loss: 0.0015308587565604184\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02834035483499368\n",
      "Average test loss: 0.0015296237051693929\n",
      "Epoch 183/300\n",
      "Average training loss: 0.028346172331107988\n",
      "Average test loss: 0.0036215754751529956\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0282753524184227\n",
      "Average test loss: 0.0015423395402936473\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0283095161318779\n",
      "Average test loss: 0.001599804949015379\n",
      "Epoch 186/300\n",
      "Average training loss: 0.028373976475662654\n",
      "Average test loss: 0.001622498984552092\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02834791518085533\n",
      "Average test loss: 0.0017958893862863382\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02813867164320416\n",
      "Average test loss: 0.0015857624265675743\n",
      "Epoch 189/300\n",
      "Average training loss: 0.028050450103150475\n",
      "Average test loss: 0.0016635793601680134\n",
      "Epoch 190/300\n",
      "Average training loss: 0.028079899994863405\n",
      "Average test loss: 0.0015838409900768764\n",
      "Epoch 191/300\n",
      "Average training loss: 0.028047218185332085\n",
      "Average test loss: 0.0015616082201401393\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02805933934946855\n",
      "Average test loss: 0.0016325594391673804\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02812572873632113\n",
      "Average test loss: 0.0016137973907300168\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02799566567937533\n",
      "Average test loss: 0.0016423591325680415\n",
      "Epoch 195/300\n",
      "Average training loss: 0.027923584467834897\n",
      "Average test loss: 0.0015773595936285952\n",
      "Epoch 196/300\n",
      "Average training loss: 0.027925996778739824\n",
      "Average test loss: 0.001597958192229271\n",
      "Epoch 197/300\n",
      "Average training loss: 0.027939049462477366\n",
      "Average test loss: 0.0015402999244009455\n",
      "Epoch 198/300\n",
      "Average training loss: 0.027900302744574015\n",
      "Average test loss: 0.0015653638488406108\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02792619484331873\n",
      "Average test loss: 0.0016051957486197352\n",
      "Epoch 200/300\n",
      "Average training loss: 0.027857140820887354\n",
      "Average test loss: 0.0015900231432169677\n",
      "Epoch 201/300\n",
      "Average training loss: 0.027785502311256198\n",
      "Average test loss: 0.0020344955570374924\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02782839686340756\n",
      "Average test loss: 0.028211144795744783\n",
      "Epoch 203/300\n",
      "Average training loss: 0.027702907499339846\n",
      "Average test loss: 0.0015919132766106891\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0277002231809828\n",
      "Average test loss: 0.0015672489184265336\n",
      "Epoch 205/300\n",
      "Average training loss: 0.027803147218293613\n",
      "Average test loss: 0.001581965779575209\n",
      "Epoch 206/300\n",
      "Average training loss: 0.027680005921257866\n",
      "Average test loss: 0.0016435964118378858\n",
      "Epoch 207/300\n",
      "Average training loss: 0.027604048295153512\n",
      "Average test loss: 0.0015854426903857124\n",
      "Epoch 208/300\n",
      "Average training loss: 0.027609707176685334\n",
      "Average test loss: 0.0015523977472136417\n",
      "Epoch 209/300\n",
      "Average training loss: 0.027659176920851073\n",
      "Average test loss: 0.0015626488572193517\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02754122175441848\n",
      "Average test loss: 0.0016179008973348473\n",
      "Epoch 211/300\n",
      "Average training loss: 0.027501961254411275\n",
      "Average test loss: 0.0015818387769783537\n",
      "Epoch 212/300\n",
      "Average training loss: 0.027526602054635682\n",
      "Average test loss: 0.0016047349615643423\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02748237593803141\n",
      "Average test loss: 0.002015048514637682\n",
      "Epoch 214/300\n",
      "Average training loss: 0.027480722485317126\n",
      "Average test loss: 0.0015407044890647134\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02742468102938599\n",
      "Average test loss: 0.001554120126335571\n",
      "Epoch 216/300\n",
      "Average training loss: 0.027407105651166705\n",
      "Average test loss: 0.0015568259179385173\n",
      "Epoch 217/300\n",
      "Average training loss: 0.027416952047083113\n",
      "Average test loss: 0.001586888329177681\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02735346493124962\n",
      "Average test loss: 0.0015773928082651561\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0273303366187546\n",
      "Average test loss: 0.0015851392133368386\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02730327570769522\n",
      "Average test loss: 0.001702667146300276\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02727803520858288\n",
      "Average test loss: 0.001630556101186408\n",
      "Epoch 222/300\n",
      "Average training loss: 0.027291288417246606\n",
      "Average test loss: 0.0015865002919195426\n",
      "Epoch 223/300\n",
      "Average training loss: 0.027281207695603372\n",
      "Average test loss: 0.0016581947506508893\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02726451936364174\n",
      "Average test loss: 0.0015709697815279166\n",
      "Epoch 225/300\n",
      "Average training loss: 0.027180553820398117\n",
      "Average test loss: 0.0016063407992737162\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02714902342028088\n",
      "Average test loss: 0.001615387437451217\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02717250443001588\n",
      "Average test loss: 0.0015682585830282833\n",
      "Epoch 228/300\n",
      "Average training loss: 0.027134672921564844\n",
      "Average test loss: 0.0015731231920007203\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02717489839759138\n",
      "Average test loss: 0.0016087189339515236\n",
      "Epoch 230/300\n",
      "Average training loss: 0.027122010499238968\n",
      "Average test loss: 0.0015835070620394416\n",
      "Epoch 231/300\n",
      "Average training loss: 0.027075010960300763\n",
      "Average test loss: 0.00160266481164015\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02709099356002278\n",
      "Average test loss: 0.0015980643006041646\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0270023126370377\n",
      "Average test loss: 0.0015661683585494756\n",
      "Epoch 234/300\n",
      "Average training loss: 0.027002899050712584\n",
      "Average test loss: 0.0016040018686196871\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02707786745329698\n",
      "Average test loss: 0.0015778217897233035\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02702319850358698\n",
      "Average test loss: 0.0015808678292151955\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02690511027806335\n",
      "Average test loss: 0.0015905400427679221\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026948721549577184\n",
      "Average test loss: 0.001620472111635738\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026936314794752333\n",
      "Average test loss: 0.001645022249677115\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0268655627336767\n",
      "Average test loss: 0.001588335517173012\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02682025013367335\n",
      "Average test loss: 0.0016269930731505156\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026818479457663164\n",
      "Average test loss: 0.0016004948756761022\n",
      "Epoch 243/300\n",
      "Average training loss: 0.026886715173721314\n",
      "Average test loss: 0.0016504420993021793\n",
      "Epoch 244/300\n",
      "Average training loss: 0.026766616700424087\n",
      "Average test loss: 0.001614342422845463\n",
      "Epoch 245/300\n",
      "Average training loss: 0.026789608447088137\n",
      "Average test loss: 0.002018150468305167\n",
      "Epoch 246/300\n",
      "Average training loss: 0.026760585566361746\n",
      "Average test loss: 0.0016295031381564008\n",
      "Epoch 247/300\n",
      "Average training loss: 0.026787521253029505\n",
      "Average test loss: 0.0015728375010399356\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026811879144774543\n",
      "Average test loss: 0.0016121183066732353\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026675439006752438\n",
      "Average test loss: 0.001689369489542312\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026691465212239158\n",
      "Average test loss: 0.0016018175707819562\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02662097930411498\n",
      "Average test loss: 0.0016110706875721614\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0266484685142835\n",
      "Average test loss: 0.0015914799689004818\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02664695641067293\n",
      "Average test loss: 0.0016444720727288062\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026605824819869464\n",
      "Average test loss: 0.0015848541216303906\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02658527318471008\n",
      "Average test loss: 0.001604305624961853\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026629602346155377\n",
      "Average test loss: 0.0016702968797956904\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02658085553182496\n",
      "Average test loss: 0.0015955607298140724\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026547262471583153\n",
      "Average test loss: 0.001601716642992364\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026521126948297023\n",
      "Average test loss: 0.0016066763325490886\n",
      "Epoch 260/300\n",
      "Average training loss: 0.026548779234290124\n",
      "Average test loss: 0.001665541347116232\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026451933802829848\n",
      "Average test loss: 0.0015799456369131803\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02655797874596384\n",
      "Average test loss: 0.0015965989070634047\n",
      "Epoch 263/300\n",
      "Average training loss: 0.026556620079610082\n",
      "Average test loss: 0.0015928392131916351\n",
      "Epoch 264/300\n",
      "Average training loss: 0.026454673025343153\n",
      "Average test loss: 0.0015527613491026892\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02639620405104425\n",
      "Average test loss: 0.0016375589323126608\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02643496094312933\n",
      "Average test loss: 0.001622018654934234\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026438354166017638\n",
      "Average test loss: 0.001587259763955242\n",
      "Epoch 268/300\n",
      "Average training loss: 0.026374759818116825\n",
      "Average test loss: 0.0015702873253160053\n",
      "Epoch 269/300\n",
      "Average training loss: 0.026386086399356523\n",
      "Average test loss: 0.001643642689515319\n",
      "Epoch 270/300\n",
      "Average training loss: 0.026401460629370477\n",
      "Average test loss: 0.0016249763009448847\n",
      "Epoch 271/300\n",
      "Average training loss: 0.026294253973497286\n",
      "Average test loss: 0.0015856371186673641\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0262519267267651\n",
      "Average test loss: 0.001600653567351401\n",
      "Epoch 273/300\n",
      "Average training loss: 0.026370201165477433\n",
      "Average test loss: 0.001702294297826787\n",
      "Epoch 274/300\n",
      "Average training loss: 0.026338170263502332\n",
      "Average test loss: 0.0016798203935225805\n",
      "Epoch 275/300\n",
      "Average training loss: 0.026251261176334487\n",
      "Average test loss: 0.0018027958804741502\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02618175991045104\n",
      "Average test loss: 0.001608145354419119\n",
      "Epoch 277/300\n",
      "Average training loss: 0.026245381327139005\n",
      "Average test loss: 0.0015747142118505307\n",
      "Epoch 278/300\n",
      "Average training loss: 0.026257213728295432\n",
      "Average test loss: 0.0015873470247412722\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02622810731497076\n",
      "Average test loss: 0.001556635873237004\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026217270423968632\n",
      "Average test loss: 0.0016280134444435438\n",
      "Epoch 281/300\n",
      "Average training loss: 0.026161599561572074\n",
      "Average test loss: 0.0016647487851894564\n",
      "Epoch 282/300\n",
      "Average training loss: 0.026113120194938447\n",
      "Average test loss: 0.0033053851284914545\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02616464357574781\n",
      "Average test loss: 0.0015768717020336125\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026106588747766283\n",
      "Average test loss: 0.001593278012238443\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02617489962114228\n",
      "Average test loss: 0.0016046422763417165\n",
      "Epoch 286/300\n",
      "Average training loss: 0.026152841717004775\n",
      "Average test loss: 0.001618930169277721\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026043596762749883\n",
      "Average test loss: 0.00158468954037461\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02602414571907785\n",
      "Average test loss: 0.0016146428171648747\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02607683851321538\n",
      "Average test loss: 0.0016298641856345864\n",
      "Epoch 290/300\n",
      "Average training loss: 0.026044215510288873\n",
      "Average test loss: 0.0016197369124533402\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02613942163851526\n",
      "Average test loss: 0.0015828062288670076\n",
      "Epoch 292/300\n",
      "Average training loss: 0.026005800974037912\n",
      "Average test loss: 0.001585154392446081\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02593656575348642\n",
      "Average test loss: 0.001619122892100778\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02600328969127602\n",
      "Average test loss: 0.0015943261299075351\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025926906233032544\n",
      "Average test loss: 0.0016285910500834385\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025989555473128954\n",
      "Average test loss: 0.001553725162624485\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025877864284647837\n",
      "Average test loss: 0.0016102603736023108\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025923523548576567\n",
      "Average test loss: 0.0016878034628720747\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02593694390025404\n",
      "Average test loss: 0.001616549215072559\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025913025707006456\n",
      "Average test loss: 0.0015680639094983538\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.35\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.69\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.38\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.14\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.06\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.19\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.39\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.48\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.48\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.38\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.70\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.04\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.52\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.56\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.46\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.36\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.62\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.74\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.82\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.91\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.01\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.19\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.74\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.02\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.08\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.11\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.39\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.30\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 33.26\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 33.12\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.30\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.25\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.47\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.38\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.52\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.62\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.64\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.85\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.76\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.94\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 34.03\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.03\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
