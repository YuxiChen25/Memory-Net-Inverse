{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.ImageDataset import ImageDataset\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import SkippedLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Skip Weighting\n",
    "loss_function = SkippedLayerLoss(skip = 5)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.03004941754043102\n",
      "Average test loss: 0.015506189295815096\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011308604708148373\n",
      "Average test loss: 0.010224835507157776\n",
      "Epoch 3/300\n",
      "Average training loss: 0.010054465064571963\n",
      "Average test loss: 0.011815867433117496\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00931713602029615\n",
      "Average test loss: 0.008908786571688122\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008869236811995506\n",
      "Average test loss: 0.009414727095928457\n",
      "Epoch 6/300\n",
      "Average training loss: 0.008534770578559902\n",
      "Average test loss: 0.008704908182223638\n",
      "Epoch 7/300\n",
      "Average training loss: 0.008145656006203757\n",
      "Average test loss: 0.008196017331547208\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007975266017019748\n",
      "Average test loss: 0.008011694773617718\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007722648240625858\n",
      "Average test loss: 0.007921565967301527\n",
      "Epoch 10/300\n",
      "Average training loss: 0.00755465510321988\n",
      "Average test loss: 0.0076764106841550935\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007386086344718933\n",
      "Average test loss: 0.0074515784084796905\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007218892902963691\n",
      "Average test loss: 0.007306126983629333\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007042107319252359\n",
      "Average test loss: 0.007181568483925528\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006916957241379552\n",
      "Average test loss: 0.007163263576312197\n",
      "Epoch 15/300\n",
      "Average training loss: 0.006771586065904962\n",
      "Average test loss: 0.007361106669737233\n",
      "Epoch 16/300\n",
      "Average training loss: 0.006670056212279531\n",
      "Average test loss: 0.007451428020166026\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00658042828241984\n",
      "Average test loss: 0.006882843833002779\n",
      "Epoch 18/300\n",
      "Average training loss: 0.006483848696367608\n",
      "Average test loss: 0.006810830596834421\n",
      "Epoch 19/300\n",
      "Average training loss: 0.006397575212021668\n",
      "Average test loss: 0.006653240166604519\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006335134443723493\n",
      "Average test loss: 0.007306215612838666\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00626809574249718\n",
      "Average test loss: 0.0065652584239012666\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0062066526143915125\n",
      "Average test loss: 0.006567495003756549\n",
      "Epoch 23/300\n",
      "Average training loss: 0.006161962969849507\n",
      "Average test loss: 0.006599302804304494\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006082795647697316\n",
      "Average test loss: 0.006455633894436889\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00604096142657929\n",
      "Average test loss: 0.0066754743680357935\n",
      "Epoch 26/300\n",
      "Average training loss: 0.005983999600427018\n",
      "Average test loss: 0.0064696537521150375\n",
      "Epoch 27/300\n",
      "Average training loss: 0.005962133996188641\n",
      "Average test loss: 0.0064327058957682716\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0058942506859699885\n",
      "Average test loss: 0.006434743306703038\n",
      "Epoch 29/300\n",
      "Average training loss: 0.005853709132307106\n",
      "Average test loss: 0.006669450571139653\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005832550668881999\n",
      "Average test loss: 0.0065564426100916335\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0057700531490974954\n",
      "Average test loss: 0.006439430233505037\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005739067247344388\n",
      "Average test loss: 0.00639496807836824\n",
      "Epoch 33/300\n",
      "Average training loss: 0.005695574793136782\n",
      "Average test loss: 0.006389843257764975\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005661489900201559\n",
      "Average test loss: 0.006629749844885535\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0056332252733409405\n",
      "Average test loss: 0.00668087049242523\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005608053975221183\n",
      "Average test loss: 0.006649765556057294\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005555092170834541\n",
      "Average test loss: 0.006485507090058591\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005530598762134711\n",
      "Average test loss: 0.0063880639817151755\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0055065405699941845\n",
      "Average test loss: 0.0065899859542648\n",
      "Epoch 40/300\n",
      "Average training loss: 0.005469887759950426\n",
      "Average test loss: 0.006522744459410508\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0054437028256555395\n",
      "Average test loss: 0.00637429772482978\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005416896989982989\n",
      "Average test loss: 0.006506932326489025\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005402086481038067\n",
      "Average test loss: 0.006302660153143936\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005366481362531582\n",
      "Average test loss: 0.0063823974670635325\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0053448426201939585\n",
      "Average test loss: 0.0064328382064898805\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005312798332836893\n",
      "Average test loss: 0.006567685067653656\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0053253881169690025\n",
      "Average test loss: 0.0068348012562427255\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005269454708529843\n",
      "Average test loss: 0.006385670636263159\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0052333822378681765\n",
      "Average test loss: 0.006446906339791086\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005233478793253502\n",
      "Average test loss: 0.0065548218049936826\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005226997293531895\n",
      "Average test loss: 0.006428049253506793\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005184086392737097\n",
      "Average test loss: 0.006594831580917041\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0051708500859224136\n",
      "Average test loss: 0.0063809164725244045\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005143215090864234\n",
      "Average test loss: 0.00644931769495209\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005126321196970013\n",
      "Average test loss: 0.006717849400308397\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005126991785234875\n",
      "Average test loss: 0.0067106570638716225\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005094540462311771\n",
      "Average test loss: 0.006373716895778974\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005074833308243089\n",
      "Average test loss: 0.006557965560712748\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0050583014115691185\n",
      "Average test loss: 0.006653769900815354\n",
      "Epoch 60/300\n",
      "Average training loss: 0.005049517006095913\n",
      "Average test loss: 0.006520318989952405\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005032867079600691\n",
      "Average test loss: 0.006365901176714235\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005018878114306264\n",
      "Average test loss: 0.0076940902057621215\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005014442966837022\n",
      "Average test loss: 0.006336203620251682\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004982605800032616\n",
      "Average test loss: 0.006290769450780418\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004986945692035887\n",
      "Average test loss: 0.006661167158848709\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004962065347780784\n",
      "Average test loss: 0.00646567734496461\n",
      "Epoch 67/300\n",
      "Average training loss: 0.004940600816574362\n",
      "Average test loss: 0.0064299322234259715\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004929428546792931\n",
      "Average test loss: 0.006683281364540259\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004920472602463431\n",
      "Average test loss: 0.0064089390515453286\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004905764071477784\n",
      "Average test loss: 0.006376562858207358\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004892543204128743\n",
      "Average test loss: 0.006423747753517495\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004868855677958992\n",
      "Average test loss: 0.007344380990498596\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004875806326253547\n",
      "Average test loss: 0.006414741286800967\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004852036722004413\n",
      "Average test loss: 0.006341968506988552\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004844139052348004\n",
      "Average test loss: 0.006337047656791077\n",
      "Epoch 76/300\n",
      "Average training loss: 0.004837569478485319\n",
      "Average test loss: 0.006390512411379152\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004824916424022781\n",
      "Average test loss: 0.006346156291663647\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004816636617812845\n",
      "Average test loss: 0.006533184233432015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004812534178296725\n",
      "Average test loss: 0.006412728225605355\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004792536746710539\n",
      "Average test loss: 0.006446095625145568\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004778592839837074\n",
      "Average test loss: 0.007408105711142222\n",
      "Epoch 82/300\n",
      "Average training loss: 0.004780820350059205\n",
      "Average test loss: 0.006860628091626697\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004763930927962064\n",
      "Average test loss: 0.006357340472439925\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004765527591937118\n",
      "Average test loss: 0.006382902423540751\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004748922946966357\n",
      "Average test loss: 0.006327760578857528\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004747564480122593\n",
      "Average test loss: 0.00645459536752767\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004718495262579786\n",
      "Average test loss: 0.006503056281970607\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004722362181792656\n",
      "Average test loss: 0.006893645661986536\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00471461811185711\n",
      "Average test loss: 0.006632953834616475\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004704151964849896\n",
      "Average test loss: 0.006368928377827009\n",
      "Epoch 91/300\n",
      "Average training loss: 0.004696096183525191\n",
      "Average test loss: 0.0063794800514976185\n",
      "Epoch 92/300\n",
      "Average training loss: 0.00467453558743\n",
      "Average test loss: 0.006567707769572734\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00467697918953167\n",
      "Average test loss: 0.006503096411625544\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004667377906541029\n",
      "Average test loss: 0.0065025267452001575\n",
      "Epoch 95/300\n",
      "Average training loss: 0.004657913710715042\n",
      "Average test loss: 0.006463527313950989\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004652804341581132\n",
      "Average test loss: 0.006327540917942922\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004640064576847686\n",
      "Average test loss: 0.006549692886985011\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004662148548083173\n",
      "Average test loss: 0.00641789871495631\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004612269792084893\n",
      "Average test loss: 0.006414725286679136\n",
      "Epoch 100/300\n",
      "Average training loss: 0.004619652530178427\n",
      "Average test loss: 0.006515092322395908\n",
      "Epoch 101/300\n",
      "Average training loss: 0.004607224004343152\n",
      "Average test loss: 0.0063536168883244195\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004606355735411247\n",
      "Average test loss: 0.006615296633707152\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004599812198223339\n",
      "Average test loss: 0.006394023615039057\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004587727413409286\n",
      "Average test loss: 0.006291333268913958\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004583823197417789\n",
      "Average test loss: 0.006401300855808788\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004623495308682323\n",
      "Average test loss: 0.006564071170985699\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0045627398552993935\n",
      "Average test loss: 0.006426949424462186\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004556171850197845\n",
      "Average test loss: 0.006649208988580439\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0045543402392003276\n",
      "Average test loss: 0.006347959635572301\n",
      "Epoch 110/300\n",
      "Average training loss: 0.004609379255523284\n",
      "Average test loss: 0.0064224514588713645\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0045451052110228275\n",
      "Average test loss: 0.006360417229433854\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004527752822058069\n",
      "Average test loss: 0.00648392860384451\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0045496559091326265\n",
      "Average test loss: 0.006455406753139364\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004513881562070714\n",
      "Average test loss: 0.006565283834520314\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004520766242096822\n",
      "Average test loss: 0.006469470838291778\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004516038147111733\n",
      "Average test loss: 0.006542876058982478\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004506892865937617\n",
      "Average test loss: 0.006537001127584113\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00453097504687806\n",
      "Average test loss: 0.007726955975509352\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0045322019180489915\n",
      "Average test loss: 0.006501807793974876\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0044805401542948355\n",
      "Average test loss: 0.006478634629398584\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004483090500864718\n",
      "Average test loss: 0.0064777645696368485\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0044721497425602545\n",
      "Average test loss: 0.006525243064595594\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004466680501484209\n",
      "Average test loss: 0.006647880285564396\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0044665398713615205\n",
      "Average test loss: 0.006531743250787258\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004464142575446102\n",
      "Average test loss: 0.0064339541250632865\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004462355941534042\n",
      "Average test loss: 0.00653704139466087\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004472169739090734\n",
      "Average test loss: 0.006533865313149161\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0044417203619248335\n",
      "Average test loss: 0.006626916907727719\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004445904809774624\n",
      "Average test loss: 0.006465583639426364\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004461355283442471\n",
      "Average test loss: 0.006416413953320848\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004442353362424506\n",
      "Average test loss: 0.006490063796854681\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004421259688006507\n",
      "Average test loss: 0.006451220980534951\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004432417849699656\n",
      "Average test loss: 0.0066717332038614486\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004418367111020618\n",
      "Average test loss: 0.006352786611351702\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004422724438417289\n",
      "Average test loss: 0.006534178429179721\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004405383145850566\n",
      "Average test loss: 0.0067214317309359705\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004400375154697233\n",
      "Average test loss: 0.0065096766433368125\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0044151107395688696\n",
      "Average test loss: 0.006703872136357758\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004391380598354671\n",
      "Average test loss: 0.006523660589423445\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004392483873499765\n",
      "Average test loss: 0.006633134559210804\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004407216503388352\n",
      "Average test loss: 0.0065024172473284935\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004380753177735541\n",
      "Average test loss: 0.0065762929092678755\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00437262895454963\n",
      "Average test loss: 0.006799403239455488\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00438006148652898\n",
      "Average test loss: 0.006478262039936251\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004367936132682694\n",
      "Average test loss: 0.006660338327288628\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004366098124947813\n",
      "Average test loss: 0.006750393509864807\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004406003358877367\n",
      "Average test loss: 0.006362422054012617\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004352713595454891\n",
      "Average test loss: 0.0065742382142278885\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004337516030089723\n",
      "Average test loss: 0.012050710605250465\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004362500290075938\n",
      "Average test loss: 0.0065298512660794785\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004351817324136694\n",
      "Average test loss: 0.00643350023859077\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0043325975702868565\n",
      "Average test loss: 0.006543928530481127\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004327616886132294\n",
      "Average test loss: 0.0066689147353172305\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0043461753183768855\n",
      "Average test loss: 0.006589905391136805\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004325869834257497\n",
      "Average test loss: 0.006500618611358934\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004322833296325472\n",
      "Average test loss: 0.006512992800523838\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004319302244732777\n",
      "Average test loss: 0.006580147964258988\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004320431383740571\n",
      "Average test loss: 0.006510194095472495\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004310533901883496\n",
      "Average test loss: 0.00659477616680993\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004306680036294791\n",
      "Average test loss: 0.006558234319918686\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004301891869968838\n",
      "Average test loss: 0.006970009317000707\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004299201482699977\n",
      "Average test loss: 0.006592015242824952\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0042959553889102406\n",
      "Average test loss: 0.006471375472843647\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004307128267569674\n",
      "Average test loss: 0.006866654529339738\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004291036187981566\n",
      "Average test loss: 0.006406682538075579\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004274400564945407\n",
      "Average test loss: 0.006525217189970943\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004278832058111827\n",
      "Average test loss: 0.0066212770053082045\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0042958269996775525\n",
      "Average test loss: 0.006574708106617133\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004265817485749722\n",
      "Average test loss: 0.006620930028872357\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004272029027963678\n",
      "Average test loss: 0.006644606917682621\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004275381345715788\n",
      "Average test loss: 0.006665541799532043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004272923149789373\n",
      "Average test loss: 0.006505645265595781\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004268329623258776\n",
      "Average test loss: 0.006691640358418226\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004279172671337922\n",
      "Average test loss: 0.006606323811949955\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004251486000915368\n",
      "Average test loss: 0.006601608640617794\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004245423572758833\n",
      "Average test loss: 0.006843067596356074\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0042358366424838706\n",
      "Average test loss: 0.006748008421725697\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004244301738217473\n",
      "Average test loss: 0.006739769244359599\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004251866147749954\n",
      "Average test loss: 0.00653102104117473\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0042293604262587095\n",
      "Average test loss: 0.006613589883678489\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00423590656535493\n",
      "Average test loss: 0.006520600245230728\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004227383138404952\n",
      "Average test loss: 0.006704908931420909\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004271853721804089\n",
      "Average test loss: 0.00652313171637555\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0042218448176152175\n",
      "Average test loss: 0.006746119230985641\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004221448697149754\n",
      "Average test loss: 0.006597116192181905\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004219191516646081\n",
      "Average test loss: 0.006636750062720643\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004205032514822152\n",
      "Average test loss: 0.00659139880620771\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004236171904123492\n",
      "Average test loss: 0.006571450582808919\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004206768553290103\n",
      "Average test loss: 0.006670624601344268\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0041950629920595225\n",
      "Average test loss: 0.006744235429498885\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004204708563784758\n",
      "Average test loss: 0.006573326913846864\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004196881909337309\n",
      "Average test loss: 0.006497987207439211\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004191445796233084\n",
      "Average test loss: 0.006665599624315898\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004191531359942423\n",
      "Average test loss: 0.006680362860361735\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004197805163347059\n",
      "Average test loss: 0.006505560878664255\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004184407077315781\n",
      "Average test loss: 0.0065336280034648045\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004186710886243317\n",
      "Average test loss: 0.006733157938553227\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0041876660558498565\n",
      "Average test loss: 0.006816639824046029\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004190742017701268\n",
      "Average test loss: 0.0067943248955739865\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004192052281772097\n",
      "Average test loss: 0.006627813362412982\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004179098755121231\n",
      "Average test loss: 0.006526251245290041\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00416046120143599\n",
      "Average test loss: 0.006536797787166304\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004165625239205029\n",
      "Average test loss: 0.006608837130582995\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004179228082299233\n",
      "Average test loss: 0.006632948822031418\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004170287723342578\n",
      "Average test loss: 0.006587901098860635\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004163637630227539\n",
      "Average test loss: 0.006556426175766521\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004154685123927063\n",
      "Average test loss: 0.006706809508303801\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004168005717297395\n",
      "Average test loss: 0.006567540312806765\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004224435711486472\n",
      "Average test loss: 0.006480266025496854\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0041424039010372424\n",
      "Average test loss: 0.006665097920431031\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00412929477252894\n",
      "Average test loss: 0.006819982544829448\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004131352091415061\n",
      "Average test loss: 0.0066567353374428215\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004142149209769235\n",
      "Average test loss: 0.006670933028062185\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004167987433986531\n",
      "Average test loss: 0.006566675751987431\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004130728344950411\n",
      "Average test loss: 0.006580182657059696\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004131473580582274\n",
      "Average test loss: 0.006709062128017346\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004126947928426994\n",
      "Average test loss: 0.006585080643081003\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004140715036127302\n",
      "Average test loss: 0.006594458698398537\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004123018655098147\n",
      "Average test loss: 0.006594425512684716\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004125428011847867\n",
      "Average test loss: 0.006662850802557336\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0041252193699280425\n",
      "Average test loss: 0.006643057351724969\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004113025347598725\n",
      "Average test loss: 0.006677104212550653\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00411741743526525\n",
      "Average test loss: 0.006677627284493711\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004500281616010599\n",
      "Average test loss: 0.006580613626374139\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004127061781369978\n",
      "Average test loss: 0.006563625769482719\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0040722048042549025\n",
      "Average test loss: 0.006821558074818717\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004083317196203602\n",
      "Average test loss: 0.006721332178761562\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004087079075889455\n",
      "Average test loss: 0.006953350436356333\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004093419873052173\n",
      "Average test loss: 0.0065117869915233715\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004106559344049957\n",
      "Average test loss: 0.0066146625938514865\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004131882779507173\n",
      "Average test loss: 0.006683668426755402\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004092547165850799\n",
      "Average test loss: 0.006678021843234698\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0041137884644170605\n",
      "Average test loss: 0.006532300644450717\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004087502829730511\n",
      "Average test loss: 0.00657888505483667\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004101176546265682\n",
      "Average test loss: 0.00657597086371647\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0040881808607114685\n",
      "Average test loss: 0.006528077865640323\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004090831742104557\n",
      "Average test loss: 0.008970604094366232\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0040936528373923566\n",
      "Average test loss: 0.006742018361886343\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0040800733615954715\n",
      "Average test loss: 0.006679503013690313\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004084677269061406\n",
      "Average test loss: 0.006604787836058272\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004071087925591402\n",
      "Average test loss: 0.006667556693156561\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004073047512314386\n",
      "Average test loss: 0.006534506929831372\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004075998693911565\n",
      "Average test loss: 0.006723172663400571\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004071223778029283\n",
      "Average test loss: 0.006765588196615378\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004106451876254545\n",
      "Average test loss: 0.006570976184060176\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004057004278111789\n",
      "Average test loss: 0.006662681610220008\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004061904923576448\n",
      "Average test loss: 0.00665002845807208\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004059845284869273\n",
      "Average test loss: 0.006687707942807012\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004057822969431678\n",
      "Average test loss: 0.006606251825475031\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004072038779656092\n",
      "Average test loss: 0.0067926788528760275\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004062201051869326\n",
      "Average test loss: 0.006659229500633147\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004100191188562248\n",
      "Average test loss: 0.006716033421870735\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004047210901147789\n",
      "Average test loss: 0.006635676381488641\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004048280381494098\n",
      "Average test loss: 0.006569132752716541\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004254189798608423\n",
      "Average test loss: 0.006576746096213659\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004052592534985807\n",
      "Average test loss: 0.006497074585821894\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004022225297159619\n",
      "Average test loss: 0.0067673182677891516\n",
      "Epoch 258/300\n",
      "Average training loss: 0.00402875040906171\n",
      "Average test loss: 0.0066817930311792425\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004021049723236097\n",
      "Average test loss: 0.006679421140915818\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004036396185970969\n",
      "Average test loss: 0.0077224085686935316\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0040555389647682506\n",
      "Average test loss: 0.007358002299649848\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004028728090847532\n",
      "Average test loss: 0.006661954293648402\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004037195191408197\n",
      "Average test loss: 0.006627993811335829\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004031604211777448\n",
      "Average test loss: 0.006943789238731067\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004029173792236381\n",
      "Average test loss: 0.006671867459184594\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004028668701648712\n",
      "Average test loss: 0.006614188225318988\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0040215525053855445\n",
      "Average test loss: 0.006636878015266525\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004031061600893736\n",
      "Average test loss: 0.006611282153262032\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004026829104042715\n",
      "Average test loss: 0.006606505376597246\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00401341717566053\n",
      "Average test loss: 0.006665363914850685\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004060989517304632\n",
      "Average test loss: 0.0069522877393497365\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004019385415853726\n",
      "Average test loss: 0.006750767161655757\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004007667903064026\n",
      "Average test loss: 0.006744915025101768\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004010155356799563\n",
      "Average test loss: 0.00657718250196841\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004003532656571931\n",
      "Average test loss: 0.006702073478864299\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0040016358780364195\n",
      "Average test loss: 0.006750610902077622\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004012311921765407\n",
      "Average test loss: 0.006653720273739762\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004014739599492815\n",
      "Average test loss: 0.006599871323340469\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004053945146708025\n",
      "Average test loss: 0.0069077819221549566\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004000581382049454\n",
      "Average test loss: 0.006610465744924214\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003985692026300563\n",
      "Average test loss: 0.006735395349148247\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00400073876314693\n",
      "Average test loss: 0.006716332619388898\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004001542108754317\n",
      "Average test loss: 0.006909454664008485\n",
      "Epoch 284/300\n",
      "Average training loss: 0.003993424754796757\n",
      "Average test loss: 0.006602550543016858\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0039889122098684315\n",
      "Average test loss: 0.00667912397119734\n",
      "Epoch 286/300\n",
      "Average training loss: 0.003991927023149199\n",
      "Average test loss: 0.006619322664621803\n",
      "Epoch 287/300\n",
      "Average training loss: 0.003997246590753396\n",
      "Average test loss: 0.006658153329458501\n",
      "Epoch 288/300\n",
      "Average training loss: 0.003986052607496579\n",
      "Average test loss: 0.0068114800196554925\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00398490545546843\n",
      "Average test loss: 0.006653551862471634\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003987955506684052\n",
      "Average test loss: 0.011345602944493294\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004017234299745824\n",
      "Average test loss: 0.006650173424432675\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003980085048824549\n",
      "Average test loss: 0.006818100339629584\n",
      "Epoch 293/300\n",
      "Average training loss: 0.003986116408473916\n",
      "Average test loss: 0.00706090472266078\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0039689148370590475\n",
      "Average test loss: 0.006660801761266258\n",
      "Epoch 295/300\n",
      "Average training loss: 0.003983601306668586\n",
      "Average test loss: 0.006687031552195549\n",
      "Epoch 296/300\n",
      "Average training loss: 0.003973243036617835\n",
      "Average test loss: 0.006756238641424312\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003972966051763958\n",
      "Average test loss: 0.010342059113913112\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003968385657916467\n",
      "Average test loss: 0.006716934408164687\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00396420174319711\n",
      "Average test loss: 0.006711791791435745\n",
      "Epoch 300/300\n",
      "Average training loss: 0.003961278854144944\n",
      "Average test loss: 0.006782934843252103\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02503531221797069\n",
      "Average test loss: 0.010382411677804258\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008000951567457781\n",
      "Average test loss: 0.008246525473064847\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007003653268847201\n",
      "Average test loss: 0.006915002867165539\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006422421197096507\n",
      "Average test loss: 0.006465557838893599\n",
      "Epoch 5/300\n",
      "Average training loss: 0.00602379895043042\n",
      "Average test loss: 0.00587641670058171\n",
      "Epoch 6/300\n",
      "Average training loss: 0.00570756060257554\n",
      "Average test loss: 0.0057500098989241655\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005409727720750703\n",
      "Average test loss: 0.005809876818623808\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005192654555870427\n",
      "Average test loss: 0.0054060241157809896\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0050397118346558675\n",
      "Average test loss: 0.004998333485590087\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0048617872970385684\n",
      "Average test loss: 0.004963061649766233\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004702472167296542\n",
      "Average test loss: 0.004743470590975549\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004588587975750367\n",
      "Average test loss: 0.004757688677559296\n",
      "Epoch 13/300\n",
      "Average training loss: 0.00445021591687368\n",
      "Average test loss: 0.004523810891641511\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0043661652145286405\n",
      "Average test loss: 0.004437695869968997\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004253330452160703\n",
      "Average test loss: 0.004438864799837272\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004179143256404333\n",
      "Average test loss: 0.004305120359692309\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0040830489860640634\n",
      "Average test loss: 0.004356905162334442\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00400964109868639\n",
      "Average test loss: 0.004218429255609711\n",
      "Epoch 19/300\n",
      "Average training loss: 0.003940036888130837\n",
      "Average test loss: 0.004383616897174054\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003881818476029568\n",
      "Average test loss: 0.0041789876793821654\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0038230722966707416\n",
      "Average test loss: 0.0040497634392231705\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0037710604303412966\n",
      "Average test loss: 0.003969608808557193\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0037269154934005603\n",
      "Average test loss: 0.003971244408024682\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0036670550987538365\n",
      "Average test loss: 0.003971718721919589\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0036222932698825997\n",
      "Average test loss: 0.003996514641576343\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0035859641881866586\n",
      "Average test loss: 0.0038552573836512036\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0035507746369888384\n",
      "Average test loss: 0.0039482397250831126\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0035103238007674615\n",
      "Average test loss: 0.0038753376859757636\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0034766644763035907\n",
      "Average test loss: 0.003850713826508986\n",
      "Epoch 30/300\n",
      "Average training loss: 0.00345337039584087\n",
      "Average test loss: 0.0038642936845620472\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0034218714773241016\n",
      "Average test loss: 0.0037724222886479563\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0033971246464384926\n",
      "Average test loss: 0.0038211401655442183\n",
      "Epoch 33/300\n",
      "Average training loss: 0.003356482872325513\n",
      "Average test loss: 0.003834987894528442\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0033555883100877208\n",
      "Average test loss: 0.003980048723104927\n",
      "Epoch 35/300\n",
      "Average training loss: 0.003317422893519203\n",
      "Average test loss: 0.003766892753334509\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0032988911798844736\n",
      "Average test loss: 0.003928994304604001\n",
      "Epoch 37/300\n",
      "Average training loss: 0.003270681694563892\n",
      "Average test loss: 0.00376424595196214\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0032519974029726452\n",
      "Average test loss: 0.003905668110276262\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0032384372390806674\n",
      "Average test loss: 0.0037769509522865216\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003215414812995328\n",
      "Average test loss: 0.0038943933389253087\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0032075363863259556\n",
      "Average test loss: 0.0037583428977264297\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0031891125546147426\n",
      "Average test loss: 0.003701929808904727\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003162732726169957\n",
      "Average test loss: 0.0037274004955672557\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0033367715533822776\n",
      "Average test loss: 0.0036702954380048647\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0031321316357288097\n",
      "Average test loss: 0.003777707941830158\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00311033576561345\n",
      "Average test loss: 0.0037048829247554142\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0031072250163803497\n",
      "Average test loss: 0.0037329872411986193\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003100314601014058\n",
      "Average test loss: 0.0037855898419188127\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0030878076807906232\n",
      "Average test loss: 0.0037210592083219023\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0030769102819677855\n",
      "Average test loss: 0.00375259352206356\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00443840338393218\n",
      "Average test loss: 0.00472192871156666\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004019541401623024\n",
      "Average test loss: 0.0038466635396083196\n",
      "Epoch 53/300\n",
      "Average training loss: 0.003626720575822724\n",
      "Average test loss: 0.0037019446616371474\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003466470135996739\n",
      "Average test loss: 0.00384874624096685\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0033462682689229645\n",
      "Average test loss: 0.0036523605932792026\n",
      "Epoch 56/300\n",
      "Average training loss: 0.003236950173974037\n",
      "Average test loss: 0.0037497819082604514\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0031424525164895586\n",
      "Average test loss: 0.003784535590145323\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0030787644622226554\n",
      "Average test loss: 0.0037565890894167954\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003055246901181009\n",
      "Average test loss: 0.003806238030600879\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003040573705194725\n",
      "Average test loss: 0.0036467823535203934\n",
      "Epoch 61/300\n",
      "Average training loss: 0.003034860130192505\n",
      "Average test loss: 0.0037331010467476313\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003033673161226842\n",
      "Average test loss: 0.0037311834941307705\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0030125007681134673\n",
      "Average test loss: 0.00379070429255565\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0031112683431969747\n",
      "Average test loss: 0.003781909484623207\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0030094226789143352\n",
      "Average test loss: 0.003684332630286614\n",
      "Epoch 66/300\n",
      "Average training loss: 0.003001891186874774\n",
      "Average test loss: 0.0036897717345919876\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002991700660644306\n",
      "Average test loss: 185.40677250162761\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005078617279314333\n",
      "Average test loss: 0.004044923053433498\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00369698354229331\n",
      "Average test loss: 0.004223391499784257\n",
      "Epoch 70/300\n",
      "Average training loss: 0.003458795757343372\n",
      "Average test loss: 0.00374283492565155\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0033159926539907854\n",
      "Average test loss: 0.0036742919919391472\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003196343191381958\n",
      "Average test loss: 0.003693708317147361\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003093380227478014\n",
      "Average test loss: 0.0036617643891109363\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0030219982790036333\n",
      "Average test loss: 0.0037304368851085504\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002977595865726471\n",
      "Average test loss: 0.003656720574738251\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0029585450815243853\n",
      "Average test loss: 0.0036633385761330526\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0029485030946218307\n",
      "Average test loss: 0.003694997044487132\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0029409762945854\n",
      "Average test loss: 0.0036365179247740244\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0029455945117192136\n",
      "Average test loss: 0.0037581693335539765\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002943527116647197\n",
      "Average test loss: 0.0037924100193712446\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00293639705164565\n",
      "Average test loss: 0.003739255327731371\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0029424652941524984\n",
      "Average test loss: 0.0037237925570872093\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002930536272418168\n",
      "Average test loss: 0.003662354909090532\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002925415357781781\n",
      "Average test loss: 0.0037320819190806813\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002904675541859534\n",
      "Average test loss: 0.0037274283824695483\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002902531588036153\n",
      "Average test loss: 0.003705134337147077\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0029309071383128564\n",
      "Average test loss: 0.003777412797841761\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0028759994092914794\n",
      "Average test loss: 0.0038387987626095615\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0028736164097984632\n",
      "Average test loss: 0.003766329398171769\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003007210735231638\n",
      "Average test loss: 0.0037569042951282527\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0028638332564797668\n",
      "Average test loss: 0.003676829749097427\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0028540191395829122\n",
      "Average test loss: 0.0037255215181244746\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002869448981144362\n",
      "Average test loss: 0.0037570554593371022\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0028494891139368215\n",
      "Average test loss: 0.003693332779324717\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002842746117255754\n",
      "Average test loss: 0.0037400015021363895\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0028357511781569983\n",
      "Average test loss: 0.004406600718489952\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002898490916730629\n",
      "Average test loss: 0.0037161434412830404\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0028148145843297245\n",
      "Average test loss: 0.0036346322345650857\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0028410800399465692\n",
      "Average test loss: 0.0037274516705009674\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0028058599986963803\n",
      "Average test loss: 0.0037476247631841237\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002808272892609239\n",
      "Average test loss: 0.003782479614019394\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002810487860503296\n",
      "Average test loss: 0.0037583330128755834\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0028078786354098057\n",
      "Average test loss: 0.003722288044169545\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0027945851176563235\n",
      "Average test loss: 0.0036672267975906532\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0027960290304488607\n",
      "Average test loss: 0.0037220095278074346\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0027792909178468916\n",
      "Average test loss: 0.0037810059674084186\n",
      "Epoch 107/300\n",
      "Average training loss: 0.00277556341389815\n",
      "Average test loss: 0.003691175623072518\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0027984404812256493\n",
      "Average test loss: 0.0036491329926583503\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002768602252834373\n",
      "Average test loss: 0.0037270839580645166\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0027628172741581995\n",
      "Average test loss: 0.003661632765291466\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002758050983771682\n",
      "Average test loss: 0.003771522005192108\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0027460541907283996\n",
      "Average test loss: 0.0037272366928971477\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0027624611800743473\n",
      "Average test loss: 0.003830751688530048\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0027393486632241145\n",
      "Average test loss: 0.003735823698962728\n",
      "Epoch 115/300\n",
      "Average training loss: 0.002775427874798576\n",
      "Average test loss: 0.003715238499144713\n",
      "Epoch 116/300\n",
      "Average training loss: 0.002731160648374094\n",
      "Average test loss: 0.003736884130992823\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002735567030393415\n",
      "Average test loss: 0.003850969043249885\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0027238582842465902\n",
      "Average test loss: 0.0037448243155247633\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0027267966226985057\n",
      "Average test loss: 0.0038130689702100223\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002763467234041956\n",
      "Average test loss: 0.0037213677312764857\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002723573373216722\n",
      "Average test loss: 0.003775537641512023\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0027157155219465494\n",
      "Average test loss: 0.003842182968639665\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002705262691196468\n",
      "Average test loss: 0.003883714632027679\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002769186481005616\n",
      "Average test loss: 0.003704186415920655\n",
      "Epoch 125/300\n",
      "Average training loss: 0.002686045352783468\n",
      "Average test loss: 0.0037300279926922586\n",
      "Epoch 126/300\n",
      "Average training loss: 0.002693835399010115\n",
      "Average test loss: 0.003731312696304586\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0026965921067943176\n",
      "Average test loss: 0.004071507489929596\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0026998328229205478\n",
      "Average test loss: 0.0037740360252145264\n",
      "Epoch 129/300\n",
      "Average training loss: 0.002680048590319024\n",
      "Average test loss: 0.0038012422213537823\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002686850746679637\n",
      "Average test loss: 0.0037677361166311633\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002675003159998192\n",
      "Average test loss: 0.0038642774331900808\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002678709118627012\n",
      "Average test loss: 0.0038380601637893253\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0026767682766334878\n",
      "Average test loss: 0.0038401870955195694\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0026681732899612853\n",
      "Average test loss: 0.0037541463654488325\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002659836469218135\n",
      "Average test loss: 0.0037789827821155387\n",
      "Epoch 136/300\n",
      "Average training loss: 0.002661904555848903\n",
      "Average test loss: 0.003916218262165785\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0026664371896121235\n",
      "Average test loss: 0.003713805709448126\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0026758816367429164\n",
      "Average test loss: 0.0037717459437747796\n",
      "Epoch 139/300\n",
      "Average training loss: 0.002672075702084435\n",
      "Average test loss: 0.004004534313248264\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00264693963713944\n",
      "Average test loss: 0.004024271017147435\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002640696112376948\n",
      "Average test loss: 0.003768648350197408\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002648506890775429\n",
      "Average test loss: 0.004177614832917849\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0026340055730607772\n",
      "Average test loss: 0.0037361582031266555\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0026394528386493523\n",
      "Average test loss: 0.00372321675469478\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002636632530225648\n",
      "Average test loss: 0.003825489069438643\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002629609323417147\n",
      "Average test loss: 0.0037645632456988097\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0026811020587467487\n",
      "Average test loss: 0.0037669916438559693\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0026461395629578167\n",
      "Average test loss: 0.003832682464685705\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0026213217224511833\n",
      "Average test loss: 0.0037877696580770944\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0026261366120436127\n",
      "Average test loss: 0.003855476351868775\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0026308574326750307\n",
      "Average test loss: 0.003748381591712435\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0026008439734578133\n",
      "Average test loss: 0.0037443962556620437\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0026151712704449892\n",
      "Average test loss: 0.00396944064895312\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002624672652118736\n",
      "Average test loss: 0.0038651272095739844\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002605864857426948\n",
      "Average test loss: 0.003846438547389375\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002604091666638851\n",
      "Average test loss: 0.003967750087794331\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002608969631087449\n",
      "Average test loss: 0.0037677848330802386\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0025977936656110816\n",
      "Average test loss: 0.00377026769436068\n",
      "Epoch 159/300\n",
      "Average training loss: 0.002595709966495633\n",
      "Average test loss: 0.0037624546647485758\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00259008672233257\n",
      "Average test loss: 0.00393545827228162\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0026001800381475026\n",
      "Average test loss: 0.003800967767006821\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0025810618177056315\n",
      "Average test loss: 0.003721239731957515\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0025857784187214242\n",
      "Average test loss: 0.0037995602490587367\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002590500652583109\n",
      "Average test loss: 0.003789329245686531\n",
      "Epoch 165/300\n",
      "Average training loss: 0.002581214643186993\n",
      "Average test loss: 0.0037933540375282367\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002584629951044917\n",
      "Average test loss: 0.003840342731525501\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0025772484271890587\n",
      "Average test loss: 0.0037986411590956977\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002574468668550253\n",
      "Average test loss: 0.0037619172295348512\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0025769591447379852\n",
      "Average test loss: 0.0038863534436871607\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0025796493287715645\n",
      "Average test loss: 0.0037564606968727376\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002576907048622767\n",
      "Average test loss: 0.0037422827122112114\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0026154063211547003\n",
      "Average test loss: 0.003920767707957162\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0025530454681979287\n",
      "Average test loss: 0.0038650944820708697\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0025620221065150365\n",
      "Average test loss: 0.003801080906142791\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0025639902776521113\n",
      "Average test loss: 0.003865149312135246\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002553160865894622\n",
      "Average test loss: 0.0037955898503876395\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0025538533054706124\n",
      "Average test loss: 0.0037847387173937427\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0025589807445390357\n",
      "Average test loss: 0.003864759504588114\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0025549876025567453\n",
      "Average test loss: 0.003913066428154707\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0025527421320892044\n",
      "Average test loss: 0.0038188723143604067\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0025484988335520028\n",
      "Average test loss: 0.0038004973787400457\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0025394548821366498\n",
      "Average test loss: 0.0037896110489964484\n",
      "Epoch 183/300\n",
      "Average training loss: 0.002550301303466161\n",
      "Average test loss: 0.003848928888224893\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0025423332438286807\n",
      "Average test loss: 0.0039052774844070275\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0025484723850256867\n",
      "Average test loss: 0.003821137706024779\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0025742996046319603\n",
      "Average test loss: 0.004003359045419428\n",
      "Epoch 187/300\n",
      "Average training loss: 0.002530791244780024\n",
      "Average test loss: 0.0038471697051492004\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002533942083724671\n",
      "Average test loss: 0.004097895638396343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0025255305148247217\n",
      "Average test loss: 0.0038103867876860833\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0025269807570924363\n",
      "Average test loss: 0.0037896567744513353\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0025426947853217524\n",
      "Average test loss: 0.0038477729596197606\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0025315584467930927\n",
      "Average test loss: 0.003780345349262158\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0025232331028415098\n",
      "Average test loss: 0.0038664456804593404\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00252556990687218\n",
      "Average test loss: 0.00386928091570735\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0025203411574992868\n",
      "Average test loss: 0.00390225499889089\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0025275467216140696\n",
      "Average test loss: 0.0038109514518744414\n",
      "Epoch 197/300\n",
      "Average training loss: 0.002516196980037623\n",
      "Average test loss: 0.0038901788567503293\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002520099823259645\n",
      "Average test loss: 0.003841927104112175\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002522571075086792\n",
      "Average test loss: 0.003894716155404846\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0025133957597944473\n",
      "Average test loss: 0.0037784087252285747\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0025127365994784567\n",
      "Average test loss: 0.003926158068080743\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0025081707862102324\n",
      "Average test loss: 0.0038379801228228543\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0025117808011256987\n",
      "Average test loss: 0.0038976933024823667\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0025220731006314356\n",
      "Average test loss: 0.003775907506959306\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0025067469485931925\n",
      "Average test loss: 0.003803713436341948\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0025086522085799113\n",
      "Average test loss: 0.003812731769763761\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0025031628807385764\n",
      "Average test loss: 0.0037962860823091533\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002500338022907575\n",
      "Average test loss: 0.004207815029140975\n",
      "Epoch 209/300\n",
      "Average training loss: 0.002497696104355984\n",
      "Average test loss: 0.0039215569628609556\n",
      "Epoch 210/300\n",
      "Average training loss: 0.002491882456259595\n",
      "Average test loss: 0.004314607028125061\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002600537070176668\n",
      "Average test loss: 0.003834955154193772\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0025320653553224273\n",
      "Average test loss: 0.0038002526354458596\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0024789428582621944\n",
      "Average test loss: 0.0038163918662402364\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0024833136366473304\n",
      "Average test loss: 0.0038834821718434494\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0024806913212976523\n",
      "Average test loss: 0.003951422159870465\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002486334096122947\n",
      "Average test loss: 0.003907061707228422\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0025020436403445073\n",
      "Average test loss: 0.003989881596010592\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00249467564581169\n",
      "Average test loss: 0.003925679293771585\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002478024578963717\n",
      "Average test loss: 0.004005874816742208\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0025091052365799746\n",
      "Average test loss: 0.003755839134463006\n",
      "Epoch 221/300\n",
      "Average training loss: 0.002477837754620446\n",
      "Average test loss: 0.0037763965742455587\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0024720922449810636\n",
      "Average test loss: 0.004027748463261459\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0024711794320286975\n",
      "Average test loss: 0.0038725242256704303\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0024771637953817844\n",
      "Average test loss: 0.003870089073975881\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002476806639176276\n",
      "Average test loss: 0.003789000326767564\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002508704056756364\n",
      "Average test loss: 86.92330951605902\n",
      "Epoch 227/300\n",
      "Average training loss: 0.005265977435227898\n",
      "Average test loss: 0.004015122535948952\n",
      "Epoch 228/300\n",
      "Average training loss: 0.003560166460979316\n",
      "Average test loss: 0.003760262605423729\n",
      "Epoch 229/300\n",
      "Average training loss: 0.003293003588087029\n",
      "Average test loss: 0.0036496141381147833\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0031126226909044714\n",
      "Average test loss: 0.003644757918185658\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0029541602888041073\n",
      "Average test loss: 0.003680320787347025\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002800562963510553\n",
      "Average test loss: 0.0036999770113163525\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0026566630638101035\n",
      "Average test loss: 0.003809016711595986\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0025482726773868006\n",
      "Average test loss: 0.0037262702265547384\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0024796236676888333\n",
      "Average test loss: 0.0038661497173209983\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002444507336243987\n",
      "Average test loss: 0.003862422947047485\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0024325501384834447\n",
      "Average test loss: 0.0038072652342832753\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0024273988670772974\n",
      "Average test loss: 0.003877972334002455\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002430653017014265\n",
      "Average test loss: 0.0038098743909762965\n",
      "Epoch 240/300\n",
      "Average training loss: 0.002444304786208603\n",
      "Average test loss: 0.004010313642107778\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0024674810680250328\n",
      "Average test loss: 0.0038603050390051473\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0024675934149159325\n",
      "Average test loss: 0.0038635261274046367\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002460680193785164\n",
      "Average test loss: 0.003792898404515452\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0024597822059359816\n",
      "Average test loss: 0.003919487110649546\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002455639123295744\n",
      "Average test loss: 0.0038747064361555708\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002463676765354143\n",
      "Average test loss: 0.0038235045826683443\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0024697419473280508\n",
      "Average test loss: 0.003990897308621142\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0024550038314320975\n",
      "Average test loss: 0.003932258523172802\n",
      "Epoch 249/300\n",
      "Average training loss: 0.002454687333251867\n",
      "Average test loss: 0.003896690025511715\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0028636277864376703\n",
      "Average test loss: 0.003878663429990411\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002464666315457887\n",
      "Average test loss: 0.0038940998907718394\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0024308618360923398\n",
      "Average test loss: 0.0038455909705824324\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0024324647407564853\n",
      "Average test loss: 0.0038723246498654287\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0024384474520468048\n",
      "Average test loss: 0.0038567959157129127\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002443055593719085\n",
      "Average test loss: 0.00386752096687754\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002441972191962931\n",
      "Average test loss: 0.003933181189828449\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002444985315617588\n",
      "Average test loss: 0.003912598545766539\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002442264767984549\n",
      "Average test loss: 0.003848795824787683\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0024558904161676763\n",
      "Average test loss: 0.003958243373781443\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0024432427320215436\n",
      "Average test loss: 0.0038206122676945394\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002461871056817472\n",
      "Average test loss: 0.003979784285856618\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00243970564690729\n",
      "Average test loss: 0.003957300254040294\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002433902731579211\n",
      "Average test loss: 0.0038749711840516993\n",
      "Epoch 264/300\n",
      "Average training loss: 0.002430798697595795\n",
      "Average test loss: 0.0038522683994637597\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0024400092303338977\n",
      "Average test loss: 0.0038649224680331017\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0024379036153356235\n",
      "Average test loss: 0.0037985999749766456\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0024330364047653145\n",
      "Average test loss: 0.0038420678195026188\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0024373982277595334\n",
      "Average test loss: 0.0038435643079380195\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002431815829748909\n",
      "Average test loss: 0.00393930437705583\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0024619277604959076\n",
      "Average test loss: 0.003902306797189845\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0024234443382463522\n",
      "Average test loss: 0.003910778472820918\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0025590443667024374\n",
      "Average test loss: 0.003742814912357264\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0025341454442176555\n",
      "Average test loss: 0.003840057393329011\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0024134744529922805\n",
      "Average test loss: 0.0038197170628441707\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0024086642116308214\n",
      "Average test loss: 0.0037759251794260407\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0024149945939166678\n",
      "Average test loss: 0.003950259110786849\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002419986917430328\n",
      "Average test loss: 0.003867652650301655\n",
      "Epoch 278/300\n",
      "Average training loss: 0.002415460438157121\n",
      "Average test loss: 0.003927303620510631\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002421609630601274\n",
      "Average test loss: 0.003871797467271487\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002427191282932957\n",
      "Average test loss: 0.0039030154301888413\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0024203136719556317\n",
      "Average test loss: 0.003971177337070306\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002417475604452193\n",
      "Average test loss: 0.003981752708140347\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002420508292400175\n",
      "Average test loss: 0.003938767340862089\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002411760390425722\n",
      "Average test loss: 0.004015813132334086\n",
      "Epoch 285/300\n",
      "Average training loss: 0.002413922678679228\n",
      "Average test loss: 0.004024097896491488\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0024221857941399017\n",
      "Average test loss: 0.0039045986582835514\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0024074873413062756\n",
      "Average test loss: 0.004384587891399861\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0024123406908992265\n",
      "Average test loss: 0.0038478551575293145\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002411546823465162\n",
      "Average test loss: 0.004006974043945471\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0024123963924745718\n",
      "Average test loss: 0.0040281044975337054\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002415249529812071\n",
      "Average test loss: 0.003898823372605774\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0024065571185201405\n",
      "Average test loss: 0.003965446473616693\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0024058456174615357\n",
      "Average test loss: 0.0038825420536514788\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0024027379562871322\n",
      "Average test loss: 0.003967921150434348\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002408329356461763\n",
      "Average test loss: 0.003894743658395277\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002513653492141101\n",
      "Average test loss: 0.003851183778502875\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0025179279485924378\n",
      "Average test loss: 0.00391939186056455\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0024014003179553484\n",
      "Average test loss: 0.003923205112003618\n",
      "Epoch 299/300\n",
      "Average training loss: 0.002397821858111355\n",
      "Average test loss: 0.003875318392697308\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0023881362066086797\n",
      "Average test loss: 0.0038961093075987364\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.021963886801567342\n",
      "Average test loss: 0.006627689602474372\n",
      "Epoch 2/300\n",
      "Average training loss: 0.006257642890430159\n",
      "Average test loss: 0.007985197635574473\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0053811206341617636\n",
      "Average test loss: 0.005264491019149622\n",
      "Epoch 4/300\n",
      "Average training loss: 0.004897247878834605\n",
      "Average test loss: 0.004620359918930464\n",
      "Epoch 5/300\n",
      "Average training loss: 0.004541965208947659\n",
      "Average test loss: 0.004724440988981062\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0042567782041927175\n",
      "Average test loss: 0.004016375237868892\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004023554877481527\n",
      "Average test loss: 0.003986373753390378\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0038330841747423014\n",
      "Average test loss: 0.0037477197206268707\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0036761842436260645\n",
      "Average test loss: 0.003820386881215705\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0035339032798591585\n",
      "Average test loss: 0.0036359459944069385\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0034224535673856734\n",
      "Average test loss: 0.004978731564763519\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003285227470927768\n",
      "Average test loss: 0.0034827755120479397\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0031842608551184335\n",
      "Average test loss: 0.00322512550610635\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0030887134874032604\n",
      "Average test loss: 0.0031987265437427494\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003020303681285845\n",
      "Average test loss: 0.0038201357304222055\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002947924927601384\n",
      "Average test loss: 0.0031167441134651503\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0028754424125783974\n",
      "Average test loss: 0.002949615779850218\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0028146952136109274\n",
      "Average test loss: 0.002997847464763456\n",
      "Epoch 19/300\n",
      "Average training loss: 0.002763573201994101\n",
      "Average test loss: 0.002899674020293686\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0027133420776161883\n",
      "Average test loss: 0.003390117412639989\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002674040621560481\n",
      "Average test loss: 0.0028552802443090413\n",
      "Epoch 22/300\n",
      "Average training loss: 0.002618585434431831\n",
      "Average test loss: 0.003100551014766097\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0026179547982497346\n",
      "Average test loss: 0.0027661027947647704\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002541547510979904\n",
      "Average test loss: 0.002729964455175731\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002529588292249375\n",
      "Average test loss: 0.0026529059624299405\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002482605103196369\n",
      "Average test loss: 0.002676662199613121\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00245601757015619\n",
      "Average test loss: 0.002637228830407063\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0024332186989486216\n",
      "Average test loss: 0.0026861745229818756\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0024056796230789686\n",
      "Average test loss: 0.002791374834875266\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0024360072093291417\n",
      "Average test loss: 0.0026472500490231647\n",
      "Epoch 31/300\n",
      "Average training loss: 0.002353659466115965\n",
      "Average test loss: 0.00257976183688475\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002409494095481932\n",
      "Average test loss: 0.002842423028209143\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0023375758355897334\n",
      "Average test loss: 0.0025698926413638724\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002291589769224326\n",
      "Average test loss: 0.002566783036829697\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002285248180437419\n",
      "Average test loss: 0.002566313039511442\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002283231575662891\n",
      "Average test loss: 0.002627054830599162\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0025734648632092607\n",
      "Average test loss: 0.0026455240390366977\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0022823311544747818\n",
      "Average test loss: 0.002516565961142381\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0022263363141359553\n",
      "Average test loss: 0.002531847443224655\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0022229971611458393\n",
      "Average test loss: 0.0025258050062176253\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0022156242229458356\n",
      "Average test loss: 0.0024898491036146877\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002211352866453429\n",
      "Average test loss: 0.002557022759897841\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0021941907939811546\n",
      "Average test loss: 0.002521488017299109\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0024204932756515013\n",
      "Average test loss: 0.0025428704445560773\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0021780932433903217\n",
      "Average test loss: 0.0026285056008232962\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0021630568363600307\n",
      "Average test loss: 0.002518806330238779\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0021553166794280212\n",
      "Average test loss: 0.0025294232122186158\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002175289869411952\n",
      "Average test loss: 0.0024940859139379527\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002149950817434324\n",
      "Average test loss: 0.0025022192338688505\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002156805682616929\n",
      "Average test loss: 0.0025567212909873988\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0021499802622323236\n",
      "Average test loss: 0.002507070322624511\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0021475814195970693\n",
      "Average test loss: 0.0025617896624737318\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0021224299669265747\n",
      "Average test loss: 0.0025129005058358114\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0021076041565587125\n",
      "Average test loss: 0.0035158643486599126\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0021001400088684428\n",
      "Average test loss: 0.002527082968917158\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002095283382054832\n",
      "Average test loss: 0.002555543968040082\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0021220909423298306\n",
      "Average test loss: 0.0035383813081102237\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002089621443922321\n",
      "Average test loss: 0.002468834308389988\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002064301457359559\n",
      "Average test loss: 0.002522740514948964\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002082768550246126\n",
      "Average test loss: 0.002599061842283441\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0020637788906072577\n",
      "Average test loss: 0.002528956865477893\n",
      "Epoch 62/300\n",
      "Average training loss: 0.00205087686319732\n",
      "Average test loss: 0.0025519740473892954\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0020389594031083916\n",
      "Average test loss: 0.0024925957522872423\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002068018071456916\n",
      "Average test loss: 0.0025682394471433427\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002033259592225982\n",
      "Average test loss: 0.002508064607365264\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0020592775883980924\n",
      "Average test loss: 0.0024994037594232295\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0020266426446744136\n",
      "Average test loss: 0.0024655832203312055\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002012209767165283\n",
      "Average test loss: 0.0025077301588737303\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0020175929207147825\n",
      "Average test loss: 0.0024658670471981166\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002024982893632518\n",
      "Average test loss: 0.0024980521887126897\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0021420979050712453\n",
      "Average test loss: 0.002453534895347224\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002008032209550341\n",
      "Average test loss: 0.002497072178249558\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0019862906800376043\n",
      "Average test loss: 0.0025979364686128164\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0020031552702809373\n",
      "Average test loss: 0.002502106795294417\n",
      "Epoch 75/300\n",
      "Average training loss: 0.001982561520300806\n",
      "Average test loss: 0.002516646155466636\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0019718488195083206\n",
      "Average test loss: 0.0027372262701392176\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002018818613866137\n",
      "Average test loss: 0.002734933524702986\n",
      "Epoch 78/300\n",
      "Average training loss: 0.001967552822497156\n",
      "Average test loss: 0.002694053110977014\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0019780795379645295\n",
      "Average test loss: 0.0025005080424663094\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0020247072664399944\n",
      "Average test loss: 0.0025164744295179844\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0019565409410537945\n",
      "Average test loss: 0.002566963599787818\n",
      "Epoch 82/300\n",
      "Average training loss: 0.001952473956056767\n",
      "Average test loss: 0.0037116185474312968\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0019944579482802914\n",
      "Average test loss: 0.00268316300596214\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0019503039387572143\n",
      "Average test loss: 0.0025208629760891197\n",
      "Epoch 85/300\n",
      "Average training loss: 0.001966229913963212\n",
      "Average test loss: 0.002537269751023915\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0019457525116288\n",
      "Average test loss: 0.002673108306609922\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0019338231484095256\n",
      "Average test loss: 0.0025021971420695383\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0019541934089114268\n",
      "Average test loss: 0.0024784944020211697\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0019610607479181555\n",
      "Average test loss: 0.0027944300681766536\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0019412310692585176\n",
      "Average test loss: 0.0024715383605410654\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0019230369429828393\n",
      "Average test loss: 0.0025218731694751314\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0019189335201970404\n",
      "Average test loss: 0.002491260077390406\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0019226935913579332\n",
      "Average test loss: 0.0025660123696757686\n",
      "Epoch 94/300\n",
      "Average training loss: 0.001921601934151517\n",
      "Average test loss: 0.002469777189195156\n",
      "Epoch 95/300\n",
      "Average training loss: 0.001959643870488637\n",
      "Average test loss: 0.0025652088514632647\n",
      "Epoch 96/300\n",
      "Average training loss: 0.001925491829816666\n",
      "Average test loss: 0.002500599487374226\n",
      "Epoch 97/300\n",
      "Average training loss: 0.001898593840499719\n",
      "Average test loss: 0.002485803870484233\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0018954178889592488\n",
      "Average test loss: 0.002499869245828854\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0019104043543338777\n",
      "Average test loss: 0.002480923298228946\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0019376311343577172\n",
      "Average test loss: 0.0024299796294007035\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001902845183490879\n",
      "Average test loss: 0.0027430644896295335\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0018930911010959083\n",
      "Average test loss: 0.0024935889987068045\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0019182493624587853\n",
      "Average test loss: 0.002513449031342235\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0018813887546873754\n",
      "Average test loss: 0.002493746845051646\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0019022782990295026\n",
      "Average test loss: 0.0025046994582646424\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0020537899939550293\n",
      "Average test loss: 0.1325966913236512\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0034466778124786087\n",
      "Average test loss: 0.00941054662110077\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0023856438857813678\n",
      "Average test loss: 0.002505894958972931\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002234669907225503\n",
      "Average test loss: 0.0024996951861927905\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0021264974837087924\n",
      "Average test loss: 0.0024411572627723215\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002035566188601984\n",
      "Average test loss: 0.0024289092901680206\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0019478160284666552\n",
      "Average test loss: 0.002464048901158902\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0018970039530346791\n",
      "Average test loss: 0.002457175010815263\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0018698234035530025\n",
      "Average test loss: 0.002513462898838851\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0018567131171002985\n",
      "Average test loss: 0.0024808579521874586\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0018671088500155342\n",
      "Average test loss: 0.0025595724164611763\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0018610879368045263\n",
      "Average test loss: 0.0024867281081775825\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0018850577457083595\n",
      "Average test loss: 0.0025097594377067353\n",
      "Epoch 119/300\n",
      "Average training loss: 0.001863867747079995\n",
      "Average test loss: 0.0025542846847739485\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0018675691434699628\n",
      "Average test loss: 0.0026436558705237177\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0018796716762913598\n",
      "Average test loss: 0.0025508692319401436\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0018610656139337355\n",
      "Average test loss: 0.0027370226866462164\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0018756973628575604\n",
      "Average test loss: 0.002507647847342822\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0018542294869613317\n",
      "Average test loss: 0.002472110295461284\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0018585902677021092\n",
      "Average test loss: 0.002591940254697369\n",
      "Epoch 126/300\n",
      "Average training loss: 0.001870906860050228\n",
      "Average test loss: 0.0025294708320870996\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0018594397172952692\n",
      "Average test loss: 0.0025471634132166704\n",
      "Epoch 128/300\n",
      "Average training loss: 0.001857340636663139\n",
      "Average test loss: 0.0025743913952675133\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0018611413706094027\n",
      "Average test loss: 0.0025071336809131834\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019282812738480666\n",
      "Average test loss: 0.0026284269916100633\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0018527473744211925\n",
      "Average test loss: 0.0024987732871539063\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0018257908390627968\n",
      "Average test loss: 0.002468353814548916\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0018286715927016404\n",
      "Average test loss: 0.002557668923710783\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0018395176456413335\n",
      "Average test loss: 0.0025063185015072424\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0018436054434213373\n",
      "Average test loss: 0.002597152021403114\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0018594102447645532\n",
      "Average test loss: 0.002599035361574756\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018410834445514613\n",
      "Average test loss: 0.0024846674690230026\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0018248335151001811\n",
      "Average test loss: 0.002643349695329865\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0018551194500178098\n",
      "Average test loss: 0.0026004546290884414\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0018180376265404953\n",
      "Average test loss: 0.002570947231310937\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0018113849946401186\n",
      "Average test loss: 0.00247198705168234\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0024090253475846516\n",
      "Average test loss: 0.006096829248799218\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0018614405331512291\n",
      "Average test loss: 0.002510538514289591\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0018046795801363058\n",
      "Average test loss: 0.002511591743470894\n",
      "Epoch 145/300\n",
      "Average training loss: 0.001796430214929084\n",
      "Average test loss: 0.002480493855973085\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0018136059414181444\n",
      "Average test loss: 0.0026021593132366737\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0018030704700698456\n",
      "Average test loss: 0.0025419827846603262\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0018073755930074387\n",
      "Average test loss: 0.002625202333347665\n",
      "Epoch 149/300\n",
      "Average training loss: 0.001815899070041875\n",
      "Average test loss: 0.002615430851156513\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018092728904965851\n",
      "Average test loss: 0.0025455413363460037\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001832691312353644\n",
      "Average test loss: 0.0025716745332918234\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0018023628767372833\n",
      "Average test loss: 0.0025223390647313663\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0018094683241926962\n",
      "Average test loss: 0.0025566476177838115\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0038762899063941505\n",
      "Average test loss: 0.0028331018932577635\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0025582913234829903\n",
      "Average test loss: 0.0026055289113687146\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002293361561993758\n",
      "Average test loss: 0.002516754946981867\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002167331002238724\n",
      "Average test loss: 0.0024354502140647837\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0020687542732598053\n",
      "Average test loss: 0.002495595538367828\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0019839151882463033\n",
      "Average test loss: 0.0024298134259879587\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0019588621283570923\n",
      "Average test loss: 0.0024984600450843573\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018708023346132702\n",
      "Average test loss: 0.0024643588695261215\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0018213400585162971\n",
      "Average test loss: 0.0025187224257323475\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001862601556504766\n",
      "Average test loss: 0.0027254182918825085\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0017947645124254956\n",
      "Average test loss: 0.002533680288121104\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0017838921931882699\n",
      "Average test loss: 0.002513172306223876\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0017811569976102974\n",
      "Average test loss: 0.0025027404234020246\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0018036579150292609\n",
      "Average test loss: 0.0031711430972855953\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0017905945671308373\n",
      "Average test loss: 0.0025642933510243893\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0018190716112860374\n",
      "Average test loss: 0.002577237823771106\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0017909532990306616\n",
      "Average test loss: 0.0028287463595883713\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0017880134747053186\n",
      "Average test loss: 0.002537272797173096\n",
      "Epoch 172/300\n",
      "Average training loss: 0.001788030279489855\n",
      "Average test loss: 0.0025688050438960395\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0017979937528984414\n",
      "Average test loss: 0.002489096775030096\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0017899519545543526\n",
      "Average test loss: 0.00255352950323787\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0017821337222639058\n",
      "Average test loss: 0.0025816224312616718\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0017869633751817875\n",
      "Average test loss: 0.0025092192178385125\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0018163791524453295\n",
      "Average test loss: 0.002553373882960942\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0017806241379843817\n",
      "Average test loss: 0.0025403167485362954\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0017807636433798406\n",
      "Average test loss: 0.0025858428919066988\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0018293346125218604\n",
      "Average test loss: 0.0025099261194053623\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001815121783874929\n",
      "Average test loss: 0.0025456915077649886\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0017667635237384174\n",
      "Average test loss: 0.002580980870872736\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0017662114362335867\n",
      "Average test loss: 0.0025060193188902406\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0017677219631150364\n",
      "Average test loss: 0.0026055449868241944\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0017746406108554867\n",
      "Average test loss: 0.003079200181282229\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0017778826883683603\n",
      "Average test loss: 0.0025722286670158308\n",
      "Epoch 187/300\n",
      "Average training loss: 0.001758002332618667\n",
      "Average test loss: 0.0025195943964645266\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0017817926085036662\n",
      "Average test loss: 0.0025385718225604957\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0017676089944110977\n",
      "Average test loss: 0.0025661568141852817\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001760880254726443\n",
      "Average test loss: 0.0026122365666346415\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0017678552533810336\n",
      "Average test loss: 0.00254752237122092\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0017921262834635046\n",
      "Average test loss: 0.0025614534347421594\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0017561321720066998\n",
      "Average test loss: 0.002509580432747801\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0019116130194937189\n",
      "Average test loss: 0.005711291924946838\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0017591441351299485\n",
      "Average test loss: 0.002549182784019245\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0017480734904400178\n",
      "Average test loss: 0.0026178884587975014\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0017437743433854646\n",
      "Average test loss: 0.0025959730409085753\n",
      "Epoch 198/300\n",
      "Average training loss: 0.001741070105176833\n",
      "Average test loss: 0.0025368981864303352\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0017863270190233986\n",
      "Average test loss: 0.0037286920363290444\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0017485214665325152\n",
      "Average test loss: 0.002596070286093487\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0017488774408379363\n",
      "Average test loss: 0.002627843764300148\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0017429859953828983\n",
      "Average test loss: 0.00255723237618804\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0017877426474458642\n",
      "Average test loss: 0.0025152470463265975\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0017355670284272895\n",
      "Average test loss: 0.0025606468955261838\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0018124690685007306\n",
      "Average test loss: 0.0025756223444930383\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0017374449843126866\n",
      "Average test loss: 0.0025329568152212436\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0018584749955270026\n",
      "Average test loss: 0.0025720392502844333\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0018150990205920406\n",
      "Average test loss: 0.0025078015443351534\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0017238365364157491\n",
      "Average test loss: 0.0025210470443384515\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0017210216276968518\n",
      "Average test loss: 0.0025683540350033176\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0017612045435234905\n",
      "Average test loss: 0.002607313466568788\n",
      "Epoch 212/300\n",
      "Average training loss: 0.001726767372339964\n",
      "Average test loss: 0.002492995699039764\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0017289310369847548\n",
      "Average test loss: 0.0025282642946889\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0017321611364475556\n",
      "Average test loss: 0.0027770241362353168\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001732537866777016\n",
      "Average test loss: 0.0025104293154759537\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0017437180823956927\n",
      "Average test loss: 0.0026012900380624664\n",
      "Epoch 217/300\n",
      "Average training loss: 0.001732225476246741\n",
      "Average test loss: 0.002582847611254288\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0017221744772460724\n",
      "Average test loss: 0.002744903835778435\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0017351036707663702\n",
      "Average test loss: 0.002648504221191009\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0017245088503178624\n",
      "Average test loss: 0.002558273982670572\n",
      "Epoch 221/300\n",
      "Average training loss: 0.001740601300365395\n",
      "Average test loss: 0.0025733329388830397\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0017315343458515903\n",
      "Average test loss: 0.0027433225096513826\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0017189727446271313\n",
      "Average test loss: 0.002540895377182298\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0017284916901133126\n",
      "Average test loss: 0.002612251188192103\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0017389732155327995\n",
      "Average test loss: 0.0025632858135634\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0017225279087821643\n",
      "Average test loss: 0.002673981047131949\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0017363109229546455\n",
      "Average test loss: 0.002977650713382496\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001731276182250844\n",
      "Average test loss: 0.002610623861973484\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0017287114451949796\n",
      "Average test loss: 0.002625324044376612\n",
      "Epoch 230/300\n",
      "Average training loss: 0.001778446453726954\n",
      "Average test loss: 0.002575264525703258\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0017061597881838678\n",
      "Average test loss: 0.002565418359099163\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0017078099377039406\n",
      "Average test loss: 0.002502201015750567\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0017244816364513503\n",
      "Average test loss: 0.0026317544171793592\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0017208285541791056\n",
      "Average test loss: 0.0025507484268811015\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0017040730498524175\n",
      "Average test loss: 0.0025836396703703534\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0017079690255017745\n",
      "Average test loss: 0.002588769589240352\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001710595281380746\n",
      "Average test loss: 0.002629899110127654\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0017034258280570309\n",
      "Average test loss: 0.0027427279766028124\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0017097372110519145\n",
      "Average test loss: 0.0025867054325838883\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0017169378352248005\n",
      "Average test loss: 0.0026582278391967216\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0017023772750463751\n",
      "Average test loss: 0.0025270618761165276\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0017481334931734535\n",
      "Average test loss: 0.0025573505109383\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0017350375696809756\n",
      "Average test loss: 0.0028034158166911865\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0016901849667645163\n",
      "Average test loss: 0.002606863876183828\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0016971135234667195\n",
      "Average test loss: 0.0026103906917075317\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0017606130028143525\n",
      "Average test loss: 0.0026582608121550745\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0017136064028988281\n",
      "Average test loss: 0.0027639926249782244\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0016834446020010446\n",
      "Average test loss: 0.0026801103096869257\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0016886831029421753\n",
      "Average test loss: 0.002572215564445489\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0016893002734933462\n",
      "Average test loss: 0.002650692058934106\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0017105422843661574\n",
      "Average test loss: 0.0026624329762740266\n",
      "Epoch 252/300\n",
      "Average training loss: 0.001695698386679093\n",
      "Average test loss: 0.0025852956268936395\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0016911104797489114\n",
      "Average test loss: 0.0025430330467513867\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0017002978872300851\n",
      "Average test loss: 0.002562398536958628\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0016975457450995842\n",
      "Average test loss: 0.0025657022762008838\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0017318088359509905\n",
      "Average test loss: 0.002673958838192953\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0016979898370595443\n",
      "Average test loss: 0.002512810305174854\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0016880136956978176\n",
      "Average test loss: 0.0026153551204543975\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0016790577688564857\n",
      "Average test loss: 0.0025596945521732173\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0016827849628817703\n",
      "Average test loss: 0.0025595737580830853\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0016846604623521368\n",
      "Average test loss: 0.002562768314033747\n",
      "Epoch 262/300\n",
      "Average training loss: 0.001732140847792228\n",
      "Average test loss: 0.002657023586643239\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0016814676786048545\n",
      "Average test loss: 0.0025419778452358314\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0016739403701697786\n",
      "Average test loss: 0.0026678294425623284\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0016791246900748876\n",
      "Average test loss: 0.002589996871124539\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0016876438309748967\n",
      "Average test loss: 0.002591767123796874\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0016947968441786037\n",
      "Average test loss: 0.0026819342558996543\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0016832359000626537\n",
      "Average test loss: 0.0026459297235641214\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0016926007212864028\n",
      "Average test loss: 0.00259309496358037\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0016775532971239752\n",
      "Average test loss: 0.002635048546310928\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0016756986418945922\n",
      "Average test loss: 0.002686451200602783\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0017078078704782658\n",
      "Average test loss: 0.002605654258177512\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0016719206675059265\n",
      "Average test loss: 0.002668750731067525\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0016714715482667088\n",
      "Average test loss: 0.0026551811742699808\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0016989701236080792\n",
      "Average test loss: 0.002543736747776469\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0016687658359814022\n",
      "Average test loss: 0.002679106577920417\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0016661753828326861\n",
      "Average test loss: 0.002650940624790059\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0017017706799217396\n",
      "Average test loss: 0.002706471455179983\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0016690688656849993\n",
      "Average test loss: 0.002675931862038043\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0016675311870252092\n",
      "Average test loss: 0.0025877494373255307\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0016678599879766504\n",
      "Average test loss: 0.002706126819468207\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0016879650810733438\n",
      "Average test loss: 0.0026346719854821762\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0016647574266211854\n",
      "Average test loss: 0.0031335672663731708\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0016682774560112092\n",
      "Average test loss: 0.002561619439886676\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0016620921367365453\n",
      "Average test loss: 0.0025646873023360967\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0028272434870402017\n",
      "Average test loss: 0.002716046736058262\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0022796419718199307\n",
      "Average test loss: 0.002493930629764994\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002049355246954494\n",
      "Average test loss: 0.0024983877771430547\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0019151755101564857\n",
      "Average test loss: 0.0025236238760666715\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0018132988024089072\n",
      "Average test loss: 0.0025591606497764588\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0017349695017975237\n",
      "Average test loss: 0.002572216283529997\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0016864133231962722\n",
      "Average test loss: 0.0025689264484163787\n",
      "Epoch 293/300\n",
      "Average training loss: 0.001657762975535459\n",
      "Average test loss: 0.0026123677115473484\n",
      "Epoch 294/300\n",
      "Average training loss: 0.001645602634590533\n",
      "Average test loss: 0.0026458275746554134\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0016431025799570813\n",
      "Average test loss: 0.0026792814897166357\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0016407144795068437\n",
      "Average test loss: 0.002638662873663836\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0016443779940406482\n",
      "Average test loss: 0.0025481438607805303\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0016517138708796765\n",
      "Average test loss: 0.002601008921965129\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0016617751785864433\n",
      "Average test loss: 0.002671428510919213\n",
      "Epoch 300/300\n",
      "Average training loss: 0.001661753435412215\n",
      "Average test loss: 0.0025522429855126473\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.019123689711921747\n",
      "Average test loss: 0.006108471412211656\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0050524100261843865\n",
      "Average test loss: 0.00480593146632115\n",
      "Epoch 3/300\n",
      "Average training loss: 0.004269110755787955\n",
      "Average test loss: 0.004275647206024991\n",
      "Epoch 4/300\n",
      "Average training loss: 0.003833384839610921\n",
      "Average test loss: 0.004640856097348862\n",
      "Epoch 5/300\n",
      "Average training loss: 0.003521596018017994\n",
      "Average test loss: 0.00346713300421834\n",
      "Epoch 6/300\n",
      "Average training loss: 0.00327211099728528\n",
      "Average test loss: 0.003208423368839754\n",
      "Epoch 7/300\n",
      "Average training loss: 0.003095294421952632\n",
      "Average test loss: 0.0030444527856177753\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0029277188788271614\n",
      "Average test loss: 0.0028612123417357605\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0027846455971399942\n",
      "Average test loss: 0.002742989744991064\n",
      "Epoch 10/300\n",
      "Average training loss: 0.002660219693970349\n",
      "Average test loss: 0.002721028777874178\n",
      "Epoch 11/300\n",
      "Average training loss: 0.002536437371124824\n",
      "Average test loss: 0.0025089972983631825\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0024635674347066217\n",
      "Average test loss: 0.0028282845694985654\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0023685725087092984\n",
      "Average test loss: 0.0023476919618745646\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002299067099682159\n",
      "Average test loss: 0.0025327108910100328\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0022060188873567516\n",
      "Average test loss: 0.002249360027205613\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002155637865368691\n",
      "Average test loss: 0.002250621863019963\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0021021617501974107\n",
      "Average test loss: 0.0021365414245261088\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0020470585773388543\n",
      "Average test loss: 0.0022420600240843163\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0020026357184267708\n",
      "Average test loss: 0.0021258178503356048\n",
      "Epoch 20/300\n",
      "Average training loss: 0.001965984771959484\n",
      "Average test loss: 0.002074563668316437\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0019430061894365482\n",
      "Average test loss: 0.0020258300786630975\n",
      "Epoch 22/300\n",
      "Average training loss: 0.001898137496577369\n",
      "Average test loss: 0.002029598957548539\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0018632570000158416\n",
      "Average test loss: 0.0019477921648778848\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0018346523144799802\n",
      "Average test loss: 0.0020456406289918556\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0018316347137507466\n",
      "Average test loss: 0.001916765444808536\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0017909152906181085\n",
      "Average test loss: 0.0019184263605210516\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0017701433940480152\n",
      "Average test loss: 0.0019182623198462857\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0017510772121863232\n",
      "Average test loss: 0.0020565055310726166\n",
      "Epoch 29/300\n",
      "Average training loss: 0.001729845852798058\n",
      "Average test loss: 0.0018675271082255575\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0017162891347996063\n",
      "Average test loss: 0.0019041915355871122\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0017013250889463558\n",
      "Average test loss: 0.0018170449124235246\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0016796456904460987\n",
      "Average test loss: 0.0018585584831113616\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0016770548052671883\n",
      "Average test loss: 0.0018377923402521346\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0016671060697279042\n",
      "Average test loss: 0.0019060225542634726\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0016563825664213962\n",
      "Average test loss: 0.0018535564144452413\n",
      "Epoch 36/300\n",
      "Average training loss: 0.001636368546117511\n",
      "Average test loss: 0.0018462155881441302\n",
      "Epoch 37/300\n",
      "Average training loss: 0.001634450536945628\n",
      "Average test loss: 0.001823818652683662\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0016506644157278868\n",
      "Average test loss: 0.0018036614507436753\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0016250470922225051\n",
      "Average test loss: 0.0017944965780609184\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0015991372263265981\n",
      "Average test loss: 0.0017907142537749476\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0016003789904837807\n",
      "Average test loss: 0.0017626846506156855\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0015859087363092436\n",
      "Average test loss: 0.0017961238194257022\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0015762720990719066\n",
      "Average test loss: 0.0017619269424014622\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0015731352088559005\n",
      "Average test loss: 0.0017448622439470555\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0015772919305082824\n",
      "Average test loss: 0.0017898588410268227\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0015658247580544815\n",
      "Average test loss: 0.0018054389014012283\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0015589218065142631\n",
      "Average test loss: 0.0017913799029257561\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0015749418389879996\n",
      "Average test loss: 0.0017783053811225625\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0015385505858187874\n",
      "Average test loss: 0.0017472250993467039\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0015317559394364556\n",
      "Average test loss: 0.0017495558481249545\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0015284487002839645\n",
      "Average test loss: 0.0017758068037736748\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0015247545659335124\n",
      "Average test loss: 0.001742347191191382\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0015221941059248316\n",
      "Average test loss: 0.0017418360674960746\n",
      "Epoch 54/300\n",
      "Average training loss: 0.001510586937992937\n",
      "Average test loss: 0.0018081492294246952\n",
      "Epoch 55/300\n",
      "Average training loss: 0.001521718157455325\n",
      "Average test loss: 0.001842870308086276\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0015008328555979663\n",
      "Average test loss: 0.0017449499219655991\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0015075579539148344\n",
      "Average test loss: 0.0017403896869056755\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0014964970941137936\n",
      "Average test loss: 0.001776156724948022\n",
      "Epoch 59/300\n",
      "Average training loss: 0.001494526838262876\n",
      "Average test loss: 0.001741192939070364\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0014805201897915038\n",
      "Average test loss: 0.001947790182299084\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0014903289404594236\n",
      "Average test loss: 0.001774331265129149\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0014786112745189004\n",
      "Average test loss: 0.00197194679826498\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0014811742394748662\n",
      "Average test loss: 0.001879192538973358\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0014718001359659764\n",
      "Average test loss: 0.001957174057347907\n",
      "Epoch 65/300\n",
      "Average training loss: 0.001479219120202793\n",
      "Average test loss: 0.001739050915464759\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0014753079757922226\n",
      "Average test loss: 0.0017186697675949996\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0018100371144505012\n",
      "Average test loss: 0.0017955413522819678\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0015020558364275429\n",
      "Average test loss: 0.0017500798104123937\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0014641262168685596\n",
      "Average test loss: 0.0017864646924038727\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0014502487284027868\n",
      "Average test loss: 0.0017345604106990828\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0014463560849221216\n",
      "Average test loss: 0.001728950665642818\n",
      "Epoch 72/300\n",
      "Average training loss: 0.001448021563804812\n",
      "Average test loss: 0.0017817519549280405\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0014470611210498545\n",
      "Average test loss: 0.0017371229810329775\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0014502791326699985\n",
      "Average test loss: 0.001796643785925375\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0014486274959312545\n",
      "Average test loss: 0.002567524064746168\n",
      "Epoch 76/300\n",
      "Average training loss: 0.001452958461207648\n",
      "Average test loss: 0.0017282816980861955\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0014360285099181865\n",
      "Average test loss: 0.0017671139171967904\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0014319263340180947\n",
      "Average test loss: 0.0017621715004659361\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0014305835083747904\n",
      "Average test loss: 0.0017434632931318547\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0014278066755375929\n",
      "Average test loss: 0.0018369576604002052\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0014418600999439755\n",
      "Average test loss: 0.001776721900742915\n",
      "Epoch 82/300\n",
      "Average training loss: 0.001431312313510312\n",
      "Average test loss: 0.0017312944036804968\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0014119720293415917\n",
      "Average test loss: 0.0017678544991132285\n",
      "Epoch 84/300\n",
      "Average training loss: 0.001414217040874064\n",
      "Average test loss: 0.001784778287427293\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0014103889203526908\n",
      "Average test loss: 0.0017316157968921793\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0014127105292346743\n",
      "Average test loss: 0.0017391230355327328\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0014117739955480729\n",
      "Average test loss: 0.001941065140792893\n",
      "Epoch 88/300\n",
      "Average training loss: 0.001404850056498415\n",
      "Average test loss: 0.0017433897983282804\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0014074470680207013\n",
      "Average test loss: 0.0017860176242474053\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0014011463893370496\n",
      "Average test loss: 0.0031175861064758567\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0014092105600155061\n",
      "Average test loss: 0.0017703385568327375\n",
      "Epoch 92/300\n",
      "Average training loss: 0.001392814670378963\n",
      "Average test loss: 0.0017563153508429726\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0013948987775171796\n",
      "Average test loss: 0.0017361541765017641\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0013898590871960752\n",
      "Average test loss: 0.0018093666616413328\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0013920761349921426\n",
      "Average test loss: 0.0018230940078695615\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0014068903270074062\n",
      "Average test loss: 0.001840680309985247\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0013971526071222293\n",
      "Average test loss: 0.0017946240287274122\n",
      "Epoch 98/300\n",
      "Average training loss: 0.001377031547224356\n",
      "Average test loss: 0.0017970449158714876\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0013785406230017543\n",
      "Average test loss: 0.0018571874710420768\n",
      "Epoch 100/300\n",
      "Average training loss: 0.001372327503748238\n",
      "Average test loss: 0.0017702214881363842\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001380033639052676\n",
      "Average test loss: 0.0017447958442692955\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0013736823062515922\n",
      "Average test loss: 0.0018420782608704436\n",
      "Epoch 103/300\n",
      "Average training loss: 0.001387981444183323\n",
      "Average test loss: 0.001758660583756864\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0013653714896386697\n",
      "Average test loss: 0.00178259567088551\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0013682445274252031\n",
      "Average test loss: 0.0017389484728789991\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0013710549388908679\n",
      "Average test loss: 0.0017766004227515724\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0013615325691385403\n",
      "Average test loss: 0.001739393461909559\n",
      "Epoch 108/300\n",
      "Average training loss: 0.001364343591551814\n",
      "Average test loss: 0.001750560328985254\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0013670284831896423\n",
      "Average test loss: 0.0017410093351370758\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0013597007087535328\n",
      "Average test loss: 0.0024616773181284466\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0013572242808424764\n",
      "Average test loss: 0.0018908843569871452\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0013505951354487074\n",
      "Average test loss: 0.0018303163163363933\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0013551771544540923\n",
      "Average test loss: 0.0017519840037243234\n",
      "Epoch 114/300\n",
      "Average training loss: 0.001358857471599347\n",
      "Average test loss: 0.001768162674581011\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0013543384687768089\n",
      "Average test loss: 0.001830079449340701\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0013505482340438499\n",
      "Average test loss: 0.0018308943702528874\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0013550591674736804\n",
      "Average test loss: 0.0017933647149345941\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0013386816820129752\n",
      "Average test loss: 0.0017684292853292491\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0013391325976699591\n",
      "Average test loss: 0.0017555275248984496\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0013384865581368406\n",
      "Average test loss: 0.0017653789113586148\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0013419769165209598\n",
      "Average test loss: 0.0017863090041403968\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0013388356839617093\n",
      "Average test loss: 0.0026723286339806187\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0013423243371459344\n",
      "Average test loss: 0.001847250949487918\n",
      "Epoch 124/300\n",
      "Average training loss: 0.001341297653193275\n",
      "Average test loss: 0.0017531406775944762\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0013344904650002718\n",
      "Average test loss: 0.0017922734780651\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0013292364841327071\n",
      "Average test loss: 0.0018257595444512037\n",
      "Epoch 127/300\n",
      "Average training loss: 0.001332444340404537\n",
      "Average test loss: 0.001787128674487273\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0013241786277439032\n",
      "Average test loss: 0.0018203751236821213\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0013240772035593789\n",
      "Average test loss: 0.0018480502754035922\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0013271263072060213\n",
      "Average test loss: 0.001792317908257246\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0013247547489590942\n",
      "Average test loss: 0.0017731060867922173\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0013212008605607683\n",
      "Average test loss: 0.0018199592118875849\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0013700959390650193\n",
      "Average test loss: 0.001769857733303474\n",
      "Epoch 134/300\n",
      "Average training loss: 0.001311784866421173\n",
      "Average test loss: 0.0018240205162308282\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0013132713754764862\n",
      "Average test loss: 0.0019225650420412422\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0013165123345744278\n",
      "Average test loss: 0.001846705608483818\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0013158190748654307\n",
      "Average test loss: 0.001805773376280235\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0013237749970414572\n",
      "Average test loss: 0.0017783936518761846\n",
      "Epoch 139/300\n",
      "Average training loss: 0.001311385332710213\n",
      "Average test loss: 0.0018167677409946918\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0013138859533808297\n",
      "Average test loss: 0.0018756342784812053\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001307307601802879\n",
      "Average test loss: 0.0017880910980618663\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0013221952145298322\n",
      "Average test loss: 0.0018098190766241814\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0013173451630605591\n",
      "Average test loss: 0.0017687900399582254\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0013027463251103957\n",
      "Average test loss: 0.0018021730896499422\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0013026545605518752\n",
      "Average test loss: 0.0017800336342511905\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0013010521874659592\n",
      "Average test loss: 0.0018060403255124888\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0013042469816282392\n",
      "Average test loss: 0.0017844838700774644\n",
      "Epoch 148/300\n",
      "Average training loss: 0.001309239805986484\n",
      "Average test loss: 0.0017921377211395238\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0012985195903521445\n",
      "Average test loss: 0.001768324253666732\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0012991914761563141\n",
      "Average test loss: 0.0018090681404703192\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0013133231315554843\n",
      "Average test loss: 0.0018023324927522075\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0012938924106872744\n",
      "Average test loss: 0.0017727978521337113\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0012907842121397456\n",
      "Average test loss: 0.0017724740536262593\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0012881081830710172\n",
      "Average test loss: 0.001779183001878361\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0012962421792455845\n",
      "Average test loss: 0.0022217609343222447\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0013104531383141876\n",
      "Average test loss: 0.0018221113783203893\n",
      "Epoch 157/300\n",
      "Average training loss: 0.001289167437185016\n",
      "Average test loss: 0.0018416233124832313\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001290264953341749\n",
      "Average test loss: 0.0018726813447558217\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00128702712788557\n",
      "Average test loss: 0.0018546937772383292\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0012871937851628495\n",
      "Average test loss: 0.0018273325289289157\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0012842036999969018\n",
      "Average test loss: 0.0017902592228104671\n",
      "Epoch 162/300\n",
      "Average training loss: 0.001283841415722337\n",
      "Average test loss: 0.0017643318593295084\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0012854144228622317\n",
      "Average test loss: 0.0017746939383861092\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0012815566146746277\n",
      "Average test loss: 0.0018181650128939913\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0012784923905920652\n",
      "Average test loss: 0.0018252926398482589\n",
      "Epoch 166/300\n",
      "Average training loss: 0.001292009465292924\n",
      "Average test loss: 0.001845903602739175\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0012783461151023706\n",
      "Average test loss: 0.001772253026564916\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0012807539080580076\n",
      "Average test loss: 0.001844203915860918\n",
      "Epoch 169/300\n",
      "Average training loss: 0.001279258429693679\n",
      "Average test loss: 0.0018876108324362172\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0012748949004130232\n",
      "Average test loss: 0.0018495853764729368\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0012768924372891584\n",
      "Average test loss: 0.0020093801490341625\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0012724077525652117\n",
      "Average test loss: 0.0018191160269909435\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0012783831280345718\n",
      "Average test loss: 0.0018567420571214623\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0012702190943269266\n",
      "Average test loss: 0.001821979032829404\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0012718588646708263\n",
      "Average test loss: 0.0018257534413908918\n",
      "Epoch 176/300\n",
      "Average training loss: 0.001272971328244441\n",
      "Average test loss: 0.0017896425344256892\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0012753343749791384\n",
      "Average test loss: 0.0017989085511200958\n",
      "Epoch 178/300\n",
      "Average training loss: 0.001271699336098714\n",
      "Average test loss: 0.001868430802702076\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0012710289756457012\n",
      "Average test loss: 0.001837572349442376\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0012719455194763012\n",
      "Average test loss: 0.0017862864604426755\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0012656960704674324\n",
      "Average test loss: 0.0018269964904627867\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0012640454919180936\n",
      "Average test loss: 0.0018545216934548485\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001263200412090454\n",
      "Average test loss: 0.0017989453453984526\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0012678656063766943\n",
      "Average test loss: 0.0018268299924416676\n",
      "Epoch 185/300\n",
      "Average training loss: 0.001263076447053916\n",
      "Average test loss: 0.0019786377733366357\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0012661305187890927\n",
      "Average test loss: 0.001823618468311098\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0012637530783605245\n",
      "Average test loss: 0.0018362461662747795\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0012585897783024443\n",
      "Average test loss: 0.0020876939944509004\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0012611695694633657\n",
      "Average test loss: 0.0019270641907221742\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001260339283798304\n",
      "Average test loss: 0.0019025909971031878\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0012565430659386846\n",
      "Average test loss: 0.0018394225096950928\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001254374885155509\n",
      "Average test loss: 0.00189459088527494\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0012605963919518723\n",
      "Average test loss: 0.0017971356691171725\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0012570785358548165\n",
      "Average test loss: 0.0018595843735254474\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0012507542147197658\n",
      "Average test loss: 0.0018088527652952407\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0012561595775704417\n",
      "Average test loss: 0.0018193299562359849\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0012540557890509565\n",
      "Average test loss: 0.0018730089933507972\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0012517007510695192\n",
      "Average test loss: 0.001796928659081459\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0012544636359024378\n",
      "Average test loss: 0.006379902169936233\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0016134377635704975\n",
      "Average test loss: 0.0018729300290449627\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0012597040224613415\n",
      "Average test loss: 0.0018077033035871055\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00122962266082565\n",
      "Average test loss: 0.0017978382499681579\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0012300048487053977\n",
      "Average test loss: 0.001867842398169968\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0012313230413322647\n",
      "Average test loss: 0.0018767434743543465\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0012368674026802182\n",
      "Average test loss: 0.0018154966893295446\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0012427802418565584\n",
      "Average test loss: 0.0018786426660501294\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0012494780520080691\n",
      "Average test loss: 0.0018626205296152169\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0012399178097645441\n",
      "Average test loss: 0.001846845929717852\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0012480639576291045\n",
      "Average test loss: 0.0017930550430383947\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0012443065996178323\n",
      "Average test loss: 0.001953184625133872\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0012438022489142087\n",
      "Average test loss: 0.0019038800342629353\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0012476236043084\n",
      "Average test loss: 0.001821567920036614\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001242261367643045\n",
      "Average test loss: 0.002008893685725828\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0012434806590899826\n",
      "Average test loss: 0.0017936716091094746\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001244613775673012\n",
      "Average test loss: 0.0018366067212902838\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001238158457705544\n",
      "Average test loss: 0.0018313330796857674\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0012455302368228635\n",
      "Average test loss: 0.0018194339715151323\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0012426162965906162\n",
      "Average test loss: 0.0017888867194867796\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0012349255825910303\n",
      "Average test loss: 0.001884804672251145\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0012329547688778905\n",
      "Average test loss: 0.0018593958465175496\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0012397225625399087\n",
      "Average test loss: 0.0018673302414309648\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0012399910872077776\n",
      "Average test loss: 0.001840807099516193\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0012371322631628977\n",
      "Average test loss: 0.00188462591295441\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0012388285814474026\n",
      "Average test loss: 0.001986486143949959\n",
      "Epoch 225/300\n",
      "Average training loss: 0.001235036454266972\n",
      "Average test loss: 0.0018173610149986215\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0012357582297797005\n",
      "Average test loss: 0.0018813973781135348\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0012329074118493332\n",
      "Average test loss: 0.001870280437792341\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0012358136107731197\n",
      "Average test loss: 0.0018276747475481695\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0012335087266854114\n",
      "Average test loss: 0.0017845018430509502\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0012316640735500388\n",
      "Average test loss: 0.001876985584696134\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0012343610032565064\n",
      "Average test loss: 0.0018855010322812531\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0012285525309853255\n",
      "Average test loss: 0.0018848358436177174\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0012282118171246515\n",
      "Average test loss: 0.0018708492534028158\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0012260816622939375\n",
      "Average test loss: 0.0018055799482390284\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0012443041711424788\n",
      "Average test loss: 0.0018776332481453817\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001222489211190906\n",
      "Average test loss: 0.0018885227871230908\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001224030094117754\n",
      "Average test loss: 0.0018869502743085226\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0012264524925396675\n",
      "Average test loss: 0.0018540197256952525\n",
      "Epoch 239/300\n",
      "Average training loss: 0.001220900335866544\n",
      "Average test loss: 0.0018694589147344232\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0012291868693298763\n",
      "Average test loss: 0.0018320425167265865\n",
      "Epoch 241/300\n",
      "Average training loss: 0.001226369876673238\n",
      "Average test loss: 0.0018739265645336773\n",
      "Epoch 242/300\n",
      "Average training loss: 0.001221899032799734\n",
      "Average test loss: 0.003077578061156803\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00123486745926655\n",
      "Average test loss: 0.0018521835668426421\n",
      "Epoch 244/300\n",
      "Average training loss: 0.001226275112066004\n",
      "Average test loss: 0.0018815395852757825\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0012176243224077754\n",
      "Average test loss: 0.0018367741062409349\n",
      "Epoch 246/300\n",
      "Average training loss: 0.001220579761494365\n",
      "Average test loss: 0.0018488794828040733\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0012210161614542206\n",
      "Average test loss: 0.0018198755899858144\n",
      "Epoch 248/300\n",
      "Average training loss: 0.001227604405851\n",
      "Average test loss: 0.0018128271522000433\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0012296038212047684\n",
      "Average test loss: 0.001790579449488885\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0012206682474869821\n",
      "Average test loss: 0.001946933813082675\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0012180529025176334\n",
      "Average test loss: 0.0018650958920932478\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0012185059299485552\n",
      "Average test loss: 0.0018347363386096226\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0012202450045798387\n",
      "Average test loss: 0.001902108533721831\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0012158304589490096\n",
      "Average test loss: 0.0018476054075484475\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0012246980911327734\n",
      "Average test loss: 0.002097706560976803\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0012300022212374541\n",
      "Average test loss: 0.0018861233340576292\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0012122596998492049\n",
      "Average test loss: 0.0019271908963306083\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0012103147842507395\n",
      "Average test loss: 0.0018937778257661396\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0012136024088184867\n",
      "Average test loss: 0.0018530799637859066\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0012136391622738706\n",
      "Average test loss: 0.001868234066085683\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0012163773947912785\n",
      "Average test loss: 0.0018738432294792598\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0012193966980816589\n",
      "Average test loss: 0.0018227021438587043\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0012113608667213055\n",
      "Average test loss: 0.0017968812667661243\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0012267504312718908\n",
      "Average test loss: 0.001832759493548009\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0012038323737473952\n",
      "Average test loss: 0.001918007765379217\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0012096261167898774\n",
      "Average test loss: 0.001829626289713714\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0012226612294196255\n",
      "Average test loss: 0.0019122982729847231\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0012067162956421575\n",
      "Average test loss: 0.0018613498797640205\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0012484177022965418\n",
      "Average test loss: 0.0018360789918030302\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0012015781009362804\n",
      "Average test loss: 0.0018438073127633996\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0012025801681189074\n",
      "Average test loss: 0.0018683731892249651\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0012093651921798786\n",
      "Average test loss: 0.0018384422034853035\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0012074689247334996\n",
      "Average test loss: 0.0018201587771375974\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0012031687174199356\n",
      "Average test loss: 0.0018897333695656723\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0012054845897687806\n",
      "Average test loss: 0.0018757372396066784\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0012027255618530842\n",
      "Average test loss: 0.0031384383984324007\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0012064607522139946\n",
      "Average test loss: 0.0018671730647070541\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0012048552596113748\n",
      "Average test loss: 0.0018329365592863825\n",
      "Epoch 279/300\n",
      "Average training loss: 0.001202872040681541\n",
      "Average test loss: 0.0018250203520680467\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0012072552119692167\n",
      "Average test loss: 0.001869701322995954\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0012064132580740584\n",
      "Average test loss: 0.0019227960432569185\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0012045734360193214\n",
      "Average test loss: 0.002016204813081357\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0012029549242514703\n",
      "Average test loss: 0.00190140729273359\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0012030060504459672\n",
      "Average test loss: 0.0018697340866654284\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0012013728129160073\n",
      "Average test loss: 0.0018639197842114502\n",
      "Epoch 286/300\n",
      "Average training loss: 0.001199855000918938\n",
      "Average test loss: 0.001816143332152731\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0012006295238518052\n",
      "Average test loss: 0.001890649712127116\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0012028354216987887\n",
      "Average test loss: 0.0018379432467950714\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0012006103366406428\n",
      "Average test loss: 0.001796477518354853\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0011988344297019973\n",
      "Average test loss: 0.0019077758234408166\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0011996364706299372\n",
      "Average test loss: 0.0018073437629888456\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0012028507607885533\n",
      "Average test loss: 0.0019836045391857624\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0011983674954002103\n",
      "Average test loss: 0.0018764512225364646\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0011952103467451202\n",
      "Average test loss: 0.0018727224622335698\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0011955401045787666\n",
      "Average test loss: 0.0018459218545920318\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0012014873558655382\n",
      "Average test loss: 0.001874899814112319\n",
      "Epoch 297/300\n",
      "Average training loss: 0.001196442752248711\n",
      "Average test loss: 0.0018844098984781238\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0011974419500264857\n",
      "Average test loss: 0.0018546217817606197\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0012018808639711804\n",
      "Average test loss: 0.001917377219018009\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0011954187974333762\n",
      "Average test loss: 0.0018751340809588632\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Skip/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 13.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 17.22\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 22.27\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.33\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 15.14\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.89\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.36\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.95\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 16.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.46\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 17.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.41\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.86\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.3106202600797017\n",
      "Average test loss: 0.393417951769299\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08813568393389384\n",
      "Average test loss: 0.12402670395374298\n",
      "Epoch 3/300\n",
      "Average training loss: 0.062315275169081155\n",
      "Average test loss: 0.008973819703691535\n",
      "Epoch 4/300\n",
      "Average training loss: 0.050085102412435745\n",
      "Average test loss: 0.010508933617422979\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04297097849514749\n",
      "Average test loss: 0.02871487447619438\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03825470705164803\n",
      "Average test loss: 0.011413269514838854\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03483988769683573\n",
      "Average test loss: 0.00795614686111609\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03246852999925613\n",
      "Average test loss: 0.008482577554467652\n",
      "Epoch 9/300\n",
      "Average training loss: 0.030737378040949503\n",
      "Average test loss: 0.007068001337349415\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02918275382452541\n",
      "Average test loss: 0.006868757369617621\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027855871995290122\n",
      "Average test loss: 0.014169764238099257\n",
      "Epoch 12/300\n",
      "Average training loss: 0.026833800120486154\n",
      "Average test loss: 0.013761956501752139\n",
      "Epoch 13/300\n",
      "Average training loss: 0.026061230215761396\n",
      "Average test loss: 0.08244393618570434\n",
      "Epoch 14/300\n",
      "Average training loss: 0.025329653145538436\n",
      "Average test loss: 0.006880727081663079\n",
      "Epoch 15/300\n",
      "Average training loss: 0.024558386988110012\n",
      "Average test loss: 0.006591811023652553\n",
      "Epoch 16/300\n",
      "Average training loss: 0.023586568247940806\n",
      "Average test loss: 0.008544564182559649\n",
      "Epoch 17/300\n",
      "Average training loss: 0.022969502164257898\n",
      "Average test loss: 0.010493803339699905\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02235882208744685\n",
      "Average test loss: 0.00690616144405471\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021709285366866324\n",
      "Average test loss: 0.014557072694102923\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021347839256127675\n",
      "Average test loss: 0.13050842393438022\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020757165413763788\n",
      "Average test loss: 0.006436827433605989\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020466417296065225\n",
      "Average test loss: 0.008991840284317732\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020832761593990855\n",
      "Average test loss: 0.0057573128996623885\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01973973601559798\n",
      "Average test loss: 0.006052470398445924\n",
      "Epoch 25/300\n",
      "Average training loss: 0.019461635001831585\n",
      "Average test loss: 0.005614955507632759\n",
      "Epoch 26/300\n",
      "Average training loss: 0.019114254886905352\n",
      "Average test loss: 0.013131802082061767\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021111700534820557\n",
      "Average test loss: 0.0061377227807210555\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01917255804936091\n",
      "Average test loss: 0.03339628143691354\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019191334502564536\n",
      "Average test loss: 0.5105224666512674\n",
      "Epoch 30/300\n",
      "Average training loss: 0.018482465248968866\n",
      "Average test loss: 0.006997657790573107\n",
      "Epoch 31/300\n",
      "Average training loss: 0.018028481971886424\n",
      "Average test loss: 0.005982075850168864\n",
      "Epoch 32/300\n",
      "Average training loss: 0.018015059360199506\n",
      "Average test loss: 0.012653523883057965\n",
      "Epoch 33/300\n",
      "Average training loss: 0.017659049060609607\n",
      "Average test loss: 0.005429087743990951\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017519612941477032\n",
      "Average test loss: 0.005449282220254341\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01737902726067437\n",
      "Average test loss: 0.021551497602628335\n",
      "Epoch 36/300\n",
      "Average training loss: 0.017130706117384962\n",
      "Average test loss: 0.007377182883520921\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019414286881685255\n",
      "Average test loss: 0.03528188009228971\n",
      "Epoch 38/300\n",
      "Average training loss: 0.017897765871551303\n",
      "Average test loss: 0.019736922552188237\n",
      "Epoch 39/300\n",
      "Average training loss: 0.017226501380403835\n",
      "Average test loss: 0.005305912053419484\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01700586566163434\n",
      "Average test loss: 0.005460871222946379\n",
      "Epoch 41/300\n",
      "Average training loss: 0.017236057185464435\n",
      "Average test loss: 0.04901889627840784\n",
      "Epoch 42/300\n",
      "Average training loss: 0.016938883918854925\n",
      "Average test loss: 0.005413990888744592\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019930263509353\n",
      "Average test loss: 0.12400272784050968\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017283306852810914\n",
      "Average test loss: 0.015391977723273966\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016794655899206798\n",
      "Average test loss: 0.005240353068129884\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016612981403867404\n",
      "Average test loss: 0.005634061415990194\n",
      "Epoch 47/300\n",
      "Average training loss: 0.016597727944453557\n",
      "Average test loss: 0.00527288469299674\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0164518732668625\n",
      "Average test loss: 0.011390633318987157\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01646413850618733\n",
      "Average test loss: 0.009383698317739698\n",
      "Epoch 50/300\n",
      "Average training loss: 0.017303120172686048\n",
      "Average test loss: 0.008750676279266675\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01724299161301719\n",
      "Average test loss: 0.0077312483592993685\n",
      "Epoch 52/300\n",
      "Average training loss: 0.016282544899317953\n",
      "Average test loss: 0.00526412747365733\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01610018431726429\n",
      "Average test loss: 0.016591262242860266\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01605207198030419\n",
      "Average test loss: 0.005502098765224219\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017561481068531674\n",
      "Average test loss: 0.009386026730553972\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01642439891397953\n",
      "Average test loss: 0.01630973423189587\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015932460149129232\n",
      "Average test loss: 0.005559120625257492\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01577542981836531\n",
      "Average test loss: 193.52388444889917\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015938497165011035\n",
      "Average test loss: 0.005199432515435748\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01559906694624159\n",
      "Average test loss: 0.018463932446721527\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015541468323932753\n",
      "Average test loss: 0.007115537204676204\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015781904813316133\n",
      "Average test loss: 0.0174872290280958\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015452799105809795\n",
      "Average test loss: 0.007197170469909906\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015380503846539391\n",
      "Average test loss: 0.005235243178076214\n",
      "Epoch 65/300\n",
      "Average training loss: 0.022515115990406936\n",
      "Average test loss: 0.10228192388307718\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016781885143783357\n",
      "Average test loss: 0.006112743980354733\n",
      "Epoch 67/300\n",
      "Average training loss: 0.016521826186113888\n",
      "Average test loss: 10329.736625278314\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01632207086516751\n",
      "Average test loss: 1.8356084773660535\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015503132175240252\n",
      "Average test loss: 0.005145333203590579\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015602110119329559\n",
      "Average test loss: 0.005217105756824215\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015236666491462124\n",
      "Average test loss: 0.0051799258117874464\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01522684759232733\n",
      "Average test loss: 0.00521437275575267\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015072059421075715\n",
      "Average test loss: 0.005190473564383056\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01500532000263532\n",
      "Average test loss: 0.005964293377887872\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01743549894955423\n",
      "Average test loss: 44.74064114777247\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015840302348136902\n",
      "Average test loss: 16.790370460530124\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015066999855968688\n",
      "Average test loss: 0.802622910887003\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015052384028832117\n",
      "Average test loss: 0.00960133898920483\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014825969788763257\n",
      "Average test loss: 0.0054151988849043846\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014777033453186353\n",
      "Average test loss: 0.005840271292461289\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014687338444093864\n",
      "Average test loss: 0.051977256301376555\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014700202372339037\n",
      "Average test loss: 4488416.152333333\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015720077036983436\n",
      "Average test loss: 0.0064580796625879075\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0147182611177365\n",
      "Average test loss: 36.21077332811637\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014745148442685605\n",
      "Average test loss: 0.005210116816270683\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014485893811616633\n",
      "Average test loss: 0.00537810963475042\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014544112015101645\n",
      "Average test loss: 8700.100159633366\n",
      "Epoch 88/300\n",
      "Average training loss: 0.016036136007143393\n",
      "Average test loss: 8.199612977617317\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014459806328018507\n",
      "Average test loss: 0.005247720347924365\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014281682895289527\n",
      "Average test loss: 0.00529056832070152\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014307134873337216\n",
      "Average test loss: 0.0051818154632217355\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014195809454553657\n",
      "Average test loss: 0.005301814736591445\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01419641296648317\n",
      "Average test loss: 0.015061232109450632\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018090115969379744\n",
      "Average test loss: 0.012683797357810868\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015180015883511967\n",
      "Average test loss: 3.4727870217379597\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015893987528151937\n",
      "Average test loss: 0.6918568594074912\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014882513179547257\n",
      "Average test loss: 14.268719346761703\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014367251430948575\n",
      "Average test loss: 0.005271155858205425\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014188893560734061\n",
      "Average test loss: 0.0052738497791190945\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01410741758843263\n",
      "Average test loss: 0.008838085713899798\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01428050649828381\n",
      "Average test loss: 635203.6934210749\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014266461469233035\n",
      "Average test loss: 0.005506997790187597\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014001078706648615\n",
      "Average test loss: 4018095.630147817\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013876967906951905\n",
      "Average test loss: 587562.695625434\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01399172591831949\n",
      "Average test loss: 0.009416300886207157\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013789353946844737\n",
      "Average test loss: 2645.086535060189\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01374815191825231\n",
      "Average test loss: 0.005414526408745183\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013713344185716576\n",
      "Average test loss: 0.006184567713489135\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013826146140694618\n",
      "Average test loss: 0.00557147831759519\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013599434679581059\n",
      "Average test loss: 0.005484464451877607\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015748034068279796\n",
      "Average test loss: 0.0067416236491666896\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01374977727068795\n",
      "Average test loss: 0.005559246826916933\n",
      "Epoch 113/300\n",
      "Average training loss: 0.013509164852400621\n",
      "Average test loss: 0.005557665667186181\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013885906211203999\n",
      "Average test loss: 0.005338487375113699\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013500562778777547\n",
      "Average test loss: 2250.8300819329907\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013482516936129995\n",
      "Average test loss: 29.845512344110343\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013368761250542269\n",
      "Average test loss: 0.006623931491540538\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013371832941969235\n",
      "Average test loss: 0.2251680533107784\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013337435106436412\n",
      "Average test loss: 206.3410512615906\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014882146319581402\n",
      "Average test loss: 0.06965308008756903\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013959641412728362\n",
      "Average test loss: 0.02639326109695766\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015157509941193793\n",
      "Average test loss: 215.27659173094565\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013441344117952718\n",
      "Average test loss: 8.280582157808459\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0132099264504181\n",
      "Average test loss: 0.005856694285240438\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013199018725090557\n",
      "Average test loss: 6.7821101706077656\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013150873616337776\n",
      "Average test loss: 0.007746522124442789\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013137088340189722\n",
      "Average test loss: 0.00564481964127885\n",
      "Epoch 128/300\n",
      "Average training loss: 0.013121865628494156\n",
      "Average test loss: 7.141265526572863\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013142889082431794\n",
      "Average test loss: 0.005325405564159155\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013103313625686699\n",
      "Average test loss: 0.005468342139489121\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01305878103358878\n",
      "Average test loss: 15.095805186798176\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013175197245346175\n",
      "Average test loss: 0.005627600560999579\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01298347013692061\n",
      "Average test loss: 0.005813358977023098\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01298278831773334\n",
      "Average test loss: 0.005452226664457056\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012956874516275195\n",
      "Average test loss: 0.012822260151306789\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013077320684161451\n",
      "Average test loss: 0.005698030206064383\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012985509026381705\n",
      "Average test loss: 0.005858533333159156\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012811060320172044\n",
      "Average test loss: 0.4192811699840758\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012919919866654608\n",
      "Average test loss: 17418633403.32075\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01941596757868926\n",
      "Average test loss: 504974044089.751\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016497370892100863\n",
      "Average test loss: 1.4415288690775634\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014674751778443655\n",
      "Average test loss: 0.005630215730104181\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013957369979057048\n",
      "Average test loss: 0.025546223908662798\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013576684006386333\n",
      "Average test loss: 0.03462326369682948\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014510606021516853\n",
      "Average test loss: 7.226864669074615\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013943292094601526\n",
      "Average test loss: 24657.745628862158\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013021340183913708\n",
      "Average test loss: 1099.134251954715\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01286453004512522\n",
      "Average test loss: 110.24144531886776\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012797796472079225\n",
      "Average test loss: 2.834826586988237\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01275821017805073\n",
      "Average test loss: 0.005442196123715904\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012745322734945351\n",
      "Average test loss: 1.287802908734729\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012703469295468596\n",
      "Average test loss: 8.264683341592136\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01650570806281434\n",
      "Average test loss: 1780.2006417232287\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014466861641241445\n",
      "Average test loss: 17.2786460268543\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013409649324913819\n",
      "Average test loss: 0.12753854052556884\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01290384341776371\n",
      "Average test loss: 0.005532388722731007\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012711436487734318\n",
      "Average test loss: 0.00537253026622865\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015219289114077886\n",
      "Average test loss: 5266273.156399386\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017434247167574034\n",
      "Average test loss: 38.78595694933873\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015394312628441386\n",
      "Average test loss: 13135242884.664888\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016524306494328712\n",
      "Average test loss: 322539926724.4169\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01486677302999629\n",
      "Average test loss: 1192.0218521516738\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01410672083000342\n",
      "Average test loss: 0.005249790690839291\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01451400802201695\n",
      "Average test loss: 15927.067989592513\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013452404657171832\n",
      "Average test loss: 48220921933.824\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01675903977536493\n",
      "Average test loss: 0.006710654877126217\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014363265939884715\n",
      "Average test loss: 0.9281276140544149\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013617465775046083\n",
      "Average test loss: 0.005404574739850229\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013616799008515146\n",
      "Average test loss: 0.009162724603795342\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013093998672233688\n",
      "Average test loss: 0.005369663569455345\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013888384320669704\n",
      "Average test loss: 0.009342571425769065\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012985053002006478\n",
      "Average test loss: 0.00541369081619713\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012823698187867801\n",
      "Average test loss: 0.008379964811934365\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012796414009398883\n",
      "Average test loss: 0.6825481788867878\n",
      "Epoch 175/300\n",
      "Average training loss: 0.012721055991119808\n",
      "Average test loss: 0.07551441059758265\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01272410612139437\n",
      "Average test loss: 0.0547739938315418\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012840505676964918\n",
      "Average test loss: 0.005373412500239081\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012626942736407121\n",
      "Average test loss: 0.005391600843932894\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015271084853344493\n",
      "Average test loss: 5235.7536513299665\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013462285720639759\n",
      "Average test loss: 96012.0309469592\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01595278909388516\n",
      "Average test loss: 4141.106640503703\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015697499595582486\n",
      "Average test loss: 3.813472207259801\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013752788530455695\n",
      "Average test loss: 1749.0568571993676\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013431588691969712\n",
      "Average test loss: 0.12901032830402254\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014007632467481825\n",
      "Average test loss: 2.417342989640517\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012999188913239373\n",
      "Average test loss: 1918.9789967668023\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01333832161873579\n",
      "Average test loss: 1.3064589700483613\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013515090948177709\n",
      "Average test loss: 1193.8303585789238\n",
      "Epoch 189/300\n",
      "Average training loss: 0.012736995466881328\n",
      "Average test loss: 0.16189419293238058\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013036637278066742\n",
      "Average test loss: 89.26709788398527\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013476266627510388\n",
      "Average test loss: 0.0075934343578086955\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012681204106244777\n",
      "Average test loss: 106387.1385997888\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016936103641986847\n",
      "Average test loss: 1409.1079042813133\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014387405505610837\n",
      "Average test loss: 0.2732274131089863\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01379782182392147\n",
      "Average test loss: 5.4811514178580705\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01338140609032578\n",
      "Average test loss: 14439634.232147666\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013151723170446024\n",
      "Average test loss: 2059.3153779067693\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013764058168563577\n",
      "Average test loss: 558.2824359179934\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014119469352066517\n",
      "Average test loss: 1141.7310639758011\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01308364641914765\n",
      "Average test loss: 7734.210465053113\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01296171136531565\n",
      "Average test loss: 4.709346191111538\n",
      "Epoch 202/300\n",
      "Average training loss: 0.012670049148301284\n",
      "Average test loss: 0.005521524496376514\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012739319389065107\n",
      "Average test loss: 636.5354506694691\n",
      "Epoch 204/300\n",
      "Average training loss: 0.012624751675046153\n",
      "Average test loss: 4.943412736790048\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012569421684576405\n",
      "Average test loss: 5141.056967627923\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01338111383964618\n",
      "Average test loss: 0.6141491164399517\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012589299333592256\n",
      "Average test loss: 2.135663881551681\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013516635218428242\n",
      "Average test loss: 0.005412488236195511\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013119634601804946\n",
      "Average test loss: 525.0440152456082\n",
      "Epoch 210/300\n",
      "Average training loss: 0.012525787453684541\n",
      "Average test loss: 2647.2263447322084\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01238768871790833\n",
      "Average test loss: 1.7058417613903682\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013325456565452946\n",
      "Average test loss: 312.63325102418577\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012433188655310207\n",
      "Average test loss: 712.4548226213025\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012368808767033947\n",
      "Average test loss: 242.69089112507842\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012428291464431418\n",
      "Average test loss: 0.006099276331563791\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015739073704514238\n",
      "Average test loss: 15343.890376609086\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013814239398472839\n",
      "Average test loss: 5208.448401234381\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014939958321551481\n",
      "Average test loss: 0.005420838276959128\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015567882978253894\n",
      "Average test loss: 0.010217036890486876\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017900662721859084\n",
      "Average test loss: 55097.25788167049\n",
      "Epoch 221/300\n",
      "Average training loss: 0.015253569412562583\n",
      "Average test loss: 0.008758089309765233\n",
      "Epoch 222/300\n",
      "Average training loss: 0.027448727597792943\n",
      "Average test loss: 47124331.82478941\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018299493380718762\n",
      "Average test loss: 3617.7539545414998\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016984936131371392\n",
      "Average test loss: 919002.8280334852\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01813562142766184\n",
      "Average test loss: 2762675.657785205\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01608161021851831\n",
      "Average test loss: 3950159.163430536\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015440491265720791\n",
      "Average test loss: 948069423.1453631\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015038519942926036\n",
      "Average test loss: 1.2330662928875057e+18\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014795120185448063\n",
      "Average test loss: 280678948.8918391\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014741863809525966\n",
      "Average test loss: 53006903050.2581\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014388021387159824\n",
      "Average test loss: 76.29777428359786\n",
      "Epoch 232/300\n",
      "Average training loss: 0.015511652901768685\n",
      "Average test loss: 1113415389.720433\n",
      "Epoch 233/300\n",
      "Average training loss: 0.015192923500306077\n",
      "Average test loss: 532.2488546387967\n",
      "Epoch 234/300\n",
      "Average training loss: 0.015032442479497856\n",
      "Average test loss: 25919284.91592796\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014334742713305685\n",
      "Average test loss: 35163.86955091618\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01401026963028643\n",
      "Average test loss: 74974504.5740082\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014579678342574173\n",
      "Average test loss: 4500661.640383046\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013516547409196694\n",
      "Average test loss: 0.11384755196505122\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01313502990288867\n",
      "Average test loss: 437200.26288189355\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013044502216080825\n",
      "Average test loss: 103.98974549318126\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012960683710045285\n",
      "Average test loss: 0.011015381699221002\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012814521490699716\n",
      "Average test loss: 2646.863994464325\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012734864763915539\n",
      "Average test loss: 630.7613910985932\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013104233697884613\n",
      "Average test loss: 2395.749587952236\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012907183814379905\n",
      "Average test loss: 2609.202229879305\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013164643705719048\n",
      "Average test loss: 34750.11685453442\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012721983243193891\n",
      "Average test loss: 5404131.440100694\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013462261143657896\n",
      "Average test loss: 2161.237244146316\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012586673301127222\n",
      "Average test loss: 14268507522.494223\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01279077294551664\n",
      "Average test loss: 103729767996496.81\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0152377809451686\n",
      "Average test loss: 0.00531715598454078\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013474458515644074\n",
      "Average test loss: 38121.42578325738\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012583606388833787\n",
      "Average test loss: 1366.7211624195643\n",
      "Epoch 254/300\n",
      "Average training loss: 0.012956038226683935\n",
      "Average test loss: 0.011935181463758151\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012690658597482576\n",
      "Average test loss: 60526.703653930665\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012581562050514751\n",
      "Average test loss: 50377.28995305273\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01482461479306221\n",
      "Average test loss: 258433.28928707694\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012930491588181919\n",
      "Average test loss: 146.48554852975738\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012351178271902932\n",
      "Average test loss: 9020685.871183667\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01218875408338176\n",
      "Average test loss: 72183.88214307417\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012241238436765141\n",
      "Average test loss: 0.0054847192466259\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012174257208903631\n",
      "Average test loss: 0.005603446801089578\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012190730071730084\n",
      "Average test loss: 5.18075321639412\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013647872127592563\n",
      "Average test loss: 0.10170643222332\n",
      "Epoch 265/300\n",
      "Average training loss: 0.012154124268227153\n",
      "Average test loss: 0.05441847118900882\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012047158296737406\n",
      "Average test loss: 0.02012495223246515\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012042045373883512\n",
      "Average test loss: 0.19333695037497414\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012029982517162958\n",
      "Average test loss: 19309.605748070637\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012021378174424172\n",
      "Average test loss: 0.07447418688237667\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013628632901443375\n",
      "Average test loss: 0.03001236465987232\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012145214440921943\n",
      "Average test loss: 0.04012136593874958\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01221611276600096\n",
      "Average test loss: 481211.4514879142\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015009579011135631\n",
      "Average test loss: 0.006005955844289727\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01327210053553184\n",
      "Average test loss: 0.005693266089591715\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012595603216853407\n",
      "Average test loss: 210.468087257598\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01213616668474343\n",
      "Average test loss: 0.005537158163057433\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012040211126208305\n",
      "Average test loss: 0.008778193731274869\n",
      "Epoch 278/300\n",
      "Average training loss: 0.011969063692622715\n",
      "Average test loss: 0.008321577339122694\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011948837995529175\n",
      "Average test loss: 16914969.960084397\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012013517141342163\n",
      "Average test loss: 3276707872951501.0\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012093932798339261\n",
      "Average test loss: 55569.273038844745\n",
      "Epoch 282/300\n",
      "Average training loss: 0.011915349721080727\n",
      "Average test loss: 0.10788038269347615\n",
      "Epoch 283/300\n",
      "Average training loss: 0.011927279036078188\n",
      "Average test loss: 0.008422264608244101\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012869938472906748\n",
      "Average test loss: 0.1485302450687935\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012047988715271156\n",
      "Average test loss: 1393.136572217471\n",
      "Epoch 286/300\n",
      "Average training loss: 0.011845607853598065\n",
      "Average test loss: 0.03928503571823239\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011972151060071256\n",
      "Average test loss: 87.5148053837075\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011904476137624846\n",
      "Average test loss: 0.006412694238126278\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01181835060649448\n",
      "Average test loss: 0.009430892573462591\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011780803360044957\n",
      "Average test loss: 2835776.7469892004\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013136741871635119\n",
      "Average test loss: 5755922.195922316\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013604587384396129\n",
      "Average test loss: 0.0058561391114360754\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012093235023319722\n",
      "Average test loss: 0.007804652530286047\n",
      "Epoch 294/300\n",
      "Average training loss: 0.011779865023990472\n",
      "Average test loss: 0.005655147448182106\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011712298303014703\n",
      "Average test loss: 0.0057010155705114205\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011678902723722987\n",
      "Average test loss: 0.017147274413042597\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0136985347279244\n",
      "Average test loss: 0.046585873800019426\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013006328591869938\n",
      "Average test loss: 0.013714734342777066\n",
      "Epoch 299/300\n",
      "Average training loss: 0.011961382413903873\n",
      "Average test loss: 0.00591509687817759\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011708526321583325\n",
      "Average test loss: 10.357838604393933\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.27710198248757256\n",
      "Average test loss: 0.7919605121910572\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0768738846845097\n",
      "Average test loss: 0.0841149108144972\n",
      "Epoch 3/300\n",
      "Average training loss: 0.051799694389104846\n",
      "Average test loss: 0.006066299367696047\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04041309484508303\n",
      "Average test loss: 0.005703068930655718\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03388122395343251\n",
      "Average test loss: 0.03141989944544103\n",
      "Epoch 6/300\n",
      "Average training loss: 0.029715251442458894\n",
      "Average test loss: 0.004935569414248069\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02673159062034554\n",
      "Average test loss: 0.004858463671265377\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02450510173042615\n",
      "Average test loss: 0.004452607201619281\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02278893224067158\n",
      "Average test loss: 0.006517875135772758\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02135541775657071\n",
      "Average test loss: 0.004781829788039128\n",
      "Epoch 11/300\n",
      "Average training loss: 0.020101406592461797\n",
      "Average test loss: 0.0047217276079787145\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019002707343134615\n",
      "Average test loss: 0.006363110279043516\n",
      "Epoch 13/300\n",
      "Average training loss: 0.018046594728198317\n",
      "Average test loss: 0.019529271863400935\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01726877420064476\n",
      "Average test loss: 0.15211625401675702\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016418344425658385\n",
      "Average test loss: 0.02482750691804621\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015859618774718708\n",
      "Average test loss: 0.06416981275876363\n",
      "Epoch 17/300\n",
      "Average training loss: 0.015267229653894902\n",
      "Average test loss: 0.005491807827519046\n",
      "Epoch 18/300\n",
      "Average training loss: 0.014848805134495099\n",
      "Average test loss: 0.003911684658792284\n",
      "Epoch 19/300\n",
      "Average training loss: 0.014369066173831622\n",
      "Average test loss: 0.0034910690368463596\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01395047896520959\n",
      "Average test loss: 0.003479166224805845\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013497370904518498\n",
      "Average test loss: 0.004601502787735727\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013132693981130919\n",
      "Average test loss: 0.007575574405077431\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012855596738557021\n",
      "Average test loss: 0.004050960230330626\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012517332235972086\n",
      "Average test loss: 0.0032989638240800965\n",
      "Epoch 25/300\n",
      "Average training loss: 0.013133759988678826\n",
      "Average test loss: 0.007041881197856532\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012177360927065213\n",
      "Average test loss: 0.0032587097912198967\n",
      "Epoch 27/300\n",
      "Average training loss: 0.011788038230604595\n",
      "Average test loss: 0.0031598484279173945\n",
      "Epoch 28/300\n",
      "Average training loss: 0.011541515875193809\n",
      "Average test loss: 0.0031612175897591643\n",
      "Epoch 29/300\n",
      "Average training loss: 0.011335605058405135\n",
      "Average test loss: 0.008399297243191136\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011145641992489496\n",
      "Average test loss: 0.0030706367558903163\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01097330637441741\n",
      "Average test loss: 0.03933936311966843\n",
      "Epoch 32/300\n",
      "Average training loss: 0.010807877067890432\n",
      "Average test loss: 0.003114671946813663\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010634234258698092\n",
      "Average test loss: 0.003107673561821381\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010434258536332184\n",
      "Average test loss: 0.003274452068739467\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01031721580442455\n",
      "Average test loss: 0.003654362842026684\n",
      "Epoch 36/300\n",
      "Average training loss: 0.010189532198011875\n",
      "Average test loss: 0.0030714845367603833\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01005105868064695\n",
      "Average test loss: 0.0030079250037670134\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009980913491298754\n",
      "Average test loss: 0.0029976893084951574\n",
      "Epoch 39/300\n",
      "Average training loss: 0.009919271397921774\n",
      "Average test loss: 0.00325489911581907\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009746350850082106\n",
      "Average test loss: 0.0031531798098650243\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009866378819776906\n",
      "Average test loss: 0.00292840839881036\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0095905704928769\n",
      "Average test loss: 0.002945282512861821\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009527519415650103\n",
      "Average test loss: 0.0029149569910433557\n",
      "Epoch 44/300\n",
      "Average training loss: 0.00943416745464007\n",
      "Average test loss: 0.0029484822501738865\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009366893349422348\n",
      "Average test loss: 0.002998086996169554\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009292361734641923\n",
      "Average test loss: 0.002897308990359306\n",
      "Epoch 47/300\n",
      "Average training loss: 0.009222329218768411\n",
      "Average test loss: 0.0030126862693578005\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009235970962378715\n",
      "Average test loss: 0.0029139185500227744\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009108022309011882\n",
      "Average test loss: 0.00289244920346472\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009062139252821604\n",
      "Average test loss: 17881.381800347222\n",
      "Epoch 51/300\n",
      "Average training loss: 0.028007120601005023\n",
      "Average test loss: 0.004788285840716627\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014882527036799325\n",
      "Average test loss: 0.00472605770577987\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012888736676838663\n",
      "Average test loss: 0.05011966029306253\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011188394598662853\n",
      "Average test loss: 0.006073636852825681\n",
      "Epoch 55/300\n",
      "Average training loss: 0.010684454635613494\n",
      "Average test loss: 0.014329317371464438\n",
      "Epoch 56/300\n",
      "Average training loss: 0.010297263840834299\n",
      "Average test loss: 0.013592460132721397\n",
      "Epoch 57/300\n",
      "Average training loss: 0.010000133552485042\n",
      "Average test loss: 0.6174848993023236\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009774670243677166\n",
      "Average test loss: 0.03307731983376046\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009547301810648706\n",
      "Average test loss: 0.0032304429086960026\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009424315550261074\n",
      "Average test loss: 0.003246403165575531\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009323115900158881\n",
      "Average test loss: 0.02251436390976111\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009236624245014455\n",
      "Average test loss: 0.0031428465243015023\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009147704129831659\n",
      "Average test loss: 0.002932633615616295\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009101796735492017\n",
      "Average test loss: 0.0028785597013516557\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009013241808861493\n",
      "Average test loss: 0.03524076926584045\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009275937168962425\n",
      "Average test loss: 0.002970821651319663\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00892306835990813\n",
      "Average test loss: 0.0028702596188005473\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008930725928809908\n",
      "Average test loss: 0.0028980748798284267\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008785241073204412\n",
      "Average test loss: 0.0031800366292397183\n",
      "Epoch 70/300\n",
      "Average training loss: 0.008756379838618968\n",
      "Average test loss: 0.0029612478382057615\n",
      "Epoch 71/300\n",
      "Average training loss: 0.008734584663477209\n",
      "Average test loss: 0.0029253864631884627\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008652137119736936\n",
      "Average test loss: 0.0031134608487288157\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008631894120118683\n",
      "Average test loss: 0.002959612087243133\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008548447504225704\n",
      "Average test loss: 0.002897131432261732\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008527412451803685\n",
      "Average test loss: 0.002905091438442469\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008849259549958839\n",
      "Average test loss: 0.002935195128743847\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014635586644212405\n",
      "Average test loss: 0.003186598741966817\n",
      "Epoch 78/300\n",
      "Average training loss: 0.010775379775298966\n",
      "Average test loss: 0.003082984533160925\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009806824122865996\n",
      "Average test loss: 0.0029505425915122034\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010496814620163705\n",
      "Average test loss: 0.004328847381803724\n",
      "Epoch 81/300\n",
      "Average training loss: 0.009799386531114578\n",
      "Average test loss: 0.0029598141505072515\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009158833501239617\n",
      "Average test loss: 0.0028751193046983744\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008926429227408435\n",
      "Average test loss: 0.0030530302772919338\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008767683612803619\n",
      "Average test loss: 0.0028940929762191242\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008768082967234983\n",
      "Average test loss: 0.0032366806235578326\n",
      "Epoch 86/300\n",
      "Average training loss: 0.008605485635085239\n",
      "Average test loss: 0.0034972129933949973\n",
      "Epoch 87/300\n",
      "Average training loss: 0.008453958091015618\n",
      "Average test loss: 0.002908272698107693\n",
      "Epoch 88/300\n",
      "Average training loss: 0.008482660008387433\n",
      "Average test loss: 0.0029465258014905785\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008380330020354853\n",
      "Average test loss: 0.002998022809624672\n",
      "Epoch 90/300\n",
      "Average training loss: 0.008376583118819529\n",
      "Average test loss: 0.002907528463130196\n",
      "Epoch 91/300\n",
      "Average training loss: 0.008428056274851162\n",
      "Average test loss: 0.005672774748462769\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008290581443657477\n",
      "Average test loss: 0.002902660343175133\n",
      "Epoch 93/300\n",
      "Average training loss: 0.008331682062811322\n",
      "Average test loss: 0.0028782111689862277\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008186739159127076\n",
      "Average test loss: 0.004095475034684771\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008174278647535378\n",
      "Average test loss: 0.003129798860910038\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008156351750509607\n",
      "Average test loss: 0.0029975874330848457\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008705648650311761\n",
      "Average test loss: 0.0032099919356405734\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015388609169258011\n",
      "Average test loss: 0.003353287967749768\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012011996963371832\n",
      "Average test loss: 0.0030741061618965535\n",
      "Epoch 100/300\n",
      "Average training loss: 0.010367548504637347\n",
      "Average test loss: 0.0029300655416316457\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009591649605168237\n",
      "Average test loss: 0.002979668949213293\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009192960002356105\n",
      "Average test loss: 0.0028720410362713866\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008922189780821403\n",
      "Average test loss: 0.0029209455269285376\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00869335166613261\n",
      "Average test loss: 0.0033339490265482004\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008469706148737008\n",
      "Average test loss: 0.002856401255147325\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008312306202120251\n",
      "Average test loss: 0.002967071060712139\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008301936977439457\n",
      "Average test loss: 0.0032656691819429396\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008157311158875625\n",
      "Average test loss: 0.0033898042030632495\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008112073525372479\n",
      "Average test loss: 0.002916344500043326\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008075062813030349\n",
      "Average test loss: 0.0029147092373006875\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008060528908338811\n",
      "Average test loss: 0.0029086177769220537\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008017602624164687\n",
      "Average test loss: 0.0029155852471788725\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008036949628757105\n",
      "Average test loss: 0.003272373805857367\n",
      "Epoch 114/300\n",
      "Average training loss: 0.007947297541217672\n",
      "Average test loss: 3.4395927361912197\n",
      "Epoch 115/300\n",
      "Average training loss: 0.007929484490719107\n",
      "Average test loss: 0.002982937001105812\n",
      "Epoch 116/300\n",
      "Average training loss: 0.007928061000174947\n",
      "Average test loss: 0.003081520768503348\n",
      "Epoch 117/300\n",
      "Average training loss: 0.007882119376212358\n",
      "Average test loss: 0.0029712643855147893\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007902070961064763\n",
      "Average test loss: 0.003008472792700761\n",
      "Epoch 119/300\n",
      "Average training loss: 0.007829449175960488\n",
      "Average test loss: 0.0029079508462713823\n",
      "Epoch 120/300\n",
      "Average training loss: 0.007892342026448913\n",
      "Average test loss: 0.0029798966072913675\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01196010232468446\n",
      "Average test loss: 0.0047258526391039295\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008922050601078405\n",
      "Average test loss: 0.0028724342131366334\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0082677843703164\n",
      "Average test loss: 0.0029182578045874835\n",
      "Epoch 124/300\n",
      "Average training loss: 0.007870217162287897\n",
      "Average test loss: 0.002916935377029909\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012835090923640463\n",
      "Average test loss: 0.002976594620694717\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008995124065213734\n",
      "Average test loss: 0.0028807120939923657\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008431976463645697\n",
      "Average test loss: 0.002861610210604138\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008106687385174963\n",
      "Average test loss: 0.00301830391999748\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007931939196255473\n",
      "Average test loss: 0.002906866791140702\n",
      "Epoch 130/300\n",
      "Average training loss: 0.007837630964815616\n",
      "Average test loss: 0.002911215341132548\n",
      "Epoch 131/300\n",
      "Average training loss: 0.00778115293259422\n",
      "Average test loss: 0.002917119214518203\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007736693882693847\n",
      "Average test loss: 0.0030283851840843755\n",
      "Epoch 133/300\n",
      "Average training loss: 0.00773698307822148\n",
      "Average test loss: 0.002903153154792057\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008778073130796354\n",
      "Average test loss: 0.0031451333854347467\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007796075774149762\n",
      "Average test loss: 0.002978468179702759\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0076494938400056625\n",
      "Average test loss: 0.003119051598219408\n",
      "Epoch 137/300\n",
      "Average training loss: 0.007701769035309553\n",
      "Average test loss: 0.0030155745670199393\n",
      "Epoch 138/300\n",
      "Average training loss: 0.007639645351303948\n",
      "Average test loss: 0.00415312720876601\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007617359933753808\n",
      "Average test loss: 0.002977104294631216\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007633448867748181\n",
      "Average test loss: 0.0033885289571351476\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0076037230408853955\n",
      "Average test loss: 0.0030159926207529173\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007597430314454767\n",
      "Average test loss: 0.0031156895785695977\n",
      "Epoch 143/300\n",
      "Average training loss: 0.007713359323640664\n",
      "Average test loss: 0.003298397950621115\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007889118400712807\n",
      "Average test loss: 0.0029802480830500525\n",
      "Epoch 145/300\n",
      "Average training loss: 0.00774915707235535\n",
      "Average test loss: 0.003332788749701447\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007474324332343208\n",
      "Average test loss: 0.0033804250738273065\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007451672470817963\n",
      "Average test loss: 0.002989284999979039\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007483352556203802\n",
      "Average test loss: 0.016203284634484186\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008122623982114925\n",
      "Average test loss: 0.0029129404810567697\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007730972142269214\n",
      "Average test loss: 0.0035969123794800705\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007407482504844665\n",
      "Average test loss: 0.003045189375264777\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00737942988342709\n",
      "Average test loss: 0.0030321456075956426\n",
      "Epoch 153/300\n",
      "Average training loss: 0.007411464521040519\n",
      "Average test loss: 0.0029871317563164566\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0079700419501298\n",
      "Average test loss: 0.0029662352990772987\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007422919489029381\n",
      "Average test loss: 0.00307053363447388\n",
      "Epoch 156/300\n",
      "Average training loss: 0.007331645890242523\n",
      "Average test loss: 0.002966211735788319\n",
      "Epoch 157/300\n",
      "Average training loss: 0.007317798889345593\n",
      "Average test loss: 0.003362703301426437\n",
      "Epoch 158/300\n",
      "Average training loss: 0.007332635988377863\n",
      "Average test loss: 3.436030789481269\n",
      "Epoch 159/300\n",
      "Average training loss: 0.007317511335015297\n",
      "Average test loss: 0.004121281098367439\n",
      "Epoch 160/300\n",
      "Average training loss: 0.007360498696565628\n",
      "Average test loss: 0.0029343936426772014\n",
      "Epoch 161/300\n",
      "Average training loss: 0.007435387978123294\n",
      "Average test loss: 0.0029985112689642443\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009485089311997095\n",
      "Average test loss: 0.006352469154737062\n",
      "Epoch 163/300\n",
      "Average training loss: 0.007538065391696162\n",
      "Average test loss: 0.003276540584448311\n",
      "Epoch 164/300\n",
      "Average training loss: 0.007236968414237102\n",
      "Average test loss: 0.003076075906554858\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008035389852606588\n",
      "Average test loss: 0.003083202631109291\n",
      "Epoch 166/300\n",
      "Average training loss: 0.007210174735221598\n",
      "Average test loss: 0.0031826417789173624\n",
      "Epoch 167/300\n",
      "Average training loss: 0.007156305094973909\n",
      "Average test loss: 0.003062570007517934\n",
      "Epoch 168/300\n",
      "Average training loss: 0.007177025187760592\n",
      "Average test loss: 0.0193737909015682\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00721016041479177\n",
      "Average test loss: 0.0029725254658195706\n",
      "Epoch 170/300\n",
      "Average training loss: 0.009286079510632489\n",
      "Average test loss: 0.0030667350304623446\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008498265383144219\n",
      "Average test loss: 0.0031105248160246345\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0076456232224073675\n",
      "Average test loss: 0.004203586493515306\n",
      "Epoch 173/300\n",
      "Average training loss: 0.007290598961214224\n",
      "Average test loss: 0.006164613062722815\n",
      "Epoch 174/300\n",
      "Average training loss: 0.007167320240288973\n",
      "Average test loss: 0.0030739269562893444\n",
      "Epoch 175/300\n",
      "Average training loss: 0.007134784925729037\n",
      "Average test loss: 0.002992216869774792\n",
      "Epoch 176/300\n",
      "Average training loss: 0.00750307618578275\n",
      "Average test loss: 0.002953903391129441\n",
      "Epoch 177/300\n",
      "Average training loss: 0.007116171290063196\n",
      "Average test loss: 0.002990783559054964\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007107366029587057\n",
      "Average test loss: 0.0034534688438806267\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0071274422167076\n",
      "Average test loss: 0.5035848703947332\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007879190870871147\n",
      "Average test loss: 0.0030122398858269056\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007109485786822107\n",
      "Average test loss: 0.003033524906469716\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007089491830931769\n",
      "Average test loss: 0.0030741393230855466\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007111899599846866\n",
      "Average test loss: 0.0030899107791483404\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00708004264243775\n",
      "Average test loss: 0.01698458284387986\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007165589041180081\n",
      "Average test loss: 0.0445471111966504\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00724396688822243\n",
      "Average test loss: 0.0030448099022938146\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007074845620327526\n",
      "Average test loss: 0.0031003213367528384\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007117682371702459\n",
      "Average test loss: 0.0030309659789005917\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007033204328682687\n",
      "Average test loss: 0.0030518058650195598\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01154656066828304\n",
      "Average test loss: 0.10083281458583143\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009607979331579473\n",
      "Average test loss: 0.004990397773579591\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008520250518288877\n",
      "Average test loss: 0.0029224088005721568\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007914299927237961\n",
      "Average test loss: 0.0029600605867389175\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0074475626407398115\n",
      "Average test loss: 0.0030354486453450387\n",
      "Epoch 195/300\n",
      "Average training loss: 0.007153784351216422\n",
      "Average test loss: 0.0030005099442270066\n",
      "Epoch 196/300\n",
      "Average training loss: 0.007037835053271718\n",
      "Average test loss: 0.0030463257901784447\n",
      "Epoch 197/300\n",
      "Average training loss: 0.006955658570345905\n",
      "Average test loss: 0.0030312792472541334\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008162666942096419\n",
      "Average test loss: 0.0030775527585711745\n",
      "Epoch 199/300\n",
      "Average training loss: 0.006993637923151255\n",
      "Average test loss: 0.0030840527313864895\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0069359760363068845\n",
      "Average test loss: 0.0030839612442586156\n",
      "Epoch 201/300\n",
      "Average training loss: 0.006927878742830621\n",
      "Average test loss: 0.0031831547946979603\n",
      "Epoch 202/300\n",
      "Average training loss: 0.006961254088828961\n",
      "Average test loss: 0.003025910737199916\n",
      "Epoch 203/300\n",
      "Average training loss: 0.006994722929265764\n",
      "Average test loss: 0.0030755585504488813\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007356191053986549\n",
      "Average test loss: 0.0030315101246039073\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007072870196567642\n",
      "Average test loss: 0.0030077699019263188\n",
      "Epoch 206/300\n",
      "Average training loss: 0.006921409550226397\n",
      "Average test loss: 0.0030940780825912955\n",
      "Epoch 207/300\n",
      "Average training loss: 0.006921401280909777\n",
      "Average test loss: 0.003066878663169013\n",
      "Epoch 208/300\n",
      "Average training loss: 0.006943685622264942\n",
      "Average test loss: 0.00315289658970303\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007009449663675494\n",
      "Average test loss: 0.003186255860659811\n",
      "Epoch 210/300\n",
      "Average training loss: 0.006971004301475154\n",
      "Average test loss: 0.7596676888035403\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0068935678328077\n",
      "Average test loss: 0.0032031431740356816\n",
      "Epoch 212/300\n",
      "Average training loss: 0.006904606486774153\n",
      "Average test loss: 0.003112454578073488\n",
      "Epoch 213/300\n",
      "Average training loss: 0.006962803573658069\n",
      "Average test loss: 0.0031199993662950065\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006917370198501481\n",
      "Average test loss: 0.0036966899927291606\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007319368336763647\n",
      "Average test loss: 0.0030721919205453662\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0068450657828814454\n",
      "Average test loss: 0.0031890004528686406\n",
      "Epoch 217/300\n",
      "Average training loss: 0.006911697951869832\n",
      "Average test loss: 0.0030292422506544327\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006830196763492293\n",
      "Average test loss: 0.0031432163959576025\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0068463772924409975\n",
      "Average test loss: 0.0031526277638557884\n",
      "Epoch 220/300\n",
      "Average training loss: 0.007009354778875907\n",
      "Average test loss: 0.007656823849926392\n",
      "Epoch 221/300\n",
      "Average training loss: 0.006953234540091621\n",
      "Average test loss: 0.0030976685233828094\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006796170688751671\n",
      "Average test loss: 0.010055842785785595\n",
      "Epoch 223/300\n",
      "Average training loss: 0.006840343358202113\n",
      "Average test loss: 0.003218986785867148\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006822061041990916\n",
      "Average test loss: 0.0030495642299453417\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006872663972278436\n",
      "Average test loss: 0.00347236073969139\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00685021069770058\n",
      "Average test loss: 0.003141119103050894\n",
      "Epoch 227/300\n",
      "Average training loss: 0.006843009145309528\n",
      "Average test loss: 0.003053380956666337\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006788007820439008\n",
      "Average test loss: 0.00327462284370429\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006836235908584462\n",
      "Average test loss: 0.0031390020863877404\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007097956402020322\n",
      "Average test loss: 0.0030796723328530787\n",
      "Epoch 231/300\n",
      "Average training loss: 0.006787455208599568\n",
      "Average test loss: 0.003086061699936787\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006726810103903214\n",
      "Average test loss: 0.0034808403961360456\n",
      "Epoch 233/300\n",
      "Average training loss: 0.006730916947126389\n",
      "Average test loss: 0.003032516601185004\n",
      "Epoch 234/300\n",
      "Average training loss: 0.006731691708167394\n",
      "Average test loss: 0.0031062234429021676\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006785330607245366\n",
      "Average test loss: 0.003309379687325822\n",
      "Epoch 236/300\n",
      "Average training loss: 0.006728649389826589\n",
      "Average test loss: 0.005249139578392108\n",
      "Epoch 237/300\n",
      "Average training loss: 0.006774126655525631\n",
      "Average test loss: 0.0031119760380436978\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00674546121723122\n",
      "Average test loss: 0.003933969551490413\n",
      "Epoch 239/300\n",
      "Average training loss: 0.006855925368352069\n",
      "Average test loss: 0.0030902080132315557\n",
      "Epoch 240/300\n",
      "Average training loss: 0.006671354759898451\n",
      "Average test loss: 0.0031368747065878577\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0073462484768695305\n",
      "Average test loss: 0.0031087119314405654\n",
      "Epoch 242/300\n",
      "Average training loss: 0.006675681384487284\n",
      "Average test loss: 0.003869720651043786\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006642680670238203\n",
      "Average test loss: 0.0033193136615057785\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007790424410253763\n",
      "Average test loss: 0.014084859499086937\n",
      "Epoch 245/300\n",
      "Average training loss: 0.006822397251509958\n",
      "Average test loss: 0.003110548747289512\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0072955553428166444\n",
      "Average test loss: 0.003068341019252936\n",
      "Epoch 247/300\n",
      "Average training loss: 0.006616574854486518\n",
      "Average test loss: 0.003093669191830688\n",
      "Epoch 248/300\n",
      "Average training loss: 0.006619817658017079\n",
      "Average test loss: 0.003571961688498656\n",
      "Epoch 249/300\n",
      "Average training loss: 0.006640238208903207\n",
      "Average test loss: 0.0032234457816100784\n",
      "Epoch 250/300\n",
      "Average training loss: 0.006667243644595146\n",
      "Average test loss: 0.004102526467707422\n",
      "Epoch 251/300\n",
      "Average training loss: 0.006739440791722801\n",
      "Average test loss: 0.0032581669377783934\n",
      "Epoch 252/300\n",
      "Average training loss: 0.006866105983240737\n",
      "Average test loss: 0.004896098006516695\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0066452049679226344\n",
      "Average test loss: 0.00306662934501138\n",
      "Epoch 254/300\n",
      "Average training loss: 0.006631285690185096\n",
      "Average test loss: 0.003111827745826708\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0066208185077541405\n",
      "Average test loss: 0.003133153954727782\n",
      "Epoch 256/300\n",
      "Average training loss: 0.00664601868391037\n",
      "Average test loss: 0.003260266135032806\n",
      "Epoch 257/300\n",
      "Average training loss: 0.006826902999232213\n",
      "Average test loss: 0.003222927758677138\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006592028567774428\n",
      "Average test loss: 0.004432464338011212\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008576887886143393\n",
      "Average test loss: 0.003918119973399573\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009324708013484876\n",
      "Average test loss: 3.5011242794493835\n",
      "Epoch 261/300\n",
      "Average training loss: 0.007443809903744194\n",
      "Average test loss: 0.0030502132587134838\n",
      "Epoch 262/300\n",
      "Average training loss: 0.006766109226478471\n",
      "Average test loss: 0.0031946560016108882\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0067421369850635526\n",
      "Average test loss: 0.0070625141962534855\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007098958382589949\n",
      "Average test loss: 0.003245661452412605\n",
      "Epoch 265/300\n",
      "Average training loss: 0.006534023719529311\n",
      "Average test loss: 0.003227540844430526\n",
      "Epoch 266/300\n",
      "Average training loss: 0.006519673504349258\n",
      "Average test loss: 0.0035932801723894147\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0065528188807268935\n",
      "Average test loss: 0.006884420870906777\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006625222650667032\n",
      "Average test loss: 0.0032099057777474323\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006907414787345462\n",
      "Average test loss: 0.0033064743458396857\n",
      "Epoch 270/300\n",
      "Average training loss: 0.006554862197902468\n",
      "Average test loss: 0.0031739821061491967\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006537765859729714\n",
      "Average test loss: 0.0031320699356082414\n",
      "Epoch 272/300\n",
      "Average training loss: 0.006615206352124612\n",
      "Average test loss: 0.0030871859319094156\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0065520150102674965\n",
      "Average test loss: 0.19567121904260582\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006593101798246304\n",
      "Average test loss: 0.03777405893471506\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006593347953425513\n",
      "Average test loss: 0.0032272062504457104\n",
      "Epoch 276/300\n",
      "Average training loss: 0.006543652372227775\n",
      "Average test loss: 0.0031066449547393453\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0068091231518321565\n",
      "Average test loss: 0.00321970132076078\n",
      "Epoch 278/300\n",
      "Average training loss: 0.006508469619270828\n",
      "Average test loss: 0.0031790171306994228\n",
      "Epoch 279/300\n",
      "Average training loss: 0.00652382039775451\n",
      "Average test loss: 0.003449704630093442\n",
      "Epoch 280/300\n",
      "Average training loss: 0.006628058899607923\n",
      "Average test loss: 0.003257729118482934\n",
      "Epoch 281/300\n",
      "Average training loss: 0.006511882782810264\n",
      "Average test loss: 0.0031525060737298593\n",
      "Epoch 282/300\n",
      "Average training loss: 0.006528838240024116\n",
      "Average test loss: 0.003114633025187585\n",
      "Epoch 283/300\n",
      "Average training loss: 0.006560071075956027\n",
      "Average test loss: 0.0032384674083441494\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0065198766038649615\n",
      "Average test loss: 0.006841970590667592\n",
      "Epoch 285/300\n",
      "Average training loss: 0.006751123196548886\n",
      "Average test loss: 0.003332462699773411\n",
      "Epoch 286/300\n",
      "Average training loss: 0.006474007980691062\n",
      "Average test loss: 0.0033079405236575337\n",
      "Epoch 287/300\n",
      "Average training loss: 0.006497549314879709\n",
      "Average test loss: 0.0036264461080233257\n",
      "Epoch 288/300\n",
      "Average training loss: 0.006539968181815413\n",
      "Average test loss: 0.003213983667186565\n",
      "Epoch 289/300\n",
      "Average training loss: 0.006505397746960322\n",
      "Average test loss: 0.003277110115211043\n",
      "Epoch 290/300\n",
      "Average training loss: 0.006506562173780468\n",
      "Average test loss: 0.003125450156422125\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0065340132315953574\n",
      "Average test loss: 0.004778637438184685\n",
      "Epoch 292/300\n",
      "Average training loss: 0.006504178632464674\n",
      "Average test loss: 0.0032383666355162858\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0064684395790100095\n",
      "Average test loss: 0.003363211750984192\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0064697155898643865\n",
      "Average test loss: 0.0031496402627478044\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006555195548468166\n",
      "Average test loss: 0.006686206861916516\n",
      "Epoch 296/300\n",
      "Average training loss: 0.009924615590108765\n",
      "Average test loss: 0.0030152343925502565\n",
      "Epoch 297/300\n",
      "Average training loss: 0.00745823595383101\n",
      "Average test loss: 0.003528573851204581\n",
      "Epoch 298/300\n",
      "Average training loss: 0.006682927670992083\n",
      "Average test loss: 0.003108247711012761\n",
      "Epoch 299/300\n",
      "Average training loss: 0.006457489025261667\n",
      "Average test loss: 0.003138626415696409\n",
      "Epoch 300/300\n",
      "Average training loss: 0.006381941342933311\n",
      "Average test loss: 0.003239488434046507\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2393799200852712\n",
      "Average test loss: 0.014092956363326973\n",
      "Epoch 2/300\n",
      "Average training loss: 0.056353714240921865\n",
      "Average test loss: 0.0053617106224927635\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03776216263903512\n",
      "Average test loss: 0.004636098902051648\n",
      "Epoch 4/300\n",
      "Average training loss: 0.030157385809553995\n",
      "Average test loss: 0.004398659334414535\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02571326572365231\n",
      "Average test loss: 0.004094869534588522\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022637924472490946\n",
      "Average test loss: 0.003588577637448907\n",
      "Epoch 7/300\n",
      "Average training loss: 0.020321082388361295\n",
      "Average test loss: 0.003583120828287469\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018568571458260217\n",
      "Average test loss: 0.004675092223203845\n",
      "Epoch 9/300\n",
      "Average training loss: 0.017147140299280484\n",
      "Average test loss: 0.0038433980486459202\n",
      "Epoch 10/300\n",
      "Average training loss: 0.016027270188762082\n",
      "Average test loss: 0.00309070676813523\n",
      "Epoch 11/300\n",
      "Average training loss: 0.015048623491492536\n",
      "Average test loss: 0.0029065659205532736\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014140194637907876\n",
      "Average test loss: 0.003191438963015874\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013369523959027397\n",
      "Average test loss: 0.002653602348640561\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012671871354182561\n",
      "Average test loss: 0.0030978296448787054\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01208766941477855\n",
      "Average test loss: 0.002621369530757268\n",
      "Epoch 16/300\n",
      "Average training loss: 0.011515990979141659\n",
      "Average test loss: 0.0023986890549874966\n",
      "Epoch 17/300\n",
      "Average training loss: 0.011044568778740035\n",
      "Average test loss: 0.0024142403637783397\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010665441333833669\n",
      "Average test loss: 0.002279205524880025\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01022007005661726\n",
      "Average test loss: 0.0023797201681882143\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00982555501990848\n",
      "Average test loss: 0.002249192188390427\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00953331405338314\n",
      "Average test loss: 0.0025359919206756685\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0092571264811688\n",
      "Average test loss: 0.002264375601998634\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008938014700180954\n",
      "Average test loss: 0.002095716424907247\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008744552785323725\n",
      "Average test loss: 0.002132523198922475\n",
      "Epoch 25/300\n",
      "Average training loss: 0.008487377176268233\n",
      "Average test loss: 0.0022247941128702626\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008292611547642284\n",
      "Average test loss: 0.00207272684853524\n",
      "Epoch 27/300\n",
      "Average training loss: 0.008080921592397823\n",
      "Average test loss: 0.002048837865702808\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007930287794106537\n",
      "Average test loss: 0.0020675738118588922\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00782299475164877\n",
      "Average test loss: 0.0020183207294386294\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0076143612224194735\n",
      "Average test loss: 0.001979407354982363\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0074634533988104925\n",
      "Average test loss: 0.0019816403771854107\n",
      "Epoch 32/300\n",
      "Average training loss: 0.007342418066329426\n",
      "Average test loss: 0.0019970323553101885\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007243837659143739\n",
      "Average test loss: 0.0019354191890193356\n",
      "Epoch 34/300\n",
      "Average training loss: 0.007158866159617901\n",
      "Average test loss: 0.001972078264794416\n",
      "Epoch 35/300\n",
      "Average training loss: 0.007290901258587838\n",
      "Average test loss: 0.0019945599008351564\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009420650675478908\n",
      "Average test loss: 0.0020190582260903385\n",
      "Epoch 37/300\n",
      "Average training loss: 0.007545583132240507\n",
      "Average test loss: 0.001980398772077428\n",
      "Epoch 38/300\n",
      "Average training loss: 0.007288409898264541\n",
      "Average test loss: 0.0020006049593082735\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007114084754139185\n",
      "Average test loss: 0.0025229682551903857\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0069473923510975305\n",
      "Average test loss: 0.0021177645458115473\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006909519731170602\n",
      "Average test loss: 0.0019453152515408066\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0068147980338997315\n",
      "Average test loss: 0.0018953888511492147\n",
      "Epoch 43/300\n",
      "Average training loss: 0.006729406289342377\n",
      "Average test loss: 0.0018939238256878324\n",
      "Epoch 44/300\n",
      "Average training loss: 0.00667276956140995\n",
      "Average test loss: 0.0019049664077659448\n",
      "Epoch 45/300\n",
      "Average training loss: 0.006601144237650765\n",
      "Average test loss: 0.0018966001350846555\n",
      "Epoch 46/300\n",
      "Average training loss: 0.006563801438444191\n",
      "Average test loss: 0.0020101981517962283\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0068444546407295595\n",
      "Average test loss: 0.0019306426639151243\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009153796805689732\n",
      "Average test loss: 0.003546489263160361\n",
      "Epoch 49/300\n",
      "Average training loss: 0.00978599547346433\n",
      "Average test loss: 0.014252164553023047\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007827579454001454\n",
      "Average test loss: 0.003161859553721216\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007312372616181771\n",
      "Average test loss: 0.0020256464439961644\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007069681994203064\n",
      "Average test loss: 0.025723078307178286\n",
      "Epoch 53/300\n",
      "Average training loss: 0.006876624442223046\n",
      "Average test loss: 0.001905666821118858\n",
      "Epoch 54/300\n",
      "Average training loss: 0.006727893192321062\n",
      "Average test loss: 0.001923332546527187\n",
      "Epoch 55/300\n",
      "Average training loss: 0.006627236622903082\n",
      "Average test loss: 0.0019638457852933144\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006545249939378765\n",
      "Average test loss: 0.001894267067934076\n",
      "Epoch 57/300\n",
      "Average training loss: 0.006501405773477422\n",
      "Average test loss: 0.0019350922231872877\n",
      "Epoch 58/300\n",
      "Average training loss: 0.006419230134122902\n",
      "Average test loss: 0.0018775033523432082\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0063941201240652135\n",
      "Average test loss: 0.00199879981070343\n",
      "Epoch 60/300\n",
      "Average training loss: 0.006345151091615359\n",
      "Average test loss: 0.0018742425630076064\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006275608274257845\n",
      "Average test loss: 0.00189811997467445\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006257710114121437\n",
      "Average test loss: 0.0027969955483244527\n",
      "Epoch 63/300\n",
      "Average training loss: 0.006223229445517063\n",
      "Average test loss: 0.001931481939016117\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006179226461384031\n",
      "Average test loss: 0.0018685854735473791\n",
      "Epoch 65/300\n",
      "Average training loss: 0.006127250812119908\n",
      "Average test loss: 0.19344866474469502\n",
      "Epoch 66/300\n",
      "Average training loss: 0.006075710906750626\n",
      "Average test loss: 0.0019931887371672525\n",
      "Epoch 67/300\n",
      "Average training loss: 0.006043109298166301\n",
      "Average test loss: 0.001911597097499503\n",
      "Epoch 68/300\n",
      "Average training loss: 0.006489775199029181\n",
      "Average test loss: 0.0018706380933937099\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006668797916629248\n",
      "Average test loss: 0.00206891546294921\n",
      "Epoch 70/300\n",
      "Average training loss: 0.006002781775676542\n",
      "Average test loss: 0.001844576908275485\n",
      "Epoch 71/300\n",
      "Average training loss: 0.00593486753726999\n",
      "Average test loss: 0.0018958643883880643\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005897518419971068\n",
      "Average test loss: 0.006900526169066628\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005864274096571737\n",
      "Average test loss: 0.002136064812954929\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00586573390621278\n",
      "Average test loss: 0.0021558001219398448\n",
      "Epoch 75/300\n",
      "Average training loss: 0.005906788842131694\n",
      "Average test loss: 0.001891718382636706\n",
      "Epoch 76/300\n",
      "Average training loss: 0.005863275610738331\n",
      "Average test loss: 0.0019258267641481426\n",
      "Epoch 77/300\n",
      "Average training loss: 0.005767492010775539\n",
      "Average test loss: 0.0018602171385039886\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005756903846230772\n",
      "Average test loss: 0.0018853084343589015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00621016459994846\n",
      "Average test loss: 0.001840158742128147\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006234108496457338\n",
      "Average test loss: 0.0018570419326424598\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00580865248810086\n",
      "Average test loss: 0.00195558528860824\n",
      "Epoch 82/300\n",
      "Average training loss: 0.005633434208109975\n",
      "Average test loss: 0.0035808226464109287\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00569192250031564\n",
      "Average test loss: 0.0018870316625883181\n",
      "Epoch 84/300\n",
      "Average training loss: 0.005589581158839994\n",
      "Average test loss: 0.0018775820483764012\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005581870884117153\n",
      "Average test loss: 0.0018564192184971438\n",
      "Epoch 86/300\n",
      "Average training loss: 0.005610460251569748\n",
      "Average test loss: 0.0018586725073142184\n",
      "Epoch 87/300\n",
      "Average training loss: 0.005614315302835571\n",
      "Average test loss: 0.0018975002261706524\n",
      "Epoch 88/300\n",
      "Average training loss: 0.005779049611133006\n",
      "Average test loss: 0.0021742669717512196\n",
      "Epoch 89/300\n",
      "Average training loss: 0.005728197127580643\n",
      "Average test loss: 0.0018737016394734384\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005467141901039415\n",
      "Average test loss: 0.0019183204602450132\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005479879469507271\n",
      "Average test loss: 0.0018726911720716292\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005435011823972066\n",
      "Average test loss: 0.0019532167391023703\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005463985653801097\n",
      "Average test loss: 0.0026739190990726154\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005523575708270073\n",
      "Average test loss: 0.0020350856836885214\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005395360212359163\n",
      "Average test loss: 0.0019643105307800904\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00546008320690857\n",
      "Average test loss: 0.0018782031808255448\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0053804266502459844\n",
      "Average test loss: 0.003101945827404658\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0054387015863839126\n",
      "Average test loss: 0.0021106497970306212\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005455376504609982\n",
      "Average test loss: 0.0018437381765494744\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005313165185352167\n",
      "Average test loss: 0.0018878003120836285\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0052871301383193995\n",
      "Average test loss: 0.0019334825482219457\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005255951248523262\n",
      "Average test loss: 0.001982576521734397\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005315970764805873\n",
      "Average test loss: 0.0019092520930700833\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00521838203817606\n",
      "Average test loss: 0.0019033230906869802\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005238076477828953\n",
      "Average test loss: 0.0019768236288800834\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0052106910035428075\n",
      "Average test loss: 0.0019666857794961995\n",
      "Epoch 107/300\n",
      "Average training loss: 0.005301293417397473\n",
      "Average test loss: 0.002364139213744137\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005259800594300031\n",
      "Average test loss: 0.0019337760588775078\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005172665898584657\n",
      "Average test loss: 0.002010327020039161\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0051716551836580036\n",
      "Average test loss: 0.002025067798793316\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005158569155881802\n",
      "Average test loss: 0.0018801879988362392\n",
      "Epoch 112/300\n",
      "Average training loss: 0.005099478715409835\n",
      "Average test loss: 0.002012747588256995\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005097731780260802\n",
      "Average test loss: 0.0019621141275597944\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005223868232220411\n",
      "Average test loss: 0.0019949521018813054\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005090117953303787\n",
      "Average test loss: 0.0022073306025316318\n",
      "Epoch 116/300\n",
      "Average training loss: 0.005071982325365146\n",
      "Average test loss: 0.0019003319390532042\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005172438581370645\n",
      "Average test loss: 0.002175857504622804\n",
      "Epoch 118/300\n",
      "Average training loss: 0.005164361987262964\n",
      "Average test loss: 0.0019493408583932453\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005032249092641804\n",
      "Average test loss: 0.0026367683191266325\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005025291578637229\n",
      "Average test loss: 0.0019559944884644615\n",
      "Epoch 121/300\n",
      "Average training loss: 0.005048116502248579\n",
      "Average test loss: 0.0019609020484818353\n",
      "Epoch 122/300\n",
      "Average training loss: 0.005036328187005387\n",
      "Average test loss: 0.002102021971717477\n",
      "Epoch 123/300\n",
      "Average training loss: 0.005006864055991172\n",
      "Average test loss: 0.0019073540748407443\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0049739167048699325\n",
      "Average test loss: 0.0019326231572777033\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004997270439234045\n",
      "Average test loss: 0.0019844799674012593\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004953198616703351\n",
      "Average test loss: 0.0019354157062868276\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004971025711546341\n",
      "Average test loss: 0.0019789672232129506\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004976459943172005\n",
      "Average test loss: 0.0022679667917804586\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00497604405052132\n",
      "Average test loss: 0.0019245482090239723\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004909780871536997\n",
      "Average test loss: 0.001976974078764518\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0049038178829683195\n",
      "Average test loss: 0.002070945681590173\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004934188229342302\n",
      "Average test loss: 0.0021586418203595613\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004901341076526377\n",
      "Average test loss: 0.0019279231193164985\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004933092281636265\n",
      "Average test loss: 0.0019790082435227102\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004871206571244531\n",
      "Average test loss: 0.00201696565002203\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004932911516063743\n",
      "Average test loss: 0.002639014578734835\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008485847882098621\n",
      "Average test loss: 0.001959135733027425\n",
      "Epoch 138/300\n",
      "Average training loss: 0.007470087612875634\n",
      "Average test loss: 0.0030330227617588307\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006165659496353732\n",
      "Average test loss: 0.001892849756611718\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0057888106542329\n",
      "Average test loss: 0.004076535956313213\n",
      "Epoch 141/300\n",
      "Average training loss: 0.005540688409573502\n",
      "Average test loss: 0.0018948310531882776\n",
      "Epoch 142/300\n",
      "Average training loss: 0.005338445144395034\n",
      "Average test loss: 0.0019084600261929962\n",
      "Epoch 143/300\n",
      "Average training loss: 0.005151519863141908\n",
      "Average test loss: 0.003069424538148774\n",
      "Epoch 144/300\n",
      "Average training loss: 0.005032382004376915\n",
      "Average test loss: 0.002094789637045728\n",
      "Epoch 145/300\n",
      "Average training loss: 0.00496511689076821\n",
      "Average test loss: 0.00201968982650174\n",
      "Epoch 146/300\n",
      "Average training loss: 0.005227687904818191\n",
      "Average test loss: 0.13505107409424252\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014350057239333788\n",
      "Average test loss: 0.01845391158759594\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007367124987145265\n",
      "Average test loss: 0.0019629161710747415\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006677315929283698\n",
      "Average test loss: 0.0019245894353629813\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006286602322840029\n",
      "Average test loss: 0.0018974641755533715\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0059913963327805205\n",
      "Average test loss: 0.0019440635761453046\n",
      "Epoch 152/300\n",
      "Average training loss: 0.005720145995418231\n",
      "Average test loss: 0.0020279391686328584\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0054489994380209185\n",
      "Average test loss: 0.0019180743143790298\n",
      "Epoch 154/300\n",
      "Average training loss: 0.005188825625512335\n",
      "Average test loss: 0.0019253800107787054\n",
      "Epoch 155/300\n",
      "Average training loss: 0.005010434534400702\n",
      "Average test loss: 0.0020180777437571024\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004899777724511094\n",
      "Average test loss: 0.001962119983302222\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004865864997108778\n",
      "Average test loss: 0.0019473389438870881\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004849017352693611\n",
      "Average test loss: 0.001974513563637932\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004856284964208802\n",
      "Average test loss: 0.00261757832682795\n",
      "Epoch 160/300\n",
      "Average training loss: 0.005035271397067441\n",
      "Average test loss: 0.0019532519828321204\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004882153704762459\n",
      "Average test loss: 0.001984487961563799\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004814596765571171\n",
      "Average test loss: 0.002010089022003942\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004795335827188359\n",
      "Average test loss: 0.001965375360618863\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0048356286494268316\n",
      "Average test loss: 0.0020386863944845067\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004855176167148683\n",
      "Average test loss: 0.002037694702338841\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004798789145631923\n",
      "Average test loss: 0.0019445555514345566\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004785418176402648\n",
      "Average test loss: 0.0020392269209648173\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004770365905016661\n",
      "Average test loss: 0.0019980764732592635\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004752646391590436\n",
      "Average test loss: 0.002281352715359794\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0047705596242513925\n",
      "Average test loss: 0.003133423682095276\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0048751356556183765\n",
      "Average test loss: 0.001974786612101727\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004807279333472252\n",
      "Average test loss: 0.001974593167917596\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004782905348473125\n",
      "Average test loss: 0.002089930825970239\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004734968008266555\n",
      "Average test loss: 0.0019921507687411374\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004702006960908572\n",
      "Average test loss: 0.002102505988130967\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004765957991489106\n",
      "Average test loss: 0.0020549338892516165\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004692524661620458\n",
      "Average test loss: 0.0019407045463513996\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004685037687420845\n",
      "Average test loss: 0.0020433976317031518\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004719477803342872\n",
      "Average test loss: 0.004594141804923614\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0046871602738069165\n",
      "Average test loss: 0.0021965704321240385\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004703145022607512\n",
      "Average test loss: 0.0028949416960693066\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004689928515503804\n",
      "Average test loss: 0.7182541908555561\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004716486136946413\n",
      "Average test loss: 0.002982832543241481\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004680549549973673\n",
      "Average test loss: 0.0019519852478471068\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0046782026597195206\n",
      "Average test loss: 0.0020334593479832013\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004661395671053065\n",
      "Average test loss: 0.002310605570053061\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004685483579420381\n",
      "Average test loss: 0.0019793190229684112\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004625596220708556\n",
      "Average test loss: 0.0020257275013460052\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004615970597498947\n",
      "Average test loss: 1.8682690346505906\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004671114688118299\n",
      "Average test loss: 0.0019926896127354767\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004622615374210808\n",
      "Average test loss: 0.0020867964879920085\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004659464567071862\n",
      "Average test loss: 0.002365192262455821\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004586224785281552\n",
      "Average test loss: 0.002026715066490902\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004612092860043049\n",
      "Average test loss: 0.00201555621665385\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004728239049928056\n",
      "Average test loss: 0.002022342381481495\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00457989376452234\n",
      "Average test loss: 0.00204989217977143\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004569574426859617\n",
      "Average test loss: 0.001987615557801392\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004656442769285706\n",
      "Average test loss: 0.0020893398937251832\n",
      "Epoch 199/300\n",
      "Average training loss: 0.00458710404444072\n",
      "Average test loss: 0.002113867612555623\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004567783812267912\n",
      "Average test loss: 0.002140311773866415\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0045582372061908245\n",
      "Average test loss: 0.0020642381254583598\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004567345725579394\n",
      "Average test loss: 0.002038884714142316\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004545124829643302\n",
      "Average test loss: 0.0030555948755807347\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004670510756058825\n",
      "Average test loss: 0.0020439585571487744\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004574740146597226\n",
      "Average test loss: 0.002021213553638922\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004558043781667948\n",
      "Average test loss: 0.002088138170643813\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004558693098421726\n",
      "Average test loss: 0.0022039588624611496\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004510345463330547\n",
      "Average test loss: 0.0019686647726015914\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004570109848140015\n",
      "Average test loss: 0.002117647005038129\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004525974939266841\n",
      "Average test loss: 0.00202404612240692\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004550482528077232\n",
      "Average test loss: 0.001997324014082551\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004512087603617046\n",
      "Average test loss: 0.002037416735250089\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004507274074066017\n",
      "Average test loss: 0.002141825279427899\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004559414303137196\n",
      "Average test loss: 0.002067905379562742\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004484660438365406\n",
      "Average test loss: 0.002053666179896229\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004505209939554334\n",
      "Average test loss: 0.0020479197101667523\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004502417001045413\n",
      "Average test loss: 0.0021445692827304205\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004539243061095476\n",
      "Average test loss: 0.0020823541298094724\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0045590722732659844\n",
      "Average test loss: 0.0020621871068659758\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004471674899881085\n",
      "Average test loss: 0.002136193735525012\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004504423041724497\n",
      "Average test loss: 0.002026184542518523\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004469164001858897\n",
      "Average test loss: 0.00206515084941768\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0045103580997222\n",
      "Average test loss: 0.0020705854495366416\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004458686324043407\n",
      "Average test loss: 0.002033147196802828\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0044522060306949746\n",
      "Average test loss: 0.0022753116550544897\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00447841274821096\n",
      "Average test loss: 0.0020235564385851224\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00477617728875743\n",
      "Average test loss: 0.002029091845990883\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004799644548859861\n",
      "Average test loss: 0.0020335582215338945\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004463232210940785\n",
      "Average test loss: 0.002084677377508746\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00442182800711857\n",
      "Average test loss: 0.002133815703706609\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004446413672632641\n",
      "Average test loss: 0.0020335361442218223\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0044270985863274995\n",
      "Average test loss: 0.002044302841544979\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004556525491178035\n",
      "Average test loss: 0.0020449629672285585\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004418459443582429\n",
      "Average test loss: 0.0020234095223454966\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004427703497310479\n",
      "Average test loss: 0.0020571018812350103\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004427675446081493\n",
      "Average test loss: 0.0020830148165631624\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004410392779442999\n",
      "Average test loss: 0.0021432198451624977\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004551096052345303\n",
      "Average test loss: 0.0021231514871534376\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004451658930215571\n",
      "Average test loss: 0.0020851416207022136\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004385929259575075\n",
      "Average test loss: 0.002028394375203384\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00450888477348619\n",
      "Average test loss: 0.0020739125564901363\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0044024001525508035\n",
      "Average test loss: 0.00204949713508702\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004392361157056358\n",
      "Average test loss: 0.0020215831520035863\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0046751686678164536\n",
      "Average test loss: 0.0020760939771102536\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004442373599443171\n",
      "Average test loss: 0.002093363802673088\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004381155929010776\n",
      "Average test loss: 0.0020900452501244017\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0043947365052170225\n",
      "Average test loss: 0.0030069984146911238\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004399728575514423\n",
      "Average test loss: 0.0021073283941174547\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004393146794703272\n",
      "Average test loss: 0.0022533020310931736\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004362258822139767\n",
      "Average test loss: 0.0020889648807545505\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004390650074101157\n",
      "Average test loss: 0.00201404212642875\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004392898857593536\n",
      "Average test loss: 0.002109304482738177\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004364248692575428\n",
      "Average test loss: 0.00332998128188774\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0044593027995692355\n",
      "Average test loss: 0.002157293304800987\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004350928354387482\n",
      "Average test loss: 0.0023494199733767244\n",
      "Epoch 256/300\n",
      "Average training loss: 0.007303755821039279\n",
      "Average test loss: 0.0022464592336780497\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007143271227677663\n",
      "Average test loss: 0.0033759193267259334\n",
      "Epoch 258/300\n",
      "Average training loss: 0.005815708199308978\n",
      "Average test loss: 0.0019403939524458515\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0050315600592229105\n",
      "Average test loss: 0.001977469521885117\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0046467108606464335\n",
      "Average test loss: 0.002536212222650647\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004568663310259581\n",
      "Average test loss: 0.004761171773076058\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004452168395535814\n",
      "Average test loss: 0.0021896569984447625\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00437975712865591\n",
      "Average test loss: 0.0020382644635521703\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004353165215295222\n",
      "Average test loss: 0.002089270691180395\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004403607380472951\n",
      "Average test loss: 0.0020239619239129955\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004365094374037451\n",
      "Average test loss: 0.002038191544926829\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004360294910354747\n",
      "Average test loss: 0.0020377689167443247\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004367705983834134\n",
      "Average test loss: 0.0020650020769486825\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004374903964292672\n",
      "Average test loss: 0.002147903659587933\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004406128466957145\n",
      "Average test loss: 0.0022631252934742305\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0043451263714167805\n",
      "Average test loss: 0.001994153909385204\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004362642619137963\n",
      "Average test loss: 0.0021808359497744174\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004331445425334904\n",
      "Average test loss: 0.0021427493968771564\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005376785053975052\n",
      "Average test loss: 0.0020214200584838787\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004444623491416375\n",
      "Average test loss: 0.002151914115374287\n",
      "Epoch 276/300\n",
      "Average training loss: 0.004311099135213428\n",
      "Average test loss: 0.0042376585507558455\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004619715739869409\n",
      "Average test loss: 0.0020745869398944906\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004288501682794757\n",
      "Average test loss: 0.002072690592664811\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004294655261768235\n",
      "Average test loss: 0.0024368967162445186\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004295299211930897\n",
      "Average test loss: 0.0021007939295636284\n",
      "Epoch 281/300\n",
      "Average training loss: 0.004843367535207007\n",
      "Average test loss: 0.002104091833345592\n",
      "Epoch 282/300\n",
      "Average training loss: 0.004309915038446585\n",
      "Average test loss: 0.002226497405105167\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004288288649171591\n",
      "Average test loss: 0.002070184057268004\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004340499787280957\n",
      "Average test loss: 0.0022131775503771173\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004315710138115618\n",
      "Average test loss: 0.002105793580205904\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004300653976905677\n",
      "Average test loss: 0.0021872486803266736\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0043030588235706095\n",
      "Average test loss: 0.0020924068692450724\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004291634055475394\n",
      "Average test loss: 0.0020906357624464566\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004327543126005266\n",
      "Average test loss: 0.0020916934174795947\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004328789091772503\n",
      "Average test loss: 0.0021114944780452386\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0045304659278028545\n",
      "Average test loss: 0.002066070686508384\n",
      "Epoch 292/300\n",
      "Average training loss: 0.004291936095803976\n",
      "Average test loss: 0.002048914303382238\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004267498713400629\n",
      "Average test loss: 0.0020483182789757847\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004272383481678035\n",
      "Average test loss: 0.0021228673531570367\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004308361404885848\n",
      "Average test loss: 0.002044887047985362\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004295448245687617\n",
      "Average test loss: 0.002075970101998084\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004359087173309591\n",
      "Average test loss: 0.005005023861510886\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004271662057066957\n",
      "Average test loss: 0.0020938678548360866\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004289036747068166\n",
      "Average test loss: 0.002042954172835582\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004278104977475272\n",
      "Average test loss: 0.0021111588459461926\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2552238852977753\n",
      "Average test loss: 0.07704231228017144\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06030250668525696\n",
      "Average test loss: 0.5018103934294648\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03862132681740655\n",
      "Average test loss: 0.01203903815564182\n",
      "Epoch 4/300\n",
      "Average training loss: 0.029638247172037762\n",
      "Average test loss: 0.003704229672749837\n",
      "Epoch 5/300\n",
      "Average training loss: 0.024734735954138967\n",
      "Average test loss: 0.003976533965104156\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021515299156308173\n",
      "Average test loss: 0.004318821370808614\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019220537816484768\n",
      "Average test loss: 0.4581766577975617\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01743230636086729\n",
      "Average test loss: 0.043447583307615584\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015942246900664436\n",
      "Average test loss: 0.004521511478142606\n",
      "Epoch 10/300\n",
      "Average training loss: 0.014742792992128266\n",
      "Average test loss: 0.5550611921813753\n",
      "Epoch 11/300\n",
      "Average training loss: 0.013688858166337014\n",
      "Average test loss: 0.0023547469180905155\n",
      "Epoch 12/300\n",
      "Average training loss: 0.012775042466819287\n",
      "Average test loss: 0.024642079196042485\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01197629157288207\n",
      "Average test loss: 0.003096403908294936\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01127759034683307\n",
      "Average test loss: 0.03504102988872263\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01066277307106389\n",
      "Average test loss: 0.004753606541910105\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010202382913480202\n",
      "Average test loss: 0.002731580498938759\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009661864215715065\n",
      "Average test loss: 0.0018757871344892514\n",
      "Epoch 18/300\n",
      "Average training loss: 0.009307578051255809\n",
      "Average test loss: 0.0020085480639504064\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008858189782334698\n",
      "Average test loss: 0.0035709972861740326\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008549243085086346\n",
      "Average test loss: 0.0020105715673416854\n",
      "Epoch 21/300\n",
      "Average training loss: 0.008186968512005276\n",
      "Average test loss: 0.002129415075812075\n",
      "Epoch 22/300\n",
      "Average training loss: 0.00786607693011562\n",
      "Average test loss: 0.001719463939468066\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007569436364703708\n",
      "Average test loss: 0.0017761237300518487\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0074984542864064375\n",
      "Average test loss: 0.0016453539044078854\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007085263085862001\n",
      "Average test loss: 0.0015930718128672903\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006787829022026725\n",
      "Average test loss: 0.0015610034335404636\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006668677378031942\n",
      "Average test loss: 0.001556988601676292\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006376230031665829\n",
      "Average test loss: 0.0015045662896914615\n",
      "Epoch 29/300\n",
      "Average training loss: 0.006219627189139525\n",
      "Average test loss: 0.0014979033722645706\n",
      "Epoch 30/300\n",
      "Average training loss: 0.006093216348025534\n",
      "Average test loss: 0.0014946695574455792\n",
      "Epoch 31/300\n",
      "Average training loss: 0.005913256542136272\n",
      "Average test loss: 0.001491942553780973\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0056840962664120726\n",
      "Average test loss: 0.0014305812857217258\n",
      "Epoch 33/300\n",
      "Average training loss: 0.00557418364741736\n",
      "Average test loss: 0.0016897131506767539\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005682698292450773\n",
      "Average test loss: 0.0014485716638672683\n",
      "Epoch 35/300\n",
      "Average training loss: 0.005303468390057484\n",
      "Average test loss: 0.0014469681139000587\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005207554635074403\n",
      "Average test loss: 0.0013750805633349552\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005106215686433845\n",
      "Average test loss: 0.002280555899772379\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005042470880680614\n",
      "Average test loss: 0.0013334208364701933\n",
      "Epoch 39/300\n",
      "Average training loss: 0.004992224245849583\n",
      "Average test loss: 0.0013273598313745525\n",
      "Epoch 40/300\n",
      "Average training loss: 0.004942522911975781\n",
      "Average test loss: 0.001357293115204407\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00527317541361683\n",
      "Average test loss: 0.0014057238754919834\n",
      "Epoch 42/300\n",
      "Average training loss: 0.004822641888219449\n",
      "Average test loss: 0.0013371542430379323\n",
      "Epoch 43/300\n",
      "Average training loss: 0.004734447425024377\n",
      "Average test loss: 0.0013243331802594992\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004677830279701286\n",
      "Average test loss: 0.0013202319045861562\n",
      "Epoch 45/300\n",
      "Average training loss: 0.004649687324547106\n",
      "Average test loss: 0.0013078841620849239\n",
      "Epoch 46/300\n",
      "Average training loss: 0.004583390335655874\n",
      "Average test loss: 0.001308711468345589\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0045598248829030325\n",
      "Average test loss: 0.0012902690144255757\n",
      "Epoch 48/300\n",
      "Average training loss: 0.004508761413395405\n",
      "Average test loss: 0.0015410798888446555\n",
      "Epoch 49/300\n",
      "Average training loss: 0.00450730872568157\n",
      "Average test loss: 0.0014181530441985362\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0049425231932351986\n",
      "Average test loss: 0.0014426640214191542\n",
      "Epoch 51/300\n",
      "Average training loss: 0.004564961329102516\n",
      "Average test loss: 0.0016147761200037268\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004389335397010048\n",
      "Average test loss: 0.001290204737904585\n",
      "Epoch 53/300\n",
      "Average training loss: 0.004373176902532578\n",
      "Average test loss: 0.0013008946225874954\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0044188401591446665\n",
      "Average test loss: 0.001311337094133099\n",
      "Epoch 55/300\n",
      "Average training loss: 0.004317718438183268\n",
      "Average test loss: 0.0012791553555677334\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0042900782322718035\n",
      "Average test loss: 0.001337330760123829\n",
      "Epoch 57/300\n",
      "Average training loss: 0.004280415192246437\n",
      "Average test loss: 0.0013055509857626425\n",
      "Epoch 58/300\n",
      "Average training loss: 0.004278465310318603\n",
      "Average test loss: 0.0013346087692512405\n",
      "Epoch 59/300\n",
      "Average training loss: 0.004211902893251843\n",
      "Average test loss: 0.0013254776861932542\n",
      "Epoch 60/300\n",
      "Average training loss: 0.004216862062613169\n",
      "Average test loss: 0.0012748010824951861\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00434596149271561\n",
      "Average test loss: 0.0012881173574262196\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004581733640904228\n",
      "Average test loss: 0.001297775789991849\n",
      "Epoch 63/300\n",
      "Average training loss: 0.004162029026283158\n",
      "Average test loss: 0.001255787764572435\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004110617087946997\n",
      "Average test loss: 0.001528692318023079\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004182082157375084\n",
      "Average test loss: 0.0012991814518140423\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004085531595059567\n",
      "Average test loss: 0.0013869945339651571\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0040900153997871615\n",
      "Average test loss: 0.0013135439577615924\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004118468942948514\n",
      "Average test loss: 0.0012678332938295272\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004190382272212041\n",
      "Average test loss: 0.0014456150416905682\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0040289241005149155\n",
      "Average test loss: 0.002715662493887875\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004005701624270943\n",
      "Average test loss: 0.0013247422283101413\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004257894859131839\n",
      "Average test loss: 0.0013075790092762973\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003954819429044922\n",
      "Average test loss: 0.0012808417527832919\n",
      "Epoch 74/300\n",
      "Average training loss: 0.003939114488040407\n",
      "Average test loss: 0.0012638568362842004\n",
      "Epoch 75/300\n",
      "Average training loss: 0.003922661505018671\n",
      "Average test loss: 0.0013655488969137272\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00395407598072456\n",
      "Average test loss: 0.0014306775939961274\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0039008928572552073\n",
      "Average test loss: 0.002174300761893392\n",
      "Epoch 78/300\n",
      "Average training loss: 0.003922867023489541\n",
      "Average test loss: 0.0014272787363992798\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00463845627506574\n",
      "Average test loss: 0.001285778895744847\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0039102403525677\n",
      "Average test loss: 0.0013649610719747014\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0039043211936950683\n",
      "Average test loss: 0.0012736735949292778\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0038267937447461816\n",
      "Average test loss: 0.0013273713866041766\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003840470408813821\n",
      "Average test loss: 0.001388568841645287\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0038297888189554215\n",
      "Average test loss: 0.0013216419880174928\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0038180870012276703\n",
      "Average test loss: 0.0013180607330674926\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003805974529021316\n",
      "Average test loss: 0.0014777048979368475\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0038949694747312203\n",
      "Average test loss: 0.001364883298287168\n",
      "Epoch 88/300\n",
      "Average training loss: 0.003798867815691564\n",
      "Average test loss: 0.001537392880146702\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0038472818440447253\n",
      "Average test loss: 0.0015268507931484945\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003745827766549256\n",
      "Average test loss: 0.0028148602447989916\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0037292391624715593\n",
      "Average test loss: 0.0014640940400875277\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0037397761723647516\n",
      "Average test loss: 0.002490621087451776\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00402364798862901\n",
      "Average test loss: 0.0024250140851363538\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0037259214312459032\n",
      "Average test loss: 0.0013820656705647706\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0037081195873518787\n",
      "Average test loss: 0.0013312235293495985\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00380364707722846\n",
      "Average test loss: 0.0016489771578667893\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0036685717861271567\n",
      "Average test loss: 0.0013715998611102502\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0036565228247394162\n",
      "Average test loss: 0.0012890024008229376\n",
      "Epoch 99/300\n",
      "Average training loss: 0.003752293218548099\n",
      "Average test loss: 0.006348908852371904\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003647356134942836\n",
      "Average test loss: 0.0013579743335851364\n",
      "Epoch 101/300\n",
      "Average training loss: 0.003874035194516182\n",
      "Average test loss: 0.0026849465796517\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004390322177360455\n",
      "Average test loss: 0.001349099859698779\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0037260975481735337\n",
      "Average test loss: 0.001299080113414675\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0036184953809198406\n",
      "Average test loss: 0.007286991611123085\n",
      "Epoch 105/300\n",
      "Average training loss: 0.003599668335997396\n",
      "Average test loss: 0.002645680871895618\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0036776288408372137\n",
      "Average test loss: 0.001340326721780002\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003592566657604443\n",
      "Average test loss: 0.0013775383445123832\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0036151909043805465\n",
      "Average test loss: 0.0013885885786472096\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0035865999925881625\n",
      "Average test loss: 0.0013436801122604972\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0035995698699520695\n",
      "Average test loss: 0.0014052319376625948\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0036535442823337184\n",
      "Average test loss: 0.0013484470090932316\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0036394622379706965\n",
      "Average test loss: 0.0013706575894935264\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0035890454798936846\n",
      "Average test loss: 0.0014245573571986623\n",
      "Epoch 114/300\n",
      "Average training loss: 0.003556590088746614\n",
      "Average test loss: 0.0012963068150501284\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0037029352074282036\n",
      "Average test loss: 0.0016047700091989503\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0035436147304458753\n",
      "Average test loss: 0.01123305777211984\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003546489141881466\n",
      "Average test loss: 0.0013131599703596697\n",
      "Epoch 118/300\n",
      "Average training loss: 0.003558072369131777\n",
      "Average test loss: 0.006441552227983872\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0035187033634218904\n",
      "Average test loss: 0.0013503630671443212\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0036956616044044495\n",
      "Average test loss: 0.011474220634748539\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0035099728237837553\n",
      "Average test loss: 0.001574917602766719\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0035045242065356837\n",
      "Average test loss: 0.0019600573358022504\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0035530806438376504\n",
      "Average test loss: 0.0020769614617650706\n",
      "Epoch 124/300\n",
      "Average training loss: 0.003552671106532216\n",
      "Average test loss: 0.004575978118098445\n",
      "Epoch 125/300\n",
      "Average training loss: 0.003483063148541583\n",
      "Average test loss: 0.030691862457328373\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0034893271956178877\n",
      "Average test loss: 0.001399275613327821\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0034645241178158257\n",
      "Average test loss: 0.0014080304042953585\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003467470144232114\n",
      "Average test loss: 0.0013076356379315257\n",
      "Epoch 129/300\n",
      "Average training loss: 0.003533463867381215\n",
      "Average test loss: 0.0035420149289485483\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0034532947215355106\n",
      "Average test loss: 0.0013485197757060329\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0034856207946108446\n",
      "Average test loss: 0.0013117425879463553\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0034462596680969\n",
      "Average test loss: 0.0014563257315506537\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0034342711919711695\n",
      "Average test loss: 0.0016291687503043148\n",
      "Epoch 134/300\n",
      "Average training loss: 0.00342827486515873\n",
      "Average test loss: 0.0013994422391470935\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0034446366710795297\n",
      "Average test loss: 0.001784664284541375\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0034349835241834323\n",
      "Average test loss: 0.0028041187489612233\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0035187158398330212\n",
      "Average test loss: 0.001431687682142688\n",
      "Epoch 138/300\n",
      "Average training loss: 0.003395382293396526\n",
      "Average test loss: 0.0015052557960152627\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003392619846595658\n",
      "Average test loss: 0.0013488172630055084\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00342215816428264\n",
      "Average test loss: 0.001298085955489013\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0034118765731238655\n",
      "Average test loss: 0.0014417783122302758\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0045359146156244805\n",
      "Average test loss: 0.0022484963362415632\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0037168761864304544\n",
      "Average test loss: 0.0014056127026884093\n",
      "Epoch 144/300\n",
      "Average training loss: 0.003404216473094291\n",
      "Average test loss: 0.0013092393140412039\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003367906375684672\n",
      "Average test loss: 0.0013358772912890546\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0033533790262622967\n",
      "Average test loss: 0.001347726841457188\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0033529449744770924\n",
      "Average test loss: 0.0013490543513455325\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0033361121728602383\n",
      "Average test loss: 0.0015187774205373393\n",
      "Epoch 149/300\n",
      "Average training loss: 0.003359977110185557\n",
      "Average test loss: 0.0016734161774317423\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0033794552048461305\n",
      "Average test loss: 0.001414289381562008\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0033992395541734164\n",
      "Average test loss: 0.0013374210710947712\n",
      "Epoch 152/300\n",
      "Average training loss: 0.003355981579878264\n",
      "Average test loss: 0.0013832825587855445\n",
      "Epoch 153/300\n",
      "Average training loss: 0.003403811818195714\n",
      "Average test loss: 0.0013754508320449126\n",
      "Epoch 154/300\n",
      "Average training loss: 0.003392027227414979\n",
      "Average test loss: 0.0013484142083260747\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0033272988947315348\n",
      "Average test loss: 0.013042497240420845\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0033467589958260458\n",
      "Average test loss: 0.001368997328966442\n",
      "Epoch 157/300\n",
      "Average training loss: 0.003314448245904512\n",
      "Average test loss: 0.0013270776805260943\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0033942801853020987\n",
      "Average test loss: 0.0013428160873138243\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0032978487699810003\n",
      "Average test loss: 0.0015646963793163498\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0033628012537956237\n",
      "Average test loss: 0.0014603124347825846\n",
      "Epoch 161/300\n",
      "Average training loss: 0.003318805415390266\n",
      "Average test loss: 0.001372928186216288\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0033109950557765034\n",
      "Average test loss: 0.0014473856867601475\n",
      "Epoch 163/300\n",
      "Average training loss: 0.003652568707242608\n",
      "Average test loss: 0.17153792821698718\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0032778627506146827\n",
      "Average test loss: 0.0026888627788672847\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0032671434320509433\n",
      "Average test loss: 0.0013455920038330886\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0034090091137008536\n",
      "Average test loss: 0.001465275313705206\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0033023766320612697\n",
      "Average test loss: 0.0015788184514062271\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0032674215998914507\n",
      "Average test loss: 0.0013614251557737588\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0032759573358214565\n",
      "Average test loss: 0.0014485910123007166\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0032614662763145235\n",
      "Average test loss: 0.0014438547814885775\n",
      "Epoch 171/300\n",
      "Average training loss: 0.003329321213066578\n",
      "Average test loss: 0.001711848224202792\n",
      "Epoch 172/300\n",
      "Average training loss: 0.003366099401894543\n",
      "Average test loss: 0.0014105121193246709\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0032555758312551512\n",
      "Average test loss: 0.001460565407656961\n",
      "Epoch 174/300\n",
      "Average training loss: 0.003526215207452575\n",
      "Average test loss: 0.0013969253798325856\n",
      "Epoch 175/300\n",
      "Average training loss: 0.003248191149905324\n",
      "Average test loss: 0.001372240347787738\n",
      "Epoch 176/300\n",
      "Average training loss: 0.003223289095072283\n",
      "Average test loss: 0.0013811230414753986\n",
      "Epoch 177/300\n",
      "Average training loss: 0.00324694918261634\n",
      "Average test loss: 0.0013531653702569505\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0032365513286656804\n",
      "Average test loss: 0.0014102910946433742\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0032329665780481366\n",
      "Average test loss: 0.001359290887394713\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004056688388602601\n",
      "Average test loss: 0.0012910549282613727\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0038089962140139605\n",
      "Average test loss: 0.002632853571429021\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0033204799368977546\n",
      "Average test loss: 0.0013566257003694772\n",
      "Epoch 183/300\n",
      "Average training loss: 0.003212097756978538\n",
      "Average test loss: 0.001943209530475239\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0038490972481667997\n",
      "Average test loss: 0.0023564100006802213\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004626586292352942\n",
      "Average test loss: 0.0013778310154254237\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0034959070705291298\n",
      "Average test loss: 0.0014364666822883818\n",
      "Epoch 187/300\n",
      "Average training loss: 0.003261280712982019\n",
      "Average test loss: 0.0013650970948446129\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003194829421531823\n",
      "Average test loss: 0.0013567436054969827\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0031907747054679527\n",
      "Average test loss: 0.0013693174705840647\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0031808052868064905\n",
      "Average test loss: 0.0013949789260514081\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0032206017720616525\n",
      "Average test loss: 0.005721844708340035\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0033072936762538222\n",
      "Average test loss: 0.003000743855204847\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00319380038852493\n",
      "Average test loss: 0.001378632148210373\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0032235485563675563\n",
      "Average test loss: 0.0014306967144625054\n",
      "Epoch 195/300\n",
      "Average training loss: 0.003223454674705863\n",
      "Average test loss: 0.0015033049564808607\n",
      "Epoch 196/300\n",
      "Average training loss: 0.003228885086770687\n",
      "Average test loss: 0.0014501384718136655\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0033088397437499627\n",
      "Average test loss: 0.0013689321820727653\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0032671987337784636\n",
      "Average test loss: 0.0014076499828758339\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0031782157760527397\n",
      "Average test loss: 0.0013852949552237987\n",
      "Epoch 200/300\n",
      "Average training loss: 0.003192493317027887\n",
      "Average test loss: 0.001359610359184444\n",
      "Epoch 201/300\n",
      "Average training loss: 0.003177532284417086\n",
      "Average test loss: 0.0013994789648180206\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0032516490819139614\n",
      "Average test loss: 0.0014106393100486862\n",
      "Epoch 203/300\n",
      "Average training loss: 0.003185046458409892\n",
      "Average test loss: 0.0014326322906547122\n",
      "Epoch 204/300\n",
      "Average training loss: 0.003331425418042474\n",
      "Average test loss: 0.0013873894804467758\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0031811131913628845\n",
      "Average test loss: 0.0013882467925755515\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0031570893803404437\n",
      "Average test loss: 0.0013858394367206428\n",
      "Epoch 207/300\n",
      "Average training loss: 0.003176499770126409\n",
      "Average test loss: 0.0014150319301212827\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003807612633953492\n",
      "Average test loss: 0.0014780954777573545\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0031944670726855597\n",
      "Average test loss: 0.0014430497491525278\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0032549281823966237\n",
      "Average test loss: 0.0013925793721444076\n",
      "Epoch 211/300\n",
      "Average training loss: 0.003191022660376297\n",
      "Average test loss: 0.0021692230846318936\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0031962331858360104\n",
      "Average test loss: 0.0014580547982412909\n",
      "Epoch 213/300\n",
      "Average training loss: 0.003160035429108474\n",
      "Average test loss: 0.001478445261458142\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00316087685401241\n",
      "Average test loss: 0.0045828668288886544\n",
      "Epoch 215/300\n",
      "Average training loss: 0.003154775032359693\n",
      "Average test loss: 0.0013989784786891605\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0050371695922480685\n",
      "Average test loss: 0.0013435534340225988\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004206094791905747\n",
      "Average test loss: 0.0013367983671940034\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0035698495312697356\n",
      "Average test loss: 0.0014346988264781733\n",
      "Epoch 219/300\n",
      "Average training loss: 0.003234486766780416\n",
      "Average test loss: 0.0014808150253569085\n",
      "Epoch 220/300\n",
      "Average training loss: 0.003150924040004611\n",
      "Average test loss: 0.0013331370856612922\n",
      "Epoch 221/300\n",
      "Average training loss: 0.003321313298203879\n",
      "Average test loss: 0.0014153393946795\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0031182901817891334\n",
      "Average test loss: 0.0013813061775225731\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0031088386529849635\n",
      "Average test loss: 0.0014726284683371585\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0031317802030179235\n",
      "Average test loss: 0.17241766347818904\n",
      "Epoch 225/300\n",
      "Average training loss: 0.00344001915347245\n",
      "Average test loss: 0.003492086140231954\n",
      "Epoch 226/300\n",
      "Average training loss: 0.003116467744939857\n",
      "Average test loss: 0.0016290292677779991\n",
      "Epoch 227/300\n",
      "Average training loss: 0.003133731970563531\n",
      "Average test loss: 0.0014087049271393982\n",
      "Epoch 228/300\n",
      "Average training loss: 0.003124001327695118\n",
      "Average test loss: 0.001475295239749054\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0031680720040781632\n",
      "Average test loss: 0.0014037064233173926\n",
      "Epoch 230/300\n",
      "Average training loss: 0.003129126877213518\n",
      "Average test loss: 0.01960506037208769\n",
      "Epoch 231/300\n",
      "Average training loss: 0.003696167372787992\n",
      "Average test loss: 0.0038195090043979386\n",
      "Epoch 232/300\n",
      "Average training loss: 0.003130807314068079\n",
      "Average test loss: 0.001390966508951452\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0031059373712374106\n",
      "Average test loss: 0.0014413467515777382\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0031130580154971944\n",
      "Average test loss: 0.00140233027541803\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0031051457832670876\n",
      "Average test loss: 0.0016422583512547943\n",
      "Epoch 236/300\n",
      "Average training loss: 0.003130618279385898\n",
      "Average test loss: 0.0015803003831663066\n",
      "Epoch 237/300\n",
      "Average training loss: 0.003740001899914609\n",
      "Average test loss: 0.0013882729793484841\n",
      "Epoch 238/300\n",
      "Average training loss: 0.003092658933666017\n",
      "Average test loss: 0.001373254885069198\n",
      "Epoch 239/300\n",
      "Average training loss: 0.003100509445907341\n",
      "Average test loss: 0.0014058479274519616\n",
      "Epoch 240/300\n",
      "Average training loss: 0.003091385060714351\n",
      "Average test loss: 0.001446778144997855\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003106417938032084\n",
      "Average test loss: 0.001413247000115613\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0031056198583294947\n",
      "Average test loss: 0.0014839342109238109\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0031128365581648216\n",
      "Average test loss: 0.0014030707102372414\n",
      "Epoch 244/300\n",
      "Average training loss: 0.003140736497938633\n",
      "Average test loss: 0.0014178691913063327\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003102619387416376\n",
      "Average test loss: 0.008086871785422165\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0031012188142372503\n",
      "Average test loss: 0.0013801441676914692\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0031488175253487297\n",
      "Average test loss: 0.0014830103042121562\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0031554437408016786\n",
      "Average test loss: 0.001836054630163643\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004274122624554568\n",
      "Average test loss: 0.003909167243581679\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005897383291274309\n",
      "Average test loss: 0.0028520530636111895\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004231683228578832\n",
      "Average test loss: 0.0021476776889628835\n",
      "Epoch 252/300\n",
      "Average training loss: 0.003751973320212629\n",
      "Average test loss: 0.0013070657894843154\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0033649038024660613\n",
      "Average test loss: 0.0013475294925479426\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0031671749566578203\n",
      "Average test loss: 0.0018389429841190576\n",
      "Epoch 255/300\n",
      "Average training loss: 0.003084980603100525\n",
      "Average test loss: 0.0014044731290390093\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0030904866916437942\n",
      "Average test loss: 0.0014337745601725248\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0030634579225960706\n",
      "Average test loss: 0.0014473714317298598\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003063812847559651\n",
      "Average test loss: 0.0014474017619569269\n",
      "Epoch 259/300\n",
      "Average training loss: 0.003090725825064712\n",
      "Average test loss: 0.001424276037245161\n",
      "Epoch 260/300\n",
      "Average training loss: 0.003101225112668342\n",
      "Average test loss: 0.0018536680874725183\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003087184405161275\n",
      "Average test loss: 0.001432623089498116\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0031682645357731317\n",
      "Average test loss: 0.0014290846181619497\n",
      "Epoch 263/300\n",
      "Average training loss: 0.003112282565070523\n",
      "Average test loss: 0.0014109885151394538\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0030632539263202086\n",
      "Average test loss: 0.0015395897589623928\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0030801485533722573\n",
      "Average test loss: 0.0014980995838219922\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0030920491524868542\n",
      "Average test loss: 0.0014829213611988559\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0030500950270021957\n",
      "Average test loss: 0.0014725074001794888\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0030776936387022337\n",
      "Average test loss: 0.0016702407158704267\n",
      "Epoch 269/300\n",
      "Average training loss: 0.003081102326926258\n",
      "Average test loss: 0.0014347031180643372\n",
      "Epoch 270/300\n",
      "Average training loss: 0.003201790154187216\n",
      "Average test loss: 0.0014615144311553902\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0030675777327269314\n",
      "Average test loss: 0.002761046048771176\n",
      "Epoch 272/300\n",
      "Average training loss: 0.003066513353337844\n",
      "Average test loss: 0.001885181753896177\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0030648802943113776\n",
      "Average test loss: 0.0014664024915546179\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0031902755244324604\n",
      "Average test loss: 0.02139461101177666\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0030732553948958716\n",
      "Average test loss: 0.0014692770157837206\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0030346103298167387\n",
      "Average test loss: 0.0014044720624677009\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0030742990084820323\n",
      "Average test loss: 0.0014254865280042093\n",
      "Epoch 278/300\n",
      "Average training loss: 0.003043227460856239\n",
      "Average test loss: 0.0014258583036975728\n",
      "Epoch 279/300\n",
      "Average training loss: 0.003041811634062065\n",
      "Average test loss: 0.002583490283952819\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0030617978599750332\n",
      "Average test loss: 0.0014006750774052407\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003049789336613483\n",
      "Average test loss: 0.0014262338584909837\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0030513822674337362\n",
      "Average test loss: 0.0014357919476201966\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0030638271276321677\n",
      "Average test loss: 0.001603574942280021\n",
      "Epoch 284/300\n",
      "Average training loss: 0.003025357055063877\n",
      "Average test loss: 0.0014465470401984122\n",
      "Epoch 285/300\n",
      "Average training loss: 0.003049933455160095\n",
      "Average test loss: 0.002039122660437392\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0032886393742842805\n",
      "Average test loss: 0.0018636972893857295\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0030000867377966644\n",
      "Average test loss: 0.0014023485245803992\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0030185509715229273\n",
      "Average test loss: 0.0014024722027695842\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0031857105952998004\n",
      "Average test loss: 0.001410245295614004\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003017942263848252\n",
      "Average test loss: 0.0021723188762035636\n",
      "Epoch 291/300\n",
      "Average training loss: 0.003020466856658459\n",
      "Average test loss: 0.001521834476954407\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0030309688544107806\n",
      "Average test loss: 0.0014811438662517401\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0030234554703864784\n",
      "Average test loss: 0.001411096648623546\n",
      "Epoch 294/300\n",
      "Average training loss: 0.003022098692134023\n",
      "Average test loss: 0.0015294723895688852\n",
      "Epoch 295/300\n",
      "Average training loss: 0.019294880980004866\n",
      "Average test loss: 57.65826715519093\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010295474626123905\n",
      "Average test loss: 0.42887088408735063\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008039224813381832\n",
      "Average test loss: 0.8364978059770333\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0065050703986651364\n",
      "Average test loss: 6.08509189982795\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00587535973224375\n",
      "Average test loss: 37.62030191520229\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0077265091186596285\n",
      "Average test loss: 139.60394711036898\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Skip/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 17.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 14.91\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 21.39\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 21.65\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.30\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 13.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 19.98\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.19\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.29\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.58\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 17.86\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 21.93\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 9.36\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.54\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 15.74\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 20.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 17.84\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.70\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 21.82\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 3.97\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 17.17\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.90\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.09\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 20.66\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.76\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.39\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 23.44\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 4.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 19.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.15\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 23.29\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.98\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.29\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.73\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.97\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.0779232665167915\n",
      "Average test loss: 0.014191029280424119\n",
      "Epoch 2/300\n",
      "Average training loss: 0.9193881488906013\n",
      "Average test loss: 0.1780388974075516\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5735250534481472\n",
      "Average test loss: 0.015060143764648173\n",
      "Epoch 4/300\n",
      "Average training loss: 0.39935909191767377\n",
      "Average test loss: 18.820599421749513\n",
      "Epoch 5/300\n",
      "Average training loss: 0.28034701418876645\n",
      "Average test loss: 2.4190942476892636\n",
      "Epoch 6/300\n",
      "Average training loss: 0.23035714575979443\n",
      "Average test loss: 6.431198268036048\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1853640051152971\n",
      "Average test loss: 22.141070969611405\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15407556290096708\n",
      "Average test loss: 6.579129048953454\n",
      "Epoch 9/300\n",
      "Average training loss: 0.13713618312941658\n",
      "Average test loss: 6.324300156235695\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1224586019648446\n",
      "Average test loss: 0.007204212781869703\n",
      "Epoch 11/300\n",
      "Average training loss: 0.10627554515335295\n",
      "Average test loss: 0.02057554715582066\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09623076971371969\n",
      "Average test loss: 1.806484663479858\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08730409629477395\n",
      "Average test loss: 1.0581281014631192\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0793420635925399\n",
      "Average test loss: 0.006117006690137916\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07231358451975717\n",
      "Average test loss: 0.17523265334467092\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06666534767548243\n",
      "Average test loss: 0.04746473254842891\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06154939122332467\n",
      "Average test loss: 0.0869815381831593\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05785837097300423\n",
      "Average test loss: 0.005961984566516346\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05467804307407803\n",
      "Average test loss: 0.15865032571305832\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05236132245262464\n",
      "Average test loss: 0.13342372792628077\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05032341133554776\n",
      "Average test loss: 0.0061269219004445605\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04956753149297502\n",
      "Average test loss: 0.005868842545482847\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0470024849375089\n",
      "Average test loss: 0.1161936953994963\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04617760899662972\n",
      "Average test loss: 0.005765378927191099\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04446750644511647\n",
      "Average test loss: 0.011797028894225757\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0440146884156598\n",
      "Average test loss: 0.3169298666123715\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04431568678220113\n",
      "Average test loss: 0.1400485581147174\n",
      "Epoch 28/300\n",
      "Average training loss: 0.042124885744518704\n",
      "Average test loss: 25.6742189355029\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04347563414772351\n",
      "Average test loss: 101.57052929736508\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0403647618525558\n",
      "Average test loss: 0.007408096282846398\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04499783477187157\n",
      "Average test loss: 0.0074666356386409866\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04478080562088225\n",
      "Average test loss: 0.00524304834794667\n",
      "Epoch 33/300\n",
      "Average training loss: 0.039101979467603895\n",
      "Average test loss: 0.0211372691773706\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03793442098961936\n",
      "Average test loss: 0.009159456621441577\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03723079261514876\n",
      "Average test loss: 0.005091585775630341\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03656293390194575\n",
      "Average test loss: 0.050168281089514495\n",
      "Epoch 37/300\n",
      "Average training loss: 0.036131967451837325\n",
      "Average test loss: 0.006734846870932314\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03582052458822727\n",
      "Average test loss: 0.11682271331176161\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04276381634341346\n",
      "Average test loss: 129.996615751995\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03610183671447966\n",
      "Average test loss: 1.9141579500801034\n",
      "Epoch 41/300\n",
      "Average training loss: 0.035202198273605774\n",
      "Average test loss: 0.053898457194368046\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03602259086900287\n",
      "Average test loss: 0.005241228110674355\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03480709010362625\n",
      "Average test loss: 0.08292105404122008\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03804036397569709\n",
      "Average test loss: 0.5760814523465103\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03573656244079272\n",
      "Average test loss: 101.65833947509941\n",
      "Epoch 46/300\n",
      "Average training loss: 0.051417303754223716\n",
      "Average test loss: 0.00544572963938117\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03674884755743874\n",
      "Average test loss: 0.7464138589331674\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03528115799691942\n",
      "Average test loss: 0.06710717791691423\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03480009488595857\n",
      "Average test loss: 19.678585795482\n",
      "Epoch 50/300\n",
      "Average training loss: 0.034058127376768325\n",
      "Average test loss: 1.2491981570803457\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03371941505372524\n",
      "Average test loss: 0.005166907050543361\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03359972704781426\n",
      "Average test loss: 21.963842383515505\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03326477549142308\n",
      "Average test loss: 28.833995638105606\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04121306795875231\n",
      "Average test loss: 0.5265950194630358\n",
      "Epoch 55/300\n",
      "Average training loss: 0.034802542845408124\n",
      "Average test loss: 2.3344214864422876\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03355007798224688\n",
      "Average test loss: 3.004799417204327\n",
      "Epoch 57/300\n",
      "Average training loss: 0.033177590638399125\n",
      "Average test loss: 0.005036960181262758\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04609390825364325\n",
      "Average test loss: 0.006507909110022915\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04223289514250225\n",
      "Average test loss: 0.04439469546079636\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03940684015552203\n",
      "Average test loss: 0.005353805243555042\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03532025684250726\n",
      "Average test loss: 0.026171077046129438\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03489944083491961\n",
      "Average test loss: 0.005212642457750109\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03402695653173658\n",
      "Average test loss: 0.00943694724763433\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04469358700844977\n",
      "Average test loss: 0.005455634500831366\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0345449985563755\n",
      "Average test loss: 0.006593148466613558\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03328043575088183\n",
      "Average test loss: 0.005230314272559352\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03272185585896174\n",
      "Average test loss: 0.005314508321177629\n",
      "Epoch 68/300\n",
      "Average training loss: 0.033706129560867944\n",
      "Average test loss: 0.02372624444299274\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03545469645162423\n",
      "Average test loss: 0.005734601416107681\n",
      "Epoch 70/300\n",
      "Average training loss: 0.046291764929890636\n",
      "Average test loss: 0.0051761029631727275\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03782261659370528\n",
      "Average test loss: 0.014847605392336845\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03433213037583563\n",
      "Average test loss: 0.007071687233944734\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03338421902391646\n",
      "Average test loss: 0.010293809012820324\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03290463543434938\n",
      "Average test loss: 0.15768446490665278\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03827341080043051\n",
      "Average test loss: 0.21445734702216254\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04225568327307701\n",
      "Average test loss: 0.16696940700482163\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0376878526955843\n",
      "Average test loss: 0.005016846943232748\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03419730096724298\n",
      "Average test loss: 0.08491512270106209\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03288618429170714\n",
      "Average test loss: 0.005603789285239246\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05068468040890164\n",
      "Average test loss: 0.042468471774417496\n",
      "Epoch 81/300\n",
      "Average training loss: 0.037155485885010824\n",
      "Average test loss: 0.14066659909031456\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034543754887249735\n",
      "Average test loss: 65.75815640871724\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03434591028425429\n",
      "Average test loss: 46.93724751974808\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03344459575414658\n",
      "Average test loss: 1140333.8894379889\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03795308842261632\n",
      "Average test loss: 0.28804777236862317\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03351310099826919\n",
      "Average test loss: 0.006831820217271646\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03253884609374735\n",
      "Average test loss: 0.0075730753480974175\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03201915199723509\n",
      "Average test loss: 11.717065862433778\n",
      "Epoch 89/300\n",
      "Average training loss: 0.034269021325641205\n",
      "Average test loss: 0.007425081998730699\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03194277432726489\n",
      "Average test loss: 0.0049738451929556\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03150662630299727\n",
      "Average test loss: 0.011029299018904567\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03135321898261706\n",
      "Average test loss: 0.033486522424759135\n",
      "Epoch 93/300\n",
      "Average training loss: 0.031227819091743894\n",
      "Average test loss: 0.009730908073070976\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03193737328383658\n",
      "Average test loss: 0.0054521511677238675\n",
      "Epoch 95/300\n",
      "Average training loss: 0.031044456713729435\n",
      "Average test loss: 8.593167584614621\n",
      "Epoch 96/300\n",
      "Average training loss: 0.035961975344353254\n",
      "Average test loss: 0.013459816726959414\n",
      "Epoch 97/300\n",
      "Average training loss: 0.031641389730903836\n",
      "Average test loss: 0.057584297681020365\n",
      "Epoch 98/300\n",
      "Average training loss: 0.030954615397585764\n",
      "Average test loss: 0.0172103031070696\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03668639793660906\n",
      "Average test loss: 0.005202941596921948\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03246116079390049\n",
      "Average test loss: 0.005854370586987998\n",
      "Epoch 101/300\n",
      "Average training loss: 0.030952378034591676\n",
      "Average test loss: 0.018508014879292913\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03059199335012171\n",
      "Average test loss: 0.004935448095202446\n",
      "Epoch 103/300\n",
      "Average training loss: 0.030388979928361044\n",
      "Average test loss: 0.023190431917707127\n",
      "Epoch 104/300\n",
      "Average training loss: 0.030274041132794485\n",
      "Average test loss: 0.005794975765049458\n",
      "Epoch 105/300\n",
      "Average training loss: 0.030204998912082778\n",
      "Average test loss: 0.1349228719626036\n",
      "Epoch 106/300\n",
      "Average training loss: 0.030146772732337315\n",
      "Average test loss: 0.005654457153131565\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04525837100711134\n",
      "Average test loss: 2.939812459124459\n",
      "Epoch 108/300\n",
      "Average training loss: 0.032716522783041\n",
      "Average test loss: 0.008755893582271206\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03113628134628137\n",
      "Average test loss: 0.00497586769113938\n",
      "Epoch 110/300\n",
      "Average training loss: 0.030495982877082294\n",
      "Average test loss: 0.010728007071961958\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030132645181483692\n",
      "Average test loss: 0.018735248419973587\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02994174736738205\n",
      "Average test loss: 0.005604855205863714\n",
      "Epoch 113/300\n",
      "Average training loss: 0.038132141103347145\n",
      "Average test loss: 0.03647469096879164\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04155473461416032\n",
      "Average test loss: 0.006182286127573914\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03354783782362938\n",
      "Average test loss: 0.00577094295538134\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03177784640921487\n",
      "Average test loss: 0.007145948246949248\n",
      "Epoch 117/300\n",
      "Average training loss: 0.031451998031801646\n",
      "Average test loss: 1841.9690670643847\n",
      "Epoch 118/300\n",
      "Average training loss: 0.030682508231865035\n",
      "Average test loss: 0.005060103309030334\n",
      "Epoch 119/300\n",
      "Average training loss: 0.030492751143044894\n",
      "Average test loss: 0.00835817290407916\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03284619219435586\n",
      "Average test loss: 0.2609641855193509\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06274991646409035\n",
      "Average test loss: 0.016278130604161157\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05526499056484964\n",
      "Average test loss: 0.03968227431426446\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04022257101866934\n",
      "Average test loss: 0.012591433075153166\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03686263392700089\n",
      "Average test loss: 1434.1671471145949\n",
      "Epoch 125/300\n",
      "Average training loss: 0.056131934695773655\n",
      "Average test loss: 82958494.08811022\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04181195012397236\n",
      "Average test loss: 622921.1020756102\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0376214827299118\n",
      "Average test loss: 4051.6986496326344\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04002279416223367\n",
      "Average test loss: 0.41383857969111865\n",
      "Epoch 129/300\n",
      "Average training loss: 0.035437995983494654\n",
      "Average test loss: 0.794456955201096\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03378083171115981\n",
      "Average test loss: 0.02244399526942935\n",
      "Epoch 131/300\n",
      "Average training loss: 0.033911467827028696\n",
      "Average test loss: 11.142007675861318\n",
      "Epoch 132/300\n",
      "Average training loss: 0.037026411135991415\n",
      "Average test loss: 0.0917262460179627\n",
      "Epoch 133/300\n",
      "Average training loss: 0.033690839389959974\n",
      "Average test loss: 345.90565255647897\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08684603305326567\n",
      "Average test loss: 2538.4971027817414\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05357433515124851\n",
      "Average test loss: 0.033685622060464486\n",
      "Epoch 136/300\n",
      "Average training loss: 0.040972820821735596\n",
      "Average test loss: 2.040175725393825\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03693300140566296\n",
      "Average test loss: 0.8551314071383741\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03574340443809827\n",
      "Average test loss: 3.3238197763864363\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03498328079779943\n",
      "Average test loss: 0.00517999174611436\n",
      "Epoch 140/300\n",
      "Average training loss: 0.035232474964525966\n",
      "Average test loss: 1.354859081349025\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03358615734014246\n",
      "Average test loss: 0.00700434616415037\n",
      "Epoch 142/300\n",
      "Average training loss: 0.033249219510290355\n",
      "Average test loss: 0.01453398447483778\n",
      "Epoch 143/300\n",
      "Average training loss: 0.032810234483745364\n",
      "Average test loss: 27.5682187525332\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05091146439479457\n",
      "Average test loss: 19149833.94020951\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05707594053612815\n",
      "Average test loss: 121093.2879089321\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03940667558378644\n",
      "Average test loss: 0.9137745277765724\n",
      "Epoch 147/300\n",
      "Average training loss: 0.036697195370992024\n",
      "Average test loss: 386.4449108098348\n",
      "Epoch 148/300\n",
      "Average training loss: 0.035475408567322626\n",
      "Average test loss: 0.030004243688450918\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03435209452112516\n",
      "Average test loss: 433.5044367113693\n",
      "Epoch 150/300\n",
      "Average training loss: 0.035005685365862314\n",
      "Average test loss: 24848403.96160186\n",
      "Epoch 151/300\n",
      "Average training loss: 0.033125631580750145\n",
      "Average test loss: 20986180.40562214\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0387516820496983\n",
      "Average test loss: 10510.091999843016\n",
      "Epoch 153/300\n",
      "Average training loss: 0.033149880450632836\n",
      "Average test loss: 59.77917927518487\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03224372321698401\n",
      "Average test loss: 42.94597349836636\n",
      "Epoch 155/300\n",
      "Average training loss: 0.032000277916590374\n",
      "Average test loss: 3.1865791816976334\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0316887895481454\n",
      "Average test loss: 0.026372514073219566\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05801822040147252\n",
      "Average test loss: 169504180.64062577\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05678127566311095\n",
      "Average test loss: 1.1813187698945402\n",
      "Epoch 159/300\n",
      "Average training loss: 0.037411403838131164\n",
      "Average test loss: 12424112.719511041\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03487297970056534\n",
      "Average test loss: 0.005059014708735049\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0353888244330883\n",
      "Average test loss: 0.812543655273815\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03546353538003233\n",
      "Average test loss: 1892488.4224287258\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0383796573513084\n",
      "Average test loss: 14044691792080.484\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03515893913308779\n",
      "Average test loss: 3084696703.280299\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03284192002813021\n",
      "Average test loss: 85003678.23984137\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04077921376625697\n",
      "Average test loss: 122055543508.48158\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03328817900021871\n",
      "Average test loss: 817711.6465323431\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04462425078286065\n",
      "Average test loss: 4005.4641492940286\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04034547413720025\n",
      "Average test loss: 3.032940726604803e+20\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04780828352603647\n",
      "Average test loss: 13.237268813475966\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03411589128772418\n",
      "Average test loss: 35662.12506846988\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03373885914021068\n",
      "Average test loss: 7681732880.465718\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03352183087004556\n",
      "Average test loss: 78429629556015.6\n",
      "Epoch 174/300\n",
      "Average training loss: 0.033019707921478486\n",
      "Average test loss: 132799.4387809853\n",
      "Epoch 175/300\n",
      "Average training loss: 0.032481990001267856\n",
      "Average test loss: 64.76288968110954\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03219817813899782\n",
      "Average test loss: 3643.4271484615674\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03140991863277223\n",
      "Average test loss: 80522.31887724288\n",
      "Epoch 178/300\n",
      "Average training loss: 0.038344295213619865\n",
      "Average test loss: 1255037931.80501\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03291548565692372\n",
      "Average test loss: 2237053708463249.8\n",
      "Epoch 180/300\n",
      "Average training loss: 0.036651883819037015\n",
      "Average test loss: 53728620.384684876\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04260915402736929\n",
      "Average test loss: 45.701118397454835\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03520327168206374\n",
      "Average test loss: 14860.455868036513\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03295871320697996\n",
      "Average test loss: 571769.7043620005\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03133401419222355\n",
      "Average test loss: 246152051827895.03\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03206183334688346\n",
      "Average test loss: 9.521720912801722\n",
      "Epoch 186/300\n",
      "Average training loss: 0.040347039740946555\n",
      "Average test loss: 97567861470.24313\n",
      "Epoch 187/300\n",
      "Average training loss: 0.031925236172146265\n",
      "Average test loss: 2354377.908653728\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03396503214372529\n",
      "Average test loss: 14554033257.536486\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03237828164961603\n",
      "Average test loss: 8696830.625984812\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031034401406844457\n",
      "Average test loss: 407018.1530823331\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03385051105254226\n",
      "Average test loss: 173.0453358407178\n",
      "Epoch 192/300\n",
      "Average training loss: 0.033911546462112\n",
      "Average test loss: 15931784.207166927\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03163214747773276\n",
      "Average test loss: 291345.8337601534\n",
      "Epoch 194/300\n",
      "Average training loss: 0.031103341066175037\n",
      "Average test loss: 751227222944.4711\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03538258243103822\n",
      "Average test loss: 1674953912.2334907\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03580693482028113\n",
      "Average test loss: 1.8356465853707037e+19\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03395467432174418\n",
      "Average test loss: 7178698042126.876\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0330172624770138\n",
      "Average test loss: 8837664325.441011\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03270169643561045\n",
      "Average test loss: 24412298.88515958\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03744519521461593\n",
      "Average test loss: 21117160653.00335\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03358811760776573\n",
      "Average test loss: 95691274745.02222\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03506994280715783\n",
      "Average test loss: 32426048.96878407\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03231124037504196\n",
      "Average test loss: 9.634483335166134\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03931378663579623\n",
      "Average test loss: 7259830154.705215\n",
      "Epoch 205/300\n",
      "Average training loss: 0.032144559719496306\n",
      "Average test loss: 163540317.79285043\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03367700065341261\n",
      "Average test loss: 0.006628477115391029\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03749022554192278\n",
      "Average test loss: 35314.26585252938\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03633196442988184\n",
      "Average test loss: 16.990208332232303\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03560050033529599\n",
      "Average test loss: 447.6012779770245\n",
      "Epoch 210/300\n",
      "Average training loss: 0.031897827131880656\n",
      "Average test loss: 2670037.1628850168\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030906362536880706\n",
      "Average test loss: 49749824.715964004\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030699838366773393\n",
      "Average test loss: 124626.11121466837\n",
      "Epoch 213/300\n",
      "Average training loss: 0.031624568257066936\n",
      "Average test loss: 6012.911724245136\n",
      "Epoch 214/300\n",
      "Average training loss: 0.031096041914489534\n",
      "Average test loss: 656564969967004.4\n",
      "Epoch 215/300\n",
      "Average training loss: 0.032071735142005814\n",
      "Average test loss: 0.151497110971974\n",
      "Epoch 216/300\n",
      "Average training loss: 0.031748340961005955\n",
      "Average test loss: 2809414714.0155964\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03081235017047988\n",
      "Average test loss: 5666.292223824322\n",
      "Epoch 218/300\n",
      "Average training loss: 0.034762032747268676\n",
      "Average test loss: 0.005051689775867595\n",
      "Epoch 219/300\n",
      "Average training loss: 0.031235727318459087\n",
      "Average test loss: 1.524855980316384\n",
      "Epoch 220/300\n",
      "Average training loss: 0.031414967876341605\n",
      "Average test loss: 41859157.23554468\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04507414959536658\n",
      "Average test loss: 11061.708303230691\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03291391775177585\n",
      "Average test loss: 46120211536168.56\n",
      "Epoch 223/300\n",
      "Average training loss: 0.032586174491378996\n",
      "Average test loss: 3756274274593.365\n",
      "Epoch 224/300\n",
      "Average training loss: 0.033671776579486\n",
      "Average test loss: 413204755192.704\n",
      "Epoch 225/300\n",
      "Average training loss: 0.031816223207447265\n",
      "Average test loss: 2.2345455313383544e+16\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0354362985988458\n",
      "Average test loss: 1064283610.2755556\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04946492011513975\n",
      "Average test loss: 1194829163.8687522\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03439336170090569\n",
      "Average test loss: 210439462297.03726\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03186404526068105\n",
      "Average test loss: 30793652.799119793\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03744810873932309\n",
      "Average test loss: 240093143121.81128\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03723063867290815\n",
      "Average test loss: 57974.11055795938\n",
      "Epoch 232/300\n",
      "Average training loss: 0.032411587786343364\n",
      "Average test loss: 1300556595.7883933\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03149794540968206\n",
      "Average test loss: 25193402970.684464\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03756326215962569\n",
      "Average test loss: 87519393.22001031\n",
      "Epoch 235/300\n",
      "Average training loss: 0.031222702735000187\n",
      "Average test loss: 39755977525.894684\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030446963747342428\n",
      "Average test loss: 138325393525.70374\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03234510248733891\n",
      "Average test loss: 1023864595922612.0\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03334327436321312\n",
      "Average test loss: 50263233676368.1\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04323074378238784\n",
      "Average test loss: 28901558733.474255\n",
      "Epoch 240/300\n",
      "Average training loss: 0.041219638920492595\n",
      "Average test loss: 72989216995.25325\n",
      "Epoch 241/300\n",
      "Average training loss: 0.033136988250745666\n",
      "Average test loss: 6459134022041.127\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03242858651114835\n",
      "Average test loss: 85050208564754.66\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03271622672180335\n",
      "Average test loss: 3195368958570652.0\n",
      "Epoch 244/300\n",
      "Average training loss: 0.031090658298797078\n",
      "Average test loss: 2655799593.2138295\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03033770040008757\n",
      "Average test loss: 4417244042092231.0\n",
      "Epoch 246/300\n",
      "Average training loss: 0.032481948163774275\n",
      "Average test loss: 47358978505.64192\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030652063548564912\n",
      "Average test loss: 806489016292.8304\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03636311168306404\n",
      "Average test loss: 2436941102155302.0\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03206231216092904\n",
      "Average test loss: 1.284479439878396e+20\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03446037430730131\n",
      "Average test loss: 327.32196262567595\n",
      "Epoch 251/300\n",
      "Average training loss: 0.038674828360478086\n",
      "Average test loss: 315998422.91202176\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03364658795297146\n",
      "Average test loss: 12233436577813.852\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03197444851530923\n",
      "Average test loss: 11532410245997.229\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03162452671262953\n",
      "Average test loss: 34080937016200.12\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03281183621618483\n",
      "Average test loss: 149612102947.65515\n",
      "Epoch 256/300\n",
      "Average training loss: 0.037063163214259676\n",
      "Average test loss: 4642689.412357141\n",
      "Epoch 257/300\n",
      "Average training loss: 0.032281835284498\n",
      "Average test loss: 332280180938.2683\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030617372560832235\n",
      "Average test loss: 6157362.408263587\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03119946739408705\n",
      "Average test loss: 1.7416948531509936e+20\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03554174244238271\n",
      "Average test loss: 5401373056.520486\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03205431719621023\n",
      "Average test loss: 9.464654934328177e+18\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03235534511672126\n",
      "Average test loss: 23571535095.85249\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03067864725490411\n",
      "Average test loss: 62359960359.86654\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029827084738347266\n",
      "Average test loss: 4510438560.991654\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0382999066180653\n",
      "Average test loss: 8682383896.635326\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03195829312503338\n",
      "Average test loss: 68461310228236.63\n",
      "Epoch 267/300\n",
      "Average training loss: 0.036895106749402155\n",
      "Average test loss: 52853739479.0129\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030503894431723487\n",
      "Average test loss: 129385473.48372331\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03355913206934929\n",
      "Average test loss: 42134046306345.54\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029632655458317864\n",
      "Average test loss: 647242.810953603\n",
      "Epoch 271/300\n",
      "Average training loss: 0.034502084523439405\n",
      "Average test loss: 14803767288.645155\n",
      "Epoch 272/300\n",
      "Average training loss: 0.030152732604079777\n",
      "Average test loss: 12921661.921721147\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02944302779932817\n",
      "Average test loss: 10422459.974997435\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029565523978736664\n",
      "Average test loss: 35219.44762511705\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030187942162156103\n",
      "Average test loss: 2306421.350098654\n",
      "Epoch 276/300\n",
      "Average training loss: 0.031438960573739475\n",
      "Average test loss: 10448106.288855476\n",
      "Epoch 277/300\n",
      "Average training loss: 0.040823902719550664\n",
      "Average test loss: 266686275314.75204\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03191550803184509\n",
      "Average test loss: 7934569468172.092\n",
      "Epoch 279/300\n",
      "Average training loss: 0.031945045828819275\n",
      "Average test loss: 87010560762.17102\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02998317635887199\n",
      "Average test loss: 4.861180376488332e+21\n",
      "Epoch 281/300\n",
      "Average training loss: 0.031207574205266105\n",
      "Average test loss: 605473263067.9325\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030298706341120932\n",
      "Average test loss: 796795438.648207\n",
      "Epoch 283/300\n",
      "Average training loss: 0.052748550842205685\n",
      "Average test loss: 408248358.2135977\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04037278784811497\n",
      "Average test loss: 198436212.71938577\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03455143509970771\n",
      "Average test loss: 128611535.26424533\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03490158870485094\n",
      "Average test loss: 1105529913.8618271\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03463971594472726\n",
      "Average test loss: 686909456.341478\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03428592933548821\n",
      "Average test loss: 8896252329.915407\n",
      "Epoch 289/300\n",
      "Average training loss: 0.031363985810014934\n",
      "Average test loss: 1214472149646.4788\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04585043544570605\n",
      "Average test loss: 6733921356148812.0\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04352758378121588\n",
      "Average test loss: 25365181620.906704\n",
      "Epoch 292/300\n",
      "Average training loss: 0.034972230345010756\n",
      "Average test loss: 1051035811.9323483\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03460584156711896\n",
      "Average test loss: 14980168801.919718\n",
      "Epoch 294/300\n",
      "Average training loss: 0.032761679702334935\n",
      "Average test loss: 1.015869526391083e+21\n",
      "Epoch 295/300\n",
      "Average training loss: 0.036250507324934005\n",
      "Average test loss: 4006209529496.906\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03153879091805882\n",
      "Average test loss: 711620.8477614834\n",
      "Epoch 297/300\n",
      "Average training loss: 0.030492460287279553\n",
      "Average test loss: 8335557372.227818\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04755791898237335\n",
      "Average test loss: 4.711388751084192\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03348602291279369\n",
      "Average test loss: 8859.348946252565\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03191333888636695\n",
      "Average test loss: 5697.524715617521\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.466619638337029\n",
      "Average test loss: 733.6651039994624\n",
      "Epoch 2/300\n",
      "Average training loss: 0.8863614582485623\n",
      "Average test loss: 21.824677263040922\n",
      "Epoch 3/300\n",
      "Average training loss: 0.523807927634981\n",
      "Average test loss: 0.008909057321233881\n",
      "Epoch 4/300\n",
      "Average training loss: 0.39881716966629027\n",
      "Average test loss: 1748.1608267379254\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3099009431997935\n",
      "Average test loss: 0.39300723529110354\n",
      "Epoch 6/300\n",
      "Average training loss: 0.263821719010671\n",
      "Average test loss: 0.5849280833039019\n",
      "Epoch 7/300\n",
      "Average training loss: 0.19583517093128627\n",
      "Average test loss: 0.3003159768809047\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15170958813031515\n",
      "Average test loss: 1088.716638480198\n",
      "Epoch 9/300\n",
      "Average training loss: 0.12343753649128808\n",
      "Average test loss: 0.06337971287448373\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10633167807261149\n",
      "Average test loss: 0.2373901821821928\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09087087131208843\n",
      "Average test loss: 0.006359549686312676\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07823154854774475\n",
      "Average test loss: 0.41550719197632535\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07001072316699558\n",
      "Average test loss: 0.033166595103840034\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06308832310305701\n",
      "Average test loss: 0.003865956170691384\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05639761235647731\n",
      "Average test loss: 0.0046586295136560995\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05239008407460319\n",
      "Average test loss: 0.23695334105690322\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04845052190621694\n",
      "Average test loss: 0.1999559140668975\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04490981120533413\n",
      "Average test loss: 0.019860182985249494\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04240187968479262\n",
      "Average test loss: 0.11235467856832676\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04043857921494378\n",
      "Average test loss: 0.008715821353097756\n",
      "Epoch 21/300\n",
      "Average training loss: 0.037767068644364674\n",
      "Average test loss: 0.003647009181065692\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03559067830608951\n",
      "Average test loss: 0.0037034099177560872\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03717371113432778\n",
      "Average test loss: 0.015063014483286275\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03235018043385612\n",
      "Average test loss: 0.003159174723343717\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03123721507853932\n",
      "Average test loss: 0.003879146830489238\n",
      "Epoch 26/300\n",
      "Average training loss: 0.031109338000416757\n",
      "Average test loss: 0.0042055504599379165\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0294916536261638\n",
      "Average test loss: 0.004065216083907419\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03127902158598105\n",
      "Average test loss: 0.0031056514034668603\n",
      "Epoch 29/300\n",
      "Average training loss: 0.027068740771876442\n",
      "Average test loss: 0.005878372998701201\n",
      "Epoch 30/300\n",
      "Average training loss: 0.026075007927086617\n",
      "Average test loss: 0.007171768993553188\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02562791114052137\n",
      "Average test loss: 0.008078590451222327\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024443339932296013\n",
      "Average test loss: 0.003100302998390463\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02427251540621122\n",
      "Average test loss: 0.002937894011537234\n",
      "Epoch 34/300\n",
      "Average training loss: 0.023678852652510008\n",
      "Average test loss: 0.0029851956837293175\n",
      "Epoch 35/300\n",
      "Average training loss: 0.022900120195415284\n",
      "Average test loss: 0.002879457460095485\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02380492569671737\n",
      "Average test loss: 0.0030967966336756946\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02219850837190946\n",
      "Average test loss: 0.04214593663977252\n",
      "Epoch 38/300\n",
      "Average training loss: 0.021690811718503632\n",
      "Average test loss: 0.002908693574368954\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02164544785519441\n",
      "Average test loss: 0.003127262913518482\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020884097327788672\n",
      "Average test loss: 0.010351622116234567\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02058324024826288\n",
      "Average test loss: 0.09877825469606452\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020291896689269277\n",
      "Average test loss: 0.003112687898385856\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01997238833953937\n",
      "Average test loss: 0.006664445573257075\n",
      "Epoch 44/300\n",
      "Average training loss: 0.025292086814840634\n",
      "Average test loss: 0.0029983982625934815\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020564810514450072\n",
      "Average test loss: 0.002896273052526845\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019980602635277643\n",
      "Average test loss: 0.0029237347304200134\n",
      "Epoch 47/300\n",
      "Average training loss: 0.020217932244141895\n",
      "Average test loss: 0.0029640454774101576\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01952081140710248\n",
      "Average test loss: 0.002832906660934289\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019119239055448108\n",
      "Average test loss: 0.0028156420652651125\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019329229099882973\n",
      "Average test loss: 0.0029131954465475346\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018836863555841975\n",
      "Average test loss: 0.002976599664737781\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018823193343149292\n",
      "Average test loss: 0.002822647548177176\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018542248111632135\n",
      "Average test loss: 0.002803279913547966\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018290423722730743\n",
      "Average test loss: 0.0028715936916155946\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01809801997327142\n",
      "Average test loss: 0.0027888783659372066\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017955245246489844\n",
      "Average test loss: 0.004946719784289598\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017968199957576064\n",
      "Average test loss: 0.002773506181521548\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01761193988058302\n",
      "Average test loss: 0.002802068926083545\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018020423582030667\n",
      "Average test loss: 0.002896692359406087\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017398567576375274\n",
      "Average test loss: 0.002808670185920265\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017360746503704123\n",
      "Average test loss: 0.0028673143167462613\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01728304726878802\n",
      "Average test loss: 0.0032080993230144184\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017074088835053975\n",
      "Average test loss: 0.0027428636288063394\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01685128319842948\n",
      "Average test loss: 0.005033409169150724\n",
      "Epoch 65/300\n",
      "Average training loss: 0.016804225214653544\n",
      "Average test loss: 0.0027768948723872504\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016761478561494087\n",
      "Average test loss: 11.869315286530389\n",
      "Epoch 67/300\n",
      "Average training loss: 0.016600190695789125\n",
      "Average test loss: 0.0027691664695739747\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016591544582611986\n",
      "Average test loss: 0.002829548103734851\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0281075003710058\n",
      "Average test loss: 0.3558646598238912\n",
      "Epoch 70/300\n",
      "Average training loss: 0.029701866025726\n",
      "Average test loss: 0.004490047729988065\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02290077302687698\n",
      "Average test loss: 65.43279450362382\n",
      "Epoch 72/300\n",
      "Average training loss: 0.021360861543152067\n",
      "Average test loss: 211.64407035059895\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020478750195768145\n",
      "Average test loss: 0.003159358779589335\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020091641227404276\n",
      "Average test loss: 3.9049613146293494\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019733908492657874\n",
      "Average test loss: 0.5232670251321462\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01906186538272434\n",
      "Average test loss: 1.5150158349134857\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018623055521812704\n",
      "Average test loss: 0.16267758260812196\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018626820656988355\n",
      "Average test loss: 0.0032829368826415802\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017945702589220472\n",
      "Average test loss: 0.09646606230270117\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01749566572904587\n",
      "Average test loss: 0.007267987695781307\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01734146538045671\n",
      "Average test loss: 3.5461773692342557\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017204736412399346\n",
      "Average test loss: 0.20155218458424012\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019204199103017648\n",
      "Average test loss: 0.0029992835455470617\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017247129035492737\n",
      "Average test loss: 0.0028511133210526573\n",
      "Epoch 85/300\n",
      "Average training loss: 0.016624576369093525\n",
      "Average test loss: 0.0029818656374182967\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0164118526958757\n",
      "Average test loss: 0.05644812574237585\n",
      "Epoch 87/300\n",
      "Average training loss: 0.016331578373081156\n",
      "Average test loss: 0.0027499233001015253\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018681954302721553\n",
      "Average test loss: 0.0027585768316768936\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016841712658603986\n",
      "Average test loss: 0.003176767497633894\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016243103253344694\n",
      "Average test loss: 0.004682312455235256\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01605889274345504\n",
      "Average test loss: 0.0028272931480573284\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015954677843385272\n",
      "Average test loss: 0.0027560823884689146\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01588030016836193\n",
      "Average test loss: 0.0030186797339055273\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01582295086234808\n",
      "Average test loss: 0.002745587664966782\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015964848634269502\n",
      "Average test loss: 0.13086995015210576\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015773325574894748\n",
      "Average test loss: 0.004330362061245574\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016098381517661943\n",
      "Average test loss: 0.003184048334757487\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015721581632064448\n",
      "Average test loss: 0.024280220695667796\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015967197140885726\n",
      "Average test loss: 0.0028127128860602774\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015383537688189083\n",
      "Average test loss: 0.002804001286625862\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01531668613354365\n",
      "Average test loss: 0.021054281024469268\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015316381216049195\n",
      "Average test loss: 0.0028666487468613517\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015361794758174155\n",
      "Average test loss: 0.0030512391140477524\n",
      "Epoch 104/300\n",
      "Average training loss: 0.020052824748886956\n",
      "Average test loss: 0.005755732787359092\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018636531160937414\n",
      "Average test loss: 0.0027595938762856854\n",
      "Epoch 106/300\n",
      "Average training loss: 0.016704830194513003\n",
      "Average test loss: 0.004377190540027287\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01576295697854625\n",
      "Average test loss: 0.0038511756350182824\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015301387072023418\n",
      "Average test loss: 0.004132243636581633\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015051068248020279\n",
      "Average test loss: 0.0027857635929766627\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014927678197622299\n",
      "Average test loss: 0.0029997344807618192\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01497219930920336\n",
      "Average test loss: 0.0033834297936409714\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014979123200807307\n",
      "Average test loss: 0.0028250839211460617\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015113522293666998\n",
      "Average test loss: 0.003465628661422266\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014879393396278223\n",
      "Average test loss: 0.002878788373122613\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014827316185666455\n",
      "Average test loss: 0.002950488004212578\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01480853373888466\n",
      "Average test loss: 0.0029556016801959937\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017704100098874832\n",
      "Average test loss: 0.0028456021282407973\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016125008751948673\n",
      "Average test loss: 0.0033982698844952715\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015172183053361045\n",
      "Average test loss: 0.003021425828130709\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014784477069146103\n",
      "Average test loss: 0.002782144783064723\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014675609382490316\n",
      "Average test loss: 0.0028156967274844645\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014731869945095646\n",
      "Average test loss: 0.0030730878921846547\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014621743019256327\n",
      "Average test loss: 0.0028057930984844762\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015013832377890746\n",
      "Average test loss: 0.0028405214585363863\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014606249055100811\n",
      "Average test loss: 0.002879348947770066\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01502615046997865\n",
      "Average test loss: 0.004667590127636989\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01446089203324583\n",
      "Average test loss: 0.002865587966102693\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014491449528270297\n",
      "Average test loss: 0.0048611433311469025\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014347654701107078\n",
      "Average test loss: 0.003026429094788101\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017005916604565248\n",
      "Average test loss: 0.0031425316472434334\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014739228509366513\n",
      "Average test loss: 0.0032580642509791586\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014253718333111868\n",
      "Average test loss: 0.003857469444887506\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01425390131274859\n",
      "Average test loss: 0.007757835208541817\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014581478664444553\n",
      "Average test loss: 0.004221180549098385\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014179055083129142\n",
      "Average test loss: 0.002920964140858915\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014195803449385695\n",
      "Average test loss: 0.0028958093625389867\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018273317019144694\n",
      "Average test loss: 0.002801164044895106\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015008411820564005\n",
      "Average test loss: 0.002903412421130472\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014141531380514305\n",
      "Average test loss: 0.003131084091547463\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01398835981471671\n",
      "Average test loss: 0.0029683865188724467\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01402888428999318\n",
      "Average test loss: 0.0028471043205095664\n",
      "Epoch 142/300\n",
      "Average training loss: 0.030203317116532062\n",
      "Average test loss: 0.0029359242285912236\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01956520157555739\n",
      "Average test loss: 0.002783033902446429\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01796609216597345\n",
      "Average test loss: 0.0031054021819598146\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01707268163975742\n",
      "Average test loss: 0.0028200035931335555\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016612314701908163\n",
      "Average test loss: 0.0028272926014744574\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016194151944584317\n",
      "Average test loss: 0.002794160126398007\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015976019310454528\n",
      "Average test loss: 0.005945872683491972\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016253088279730744\n",
      "Average test loss: 0.003028263048993217\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01565425140493446\n",
      "Average test loss: 0.003056298679775662\n",
      "Epoch 151/300\n",
      "Average training loss: 0.015428788103991085\n",
      "Average test loss: 0.009912060033943917\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03824442970752716\n",
      "Average test loss: 0.08039030580057038\n",
      "Epoch 153/300\n",
      "Average training loss: 0.025250381997889943\n",
      "Average test loss: 0.022804699058334033\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022119422923359607\n",
      "Average test loss: 0.003037270835083392\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02013357366952631\n",
      "Average test loss: 0.3218054846127828\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019510191498531237\n",
      "Average test loss: 0.00526087124273181\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01900098075138198\n",
      "Average test loss: 0.007917577502214246\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01790552148388492\n",
      "Average test loss: 0.00309628014659716\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017338682312104437\n",
      "Average test loss: 0.0028715356809811458\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01694051720533106\n",
      "Average test loss: 0.1001906547612614\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016657519140177303\n",
      "Average test loss: 14.57701742055681\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01647347840666771\n",
      "Average test loss: 0.005451673854556348\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0161726539482673\n",
      "Average test loss: 0.009149422861635685\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016005666350324948\n",
      "Average test loss: 0.04648551427324613\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015973977096378805\n",
      "Average test loss: 0.003099490371429258\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01575175852411323\n",
      "Average test loss: 0.23223629952967167\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016614059623744754\n",
      "Average test loss: 0.002930594095753299\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01541474226117134\n",
      "Average test loss: 0.0027932849133180246\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01526805793080065\n",
      "Average test loss: 0.0028907698498417934\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015169042341411114\n",
      "Average test loss: 0.0032699423951821193\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015205299672981103\n",
      "Average test loss: 0.008795188577845692\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015129723677204715\n",
      "Average test loss: 0.002945352894978391\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015478373332156075\n",
      "Average test loss: 0.48559229526254866\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015834319922659133\n",
      "Average test loss: 0.003142216948585378\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014868139079875416\n",
      "Average test loss: 0.0029336180355813766\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014862024052275552\n",
      "Average test loss: 0.007984062993692026\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016193855740129947\n",
      "Average test loss: 0.0029451396036893128\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015215386062860489\n",
      "Average test loss: 0.0030493328709983165\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014717205877933237\n",
      "Average test loss: 0.0028612672353370322\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014636269431147311\n",
      "Average test loss: 0.0028735985012931955\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014561935899158319\n",
      "Average test loss: 0.0028358542412105533\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014609530195593834\n",
      "Average test loss: 0.00304739218370782\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014533755987882615\n",
      "Average test loss: 0.002951151369760434\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014631959911849764\n",
      "Average test loss: 0.0030740864806705053\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015228017209304704\n",
      "Average test loss: 0.003184552748998006\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014388244321776761\n",
      "Average test loss: 0.003003476163165437\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014292889453470707\n",
      "Average test loss: 0.009559490359491772\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014397991726795832\n",
      "Average test loss: 0.011626019189755123\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014506843193537658\n",
      "Average test loss: 0.0029265975927313167\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014256514329049322\n",
      "Average test loss: 0.00309515765019589\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014183806700011094\n",
      "Average test loss: 0.0028271726814823018\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015437899874316322\n",
      "Average test loss: 0.002968438750339879\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014179054343038134\n",
      "Average test loss: 0.0028760056909587647\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014108040388259623\n",
      "Average test loss: 0.002921852852528294\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020072046005063588\n",
      "Average test loss: 0.0028722139741811487\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017198324509792858\n",
      "Average test loss: 0.002889524382435613\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015339014121227795\n",
      "Average test loss: 0.003934465962151686\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015003399090634452\n",
      "Average test loss: 0.003712486470014685\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01437225992563698\n",
      "Average test loss: 0.003779050865314073\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014435362939205434\n",
      "Average test loss: 0.00310775421332154\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014088658213615417\n",
      "Average test loss: 0.003327252360060811\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014661069115830792\n",
      "Average test loss: 0.07692838948302799\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01423534197691414\n",
      "Average test loss: 0.0029376022608743773\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013867798373103141\n",
      "Average test loss: 0.0029835104938182567\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013911489885714319\n",
      "Average test loss: 0.002903301045919458\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023811672254569\n",
      "Average test loss: 0.0029396748267528083\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019439533290763696\n",
      "Average test loss: 0.0029449936882075334\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016197354089054795\n",
      "Average test loss: 0.0028747799947030014\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015102159736884965\n",
      "Average test loss: 0.002900731058170398\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014592486018107998\n",
      "Average test loss: 0.0029032329958346157\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014379322684473462\n",
      "Average test loss: 0.003034302947214908\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01417306702915165\n",
      "Average test loss: 0.0029682445294327204\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014029930361443095\n",
      "Average test loss: 0.01436870374282201\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013905343025094934\n",
      "Average test loss: 0.0029590292116627097\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01408634221388234\n",
      "Average test loss: 0.0030201881805227864\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013874257685409652\n",
      "Average test loss: 0.0028692005860308806\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014293406154546473\n",
      "Average test loss: 0.002878977307014995\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01369024195190933\n",
      "Average test loss: 0.0029514975899623498\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013759910085962878\n",
      "Average test loss: 0.003154234879753656\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013767185950030883\n",
      "Average test loss: 0.002983198252403074\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013799057688977984\n",
      "Average test loss: 0.004155422737821937\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013771897523767418\n",
      "Average test loss: 0.002898105847545796\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013811494970487224\n",
      "Average test loss: 0.0029987914177278677\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01834426018347343\n",
      "Average test loss: 0.0045512431301176545\n",
      "Epoch 225/300\n",
      "Average training loss: 0.015801721721059748\n",
      "Average test loss: 0.002821870254559649\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014152782650457487\n",
      "Average test loss: 0.004169876576297813\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013694858328335816\n",
      "Average test loss: 0.0030091814338746997\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01445636575917403\n",
      "Average test loss: 0.002962521516200569\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013588055603206157\n",
      "Average test loss: 0.0030256342569159136\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013554089572694567\n",
      "Average test loss: 0.004368795541425546\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01350852945778105\n",
      "Average test loss: 0.002919831493558983\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013455175082716677\n",
      "Average test loss: 0.06274548566341401\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013977697777251403\n",
      "Average test loss: 0.00291155374277797\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013524406963752376\n",
      "Average test loss: 0.0028940518502559927\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013427579135530525\n",
      "Average test loss: 0.0028928543817665843\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013536999117996957\n",
      "Average test loss: 0.0029386166652871503\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014574519030749798\n",
      "Average test loss: 0.0036658608611259197\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013454199564125803\n",
      "Average test loss: 0.0028853290693627463\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013545383863151074\n",
      "Average test loss: 0.002926097838828961\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013406518928706646\n",
      "Average test loss: 0.0028980983646793496\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013598407171666622\n",
      "Average test loss: 0.0029862723919666474\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013328479528427124\n",
      "Average test loss: 0.003087351347423262\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013315105927487214\n",
      "Average test loss: 0.0028929766733199356\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013809927370813158\n",
      "Average test loss: 0.0028942005435625712\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013387007320092783\n",
      "Average test loss: 0.0030137084925340282\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013215278310908211\n",
      "Average test loss: 0.0028826461740665966\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01329489516797993\n",
      "Average test loss: 0.0029167612695859537\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013452421825793055\n",
      "Average test loss: 0.005805683156682386\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01335501154470775\n",
      "Average test loss: 0.003084053466303481\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015547362904581759\n",
      "Average test loss: 0.0030220662206411363\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01675037506967783\n",
      "Average test loss: 0.003764834169505371\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014210166302820047\n",
      "Average test loss: 0.012879081328295999\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013473696370919545\n",
      "Average test loss: 0.09004023311038813\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01431031282991171\n",
      "Average test loss: 0.00300383269517786\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013136154374314679\n",
      "Average test loss: 0.002924596574364437\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013071712536944283\n",
      "Average test loss: 0.002952789074016942\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013069837721685568\n",
      "Average test loss: 0.003440332437141074\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013149162714680036\n",
      "Average test loss: 0.07453450771503978\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01314449005573988\n",
      "Average test loss: 0.0029608565899543462\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01316002915882402\n",
      "Average test loss: 0.0029394669729388423\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013629156505895985\n",
      "Average test loss: 0.03742478865219487\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013036806791192955\n",
      "Average test loss: 0.017813094064593315\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013076606118016773\n",
      "Average test loss: 0.0029702582969847653\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018080283318128852\n",
      "Average test loss: 0.0029846225854837233\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015870452711151707\n",
      "Average test loss: 0.0029571894359671407\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014425690638522308\n",
      "Average test loss: 0.0037092095309247575\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013906828723847867\n",
      "Average test loss: 0.0029053231249045993\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01354568152791924\n",
      "Average test loss: 0.02076812841826015\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01365446000877354\n",
      "Average test loss: 0.004505772302962012\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013357071576019128\n",
      "Average test loss: 0.0029346053813480667\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013388332993206051\n",
      "Average test loss: 0.003199343600620826\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013423897365729014\n",
      "Average test loss: 0.004957616595551372\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013139501182569398\n",
      "Average test loss: 0.002960408078506589\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013961011556287607\n",
      "Average test loss: 0.0029298788560554387\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013123146012425422\n",
      "Average test loss: 0.0029242664854973554\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01323142304768165\n",
      "Average test loss: 0.003013166224790944\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01360467740934756\n",
      "Average test loss: 0.003324663298411502\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012951852669318517\n",
      "Average test loss: 0.0031389219566351836\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01305468332933055\n",
      "Average test loss: 0.0029408171489420866\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013038381112118563\n",
      "Average test loss: 0.003003434756356809\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01311825184027354\n",
      "Average test loss: 0.002927361732141839\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013033590343263414\n",
      "Average test loss: 0.0030682914092515904\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013944812578459581\n",
      "Average test loss: 0.002946698929286665\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012890904770129257\n",
      "Average test loss: 0.00296291148621175\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012790862411260605\n",
      "Average test loss: 0.005188257278460596\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018444828742080265\n",
      "Average test loss: 0.0030110286256919304\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015000327786637678\n",
      "Average test loss: 0.0029762028937952386\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014740673686895106\n",
      "Average test loss: 0.004548039492012726\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015375424834589164\n",
      "Average test loss: 0.002983986248469187\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01440729432221916\n",
      "Average test loss: 0.00300988062346975\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01357994456589222\n",
      "Average test loss: 0.002899290030201276\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013461599849992328\n",
      "Average test loss: 0.0029264814073426855\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014528106138938002\n",
      "Average test loss: 0.0030343793300497864\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013205640781256888\n",
      "Average test loss: 0.00385329653074344\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013122933123674657\n",
      "Average test loss: 0.003264238154515624\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013041194015079074\n",
      "Average test loss: 0.0029945121521337166\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013084171776142385\n",
      "Average test loss: 0.003772664931706256\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013208424388948413\n",
      "Average test loss: 0.003085726250376966\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012912372441755401\n",
      "Average test loss: 0.0029394080258078044\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017873972334795526\n",
      "Average test loss: 15.427612884046303\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.1155970400704276\n",
      "Average test loss: 0.21660489862495\n",
      "Epoch 2/300\n",
      "Average training loss: 1.8347083042992487\n",
      "Average test loss: 13930.494458224826\n",
      "Epoch 3/300\n",
      "Average training loss: 1.4854273826811049\n",
      "Average test loss: 323.53772054779114\n",
      "Epoch 4/300\n",
      "Average training loss: 1.173361600557963\n",
      "Average test loss: 1.9681661499374443\n",
      "Epoch 5/300\n",
      "Average training loss: 0.939131357828776\n",
      "Average test loss: 480868.012683431\n",
      "Epoch 6/300\n",
      "Average training loss: 0.7574392810397678\n",
      "Average test loss: 0.019878050319850446\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6429964706103007\n",
      "Average test loss: 561.6190440912776\n",
      "Epoch 8/300\n",
      "Average training loss: 0.5512698861757914\n",
      "Average test loss: 207.94937961114323\n",
      "Epoch 9/300\n",
      "Average training loss: 0.4671720584233602\n",
      "Average test loss: 82.14002636300525\n",
      "Epoch 10/300\n",
      "Average training loss: 0.39492661476135255\n",
      "Average test loss: 1.0959260481740865\n",
      "Epoch 11/300\n",
      "Average training loss: 0.34946079805162217\n",
      "Average test loss: 0.03007767690407733\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3053707984023624\n",
      "Average test loss: 20.687671400617393\n",
      "Epoch 13/300\n",
      "Average training loss: 0.26504565625720555\n",
      "Average test loss: 6127.190728277418\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2325238689184189\n",
      "Average test loss: 0.0028503063482542834\n",
      "Epoch 15/300\n",
      "Average training loss: 0.20362592629591625\n",
      "Average test loss: 2.752254866374036\n",
      "Epoch 16/300\n",
      "Average training loss: 0.18841974390877617\n",
      "Average test loss: 0.0026161119213534726\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1688924878968133\n",
      "Average test loss: 3.7543283703078827\n",
      "Epoch 18/300\n",
      "Average training loss: 0.14291904491848414\n",
      "Average test loss: 0.02933035472780466\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1254615699979994\n",
      "Average test loss: 6.550568984267199\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10719447894891103\n",
      "Average test loss: 0.0034668207698398167\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0919656754732132\n",
      "Average test loss: 6.278147382530901\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08020170669423209\n",
      "Average test loss: 0.02282024955563247\n",
      "Epoch 23/300\n",
      "Average training loss: 0.069494617505206\n",
      "Average test loss: 0.0023032339942745035\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06011788271533118\n",
      "Average test loss: 1.660753134086728\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05361232385701603\n",
      "Average test loss: 0.007897201429638599\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05149975558121999\n",
      "Average test loss: 0.24602793938501014\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0442018953760465\n",
      "Average test loss: 0.49694682544769925\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03955604936016931\n",
      "Average test loss: 0.010309658701014187\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0362358575993114\n",
      "Average test loss: 0.002829310572395722\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03412391721705595\n",
      "Average test loss: 0.0025937948191745415\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03124405789044168\n",
      "Average test loss: 0.0029290196001529696\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02924850951631864\n",
      "Average test loss: 0.0019723172296459476\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02762240280707677\n",
      "Average test loss: 0.001977533536652724\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02599376402133041\n",
      "Average test loss: 0.0020796305042588047\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024545729766289392\n",
      "Average test loss: 0.0019136420525610446\n",
      "Epoch 36/300\n",
      "Average training loss: 0.023239709930287466\n",
      "Average test loss: 0.005892614313918684\n",
      "Epoch 37/300\n",
      "Average training loss: 0.022564396176073285\n",
      "Average test loss: 0.0019406704861256812\n",
      "Epoch 38/300\n",
      "Average training loss: 0.021126216431458792\n",
      "Average test loss: 0.005356284870455663\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020259590006536907\n",
      "Average test loss: 0.0019186867216808927\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01924308152165678\n",
      "Average test loss: 0.002957422969862819\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01935341990325186\n",
      "Average test loss: 0.004138082375336025\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018699510938591426\n",
      "Average test loss: 0.0045625069828497036\n",
      "Epoch 43/300\n",
      "Average training loss: 0.017394296104709307\n",
      "Average test loss: 0.0018802755028009416\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017159918732941152\n",
      "Average test loss: 0.0020475048853291406\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016235447688235177\n",
      "Average test loss: 0.0018907910571951006\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015593466745482551\n",
      "Average test loss: 0.0020268272465715806\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0154928434226248\n",
      "Average test loss: 0.0018312928002948562\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014805116213858127\n",
      "Average test loss: 0.0018250438624786006\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014450994103319115\n",
      "Average test loss: 0.0018147668296264277\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014152004910012086\n",
      "Average test loss: 0.0024125568089592786\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013792117377122243\n",
      "Average test loss: 0.0018048682293544214\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014990867650343312\n",
      "Average test loss: 0.0017758373972028493\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013490779962804582\n",
      "Average test loss: 0.0018577907847551008\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013438551024430328\n",
      "Average test loss: 0.002017705119628873\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013107444621622562\n",
      "Average test loss: 0.0017904622558918264\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012721474093695482\n",
      "Average test loss: 0.0024893947281978196\n",
      "Epoch 57/300\n",
      "Average training loss: 0.016735946090684996\n",
      "Average test loss: 0.0018875184377862347\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01382982719110118\n",
      "Average test loss: 0.0017802858115691278\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013132982685334152\n",
      "Average test loss: 0.0017747354486750233\n",
      "Epoch 60/300\n",
      "Average training loss: 0.012725410778489378\n",
      "Average test loss: 0.0020257719635135597\n",
      "Epoch 61/300\n",
      "Average training loss: 0.012573792759743001\n",
      "Average test loss: 0.002730494987633493\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01236636341859897\n",
      "Average test loss: 0.0017619588934919901\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012450136004222763\n",
      "Average test loss: 0.0019753186931419702\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012236718407107725\n",
      "Average test loss: 0.0017561181511522995\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012284346193903022\n",
      "Average test loss: 0.001781877858357297\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011860030552579297\n",
      "Average test loss: 0.0018669623074432215\n",
      "Epoch 67/300\n",
      "Average training loss: 0.011802275612950325\n",
      "Average test loss: 0.0018079886617035502\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011639378554291195\n",
      "Average test loss: 0.0017743188519962132\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013252354996071922\n",
      "Average test loss: 0.0017940166256287032\n",
      "Epoch 70/300\n",
      "Average training loss: 0.011580050891472234\n",
      "Average test loss: 0.0018480340479355719\n",
      "Epoch 71/300\n",
      "Average training loss: 0.011387265395787027\n",
      "Average test loss: 0.0017531726958437098\n",
      "Epoch 72/300\n",
      "Average training loss: 0.011352674580282635\n",
      "Average test loss: 0.0020734850445555315\n",
      "Epoch 73/300\n",
      "Average training loss: 0.011286591230995124\n",
      "Average test loss: 0.0018004775909293029\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011159242501689329\n",
      "Average test loss: 0.001792401931869487\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012195461377501488\n",
      "Average test loss: 0.0017569143443057933\n",
      "Epoch 76/300\n",
      "Average training loss: 0.011167771139906512\n",
      "Average test loss: 0.0017704267323844962\n",
      "Epoch 77/300\n",
      "Average training loss: 0.011070076040095753\n",
      "Average test loss: 0.0018882106350113948\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012809046752750873\n",
      "Average test loss: 0.0017803019312106901\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012231101070841154\n",
      "Average test loss: 0.0017405921285972\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011341830061541663\n",
      "Average test loss: 0.004248307328050335\n",
      "Epoch 81/300\n",
      "Average training loss: 0.011148536642392477\n",
      "Average test loss: 0.0017528590064288842\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01093494247727924\n",
      "Average test loss: 0.00947044115099642\n",
      "Epoch 83/300\n",
      "Average training loss: 0.011228741731908586\n",
      "Average test loss: 0.0017356851775613097\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01071866555015246\n",
      "Average test loss: 0.0018417604736362895\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01109787584675683\n",
      "Average test loss: 0.003633468795981672\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010546803683870369\n",
      "Average test loss: 0.0017906782299073206\n",
      "Epoch 87/300\n",
      "Average training loss: 0.010511119537055493\n",
      "Average test loss: 0.00178158489211152\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011250220764842299\n",
      "Average test loss: 0.0017885601407744817\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011410499686996142\n",
      "Average test loss: 0.0018414534782576892\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01043109506368637\n",
      "Average test loss: 0.001757520931565927\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010387742364158233\n",
      "Average test loss: 0.010490288483599822\n",
      "Epoch 92/300\n",
      "Average training loss: 0.010227878611948755\n",
      "Average test loss: 0.0017544272829675012\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010184448473155498\n",
      "Average test loss: 0.0022994723276545605\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011848477069702414\n",
      "Average test loss: 0.0017907397888807786\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01926158480760124\n",
      "Average test loss: 0.11543997411926588\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013279593823684587\n",
      "Average test loss: 0.002782269081307782\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012145626310673025\n",
      "Average test loss: 0.004227853710349235\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011650059512092007\n",
      "Average test loss: 0.0055216070171445605\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011308169561127821\n",
      "Average test loss: 0.002054213977936241\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01102999126083321\n",
      "Average test loss: 0.002976394562464621\n",
      "Epoch 101/300\n",
      "Average training loss: 0.010840922344889906\n",
      "Average test loss: 0.0071542538661095835\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010729646183550358\n",
      "Average test loss: 0.0034531179906593427\n",
      "Epoch 103/300\n",
      "Average training loss: 0.010563825348185169\n",
      "Average test loss: 1.4301338732507494\n",
      "Epoch 104/300\n",
      "Average training loss: 0.010532074672480424\n",
      "Average test loss: 0.0033065520596380037\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01170051972899172\n",
      "Average test loss: 0.0018931710264748997\n",
      "Epoch 106/300\n",
      "Average training loss: 0.010927365269925859\n",
      "Average test loss: 0.003189133553248313\n",
      "Epoch 107/300\n",
      "Average training loss: 0.010623478149374326\n",
      "Average test loss: 0.0017556546891315116\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01034289516094658\n",
      "Average test loss: 0.03614874956674046\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01022849857269062\n",
      "Average test loss: 0.0018302012852703532\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010110086110730965\n",
      "Average test loss: 0.0019224704689242774\n",
      "Epoch 111/300\n",
      "Average training loss: 0.010134872170786064\n",
      "Average test loss: 0.002198064295368062\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01013863660974635\n",
      "Average test loss: 0.01445283220294449\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009936508080197705\n",
      "Average test loss: 0.0024918423911763563\n",
      "Epoch 114/300\n",
      "Average training loss: 0.009819148566159936\n",
      "Average test loss: 0.0023527953260474733\n",
      "Epoch 115/300\n",
      "Average training loss: 0.009832149556113614\n",
      "Average test loss: 0.0017714296699398093\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011876514062699345\n",
      "Average test loss: 0.0017960297607092393\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0098518022927973\n",
      "Average test loss: 0.0017941932228083411\n",
      "Epoch 118/300\n",
      "Average training loss: 0.009668616433938345\n",
      "Average test loss: 0.002335864823932449\n",
      "Epoch 119/300\n",
      "Average training loss: 0.009674783292743894\n",
      "Average test loss: 0.0018699238879813088\n",
      "Epoch 120/300\n",
      "Average training loss: 0.009581313719352086\n",
      "Average test loss: 0.001785676509141922\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009850216258317232\n",
      "Average test loss: 0.001861309225567513\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009816876295126146\n",
      "Average test loss: 0.0018893825815369685\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009444183067315155\n",
      "Average test loss: 0.001824462257222169\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009479909806201855\n",
      "Average test loss: 0.0019043476996529432\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012816953420639038\n",
      "Average test loss: 0.0019525754575928052\n",
      "Epoch 126/300\n",
      "Average training loss: 0.010073947495056523\n",
      "Average test loss: 0.0018122979908560713\n",
      "Epoch 127/300\n",
      "Average training loss: 0.009704959965414471\n",
      "Average test loss: 0.0018197751690944037\n",
      "Epoch 128/300\n",
      "Average training loss: 0.010034627298514048\n",
      "Average test loss: 0.0017898336831066344\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00991088755759928\n",
      "Average test loss: 0.001786869718796677\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009382001334594355\n",
      "Average test loss: 0.0018056633834623629\n",
      "Epoch 131/300\n",
      "Average training loss: 0.009401413958105776\n",
      "Average test loss: 0.002001983301507102\n",
      "Epoch 132/300\n",
      "Average training loss: 0.009254584563275178\n",
      "Average test loss: 0.0018254074447063937\n",
      "Epoch 133/300\n",
      "Average training loss: 0.009495033121357362\n",
      "Average test loss: 0.001955971022033029\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009875377884755532\n",
      "Average test loss: 0.0018769672847249441\n",
      "Epoch 135/300\n",
      "Average training loss: 0.009265290478865306\n",
      "Average test loss: 0.0018131646764361197\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013845157571964794\n",
      "Average test loss: 0.0018333464620841873\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01040604164203008\n",
      "Average test loss: 0.0018227542785720693\n",
      "Epoch 138/300\n",
      "Average training loss: 0.009563371581335862\n",
      "Average test loss: 0.006453350368266305\n",
      "Epoch 139/300\n",
      "Average training loss: 0.009298475628925695\n",
      "Average test loss: 0.0018243648215300508\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009815217803749773\n",
      "Average test loss: 0.0018513559568673372\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009145609471533034\n",
      "Average test loss: 0.0018788056784412926\n",
      "Epoch 142/300\n",
      "Average training loss: 0.009110924457924234\n",
      "Average test loss: 0.009855311604837576\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01246858128077454\n",
      "Average test loss: 0.0019555650781840086\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013179549246198602\n",
      "Average test loss: 0.0017859423623627258\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01041683301412397\n",
      "Average test loss: 0.0029901265665474865\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009556355226784944\n",
      "Average test loss: 0.001928215674435099\n",
      "Epoch 147/300\n",
      "Average training loss: 0.009238952789455652\n",
      "Average test loss: 0.0018517136948390139\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00912036944511864\n",
      "Average test loss: 0.0018537509662823544\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009238800319532553\n",
      "Average test loss: 0.0018573804748141104\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00914461888290114\n",
      "Average test loss: 0.0018217723928391934\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009348672754234738\n",
      "Average test loss: 0.0019462940564586057\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009262635032335917\n",
      "Average test loss: 0.0018455764152523545\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008947866179049016\n",
      "Average test loss: 0.0018592941494037709\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008937360851715008\n",
      "Average test loss: 0.001834692211718195\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009394109732988808\n",
      "Average test loss: 0.0018533440708286232\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009015155110922124\n",
      "Average test loss: 0.0023076339862826796\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008978945462240113\n",
      "Average test loss: 0.007075490686628554\n",
      "Epoch 158/300\n",
      "Average training loss: 0.009925871012939347\n",
      "Average test loss: 0.0019454059811929861\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008844037431809637\n",
      "Average test loss: 0.001899291405764719\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008806596684787008\n",
      "Average test loss: 0.0018251740154292847\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009077023415101899\n",
      "Average test loss: 0.0019043990718200802\n",
      "Epoch 162/300\n",
      "Average training loss: 0.00873583668553167\n",
      "Average test loss: 0.0023247761831929285\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008879689792792002\n",
      "Average test loss: 0.0020865040899564822\n",
      "Epoch 164/300\n",
      "Average training loss: 0.010497618723246786\n",
      "Average test loss: 0.002575643751770258\n",
      "Epoch 165/300\n",
      "Average training loss: 0.012505476826181014\n",
      "Average test loss: 0.026587886341744\n",
      "Epoch 166/300\n",
      "Average training loss: 0.00961057941450013\n",
      "Average test loss: 0.0018697587553825642\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00899224916100502\n",
      "Average test loss: 0.001864258263881008\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008794628521634472\n",
      "Average test loss: 0.0018583345694674386\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00871961351028747\n",
      "Average test loss: 0.0018659398753402962\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00894478049253424\n",
      "Average test loss: 0.0019035640339263612\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008644711235331164\n",
      "Average test loss: 0.0018847803317217364\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008654005349924167\n",
      "Average test loss: 0.0022805020441818567\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00876493269784583\n",
      "Average test loss: 0.0021599370785471464\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008851681807388862\n",
      "Average test loss: 0.0019428951907902955\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008615400422778394\n",
      "Average test loss: 0.0019159781514770456\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008623730429344707\n",
      "Average test loss: 0.0019154945528134704\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009786694419052866\n",
      "Average test loss: 0.0018281216354419788\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00859263761796885\n",
      "Average test loss: 0.0021023600256691376\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009548163579569922\n",
      "Average test loss: 0.0018534109807676739\n",
      "Epoch 180/300\n",
      "Average training loss: 0.00860160154932075\n",
      "Average test loss: 0.0019151667158843742\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008446316574182775\n",
      "Average test loss: 0.0018575320536684658\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00843868472137385\n",
      "Average test loss: 0.001940930325227479\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015258231993350718\n",
      "Average test loss: 0.0018961083307448361\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010150447810275687\n",
      "Average test loss: 0.002340224885278278\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009103563108791908\n",
      "Average test loss: 0.0018546056902656954\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008684673734837108\n",
      "Average test loss: 0.0036463782870107225\n",
      "Epoch 187/300\n",
      "Average training loss: 0.00853054733077685\n",
      "Average test loss: 0.002925898598610527\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008470758115251859\n",
      "Average test loss: 0.0019090055420787797\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00837082861115535\n",
      "Average test loss: 0.002073143366103371\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008362767106956905\n",
      "Average test loss: 0.0019442296229923764\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008384942269159689\n",
      "Average test loss: 0.0019023910603589481\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008379713464528322\n",
      "Average test loss: 0.0020541429518618516\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00847744570279287\n",
      "Average test loss: 0.0020420370472388136\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008721466297490729\n",
      "Average test loss: 0.0018980576522234414\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008475020814273093\n",
      "Average test loss: 0.0019310740624657935\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008353027002678978\n",
      "Average test loss: 0.0019036221612865726\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008759652076496018\n",
      "Average test loss: 0.0027750939279794695\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008377456159641345\n",
      "Average test loss: 37248.569178819445\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008745294215364588\n",
      "Average test loss: 0.0019018547042376466\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00830816587805748\n",
      "Average test loss: 0.001945549867219395\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00821857574996021\n",
      "Average test loss: 0.0018681090894258684\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00826578061365419\n",
      "Average test loss: 0.0019198270212444995\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008735999325497283\n",
      "Average test loss: 0.003370030380371544\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008253559187882476\n",
      "Average test loss: 0.0019024954112246632\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008184193132238256\n",
      "Average test loss: 0.00241449338156316\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008291049041060938\n",
      "Average test loss: 0.001984778578289681\n",
      "Epoch 207/300\n",
      "Average training loss: 0.008222916356391377\n",
      "Average test loss: 0.0019630311427430974\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009060501316355335\n",
      "Average test loss: 0.0018742311240898238\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008334100897941324\n",
      "Average test loss: 0.0024625756982713937\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008162389295382632\n",
      "Average test loss: 0.0019368489428112903\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008138480477035045\n",
      "Average test loss: 0.001985118857584894\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008157930312057336\n",
      "Average test loss: 0.0019537983822325864\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008435038664688666\n",
      "Average test loss: 0.003264304261861576\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008121993595113357\n",
      "Average test loss: 0.0019173034398506086\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0081156471123298\n",
      "Average test loss: 0.0019292402060495483\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008111995600577859\n",
      "Average test loss: 0.002042417896911502\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008097104179776377\n",
      "Average test loss: 0.00301781064313319\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008265318594872952\n",
      "Average test loss: 0.0021972438690976964\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008094289439005984\n",
      "Average test loss: 0.0020268359835156134\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008052207575903999\n",
      "Average test loss: 0.0029686521738767624\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008505362559109926\n",
      "Average test loss: 0.0018938003480434417\n",
      "Epoch 222/300\n",
      "Average training loss: 0.008003988484955496\n",
      "Average test loss: 0.0019839987826223175\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010044446703460481\n",
      "Average test loss: 0.0017829600827147564\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010015708159241412\n",
      "Average test loss: 0.0018677759713803727\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008473622425562806\n",
      "Average test loss: 0.001948918916285038\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00802582584362891\n",
      "Average test loss: 0.0019216568809416559\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009432789835664961\n",
      "Average test loss: 0.004456851574074891\n",
      "Epoch 228/300\n",
      "Average training loss: 0.007941997297936015\n",
      "Average test loss: 0.0019121978231188325\n",
      "Epoch 229/300\n",
      "Average training loss: 0.007894631756262647\n",
      "Average test loss: 0.001898927320829696\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007884401272568438\n",
      "Average test loss: 0.001919291625213292\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007898965158396297\n",
      "Average test loss: 0.001998354791353146\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008178698433356153\n",
      "Average test loss: 0.0019417145371230112\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007936070133414533\n",
      "Average test loss: 0.0019355145098848476\n",
      "Epoch 234/300\n",
      "Average training loss: 0.00809018417323629\n",
      "Average test loss: 0.0020892279808306033\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007916566079689397\n",
      "Average test loss: 0.0019228811694516077\n",
      "Epoch 236/300\n",
      "Average training loss: 0.007947172521303097\n",
      "Average test loss: 0.002074029896925721\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00811698173979918\n",
      "Average test loss: 0.001970446562084059\n",
      "Epoch 238/300\n",
      "Average training loss: 0.007913917791926198\n",
      "Average test loss: 0.0020342318067948025\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007918642573886447\n",
      "Average test loss: 0.0018888270447237625\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0080612817928195\n",
      "Average test loss: 0.002089144977947904\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008413332615047692\n",
      "Average test loss: 0.0019517686197327243\n",
      "Epoch 242/300\n",
      "Average training loss: 0.007787125127183067\n",
      "Average test loss: 0.002015107946263419\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0077550093800657326\n",
      "Average test loss: 0.0024407263613409466\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007834849479711718\n",
      "Average test loss: 0.0018874398874532846\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0134448606595397\n",
      "Average test loss: 0.0019566491198622516\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010483987745311524\n",
      "Average test loss: 0.0018261003454940186\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009198020042230685\n",
      "Average test loss: 0.0018384116860106587\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008493972042368517\n",
      "Average test loss: 0.0019246431053098706\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008067555560006035\n",
      "Average test loss: 0.0019709032369363637\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008052417054772377\n",
      "Average test loss: 0.002266095961133639\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007843241687864065\n",
      "Average test loss: 0.002082963383135696\n",
      "Epoch 252/300\n",
      "Average training loss: 0.007720744676887989\n",
      "Average test loss: 0.0019494139737346106\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0078008872055345116\n",
      "Average test loss: 0.001990977164564861\n",
      "Epoch 254/300\n",
      "Average training loss: 0.007760026570409536\n",
      "Average test loss: 0.0019405691530555488\n",
      "Epoch 255/300\n",
      "Average training loss: 0.007769732837875684\n",
      "Average test loss: 0.0023944809109800393\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008222788437373108\n",
      "Average test loss: 0.001939726746123698\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007719418798883756\n",
      "Average test loss: 0.0021133052076523504\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007709284982747502\n",
      "Average test loss: 0.001966849178282751\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00822674253417386\n",
      "Average test loss: 0.0020470954407420423\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007709991775039169\n",
      "Average test loss: 0.0019998756216631996\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008651186425652768\n",
      "Average test loss: 0.001971526826214459\n",
      "Epoch 262/300\n",
      "Average training loss: 0.007836001652603349\n",
      "Average test loss: 0.001962187831186586\n",
      "Epoch 263/300\n",
      "Average training loss: 0.007643492829054594\n",
      "Average test loss: 0.0021467422327647607\n",
      "Epoch 264/300\n",
      "Average training loss: 0.009966950400835938\n",
      "Average test loss: 0.0019245462926725547\n",
      "Epoch 265/300\n",
      "Average training loss: 0.007830841979632776\n",
      "Average test loss: 0.0020013545660509005\n",
      "Epoch 266/300\n",
      "Average training loss: 0.007659773567070564\n",
      "Average test loss: 0.0019507841550641589\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008156858245945638\n",
      "Average test loss: 0.0019745804079704813\n",
      "Epoch 268/300\n",
      "Average training loss: 0.007793621782627371\n",
      "Average test loss: 0.0019847652471313873\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007613556186772055\n",
      "Average test loss: 0.021193463461266625\n",
      "Epoch 270/300\n",
      "Average training loss: 0.007763854670027891\n",
      "Average test loss: 0.002563281186338928\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007763627487338252\n",
      "Average test loss: 0.004562239608416955\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007651731511371003\n",
      "Average test loss: 0.001981590071072181\n",
      "Epoch 273/300\n",
      "Average training loss: 0.007791648062566916\n",
      "Average test loss: 0.001995373157163461\n",
      "Epoch 274/300\n",
      "Average training loss: 0.007652390135245191\n",
      "Average test loss: 0.0019679162099750505\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008890021262069543\n",
      "Average test loss: 0.002118882308403651\n",
      "Epoch 276/300\n",
      "Average training loss: 0.007702001538955503\n",
      "Average test loss: 0.0019456156642279692\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007758475807391935\n",
      "Average test loss: 0.0019719931916851135\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0075836244771877925\n",
      "Average test loss: 0.002007537287970384\n",
      "Epoch 279/300\n",
      "Average training loss: 0.007607717492100265\n",
      "Average test loss: 0.002050821773707867\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008124077830049727\n",
      "Average test loss: 0.0019281067099008295\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0075867973383929995\n",
      "Average test loss: 0.0019804267088572186\n",
      "Epoch 282/300\n",
      "Average training loss: 0.007551959449218379\n",
      "Average test loss: 0.0019804015179268187\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00766393248985211\n",
      "Average test loss: 0.002530016815289855\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008637010441472134\n",
      "Average test loss: 0.0021953338474656144\n",
      "Epoch 285/300\n",
      "Average training loss: 0.007793571047898796\n",
      "Average test loss: 0.001995478542935517\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008078082225388951\n",
      "Average test loss: 0.0019737969268527294\n",
      "Epoch 287/300\n",
      "Average training loss: 0.007548076910691129\n",
      "Average test loss: 0.002086077486475309\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007502600023316012\n",
      "Average test loss: 0.001970358652373155\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007537485429810153\n",
      "Average test loss: 0.003154581226615442\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007910818768044312\n",
      "Average test loss: 0.001943211594493025\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007543412540935808\n",
      "Average test loss: 0.0021212709370172687\n",
      "Epoch 292/300\n",
      "Average training loss: 0.007855143083466423\n",
      "Average test loss: 0.0021266911230567427\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0075265482887625695\n",
      "Average test loss: 0.00558694046528803\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007508978098216984\n",
      "Average test loss: 0.00220993111624072\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0076798040059705575\n",
      "Average test loss: 0.002067664996099969\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007629692811518907\n",
      "Average test loss: 0.002354410102073517\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007456358412901561\n",
      "Average test loss: 0.0030983305325110754\n",
      "Epoch 298/300\n",
      "Average training loss: 0.007545347761776712\n",
      "Average test loss: 0.0020664775152173308\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010249676873286566\n",
      "Average test loss: 375.1806580337948\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014520322396523422\n",
      "Average test loss: 0.0018207640450240837\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 349.4195618042416\n",
      "Average test loss: 1199.8341341090468\n",
      "Epoch 2/300\n",
      "Average training loss: 2.0523562739690147\n",
      "Average test loss: 8.258758505864275\n",
      "Epoch 3/300\n",
      "Average training loss: 1.4953619155883788\n",
      "Average test loss: 0.05772806596424845\n",
      "Epoch 4/300\n",
      "Average training loss: 1.160616501490275\n",
      "Average test loss: 0.010943502246919605\n",
      "Epoch 5/300\n",
      "Average training loss: 1.1459685407214695\n",
      "Average test loss: 4.440605629569954\n",
      "Epoch 6/300\n",
      "Average training loss: 1.4090866843329535\n",
      "Average test loss: 7.435397431881063\n",
      "Epoch 7/300\n",
      "Average training loss: 1.2331702868143717\n",
      "Average test loss: 0.009814305982655949\n",
      "Epoch 8/300\n",
      "Average training loss: 0.9965295722749499\n",
      "Average test loss: 0.02341433094152146\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7581410233179728\n",
      "Average test loss: 0.31536740362975335\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7536174490186903\n",
      "Average test loss: 0.007073251533011596\n",
      "Epoch 11/300\n",
      "Average training loss: 0.689947812239329\n",
      "Average test loss: 207.57465732359225\n",
      "Epoch 12/300\n",
      "Average training loss: 0.4944150595135159\n",
      "Average test loss: 0.643686162247426\n",
      "Epoch 13/300\n",
      "Average training loss: 0.465803484360377\n",
      "Average test loss: 33.48299182300104\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4114068914519416\n",
      "Average test loss: 2.028673024735517\n",
      "Epoch 15/300\n",
      "Average training loss: 0.34146241426467894\n",
      "Average test loss: 0.004201462270071109\n",
      "Epoch 16/300\n",
      "Average training loss: 0.29589551536242165\n",
      "Average test loss: 0.25189616609447535\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2657602306339476\n",
      "Average test loss: 0.8598057792650329\n",
      "Epoch 18/300\n",
      "Average training loss: 0.22663899185922412\n",
      "Average test loss: 0.2915796511049072\n",
      "Epoch 19/300\n",
      "Average training loss: 0.19327535863717396\n",
      "Average test loss: 0.14098657732705275\n",
      "Epoch 20/300\n",
      "Average training loss: 0.16541935316721598\n",
      "Average test loss: 0.6780625587436888\n",
      "Epoch 21/300\n",
      "Average training loss: 0.14523670535617406\n",
      "Average test loss: 0.032406446690360705\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12409672549035813\n",
      "Average test loss: 0.4343080419761439\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10402832061714597\n",
      "Average test loss: 0.03176591402768261\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08719627534680896\n",
      "Average test loss: 6.254216380678945\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07284119024541642\n",
      "Average test loss: 0.06983244068506692\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06236950686905119\n",
      "Average test loss: 0.8079419904640979\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05459673255681992\n",
      "Average test loss: 0.3355989396782178\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04815328724185626\n",
      "Average test loss: 0.10252072808912231\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04331256404850218\n",
      "Average test loss: 0.9770919432847036\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03912919382916556\n",
      "Average test loss: 0.07747134015791945\n",
      "Epoch 31/300\n",
      "Average training loss: 0.035743959416945775\n",
      "Average test loss: 6.5735089211530155\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03320016463763184\n",
      "Average test loss: 0.13517609190444152\n",
      "Epoch 33/300\n",
      "Average training loss: 0.029987726324134403\n",
      "Average test loss: 0.6498881921773363\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02763688752055168\n",
      "Average test loss: 0.6352009060137594\n",
      "Epoch 35/300\n",
      "Average training loss: 0.025603006263573966\n",
      "Average test loss: 0.13508638999114433\n",
      "Epoch 36/300\n",
      "Average training loss: 0.024011702792512045\n",
      "Average test loss: 0.013778490693411893\n",
      "Epoch 37/300\n",
      "Average training loss: 0.022331137135624887\n",
      "Average test loss: 0.00474731983948085\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02095482312142849\n",
      "Average test loss: 0.00251830594562408\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019896722661124334\n",
      "Average test loss: 0.026323124502268102\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018855108219716286\n",
      "Average test loss: 0.04991141501069069\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01775689063469569\n",
      "Average test loss: 0.00763707461539242\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01732969607081678\n",
      "Average test loss: 0.031314751962820686\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0162775327919258\n",
      "Average test loss: 1.8464560531886915\n",
      "Epoch 44/300\n",
      "Average training loss: 0.016045443972365722\n",
      "Average test loss: 0.0016035736778544055\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015217507529589865\n",
      "Average test loss: 5.200227708922492\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01557200237363577\n",
      "Average test loss: 0.0029049622842835057\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014508699085977343\n",
      "Average test loss: 0.0014583705074878203\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014138144883844588\n",
      "Average test loss: 0.0016499268715787264\n",
      "Epoch 49/300\n",
      "Average training loss: 0.013242432917157808\n",
      "Average test loss: 0.004773374484231075\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013831150622003609\n",
      "Average test loss: 0.005948047361440128\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012899306264188554\n",
      "Average test loss: 0.0017031501084566116\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012383529952416818\n",
      "Average test loss: 0.0014394082604493532\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012780465319752693\n",
      "Average test loss: 0.003653042766989933\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011806675987111197\n",
      "Average test loss: 0.06781321818464332\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012896277400354544\n",
      "Average test loss: 0.0018501410165594684\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011305910408082936\n",
      "Average test loss: 0.0013586375564336778\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01101458997776111\n",
      "Average test loss: 0.0015472850296646356\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01091468490784367\n",
      "Average test loss: 0.0013347027274883455\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011315443702869946\n",
      "Average test loss: 0.0014105506748374966\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010196195945971542\n",
      "Average test loss: 0.0013131309454329312\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00995284199838837\n",
      "Average test loss: 0.001282118770086931\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009863755798588196\n",
      "Average test loss: 0.0014318887601710028\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009940652153558202\n",
      "Average test loss: 0.0012745075123384594\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009571290894514984\n",
      "Average test loss: 0.001243724087253213\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009575319870064656\n",
      "Average test loss: 0.0016346048346410195\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009272074587643146\n",
      "Average test loss: 0.001288429388569461\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009085062677247657\n",
      "Average test loss: 0.0012342433997740347\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008793707936174339\n",
      "Average test loss: 0.0012540936660952865\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008841131616383792\n",
      "Average test loss: 0.0012540813590296441\n",
      "Epoch 70/300\n",
      "Average training loss: 0.008524274375289678\n",
      "Average test loss: 0.0014649123612066938\n",
      "Epoch 71/300\n",
      "Average training loss: 0.00855947230425146\n",
      "Average test loss: 0.0016363551033039888\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008455247008552155\n",
      "Average test loss: 0.001961185144053565\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009081751990649435\n",
      "Average test loss: 0.0012170224700950914\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008151722331841787\n",
      "Average test loss: 0.0012444259324628445\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008027338296175003\n",
      "Average test loss: 0.001214344173980256\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008084498920788368\n",
      "Average test loss: 0.0012964990412712926\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009383143070671294\n",
      "Average test loss: 0.001661378782759938\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008457854566060834\n",
      "Average test loss: 0.0012342289384040567\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007917718638562493\n",
      "Average test loss: 0.0013892790460959077\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007805626320342223\n",
      "Average test loss: 0.001248439978601204\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008180432039002577\n",
      "Average test loss: 0.0021473332403434647\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00857037297056781\n",
      "Average test loss: 0.0012809326621289882\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007697592972881264\n",
      "Average test loss: 0.0012570372977190548\n",
      "Epoch 84/300\n",
      "Average training loss: 0.00767457497616609\n",
      "Average test loss: 0.0013619107178722818\n",
      "Epoch 85/300\n",
      "Average training loss: 0.007579380047818025\n",
      "Average test loss: 0.0011992503532932863\n",
      "Epoch 86/300\n",
      "Average training loss: 0.007502014656447702\n",
      "Average test loss: 0.001195599070801917\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007681649704774221\n",
      "Average test loss: 0.001718139487422175\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007942443714166681\n",
      "Average test loss: 0.0014462705641571018\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007401137729899751\n",
      "Average test loss: 0.001337702490389347\n",
      "Epoch 90/300\n",
      "Average training loss: 0.007737510127739774\n",
      "Average test loss: 0.04713997742036979\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007246997006651428\n",
      "Average test loss: 0.0015909692750105428\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007630543241070376\n",
      "Average test loss: 0.0020341040521549684\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00719428156564633\n",
      "Average test loss: 0.0012868128455399226\n",
      "Epoch 94/300\n",
      "Average training loss: 0.00729918146919873\n",
      "Average test loss: 0.25459137789408365\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007622991174459457\n",
      "Average test loss: 0.0012684553927845425\n",
      "Epoch 96/300\n",
      "Average training loss: 0.007303502745512459\n",
      "Average test loss: 0.002978764473978016\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007142777139941851\n",
      "Average test loss: 0.0011865685045615666\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008259723666227526\n",
      "Average test loss: 0.0030050467238761486\n",
      "Epoch 99/300\n",
      "Average training loss: 0.007038659325904317\n",
      "Average test loss: 0.0012989229190473756\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0069453546893265515\n",
      "Average test loss: 0.0020642949026077988\n",
      "Epoch 101/300\n",
      "Average training loss: 0.006980559527873993\n",
      "Average test loss: 0.001703652773466375\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007091648638662365\n",
      "Average test loss: 0.001226500314142969\n",
      "Epoch 103/300\n",
      "Average training loss: 0.007256794960962401\n",
      "Average test loss: 0.002974823516379628\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006806751458595196\n",
      "Average test loss: 0.0012737654108657605\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006832990256034665\n",
      "Average test loss: 0.0013158309483486746\n",
      "Epoch 106/300\n",
      "Average training loss: 0.007536199603643682\n",
      "Average test loss: 0.0015701386322163873\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0071216482834683525\n",
      "Average test loss: 0.0011919938324847154\n",
      "Epoch 108/300\n",
      "Average training loss: 0.030317109285129443\n",
      "Average test loss: 0.007812900643588768\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013375846211281088\n",
      "Average test loss: 0.002881025050663286\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010583136047754023\n",
      "Average test loss: 0.001776560648240977\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00936030074208975\n",
      "Average test loss: 1.0048252852637736\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008638174826072322\n",
      "Average test loss: 0.0014869176698848606\n",
      "Epoch 113/300\n",
      "Average training loss: 0.00823188508550326\n",
      "Average test loss: 0.0025258127350567114\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008030319215936794\n",
      "Average test loss: 0.0011815198414131171\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008467992823984888\n",
      "Average test loss: 0.0012811008543810911\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0076565181795093746\n",
      "Average test loss: 0.0012148918073831333\n",
      "Epoch 117/300\n",
      "Average training loss: 0.007577574952195088\n",
      "Average test loss: 28.36137254163954\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008148495969259076\n",
      "Average test loss: 0.12827155625157885\n",
      "Epoch 119/300\n",
      "Average training loss: 0.007342890070130428\n",
      "Average test loss: 0.0031214795036034453\n",
      "Epoch 120/300\n",
      "Average training loss: 0.007309733169774214\n",
      "Average test loss: 0.0012150413531603085\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007386710941170653\n",
      "Average test loss: 0.0012178392422695955\n",
      "Epoch 122/300\n",
      "Average training loss: 0.007307689007785586\n",
      "Average test loss: 0.001975605752629538\n",
      "Epoch 123/300\n",
      "Average training loss: 0.007251967659013139\n",
      "Average test loss: 0.0019453484049687783\n",
      "Epoch 124/300\n",
      "Average training loss: 0.007327000645299752\n",
      "Average test loss: 0.14473816805001763\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0129930920559499\n",
      "Average test loss: 0.0012179777074812187\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008235123355355528\n",
      "Average test loss: 0.0027365088013725147\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008518517699920469\n",
      "Average test loss: 0.001711215850069291\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007422048846466673\n",
      "Average test loss: 0.0012408750779512856\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007186990191539129\n",
      "Average test loss: 0.0012954004129601849\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009383007830215825\n",
      "Average test loss: 0.0012632810320291254\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0072582376011543805\n",
      "Average test loss: 0.001303714958847397\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007005492405345043\n",
      "Average test loss: 0.0011748173142679863\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012754272744059563\n",
      "Average test loss: 0.030634531514926087\n",
      "Epoch 134/300\n",
      "Average training loss: 0.010646347142342064\n",
      "Average test loss: 0.0017891234543381466\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008831500742998388\n",
      "Average test loss: 0.0012315036411293678\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007611056995888551\n",
      "Average test loss: 0.00453239292682459\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0073714897765053645\n",
      "Average test loss: 0.0015230712565696902\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011367057050267856\n",
      "Average test loss: 0.004147088297642767\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007723208805339204\n",
      "Average test loss: 0.0015183354066684843\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0077155987330608895\n",
      "Average test loss: 0.008497551855527693\n",
      "Epoch 141/300\n",
      "Average training loss: 0.007268751801715957\n",
      "Average test loss: 0.0011847989871684049\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007356702890661028\n",
      "Average test loss: 0.003348564391748773\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0070289062166379555\n",
      "Average test loss: 0.0426861602274908\n",
      "Epoch 144/300\n",
      "Average training loss: 0.006912842906597588\n",
      "Average test loss: 0.006300352478503353\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011023385836846299\n",
      "Average test loss: 0.0023917932371712394\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009901246584951878\n",
      "Average test loss: 0.0012021719149520828\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008164984566883908\n",
      "Average test loss: 0.0011700757674665914\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007604762286775642\n",
      "Average test loss: 0.0038695800376849043\n",
      "Epoch 149/300\n",
      "Average training loss: 0.007561516458789507\n",
      "Average test loss: 0.0017948809357153045\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007419190001984437\n",
      "Average test loss: 0.0012287064973885815\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007215291587428914\n",
      "Average test loss: 0.9433563258647919\n",
      "Epoch 152/300\n",
      "Average training loss: 0.018958619223286708\n",
      "Average test loss: 0.0058837691868344946\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009914965721882052\n",
      "Average test loss: 0.05702788034743733\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008556392605106036\n",
      "Average test loss: 2.1620682357155925\n",
      "Epoch 155/300\n",
      "Average training loss: 0.00804177063289616\n",
      "Average test loss: 0.0012434446094557643\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00806036506800188\n",
      "Average test loss: 0.01860243987209267\n",
      "Epoch 157/300\n",
      "Average training loss: 0.007476179567476113\n",
      "Average test loss: 0.014262000523921516\n",
      "Epoch 158/300\n",
      "Average training loss: 0.009236610762154063\n",
      "Average test loss: 0.003845741789167126\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00864869862753484\n",
      "Average test loss: 576.2099558462658\n",
      "Epoch 160/300\n",
      "Average training loss: 0.007736505281594064\n",
      "Average test loss: 3.191660781364474\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009845393738398949\n",
      "Average test loss: 0.02428949563515683\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009056151565164327\n",
      "Average test loss: 0.03428995838678545\n",
      "Epoch 163/300\n",
      "Average training loss: 0.007612604839934243\n",
      "Average test loss: 1.5212243925770745\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0073441149873865975\n",
      "Average test loss: 0.007733316434340345\n",
      "Epoch 165/300\n",
      "Average training loss: 0.007503655636890067\n",
      "Average test loss: 5.565387590675718\n",
      "Epoch 166/300\n",
      "Average training loss: 0.00729925804792179\n",
      "Average test loss: 14.407926550012496\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0070608069019185175\n",
      "Average test loss: 0.0031943046688619587\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008020989599741167\n",
      "Average test loss: 0.001182856680204471\n",
      "Epoch 169/300\n",
      "Average training loss: 0.006949689290175835\n",
      "Average test loss: 2537.850798320558\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007283737091140614\n",
      "Average test loss: 0.0017317663667102655\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01923220904916525\n",
      "Average test loss: 52.161216514666876\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015513322685327794\n",
      "Average test loss: 0.8698720811865189\n",
      "Epoch 173/300\n",
      "Average training loss: 0.010524496705995666\n",
      "Average test loss: 1468.0252564395128\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009420162574284607\n",
      "Average test loss: 1.2076769938030176\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008355567386580838\n",
      "Average test loss: 0.001981406500459545\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007855961494561699\n",
      "Average test loss: 0.014887925904140705\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011103798588116963\n",
      "Average test loss: 0.001928207533640994\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007688755960928069\n",
      "Average test loss: 0.00528681466045479\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007409642554819584\n",
      "Average test loss: 0.01618725769014822\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007267774931672547\n",
      "Average test loss: 0.012835682343277666\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007090785873019033\n",
      "Average test loss: 0.0026611087661650447\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007242121404657761\n",
      "Average test loss: 0.00901397766969684\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007999250904553466\n",
      "Average test loss: 0.017546104301595027\n",
      "Epoch 184/300\n",
      "Average training loss: 0.006870652572976218\n",
      "Average test loss: 0.00934108199737966\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006814838137477636\n",
      "Average test loss: 0.0014297815228088034\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00671394932601187\n",
      "Average test loss: 0.005251643055842982\n",
      "Epoch 187/300\n",
      "Average training loss: 0.006733755804598332\n",
      "Average test loss: 2.1470338885304\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008915427715000179\n",
      "Average test loss: 0.03635120121141275\n",
      "Epoch 189/300\n",
      "Average training loss: 0.006935577164921496\n",
      "Average test loss: 0.02095272760010428\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007187895896534125\n",
      "Average test loss: 10.558735047445737\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017540105441378222\n",
      "Average test loss: 2415.1717931885364\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013973595674667093\n",
      "Average test loss: 23449091.718390916\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009721174117591646\n",
      "Average test loss: 49740.24047889757\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008880631026708417\n",
      "Average test loss: 0.8172674373151321\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008378198344674376\n",
      "Average test loss: 0.975470039838149\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01137196524772379\n",
      "Average test loss: 18.9365391799584\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00991682032826874\n",
      "Average test loss: 0.00140537318525215\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007814686757408911\n",
      "Average test loss: 1.8270461523541663\n",
      "Epoch 199/300\n",
      "Average training loss: 0.007793336008157995\n",
      "Average test loss: 5.90238129684536\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0072608956466946335\n",
      "Average test loss: 3.4001376998511454\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007061005556748973\n",
      "Average test loss: 86664.46929802308\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008311224898530378\n",
      "Average test loss: 2949.5351792789997\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007698667648765776\n",
      "Average test loss: 0.0012352205319330096\n",
      "Epoch 204/300\n",
      "Average training loss: 0.00706452450197604\n",
      "Average test loss: 0.07675188931077719\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007266499968038665\n",
      "Average test loss: 0.0013872088050573236\n",
      "Epoch 206/300\n",
      "Average training loss: 0.006857340161585145\n",
      "Average test loss: 541.2698921660677\n",
      "Epoch 207/300\n",
      "Average training loss: 0.007638714826562338\n",
      "Average test loss: 163.67289505257747\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0069403181001543995\n",
      "Average test loss: 3.492791887432337\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019409151489535967\n",
      "Average test loss: 41.86684894569011\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013131002808817559\n",
      "Average test loss: 0.1969293736666441\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009175485541423162\n",
      "Average test loss: 0.09465261061530975\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008030919538189967\n",
      "Average test loss: 0.0012169507916809784\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007507027511795362\n",
      "Average test loss: 11693651.673137207\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0072080173157155515\n",
      "Average test loss: 0.0013225993760344055\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0069661096798049075\n",
      "Average test loss: 0.8460864032511082\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008843673417137729\n",
      "Average test loss: 139.7055021059753\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0070385111880799135\n",
      "Average test loss: 0.0014298166650243932\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006750972126921018\n",
      "Average test loss: 0.06291019212827087\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008057141084223986\n",
      "Average test loss: 26.912227301364556\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008720830239148604\n",
      "Average test loss: 0.0019471073822221823\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007245681128982041\n",
      "Average test loss: 2.1803407913930197\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006654045590509971\n",
      "Average test loss: 0.08688108989803328\n",
      "Epoch 223/300\n",
      "Average training loss: 0.006518010906461212\n",
      "Average test loss: 2947767.651599846\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006722462406588925\n",
      "Average test loss: 0.002360378369068106\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0065474688770870365\n",
      "Average test loss: 100.33796213932915\n",
      "Epoch 226/300\n",
      "Average training loss: 0.006537430004941093\n",
      "Average test loss: 0.041244151723881566\n",
      "Epoch 227/300\n",
      "Average training loss: 0.006843370649135775\n",
      "Average test loss: 100.4499816331644\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00854192226131757\n",
      "Average test loss: 0.4275378385093063\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006735925430638922\n",
      "Average test loss: 88248118111.63348\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009207764697571596\n",
      "Average test loss: 0.0012068058488010945\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007458559436102708\n",
      "Average test loss: 0.17802690222693815\n",
      "Epoch 232/300\n",
      "Average training loss: 0.009585574691080385\n",
      "Average test loss: 1.8382006834621232\n",
      "Epoch 233/300\n",
      "Average training loss: 0.006985896394898494\n",
      "Average test loss: 0.18878438820172516\n",
      "Epoch 234/300\n",
      "Average training loss: 0.006600775602377124\n",
      "Average test loss: 349.22621416683813\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011657436105940076\n",
      "Average test loss: 932.5012453022485\n",
      "Epoch 236/300\n",
      "Average training loss: 0.006964872427698639\n",
      "Average test loss: 17.375706930271143\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0066201985122429\n",
      "Average test loss: 13918.243106132904\n",
      "Epoch 238/300\n",
      "Average training loss: 0.006554970348046886\n",
      "Average test loss: 492.9064539290436\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0066184685685568385\n",
      "Average test loss: 302.8545308436113\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009642886947426531\n",
      "Average test loss: 1651.7985339252\n",
      "Epoch 241/300\n",
      "Average training loss: 0.006949929410591722\n",
      "Average test loss: 1.2778233236070309\n",
      "Epoch 242/300\n",
      "Average training loss: 0.006481110313286384\n",
      "Average test loss: 101.99660499369229\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006593725039313237\n",
      "Average test loss: 10.840488758036557\n",
      "Epoch 244/300\n",
      "Average training loss: 0.006614748585141368\n",
      "Average test loss: 231.27503299771084\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009831113549156322\n",
      "Average test loss: 7386.087407367429\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008618443147589763\n",
      "Average test loss: 202.12399503345932\n",
      "Epoch 247/300\n",
      "Average training loss: 0.006972923627744119\n",
      "Average test loss: 8628.893524409898\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016260202786988684\n",
      "Average test loss: 5.109034563890348\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0099351606534587\n",
      "Average test loss: 27782.13398395344\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008474595681660705\n",
      "Average test loss: 364.39924243290227\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008703500840812922\n",
      "Average test loss: 3832137.3995775417\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008259724026338922\n",
      "Average test loss: 5214.826972709493\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007697532282935248\n",
      "Average test loss: 3.428040448749024\n",
      "Epoch 254/300\n",
      "Average training loss: 0.007277746279620462\n",
      "Average test loss: 60161.68370676099\n",
      "Epoch 255/300\n",
      "Average training loss: 0.007965995107673936\n",
      "Average test loss: 490227562.3651463\n",
      "Epoch 256/300\n",
      "Average training loss: 0.006703814808693197\n",
      "Average test loss: 243.2688294789458\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008517066768060128\n",
      "Average test loss: 616.6820191305644\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008486437499523164\n",
      "Average test loss: 0.0012270478958057033\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009756502992577023\n",
      "Average test loss: 96.0393411289824\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007605564473403824\n",
      "Average test loss: 34872.32426751484\n",
      "Epoch 261/300\n",
      "Average training loss: 0.007039929091102547\n",
      "Average test loss: 3024.346551867451\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008778753326170974\n",
      "Average test loss: 1474.3693191757864\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0069488931372761725\n",
      "Average test loss: 4458697.587315687\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007036935025205215\n",
      "Average test loss: 24485.70318828846\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010476779218763114\n",
      "Average test loss: 37.89544851991865\n",
      "Epoch 266/300\n",
      "Average training loss: 0.007929935742169618\n",
      "Average test loss: 0.02047401536297467\n",
      "Epoch 267/300\n",
      "Average training loss: 0.006790568909711308\n",
      "Average test loss: 7.365803707821915\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006498115176128016\n",
      "Average test loss: 1.6363322422616184\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006432405984236135\n",
      "Average test loss: 9.042999683775008\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00672646901011467\n",
      "Average test loss: 923.6050172252195\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006736944212681718\n",
      "Average test loss: 0.01761921619861904\n",
      "Epoch 272/300\n",
      "Average training loss: 0.00782603648967213\n",
      "Average test loss: 9.154707693612824\n",
      "Epoch 273/300\n",
      "Average training loss: 0.006346010516915057\n",
      "Average test loss: 7.681734035696058\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014768739096820355\n",
      "Average test loss: 5893.651660703722\n",
      "Epoch 275/300\n",
      "Average training loss: 0.00841315775281853\n",
      "Average test loss: 51.128003083390496\n",
      "Epoch 276/300\n",
      "Average training loss: 0.007060519241210487\n",
      "Average test loss: 21790.513395006747\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013766842189348406\n",
      "Average test loss: 68.89340425980588\n",
      "Epoch 278/300\n",
      "Average training loss: 0.007248670618567201\n",
      "Average test loss: 11.599383284166041\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008459231865902741\n",
      "Average test loss: 1273.3990260118394\n",
      "Epoch 280/300\n",
      "Average training loss: 0.006747139297425747\n",
      "Average test loss: 3983.7039904796284\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008035807605832815\n",
      "Average test loss: 8.22985839957506\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008409363102995688\n",
      "Average test loss: 69761552733.75288\n",
      "Epoch 283/300\n",
      "Average training loss: 0.022250211624635592\n",
      "Average test loss: 1186.3979490490526\n",
      "Epoch 284/300\n",
      "Average training loss: 0.009139121082921822\n",
      "Average test loss: 0.0011829540502900878\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008048883425278796\n",
      "Average test loss: 26489035.333054617\n",
      "Epoch 286/300\n",
      "Average training loss: 0.007276430941290326\n",
      "Average test loss: 23502709.098811332\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0069639467497666675\n",
      "Average test loss: 62573.410869372696\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007288166109886435\n",
      "Average test loss: 1983847917.657697\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0075239707115623685\n",
      "Average test loss: 27.779253580368124\n",
      "Epoch 290/300\n",
      "Average training loss: 0.009900320002602206\n",
      "Average test loss: 0.052800843655131754\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013721905964116255\n",
      "Average test loss: 168.757060530641\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010126365948054526\n",
      "Average test loss: 17.292750523195497\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008511680754108561\n",
      "Average test loss: 600.7194974253731\n",
      "Epoch 294/300\n",
      "Average training loss: 0.009824863793121443\n",
      "Average test loss: 124.9416724631538\n",
      "Epoch 295/300\n",
      "Average training loss: 0.00765655527346664\n",
      "Average test loss: 1550991.1544332064\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008678159024980333\n",
      "Average test loss: 1220851576.280752\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0074977400737504165\n",
      "Average test loss: 14183447354.049822\n",
      "Epoch 298/300\n",
      "Average training loss: 0.006897116966131661\n",
      "Average test loss: 30780612.70556585\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021392375951839817\n",
      "Average test loss: 0.6222510183928535\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011154386090735595\n",
      "Average test loss: 19.367778610527257\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Skip/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 12.18\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 17.04\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 14.67\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 21.92\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.95\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 9.85\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 14.98\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 19.68\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 22.62\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 13.62\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 18.47\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 21.89\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 24.21\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 15.17\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 18.93\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 21.68\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 24.14\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.65\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 14.04\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 20.40\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 23.82\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 25.46\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 16.63\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 22.19\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 23.88\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 25.68\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 4.81\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 18.67\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.38\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.52\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.03\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 15.99\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 21.48\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.61\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 16.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 22.15\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.24\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 19.32\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 23.23\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.90\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 21.68\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 24.74\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 22.94\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 25.75\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 14.27\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.40\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.91\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 20.05\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.27\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 18.94\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 21.99\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.66\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.57\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.02\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 21.91\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.03\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.72\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 23.77\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.21\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.25\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 24.25\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.20\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.72\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 4.96\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 15.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.57\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 21.87\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.85\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.69\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.39\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 23.30\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.84\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.28\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 25.13\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.94\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.85\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.14\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 25.90\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.12\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 34.35\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.01\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.23\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.72\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
