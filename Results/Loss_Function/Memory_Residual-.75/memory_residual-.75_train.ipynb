{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.75)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10070351010560989\n",
      "Average test loss: 0.011001851793792513\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03815794432659944\n",
      "Average test loss: 0.009452824941939778\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0339004208346208\n",
      "Average test loss: 0.009846663274698787\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03166059834427304\n",
      "Average test loss: 0.00846192963876658\n",
      "Epoch 5/300\n",
      "Average training loss: 0.030204956549737188\n",
      "Average test loss: 0.008172177080478934\n",
      "Epoch 6/300\n",
      "Average training loss: 0.028629399710231356\n",
      "Average test loss: 0.008681627176288101\n",
      "Epoch 7/300\n",
      "Average training loss: 0.027697207305166456\n",
      "Average test loss: 0.008552178001238241\n",
      "Epoch 8/300\n",
      "Average training loss: 0.026892237961292267\n",
      "Average test loss: 0.00804628196813994\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02638702644407749\n",
      "Average test loss: 0.007800589585469829\n",
      "Epoch 10/300\n",
      "Average training loss: 0.025820666874448457\n",
      "Average test loss: 0.007667972678939501\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02539249070154296\n",
      "Average test loss: 0.007300989966839552\n",
      "Epoch 12/300\n",
      "Average training loss: 0.024887545673383606\n",
      "Average test loss: 0.007118425527380573\n",
      "Epoch 13/300\n",
      "Average training loss: 0.024355998527672555\n",
      "Average test loss: 0.007559105715403954\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02404086213144991\n",
      "Average test loss: 0.006937765954683224\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02371921045747068\n",
      "Average test loss: 0.007468859305812253\n",
      "Epoch 16/300\n",
      "Average training loss: 0.023427930156389873\n",
      "Average test loss: 0.00711597247628702\n",
      "Epoch 17/300\n",
      "Average training loss: 0.023202803625000847\n",
      "Average test loss: 0.006844812627467845\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02281898219883442\n",
      "Average test loss: 0.006653512148393525\n",
      "Epoch 19/300\n",
      "Average training loss: 0.022553543370631007\n",
      "Average test loss: 0.006772827879422241\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022359072243173916\n",
      "Average test loss: 0.006660717758039634\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02210221967763371\n",
      "Average test loss: 0.006608181956741545\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021941502021418677\n",
      "Average test loss: 0.006538002897467878\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021776634830567573\n",
      "Average test loss: 0.006885163866811328\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021581325580676398\n",
      "Average test loss: 0.0063656580423315364\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02140798884464635\n",
      "Average test loss: 0.006643130898061726\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02131663912369145\n",
      "Average test loss: 0.00647596297868424\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021113116507728895\n",
      "Average test loss: 0.006478133464852969\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020921941566798424\n",
      "Average test loss: 0.006412606859786643\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02085269296500418\n",
      "Average test loss: 0.006301825888868835\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020838623396224446\n",
      "Average test loss: 0.009473195739090442\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02059049008952247\n",
      "Average test loss: 0.006399114992469549\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020475230026576256\n",
      "Average test loss: 0.006161186266276572\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020404758556021585\n",
      "Average test loss: 0.006135761532518599\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020311200282639927\n",
      "Average test loss: 0.006214621733874082\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020172900231348143\n",
      "Average test loss: 0.006273021996021271\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020107545162240663\n",
      "Average test loss: 0.006941580064180825\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020035328004095288\n",
      "Average test loss: 0.006062287834369474\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019962458693318898\n",
      "Average test loss: 0.006046550532595979\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019819213307566114\n",
      "Average test loss: 0.00611224629274673\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019757658989893064\n",
      "Average test loss: 0.006216438567472829\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019777482920222814\n",
      "Average test loss: 0.00598298064329558\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01962774973611037\n",
      "Average test loss: 0.0061518289856612686\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019617461941308447\n",
      "Average test loss: 0.0060858946558501985\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019461530789732933\n",
      "Average test loss: 0.0073966250601742\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01941657521161768\n",
      "Average test loss: 0.006050688698887825\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01937206993334823\n",
      "Average test loss: 0.005995811179694202\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019383850481775072\n",
      "Average test loss: 0.006041697869698206\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01930991477933195\n",
      "Average test loss: 0.006419147555612855\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019154277188082536\n",
      "Average test loss: 0.00603009023434586\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01917236014207204\n",
      "Average test loss: 0.006086723255200518\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01912022092110581\n",
      "Average test loss: 0.006477173487759298\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019043857357568212\n",
      "Average test loss: 0.006042740972091754\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019017666227287715\n",
      "Average test loss: 0.006057142859531774\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018929020875030095\n",
      "Average test loss: 0.005977145725654231\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018908621658881505\n",
      "Average test loss: 0.005936881875826252\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018910651514927546\n",
      "Average test loss: 0.006401343930512666\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01880563876363966\n",
      "Average test loss: 0.005939068054159482\n",
      "Epoch 58/300\n",
      "Average training loss: 0.018773898222380216\n",
      "Average test loss: 0.006039830293506384\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018710579002069103\n",
      "Average test loss: 0.006020379616154565\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018705409770210586\n",
      "Average test loss: 0.005902563257763783\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018644553975926504\n",
      "Average test loss: 0.006127878999130593\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018631005990836357\n",
      "Average test loss: 0.006626100250830253\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018592818584707047\n",
      "Average test loss: 0.006018359243041939\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01856160073644585\n",
      "Average test loss: 0.006166714962985781\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018598312618003952\n",
      "Average test loss: 17.56051882425944\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1413437118563387\n",
      "Average test loss: 0.009550758842792776\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03132878842453162\n",
      "Average test loss: 0.007261895833743943\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02642225284708871\n",
      "Average test loss: 0.006767193013181289\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024985640145010417\n",
      "Average test loss: 0.00643915418535471\n",
      "Epoch 70/300\n",
      "Average training loss: 0.023881291482183667\n",
      "Average test loss: 0.0064662450949350995\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02312082915339205\n",
      "Average test loss: 0.006310357474204567\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022408609266082447\n",
      "Average test loss: 0.006513421001533667\n",
      "Epoch 73/300\n",
      "Average training loss: 0.021884080236156782\n",
      "Average test loss: 0.006546810530126095\n",
      "Epoch 74/300\n",
      "Average training loss: 0.021345774099230767\n",
      "Average test loss: 0.006065750885133942\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020814835616283948\n",
      "Average test loss: 0.006227131656474537\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02042294296456708\n",
      "Average test loss: 0.00588837532657716\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020042463853955268\n",
      "Average test loss: 0.006077318992879656\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019706476436720955\n",
      "Average test loss: 0.005925426047295332\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019427490941352314\n",
      "Average test loss: 0.0063377492150498765\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019214453636772103\n",
      "Average test loss: 0.005997299869855245\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01904267907473776\n",
      "Average test loss: 0.005882839612662792\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01891328047795428\n",
      "Average test loss: 0.006367601433561908\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01874152044951916\n",
      "Average test loss: 0.006077880294786559\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018662193167540763\n",
      "Average test loss: 0.005874086992608176\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01858530536128415\n",
      "Average test loss: 0.005858346896866957\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01848660493724876\n",
      "Average test loss: 0.00595439598026375\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018460245781474642\n",
      "Average test loss: 0.00591697567080458\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018408022796114285\n",
      "Average test loss: 0.005933855831209156\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018422010439137618\n",
      "Average test loss: 0.005935682871689399\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01838613088097837\n",
      "Average test loss: 0.00586812437698245\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018323516322506797\n",
      "Average test loss: 0.0062411976920233835\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018281631923384136\n",
      "Average test loss: 0.005899733405560255\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018277680470711655\n",
      "Average test loss: 0.005968692940970262\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018245176603396734\n",
      "Average test loss: 0.00591249277898007\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018221350229448742\n",
      "Average test loss: 0.0060837298387454615\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018166032830874125\n",
      "Average test loss: 0.005899347768061691\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01814491785234875\n",
      "Average test loss: 0.005871230895734496\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018090296296609774\n",
      "Average test loss: 0.005830335697780053\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0181026770422856\n",
      "Average test loss: 0.0060834549135631985\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01809280477464199\n",
      "Average test loss: 0.0059398464237650234\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01802676766282982\n",
      "Average test loss: 0.005994962136364646\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017986353644066387\n",
      "Average test loss: 0.005883087019125621\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017956354246371323\n",
      "Average test loss: 0.00591342747459809\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017943162063757578\n",
      "Average test loss: 0.00592384536150429\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017904310342338352\n",
      "Average test loss: 0.00590302147426539\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01789857727785905\n",
      "Average test loss: 0.00597177716344595\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017858150722251998\n",
      "Average test loss: 0.005857207386444012\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01784276830156644\n",
      "Average test loss: 0.007508974536839459\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01781700950364272\n",
      "Average test loss: 0.0061718361845446955\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017800502406226264\n",
      "Average test loss: 0.005897028309603532\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017771314048104815\n",
      "Average test loss: 0.005872922964808014\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017725102820330196\n",
      "Average test loss: 0.0061344526285926505\n",
      "Epoch 113/300\n",
      "Average training loss: 0.017726549458172588\n",
      "Average test loss: 0.006439774490065045\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017688697069055504\n",
      "Average test loss: 0.006192890959481398\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017705308861202664\n",
      "Average test loss: 0.005895079166111019\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01765490908258491\n",
      "Average test loss: 0.005875291852901379\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017721793077058262\n",
      "Average test loss: 0.005936008154725035\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017596101586189534\n",
      "Average test loss: 0.005953841197407908\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0175650190114975\n",
      "Average test loss: 0.006069220780498452\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017573340361317\n",
      "Average test loss: 0.005991101595676607\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017547242086794642\n",
      "Average test loss: 0.005883329807056321\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017553884375426505\n",
      "Average test loss: 0.005969892235265838\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017515187414156065\n",
      "Average test loss: 0.005933412778294749\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017486114770174026\n",
      "Average test loss: 0.005974908264146911\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01751773981832796\n",
      "Average test loss: 0.005973044317215681\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017459924230972926\n",
      "Average test loss: 0.005966138683259487\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017437033911546072\n",
      "Average test loss: 0.00608633422685994\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017447535439497893\n",
      "Average test loss: 0.005933927139474286\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01740259141723315\n",
      "Average test loss: 0.005980070473833216\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017400087147951125\n",
      "Average test loss: 0.005899533289174239\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01738638765944375\n",
      "Average test loss: 0.005937321728302373\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017309449798531003\n",
      "Average test loss: 0.005902337631417645\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017336014070444637\n",
      "Average test loss: 0.0060861859656870364\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01733978636893961\n",
      "Average test loss: 0.0068017180375754835\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01730524626043108\n",
      "Average test loss: 0.00601175770494673\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01728965055776967\n",
      "Average test loss: 0.006124297100222773\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017269240823884806\n",
      "Average test loss: 0.005973256058783995\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017256511949830584\n",
      "Average test loss: 0.005903049635804362\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017259083788428043\n",
      "Average test loss: 0.005969847358349297\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017214246647225488\n",
      "Average test loss: 0.006041662886324856\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017258265665835805\n",
      "Average test loss: 0.0060316736209723686\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01725104821390576\n",
      "Average test loss: 0.006017258042676581\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017193675546182525\n",
      "Average test loss: 0.005979682900425461\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017157234062751135\n",
      "Average test loss: 0.0060283009360233945\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017168785783979627\n",
      "Average test loss: 0.0059860785421397954\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01710660220020347\n",
      "Average test loss: 0.005967912877599398\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01714556929634677\n",
      "Average test loss: 0.006019469167209334\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01712472945700089\n",
      "Average test loss: 0.005964492154204183\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01710223959551917\n",
      "Average test loss: 0.005996425156792005\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01709731434782346\n",
      "Average test loss: 0.006003417464594046\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01708513181242678\n",
      "Average test loss: 0.006676885504689482\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017070216672288047\n",
      "Average test loss: 0.008151395350280735\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017053399252394835\n",
      "Average test loss: 0.006045673444867134\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017037202229102454\n",
      "Average test loss: 0.005963387796448337\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01697996741367711\n",
      "Average test loss: 0.005878796340690719\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016987215114136537\n",
      "Average test loss: 0.006022909660513202\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017043815074695483\n",
      "Average test loss: 0.00588383970864945\n",
      "Epoch 158/300\n",
      "Average training loss: 0.016997440212302737\n",
      "Average test loss: 0.005980587755640348\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016942886646423076\n",
      "Average test loss: 0.005997491760800282\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016954577498965794\n",
      "Average test loss: 0.006458713602274656\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016930982285075717\n",
      "Average test loss: 0.005928155965689155\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016904398118456204\n",
      "Average test loss: 0.005987528537296586\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016939250133103795\n",
      "Average test loss: 0.006716815895918343\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01692203212529421\n",
      "Average test loss: 0.006021410813762082\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016892278343439104\n",
      "Average test loss: 0.006030650227020184\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0168449823939138\n",
      "Average test loss: 0.005945722994705041\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016889851739837064\n",
      "Average test loss: 0.005996514950361517\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016871886889967654\n",
      "Average test loss: 0.0059552904661330916\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016818151387075583\n",
      "Average test loss: 0.006132402450260189\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01682765909532706\n",
      "Average test loss: 0.005940189088798231\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016822698468963306\n",
      "Average test loss: 0.00610017566051748\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016850417306853667\n",
      "Average test loss: 0.005970338406041264\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016801866190301048\n",
      "Average test loss: 0.006022165722317166\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016801151510741977\n",
      "Average test loss: 0.006071680828721987\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016800214082002638\n",
      "Average test loss: 0.005949802428897884\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01678588106897142\n",
      "Average test loss: 0.009635014029012786\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016747541843189134\n",
      "Average test loss: 0.006107707808415095\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016749600774712034\n",
      "Average test loss: 0.006735897799953819\n",
      "Epoch 179/300\n",
      "Average training loss: 0.016743228374256028\n",
      "Average test loss: 0.005976387248271041\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016758401351670425\n",
      "Average test loss: 0.006057742836160792\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01672252822584576\n",
      "Average test loss: 0.006038295153528452\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016698297828435897\n",
      "Average test loss: 0.006144897236385279\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016705422312021257\n",
      "Average test loss: 0.006016324598549141\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016721326412426102\n",
      "Average test loss: 0.00687718058956994\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016704678061935638\n",
      "Average test loss: 0.006073570819364654\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016677298011051285\n",
      "Average test loss: 0.006032683589806159\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016652911736733382\n",
      "Average test loss: 0.0061136892595224914\n",
      "Epoch 188/300\n",
      "Average training loss: 0.016669500332739617\n",
      "Average test loss: 0.006086675303263797\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016640929609537124\n",
      "Average test loss: 0.0060324540626671575\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016631328741709393\n",
      "Average test loss: 0.006583387687802315\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016621010987295046\n",
      "Average test loss: 0.005979933843016624\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01664052843550841\n",
      "Average test loss: 0.006114058222207758\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016612464122474194\n",
      "Average test loss: 0.006036813362605042\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016603716825445494\n",
      "Average test loss: 0.006090574631260501\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016589972576333416\n",
      "Average test loss: 0.006028590666751067\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016566130463447834\n",
      "Average test loss: 0.006286310225725174\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016590058758854866\n",
      "Average test loss: 0.006117486591554351\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016570225490464104\n",
      "Average test loss: 0.006175620277722676\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016550678398874072\n",
      "Average test loss: 0.006117310904380348\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016597372606396675\n",
      "Average test loss: 0.006094628990110424\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016555605471134185\n",
      "Average test loss: 0.005985874027427699\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016502820822099844\n",
      "Average test loss: 0.0060637199506163595\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01653987158089876\n",
      "Average test loss: 0.006062190389881531\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016521679994960625\n",
      "Average test loss: 0.006580672178831366\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016534379578298994\n",
      "Average test loss: 0.006022992345194022\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0164710747467147\n",
      "Average test loss: 0.005975544198519654\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016486498662167125\n",
      "Average test loss: 0.00642974308381478\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016517631764213243\n",
      "Average test loss: 0.006090573413504495\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016483401482303938\n",
      "Average test loss: 0.0061274538528588085\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01644379549059603\n",
      "Average test loss: 0.006258656728184885\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01643745712439219\n",
      "Average test loss: 0.006249372241397699\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016453363286124335\n",
      "Average test loss: 0.006086235732667976\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016453217526276907\n",
      "Average test loss: 0.006355137491391764\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01644839695923858\n",
      "Average test loss: 0.0061480509903695845\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016420546936492125\n",
      "Average test loss: 0.0063530425532824465\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016415188962386713\n",
      "Average test loss: 0.006210254040443235\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016412518384555975\n",
      "Average test loss: 0.006084300474574168\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016413190500603782\n",
      "Average test loss: 0.006108790751546621\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016438066009018155\n",
      "Average test loss: 0.00604101575538516\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016396401479012435\n",
      "Average test loss: 0.006245839194291168\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016355025028189023\n",
      "Average test loss: 0.006325760579771466\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016380626515381864\n",
      "Average test loss: 0.006373930223286152\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016381114391816987\n",
      "Average test loss: 0.0061031387162705265\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016385900816983646\n",
      "Average test loss: 0.006218334298580885\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016347042236063217\n",
      "Average test loss: 0.005982806115928624\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016373217928740712\n",
      "Average test loss: 0.006133450716733932\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016361454569631152\n",
      "Average test loss: 0.006171878871404462\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016329436770743794\n",
      "Average test loss: 0.006252782104743852\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016306057156787977\n",
      "Average test loss: 0.006058508105576038\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016314737697442374\n",
      "Average test loss: 0.00605203733055128\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016304695235358345\n",
      "Average test loss: 0.006173727314505312\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01636558584454987\n",
      "Average test loss: 0.0061103324625227185\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016349352767070134\n",
      "Average test loss: 0.00644535264165865\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016296828471952015\n",
      "Average test loss: 0.0061620465446677476\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01627930632730325\n",
      "Average test loss: 0.006117749481979344\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01628847612440586\n",
      "Average test loss: 0.005988041569375329\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01628950466050042\n",
      "Average test loss: 0.006223931257095602\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016300112954444355\n",
      "Average test loss: 0.0065614106117023365\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016289608619279333\n",
      "Average test loss: 0.006628498472687271\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01625903453760677\n",
      "Average test loss: 0.006255432840850618\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01626073575268189\n",
      "Average test loss: 0.006301331502281957\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016230473718709417\n",
      "Average test loss: 0.006125140948841969\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01624937728130155\n",
      "Average test loss: 0.0060903848864965965\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01620491929517852\n",
      "Average test loss: 0.006160884733415312\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01620299254109462\n",
      "Average test loss: 0.006080550332243244\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01620566962245438\n",
      "Average test loss: 0.006015757298303975\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016241344985862573\n",
      "Average test loss: 0.006304487843480375\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01621134950220585\n",
      "Average test loss: 0.00621509094370736\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016236291372113756\n",
      "Average test loss: 0.006086437017967304\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016197529117266338\n",
      "Average test loss: 0.006216456294473674\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016197130314177938\n",
      "Average test loss: 0.006153516543408235\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01620701755914423\n",
      "Average test loss: 0.006093390655600362\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01618180417848958\n",
      "Average test loss: 0.00603732418641448\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016167804092168808\n",
      "Average test loss: 0.0060532931892408265\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01616166018611855\n",
      "Average test loss: 0.0061307476936943005\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0161317155311505\n",
      "Average test loss: 0.00603791302566727\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01618626775509781\n",
      "Average test loss: 0.0063757224794891145\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016158082384202216\n",
      "Average test loss: 0.006316573368178474\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01611833915942245\n",
      "Average test loss: 0.006196916690717141\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016181271211968527\n",
      "Average test loss: 0.006069347444921732\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016145574781629774\n",
      "Average test loss: 0.006008514308681091\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016152968857851294\n",
      "Average test loss: 0.00630636834518777\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016094823572370742\n",
      "Average test loss: 0.006509942575461334\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016147780757811335\n",
      "Average test loss: 0.006249109744197793\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016117753257354102\n",
      "Average test loss: 0.006210663661774662\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016119305676884122\n",
      "Average test loss: 0.0062865703188710745\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01610518204420805\n",
      "Average test loss: 0.00616055336015092\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016134710773825646\n",
      "Average test loss: 0.00648075786481301\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016115966512097252\n",
      "Average test loss: 0.006296604704525735\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016079193358620007\n",
      "Average test loss: 0.006218912963651948\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016075028075112237\n",
      "Average test loss: 0.006056791547685862\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016082696667148007\n",
      "Average test loss: 0.0061661910605099466\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016080086496141222\n",
      "Average test loss: 0.006145365407897367\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016049777827329106\n",
      "Average test loss: 0.0060753479529586105\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016059589334660108\n",
      "Average test loss: 0.007293952157927884\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016117858687208757\n",
      "Average test loss: 0.006122126045326392\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01607032160129812\n",
      "Average test loss: 0.0064458350655105376\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016024171135491795\n",
      "Average test loss: 0.006248887397762802\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01605787230696943\n",
      "Average test loss: 0.006333958892358674\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016038857363992267\n",
      "Average test loss: 0.006190350251893202\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016003468059831195\n",
      "Average test loss: 0.006141463513589567\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01604045911629995\n",
      "Average test loss: 0.006076576701468892\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016030296272701687\n",
      "Average test loss: 0.006118328880104754\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016023724087410502\n",
      "Average test loss: 0.006241088125026889\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01597885736491945\n",
      "Average test loss: 0.006343414606319533\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01601540035340521\n",
      "Average test loss: 0.006062647245824337\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016043378947509658\n",
      "Average test loss: 0.006430934484634134\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016026851106021138\n",
      "Average test loss: 0.006314928176088466\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015994678873154852\n",
      "Average test loss: 0.00618488256384929\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015989404987957742\n",
      "Average test loss: 0.006144335019091765\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015964392721652985\n",
      "Average test loss: 0.006386376718680064\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01603172693318791\n",
      "Average test loss: 0.006977893645564715\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016023911957939466\n",
      "Average test loss: 0.007020262089454466\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015998125006755192\n",
      "Average test loss: 0.006068974702515536\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016083042501575415\n",
      "Average test loss: 0.006141994174983767\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01646528010484245\n",
      "Average test loss: 0.006434455310304959\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01590947157310115\n",
      "Average test loss: 0.006224272626555628\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015879878893494605\n",
      "Average test loss: 0.0062091950861116255\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015927848665250672\n",
      "Average test loss: 0.006377688015914626\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015945418912503454\n",
      "Average test loss: 0.00612051881560021\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.08506321408351263\n",
      "Average test loss: 0.007632716414829095\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02905383031566938\n",
      "Average test loss: 0.010413435317575931\n",
      "Epoch 3/300\n",
      "Average training loss: 0.025845411052306495\n",
      "Average test loss: 0.00637801666268044\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02371320841378636\n",
      "Average test loss: 0.005654976131187545\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022154302906658915\n",
      "Average test loss: 0.005358931569589509\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021024209946393965\n",
      "Average test loss: 0.006281670211503903\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019913373604416848\n",
      "Average test loss: 0.005477041494101286\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018996815068854225\n",
      "Average test loss: 0.005136126086115837\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01848190176818106\n",
      "Average test loss: 0.004820332025902139\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01780434925854206\n",
      "Average test loss: 0.0048116868123826055\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01727911788721879\n",
      "Average test loss: 0.0046312233503494\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0168889568630192\n",
      "Average test loss: 0.00462342558014724\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016459751705328624\n",
      "Average test loss: 0.004301211648310224\n",
      "Epoch 14/300\n",
      "Average training loss: 0.016188355008761087\n",
      "Average test loss: 0.004410507970800002\n",
      "Epoch 15/300\n",
      "Average training loss: 0.015836113719476595\n",
      "Average test loss: 0.004370540285689963\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01563552410238319\n",
      "Average test loss: 0.004215257478981382\n",
      "Epoch 17/300\n",
      "Average training loss: 0.015364790327847004\n",
      "Average test loss: 0.004201108527680238\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015105232887797885\n",
      "Average test loss: 0.003984735112430321\n",
      "Epoch 19/300\n",
      "Average training loss: 0.014906672760016388\n",
      "Average test loss: 0.004402044089924958\n",
      "Epoch 20/300\n",
      "Average training loss: 0.014711662537521786\n",
      "Average test loss: 0.003941317368919651\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014615563717153337\n",
      "Average test loss: 0.003963682965065042\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0144486451074481\n",
      "Average test loss: 0.0038390536825690003\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014255526962379615\n",
      "Average test loss: 0.003783201683830056\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014122924570408132\n",
      "Average test loss: 0.0037595637997405395\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0140048164634241\n",
      "Average test loss: 0.003789072651002142\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01387187834497955\n",
      "Average test loss: 0.0037669255110538667\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01377568498502175\n",
      "Average test loss: 0.0037133038317163783\n",
      "Epoch 28/300\n",
      "Average training loss: 0.013733197170827124\n",
      "Average test loss: 0.004422460394187106\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013575765767859089\n",
      "Average test loss: 0.003789350530753533\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013530609651986096\n",
      "Average test loss: 0.0037514496768514317\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013462308830685086\n",
      "Average test loss: 0.003618583452059991\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013362613153954346\n",
      "Average test loss: 0.003592402564568652\n",
      "Epoch 33/300\n",
      "Average training loss: 0.013268801329036553\n",
      "Average test loss: 0.00364718133997586\n",
      "Epoch 34/300\n",
      "Average training loss: 0.013220206199420824\n",
      "Average test loss: 0.003674430882765187\n",
      "Epoch 35/300\n",
      "Average training loss: 0.013167661378781\n",
      "Average test loss: 0.0035732243601232765\n",
      "Epoch 36/300\n",
      "Average training loss: 0.013113265794184472\n",
      "Average test loss: 0.004012602548425396\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013022771817114618\n",
      "Average test loss: 0.0035362658351659774\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012984625393317805\n",
      "Average test loss: 0.003684300479789575\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012926191513737043\n",
      "Average test loss: 0.0035448060627612803\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0128911738064554\n",
      "Average test loss: 0.0036259869223253596\n",
      "Epoch 41/300\n",
      "Average training loss: 0.012880319598648283\n",
      "Average test loss: 0.003590616065594885\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012814034037292003\n",
      "Average test loss: 0.003510455299375786\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012765777411560217\n",
      "Average test loss: 0.003560895924145977\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01274587250335349\n",
      "Average test loss: 0.0037394531255380975\n",
      "Epoch 45/300\n",
      "Average training loss: 0.012708968191511101\n",
      "Average test loss: 0.003632419796039661\n",
      "Epoch 46/300\n",
      "Average training loss: 0.012635255951848294\n",
      "Average test loss: 0.0034666843252877393\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012626967230604755\n",
      "Average test loss: 0.0034978555645793674\n",
      "Epoch 48/300\n",
      "Average training loss: 0.012586136663125622\n",
      "Average test loss: 0.0035733598845286503\n",
      "Epoch 49/300\n",
      "Average training loss: 0.012570853365792169\n",
      "Average test loss: 0.0034390715739379325\n",
      "Epoch 50/300\n",
      "Average training loss: 0.012534637781480949\n",
      "Average test loss: 0.0034692278827230137\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012498554080724715\n",
      "Average test loss: 0.003441959497001436\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012484077795512147\n",
      "Average test loss: 0.0034499438839654126\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012440988168948227\n",
      "Average test loss: 0.0034311109421153864\n",
      "Epoch 54/300\n",
      "Average training loss: 0.012422911845975451\n",
      "Average test loss: 0.0035156033223287925\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012401143801709017\n",
      "Average test loss: 0.0036724126467274293\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012371531588335831\n",
      "Average test loss: 0.004784132928484016\n",
      "Epoch 57/300\n",
      "Average training loss: 0.012377445794641971\n",
      "Average test loss: 0.0036780256641407807\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01231537985553344\n",
      "Average test loss: 0.003442575359923972\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01230491088082393\n",
      "Average test loss: 0.0034205443925327725\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01225954848031203\n",
      "Average test loss: 0.0034514180032743347\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01224427552272876\n",
      "Average test loss: 0.00343942511247264\n",
      "Epoch 62/300\n",
      "Average training loss: 0.012342124668260415\n",
      "Average test loss: 0.0034383446060948903\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012228671451409658\n",
      "Average test loss: 0.0034902811776846648\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012218787047598097\n",
      "Average test loss: 0.003433415101427171\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012145707954963048\n",
      "Average test loss: 0.0034277184762888484\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012171214328043991\n",
      "Average test loss: 0.003405190555171834\n",
      "Epoch 67/300\n",
      "Average training loss: 0.012138567191859087\n",
      "Average test loss: 0.0034332128138177924\n",
      "Epoch 68/300\n",
      "Average training loss: 0.012142994698550966\n",
      "Average test loss: 0.0033992544015248615\n",
      "Epoch 69/300\n",
      "Average training loss: 0.012104703880018658\n",
      "Average test loss: 0.0037717705292420254\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012072593737807539\n",
      "Average test loss: 0.0035211502094235686\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0120743198543787\n",
      "Average test loss: 0.0035805520166953403\n",
      "Epoch 72/300\n",
      "Average training loss: 0.012108104478153917\n",
      "Average test loss: 0.0033813363160524103\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012016278963122103\n",
      "Average test loss: 0.0037053520762258107\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011998632320099407\n",
      "Average test loss: 0.003472069496081935\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011999204161266486\n",
      "Average test loss: 0.0034256623306622106\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012001221094694403\n",
      "Average test loss: 0.003418484476291471\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01197812543478277\n",
      "Average test loss: 0.003398666039833592\n",
      "Epoch 78/300\n",
      "Average training loss: 0.011960751255353291\n",
      "Average test loss: 0.00339406812004745\n",
      "Epoch 79/300\n",
      "Average training loss: 0.011981995646324423\n",
      "Average test loss: 0.003451315377321508\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011939141524334748\n",
      "Average test loss: 0.003498808204713795\n",
      "Epoch 81/300\n",
      "Average training loss: 0.011912451629837353\n",
      "Average test loss: 0.003420381525531411\n",
      "Epoch 82/300\n",
      "Average training loss: 0.011886847577161259\n",
      "Average test loss: 0.003445634519060453\n",
      "Epoch 83/300\n",
      "Average training loss: 0.011939087908301088\n",
      "Average test loss: 0.0033590828605617085\n",
      "Epoch 84/300\n",
      "Average training loss: 0.011859441223243873\n",
      "Average test loss: 0.003391641355637047\n",
      "Epoch 85/300\n",
      "Average training loss: 0.011848579516013463\n",
      "Average test loss: 0.0036037684372729724\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011852461583912373\n",
      "Average test loss: 0.00340801781643596\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011844463668763638\n",
      "Average test loss: 0.0034362472283343474\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011821748707857398\n",
      "Average test loss: 0.0034908100478351117\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011817676323155562\n",
      "Average test loss: 0.0034547902043494914\n",
      "Epoch 90/300\n",
      "Average training loss: 0.011789712357024352\n",
      "Average test loss: 0.003568168601021171\n",
      "Epoch 91/300\n",
      "Average training loss: 0.011796835927499666\n",
      "Average test loss: 0.0033979065710057813\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01178327589150932\n",
      "Average test loss: 0.0033659857577747767\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011766856015556389\n",
      "Average test loss: 0.0034108978174626826\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011777122682167424\n",
      "Average test loss: 0.004007968343587385\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011727802699638738\n",
      "Average test loss: 0.0034334225758082338\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011740696248080995\n",
      "Average test loss: 0.0034457230375458795\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01172098044057687\n",
      "Average test loss: 0.003492966094985604\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01171868803186549\n",
      "Average test loss: 0.003361805794553624\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011719311529563533\n",
      "Average test loss: 0.0033500187603963745\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01167724699858162\n",
      "Average test loss: 0.0033974694388194215\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011697864010930061\n",
      "Average test loss: 0.003509515912168556\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011694116204149193\n",
      "Average test loss: 0.0033874821441455018\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011651766169402335\n",
      "Average test loss: 0.003493021905836132\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01164358202368021\n",
      "Average test loss: 0.003476345423609018\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01165756022102303\n",
      "Average test loss: 0.0035717599327779476\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011639901815189256\n",
      "Average test loss: 0.0034031113982200623\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011614677177535162\n",
      "Average test loss: 0.003537314433604479\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011624538670811388\n",
      "Average test loss: 0.003374722169091304\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011604781887597507\n",
      "Average test loss: 0.003389676446715991\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011574404378732046\n",
      "Average test loss: 0.0034197554460002316\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011593018904328347\n",
      "Average test loss: 0.0034542183353462154\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011566388108664089\n",
      "Average test loss: 0.003539625589011444\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011594445380071799\n",
      "Average test loss: 0.00349966014838881\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011555905312299728\n",
      "Average test loss: 0.0035524704574296873\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011572088655498292\n",
      "Average test loss: 0.003407875246057908\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011562954027619627\n",
      "Average test loss: 0.0034336483747594887\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011544953554868698\n",
      "Average test loss: 0.00351207473480867\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01151336341847976\n",
      "Average test loss: 0.0034868983862300714\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011515637381209268\n",
      "Average test loss: 0.0034282326623797416\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01154096673346228\n",
      "Average test loss: 0.0033997635261880025\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011501778372459942\n",
      "Average test loss: 0.0034344718580444655\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011486609806617101\n",
      "Average test loss: 0.003833518465360006\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01149413210567501\n",
      "Average test loss: 0.0035279995906684135\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011473709508776665\n",
      "Average test loss: 0.0033598396726366546\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011467430185112688\n",
      "Average test loss: 0.003440953412817584\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011467475251191192\n",
      "Average test loss: 0.003642709132283926\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011464569186170896\n",
      "Average test loss: 0.0038104816135019065\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01145270252890057\n",
      "Average test loss: 0.0034650526487578948\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011449796699815325\n",
      "Average test loss: 0.0033677042461931705\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011444451319674651\n",
      "Average test loss: 0.003457960790644089\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011423935742841826\n",
      "Average test loss: 0.0034463574066758156\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011429527226421567\n",
      "Average test loss: 0.003424603156538473\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011434334030581845\n",
      "Average test loss: 0.003523551158607006\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011408198760615456\n",
      "Average test loss: 0.0034305077609088686\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011396734649108516\n",
      "Average test loss: 0.0034826988532311387\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011382032224701511\n",
      "Average test loss: 0.003678950820946031\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011377929988834593\n",
      "Average test loss: 0.0034684419855475425\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011388356696400377\n",
      "Average test loss: 0.0034519590499500433\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011381722683707874\n",
      "Average test loss: 0.003783020919188857\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01137184820820888\n",
      "Average test loss: 0.004160146155705055\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011345786539216836\n",
      "Average test loss: 0.003420097298299273\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011357980787754058\n",
      "Average test loss: 1.2529692083994548\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011556718611882793\n",
      "Average test loss: 0.0033857424673106934\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011309452989863024\n",
      "Average test loss: 0.0033822106627954375\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011310476542347007\n",
      "Average test loss: 0.0034561026555796465\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011329137647317515\n",
      "Average test loss: 0.003390150817318095\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011316798600885603\n",
      "Average test loss: 0.003430752417486575\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011338391963806418\n",
      "Average test loss: 0.003534673947840929\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011294488305019008\n",
      "Average test loss: 0.0035063740091605317\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011298223124196132\n",
      "Average test loss: 0.0036443249812970558\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011311671158505811\n",
      "Average test loss: 0.0034324572787930567\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011278375498122639\n",
      "Average test loss: 0.00341061039455235\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011281924712989066\n",
      "Average test loss: 0.003460295578671826\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01127744904657205\n",
      "Average test loss: 0.0035223992177181775\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01126462386879656\n",
      "Average test loss: 0.0034541150248712962\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011270442809495662\n",
      "Average test loss: 0.0035475051092604794\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011282181283666029\n",
      "Average test loss: 0.0035238912991351554\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011226418161557781\n",
      "Average test loss: 0.003570845638298326\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011286872055795458\n",
      "Average test loss: 0.0034725011772372655\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011270335365500716\n",
      "Average test loss: 0.003415531785537799\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011249147279394998\n",
      "Average test loss: 0.003587739589727587\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01121020904266172\n",
      "Average test loss: 0.003388865061311258\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011233576628896924\n",
      "Average test loss: 0.0034271459703644116\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01123285550210211\n",
      "Average test loss: 0.003470491444898976\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011223569230073028\n",
      "Average test loss: 0.0037274389759533936\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011228323467903667\n",
      "Average test loss: 0.003606882163633903\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011210009097225136\n",
      "Average test loss: 0.003540770647426446\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011185195246504413\n",
      "Average test loss: 0.0034415729052076735\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011206750677691566\n",
      "Average test loss: 0.0034528169522268903\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011202576154636013\n",
      "Average test loss: 0.0034344384096976783\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011193319271836016\n",
      "Average test loss: 0.003404487728348209\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011192232675850391\n",
      "Average test loss: 0.003515570469614532\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01117906131512589\n",
      "Average test loss: 0.003541698160684771\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0111876196977165\n",
      "Average test loss: 0.0038170883945292896\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011172125197119184\n",
      "Average test loss: 0.0035334937133722835\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011139123344586955\n",
      "Average test loss: 0.003510060584379567\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011164107137256199\n",
      "Average test loss: 0.003495524149801996\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011167991864184539\n",
      "Average test loss: 0.0035547260753810406\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011168129241300954\n",
      "Average test loss: 0.0034396559682985145\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0111354103560249\n",
      "Average test loss: 0.003476407556484143\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01115470428019762\n",
      "Average test loss: 0.0035641873334017063\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011129879722164737\n",
      "Average test loss: 0.003481446098329292\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011127977520227433\n",
      "Average test loss: 0.0034529036130342217\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011139513512452444\n",
      "Average test loss: 0.003510303056281474\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011112393790649043\n",
      "Average test loss: 0.0034878330803993674\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01112378984110223\n",
      "Average test loss: 0.0035887927112893928\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01110872677134143\n",
      "Average test loss: 0.00363544724881649\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011112685065716506\n",
      "Average test loss: 0.004117033504570524\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011089822117653157\n",
      "Average test loss: 0.003471681571048167\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011097744332419501\n",
      "Average test loss: 0.003412654416842593\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011105853435066012\n",
      "Average test loss: 0.0034550915426678126\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011071313906874922\n",
      "Average test loss: 0.0035297251908729473\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01108742302738958\n",
      "Average test loss: 0.0035116310012009407\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011104109524024857\n",
      "Average test loss: 0.0034868980592323673\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01107695418347915\n",
      "Average test loss: 0.0036039722772936026\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011069065902796056\n",
      "Average test loss: 0.0037915805040134322\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011084417342311807\n",
      "Average test loss: 0.0034968122930990326\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01107569423566262\n",
      "Average test loss: 0.0036258924181262653\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01106695627503925\n",
      "Average test loss: 0.0035038233960254326\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011055904618567891\n",
      "Average test loss: 0.0034323194149053758\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011042750254273415\n",
      "Average test loss: 0.0034516545378913484\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011066994498173395\n",
      "Average test loss: 0.0034970796555280686\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011032278423508008\n",
      "Average test loss: 0.0035372441766990557\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011036546864443355\n",
      "Average test loss: 0.0036343074238134754\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011033534569044908\n",
      "Average test loss: 0.0034754767285452948\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011049003595279322\n",
      "Average test loss: 0.003588546965064274\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011011604297492239\n",
      "Average test loss: 0.003514987018580238\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011014273717171615\n",
      "Average test loss: 0.044574443330367405\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011017626912229591\n",
      "Average test loss: 0.003460169326514006\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011013842947781086\n",
      "Average test loss: 0.003440234830396043\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011012242161565357\n",
      "Average test loss: 0.0035356764106286897\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011003668098813958\n",
      "Average test loss: 0.0037464724679787953\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01101568894667758\n",
      "Average test loss: 0.0034549403650065264\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011002647658189138\n",
      "Average test loss: 0.003447226325256957\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010993915296263165\n",
      "Average test loss: 0.003706092327005333\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011012673378404643\n",
      "Average test loss: 0.0035887066442519427\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010993006114744477\n",
      "Average test loss: 0.003942997090414994\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010983901636054118\n",
      "Average test loss: 0.003485286202488674\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010983007402055793\n",
      "Average test loss: 0.003460563317562143\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010961031624840365\n",
      "Average test loss: 0.0033855363616926803\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010972400504681798\n",
      "Average test loss: 0.0034529105031655895\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0109904394985901\n",
      "Average test loss: 0.003874056746562322\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010961163470314608\n",
      "Average test loss: 0.003594771746132109\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010963877083940638\n",
      "Average test loss: 0.003520895053115156\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010966087450583776\n",
      "Average test loss: 0.003442207570291228\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0109561629038718\n",
      "Average test loss: 0.003560931375871102\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010962988399797017\n",
      "Average test loss: 0.0036117632403555844\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010952646002173425\n",
      "Average test loss: 0.00370751727041271\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010969486449327734\n",
      "Average test loss: 0.0034846618523200353\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010954543203115463\n",
      "Average test loss: 0.003480656780095564\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01094331254147821\n",
      "Average test loss: 0.003549893352099591\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010956276070740487\n",
      "Average test loss: 0.0034738775359259714\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010932380818658405\n",
      "Average test loss: 0.0035829955355988607\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010940459858212206\n",
      "Average test loss: 0.003551215705772241\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010912758789128727\n",
      "Average test loss: 0.0034987747482955454\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010914401033686266\n",
      "Average test loss: 0.0036405266659955185\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010956680242386128\n",
      "Average test loss: 0.00349340705283814\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010895820781588555\n",
      "Average test loss: 0.003689507403928373\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010912733364436362\n",
      "Average test loss: 0.0034934292696416377\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010941649121542772\n",
      "Average test loss: 0.0035042582508176567\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010903668146994379\n",
      "Average test loss: 0.003590038469268216\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010942956629726622\n",
      "Average test loss: 0.0035564980308214825\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010896207952664959\n",
      "Average test loss: 0.0034212192487385537\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010888148088422086\n",
      "Average test loss: 0.0035666295968823964\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01090948420100742\n",
      "Average test loss: 0.0035108245212791696\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010911129731271002\n",
      "Average test loss: 0.0034765916145924063\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010889277342292997\n",
      "Average test loss: 0.0035710754841566084\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010886817044268051\n",
      "Average test loss: 0.0035163872285435597\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010873048852301306\n",
      "Average test loss: 0.00356024468710853\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010896178730660015\n",
      "Average test loss: 0.00349941536390947\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010856225255462858\n",
      "Average test loss: 0.0036274683876997894\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01086692358718978\n",
      "Average test loss: 0.0035936153241329723\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010850588248007827\n",
      "Average test loss: 0.0035679868842578596\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01086139992541737\n",
      "Average test loss: 0.0035008222291039097\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01086951816164785\n",
      "Average test loss: 0.0036444402440554565\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01086969338523017\n",
      "Average test loss: 0.00355855905988978\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010873171809646818\n",
      "Average test loss: 0.003490144911739561\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010839712012973096\n",
      "Average test loss: 0.003500964102231794\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01084457985849844\n",
      "Average test loss: 0.0035693180068499513\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010848102267417642\n",
      "Average test loss: 0.003622179851556818\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010854751757449575\n",
      "Average test loss: 0.0037765869345102046\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010843077457613415\n",
      "Average test loss: 0.0038564675711095335\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010844431183404392\n",
      "Average test loss: 0.003803226346770922\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010829908077087667\n",
      "Average test loss: 0.0035140563622117044\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01083861889441808\n",
      "Average test loss: 0.0034850885120944843\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010829661708739068\n",
      "Average test loss: 0.003603673279285431\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010809787189794912\n",
      "Average test loss: 0.003678986345521278\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01083863511764341\n",
      "Average test loss: 0.0034989146101805898\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01081471959501505\n",
      "Average test loss: 0.0037390541469471323\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010802821774449613\n",
      "Average test loss: 0.003593953404575586\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01089192389903797\n",
      "Average test loss: 0.0036035745955175823\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010811941377818584\n",
      "Average test loss: 0.003481292016390297\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010811032477352354\n",
      "Average test loss: 0.003607584963242213\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010792601337863339\n",
      "Average test loss: 0.0034507137400408587\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010820063815348679\n",
      "Average test loss: 0.0035600268840789793\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01079694271998273\n",
      "Average test loss: 0.003551058047554559\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010795373001860249\n",
      "Average test loss: 0.0035335198307616844\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010788084258635838\n",
      "Average test loss: 0.003527967334414522\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010800583109259606\n",
      "Average test loss: 0.0035458713199736343\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010792357674075497\n",
      "Average test loss: 0.0036458050921145415\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010795767871042092\n",
      "Average test loss: 0.0035516046821657153\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010794573339737124\n",
      "Average test loss: 0.0036561493078867595\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010787357232636876\n",
      "Average test loss: 0.003742774695985847\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010794591087434026\n",
      "Average test loss: 0.003585053907086452\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010772346874905957\n",
      "Average test loss: 0.00365802146659957\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010769382474323113\n",
      "Average test loss: 0.0034839862789958717\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010770743825369411\n",
      "Average test loss: 0.0037404718154834377\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010765402698682414\n",
      "Average test loss: 0.003652170360295309\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0107914398436745\n",
      "Average test loss: 0.003568194134367837\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010777684518032604\n",
      "Average test loss: 0.0036047901074505516\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01076231706639131\n",
      "Average test loss: 0.003596690833568573\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010774208074642553\n",
      "Average test loss: 0.003581538197067049\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010769599578446813\n",
      "Average test loss: 0.0035358094399174055\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010758069963918792\n",
      "Average test loss: 0.0036466973096960123\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01074101330505477\n",
      "Average test loss: 0.0036953376817206542\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010745368921922313\n",
      "Average test loss: 0.0035803854792482324\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01074810474117597\n",
      "Average test loss: 0.0036117474912769265\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010748753438393275\n",
      "Average test loss: 0.004067440529043476\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010751048266887666\n",
      "Average test loss: 0.003628596787237459\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01104710519893302\n",
      "Average test loss: 0.0034987245918148093\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.07622484937310219\n",
      "Average test loss: 0.005916884747644266\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02408563632104132\n",
      "Average test loss: 0.005104390878644255\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020906338395343886\n",
      "Average test loss: 0.004412222586158249\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0191257773703999\n",
      "Average test loss: 0.004154575779206223\n",
      "Epoch 5/300\n",
      "Average training loss: 0.017785745822721057\n",
      "Average test loss: 0.003948288162549337\n",
      "Epoch 6/300\n",
      "Average training loss: 0.016681629920999207\n",
      "Average test loss: 0.0038047655208243263\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01570753149274323\n",
      "Average test loss: 0.0038929613271935118\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014908678457140922\n",
      "Average test loss: 0.003664666486076183\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01424559043513404\n",
      "Average test loss: 0.00371426089377039\n",
      "Epoch 10/300\n",
      "Average training loss: 0.013696851769255267\n",
      "Average test loss: 0.003386905154213309\n",
      "Epoch 11/300\n",
      "Average training loss: 0.013268205078111755\n",
      "Average test loss: 0.003427618896174762\n",
      "Epoch 12/300\n",
      "Average training loss: 0.012854399575955338\n",
      "Average test loss: 0.0032582756624453596\n",
      "Epoch 13/300\n",
      "Average training loss: 0.012526432337032423\n",
      "Average test loss: 0.0030962558256255254\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012246619845430056\n",
      "Average test loss: 0.002937546463062366\n",
      "Epoch 15/300\n",
      "Average training loss: 0.011948297043641408\n",
      "Average test loss: 0.007972635459568765\n",
      "Epoch 16/300\n",
      "Average training loss: 0.011768411445948812\n",
      "Average test loss: 0.0029230942440529664\n",
      "Epoch 17/300\n",
      "Average training loss: 0.011516843809849686\n",
      "Average test loss: 0.0027903160519070097\n",
      "Epoch 18/300\n",
      "Average training loss: 0.011329417075547908\n",
      "Average test loss: 0.0028701028959411713\n",
      "Epoch 19/300\n",
      "Average training loss: 0.011170492733518283\n",
      "Average test loss: 0.0027220810453097025\n",
      "Epoch 20/300\n",
      "Average training loss: 0.011030051560865508\n",
      "Average test loss: 0.0026886933758440944\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010887520470552975\n",
      "Average test loss: 0.0025888703275058003\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010779372816284497\n",
      "Average test loss: 0.002749243078546392\n",
      "Epoch 23/300\n",
      "Average training loss: 0.010688374562395943\n",
      "Average test loss: 0.002552120203980141\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01054616235031022\n",
      "Average test loss: 0.002555811988810698\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010461754929688242\n",
      "Average test loss: 0.0025116592270011703\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010386395244134797\n",
      "Average test loss: 0.002520241368148062\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01033333194338613\n",
      "Average test loss: 0.0024822053658879466\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010244252920150757\n",
      "Average test loss: 0.002472831106434266\n",
      "Epoch 29/300\n",
      "Average training loss: 0.010189189659224615\n",
      "Average test loss: 0.002567326844980319\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010109327985180749\n",
      "Average test loss: 0.0025910195412321224\n",
      "Epoch 31/300\n",
      "Average training loss: 0.010067715741693973\n",
      "Average test loss: 0.002412983004210724\n",
      "Epoch 32/300\n",
      "Average training loss: 0.010005563059614765\n",
      "Average test loss: 0.0025041744414096077\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009950110983517434\n",
      "Average test loss: 0.002397442435638772\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00989721321562926\n",
      "Average test loss: 0.0024069394432008266\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009841963412861029\n",
      "Average test loss: 0.002409703966644075\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009809792742547062\n",
      "Average test loss: 0.0024910867100374568\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009783987009690867\n",
      "Average test loss: 0.0024332958236336706\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00973366676022609\n",
      "Average test loss: 0.0023581520894335375\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00969018048296372\n",
      "Average test loss: 0.0023687406331300734\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009664116681863864\n",
      "Average test loss: 0.002360076760769718\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009644503036306965\n",
      "Average test loss: 0.0023293187837633822\n",
      "Epoch 42/300\n",
      "Average training loss: 0.009612577641175853\n",
      "Average test loss: 0.0026196656682425075\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009573318137476841\n",
      "Average test loss: 0.002335163849302464\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009555321393327581\n",
      "Average test loss: 0.0023886535252547925\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009524408594808645\n",
      "Average test loss: 0.002444783906969759\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00949151315788428\n",
      "Average test loss: 0.002359452227751414\n",
      "Epoch 47/300\n",
      "Average training loss: 0.009475776099496418\n",
      "Average test loss: 0.0023564958133631283\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009450675454404618\n",
      "Average test loss: 0.002307879662969046\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009423353566063775\n",
      "Average test loss: 0.002307421508555611\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009408423350089127\n",
      "Average test loss: 0.0023668905516258543\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009377305138442251\n",
      "Average test loss: 0.0023263451631905306\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00937682095501158\n",
      "Average test loss: 0.002334778148267004\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009328309123714766\n",
      "Average test loss: 0.0023099406061487067\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009304465851849979\n",
      "Average test loss: 0.0023378849216840335\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009300123958951897\n",
      "Average test loss: 0.00231149983447459\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009273079034354951\n",
      "Average test loss: 0.0022995211215068897\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009276330093542735\n",
      "Average test loss: 0.002443160891946819\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009240442715585231\n",
      "Average test loss: 0.002276117669832375\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009222223474747604\n",
      "Average test loss: 0.0023107605998714765\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009196908042662674\n",
      "Average test loss: 0.00238405696168128\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009193904898646806\n",
      "Average test loss: 0.002317679576161835\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009177418995648622\n",
      "Average test loss: 0.0023924484302600226\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009168572721381982\n",
      "Average test loss: 0.002279158776005109\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009146111861285236\n",
      "Average test loss: 0.002383810441113181\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009127905122935771\n",
      "Average test loss: 0.0022761974653436076\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009119586609717872\n",
      "Average test loss: 0.0022886548439661663\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009113976563844417\n",
      "Average test loss: 0.002276910705077979\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009091422603362136\n",
      "Average test loss: 0.002288513838003079\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009089281587137116\n",
      "Average test loss: 0.002538720686816507\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009068591423332691\n",
      "Average test loss: 0.002296787538876136\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009053545009758737\n",
      "Average test loss: 0.002251347647772895\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009045597554081016\n",
      "Average test loss: 0.0023615488157504136\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009023964625679785\n",
      "Average test loss: 0.002355639718886879\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00902741648753484\n",
      "Average test loss: 0.0022992703807022835\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009001900834341844\n",
      "Average test loss: 0.002279641091823578\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008978772044181824\n",
      "Average test loss: 0.0023131059331612455\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008991411765416462\n",
      "Average test loss: 0.0023024432907501856\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008962887924578455\n",
      "Average test loss: 0.002315091192929281\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008960204350451629\n",
      "Average test loss: 0.0023250479294608037\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008955353983574443\n",
      "Average test loss: 0.0024580094094077748\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008935666603760587\n",
      "Average test loss: 0.0031204605675819847\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008949466709461477\n",
      "Average test loss: 0.002280613338057366\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008911976489755842\n",
      "Average test loss: 0.002342304634551207\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008915537358986007\n",
      "Average test loss: 0.0023124630857879916\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008901706622706519\n",
      "Average test loss: 0.0023496183001746732\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00887989215263062\n",
      "Average test loss: 0.0024324688168449534\n",
      "Epoch 87/300\n",
      "Average training loss: 0.008878165876285897\n",
      "Average test loss: 0.002298691470589903\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00887339876840512\n",
      "Average test loss: 0.0023019210687941975\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008883396743486325\n",
      "Average test loss: 0.0023029501847922802\n",
      "Epoch 90/300\n",
      "Average training loss: 0.00884884872701433\n",
      "Average test loss: 0.002316862247677313\n",
      "Epoch 91/300\n",
      "Average training loss: 0.008835265756481223\n",
      "Average test loss: 0.002323078855458233\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008842748869624403\n",
      "Average test loss: 0.0023039916563365194\n",
      "Epoch 93/300\n",
      "Average training loss: 0.008819984214587344\n",
      "Average test loss: 0.0023391555840563444\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008818237932191955\n",
      "Average test loss: 0.0022734584824906454\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00882190994090504\n",
      "Average test loss: 0.0023491052994504573\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008787500880658626\n",
      "Average test loss: 0.0023536270753377014\n",
      "Epoch 97/300\n",
      "Average training loss: 0.00878581799318393\n",
      "Average test loss: 0.0023110678404983545\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008800044865243965\n",
      "Average test loss: 0.002345800903936227\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008778744239774015\n",
      "Average test loss: 0.002300316003461679\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008766765977773401\n",
      "Average test loss: 0.0022261285167187454\n",
      "Epoch 101/300\n",
      "Average training loss: 0.008776824086490605\n",
      "Average test loss: 0.002379815536758138\n",
      "Epoch 102/300\n",
      "Average training loss: 0.008756780750221677\n",
      "Average test loss: 0.002312385153128869\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008735847680105104\n",
      "Average test loss: 0.002467637201357219\n",
      "Epoch 104/300\n",
      "Average training loss: 0.008731228514677948\n",
      "Average test loss: 0.0022887880026052397\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008742209996614191\n",
      "Average test loss: 0.0022879178857223856\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008744849365618495\n",
      "Average test loss: 0.0023659374521424375\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008720141504373816\n",
      "Average test loss: 0.002304564558590452\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008727887001716427\n",
      "Average test loss: 0.002300977731330527\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008699976343247626\n",
      "Average test loss: 0.0022908541897518767\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008704455093377167\n",
      "Average test loss: 0.002271631282236841\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008681741088628769\n",
      "Average test loss: 0.0022543033976107837\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008686458120743434\n",
      "Average test loss: 0.002477295572352078\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008691551928718885\n",
      "Average test loss: 0.002299019192655881\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008678634311176008\n",
      "Average test loss: 0.004494386488571763\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008669006692866485\n",
      "Average test loss: 0.002376108150825732\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008650160571353304\n",
      "Average test loss: 0.0023160501186632446\n",
      "Epoch 117/300\n",
      "Average training loss: 0.008658625079525842\n",
      "Average test loss: 0.0023832462866687112\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008655314240190719\n",
      "Average test loss: 0.0023797058501384325\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008663131684064865\n",
      "Average test loss: 0.0023648802366935543\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008631440841489368\n",
      "Average test loss: 0.0023596728787653974\n",
      "Epoch 121/300\n",
      "Average training loss: 0.008629117535634173\n",
      "Average test loss: 0.0023725053684579\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00861762282003959\n",
      "Average test loss: 0.0026609827041005093\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008631467148661613\n",
      "Average test loss: 0.0022945251046783393\n",
      "Epoch 124/300\n",
      "Average training loss: 0.00860562122033702\n",
      "Average test loss: 0.0022578190352974667\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008594314145545165\n",
      "Average test loss: 0.0023767315761910545\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008608247150149611\n",
      "Average test loss: 0.002258859638642106\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00859046214984523\n",
      "Average test loss: 0.0022578969018326863\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008607950165867805\n",
      "Average test loss: 0.002396543277427554\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008592366793917285\n",
      "Average test loss: 0.00229756062808964\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00858642099549373\n",
      "Average test loss: 0.0034692471546845303\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008568289233578576\n",
      "Average test loss: 0.0022869026220093173\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008576721277501849\n",
      "Average test loss: 0.0022431690976437595\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008560565513869127\n",
      "Average test loss: 0.0024004020624690584\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008565652334855662\n",
      "Average test loss: 0.002274294590163562\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008558847531676293\n",
      "Average test loss: 0.002409157551618086\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008555634352895948\n",
      "Average test loss: 0.0023427179584072696\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00854331440312995\n",
      "Average test loss: 0.002234744830057025\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00853391077162491\n",
      "Average test loss: 0.0023210157997285326\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00853393327941497\n",
      "Average test loss: 0.002415869967598054\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008544128702332576\n",
      "Average test loss: 0.0023325106313245164\n",
      "Epoch 141/300\n",
      "Average training loss: 0.008510069285829862\n",
      "Average test loss: 0.002350725449414717\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008519045695248577\n",
      "Average test loss: 0.00236380855490764\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008514655714647637\n",
      "Average test loss: 0.0023422131058242588\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008514793799155288\n",
      "Average test loss: 0.002283328923707207\n",
      "Epoch 145/300\n",
      "Average training loss: 0.008505635946161216\n",
      "Average test loss: 0.0025026674767335256\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008517902364747391\n",
      "Average test loss: 0.0024221473046474986\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008509188606507248\n",
      "Average test loss: 0.0022855838920093244\n",
      "Epoch 148/300\n",
      "Average training loss: 0.008504133538239532\n",
      "Average test loss: 0.0023742018284069167\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008491923532138268\n",
      "Average test loss: 0.0024329129351923864\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00847492805454466\n",
      "Average test loss: 0.0023404596483127937\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008470872135625945\n",
      "Average test loss: 0.002328831352603932\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008477541456619898\n",
      "Average test loss: 0.0023218438926463324\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008477339992092716\n",
      "Average test loss: 0.0029687946711977322\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008476429929335913\n",
      "Average test loss: 0.002396062086025874\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008463894098997116\n",
      "Average test loss: 0.0022863681964162322\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008465586314184799\n",
      "Average test loss: 0.0022801629395948517\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008442726396024227\n",
      "Average test loss: 0.002290755945775244\n",
      "Epoch 158/300\n",
      "Average training loss: 0.008442917087011868\n",
      "Average test loss: 0.0023427111461965574\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008460382228096325\n",
      "Average test loss: 0.002304608965706494\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008438463520672587\n",
      "Average test loss: 0.0023274520221683716\n",
      "Epoch 161/300\n",
      "Average training loss: 0.00846342531674438\n",
      "Average test loss: 0.0022447960985203584\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008430150074263414\n",
      "Average test loss: 0.0023333938845122855\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008427470571464962\n",
      "Average test loss: 0.04455286803344886\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00842656935668654\n",
      "Average test loss: 0.00238762911181483\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008455184075567458\n",
      "Average test loss: 0.0023034204634734324\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008413244152234661\n",
      "Average test loss: 0.0023030272830898563\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00840658694008986\n",
      "Average test loss: 0.00236639849965771\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008408840569357078\n",
      "Average test loss: 0.0023435286217265657\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00839723703927464\n",
      "Average test loss: 0.002363926898688078\n",
      "Epoch 170/300\n",
      "Average training loss: 0.008401632816841205\n",
      "Average test loss: 0.0023399734895469416\n",
      "Epoch 171/300\n",
      "Average training loss: 0.00839569444126553\n",
      "Average test loss: 0.002298413687075178\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008401518358538548\n",
      "Average test loss: 0.002343569602817297\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008407412508295641\n",
      "Average test loss: 0.0022619147958854832\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00840358580607507\n",
      "Average test loss: 0.0023672788917190497\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008392070578618183\n",
      "Average test loss: 0.002376836996111605\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008370560644401445\n",
      "Average test loss: 0.002267403358283142\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008373591195378038\n",
      "Average test loss: 0.0023883585372111863\n",
      "Epoch 178/300\n",
      "Average training loss: 0.008380383585890135\n",
      "Average test loss: 0.0023676602422363227\n",
      "Epoch 179/300\n",
      "Average training loss: 0.00837868850512637\n",
      "Average test loss: 0.002342798617978891\n",
      "Epoch 180/300\n",
      "Average training loss: 0.008376996188114086\n",
      "Average test loss: 0.0023694839143297737\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008369740520086554\n",
      "Average test loss: 0.00237725328716139\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008367398983074559\n",
      "Average test loss: 0.0023680885947412913\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008349296398460866\n",
      "Average test loss: 0.0022906521511160665\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008353944914208518\n",
      "Average test loss: 0.0023457895366268025\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008343996250794994\n",
      "Average test loss: 0.0023645760748121475\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008344175879326132\n",
      "Average test loss: 0.002404090026807454\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008344102174457576\n",
      "Average test loss: 0.0022965132193639875\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008344649218022823\n",
      "Average test loss: 0.0023247233233931993\n",
      "Epoch 189/300\n",
      "Average training loss: 0.008326089202529854\n",
      "Average test loss: 0.002348675875303646\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008342031253708733\n",
      "Average test loss: 0.002305582037092083\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008321938944359621\n",
      "Average test loss: 0.0024502868476427263\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008340831316179699\n",
      "Average test loss: 0.0024390433768017423\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008328218838406934\n",
      "Average test loss: 0.0022887260876595975\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00832282046104471\n",
      "Average test loss: 0.0022825713043825493\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008331452505456076\n",
      "Average test loss: 0.002446764682316118\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008321930404338571\n",
      "Average test loss: 0.002369303354683022\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008304564995070299\n",
      "Average test loss: 0.0023755919122033647\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008310513141254585\n",
      "Average test loss: 0.0023493778221309186\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008322966515603993\n",
      "Average test loss: 0.045247745727499324\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008302058866868417\n",
      "Average test loss: 0.0023788415697506733\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00832898166610135\n",
      "Average test loss: 0.0024278793055564165\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00829287937283516\n",
      "Average test loss: 0.0023404371997134553\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00829739992113577\n",
      "Average test loss: 0.0023115903973165487\n",
      "Epoch 204/300\n",
      "Average training loss: 0.00830573487199015\n",
      "Average test loss: 0.0024702461871008077\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008302070183472501\n",
      "Average test loss: 0.0024009979522476594\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00827159797731373\n",
      "Average test loss: 0.002314116503422459\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00828946142229769\n",
      "Average test loss: 0.002321749267168343\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008281940903100702\n",
      "Average test loss: 0.0023102993994123405\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008272035753561391\n",
      "Average test loss: 0.002357400457064311\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008272021131383048\n",
      "Average test loss: 0.0023611448785911004\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008263486110087898\n",
      "Average test loss: 0.0023648562598973513\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008276009418898158\n",
      "Average test loss: 0.0022780124625811976\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008279235745469729\n",
      "Average test loss: 0.0022897049501124354\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008268659187687767\n",
      "Average test loss: 0.0025730233188304636\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008255071332057317\n",
      "Average test loss: 0.002282941974699497\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008254201179163324\n",
      "Average test loss: 0.0023497123511301145\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008263483584754996\n",
      "Average test loss: 0.002363306995895174\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0082418150305748\n",
      "Average test loss: 0.002349528447414438\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00824079947008027\n",
      "Average test loss: 0.0025832050206760567\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008254203230142594\n",
      "Average test loss: 0.002361675942523612\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008245352360109488\n",
      "Average test loss: 0.0028078342465062936\n",
      "Epoch 222/300\n",
      "Average training loss: 0.008243901059652368\n",
      "Average test loss: 0.0024959191303286286\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008243647216094864\n",
      "Average test loss: 0.0023177263422144785\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008239249191764329\n",
      "Average test loss: 0.002433977512849702\n",
      "Epoch 225/300\n",
      "Average training loss: 0.00823769118057357\n",
      "Average test loss: 0.0023300128707455263\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008233391143381596\n",
      "Average test loss: 0.002440390894810359\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008237019105917877\n",
      "Average test loss: 0.0022865250773934855\n",
      "Epoch 228/300\n",
      "Average training loss: 0.008235152044230037\n",
      "Average test loss: 0.002392238494629661\n",
      "Epoch 229/300\n",
      "Average training loss: 0.00823373290316926\n",
      "Average test loss: 0.0024118815761887363\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008233633038898309\n",
      "Average test loss: 0.002363401878004273\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008214450228545401\n",
      "Average test loss: 0.0022813162830554777\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008235802882247501\n",
      "Average test loss: 0.0023175572293500104\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008219993669125769\n",
      "Average test loss: 0.0024000791888684034\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008218686385287178\n",
      "Average test loss: 0.002335539732542303\n",
      "Epoch 235/300\n",
      "Average training loss: 0.008215810937186083\n",
      "Average test loss: 0.002330225926513473\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008209965524988042\n",
      "Average test loss: 0.002382437491996421\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008201315157943301\n",
      "Average test loss: 0.0024192973480870327\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00820168208827575\n",
      "Average test loss: 0.002480697328121298\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008201640273961756\n",
      "Average test loss: 0.002509862502829896\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008188205121292008\n",
      "Average test loss: 0.002566645868226058\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008200567478934924\n",
      "Average test loss: 0.0022789520925531786\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008191722184419633\n",
      "Average test loss: 0.0024419255685061217\n",
      "Epoch 243/300\n",
      "Average training loss: 0.008215040663878122\n",
      "Average test loss: 0.0023238417427572937\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008182669551008277\n",
      "Average test loss: 0.0023382624180780516\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008193925706048806\n",
      "Average test loss: 0.0023544168539552223\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008192267922891511\n",
      "Average test loss: 0.002356081954513987\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008183062937938504\n",
      "Average test loss: 0.0025761298039514158\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008182134593526522\n",
      "Average test loss: 0.0024222240125139553\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008180819798674849\n",
      "Average test loss: 0.002383366015636259\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008196755108733971\n",
      "Average test loss: 0.0024728303274346723\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008159628390438026\n",
      "Average test loss: 0.0023873721924093034\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008171011772834592\n",
      "Average test loss: 0.0023614612256901133\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00817002214367191\n",
      "Average test loss: 0.002430026955385175\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00818190466447009\n",
      "Average test loss: 0.002369497473248177\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008173539271785154\n",
      "Average test loss: 0.0022923856468664277\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008160169208629263\n",
      "Average test loss: 0.0023741541957068776\n",
      "Epoch 257/300\n",
      "Average training loss: 0.00816986837858955\n",
      "Average test loss: 0.0023722846178958813\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008173865457375844\n",
      "Average test loss: 0.002541341183603638\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008161446794039674\n",
      "Average test loss: 0.0025318653475907113\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008159720486236944\n",
      "Average test loss: 0.002318099338043895\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008161079516841306\n",
      "Average test loss: 0.002335313914136754\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008152317098445362\n",
      "Average test loss: 0.0023742873436874814\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008160341484679116\n",
      "Average test loss: 0.0023347888924181463\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008142680287775067\n",
      "Average test loss: 0.0023572300309315326\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008150641926460795\n",
      "Average test loss: 0.0023734730581442516\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0081508842094077\n",
      "Average test loss: 0.0030924142580479384\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008132605707893769\n",
      "Average test loss: 0.002417685635905299\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008137983821746375\n",
      "Average test loss: 0.0024508317170871628\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008138782968123754\n",
      "Average test loss: 0.002339027474116948\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008135926686641242\n",
      "Average test loss: 0.002403914921192659\n",
      "Epoch 271/300\n",
      "Average training loss: 0.00814112618813912\n",
      "Average test loss: 0.002431213554615776\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008127171381066244\n",
      "Average test loss: 0.002439870452094409\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008141518894996909\n",
      "Average test loss: 0.0024968572494884333\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008132871333923604\n",
      "Average test loss: 0.0024037218869974214\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008122977538655202\n",
      "Average test loss: 0.002395658818797933\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008119592183580002\n",
      "Average test loss: 0.002447245582420793\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008122355160199934\n",
      "Average test loss: 0.0024208732129385076\n",
      "Epoch 278/300\n",
      "Average training loss: 0.00812573261724578\n",
      "Average test loss: 0.0024335604574945236\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008119685452017519\n",
      "Average test loss: 0.0024112129343880546\n",
      "Epoch 280/300\n",
      "Average training loss: 0.00810841488589843\n",
      "Average test loss: 0.0023900020054231086\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008119811772472329\n",
      "Average test loss: 0.0023575724150157636\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008126443449821738\n",
      "Average test loss: 0.002512108443304896\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008115675897234016\n",
      "Average test loss: 0.0023717853122701246\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008112774633698994\n",
      "Average test loss: 0.0023816415640629\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00809878864677416\n",
      "Average test loss: 0.0023666885201301838\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008114347560538186\n",
      "Average test loss: 0.002418362193223503\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008109092097315524\n",
      "Average test loss: 0.0023713568470751247\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008102278981357814\n",
      "Average test loss: 0.0023956331399579843\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00810843903447191\n",
      "Average test loss: 0.0023932897200187046\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008100333070589437\n",
      "Average test loss: 0.00240420831926167\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008088274800943004\n",
      "Average test loss: 0.00234900250679089\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008082244695474704\n",
      "Average test loss: 0.0023805903642334873\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008087934013870028\n",
      "Average test loss: 0.0023525716883854735\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008091107199589411\n",
      "Average test loss: 0.002382409926710857\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008101451881229878\n",
      "Average test loss: 0.002406183399984406\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008082524373299546\n",
      "Average test loss: 0.002398730330583122\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008092704793645275\n",
      "Average test loss: 0.0023519751050819953\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008076580107212067\n",
      "Average test loss: 0.0024125394301695957\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008078734341594907\n",
      "Average test loss: 0.0025414468623283838\n",
      "Epoch 300/300\n",
      "Average training loss: 0.00808473642749919\n",
      "Average test loss: 0.0023258018282552562\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06800713734991021\n",
      "Average test loss: 0.004954176038503647\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02012418172756831\n",
      "Average test loss: 0.003998399260971282\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017243284616205428\n",
      "Average test loss: 0.004312572160321805\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01572449592914846\n",
      "Average test loss: 0.003445359379053116\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014702812198135589\n",
      "Average test loss: 0.0030393133562886055\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013590065508253044\n",
      "Average test loss: 0.0030144862323585484\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012829011036290063\n",
      "Average test loss: 0.0029067937530991104\n",
      "Epoch 8/300\n",
      "Average training loss: 0.012077549176911513\n",
      "Average test loss: 0.0028739951896584698\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011495120277007422\n",
      "Average test loss: 0.0025976807301243145\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011029351437257397\n",
      "Average test loss: 0.0024997965892155963\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010656825258500046\n",
      "Average test loss: 0.0023556842936409846\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010350733954045508\n",
      "Average test loss: 0.0024677825462487007\n",
      "Epoch 13/300\n",
      "Average training loss: 0.010073414345582326\n",
      "Average test loss: 0.002279536634683609\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009834526660541694\n",
      "Average test loss: 0.0023470727343940074\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009574597092966238\n",
      "Average test loss: 0.0022751598929365477\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009374344166782168\n",
      "Average test loss: 0.0020877484027296304\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00921036840064658\n",
      "Average test loss: 0.002054821732557482\n",
      "Epoch 18/300\n",
      "Average training loss: 0.009037165317270491\n",
      "Average test loss: 0.0019878037687804965\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008921917234030035\n",
      "Average test loss: 0.002007303363643587\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008793828278779984\n",
      "Average test loss: 0.0018912887728462617\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00867334205740028\n",
      "Average test loss: 0.0019377990356749958\n",
      "Epoch 22/300\n",
      "Average training loss: 0.00858042245482405\n",
      "Average test loss: 0.0019309001120014324\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008472673383024003\n",
      "Average test loss: 0.0018749270023157199\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00840816834775938\n",
      "Average test loss: 0.0020296070826136404\n",
      "Epoch 25/300\n",
      "Average training loss: 0.008328337026139101\n",
      "Average test loss: 0.0018470219880756405\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008256320590774219\n",
      "Average test loss: 0.0018491258362515106\n",
      "Epoch 27/300\n",
      "Average training loss: 0.008180892402927081\n",
      "Average test loss: 0.0017981092809802957\n",
      "Epoch 28/300\n",
      "Average training loss: 0.008151547060658534\n",
      "Average test loss: 0.0018849704932007525\n",
      "Epoch 29/300\n",
      "Average training loss: 0.008065643543998401\n",
      "Average test loss: 0.001758260429215928\n",
      "Epoch 30/300\n",
      "Average training loss: 0.008027800087299612\n",
      "Average test loss: 0.001778360363923841\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007987462333507007\n",
      "Average test loss: 0.0017130811421407594\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0079287441952361\n",
      "Average test loss: 0.0017486279549904997\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007896756667643785\n",
      "Average test loss: 0.0017539031620447834\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00788775761342711\n",
      "Average test loss: 0.0017466281556213895\n",
      "Epoch 35/300\n",
      "Average training loss: 0.007836608873473273\n",
      "Average test loss: 0.0017282781360877886\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007786971488346656\n",
      "Average test loss: 0.0016992285158485174\n",
      "Epoch 37/300\n",
      "Average training loss: 0.007762719497498538\n",
      "Average test loss: 0.001698568475743135\n",
      "Epoch 38/300\n",
      "Average training loss: 0.007724061315672265\n",
      "Average test loss: 0.0017523377466325959\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007710547654165162\n",
      "Average test loss: 0.0017355426982459095\n",
      "Epoch 40/300\n",
      "Average training loss: 0.007693905514147547\n",
      "Average test loss: 0.0017088985741138458\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00765391233853168\n",
      "Average test loss: 0.0016610466200444433\n",
      "Epoch 42/300\n",
      "Average training loss: 0.007629102860887845\n",
      "Average test loss: 0.0016813970737987094\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007624641163067685\n",
      "Average test loss: 0.0016868820720248752\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0075768416366643375\n",
      "Average test loss: 0.0016558636508675086\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007583838376733992\n",
      "Average test loss: 0.0018807943041125933\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007551160931587219\n",
      "Average test loss: 0.001732623908875717\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0075142423663702275\n",
      "Average test loss: 0.001647967588932564\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0074994524729748565\n",
      "Average test loss: 0.0016758413507292668\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007478749531424708\n",
      "Average test loss: 0.001705681055680745\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007486506720797883\n",
      "Average test loss: 0.0016587148058331675\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007467989235288567\n",
      "Average test loss: 0.0016572530766328175\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007422392971813679\n",
      "Average test loss: 0.0016632152265972562\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007436444424920612\n",
      "Average test loss: 0.001639551453706291\n",
      "Epoch 54/300\n",
      "Average training loss: 0.007399189677503374\n",
      "Average test loss: 0.0017052838344954783\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007397641382283634\n",
      "Average test loss: 0.0017514480028508438\n",
      "Epoch 56/300\n",
      "Average training loss: 0.00737220441053311\n",
      "Average test loss: 0.0016427534864180617\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007358650998108917\n",
      "Average test loss: 0.0016491205527757605\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007338386851880286\n",
      "Average test loss: 0.0016483801125238339\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007345605619665649\n",
      "Average test loss: 0.0016602856503385637\n",
      "Epoch 60/300\n",
      "Average training loss: 0.007325496238552861\n",
      "Average test loss: 0.001720722531278928\n",
      "Epoch 61/300\n",
      "Average training loss: 0.007308976417614354\n",
      "Average test loss: 0.0017249422202714616\n",
      "Epoch 62/300\n",
      "Average training loss: 0.007309304429839055\n",
      "Average test loss: 0.0017086056083854702\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007281663972056574\n",
      "Average test loss: 0.0016815809892076585\n",
      "Epoch 64/300\n",
      "Average training loss: 0.007274322926998139\n",
      "Average test loss: 0.0017851392692989773\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007256184002591504\n",
      "Average test loss: 0.0016271293453044362\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00725932341772649\n",
      "Average test loss: 0.0016048977826204564\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007237771736250983\n",
      "Average test loss: 0.001809451472842031\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007226757273491886\n",
      "Average test loss: 0.0016995480545899935\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007222200918114848\n",
      "Average test loss: 0.0016677787887553375\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007211259171366692\n",
      "Average test loss: 0.0016443794424542122\n",
      "Epoch 71/300\n",
      "Average training loss: 0.007196739641742574\n",
      "Average test loss: 0.001646423890772793\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0071810993001692825\n",
      "Average test loss: 0.0016329405908990238\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007197920365879933\n",
      "Average test loss: 0.0016623304317601854\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00718208912304706\n",
      "Average test loss: 0.0016904704798426894\n",
      "Epoch 75/300\n",
      "Average training loss: 0.007155210765699546\n",
      "Average test loss: 0.001663318140639199\n",
      "Epoch 76/300\n",
      "Average training loss: 0.007155929544733631\n",
      "Average test loss: 0.001666988504015737\n",
      "Epoch 77/300\n",
      "Average training loss: 0.00714753937555684\n",
      "Average test loss: 0.001651651535079711\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0071316025203300845\n",
      "Average test loss: 0.001717066730476088\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007124456092301342\n",
      "Average test loss: 0.001653911540698674\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007111589432176616\n",
      "Average test loss: 0.0017379265713195007\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007103661583115657\n",
      "Average test loss: 0.0016311616439165341\n",
      "Epoch 82/300\n",
      "Average training loss: 0.007104494089053737\n",
      "Average test loss: 0.0016012429199698899\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0070873435334199006\n",
      "Average test loss: 0.0016266237549069854\n",
      "Epoch 84/300\n",
      "Average training loss: 0.00709965365462833\n",
      "Average test loss: 0.001696069088867969\n",
      "Epoch 85/300\n",
      "Average training loss: 0.007088981600271331\n",
      "Average test loss: 0.0016542973264844882\n",
      "Epoch 86/300\n",
      "Average training loss: 0.007060481272223923\n",
      "Average test loss: 0.0016218571299687029\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007059272830064098\n",
      "Average test loss: 0.0018376893169350095\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007057386656602224\n",
      "Average test loss: 0.0016238526267309983\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007046914709524976\n",
      "Average test loss: 0.0016641647720502483\n",
      "Epoch 90/300\n",
      "Average training loss: 0.007040729941593276\n",
      "Average test loss: 0.0026891809770216543\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007049529460983144\n",
      "Average test loss: 0.001655796010668079\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007018676326506668\n",
      "Average test loss: 0.0016314414599910378\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007020932359413968\n",
      "Average test loss: 0.0016681652644442188\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007005310873190562\n",
      "Average test loss: 0.00163547147034357\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007019320850157076\n",
      "Average test loss: 0.001688393014586634\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006997763410624531\n",
      "Average test loss: 0.0016723435804661777\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007001393478777674\n",
      "Average test loss: 0.0016356184550871452\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006998218684560723\n",
      "Average test loss: 0.001647440113334192\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006989527023500867\n",
      "Average test loss: 0.0016989029716286395\n",
      "Epoch 100/300\n",
      "Average training loss: 0.006966188360833459\n",
      "Average test loss: 0.0016329060849837131\n",
      "Epoch 101/300\n",
      "Average training loss: 0.006969194085647662\n",
      "Average test loss: 0.0016275116106909183\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006968749142355389\n",
      "Average test loss: 0.0016317735145696336\n",
      "Epoch 103/300\n",
      "Average training loss: 0.006959139725400342\n",
      "Average test loss: 0.0016114645981126362\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006948451387385527\n",
      "Average test loss: 0.0017400069071186912\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006954747077905469\n",
      "Average test loss: 0.001601782952124874\n",
      "Epoch 106/300\n",
      "Average training loss: 0.006945310775604513\n",
      "Average test loss: 0.001644960414049112\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006932460017502308\n",
      "Average test loss: 0.0016084123200012577\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006932287318425046\n",
      "Average test loss: 0.0016446326304641035\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0069269926208588815\n",
      "Average test loss: 0.001623188246662418\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006913248707850774\n",
      "Average test loss: 0.002245304329631229\n",
      "Epoch 111/300\n",
      "Average training loss: 0.006919549444483386\n",
      "Average test loss: 0.001740197685547173\n",
      "Epoch 112/300\n",
      "Average training loss: 0.006917755088458459\n",
      "Average test loss: 0.001738143273525768\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0069107513692643905\n",
      "Average test loss: 0.0016233851085934374\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0068991472634176414\n",
      "Average test loss: 0.0016766537367883655\n",
      "Epoch 115/300\n",
      "Average training loss: 0.006888363352252377\n",
      "Average test loss: 0.0016489619260860813\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006892417882258694\n",
      "Average test loss: 0.001759697045923935\n",
      "Epoch 117/300\n",
      "Average training loss: 0.006884370343552696\n",
      "Average test loss: 0.0016899913643590278\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0068865878019067975\n",
      "Average test loss: 0.0016367169223311874\n",
      "Epoch 119/300\n",
      "Average training loss: 0.006871491048898962\n",
      "Average test loss: 0.0016746547936151426\n",
      "Epoch 120/300\n",
      "Average training loss: 0.006877084397193458\n",
      "Average test loss: 0.0016203122126559417\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006872724611312151\n",
      "Average test loss: 0.0016530489559388824\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006861628386295504\n",
      "Average test loss: 0.004088919706849588\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006868935263819165\n",
      "Average test loss: 0.0016709834109577867\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006851914125184218\n",
      "Average test loss: 0.0015928464011392659\n",
      "Epoch 125/300\n",
      "Average training loss: 0.00684924479160044\n",
      "Average test loss: 0.0016198118078625864\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0068450773643950625\n",
      "Average test loss: 0.0016553155138260788\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006833837209890286\n",
      "Average test loss: 0.0016508013233542443\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0068309903918868965\n",
      "Average test loss: 0.0016937590080520345\n",
      "Epoch 129/300\n",
      "Average training loss: 0.006836335826665163\n",
      "Average test loss: 0.0016955734309222962\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00683228777266211\n",
      "Average test loss: 0.0016321247075166967\n",
      "Epoch 131/300\n",
      "Average training loss: 0.006810353661162985\n",
      "Average test loss: 0.001636629252280626\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006820010889735487\n",
      "Average test loss: 0.001697584899349345\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006806871910889943\n",
      "Average test loss: 0.0016147839441481564\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0068093501896493965\n",
      "Average test loss: 0.0016776148454389639\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006807849895209074\n",
      "Average test loss: 0.0017456534200658402\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0067863911353051665\n",
      "Average test loss: 0.0016588018055384358\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006804513704859548\n",
      "Average test loss: 0.001674952765305837\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00679180522221658\n",
      "Average test loss: 0.001659803263636099\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006787088125116295\n",
      "Average test loss: 0.0017876266656029556\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006796239851249589\n",
      "Average test loss: 0.0016818395998949806\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006786736047516267\n",
      "Average test loss: 0.0017010793255435096\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006786465610894892\n",
      "Average test loss: 0.0016463294144098957\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006763099652611547\n",
      "Average test loss: 0.001609021834201283\n",
      "Epoch 144/300\n",
      "Average training loss: 0.006771807871758938\n",
      "Average test loss: 0.0016023276394440068\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006764508190668292\n",
      "Average test loss: 0.0016108421532230245\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0067614910465975606\n",
      "Average test loss: 0.0016981253580500683\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006759887783063782\n",
      "Average test loss: 0.0016662250570952891\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0067660032514896655\n",
      "Average test loss: 0.0016258864228924116\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0067481878379152885\n",
      "Average test loss: 0.0016486035773737562\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006751297653963168\n",
      "Average test loss: 0.0016743580388526122\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0067382690306339\n",
      "Average test loss: 0.0016902278150535293\n",
      "Epoch 152/300\n",
      "Average training loss: 0.006744829035881493\n",
      "Average test loss: 0.0016214827169767684\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006728040232012669\n",
      "Average test loss: 0.0016204878437436289\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006732018909520573\n",
      "Average test loss: 0.001636539769358933\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0067322408416204986\n",
      "Average test loss: 0.0017787561116533147\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0067280811294913295\n",
      "Average test loss: 0.0016500311785688003\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00672493982894553\n",
      "Average test loss: 0.0016632033416794406\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00671040807912747\n",
      "Average test loss: 0.001709177421612872\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00671606784976191\n",
      "Average test loss: 0.001677430781432324\n",
      "Epoch 160/300\n",
      "Average training loss: 0.006707396148393552\n",
      "Average test loss: 0.001679101201498674\n",
      "Epoch 161/300\n",
      "Average training loss: 0.006714580537958277\n",
      "Average test loss: 0.0016269132471125986\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006697913368542989\n",
      "Average test loss: 0.0016224538476930725\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006713499855664042\n",
      "Average test loss: 0.0016929481931858593\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00670219692422284\n",
      "Average test loss: 0.001721341120493081\n",
      "Epoch 165/300\n",
      "Average training loss: 0.006688485418342882\n",
      "Average test loss: 0.001676292549404833\n",
      "Epoch 166/300\n",
      "Average training loss: 0.006697081015755733\n",
      "Average test loss: 0.0017161792462898626\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0066913063356445895\n",
      "Average test loss: 0.0016510471316675344\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006700202751490805\n",
      "Average test loss: 0.0016497871612923013\n",
      "Epoch 169/300\n",
      "Average training loss: 0.006697876190145811\n",
      "Average test loss: 0.0016716593532926506\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0066815909242464435\n",
      "Average test loss: 0.001722506800873412\n",
      "Epoch 171/300\n",
      "Average training loss: 0.00667718691792753\n",
      "Average test loss: 0.0017954226068945395\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006680402547121048\n",
      "Average test loss: 0.001641175240278244\n",
      "Epoch 173/300\n",
      "Average training loss: 0.006680275313556194\n",
      "Average test loss: 0.0017060596860117383\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006667263538473182\n",
      "Average test loss: 0.001672706817380256\n",
      "Epoch 175/300\n",
      "Average training loss: 0.006656036122805542\n",
      "Average test loss: 0.0016797974746053418\n",
      "Epoch 176/300\n",
      "Average training loss: 0.006672775436606672\n",
      "Average test loss: 0.0016524459305736754\n",
      "Epoch 177/300\n",
      "Average training loss: 0.006670248220364253\n",
      "Average test loss: 0.0016971978700409333\n",
      "Epoch 178/300\n",
      "Average training loss: 0.006657968169699113\n",
      "Average test loss: 0.001763045818131003\n",
      "Epoch 179/300\n",
      "Average training loss: 0.006660976534502374\n",
      "Average test loss: 0.0016946639120578766\n",
      "Epoch 180/300\n",
      "Average training loss: 0.006656759162329965\n",
      "Average test loss: 0.0016173707576365107\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0066519272733065815\n",
      "Average test loss: 0.001675658755728768\n",
      "Epoch 182/300\n",
      "Average training loss: 0.006649996085713307\n",
      "Average test loss: 0.0016526093665096494\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006637856273187532\n",
      "Average test loss: 0.001633981693122122\n",
      "Epoch 184/300\n",
      "Average training loss: 0.006641691401186917\n",
      "Average test loss: 0.0017057789939766128\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006630722843938404\n",
      "Average test loss: 0.002381019604081909\n",
      "Epoch 186/300\n",
      "Average training loss: 0.006645411104791694\n",
      "Average test loss: 0.0016618027055843008\n",
      "Epoch 187/300\n",
      "Average training loss: 0.006637161376575629\n",
      "Average test loss: 0.0016443305299099948\n",
      "Epoch 188/300\n",
      "Average training loss: 0.006628961884727081\n",
      "Average test loss: 0.0017655142127639717\n",
      "Epoch 189/300\n",
      "Average training loss: 0.006627993295176162\n",
      "Average test loss: 0.0016638253815472125\n",
      "Epoch 190/300\n",
      "Average training loss: 0.006617247003234095\n",
      "Average test loss: 0.0017876941971480846\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0066272122872372465\n",
      "Average test loss: 0.001694363676218523\n",
      "Epoch 192/300\n",
      "Average training loss: 0.006618200077778763\n",
      "Average test loss: 0.0018107788492408065\n",
      "Epoch 193/300\n",
      "Average training loss: 0.006616660101546182\n",
      "Average test loss: 0.00162142188495232\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006616693567070696\n",
      "Average test loss: 0.0016870194454160002\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0066130608394742016\n",
      "Average test loss: 0.0016486815254514416\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00661181001820498\n",
      "Average test loss: 0.0016768867717538443\n",
      "Epoch 197/300\n",
      "Average training loss: 0.006606014312141472\n",
      "Average test loss: 0.00165352139249444\n",
      "Epoch 198/300\n",
      "Average training loss: 0.006607651521348291\n",
      "Average test loss: 0.0016768082157812185\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0066002935941020646\n",
      "Average test loss: 0.010093011571301354\n",
      "Epoch 200/300\n",
      "Average training loss: 0.006636114904243085\n",
      "Average test loss: 0.0017610923348822527\n",
      "Epoch 201/300\n",
      "Average training loss: 0.006589925019691388\n",
      "Average test loss: 0.001649904432758275\n",
      "Epoch 202/300\n",
      "Average training loss: 0.006588567109571563\n",
      "Average test loss: 0.0016588591493459212\n",
      "Epoch 203/300\n",
      "Average training loss: 0.006604012423919307\n",
      "Average test loss: 0.001740886802267697\n",
      "Epoch 204/300\n",
      "Average training loss: 0.006599112079789241\n",
      "Average test loss: 0.0016560613491261999\n",
      "Epoch 205/300\n",
      "Average training loss: 0.006583940826770332\n",
      "Average test loss: 0.0016598001381175386\n",
      "Epoch 206/300\n",
      "Average training loss: 0.006586669094446633\n",
      "Average test loss: 0.0016934429222924842\n",
      "Epoch 207/300\n",
      "Average training loss: 0.006601544111553166\n",
      "Average test loss: 0.001701481654205256\n",
      "Epoch 208/300\n",
      "Average training loss: 0.006574255794700649\n",
      "Average test loss: 0.0016899438586292996\n",
      "Epoch 209/300\n",
      "Average training loss: 0.006592688763307201\n",
      "Average test loss: 0.001644356201713284\n",
      "Epoch 210/300\n",
      "Average training loss: 0.006573726292699576\n",
      "Average test loss: 0.0016816688196526633\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00657409630715847\n",
      "Average test loss: 0.0017351528545841576\n",
      "Epoch 212/300\n",
      "Average training loss: 0.006570570704423719\n",
      "Average test loss: 0.0017174999345507887\n",
      "Epoch 213/300\n",
      "Average training loss: 0.006582271292805672\n",
      "Average test loss: 0.0016831424839587675\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006586937935815917\n",
      "Average test loss: 0.0016528081356858213\n",
      "Epoch 215/300\n",
      "Average training loss: 0.006573183567573627\n",
      "Average test loss: 0.001689107154806455\n",
      "Epoch 216/300\n",
      "Average training loss: 0.006563095857285791\n",
      "Average test loss: 0.0016733620814565154\n",
      "Epoch 217/300\n",
      "Average training loss: 0.006559852032611767\n",
      "Average test loss: 0.0016314855744648312\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0065517862261169485\n",
      "Average test loss: 0.001658709581527445\n",
      "Epoch 219/300\n",
      "Average training loss: 0.006557495918124914\n",
      "Average test loss: 0.0017307460024538967\n",
      "Epoch 220/300\n",
      "Average training loss: 0.006556132827781969\n",
      "Average test loss: 0.0017070896963899335\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0065593545701768665\n",
      "Average test loss: 0.0017505856553713482\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0065524007777373\n",
      "Average test loss: 0.0016852391024844513\n",
      "Epoch 223/300\n",
      "Average training loss: 0.006544995239211453\n",
      "Average test loss: 0.0017009649444371462\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006560407555351654\n",
      "Average test loss: 0.001819815216999915\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006552040476351976\n",
      "Average test loss: 0.0017223975374880765\n",
      "Epoch 226/300\n",
      "Average training loss: 0.006545674324035644\n",
      "Average test loss: 0.0018090053239009447\n",
      "Epoch 227/300\n",
      "Average training loss: 0.006549466402166419\n",
      "Average test loss: 0.0017454389057432612\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006539721868104405\n",
      "Average test loss: 0.0016445222563213771\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006544509401337968\n",
      "Average test loss: 0.001659562765310208\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0065381166330642174\n",
      "Average test loss: 0.0017064369403653674\n",
      "Epoch 231/300\n",
      "Average training loss: 0.006529851766096221\n",
      "Average test loss: 0.0016966867001934184\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006530164827903112\n",
      "Average test loss: 0.0017021921137347817\n",
      "Epoch 233/300\n",
      "Average training loss: 0.006533829734971126\n",
      "Average test loss: 0.0017387803244507975\n",
      "Epoch 234/300\n",
      "Average training loss: 0.006534874081611634\n",
      "Average test loss: 0.0016245685572632484\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006533900230295128\n",
      "Average test loss: 0.0017171174130505985\n",
      "Epoch 236/300\n",
      "Average training loss: 0.006523323581036594\n",
      "Average test loss: 0.0017137310958674384\n",
      "Epoch 237/300\n",
      "Average training loss: 0.006519366210533513\n",
      "Average test loss: 0.0017571554542001751\n",
      "Epoch 238/300\n",
      "Average training loss: 0.006522513597375817\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.75/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.75/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.75/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
