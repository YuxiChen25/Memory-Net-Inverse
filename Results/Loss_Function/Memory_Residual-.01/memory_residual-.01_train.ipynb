{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.01)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.030725298618276914\n",
      "Average test loss: 0.013811268901659382\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011464753658407264\n",
      "Average test loss: 0.010130352490892013\n",
      "Epoch 3/300\n",
      "Average training loss: 0.010085056552042563\n",
      "Average test loss: 0.011192757579187551\n",
      "Epoch 4/300\n",
      "Average training loss: 0.009331488622559442\n",
      "Average test loss: 0.009217868332233694\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008841032303869725\n",
      "Average test loss: 0.008919216330680583\n",
      "Epoch 6/300\n",
      "Average training loss: 0.008473076848520174\n",
      "Average test loss: 0.008719378208948506\n",
      "Epoch 7/300\n",
      "Average training loss: 0.008160764770375358\n",
      "Average test loss: 0.00823952609880103\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007947039120727115\n",
      "Average test loss: 0.008188192459444205\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007735793724242184\n",
      "Average test loss: 0.007810555444823371\n",
      "Epoch 10/300\n",
      "Average training loss: 0.007539832996825377\n",
      "Average test loss: 0.007480979291101297\n",
      "Epoch 11/300\n",
      "Average training loss: 0.00738196787238121\n",
      "Average test loss: 0.007327278803206152\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00721382013335824\n",
      "Average test loss: 0.007173947262681193\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0070718356279863255\n",
      "Average test loss: 0.007149158313042588\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006976934511628416\n",
      "Average test loss: 0.007172874328576856\n",
      "Epoch 15/300\n",
      "Average training loss: 0.006858450916906198\n",
      "Average test loss: 0.007231112754179371\n",
      "Epoch 16/300\n",
      "Average training loss: 0.006757357284840611\n",
      "Average test loss: 0.007097088312523233\n",
      "Epoch 17/300\n",
      "Average training loss: 0.006677254382106993\n",
      "Average test loss: 0.006904844081236257\n",
      "Epoch 18/300\n",
      "Average training loss: 0.006579430374006431\n",
      "Average test loss: 0.006864296543929312\n",
      "Epoch 19/300\n",
      "Average training loss: 0.006477828819718626\n",
      "Average test loss: 0.006718813642859459\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006413625332630343\n",
      "Average test loss: 0.006747139125234551\n",
      "Epoch 21/300\n",
      "Average training loss: 0.006353794922845231\n",
      "Average test loss: 0.006587780501693487\n",
      "Epoch 22/300\n",
      "Average training loss: 0.00628291207510564\n",
      "Average test loss: 0.006564692185570796\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0062194127076201965\n",
      "Average test loss: 0.0065427774836619695\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006140730612393883\n",
      "Average test loss: 0.0065805388730433255\n",
      "Epoch 25/300\n",
      "Average training loss: 0.006092528257105085\n",
      "Average test loss: 0.006444950569834974\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00603883487979571\n",
      "Average test loss: 0.0064598692908055255\n",
      "Epoch 27/300\n",
      "Average training loss: 0.005979896924975845\n",
      "Average test loss: 0.006378805144793457\n",
      "Epoch 28/300\n",
      "Average training loss: 0.005922881487756967\n",
      "Average test loss: 0.006416173723422819\n",
      "Epoch 29/300\n",
      "Average training loss: 0.005885250526583857\n",
      "Average test loss: 0.006459757949329085\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005861659054127004\n",
      "Average test loss: 0.006727813869714737\n",
      "Epoch 31/300\n",
      "Average training loss: 0.005790924112829897\n",
      "Average test loss: 0.0064296294132040605\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005751159334348307\n",
      "Average test loss: 0.006324799871693054\n",
      "Epoch 33/300\n",
      "Average training loss: 0.005716707159661585\n",
      "Average test loss: 0.006344654324154059\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005682864638666312\n",
      "Average test loss: 0.0067045456415249245\n",
      "Epoch 35/300\n",
      "Average training loss: 0.005646334707736969\n",
      "Average test loss: 0.006425334591004583\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005602755750219028\n",
      "Average test loss: 0.006324002337538534\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005570210418767399\n",
      "Average test loss: 0.006293204505410459\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005545254879527622\n",
      "Average test loss: 0.006318474544833104\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005509587118402124\n",
      "Average test loss: 0.006651392545964983\n",
      "Epoch 40/300\n",
      "Average training loss: 0.005473732935471667\n",
      "Average test loss: 0.006421902630064222\n",
      "Epoch 41/300\n",
      "Average training loss: 0.005447542189309994\n",
      "Average test loss: 0.006262593842215008\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0054240058732943405\n",
      "Average test loss: 0.006301640094154411\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005405285890731547\n",
      "Average test loss: 0.006349695429619815\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005366997398022148\n",
      "Average test loss: 0.006325774326920509\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00533508693261279\n",
      "Average test loss: 0.006326533522042963\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005324446044034428\n",
      "Average test loss: 0.006447782143950463\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005308557105975019\n",
      "Average test loss: 0.006419617233177026\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005286168421308199\n",
      "Average test loss: 0.00672783791522185\n",
      "Epoch 49/300\n",
      "Average training loss: 0.005241991956614785\n",
      "Average test loss: 0.006406549408204025\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005234810180548164\n",
      "Average test loss: 0.006409004281792376\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00520769350644615\n",
      "Average test loss: 0.00623020262312558\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005188388079818752\n",
      "Average test loss: 0.006490983982053068\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005172695599703325\n",
      "Average test loss: 0.006436811130493879\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00515357827519377\n",
      "Average test loss: 0.006289917637490564\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005124330881983042\n",
      "Average test loss: 0.006419142172568375\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005125033548308743\n",
      "Average test loss: 0.0065014382215837635\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005094129796657298\n",
      "Average test loss: 0.006603650886979368\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005076152208778593\n",
      "Average test loss: 0.006637683265325096\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005063872205300464\n",
      "Average test loss: 0.006470898970961571\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0050529285116742055\n",
      "Average test loss: 0.006316011045128107\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005035790336628755\n",
      "Average test loss: 0.006341447720925013\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005022701414095031\n",
      "Average test loss: 0.006454938836809662\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005015357342031267\n",
      "Average test loss: 0.006268392594738139\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004989430661002795\n",
      "Average test loss: 0.006293230074147383\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004993133424056901\n",
      "Average test loss: 0.010211350896292262\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004976567058927483\n",
      "Average test loss: 0.006346235792256064\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00494453848608666\n",
      "Average test loss: 0.006330165906912751\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004928246164487468\n",
      "Average test loss: 0.00654345126160317\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004929063177357118\n",
      "Average test loss: 0.00628949822526839\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004917009824472997\n",
      "Average test loss: 0.006335400823503733\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004902266070660618\n",
      "Average test loss: 0.006408026044153505\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004881110974070099\n",
      "Average test loss: 0.006438305300970872\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004869178665181001\n",
      "Average test loss: 0.006389659570323097\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004853202191905843\n",
      "Average test loss: 0.006395237857061956\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0048555189362830585\n",
      "Average test loss: 0.006318392520977391\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0048484189249575135\n",
      "Average test loss: 0.006388990665475528\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004826750270194477\n",
      "Average test loss: 0.006329827032569382\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004821479126397106\n",
      "Average test loss: 0.00627787854646643\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004819940122051372\n",
      "Average test loss: 0.006310300625860691\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004797483609575364\n",
      "Average test loss: 0.006453960084666808\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004784378587371773\n",
      "Average test loss: 0.012601741905841563\n",
      "Epoch 82/300\n",
      "Average training loss: 0.004802207242904438\n",
      "Average test loss: 0.006737497280041377\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00476904174271557\n",
      "Average test loss: 0.0063621808489163715\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004771917548444536\n",
      "Average test loss: 0.006326113398704264\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004750202344523536\n",
      "Average test loss: 0.006383882463806206\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004742057492335638\n",
      "Average test loss: 0.0063637620678378476\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004729902909861671\n",
      "Average test loss: 0.006371043184979094\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004718571099142234\n",
      "Average test loss: 0.006484255245162381\n",
      "Epoch 89/300\n",
      "Average training loss: 0.004718429590264956\n",
      "Average test loss: 0.006635179000596205\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0047011220964292685\n",
      "Average test loss: 0.0063426717933681275\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0047095173427628145\n",
      "Average test loss: 0.0064993146678639785\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0046842747554183\n",
      "Average test loss: 0.00634827884617779\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004675416109462579\n",
      "Average test loss: 0.006412917404539056\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0047077433582809235\n",
      "Average test loss: 0.006394456099304888\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0046667642721699345\n",
      "Average test loss: 0.006509503872444232\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004656397978050841\n",
      "Average test loss: 0.006326216366142034\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004658859553850359\n",
      "Average test loss: 0.0064202470502091775\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004632818239016665\n",
      "Average test loss: 0.006385193393876156\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004627597335726022\n",
      "Average test loss: 0.006369071380959617\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0046303558970491095\n",
      "Average test loss: 0.00641777379396889\n",
      "Epoch 101/300\n",
      "Average training loss: 0.004610982038701574\n",
      "Average test loss: 0.00632652249518368\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0046397752414147056\n",
      "Average test loss: 0.006521264263739188\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004611539920378062\n",
      "Average test loss: 0.006415763684031036\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004596434364716212\n",
      "Average test loss: 0.0062790362164378164\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0045908556882705955\n",
      "Average test loss: 0.006321523578630553\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004585470046434137\n",
      "Average test loss: 0.0066619222284191185\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004583807250277863\n",
      "Average test loss: 0.006379466442598237\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0045674452086289724\n",
      "Average test loss: 0.0064036499576436146\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0045746761419706876\n",
      "Average test loss: 0.00643375779191653\n",
      "Epoch 110/300\n",
      "Average training loss: 0.004563914404975044\n",
      "Average test loss: 0.006427309870719909\n",
      "Epoch 111/300\n",
      "Average training loss: 0.004571685201591916\n",
      "Average test loss: 0.006261538007193142\n",
      "Epoch 112/300\n",
      "Average training loss: 0.00454409007438355\n",
      "Average test loss: 0.006404485310531325\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004546709051976601\n",
      "Average test loss: 0.006450982758568393\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004515569353683128\n",
      "Average test loss: 0.0065002270845903294\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00452583409845829\n",
      "Average test loss: 0.0063603850420978335\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004519919766320123\n",
      "Average test loss: 0.006569839671668079\n",
      "Epoch 117/300\n",
      "Average training loss: 0.00451700562135213\n",
      "Average test loss: 0.006520610925638014\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0045046653126676875\n",
      "Average test loss: 0.006458418814258443\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004498230093883143\n",
      "Average test loss: 0.0064894293745358786\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0044987754457526735\n",
      "Average test loss: 0.006421373496039046\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004498519703331921\n",
      "Average test loss: 0.006461961838934157\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004490448780771759\n",
      "Average test loss: 0.006382825835711426\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004476571492022938\n",
      "Average test loss: 0.006537520392073525\n",
      "Epoch 124/300\n",
      "Average training loss: 0.004496317475206322\n",
      "Average test loss: 0.006426090979741679\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004465604958020978\n",
      "Average test loss: 0.006436240365521775\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0044574595275852415\n",
      "Average test loss: 0.0064799548722803595\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004472805275271336\n",
      "Average test loss: 0.006506351379884614\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004457013608680831\n",
      "Average test loss: 0.006493618280523353\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004444759328953094\n",
      "Average test loss: 0.006391486002339257\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004444603856652975\n",
      "Average test loss: 0.006385314749346839\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004448364947819048\n",
      "Average test loss: 0.006423634903298484\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004427839749803146\n",
      "Average test loss: 0.0064664324666890835\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004427931420505047\n",
      "Average test loss: 0.0065044764060941005\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004424944390646285\n",
      "Average test loss: 0.006297762769584854\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004446449284959171\n",
      "Average test loss: 0.006441514931619167\n",
      "Epoch 136/300\n",
      "Average training loss: 0.00441019051687585\n",
      "Average test loss: 0.006734801281243563\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004410590374014444\n",
      "Average test loss: 0.00650644955266681\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004410699154353804\n",
      "Average test loss: 0.006462328767610921\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004391584638092253\n",
      "Average test loss: 0.006521131529162327\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004410200411660804\n",
      "Average test loss: 0.006445131464550892\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0043967812549736765\n",
      "Average test loss: 0.006804075231568681\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004393705140799284\n",
      "Average test loss: 0.006472875790256593\n",
      "Epoch 143/300\n",
      "Average training loss: 0.004388147273825274\n",
      "Average test loss: 0.006433674680689971\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004379669088456366\n",
      "Average test loss: 0.006497698665079143\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004380473058256838\n",
      "Average test loss: 0.0066688553417722386\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004365709482588701\n",
      "Average test loss: 0.006582014270540741\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0043661138382222915\n",
      "Average test loss: 0.006335418050073915\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004376752757156889\n",
      "Average test loss: 0.006579971000552178\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0043636745404866005\n",
      "Average test loss: 0.006608261640287108\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004380660804609458\n",
      "Average test loss: 0.006339120268407795\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004346940803238087\n",
      "Average test loss: 0.00646909690524141\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00433482390228245\n",
      "Average test loss: 0.006462645038962364\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0043448772372470966\n",
      "Average test loss: 0.006704853650596407\n",
      "Epoch 154/300\n",
      "Average training loss: 0.00433514449869593\n",
      "Average test loss: 0.006600160862008731\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004322936691757705\n",
      "Average test loss: 0.00646655896719959\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004319594543427229\n",
      "Average test loss: 0.006580344015732408\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004328632454905245\n",
      "Average test loss: 0.006436442433959908\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004330271962823139\n",
      "Average test loss: 0.006461044872800509\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004310863309022453\n",
      "Average test loss: 0.006522483447773589\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0043142931856628925\n",
      "Average test loss: 0.006558547631733947\n",
      "Epoch 161/300\n",
      "Average training loss: 0.00431208150419924\n",
      "Average test loss: 0.00695323846116662\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004537082980904314\n",
      "Average test loss: 0.006420152658803595\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004309574386311902\n",
      "Average test loss: 0.0063822774183419015\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004297291713249352\n",
      "Average test loss: 0.0065085282756222615\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004280049830261204\n",
      "Average test loss: 0.006414144211345249\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004286919312758578\n",
      "Average test loss: 0.006501454178243876\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004281021491727895\n",
      "Average test loss: 0.006552441731095314\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004279197180643677\n",
      "Average test loss: 0.0065800612912409835\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004279002156522539\n",
      "Average test loss: 0.0065219203490349984\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004288111516998874\n",
      "Average test loss: 0.006547895073890686\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0042784422573943935\n",
      "Average test loss: 0.006484151613381174\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004798236761656072\n",
      "Average test loss: 0.0062831800449639555\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0045252472559611\n",
      "Average test loss: 0.006635635187228521\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004260722062653965\n",
      "Average test loss: 0.006468647114311656\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0042444300109313595\n",
      "Average test loss: 0.0065244492234455215\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0042350281555619505\n",
      "Average test loss: 0.006673874085562097\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004231860479546917\n",
      "Average test loss: 0.006690561414592796\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004242713837160005\n",
      "Average test loss: 0.006723895801438225\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004245513913946019\n",
      "Average test loss: 0.006435387928452756\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004252905686903331\n",
      "Average test loss: 0.006581152284724845\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004255606301128864\n",
      "Average test loss: 0.006538321099761459\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004239253389338653\n",
      "Average test loss: 0.006538262955430481\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0042349405768844816\n",
      "Average test loss: 0.006428195774969127\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004245813594924079\n",
      "Average test loss: 0.006572384907553594\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00423406892352634\n",
      "Average test loss: 0.006647725240637859\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00423015241097245\n",
      "Average test loss: 0.006527473575125138\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004220570028035177\n",
      "Average test loss: 0.006684789311968618\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004214380487799645\n",
      "Average test loss: 0.006458687064962255\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0042303220373060965\n",
      "Average test loss: 0.0068120426221026315\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004222571529861953\n",
      "Average test loss: 0.006417644559095303\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004212427888065576\n",
      "Average test loss: 0.00642095092849599\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00422738689225581\n",
      "Average test loss: 0.006495346260567506\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004205523640745216\n",
      "Average test loss: 0.006515253389047251\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004191166340476937\n",
      "Average test loss: 0.00658713019390901\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004205424745049742\n",
      "Average test loss: 0.006609254157377614\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004192225909481446\n",
      "Average test loss: 0.0066138665775458015\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004196470983326435\n",
      "Average test loss: 0.006711239918238587\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004190735798329115\n",
      "Average test loss: 0.006604019367860424\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004189315443237623\n",
      "Average test loss: 0.006773129375030597\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004195840956643224\n",
      "Average test loss: 0.006634264464179675\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00419182032884823\n",
      "Average test loss: 0.006491187134136756\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004183767253533006\n",
      "Average test loss: 0.006456528775393963\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004173368253434698\n",
      "Average test loss: 0.006423524136758513\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004188050306505627\n",
      "Average test loss: 0.006633280730909771\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0041731095562378565\n",
      "Average test loss: 0.0065646805663903554\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0041704679586821135\n",
      "Average test loss: 0.006458475318219927\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004156536346922318\n",
      "Average test loss: 0.006531260939935843\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0041693171871205175\n",
      "Average test loss: 0.006549419237093793\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004168413105110328\n",
      "Average test loss: 0.006424573559314013\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00416458758753207\n",
      "Average test loss: 0.006612300291657448\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004148818160510726\n",
      "Average test loss: 0.006807462965862618\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004143032622213165\n",
      "Average test loss: 0.006508981062306298\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004160051696416404\n",
      "Average test loss: 0.006759029449274143\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004150041917752889\n",
      "Average test loss: 0.006455003182921144\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004151531496809588\n",
      "Average test loss: 0.006633668255474832\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0041726423911750314\n",
      "Average test loss: 0.006766298085865047\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0041286861428783996\n",
      "Average test loss: 0.006435142976542314\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004133181508216593\n",
      "Average test loss: 0.006616061783085267\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004134418886982732\n",
      "Average test loss: 0.006451087457852231\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004131844377766053\n",
      "Average test loss: 0.006687770740853416\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004127431140591701\n",
      "Average test loss: 0.006521009635180235\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004131606218922469\n",
      "Average test loss: 0.00674683854315016\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00412170780553586\n",
      "Average test loss: 0.006657848610232274\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004143127817246649\n",
      "Average test loss: 0.006623169665949212\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004126497919153836\n",
      "Average test loss: 0.006504925633884139\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004116674614035421\n",
      "Average test loss: 0.00678897676202986\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00413178504196306\n",
      "Average test loss: 0.006868060111999512\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0041146614452203115\n",
      "Average test loss: 0.006821492472456561\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004107293821871281\n",
      "Average test loss: 0.006465582167108854\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004109445688211256\n",
      "Average test loss: 0.006443249682999319\n",
      "Epoch 231/300\n",
      "Average training loss: 0.00411358948610723\n",
      "Average test loss: 0.006519092827621434\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004118819281458855\n",
      "Average test loss: 0.006530681951178445\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004102272057284912\n",
      "Average test loss: 0.00650533443937699\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004100168001734548\n",
      "Average test loss: 0.006498600653476186\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004123537977122598\n",
      "Average test loss: 0.006596655297610495\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004091411778496371\n",
      "Average test loss: 0.006411794998579555\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004100322528431813\n",
      "Average test loss: 0.006597229492747121\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004096780543526014\n",
      "Average test loss: 0.006587270889017317\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004090467503087388\n",
      "Average test loss: 0.00654798107014762\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004089429892185662\n",
      "Average test loss: 0.0065945292284919155\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0040850801221612425\n",
      "Average test loss: 0.006572706306974093\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004083790638794502\n",
      "Average test loss: 0.006522404071357515\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004072577784251835\n",
      "Average test loss: 0.006714823493527041\n",
      "Epoch 244/300\n",
      "Average training loss: 0.00407750634683503\n",
      "Average test loss: 0.006886953730550077\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004069766220533185\n",
      "Average test loss: 0.0066299864898125335\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004080078792447845\n",
      "Average test loss: 0.006597270325654083\n",
      "Epoch 247/300\n",
      "Average training loss: 0.00407896555442777\n",
      "Average test loss: 0.006630244894988007\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004091369614832931\n",
      "Average test loss: 0.00663332536112931\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004052908595237467\n",
      "Average test loss: 0.006505999588304096\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004066210244264868\n",
      "Average test loss: 0.0066486463447411855\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004158837893770801\n",
      "Average test loss: 0.006458245765417815\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004187136097293761\n",
      "Average test loss: 0.0065995750659041934\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00403896990749571\n",
      "Average test loss: 0.006627508759912517\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004046078217112356\n",
      "Average test loss: 0.006470450397166941\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004061979435798195\n",
      "Average test loss: 0.006586719113505549\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0040399865735736154\n",
      "Average test loss: 0.006496315305431684\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004054865579224295\n",
      "Average test loss: 0.007274819667140643\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0040546907687352765\n",
      "Average test loss: 0.0067684592525992125\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004033804120081994\n",
      "Average test loss: 0.006665727453927199\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0040515487034701635\n",
      "Average test loss: 0.00666399393727382\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004043890129774809\n",
      "Average test loss: 0.006705063767317269\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004041961192670795\n",
      "Average test loss: 0.006837820194661617\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004109818949467606\n",
      "Average test loss: 0.006543958334459199\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004030718869633145\n",
      "Average test loss: 0.00665643319942885\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0040264132327089705\n",
      "Average test loss: 0.006591742732044724\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004032118386485511\n",
      "Average test loss: 0.00664039910170767\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004021491371716062\n",
      "Average test loss: 0.006570711120963096\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004024204704082674\n",
      "Average test loss: 0.006637084831794103\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004025332055985927\n",
      "Average test loss: 0.006501518884466754\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0040229270143641365\n",
      "Average test loss: 0.006703349178036054\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004031446760313378\n",
      "Average test loss: 0.0065598454996943474\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004024214616459277\n",
      "Average test loss: 0.006646058832398719\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004017895382311609\n",
      "Average test loss: 0.006653029429415862\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004018364682379696\n",
      "Average test loss: 0.006525212828069925\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0040270625537054405\n",
      "Average test loss: 0.006876268429060777\n",
      "Epoch 276/300\n",
      "Average training loss: 0.004017424365712537\n",
      "Average test loss: 0.0068188587704466445\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004020767006609175\n",
      "Average test loss: 0.00692355894876851\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0040061097008486585\n",
      "Average test loss: 0.006535478078242805\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004030763904046681\n",
      "Average test loss: 0.006768821221258905\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0040075571843319465\n",
      "Average test loss: 0.006487238328489992\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0040022245028780564\n",
      "Average test loss: 0.006578292224142287\n",
      "Epoch 282/300\n",
      "Average training loss: 0.003994450554665592\n",
      "Average test loss: 0.006613830813931095\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00408449949324131\n",
      "Average test loss: 0.007192041127632062\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004171690347707934\n",
      "Average test loss: 0.006542495439449946\n",
      "Epoch 285/300\n",
      "Average training loss: 0.003977780981610219\n",
      "Average test loss: 0.006593604678908984\n",
      "Epoch 286/300\n",
      "Average training loss: 0.00396673253344165\n",
      "Average test loss: 0.006545307641641961\n",
      "Epoch 287/300\n",
      "Average training loss: 0.003986612841279971\n",
      "Average test loss: 0.006631611504488521\n",
      "Epoch 288/300\n",
      "Average training loss: 0.003986192092506422\n",
      "Average test loss: 0.0067284647155967025\n",
      "Epoch 289/300\n",
      "Average training loss: 0.003989664399375518\n",
      "Average test loss: 0.006619611913131343\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003981614452890224\n",
      "Average test loss: 0.06455540802081426\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00404206018232637\n",
      "Average test loss: 0.006618074134406116\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003983136540485753\n",
      "Average test loss: 0.00666593518294394\n",
      "Epoch 293/300\n",
      "Average training loss: 0.003983399788124693\n",
      "Average test loss: 0.007048573004702727\n",
      "Epoch 294/300\n",
      "Average training loss: 0.003978092826488945\n",
      "Average test loss: 0.006592200573119852\n",
      "Epoch 295/300\n",
      "Average training loss: 0.003991443961444828\n",
      "Average test loss: 0.006548743136227131\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00397722188093596\n",
      "Average test loss: 0.006744422494537301\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0039725648690429\n",
      "Average test loss: 0.006982163408978117\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003989608525608977\n",
      "Average test loss: 0.006574701169712676\n",
      "Epoch 299/300\n",
      "Average training loss: 0.003980350992124942\n",
      "Average test loss: 0.0065915997094578216\n",
      "Epoch 300/300\n",
      "Average training loss: 0.003973602292438348\n",
      "Average test loss: 0.006671122749646504\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02515814658254385\n",
      "Average test loss: 0.009621210931903787\n",
      "Epoch 2/300\n",
      "Average training loss: 0.007968201316479179\n",
      "Average test loss: 0.008396468658414152\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007014498025178909\n",
      "Average test loss: 0.006963379251460234\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006437157297713889\n",
      "Average test loss: 0.006390079667584763\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0060560025291310415\n",
      "Average test loss: 0.0057367995555202165\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0057490895117322605\n",
      "Average test loss: 0.005708197325468063\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005452128294441435\n",
      "Average test loss: 0.005808259506606394\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005238522873570521\n",
      "Average test loss: 0.005386078764994939\n",
      "Epoch 9/300\n",
      "Average training loss: 0.005096008499463399\n",
      "Average test loss: 0.005043243847373459\n",
      "Epoch 10/300\n",
      "Average training loss: 0.004911363874665565\n",
      "Average test loss: 0.004904466588257088\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004750277998546759\n",
      "Average test loss: 0.004671069183697303\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004623651783085532\n",
      "Average test loss: 0.004776541092329555\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004489118403444688\n",
      "Average test loss: 0.004510826028262575\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004393509563058615\n",
      "Average test loss: 0.004408204338616795\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004294905945658684\n",
      "Average test loss: 0.004394002694222662\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004219839600018329\n",
      "Average test loss: 0.00430603904877272\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0041232819621347715\n",
      "Average test loss: 0.004281826539585988\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00404970468290978\n",
      "Average test loss: 0.004192253913109501\n",
      "Epoch 19/300\n",
      "Average training loss: 0.003987132141780522\n",
      "Average test loss: 0.004396900427217285\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003926677578853236\n",
      "Average test loss: 0.004145198346012169\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003869782574267851\n",
      "Average test loss: 0.0040393033677505125\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0038189364516486722\n",
      "Average test loss: 0.003987902046491703\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003766686941186587\n",
      "Average test loss: 0.004001822808550464\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0037170241318850053\n",
      "Average test loss: 0.00394646385539737\n",
      "Epoch 25/300\n",
      "Average training loss: 0.003670068313471145\n",
      "Average test loss: 0.00401239117855827\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003640066688466403\n",
      "Average test loss: 0.003856121485431989\n",
      "Epoch 27/300\n",
      "Average training loss: 0.003595600263733003\n",
      "Average test loss: 0.003915150517390834\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003560064324074321\n",
      "Average test loss: 0.0038796946433269316\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00352474449450771\n",
      "Average test loss: 0.003839772628206346\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0035052534095529053\n",
      "Average test loss: 0.003854189682337973\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0034815674749099548\n",
      "Average test loss: 0.003757754179649055\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0034378043235176138\n",
      "Average test loss: 0.0038009376579688655\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0034040467627346515\n",
      "Average test loss: 0.003921802481636405\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0033997637157638866\n",
      "Average test loss: 0.0041074116846753495\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0033675206334640583\n",
      "Average test loss: 0.0038771100772751702\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003335301373153925\n",
      "Average test loss: 0.004116620531926553\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0033173563060247234\n",
      "Average test loss: 0.0037648791283782985\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003300418065653907\n",
      "Average test loss: 0.0038190449002302356\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003303648579865694\n",
      "Average test loss: 0.003739182903120915\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0032542131272041137\n",
      "Average test loss: 0.004049053293756313\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003239333963021636\n",
      "Average test loss: 0.003756978457172712\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0032238468349807792\n",
      "Average test loss: 0.0037187254486812484\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0032076134362982378\n",
      "Average test loss: 0.003929370463308361\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0032005944785972436\n",
      "Average test loss: 0.0036800553210907514\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0031877670191849273\n",
      "Average test loss: 0.0037624806012544368\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0031567307396067512\n",
      "Average test loss: 0.0037727419953379367\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0031514833873758715\n",
      "Average test loss: 0.0037258825581520795\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0031401163743187983\n",
      "Average test loss: 0.0038028074716114336\n",
      "Epoch 49/300\n",
      "Average training loss: 0.003157035680487752\n",
      "Average test loss: 0.0037271679250730407\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0032698915793250003\n",
      "Average test loss: 0.005949916294051541\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0034038638712631333\n",
      "Average test loss: 0.0036969112075037427\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0031066932611995275\n",
      "Average test loss: 0.0037171655599441793\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0030698093639479744\n",
      "Average test loss: 0.0037162308293498224\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0030638842297097046\n",
      "Average test loss: 0.00378743569035497\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0030669553257111045\n",
      "Average test loss: 0.0037483532360444464\n",
      "Epoch 56/300\n",
      "Average training loss: 0.003059299197047949\n",
      "Average test loss: 0.00381983290405737\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0030527356300089096\n",
      "Average test loss: 0.003911890303509103\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0030337131472511422\n",
      "Average test loss: 0.00548744247895148\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0030375029931051863\n",
      "Average test loss: 0.02295707549651464\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003057811898489793\n",
      "Average test loss: 0.0036793045095271535\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0030359132770035003\n",
      "Average test loss: 0.0037101797345611785\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0030142120654798217\n",
      "Average test loss: 0.0037364900070759985\n",
      "Epoch 63/300\n",
      "Average training loss: 0.003009295528754592\n",
      "Average test loss: 0.0037811856077363095\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003031971284602251\n",
      "Average test loss: 0.0037811677768412565\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0029711807403299544\n",
      "Average test loss: 0.003697591454204586\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0029865993664910397\n",
      "Average test loss: 0.003689530149930053\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0029629125787566106\n",
      "Average test loss: 0.023882044850124254\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0030007547672010132\n",
      "Average test loss: 0.0036901634542478457\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002953121217381623\n",
      "Average test loss: 0.003796254306824671\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002954687007185486\n",
      "Average test loss: 0.003866671710792515\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0029357164102709957\n",
      "Average test loss: 0.0036616153464549116\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0029318465880221792\n",
      "Average test loss: 0.003720239308145311\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002926773074807392\n",
      "Average test loss: 0.0041879896161456905\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0029211059239589504\n",
      "Average test loss: 0.003822446922461192\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0029128604609933166\n",
      "Average test loss: 0.003678207587864664\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002926734168289436\n",
      "Average test loss: 0.003676237341016531\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0028976162436107795\n",
      "Average test loss: 0.0036877759519136615\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0043883472385091915\n",
      "Average test loss: 0.003975927408784628\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0035854679357467424\n",
      "Average test loss: 0.003687602755510145\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0031796591164958148\n",
      "Average test loss: 0.003693642597645521\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0029586144087629185\n",
      "Average test loss: 0.003706612622158395\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002876389004704025\n",
      "Average test loss: 0.0037280212301346993\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0028508588458514878\n",
      "Average test loss: 0.003670438164845109\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0028448872164719636\n",
      "Average test loss: 0.00379500056265129\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0028456569893492594\n",
      "Average test loss: 0.003733626471211513\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002854227694372336\n",
      "Average test loss: 0.003724298256345921\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0028521750853914354\n",
      "Average test loss: 0.0037538195997476576\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0028503909767087963\n",
      "Average test loss: 0.0038273329569233788\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0028560925455143053\n",
      "Average test loss: 0.0038565468436314\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0028691455971242654\n",
      "Average test loss: 0.003846123789333635\n",
      "Epoch 91/300\n",
      "Average training loss: 0.002852015796014004\n",
      "Average test loss: 0.003691410483999385\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0028396434049225514\n",
      "Average test loss: 0.0037378614942232768\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0029066909408817687\n",
      "Average test loss: 0.003796537734154198\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002833900429101454\n",
      "Average test loss: 0.0037495309664971298\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0028278258060001663\n",
      "Average test loss: 0.003815674889004893\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0028356276011715334\n",
      "Average test loss: 0.003723985028660132\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0028316239290353323\n",
      "Average test loss: 0.0037189485538336965\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0028308492983794876\n",
      "Average test loss: 0.003688681666428844\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002806673191487789\n",
      "Average test loss: 0.0037303982687493164\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0027986811188360056\n",
      "Average test loss: 0.0037521812675727737\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002797309089038107\n",
      "Average test loss: 0.0037870213931633367\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002798532414353556\n",
      "Average test loss: 0.003785308553526799\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002795001010513968\n",
      "Average test loss: 0.003769569170764751\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0027867278945114876\n",
      "Average test loss: 0.0037005008479787244\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0028398750730686717\n",
      "Average test loss: 0.003750493686853184\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002766129422105021\n",
      "Average test loss: 0.0037157809076209863\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0027680125611109865\n",
      "Average test loss: 0.003727232793139087\n",
      "Epoch 108/300\n",
      "Average training loss: 0.002762834336401688\n",
      "Average test loss: 0.003722175763712989\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0027761658606015975\n",
      "Average test loss: 0.0037669009293119114\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0027632791101932525\n",
      "Average test loss: 0.0037247230362974933\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0028402090246478716\n",
      "Average test loss: 0.0038137737442221907\n",
      "Epoch 112/300\n",
      "Average training loss: 0.00274520848525895\n",
      "Average test loss: 0.0037273702377246484\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0027548363546116483\n",
      "Average test loss: 0.0038832850555578866\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0027379863823039662\n",
      "Average test loss: 0.003777055582445529\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0029947380303508705\n",
      "Average test loss: 0.003687137461370892\n",
      "Epoch 116/300\n",
      "Average training loss: 0.002737132239051991\n",
      "Average test loss: 0.003793540579163366\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0027217332849072086\n",
      "Average test loss: 0.003883701477613714\n",
      "Epoch 118/300\n",
      "Average training loss: 0.002717963332310319\n",
      "Average test loss: 0.003763879499087731\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0027246874663978816\n",
      "Average test loss: 0.0038148705079737635\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0027454183569384947\n",
      "Average test loss: 0.003981293199376928\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002724085304679142\n",
      "Average test loss: 0.003835430069723063\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002717765715283652\n",
      "Average test loss: 0.0038857035272651247\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002710891746191515\n",
      "Average test loss: 0.003899356048968103\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0027051547575328086\n",
      "Average test loss: 0.003710131958540943\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0027111999369743797\n",
      "Average test loss: 0.004921777026106914\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0027391216645224225\n",
      "Average test loss: 0.0037202970451778836\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0027065801297624908\n",
      "Average test loss: 0.008899477168089814\n",
      "Epoch 128/300\n",
      "Average training loss: 0.002707707091752026\n",
      "Average test loss: 0.0037927124570641254\n",
      "Epoch 129/300\n",
      "Average training loss: 0.002803973316318459\n",
      "Average test loss: 0.0038010473218229083\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002693810095596645\n",
      "Average test loss: 0.003812995089424981\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0026803582209265894\n",
      "Average test loss: 0.003850654687318537\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0026829016585316924\n",
      "Average test loss: 0.003838074992100398\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0026875603687432077\n",
      "Average test loss: 0.003817939129968484\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0026804322459631495\n",
      "Average test loss: 0.0037793004227181276\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0026804416889531744\n",
      "Average test loss: 0.003773803317298492\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0026752005867246124\n",
      "Average test loss: 0.003871931137724055\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002679776595491502\n",
      "Average test loss: 0.0037315613052083386\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0026765960898871223\n",
      "Average test loss: 0.003797139857792192\n",
      "Epoch 139/300\n",
      "Average training loss: 0.002689015514527758\n",
      "Average test loss: 0.003930083334445954\n",
      "Epoch 140/300\n",
      "Average training loss: 0.002671403233996696\n",
      "Average test loss: 0.004029164114346107\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0026583469800858035\n",
      "Average test loss: 0.0037859041788097887\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0026615011480947337\n",
      "Average test loss: 0.0039349798365599575\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0026524027136878834\n",
      "Average test loss: 0.0037523178642408714\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0026792285908013584\n",
      "Average test loss: 0.003737596465481652\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002651979958638549\n",
      "Average test loss: 0.0038636485963231986\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0026411723347587717\n",
      "Average test loss: 0.003743520492480861\n",
      "Epoch 147/300\n",
      "Average training loss: 0.002649672384477324\n",
      "Average test loss: 0.0037924007800304227\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002649750757548544\n",
      "Average test loss: 0.0038669348946875994\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002638072758085198\n",
      "Average test loss: 0.0038356833294447925\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0026442142656693856\n",
      "Average test loss: 0.0038953209022680917\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0026605432762040033\n",
      "Average test loss: 0.0037943628711832895\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0026210021105491452\n",
      "Average test loss: 0.0037814666943417653\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0026320264184226594\n",
      "Average test loss: 0.003913731296857198\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0026404087273404\n",
      "Average test loss: 0.003922682498892148\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002634209209639165\n",
      "Average test loss: 0.00429070442294081\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0026233870993471807\n",
      "Average test loss: 0.003994163796305656\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002681148898891277\n",
      "Average test loss: 0.003766857953121265\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0026124751259469325\n",
      "Average test loss: 0.0037951422242654695\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0026165183418326906\n",
      "Average test loss: 0.003751681617771586\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0026078753183699315\n",
      "Average test loss: 0.003996949582878087\n",
      "Epoch 161/300\n",
      "Average training loss: 0.002617559010783831\n",
      "Average test loss: 0.003806801832177573\n",
      "Epoch 162/300\n",
      "Average training loss: 0.002603614015297757\n",
      "Average test loss: 0.0037542502027418877\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002607866449074613\n",
      "Average test loss: 0.0038578162317474685\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0026212996180272766\n",
      "Average test loss: 0.0038144056710104146\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0026022466040319867\n",
      "Average test loss: 0.003891593247652054\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0026039463627255625\n",
      "Average test loss: 0.003842038335899512\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0025953083644724554\n",
      "Average test loss: 0.0037910817368990847\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0025949216296689377\n",
      "Average test loss: 0.0038025946925497717\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0026351317554298374\n",
      "Average test loss: 0.003806329684332013\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0025998211891079944\n",
      "Average test loss: 0.0037899615888794262\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002583975826183127\n",
      "Average test loss: 0.0037681206221588785\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0025912904602785904\n",
      "Average test loss: 0.003864086986415916\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00258366811213394\n",
      "Average test loss: 0.004456027911769019\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00259638786315918\n",
      "Average test loss: 0.0039032256272104053\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00258915824484494\n",
      "Average test loss: 0.0038506041572739682\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0025793694129420653\n",
      "Average test loss: 0.003822253254552682\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0025758712403476237\n",
      "Average test loss: 0.0037973018425206343\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0025904653614593877\n",
      "Average test loss: 0.003884258411824703\n",
      "Epoch 179/300\n",
      "Average training loss: 0.00256628157529566\n",
      "Average test loss: 0.003875228243155612\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002573411501944065\n",
      "Average test loss: 0.0038394699034591515\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0025728844441473483\n",
      "Average test loss: 0.0038795922212302683\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0025701924696978594\n",
      "Average test loss: 0.003782154976286822\n",
      "Epoch 183/300\n",
      "Average training loss: 0.002640897633093927\n",
      "Average test loss: 0.003894622599085172\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002556190907748209\n",
      "Average test loss: 0.003929135467857122\n",
      "Epoch 185/300\n",
      "Average training loss: 0.002559762822257148\n",
      "Average test loss: 0.0038298670024507577\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0025706257885321974\n",
      "Average test loss: 0.003949892630593644\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0025551297983361617\n",
      "Average test loss: 0.0038118846012900275\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0025732639682375723\n",
      "Average test loss: 0.004126613083812926\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0025507466652327114\n",
      "Average test loss: 0.0037941446618901357\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0025471867790652645\n",
      "Average test loss: 0.0037815261288649507\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002560323915133874\n",
      "Average test loss: 0.003880295089135567\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002552002011694842\n",
      "Average test loss: 0.0038314967995716465\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0025516594973289306\n",
      "Average test loss: 0.003840868739204274\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0026085350587964056\n",
      "Average test loss: 0.0038178777972029316\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0025386017720318505\n",
      "Average test loss: 0.0038785578869283198\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0025418897757513654\n",
      "Average test loss: 0.0038599018044769766\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0025496125923883586\n",
      "Average test loss: 0.003931225654151704\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002541402683282892\n",
      "Average test loss: 0.003808821994397375\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0025396312127510708\n",
      "Average test loss: 0.0038723522441254724\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002595792937195963\n",
      "Average test loss: 0.00377836638647649\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002540924004175597\n",
      "Average test loss: 0.0037836443363792366\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0025288514701856508\n",
      "Average test loss: 0.0037886992624650397\n",
      "Epoch 203/300\n",
      "Average training loss: 0.002533124845371478\n",
      "Average test loss: 0.003893536847912603\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0025329135430769787\n",
      "Average test loss: 0.0037995150917106203\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0025315638991693654\n",
      "Average test loss: 0.0037860843961437544\n",
      "Epoch 206/300\n",
      "Average training loss: 0.002534893739968538\n",
      "Average test loss: 0.0038193200456185473\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0025271744730157985\n",
      "Average test loss: 0.0038262032591220405\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0025209264064000713\n",
      "Average test loss: 0.004579108416620228\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0025266093960445786\n",
      "Average test loss: 0.003972755678618948\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0025287341340962384\n",
      "Average test loss: 0.003938263505904211\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0025423604771494866\n",
      "Average test loss: 0.0038688669063978724\n",
      "Epoch 212/300\n",
      "Average training loss: 0.003179706056498819\n",
      "Average test loss: 0.0037644476702229845\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0027619100341366395\n",
      "Average test loss: 0.0037565169487562442\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0025484918509092597\n",
      "Average test loss: 0.003832183616028892\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0024947672583576705\n",
      "Average test loss: 0.003996693958424859\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002492977318353951\n",
      "Average test loss: 0.0039742267876863475\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0024962999993521306\n",
      "Average test loss: 0.003990311581020554\n",
      "Epoch 218/300\n",
      "Average training loss: 0.002500654337824219\n",
      "Average test loss: 0.003932213013784753\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002504829432194432\n",
      "Average test loss: 0.004009389368403289\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0025044653264598713\n",
      "Average test loss: 0.0037561425481819444\n",
      "Epoch 221/300\n",
      "Average training loss: 0.002512892975989315\n",
      "Average test loss: 0.0037989615843527845\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0025182033112893503\n",
      "Average test loss: 0.003998988494483961\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0025078547159209847\n",
      "Average test loss: 0.00391701277386811\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0025222787210303875\n",
      "Average test loss: 0.0039135592215591005\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0024971811688608596\n",
      "Average test loss: 0.0038483423652748267\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0025131666478183535\n",
      "Average test loss: 0.039143141620688965\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0025412821862846615\n",
      "Average test loss: 0.003899740297968189\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0025043385889795093\n",
      "Average test loss: 0.003918546796051993\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002492368716125687\n",
      "Average test loss: 0.0038428247813135385\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0024985624187522466\n",
      "Average test loss: 0.0038087806300156645\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0025006144951201146\n",
      "Average test loss: 0.00385689447965059\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0025000181801410185\n",
      "Average test loss: 0.0038638801649212838\n",
      "Epoch 233/300\n",
      "Average training loss: 0.002491246769411696\n",
      "Average test loss: 0.0038853444792330263\n",
      "Epoch 234/300\n",
      "Average training loss: 0.002492352077944411\n",
      "Average test loss: 0.0038244660819570222\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0024972383570339945\n",
      "Average test loss: 0.0038362420834600927\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002509391118461887\n",
      "Average test loss: 0.003931636965523164\n",
      "Epoch 237/300\n",
      "Average training loss: 0.002498009288476573\n",
      "Average test loss: 0.0038123466645677883\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0024818153356512386\n",
      "Average test loss: 0.00389290790590975\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002478882020132409\n",
      "Average test loss: 0.0038466369890504414\n",
      "Epoch 240/300\n",
      "Average training loss: 0.002486014700598187\n",
      "Average test loss: 0.004003974823281169\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0025209112570931513\n",
      "Average test loss: 0.003848112526867125\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0024961359581599633\n",
      "Average test loss: 0.003885574152900113\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0024798633141650092\n",
      "Average test loss: 0.0037937992215156557\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0024747516678439246\n",
      "Average test loss: 0.004015212609122197\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0024789703796721167\n",
      "Average test loss: 0.0040132359529121055\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002487663957186871\n",
      "Average test loss: 0.0038458415516134767\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0025427071375565395\n",
      "Average test loss: 0.003955793865438964\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0024678555076114005\n",
      "Average test loss: 0.0038733855937090186\n",
      "Epoch 249/300\n",
      "Average training loss: 0.002463852963927719\n",
      "Average test loss: 0.004057173446855611\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0024760695840749477\n",
      "Average test loss: 0.003915371583774686\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0024687529113143684\n",
      "Average test loss: 0.003863310944288969\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0024866277020838524\n",
      "Average test loss: 0.003949737233006292\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0024644665929178396\n",
      "Average test loss: 0.0038851856254041193\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0024655652925786044\n",
      "Average test loss: 0.0038624182322786913\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002468629322118229\n",
      "Average test loss: 0.003876272540125582\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002467405644762847\n",
      "Average test loss: 0.003933146389615205\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0024679763375057115\n",
      "Average test loss: 0.003928817451414135\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002465410796718465\n",
      "Average test loss: 0.003885730333833231\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0024733628317092854\n",
      "Average test loss: 0.0039815110907786425\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002460085857659578\n",
      "Average test loss: 0.0038541162496225703\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0024631780888885258\n",
      "Average test loss: 0.004006810925487015\n",
      "Epoch 262/300\n",
      "Average training loss: 0.002462580402071277\n",
      "Average test loss: 0.0038789042486912675\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002454820999254783\n",
      "Average test loss: 0.0038851247798237535\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0024610634541345966\n",
      "Average test loss: 0.0038531182408332826\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0024596391489936247\n",
      "Average test loss: 0.0038964640591293573\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0024699186289476025\n",
      "Average test loss: 0.0038324675175050896\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0024507852912777\n",
      "Average test loss: 0.0039041491780016156\n",
      "Epoch 268/300\n",
      "Average training loss: 0.002457920545091232\n",
      "Average test loss: 0.003866325105643935\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0024455162992493974\n",
      "Average test loss: 0.003917841907176706\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00245487885373748\n",
      "Average test loss: 0.0039129530888878635\n",
      "Epoch 271/300\n",
      "Average training loss: 0.002447650673488776\n",
      "Average test loss: 0.003872865272892846\n",
      "Epoch 272/300\n",
      "Average training loss: 0.00246146804963549\n",
      "Average test loss: 0.003924287351262238\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0024659250016427704\n",
      "Average test loss: 0.0039767124967442615\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002442792895560463\n",
      "Average test loss: 0.0038335262841234603\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0024428258834199774\n",
      "Average test loss: 0.0038440877127771575\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0024422486511369544\n",
      "Average test loss: 0.003949191492464807\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002443028894149595\n",
      "Average test loss: 0.0038340097806519933\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0024414206854999065\n",
      "Average test loss: 0.003956786904897955\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002452315542846918\n",
      "Average test loss: 0.003906354214789139\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002444581818352971\n",
      "Average test loss: 0.003907653483872613\n",
      "Epoch 281/300\n",
      "Average training loss: 0.002439388365381294\n",
      "Average test loss: 0.00402643181466394\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0024358007659514747\n",
      "Average test loss: 0.0040245010194679106\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0024358756109658214\n",
      "Average test loss: 0.004016951448180609\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0024392488540874585\n",
      "Average test loss: 0.004231154800703128\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0024333950558470354\n",
      "Average test loss: 0.003979785746998257\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002434400520597895\n",
      "Average test loss: 0.0039028977846933734\n",
      "Epoch 287/300\n",
      "Average training loss: 0.002429634041359855\n",
      "Average test loss: 0.004043011729088095\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0024374085548851225\n",
      "Average test loss: 0.0038373936334004004\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002466760765140255\n",
      "Average test loss: 0.0039942819351951285\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0024247737609677843\n",
      "Average test loss: 0.00537477571475837\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0024298349351932606\n",
      "Average test loss: 0.0038706223467985788\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002423991752271023\n",
      "Average test loss: 0.004029092005764445\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0024281250240488187\n",
      "Average test loss: 0.0038795971042580074\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0024310384476557375\n",
      "Average test loss: 0.003953495282265875\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002427385775786307\n",
      "Average test loss: 0.004012033740265502\n",
      "Epoch 296/300\n",
      "Average training loss: 0.003463580800841252\n",
      "Average test loss: 0.003899552080159386\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0030589199155155156\n",
      "Average test loss: 0.0037471085211469067\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0025577380038383935\n",
      "Average test loss: 0.003858031544627415\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0024409888612313404\n",
      "Average test loss: 0.0039333328853050865\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0023909071737693416\n",
      "Average test loss: 0.003854700245584051\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02254230282538467\n",
      "Average test loss: 0.006835404228005144\n",
      "Epoch 2/300\n",
      "Average training loss: 0.006496826760884788\n",
      "Average test loss: 0.0072842855602502825\n",
      "Epoch 3/300\n",
      "Average training loss: 0.005528536085039377\n",
      "Average test loss: 0.005270902871257729\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0050118779142697654\n",
      "Average test loss: 0.004582063909620047\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0046347695245511\n",
      "Average test loss: 0.004880261939432886\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004333943164183034\n",
      "Average test loss: 0.0040599543352921805\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004090133484866884\n",
      "Average test loss: 0.004010660014218754\n",
      "Epoch 8/300\n",
      "Average training loss: 0.003895149947868453\n",
      "Average test loss: 0.0038089253567159176\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0037254104485942257\n",
      "Average test loss: 0.0038709412864926787\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0035785647314041853\n",
      "Average test loss: 0.0035928165382809107\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0034585204147216347\n",
      "Average test loss: 0.003949355886628231\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003328918613700403\n",
      "Average test loss: 0.0034798708851966594\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0032257819573084513\n",
      "Average test loss: 0.0031749724886483617\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0031247021115074556\n",
      "Average test loss: 0.0031539739320675534\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0030508901754187213\n",
      "Average test loss: 0.004570930605133375\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0029771970997874934\n",
      "Average test loss: 0.003180147257530027\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0028947068597707484\n",
      "Average test loss: 0.0029481409144484333\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0028251893474823897\n",
      "Average test loss: 0.00292925462168124\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0027765433587547805\n",
      "Average test loss: 0.0028506886015335717\n",
      "Epoch 20/300\n",
      "Average training loss: 0.002729266820061538\n",
      "Average test loss: 0.003017553411838081\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002675284564288126\n",
      "Average test loss: 0.0028139660944127374\n",
      "Epoch 22/300\n",
      "Average training loss: 0.002635940212549435\n",
      "Average test loss: 0.0029749102261331347\n",
      "Epoch 23/300\n",
      "Average training loss: 0.002608925384986732\n",
      "Average test loss: 0.002747844271775749\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002554029543366697\n",
      "Average test loss: 0.0027190898031824165\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0025229246776757967\n",
      "Average test loss: 0.002644995766898824\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00250047300528321\n",
      "Average test loss: 0.002692409226670861\n",
      "Epoch 27/300\n",
      "Average training loss: 0.002481331079784367\n",
      "Average test loss: 0.002606791271517674\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0024414839706280167\n",
      "Average test loss: 0.0026411827821284533\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0024236882554574146\n",
      "Average test loss: 0.002870635668850607\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002668327707797289\n",
      "Average test loss: 0.0040406986996531485\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0027327993127206963\n",
      "Average test loss: 0.0026188978167871633\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002471843608137634\n",
      "Average test loss: 0.0026275276149519615\n",
      "Epoch 33/300\n",
      "Average training loss: 0.002405227129140662\n",
      "Average test loss: 0.0025996326773116986\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002374164881598618\n",
      "Average test loss: 0.0025624176979892783\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0023585295755830077\n",
      "Average test loss: 0.0025815388394726645\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002339793521600465\n",
      "Average test loss: 0.0026179990499383874\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0023253098192314308\n",
      "Average test loss: 0.0025669183337854013\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0023322509413378108\n",
      "Average test loss: 0.0026315697545392647\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002316349731758237\n",
      "Average test loss: 0.0025294373213417\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002283319957864781\n",
      "Average test loss: 0.0025165001408507426\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0022862386144697666\n",
      "Average test loss: 0.0024891393478545876\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0022937384471297265\n",
      "Average test loss: 0.002670346843699614\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0022423771747077507\n",
      "Average test loss: 0.0025130729217910106\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0022216684713752734\n",
      "Average test loss: 0.0025648517279575267\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0022118442274319627\n",
      "Average test loss: 0.0026867893150904114\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002204309164753391\n",
      "Average test loss: 0.0025215989291254015\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002213492672683464\n",
      "Average test loss: 0.0025223599686804744\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002191313520177371\n",
      "Average test loss: 0.0024791922056012685\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0021893425949124825\n",
      "Average test loss: 0.0024993913740747505\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0021742883721987405\n",
      "Average test loss: 0.0025018269564542504\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0022170622436743642\n",
      "Average test loss: 0.002497235165288051\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002146987876751357\n",
      "Average test loss: 0.0026228515536834795\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0021327329269713825\n",
      "Average test loss: 0.002521565519687202\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0021285818660010895\n",
      "Average test loss: 0.012412042655878596\n",
      "Epoch 55/300\n",
      "Average training loss: 0.004886871184739802\n",
      "Average test loss: 0.0032426324772338072\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0028600249343241255\n",
      "Average test loss: 0.0028133960785344243\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00265070541927384\n",
      "Average test loss: 0.002705794731155038\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002518695600951711\n",
      "Average test loss: 0.002580821079512437\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0024301470056590107\n",
      "Average test loss: 0.0027548106590078937\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0023593958930836784\n",
      "Average test loss: 0.0025956559526837536\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002297611340569953\n",
      "Average test loss: 0.002494794748723507\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002241967824908594\n",
      "Average test loss: 0.0025480193636483617\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0021921742875128983\n",
      "Average test loss: 0.002487244087892274\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0021665224342917403\n",
      "Average test loss: 0.0026404934424079127\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0021565209487453105\n",
      "Average test loss: 0.002502815638151434\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0021375294367058408\n",
      "Average test loss: 0.002553452412908276\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002134025206582414\n",
      "Average test loss: 0.0025430635213851928\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0021241684976137346\n",
      "Average test loss: 0.0025572609760695034\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0021285724988621144\n",
      "Average test loss: 0.002498785208703743\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0021060851604367296\n",
      "Average test loss: 0.0024793615916536916\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0021027042941293783\n",
      "Average test loss: 0.0025027706165694527\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0021135943554755716\n",
      "Average test loss: 0.002579160768124792\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00209628635748393\n",
      "Average test loss: 0.0028280126524882184\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002087386469874117\n",
      "Average test loss: 0.00248781710718241\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0020690049422490927\n",
      "Average test loss: 0.0025061214247511492\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0020576681782388025\n",
      "Average test loss: 0.0025125620199574366\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002064818492780129\n",
      "Average test loss: 0.00249620859304236\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0020481587851213083\n",
      "Average test loss: 0.0025072880532178614\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0020782029036846427\n",
      "Average test loss: 0.002607637458998296\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002051489984099236\n",
      "Average test loss: 0.0027220380867107046\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002040383838738004\n",
      "Average test loss: 0.002561052571154303\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0020364892912200757\n",
      "Average test loss: 0.002452967985843619\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00202095736666686\n",
      "Average test loss: 0.002718044111608631\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002017159022908244\n",
      "Average test loss: 0.002495901165323125\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002010803105102645\n",
      "Average test loss: 0.002551140167439977\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002005305724321968\n",
      "Average test loss: 0.0026207422477503616\n",
      "Epoch 87/300\n",
      "Average training loss: 0.002008214310432474\n",
      "Average test loss: 0.0025577252147098383\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002009751139001714\n",
      "Average test loss: 0.0024332393002178933\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00200152297069629\n",
      "Average test loss: 0.002557545618257589\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0019875115588721304\n",
      "Average test loss: 0.0028921439902236065\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0019891105500153368\n",
      "Average test loss: 0.0025324137475755478\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0019955990007147193\n",
      "Average test loss: 0.0024815936150650183\n",
      "Epoch 93/300\n",
      "Average training loss: 0.001986280749965873\n",
      "Average test loss: 0.0025566654396760795\n",
      "Epoch 94/300\n",
      "Average training loss: 0.001971345497812662\n",
      "Average test loss: 0.0024838715015600126\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00197230034114586\n",
      "Average test loss: 0.0025469494186755683\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0019551259039176836\n",
      "Average test loss: 0.0025055805175668664\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0019502550716408425\n",
      "Average test loss: 0.002518648248165846\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0019522888521767325\n",
      "Average test loss: 0.0025017030926214323\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0019616924294581016\n",
      "Average test loss: 0.002463981348917716\n",
      "Epoch 100/300\n",
      "Average training loss: 0.001953827469713158\n",
      "Average test loss: 0.0025020037297573353\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0020016236544276277\n",
      "Average test loss: 0.002782424161521097\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0019645348973572254\n",
      "Average test loss: 0.0024486753794675073\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0019195563620370295\n",
      "Average test loss: 0.002507724430722495\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0019266457181009982\n",
      "Average test loss: 0.0024715193828774825\n",
      "Epoch 105/300\n",
      "Average training loss: 0.001926137403998938\n",
      "Average test loss: 0.0025616523406157892\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0019308601187335121\n",
      "Average test loss: 0.0025393717887087002\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0019271910507231951\n",
      "Average test loss: 0.0024906414755516584\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0019266897403738565\n",
      "Average test loss: 0.0025153204119867747\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0019200335631353988\n",
      "Average test loss: 0.0025776225098719198\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0019165147892716858\n",
      "Average test loss: 0.0025089996645434034\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0019070711484592822\n",
      "Average test loss: 0.00247046305756602\n",
      "Epoch 112/300\n",
      "Average training loss: 0.001901212131914993\n",
      "Average test loss: 0.002560208730606569\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0019052745228012402\n",
      "Average test loss: 0.0025040368390166095\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0019133467373127738\n",
      "Average test loss: 0.0028556272890418767\n",
      "Epoch 115/300\n",
      "Average training loss: 0.001893247120289339\n",
      "Average test loss: 0.002807072293634216\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0019001442338857386\n",
      "Average test loss: 0.0025227362180335654\n",
      "Epoch 117/300\n",
      "Average training loss: 0.001894334893570178\n",
      "Average test loss: 0.0026918331273934905\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0018930969105826485\n",
      "Average test loss: 0.002520257206219766\n",
      "Epoch 119/300\n",
      "Average training loss: 0.001890260830625064\n",
      "Average test loss: 0.002900782094647487\n",
      "Epoch 120/300\n",
      "Average training loss: 0.001885623743964566\n",
      "Average test loss: 0.0025599836260080337\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0018841930075238148\n",
      "Average test loss: 0.0025365926186657613\n",
      "Epoch 122/300\n",
      "Average training loss: 0.001879706835374236\n",
      "Average test loss: 0.0027494790537489783\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0018739917013380263\n",
      "Average test loss: 0.0025135658904392685\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0019021752786098254\n",
      "Average test loss: 0.002449020684800214\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0018609983153227304\n",
      "Average test loss: 0.0024767588577750657\n",
      "Epoch 126/300\n",
      "Average training loss: 0.001865614796574745\n",
      "Average test loss: 0.0024811931718140843\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0018651274165345563\n",
      "Average test loss: 0.0025135564849608476\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0018618499977100227\n",
      "Average test loss: 0.002568265546941095\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0018623860036540362\n",
      "Average test loss: 0.002705746726857291\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0018605471324796479\n",
      "Average test loss: 0.002684648214942879\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0018536360600135394\n",
      "Average test loss: 0.002512421629909012\n",
      "Epoch 132/300\n",
      "Average training loss: 0.001852157605604993\n",
      "Average test loss: 0.0024887909375958975\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0018503094306215643\n",
      "Average test loss: 0.0026745665924002727\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0018500479481493434\n",
      "Average test loss: 0.002492380255833268\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0018509373291292124\n",
      "Average test loss: 0.002614441511945592\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0018471729891995589\n",
      "Average test loss: 0.0026116204503923656\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018566269110888243\n",
      "Average test loss: 0.0025081512476834984\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0018491822651897868\n",
      "Average test loss: 0.002526237376448181\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0018314340848268734\n",
      "Average test loss: 0.0025681922785523864\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0018305585241566102\n",
      "Average test loss: 0.002588049180391762\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0018259615906410748\n",
      "Average test loss: 0.0025580236808293395\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0018318968170012036\n",
      "Average test loss: 0.0026311258065203824\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0018284773848329982\n",
      "Average test loss: 0.0027134600784629584\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001826241071232491\n",
      "Average test loss: 0.0025182179705136353\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0018339568491404256\n",
      "Average test loss: 0.0025038613254825272\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0018291569758827487\n",
      "Average test loss: 0.0026194922940598595\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0018241634140205051\n",
      "Average test loss: 0.0025544263809505435\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0018181923350526227\n",
      "Average test loss: 0.0026381827803949516\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0018207002732281883\n",
      "Average test loss: 0.0027012115325778722\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018193480292749073\n",
      "Average test loss: 0.0025294930204335184\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001813651318144467\n",
      "Average test loss: 0.002624138310137722\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0018115887897503045\n",
      "Average test loss: 0.002584196589473221\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0018152196751907467\n",
      "Average test loss: 0.0026948467513753308\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0018077618059598737\n",
      "Average test loss: 0.00252761779146062\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0018097768190006415\n",
      "Average test loss: 0.0024915923240284125\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0018083986894538006\n",
      "Average test loss: 0.002520856851297948\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0018059593199027909\n",
      "Average test loss: 0.002504640731960535\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0018005046860004464\n",
      "Average test loss: 0.00261733092698786\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0018906921601543824\n",
      "Average test loss: 0.0024971846994012595\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0017865387353425224\n",
      "Average test loss: 0.0025750413528747027\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018031843965873122\n",
      "Average test loss: 0.0025167273295422397\n",
      "Epoch 162/300\n",
      "Average training loss: 0.001794578122285505\n",
      "Average test loss: 0.0025471369456499817\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001788612576511999\n",
      "Average test loss: 0.005832468455450403\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0017911873097634977\n",
      "Average test loss: 0.0025838752749065557\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001792142175655398\n",
      "Average test loss: 0.0025410338735414877\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0017893968253499932\n",
      "Average test loss: 0.002536290648496813\n",
      "Epoch 167/300\n",
      "Average training loss: 0.001786409753892157\n",
      "Average test loss: 0.002582423146503667\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0017889638592799505\n",
      "Average test loss: 0.0026792721740073627\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00179855691589829\n",
      "Average test loss: 0.0028102437706871166\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0017969055058848526\n",
      "Average test loss: 0.002576027172928055\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0017768393346211976\n",
      "Average test loss: 0.0025292429260702597\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0017772700955263443\n",
      "Average test loss: 0.0025867466578880947\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001780978914246791\n",
      "Average test loss: 0.0024952489189389677\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0017799700687949856\n",
      "Average test loss: 0.002557677639855279\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0017755025966713826\n",
      "Average test loss: 0.0026383020118292836\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0017758972038411432\n",
      "Average test loss: 0.002546553034335375\n",
      "Epoch 177/300\n",
      "Average training loss: 0.001775523292211195\n",
      "Average test loss: 0.002555455117693378\n",
      "Epoch 178/300\n",
      "Average training loss: 0.001771629468537867\n",
      "Average test loss: 0.0025597575530409812\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0017772470857534144\n",
      "Average test loss: 0.002645417617426978\n",
      "Epoch 180/300\n",
      "Average training loss: 0.001772218969857527\n",
      "Average test loss: 0.0025537565323627658\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001764335683753921\n",
      "Average test loss: 0.0027956444836325115\n",
      "Epoch 182/300\n",
      "Average training loss: 0.001761898012417886\n",
      "Average test loss: 0.002716162830591202\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0017664646149302523\n",
      "Average test loss: 0.002524867967598968\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001763246207911935\n",
      "Average test loss: 0.0026026938133355644\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0017605898356479075\n",
      "Average test loss: 0.002698297798840536\n",
      "Epoch 186/300\n",
      "Average training loss: 0.001753985672361321\n",
      "Average test loss: 0.0025878121939798197\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0017555634019275506\n",
      "Average test loss: 0.0025292531568557024\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0017652910592862302\n",
      "Average test loss: 0.0028546452240811454\n",
      "Epoch 189/300\n",
      "Average training loss: 0.001763815337067677\n",
      "Average test loss: 0.002577555656950507\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0017597101630849972\n",
      "Average test loss: 0.0025437643786685336\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0017520992714497778\n",
      "Average test loss: 0.002561029395916396\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0017522490071132778\n",
      "Average test loss: 0.002549867779844337\n",
      "Epoch 193/300\n",
      "Average training loss: 0.001753018656331632\n",
      "Average test loss: 0.002535862063989043\n",
      "Epoch 194/300\n",
      "Average training loss: 0.001759624056207637\n",
      "Average test loss: 0.0025539981263379254\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0017560723852366208\n",
      "Average test loss: 0.0025894900663859313\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0017477504498221808\n",
      "Average test loss: 0.00263872353773978\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0017464920483115647\n",
      "Average test loss: 0.0026239428019358053\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0017470384363291991\n",
      "Average test loss: 0.002566994541221195\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0017412442706732287\n",
      "Average test loss: 0.030646926399734285\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0017726757959980104\n",
      "Average test loss: 0.0026267408937629727\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0017455825148564245\n",
      "Average test loss: 0.0026612409512615865\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0017403118319602477\n",
      "Average test loss: 0.002641637650421924\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0017373133875015709\n",
      "Average test loss: 0.002549682388496068\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0017424977370020416\n",
      "Average test loss: 0.002599594057434135\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0017405106759526663\n",
      "Average test loss: 0.0025805118495805398\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0017407352609766853\n",
      "Average test loss: 0.002551519412857791\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0017356531599329577\n",
      "Average test loss: 0.0027301658153947857\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0017365742583448688\n",
      "Average test loss: 0.0025376415857010417\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0017318817124598557\n",
      "Average test loss: 0.0025409505835009946\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0017325909613735146\n",
      "Average test loss: 0.002595935727780064\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0017281399204706152\n",
      "Average test loss: 0.0026621947940438984\n",
      "Epoch 212/300\n",
      "Average training loss: 0.001730809770627982\n",
      "Average test loss: 0.002529499820743998\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001737271834268338\n",
      "Average test loss: 0.0025500061329868104\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0017320134831178518\n",
      "Average test loss: 0.0026879545220484338\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001734487181632883\n",
      "Average test loss: 0.0025244745278937947\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001727474132552743\n",
      "Average test loss: 0.0025869267765018674\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0017229373137363129\n",
      "Average test loss: 0.0026367235593497754\n",
      "Epoch 218/300\n",
      "Average training loss: 0.001720246163300342\n",
      "Average test loss: 0.0025767702110525634\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0017200449547833867\n",
      "Average test loss: 0.0026959804805616536\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0017226890221031176\n",
      "Average test loss: 0.0025904457622932062\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0017204025485035447\n",
      "Average test loss: 0.002748338259756565\n",
      "Epoch 222/300\n",
      "Average training loss: 0.001720614497601572\n",
      "Average test loss: 0.002732303958800104\n",
      "Epoch 223/300\n",
      "Average training loss: 0.001721643779012892\n",
      "Average test loss: 0.00254498651665118\n",
      "Epoch 224/300\n",
      "Average training loss: 0.001720717834826145\n",
      "Average test loss: 0.002610016340803769\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0017196530521743827\n",
      "Average test loss: 0.0025911285024550227\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00171233162501206\n",
      "Average test loss: 0.002600078216029538\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0017150209341198206\n",
      "Average test loss: 0.002529971663322714\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0017161002156014244\n",
      "Average test loss: 0.002631978495667378\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0017103919755253527\n",
      "Average test loss: 0.002668590753649672\n",
      "Epoch 230/300\n",
      "Average training loss: 0.001713302706885669\n",
      "Average test loss: 0.0026185172705186738\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0017074273264863424\n",
      "Average test loss: 0.0025667067911061977\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0017145706434837647\n",
      "Average test loss: 0.0025436692556573283\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0017147358538996843\n",
      "Average test loss: 0.002611705412984722\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0017067896937951446\n",
      "Average test loss: 0.002597425735452109\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0017100666945593225\n",
      "Average test loss: 0.002556117258138127\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0017124540369129844\n",
      "Average test loss: 0.002643362489839395\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0017024703015469842\n",
      "Average test loss: 0.002660529144729177\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0017073704754519794\n",
      "Average test loss: 0.002990701904623873\n",
      "Epoch 239/300\n",
      "Average training loss: 0.001702436309411294\n",
      "Average test loss: 0.0026317507007883657\n",
      "Epoch 240/300\n",
      "Average training loss: 0.001696594404315369\n",
      "Average test loss: 0.002730817214585841\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0017047212147671316\n",
      "Average test loss: 0.0025421012029465703\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0017034882016273008\n",
      "Average test loss: 0.0026264120139595534\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0017087584131707748\n",
      "Average test loss: 0.0026180788562115698\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0016971964615190194\n",
      "Average test loss: 0.002618199860263202\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0017032847855654029\n",
      "Average test loss: 0.0026271239524293276\n",
      "Epoch 246/300\n",
      "Average training loss: 0.001699806480978926\n",
      "Average test loss: 0.00263824705613984\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0017030853711896472\n",
      "Average test loss: 0.0030588261340227393\n",
      "Epoch 248/300\n",
      "Average training loss: 0.001695399973438018\n",
      "Average test loss: 0.0026680754814296962\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0016973547961355911\n",
      "Average test loss: 0.0026129358096255197\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0016987496604108148\n",
      "Average test loss: 0.0026682450227025483\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0016881383592262864\n",
      "Average test loss: 0.0027268584836274384\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0016928009090738164\n",
      "Average test loss: 0.002608229918612374\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0016891246090332667\n",
      "Average test loss: 0.002624386091923548\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0016992543298337195\n",
      "Average test loss: 0.0025972485763745175\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0016931602966247334\n",
      "Average test loss: 0.002562292059779995\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0016894843682853712\n",
      "Average test loss: 0.0026213682295961513\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0016980834047620496\n",
      "Average test loss: 0.0026097734382169112\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0016909590562184651\n",
      "Average test loss: 0.0026311536326797474\n",
      "Epoch 259/300\n",
      "Average training loss: 0.001682165191301869\n",
      "Average test loss: 0.002578633608710435\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0016869488916256361\n",
      "Average test loss: 0.0025967375833748116\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0016859297822746965\n",
      "Average test loss: 0.0026674846518370842\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0016912098731845617\n",
      "Average test loss: 0.002618004072457552\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0016943339925880234\n",
      "Average test loss: 0.0025593003672030237\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0016860494430487355\n",
      "Average test loss: 0.0026050783892472584\n",
      "Epoch 265/300\n",
      "Average training loss: 0.001683615729212761\n",
      "Average test loss: 0.0026009467986101904\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0016887433970760968\n",
      "Average test loss: 0.002608602593756384\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0016833103862073685\n",
      "Average test loss: 0.0026953185666352512\n",
      "Epoch 268/300\n",
      "Average training loss: 0.001681031365154518\n",
      "Average test loss: 0.0026486903785003556\n",
      "Epoch 269/300\n",
      "Average training loss: 0.001681562688615587\n",
      "Average test loss: 0.002591448406378428\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0016808726430560153\n",
      "Average test loss: 0.002664782416178948\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0016736581421767673\n",
      "Average test loss: 0.002695388013496995\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0016717567213086618\n",
      "Average test loss: 0.002610888660574953\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0016760190464556216\n",
      "Average test loss: 0.0027302805797921287\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0016775798828651508\n",
      "Average test loss: 0.0026407171982443995\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0016775323562324047\n",
      "Average test loss: 0.002573765978010164\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0016720849826104111\n",
      "Average test loss: 0.002658238359209564\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0016779148288898998\n",
      "Average test loss: 0.00269128532645603\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0016771009013884597\n",
      "Average test loss: 0.0026748488133566247\n",
      "Epoch 279/300\n",
      "Average training loss: 0.001678695646321608\n",
      "Average test loss: 0.002690954675897956\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0016686925316850344\n",
      "Average test loss: 0.0026558158138973846\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0016721877469163802\n",
      "Average test loss: 0.0026236473568197753\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0016715284338634875\n",
      "Average test loss: 0.002981277857389715\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0016711389391372601\n",
      "Average test loss: 0.0026458130180835723\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0016705907777779631\n",
      "Average test loss: 0.0026183139484168756\n",
      "Epoch 285/300\n",
      "Average training loss: 0.001668474059138033\n",
      "Average test loss: 0.0025994622049232323\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0016773926286647717\n",
      "Average test loss: 0.0026569979095624552\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0016669291820791033\n",
      "Average test loss: 0.0025823738259366817\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0016628580153402355\n",
      "Average test loss: 0.0026212025951180195\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0016670325304278069\n",
      "Average test loss: 0.002688482908325063\n",
      "Epoch 290/300\n",
      "Average training loss: 0.001670009160724779\n",
      "Average test loss: 0.0026419140135662423\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0016667783948489362\n",
      "Average test loss: 0.002604518171813753\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0016573666873284513\n",
      "Average test loss: 0.0025984058524999355\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0016602082069342335\n",
      "Average test loss: 0.0026320824529975652\n",
      "Epoch 294/300\n",
      "Average training loss: 0.001661660262900922\n",
      "Average test loss: 0.002683922872775131\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0016607357028664814\n",
      "Average test loss: 0.0026686473157670762\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0016645341196821796\n",
      "Average test loss: 0.0026465814345412782\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0016599284530513815\n",
      "Average test loss: 0.0026373673491179943\n",
      "Epoch 298/300\n",
      "Average training loss: 0.001658073958216442\n",
      "Average test loss: 0.0026418902035802603\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0016605417519393894\n",
      "Average test loss: 0.002833755897771981\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0016549496091902256\n",
      "Average test loss: 0.00259199760398931\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.019500331895218954\n",
      "Average test loss: 0.006033285838034418\n",
      "Epoch 2/300\n",
      "Average training loss: 0.005087847986982928\n",
      "Average test loss: 0.004708055006547107\n",
      "Epoch 3/300\n",
      "Average training loss: 0.004282226533939441\n",
      "Average test loss: 0.004168195752840903\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0038332483977493313\n",
      "Average test loss: 0.003992909349087212\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0035251601052780947\n",
      "Average test loss: 0.003352255834059583\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0032898485987550683\n",
      "Average test loss: 0.0031800156777931584\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0031160737636188665\n",
      "Average test loss: 0.0030842969850119616\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0029478997062477802\n",
      "Average test loss: 0.002856153351151281\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0027979303625308804\n",
      "Average test loss: 0.0027408638791077667\n",
      "Epoch 10/300\n",
      "Average training loss: 0.002683738958504465\n",
      "Average test loss: 0.0027358569376584557\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0025630578812625674\n",
      "Average test loss: 0.002483010806557205\n",
      "Epoch 12/300\n",
      "Average training loss: 0.002490116965646545\n",
      "Average test loss: 0.0027481391336768865\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0023886163648631836\n",
      "Average test loss: 0.002339724602177739\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002318814302277234\n",
      "Average test loss: 0.0023557277541193695\n",
      "Epoch 15/300\n",
      "Average training loss: 0.002227703580115404\n",
      "Average test loss: 0.0022177544271366465\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00218119914105369\n",
      "Average test loss: 0.0022521242732182144\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0021194687652298146\n",
      "Average test loss: 0.002164676145857407\n",
      "Epoch 18/300\n",
      "Average training loss: 0.002069447472484575\n",
      "Average test loss: 0.0021534353631238144\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0020270703727793362\n",
      "Average test loss: 0.0021444400186753933\n",
      "Epoch 20/300\n",
      "Average training loss: 0.001988927477970719\n",
      "Average test loss: 0.002065705536864698\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0019542382129778464\n",
      "Average test loss: 0.0020463967971089815\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0019186320004777775\n",
      "Average test loss: 0.0020194447812520795\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0018808146656180422\n",
      "Average test loss: 0.0019531913756703338\n",
      "Epoch 24/300\n",
      "Average training loss: 0.001864656755183306\n",
      "Average test loss: 0.002029975247879823\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0018342647140638696\n",
      "Average test loss: 0.0019066191549516386\n",
      "Epoch 26/300\n",
      "Average training loss: 0.001805706749152806\n",
      "Average test loss: 0.0019340137917962339\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0017876088672007123\n",
      "Average test loss: 0.0019821385310755837\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0017748483520311615\n",
      "Average test loss: 0.001990379035265909\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0017845837478008535\n",
      "Average test loss: 0.0018741412175198396\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0017559390337102943\n",
      "Average test loss: 0.0018608226796819104\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0017535018102369374\n",
      "Average test loss: 0.0018113011193151276\n",
      "Epoch 32/300\n",
      "Average training loss: 0.001709026776978539\n",
      "Average test loss: 0.001870727746643954\n",
      "Epoch 33/300\n",
      "Average training loss: 0.001703291696910229\n",
      "Average test loss: 0.001813955810955829\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0016806043645160066\n",
      "Average test loss: 0.0018624818933506806\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0016802586765132016\n",
      "Average test loss: 0.0018502796195033523\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0016723000415497357\n",
      "Average test loss: 0.001793103201728728\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0016436486187287502\n",
      "Average test loss: 0.001823549850222965\n",
      "Epoch 38/300\n",
      "Average training loss: 0.001642410763228933\n",
      "Average test loss: 0.0017936727065179083\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0016306795900066693\n",
      "Average test loss: 0.0017916239046802123\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0016256294545407097\n",
      "Average test loss: 0.0017805031204803123\n",
      "Epoch 41/300\n",
      "Average training loss: 0.001615505394525826\n",
      "Average test loss: 0.0017515939547576838\n",
      "Epoch 42/300\n",
      "Average training loss: 0.001605149099085894\n",
      "Average test loss: 0.001800874708634284\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0015994892061377565\n",
      "Average test loss: 0.0017736401354066199\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0015993676260113716\n",
      "Average test loss: 0.0017461790156861146\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00158777396618906\n",
      "Average test loss: 0.001812189115728769\n",
      "Epoch 46/300\n",
      "Average training loss: 0.001582595678149826\n",
      "Average test loss: 0.0018073045568954614\n",
      "Epoch 47/300\n",
      "Average training loss: 0.001574768468323681\n",
      "Average test loss: 0.0017626225251911416\n",
      "Epoch 48/300\n",
      "Average training loss: 0.001592649492331677\n",
      "Average test loss: 0.001766267774419652\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0015675829054994715\n",
      "Average test loss: 0.001753342930537959\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0015453323209658266\n",
      "Average test loss: 0.001739816319818298\n",
      "Epoch 51/300\n",
      "Average training loss: 0.001544646921257178\n",
      "Average test loss: 0.0017616131258093649\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0015784583313183651\n",
      "Average test loss: 0.0017505076231641903\n",
      "Epoch 53/300\n",
      "Average training loss: 0.001527604944486585\n",
      "Average test loss: 0.0017261794393675196\n",
      "Epoch 54/300\n",
      "Average training loss: 0.001529018008771042\n",
      "Average test loss: 0.001793364730042716\n",
      "Epoch 55/300\n",
      "Average training loss: 0.001527819157578051\n",
      "Average test loss: 0.0018395807597165307\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0015253649522653885\n",
      "Average test loss: 0.0017747097083677848\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0015297257903342447\n",
      "Average test loss: 0.0017329658085687293\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0015113367468325629\n",
      "Average test loss: 0.001762193221391903\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0015192033645386497\n",
      "Average test loss: 0.0017281135686983665\n",
      "Epoch 60/300\n",
      "Average training loss: 0.00150106626138505\n",
      "Average test loss: 0.0019184963107109069\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0015210552276629541\n",
      "Average test loss: 0.0017606941161470281\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0014886629536954893\n",
      "Average test loss: 0.0018412150173551507\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0014850670238956809\n",
      "Average test loss: 0.0020554029741841886\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005335682830773294\n",
      "Average test loss: 0.0056524375358389485\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0032057192113457453\n",
      "Average test loss: 0.002294282770612174\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002111478704545233\n",
      "Average test loss: 0.0020007270601474576\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0018935494324606326\n",
      "Average test loss: 0.0019162222215284904\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0017904373004825579\n",
      "Average test loss: 0.0018341656722542312\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0017211929235814347\n",
      "Average test loss: 0.001794514574110508\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0016687365756887528\n",
      "Average test loss: 0.0017570757396105263\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0016253521906005011\n",
      "Average test loss: 0.0017641312935286098\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0015857159350481298\n",
      "Average test loss: 0.0017572002399505841\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0015518175391480327\n",
      "Average test loss: 0.0017595886407523519\n",
      "Epoch 74/300\n",
      "Average training loss: 0.001525199486149682\n",
      "Average test loss: 0.0018134790163280236\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0015025760016093652\n",
      "Average test loss: 0.001774463972904616\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0014900314967251486\n",
      "Average test loss: 0.001735246300386886\n",
      "Epoch 77/300\n",
      "Average training loss: 0.001489195273257792\n",
      "Average test loss: 0.001763046571881407\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0014829972801833518\n",
      "Average test loss: 0.0017398617424898676\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0014846856916944187\n",
      "Average test loss: 0.0017403981513861152\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0014790534472299947\n",
      "Average test loss: 0.0017833382383816772\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0014844594570911592\n",
      "Average test loss: 0.0017696236841794517\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0014740958019263215\n",
      "Average test loss: 0.00172946234088805\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0014627613592892886\n",
      "Average test loss: 0.0017635529970543252\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0014645721731293532\n",
      "Average test loss: 0.0017862048676858346\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0014610476655264695\n",
      "Average test loss: 0.001729501967318356\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0014802958568972018\n",
      "Average test loss: 0.0017422708499555785\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0014513546647503973\n",
      "Average test loss: 0.0019808634991447132\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0014507056845145094\n",
      "Average test loss: 0.001717018098053005\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0014470036138469974\n",
      "Average test loss: 0.001796346725896001\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0014454305383066336\n",
      "Average test loss: 0.002419893254008558\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0014419692954462436\n",
      "Average test loss: 0.0017604822491606076\n",
      "Epoch 92/300\n",
      "Average training loss: 0.001440529268545409\n",
      "Average test loss: 0.0017606021122386058\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0014500507991760969\n",
      "Average test loss: 0.0017152209354357587\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0014365906881996327\n",
      "Average test loss: 0.0017577151156341036\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0014402463690688214\n",
      "Average test loss: 0.0017932495228532288\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0014242239494083656\n",
      "Average test loss: 0.001818787292887767\n",
      "Epoch 97/300\n",
      "Average training loss: 0.001427647389471531\n",
      "Average test loss: 0.001769888694708546\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0014242855142802\n",
      "Average test loss: 0.001824641002342105\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0014213433252233598\n",
      "Average test loss: 0.0018245978648256925\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0014166253281032873\n",
      "Average test loss: 0.001762074067360825\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0014427375005972053\n",
      "Average test loss: 0.0017273563361830183\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0014050702437137564\n",
      "Average test loss: 0.0017784265988609858\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0014096243950641818\n",
      "Average test loss: 0.0017669635127402015\n",
      "Epoch 104/300\n",
      "Average training loss: 0.001402681728462792\n",
      "Average test loss: 0.0017762404254948099\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0014065818693488835\n",
      "Average test loss: 0.0017270013588584132\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0014130270707731446\n",
      "Average test loss: 0.0017464690235339934\n",
      "Epoch 107/300\n",
      "Average training loss: 0.001398661087991463\n",
      "Average test loss: 0.0017352264710805481\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0013952773105767038\n",
      "Average test loss: 0.0017473009690228435\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00139633314901342\n",
      "Average test loss: 0.001724218398746517\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0013922145444796316\n",
      "Average test loss: 0.003846925299304227\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0013915030142913262\n",
      "Average test loss: 0.0018547206673150262\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0013869136249025664\n",
      "Average test loss: 0.0018274130472499463\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0013897819766360852\n",
      "Average test loss: 0.0017637027758691047\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0013863808140158653\n",
      "Average test loss: 0.0017725711858106983\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0013802267488920027\n",
      "Average test loss: 0.0017475589577936464\n",
      "Epoch 116/300\n",
      "Average training loss: 0.001378773741838005\n",
      "Average test loss: 0.0018633487601247098\n",
      "Epoch 117/300\n",
      "Average training loss: 0.001381482473678059\n",
      "Average test loss: 0.0017980538158574037\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0013780454895976518\n",
      "Average test loss: 0.0017633480313751432\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0013798949360433552\n",
      "Average test loss: 0.0017679455845306317\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0013652816590749555\n",
      "Average test loss: 0.0017370361609177457\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0013665703184281787\n",
      "Average test loss: 0.0018006237541428871\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0013739333330757089\n",
      "Average test loss: 0.0023451658105477693\n",
      "Epoch 123/300\n",
      "Average training loss: 0.001390560153302633\n",
      "Average test loss: 0.001802568063346876\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0013670381409012608\n",
      "Average test loss: 0.0017387998097886641\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0013564311924080055\n",
      "Average test loss: 0.0017679012208763095\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0013559943518290918\n",
      "Average test loss: 0.001753197426493797\n",
      "Epoch 127/300\n",
      "Average training loss: 0.001359641185754703\n",
      "Average test loss: 0.0017729041357007291\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0013552078453616964\n",
      "Average test loss: 0.0017976998741635018\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0014222916572665175\n",
      "Average test loss: 0.0017955111283808947\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0013465904085379508\n",
      "Average test loss: 0.0017781847313874297\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0013439496556400426\n",
      "Average test loss: 0.001772974198891057\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0013486951980739832\n",
      "Average test loss: 0.0017979184332200223\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0013492104533231921\n",
      "Average test loss: 0.0017527890460979608\n",
      "Epoch 134/300\n",
      "Average training loss: 0.001346770858909521\n",
      "Average test loss: 0.0017697952146538429\n",
      "Epoch 135/300\n",
      "Average training loss: 0.001365095204466747\n",
      "Average test loss: 0.0018733748748070663\n",
      "Epoch 136/300\n",
      "Average training loss: 0.001335958265285525\n",
      "Average test loss: 0.001836110081213216\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0013377650536389814\n",
      "Average test loss: 0.001796341872877545\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0013428488255788882\n",
      "Average test loss: 0.0017677480013420185\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0013408063999894592\n",
      "Average test loss: 0.0018664819719269872\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0013429149120218224\n",
      "Average test loss: 0.0018385922524871098\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0013358047240310245\n",
      "Average test loss: 0.0017345542804234559\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0013420290548561348\n",
      "Average test loss: 0.001821998235459129\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0013663012457804547\n",
      "Average test loss: 0.0017432859628978702\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0013342815871453947\n",
      "Average test loss: 0.0017349689102007283\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0013252381074966655\n",
      "Average test loss: 0.0017540423769710793\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0013290963830012415\n",
      "Average test loss: 0.0017967536085181767\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0013296946701076295\n",
      "Average test loss: 0.0017627454532517328\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0013342524405775798\n",
      "Average test loss: 0.0017453578087604708\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0013269660843329297\n",
      "Average test loss: 0.0017526978939357732\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0013225657147251898\n",
      "Average test loss: 0.0018083249932775896\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0013222541700427732\n",
      "Average test loss: 0.0017772734521163834\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0013227958131788505\n",
      "Average test loss: 0.001747689572473367\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001324510980087022\n",
      "Average test loss: 0.0017484295137433542\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0013153245762611428\n",
      "Average test loss: 0.0017607775962808067\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0013140379910667737\n",
      "Average test loss: 0.0019662721497524117\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0013194301707877053\n",
      "Average test loss: 0.0018154779982028737\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0013185465110258923\n",
      "Average test loss: 0.0018136499176422754\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001323641349768473\n",
      "Average test loss: 0.0018447712796429794\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00130766918660245\n",
      "Average test loss: 0.0018202943423142035\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0013242632639076976\n",
      "Average test loss: 0.0018230162672698497\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0013119367674613992\n",
      "Average test loss: 0.0017856914245833954\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0013064685608777736\n",
      "Average test loss: 0.0017708950186562206\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00130603537387732\n",
      "Average test loss: 0.0018021491741140683\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0013051781535355581\n",
      "Average test loss: 0.0018039061026130286\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0013022139730035432\n",
      "Average test loss: 0.001804797997077306\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0013134552757255733\n",
      "Average test loss: 0.0018496326016676095\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0013040048761500252\n",
      "Average test loss: 0.0017576143466350104\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0013097139559686184\n",
      "Average test loss: 0.0018456658677508434\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0013115605396322079\n",
      "Average test loss: 0.0017726660050037834\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0012939739035856392\n",
      "Average test loss: 0.0018261997233041459\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0012991143516782257\n",
      "Average test loss: 0.0017692811101054152\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0012950959658871095\n",
      "Average test loss: 0.0018150821512358055\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0012953167540124722\n",
      "Average test loss: 0.0018594440788858467\n",
      "Epoch 174/300\n",
      "Average training loss: 0.001292999186553061\n",
      "Average test loss: 0.0018565508727398183\n",
      "Epoch 175/300\n",
      "Average training loss: 0.001289941951011618\n",
      "Average test loss: 0.0018225689308924808\n",
      "Epoch 176/300\n",
      "Average training loss: 0.001292642087675631\n",
      "Average test loss: 0.0017722120114291708\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0012952696812442608\n",
      "Average test loss: 0.0017983547028981976\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0012929498363389736\n",
      "Average test loss: 0.0018481067901270256\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0012909170690303048\n",
      "Average test loss: 0.0018077689549989171\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0012876749506427181\n",
      "Average test loss: 0.0017736766808148887\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0012902954345982937\n",
      "Average test loss: 0.0018219819394871592\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0012861717503724827\n",
      "Average test loss: 0.0018174762999018034\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0012833203051446213\n",
      "Average test loss: 0.0017800720954934755\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0012854324008027712\n",
      "Average test loss: 0.0018489612797275186\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0012842518325067228\n",
      "Average test loss: 0.001978623517271545\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0012865173214425642\n",
      "Average test loss: 0.0018166944939229223\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0012808810575968689\n",
      "Average test loss: 0.0017934460409192575\n",
      "Epoch 188/300\n",
      "Average training loss: 0.001279798868836628\n",
      "Average test loss: 0.0018028923790487977\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0012823781152773235\n",
      "Average test loss: 0.0017992968834522698\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0012812792861627209\n",
      "Average test loss: 0.001871050586199595\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0012797505949727363\n",
      "Average test loss: 0.00183304674944116\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0012775885330823561\n",
      "Average test loss: 0.0018630938817643457\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0012744399905204773\n",
      "Average test loss: 0.0017794221453368664\n",
      "Epoch 194/300\n",
      "Average training loss: 0.001277423851398958\n",
      "Average test loss: 0.0018513720904787382\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0012725428046865596\n",
      "Average test loss: 0.0018043170054960582\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0012761599633635747\n",
      "Average test loss: 0.0018089926802656716\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0012780247990869813\n",
      "Average test loss: 0.0018307608019353615\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0012705287730528248\n",
      "Average test loss: 0.0018084333773909344\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0012731914435409838\n",
      "Average test loss: 0.0043420032072398395\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0012712218512574004\n",
      "Average test loss: 0.001907142807626062\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0012706599473539327\n",
      "Average test loss: 0.001833799488325086\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0012834533529563082\n",
      "Average test loss: 0.0018189358806444539\n",
      "Epoch 203/300\n",
      "Average training loss: 0.001264697709845172\n",
      "Average test loss: 0.0018610844862543875\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001260498265011443\n",
      "Average test loss: 0.0018502158605390126\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0012650220149921045\n",
      "Average test loss: 0.0018296898108803564\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0012614221964031458\n",
      "Average test loss: 0.0018446002037574847\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0012696828718504144\n",
      "Average test loss: 0.0018635862540039752\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0012773805444853173\n",
      "Average test loss: 0.0018879871015540428\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0012605472879691256\n",
      "Average test loss: 0.0017789328946835464\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0012589421847628223\n",
      "Average test loss: 0.001884255242637462\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0012604927061539558\n",
      "Average test loss: 0.001869029361133774\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0012614215813163254\n",
      "Average test loss: 0.0018177344279570711\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0012618384861076872\n",
      "Average test loss: 0.0018326911860042149\n",
      "Epoch 214/300\n",
      "Average training loss: 0.001259143228435682\n",
      "Average test loss: 0.0017981693875044585\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0012591372046412692\n",
      "Average test loss: 0.0018269815355953243\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001254541777384778\n",
      "Average test loss: 0.0019006485850032832\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0012577276594109006\n",
      "Average test loss: 0.0018099568288566339\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0012576239350148373\n",
      "Average test loss: 0.0018182916591564814\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0012565023911496004\n",
      "Average test loss: 0.001869935582495398\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0012515198102013932\n",
      "Average test loss: 0.0018899567667394876\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0012549021163334448\n",
      "Average test loss: 0.001842059062172969\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0012523262539050645\n",
      "Average test loss: 0.001852522486820817\n",
      "Epoch 223/300\n",
      "Average training loss: 0.001251296227901346\n",
      "Average test loss: 0.001820760839101341\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0012547241070618232\n",
      "Average test loss: 0.001959059057343337\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0012511872441197435\n",
      "Average test loss: 0.001827482089193331\n",
      "Epoch 226/300\n",
      "Average training loss: 0.001249097891462346\n",
      "Average test loss: 0.0018857779631184207\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0012461219984624121\n",
      "Average test loss: 0.001876948801593648\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001250245843600068\n",
      "Average test loss: 0.0018198289279308584\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0012485353421005938\n",
      "Average test loss: 0.001816362524819043\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0012450162166108688\n",
      "Average test loss: 0.0018927475744858384\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0012459526265867881\n",
      "Average test loss: 0.0018826799378212956\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0012442920420629284\n",
      "Average test loss: 0.0019045739302204715\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0012445137933310535\n",
      "Average test loss: 0.001870502680954006\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0012460643236214917\n",
      "Average test loss: 0.001802988615507881\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0012458980580170949\n",
      "Average test loss: 0.0018663033596757386\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001241779040855666\n",
      "Average test loss: 0.0019107947736564609\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0012406092641047306\n",
      "Average test loss: 0.0019708479662529296\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0012561605158779357\n",
      "Average test loss: 0.001840825750802954\n",
      "Epoch 239/300\n",
      "Average training loss: 0.001233714866037998\n",
      "Average test loss: 0.001851253861354457\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0012426935548169745\n",
      "Average test loss: 0.0018237113583212097\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0012426792746927176\n",
      "Average test loss: 0.0018774743515791164\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0012343008521323402\n",
      "Average test loss: 0.003470956877287891\n",
      "Epoch 243/300\n",
      "Average training loss: 0.001241240247256226\n",
      "Average test loss: 0.0018552965647023584\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0012426604330022302\n",
      "Average test loss: 0.0018873256983028518\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0012350618782349758\n",
      "Average test loss: 0.0018527222778648138\n",
      "Epoch 246/300\n",
      "Average training loss: 0.001236344038715793\n",
      "Average test loss: 0.0018587420765931407\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0012338070118178923\n",
      "Average test loss: 0.0018596620454142492\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0012412889042041368\n",
      "Average test loss: 0.0018229565117508172\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0012585980001216133\n",
      "Average test loss: 0.0017947177043081159\n",
      "Epoch 250/300\n",
      "Average training loss: 0.001230604867450893\n",
      "Average test loss: 0.0018789034457877278\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0012287539929772416\n",
      "Average test loss: 0.0018759528253641394\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0012319358435148994\n",
      "Average test loss: 0.0018310037486048208\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0012328197421092126\n",
      "Average test loss: 0.001877465859055519\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0012342777099046442\n",
      "Average test loss: 0.001856519921682775\n",
      "Epoch 255/300\n",
      "Average training loss: 0.001231707345901264\n",
      "Average test loss: 0.0018251067444475162\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0012407610782732565\n",
      "Average test loss: 0.0018569284974493915\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0012253466352509955\n",
      "Average test loss: 0.0018642800723512967\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0012280023042112588\n",
      "Average test loss: 0.0018941132351756097\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0012320747744912902\n",
      "Average test loss: 0.0018424640987068416\n",
      "Epoch 260/300\n",
      "Average training loss: 0.001226681145425472\n",
      "Average test loss: 0.0018855488964666922\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0012259533955819076\n",
      "Average test loss: 0.0018642034427159362\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0012262761569581926\n",
      "Average test loss: 0.001831051744831105\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0012254408763514625\n",
      "Average test loss: 0.0018380705890142254\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0012242724579862422\n",
      "Average test loss: 0.0018708361716320116\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0012256250640687843\n",
      "Average test loss: 0.0018941633250150415\n",
      "Epoch 266/300\n",
      "Average training loss: 0.001229428512044251\n",
      "Average test loss: 0.0018472946108215385\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0012261394049661856\n",
      "Average test loss: 0.001920879716053605\n",
      "Epoch 268/300\n",
      "Average training loss: 0.001225534603672309\n",
      "Average test loss: 0.0018664186161218418\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0012216561081715756\n",
      "Average test loss: 0.0018303934320703977\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0012220222384979327\n",
      "Average test loss: 0.0018239961670090754\n",
      "Epoch 271/300\n",
      "Average training loss: 0.001222238847364982\n",
      "Average test loss: 0.0018409984519498215\n",
      "Epoch 272/300\n",
      "Average training loss: 0.001223947004829016\n",
      "Average test loss: 0.0018174685222200223\n",
      "Epoch 273/300\n",
      "Average training loss: 0.001223502649511728\n",
      "Average test loss: 0.001832228503500422\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0012176498823488753\n",
      "Average test loss: 0.001898481601331797\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0012463817664732535\n",
      "Average test loss: 0.0018416639542621043\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0012154365952851043\n",
      "Average test loss: 0.002215450560881032\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0012145791377147868\n",
      "Average test loss: 0.0018677305550211006\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0012167785552640756\n",
      "Average test loss: 0.0018745177272810705\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0012147427584148116\n",
      "Average test loss: 0.0018165517337620258\n",
      "Epoch 280/300\n",
      "Average training loss: 0.001218865240406659\n",
      "Average test loss: 0.001858795349486172\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0012183497136251794\n",
      "Average test loss: 0.00207515695794589\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001217238383885059\n",
      "Average test loss: 0.0019362694438960818\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0012404386698164874\n",
      "Average test loss: 0.0018562865938163465\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0012152297991431422\n",
      "Average test loss: 0.001857549029091994\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0012092634219055376\n",
      "Average test loss: 0.0018275176187356314\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0012128563625738024\n",
      "Average test loss: 0.0018168482900493676\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0012124134801431662\n",
      "Average test loss: 0.0018704144834644264\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0012115063229575754\n",
      "Average test loss: 0.0018481533750891684\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0012144468239405088\n",
      "Average test loss: 0.0018094562815709246\n",
      "Epoch 290/300\n",
      "Average training loss: 0.001214196383383953\n",
      "Average test loss: 0.0019140688058816725\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0012180161225712961\n",
      "Average test loss: 0.0018004910966588391\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0012113780339455438\n",
      "Average test loss: 0.0018663688537975153\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0012107528062123391\n",
      "Average test loss: 0.0019069219907331797\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0012113004364073276\n",
      "Average test loss: 0.0018523138900183968\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0012110440249865253\n",
      "Average test loss: 0.001836444997539123\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0012150100182948841\n",
      "Average test loss: 0.0018568702942381302\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0012097457419666979\n",
      "Average test loss: 0.0021660498548299076\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0012112289864776864\n",
      "Average test loss: 0.0018738724152661031\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0012090610888165732\n",
      "Average test loss: 0.001965372594590816\n",
      "Epoch 300/300\n",
      "Average training loss: 0.001206562566674418\n",
      "Average test loss: 0.0018439575860069858\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.01/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 13.85\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 18.33\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.53\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 15.49\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.07\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.29\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.94\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 16.06\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.98\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.01\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.34\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 18.09\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.34\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.84\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.08226004752020041\n",
      "Average test loss: 0.19359934769074122\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01476592903998163\n",
      "Average test loss: 0.04062822599874603\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013050348930888706\n",
      "Average test loss: 0.0110582356924812\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01190564193452398\n",
      "Average test loss: 0.010547818208734194\n",
      "Epoch 5/300\n",
      "Average training loss: 0.011279047611686919\n",
      "Average test loss: 0.07808771166702111\n",
      "Epoch 6/300\n",
      "Average training loss: 0.010520756622155507\n",
      "Average test loss: 0.1697942993044853\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009644534893333912\n",
      "Average test loss: 0.0103211953813831\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008940444937182797\n",
      "Average test loss: 0.008516209637125334\n",
      "Epoch 9/300\n",
      "Average training loss: 0.008482785407039855\n",
      "Average test loss: 0.008223243502279123\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008212777940763367\n",
      "Average test loss: 0.007896080045650403\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007859538628823227\n",
      "Average test loss: 0.026810045888026556\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007615720798157984\n",
      "Average test loss: 0.010734514051841365\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007354778181761504\n",
      "Average test loss: 0.024313487160536978\n",
      "Epoch 14/300\n",
      "Average training loss: 0.007210691733078824\n",
      "Average test loss: 0.00703488194445769\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0070628383590115446\n",
      "Average test loss: 0.007343431147436301\n",
      "Epoch 16/300\n",
      "Average training loss: 0.006857732984340853\n",
      "Average test loss: 0.01772551287545098\n",
      "Epoch 17/300\n",
      "Average training loss: 0.006749671017130216\n",
      "Average test loss: 0.00750075488206413\n",
      "Epoch 18/300\n",
      "Average training loss: 0.006613546464178297\n",
      "Average test loss: 0.04440893746084637\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0064933517571124765\n",
      "Average test loss: 0.009185901603764958\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00642796790972352\n",
      "Average test loss: 0.008812633957299922\n",
      "Epoch 21/300\n",
      "Average training loss: 0.006326626691553328\n",
      "Average test loss: 0.5204264203574922\n",
      "Epoch 22/300\n",
      "Average training loss: 0.006246811159782939\n",
      "Average test loss: 0.018149581001864538\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00620834041842156\n",
      "Average test loss: 0.01980318475017945\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0060942232277658255\n",
      "Average test loss: 0.06603987148238553\n",
      "Epoch 25/300\n",
      "Average training loss: 0.006025720902201202\n",
      "Average test loss: 0.008182985571523508\n",
      "Epoch 26/300\n",
      "Average training loss: 0.005963864833944374\n",
      "Average test loss: 0.47197558776372006\n",
      "Epoch 27/300\n",
      "Average training loss: 0.005921210401174095\n",
      "Average test loss: 0.011053781571901507\n",
      "Epoch 28/300\n",
      "Average training loss: 0.005805206963171562\n",
      "Average test loss: 12.325468116275966\n",
      "Epoch 29/300\n",
      "Average training loss: 0.005747776709910896\n",
      "Average test loss: 0.0076542204022407535\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005706400737580326\n",
      "Average test loss: 0.5072107981277837\n",
      "Epoch 31/300\n",
      "Average training loss: 0.005669445095790757\n",
      "Average test loss: 0.6283816454211871\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005616098533074061\n",
      "Average test loss: 334.07543852742515\n",
      "Epoch 33/300\n",
      "Average training loss: 0.006931869659158919\n",
      "Average test loss: 0.02643820578356584\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005811956760370069\n",
      "Average test loss: 0.6686737295120126\n",
      "Epoch 35/300\n",
      "Average training loss: 0.005642364377776782\n",
      "Average test loss: 0.01888956042337749\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005610917335997025\n",
      "Average test loss: 4.54112702647431\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005618497637410958\n",
      "Average test loss: 3.50512073522475\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005558360366771618\n",
      "Average test loss: 16.36430030452874\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005520273779415422\n",
      "Average test loss: 0.019787142964700858\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0054698247921963535\n",
      "Average test loss: 0.027461038369271492\n",
      "Epoch 41/300\n",
      "Average training loss: 0.005450960462292036\n",
      "Average test loss: 2168.07221234934\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005463516966543264\n",
      "Average test loss: 0.014090748009582361\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005430785493304332\n",
      "Average test loss: 15.788201231152232\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005385962719304694\n",
      "Average test loss: 0.5304561178319984\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005389179937541485\n",
      "Average test loss: 56.212722792327405\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005724611156930526\n",
      "Average test loss: 4.350301398227612\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005342068141533269\n",
      "Average test loss: 0.25624651564160983\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0053291999254789614\n",
      "Average test loss: 30.785817587632273\n",
      "Epoch 49/300\n",
      "Average training loss: 0.00531063460972574\n",
      "Average test loss: 5.232985459773077\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005290346453587214\n",
      "Average test loss: 1217.804277410421\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005300090629193518\n",
      "Average test loss: 67814.68025766413\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005496200347940127\n",
      "Average test loss: 0.2931105169736677\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005283839298412204\n",
      "Average test loss: 35209.435670077415\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005283220675049557\n",
      "Average test loss: 0.12400351369049814\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005217349302851492\n",
      "Average test loss: 30.663127598333276\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005213664246102174\n",
      "Average test loss: 440.6903039821916\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00516611747443676\n",
      "Average test loss: 0.03395882520245181\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005154248216913806\n",
      "Average test loss: 17.305489150534072\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005128254602559739\n",
      "Average test loss: 0.10904997781043252\n",
      "Epoch 60/300\n",
      "Average training loss: 0.005120338862968816\n",
      "Average test loss: 9.08894068819988\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005077555797994137\n",
      "Average test loss: 14.451219535744853\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005092756494879723\n",
      "Average test loss: 3140.1516402340467\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005076869953423739\n",
      "Average test loss: 32.555896660791504\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005022394114070469\n",
      "Average test loss: 71.38925619140682\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005038915372764071\n",
      "Average test loss: 8238.50546645156\n",
      "Epoch 66/300\n",
      "Average training loss: 0.005017788805067539\n",
      "Average test loss: 1121.3009651656862\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005491087277730306\n",
      "Average test loss: 734.0972728972187\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005146439489391115\n",
      "Average test loss: 30.17867117801888\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005004383330129915\n",
      "Average test loss: 0.784455342889246\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004950729346109761\n",
      "Average test loss: 0.33932742944587435\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004938767481802238\n",
      "Average test loss: 38.42128299463457\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004926354501603379\n",
      "Average test loss: 0.03106661730756362\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0049056015060179765\n",
      "Average test loss: 0.0056742887194785804\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004900531187653542\n",
      "Average test loss: 839.4268942887949\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004899716135528353\n",
      "Average test loss: 703.0316455855932\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0048956571235838865\n",
      "Average test loss: 365.2843393140667\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004882218613185816\n",
      "Average test loss: 0.928225544432799\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004867067968679799\n",
      "Average test loss: 0.005667070077525245\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006457420143402285\n",
      "Average test loss: 0.01503223733479778\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005414006896523966\n",
      "Average test loss: 0.8177800486435493\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0050694759541915525\n",
      "Average test loss: 205.74054864421487\n",
      "Epoch 82/300\n",
      "Average training loss: 0.004954802681588464\n",
      "Average test loss: 507.10010873290565\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004916046547599965\n",
      "Average test loss: 0.9856871078477966\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004857918128371239\n",
      "Average test loss: 322.90611966026825\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004838081905825271\n",
      "Average test loss: 5.9354045472587975\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0048010402499801584\n",
      "Average test loss: 5289.01356116018\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004801118399533961\n",
      "Average test loss: 261.32699652286493\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0049972555198603205\n",
      "Average test loss: 0.11217785013053153\n",
      "Epoch 89/300\n",
      "Average training loss: 0.004841784146303932\n",
      "Average test loss: 0.005627551349914736\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004775537089755138\n",
      "Average test loss: 4283.945648816905\n",
      "Epoch 91/300\n",
      "Average training loss: 0.004762923094961379\n",
      "Average test loss: 99.89384995153712\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004751157599811753\n",
      "Average test loss: 269.91744923943446\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004751827680609292\n",
      "Average test loss: 0.37940502332316506\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004726784649408526\n",
      "Average test loss: 1005508.5180804507\n",
      "Epoch 95/300\n",
      "Average training loss: 0.004736477075351609\n",
      "Average test loss: 5211.800516247448\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0046895721277429\n",
      "Average test loss: 44650.39316319878\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004757446058094502\n",
      "Average test loss: 30.23650996509981\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004712811997574237\n",
      "Average test loss: 0.3003420716760059\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004669845282203621\n",
      "Average test loss: 0.08923450746263067\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00464766072606047\n",
      "Average test loss: 0.006961398503846593\n",
      "Epoch 101/300\n",
      "Average training loss: 0.00463173465244472\n",
      "Average test loss: 0.005609324397726191\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004659136661224895\n",
      "Average test loss: 0.008767073707448112\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004607061280351546\n",
      "Average test loss: 5614.01737753635\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004603555299341679\n",
      "Average test loss: 431.9238550997674\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0045966473929584025\n",
      "Average test loss: 141311.90685416668\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0046083096145755715\n",
      "Average test loss: 0.5830768170058728\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004568131829508477\n",
      "Average test loss: 0.013495496148864429\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004555408074624009\n",
      "Average test loss: 46.52067112077276\n",
      "Epoch 109/300\n",
      "Average training loss: 0.004542192914419704\n",
      "Average test loss: 3.644525969748696\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005604989638345109\n",
      "Average test loss: 0.015271616594245037\n",
      "Epoch 111/300\n",
      "Average training loss: 0.004934832831223806\n",
      "Average test loss: 1.670741502088805\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004636501186423832\n",
      "Average test loss: 127.48691522327397\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0045735237238307795\n",
      "Average test loss: 0.010505904957652091\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004573832646219266\n",
      "Average test loss: 0.008915431121985118\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004532422487106588\n",
      "Average test loss: 0.9877164136026468\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0045025271719528566\n",
      "Average test loss: 0.37745493139161\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004482063806305329\n",
      "Average test loss: 0.006158543819768561\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00450126617319054\n",
      "Average test loss: 0.15339426838027106\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004506797221799691\n",
      "Average test loss: 1020.024665050192\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0044905211710267595\n",
      "Average test loss: 1.3734432990931802\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004485390713645353\n",
      "Average test loss: 0.42228738628658985\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004489946623229318\n",
      "Average test loss: 0.1108742185106708\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004477696616202593\n",
      "Average test loss: 0.00626808231903447\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0044539759763413\n",
      "Average test loss: 101.7519929047823\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004491818064616786\n",
      "Average test loss: 3444.628092862396\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004464526859008604\n",
      "Average test loss: 695.2006726656523\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00445569789244069\n",
      "Average test loss: 0.005548412543618017\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004414202803745866\n",
      "Average test loss: 0.21485281744433773\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004422633635294106\n",
      "Average test loss: 2.4649347314106094\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004397335843907463\n",
      "Average test loss: 29.672114570422305\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0044140178155567915\n",
      "Average test loss: 671.2855401258635\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004365062482655048\n",
      "Average test loss: 0.1716074688049654\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004878507294174697\n",
      "Average test loss: 59.58097990682721\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004450787979281611\n",
      "Average test loss: 0.363713004829569\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004374379942193627\n",
      "Average test loss: 3.026772737733192\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004351090266886685\n",
      "Average test loss: 0.2669207211683194\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004334416754957702\n",
      "Average test loss: 458.7310446467797\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004347031638440159\n",
      "Average test loss: 1038.7190886203282\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004324094839808014\n",
      "Average test loss: 339.7925615488953\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004340164471003744\n",
      "Average test loss: 0.7614886371977627\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004318116495178805\n",
      "Average test loss: 0.18705770382657647\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00430134787886507\n",
      "Average test loss: 0.005757698040041659\n",
      "Epoch 143/300\n",
      "Average training loss: 0.004311797122988436\n",
      "Average test loss: 0.0064377639744844705\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004283298189855284\n",
      "Average test loss: 0.9094109908909433\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004313146303718289\n",
      "Average test loss: 0.06379685103396575\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004303696818649769\n",
      "Average test loss: 0.03955285986885428\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004269464223335187\n",
      "Average test loss: 8388.732009945263\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004261257603350613\n",
      "Average test loss: 6.663630511081881\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004262558886574374\n",
      "Average test loss: 0.005826396121746964\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0042560160356677245\n",
      "Average test loss: 0.0842377210189071\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004225675179312626\n",
      "Average test loss: 0.005558560618509849\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004253020030756792\n",
      "Average test loss: 0.013900804324282541\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004225038158189919\n",
      "Average test loss: 8.662780495560831\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004283521802475055\n",
      "Average test loss: 2.6642520531217255\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004245561402291059\n",
      "Average test loss: 3.308640276540071\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004257539299627145\n",
      "Average test loss: 475.1016986300796\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004236137721687555\n",
      "Average test loss: 0.4259742828905582\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004226906203354398\n",
      "Average test loss: 794.0946154671021\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004262015049242311\n",
      "Average test loss: 0.0057251256050335035\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004230560010092126\n",
      "Average test loss: 98.27245228650835\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004233576888011561\n",
      "Average test loss: 29.0440941856073\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004171215795394447\n",
      "Average test loss: 27.020669559955596\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004180533091641135\n",
      "Average test loss: 89941.86309652755\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0041898489101893375\n",
      "Average test loss: 0.2693954970708324\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004223027615083588\n",
      "Average test loss: 11.738971878141164\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004154708235214154\n",
      "Average test loss: 2.224633906563123\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004165176654027568\n",
      "Average test loss: 257794.01732118055\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0042716225323577725\n",
      "Average test loss: 0.7253284616784917\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00423819302001761\n",
      "Average test loss: 0.005842635770638783\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0041645457587308355\n",
      "Average test loss: 2.4293597944784495\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004369794586880339\n",
      "Average test loss: 0.005695663985278872\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004217774821652307\n",
      "Average test loss: 0.006194551760123836\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004151547750251161\n",
      "Average test loss: 1.1709132958617476\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004135435202262468\n",
      "Average test loss: 0.019974602406223615\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004192587934848335\n",
      "Average test loss: 0.019389005145264995\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004346127921094497\n",
      "Average test loss: 1.3768777022295529\n",
      "Epoch 177/300\n",
      "Average training loss: 0.00446271004817552\n",
      "Average test loss: 524.4536570453321\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004242340115830303\n",
      "Average test loss: 18875.05123238591\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004149599122918315\n",
      "Average test loss: 0.008237818873590893\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0041290147648089465\n",
      "Average test loss: 6399.450874186198\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00412797979472412\n",
      "Average test loss: 918.848350418414\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010818172164261342\n",
      "Average test loss: 234.77089260652414\n",
      "Epoch 183/300\n",
      "Average training loss: 0.00684108638515075\n",
      "Average test loss: 101087.88907788889\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0063048442635271285\n",
      "Average test loss: 338379.5103643598\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006053478223168188\n",
      "Average test loss: 393069.2975108564\n",
      "Epoch 186/300\n",
      "Average training loss: 0.005557944536209106\n",
      "Average test loss: 613.3090801758824\n",
      "Epoch 187/300\n",
      "Average training loss: 0.005278575917085012\n",
      "Average test loss: 62827.522210154995\n",
      "Epoch 188/300\n",
      "Average training loss: 0.005110185889320241\n",
      "Average test loss: 66749.66050238699\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004972060386505392\n",
      "Average test loss: 15849.54241205622\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004741944264620542\n",
      "Average test loss: 6759375.2283333335\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0046579410497927\n",
      "Average test loss: 212225.64804781205\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004563575761599673\n",
      "Average test loss: 740.8334718980839\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00443813225088848\n",
      "Average test loss: 326121.5986400591\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004364787698619896\n",
      "Average test loss: 9004673.18233879\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004397582231710355\n",
      "Average test loss: 43926.7243131259\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005142649297705955\n",
      "Average test loss: 0.13422033111916648\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004430859168577526\n",
      "Average test loss: 34.461359888745676\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004304690011259583\n",
      "Average test loss: 0.16556628158150447\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0042102601296371885\n",
      "Average test loss: 30.015966877199297\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004164217635989189\n",
      "Average test loss: 0.005948149897158146\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00431647216487262\n",
      "Average test loss: 1.4675620615018738\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004243891273935636\n",
      "Average test loss: 0.1372221570710341\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004319430406308836\n",
      "Average test loss: 0.006439832846323649\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004310520758438441\n",
      "Average test loss: 1.5993855638524725\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004154951802558369\n",
      "Average test loss: 5.513510906457901\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00411246363653077\n",
      "Average test loss: 0.031496136914110844\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004151803717017174\n",
      "Average test loss: 0.08639051990624931\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004131369386903114\n",
      "Average test loss: 0.07902532151382831\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004136501633251707\n",
      "Average test loss: 1.719662849323617\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004105151988772883\n",
      "Average test loss: 0.6035775033351448\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00410556735470891\n",
      "Average test loss: 0.019104185585346486\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004267897123263942\n",
      "Average test loss: 0.007253345875276459\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004162264397574796\n",
      "Average test loss: 71.11861592965656\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0042792812296085884\n",
      "Average test loss: 21.195210991613568\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004121803270032008\n",
      "Average test loss: 0.016499407652765512\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005624147995478577\n",
      "Average test loss: 0.9632433266705936\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005231084533449676\n",
      "Average test loss: 0.4023367503368192\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004710552581068542\n",
      "Average test loss: 726.3322715599305\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004432404716809591\n",
      "Average test loss: 60.9767146834698\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004256624237530761\n",
      "Average test loss: 2.3253385095952286\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00417068190086219\n",
      "Average test loss: 1755.787410252889\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004090073567297723\n",
      "Average test loss: 44.807722905503795\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004047639204396142\n",
      "Average test loss: 29.195330673128367\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004062388889491558\n",
      "Average test loss: 2.9491686688367693\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004048864851188328\n",
      "Average test loss: 0.20303047056868673\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004012049326052269\n",
      "Average test loss: 0.6670977399398883\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0040407090762423145\n",
      "Average test loss: 55.93846167859435\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00402907862741914\n",
      "Average test loss: 0.6433217501052552\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004028134103036589\n",
      "Average test loss: 27.537093182371724\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004009066003892157\n",
      "Average test loss: 321.2737666375732\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004018633930219544\n",
      "Average test loss: 0.05090255137988263\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004082811931976013\n",
      "Average test loss: 22.054939335187274\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0040913929686778126\n",
      "Average test loss: 407.4185615952048\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004031954252057606\n",
      "Average test loss: 0.01963195911794901\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004011723635718226\n",
      "Average test loss: 23.917147342235676\n",
      "Epoch 236/300\n",
      "Average training loss: 0.003982185104447935\n",
      "Average test loss: 81.88663964015412\n",
      "Epoch 237/300\n",
      "Average training loss: 0.003955550935533312\n",
      "Average test loss: 35.111259993488176\n",
      "Epoch 238/300\n",
      "Average training loss: 0.003986232603382733\n",
      "Average test loss: 4.184138106255068\n",
      "Epoch 239/300\n",
      "Average training loss: 0.003997616829764512\n",
      "Average test loss: 0.3483667721607619\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004016332673529784\n",
      "Average test loss: 104.78876085279386\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0039703716730905905\n",
      "Average test loss: 3626347.1455555554\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00399945461211933\n",
      "Average test loss: 0.006104644980281591\n",
      "Epoch 243/300\n",
      "Average training loss: 0.003949850036245254\n",
      "Average test loss: 2797.4033280917074\n",
      "Epoch 244/300\n",
      "Average training loss: 0.003965430683145921\n",
      "Average test loss: 6884.06729217318\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003936243942214383\n",
      "Average test loss: 574.4361438259549\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00393754299895631\n",
      "Average test loss: 1114.633289639496\n",
      "Epoch 247/300\n",
      "Average training loss: 0.003981373509185182\n",
      "Average test loss: 20630.957104702604\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0039864489581022\n",
      "Average test loss: 103.16498168934716\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004004885593222247\n",
      "Average test loss: 32.09610937534107\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004080722314616044\n",
      "Average test loss: 379.5447649841027\n",
      "Epoch 251/300\n",
      "Average training loss: 0.003987099742723836\n",
      "Average test loss: 43.771687211261856\n",
      "Epoch 252/300\n",
      "Average training loss: 0.00404300357401371\n",
      "Average test loss: 4.1159875575535825\n",
      "Epoch 253/300\n",
      "Average training loss: 0.003925464199235042\n",
      "Average test loss: 1501032.6765392278\n",
      "Epoch 254/300\n",
      "Average training loss: 0.003921286134256257\n",
      "Average test loss: 349.64934331216085\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0038882530079119735\n",
      "Average test loss: 0.012699771303269599\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003935353892544905\n",
      "Average test loss: 994710.6317906545\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003912104442508685\n",
      "Average test loss: 1842.9576106344705\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003885164962253637\n",
      "Average test loss: 10.897076452174534\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00391457981016073\n",
      "Average test loss: 102915.71201449187\n",
      "Epoch 260/300\n",
      "Average training loss: 0.003880590421458085\n",
      "Average test loss: 40909467.82441587\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0039302400611341\n",
      "Average test loss: 0.005677924630956517\n",
      "Epoch 262/300\n",
      "Average training loss: 0.003891634633971585\n",
      "Average test loss: 2.175570075102978\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0038472823231584497\n",
      "Average test loss: 0.0485738359482752\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0038458557861546674\n",
      "Average test loss: 61.70139963813954\n",
      "Epoch 265/300\n",
      "Average training loss: 0.003944418538361788\n",
      "Average test loss: 21689.652195322276\n",
      "Epoch 266/300\n",
      "Average training loss: 0.003919553006274832\n",
      "Average test loss: 63562.8032343514\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0038528119017266563\n",
      "Average test loss: 0.4029174247831106\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0038409174924923313\n",
      "Average test loss: 0.09749092823060022\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0038390452017386754\n",
      "Average test loss: 0.005879614022043016\n",
      "Epoch 270/300\n",
      "Average training loss: 0.003841551513928506\n",
      "Average test loss: 0.00610845833685663\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0038336521378821795\n",
      "Average test loss: 4.666325112646239\n",
      "Epoch 272/300\n",
      "Average training loss: 0.003839154208699862\n",
      "Average test loss: 3693.3049307755364\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0038280726166235075\n",
      "Average test loss: 0.006139041591021749\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0038230367882384196\n",
      "Average test loss: 1.7803088943461578\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0038081049730794296\n",
      "Average test loss: 286.2586444069925\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0038176667485386134\n",
      "Average test loss: 0.03959382232526938\n",
      "Epoch 277/300\n",
      "Average training loss: 0.003809477636176679\n",
      "Average test loss: 106.77418278031548\n",
      "Epoch 278/300\n",
      "Average training loss: 0.003815735138538811\n",
      "Average test loss: 135.35150377932854\n",
      "Epoch 279/300\n",
      "Average training loss: 0.003909941412301527\n",
      "Average test loss: 12.77816258336107\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0038529605170090995\n",
      "Average test loss: 8.524114828523663\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003820901626927985\n",
      "Average test loss: 15.866051652719577\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0038022457530928985\n",
      "Average test loss: 0.8955380075619452\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0038171178369472425\n",
      "Average test loss: 24.042743755208534\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0037836788969321383\n",
      "Average test loss: 5.134290570085661\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0037724971611880595\n",
      "Average test loss: 704.5370593886176\n",
      "Epoch 286/300\n",
      "Average training loss: 0.00375776620912883\n",
      "Average test loss: 49.435459405202\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0037932850348038807\n",
      "Average test loss: 0.0058472847644653585\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0037648562830355434\n",
      "Average test loss: 147.0757994991127\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0038230432108458546\n",
      "Average test loss: 1858552.0017137586\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0037712818512486085\n",
      "Average test loss: 161.91564025857383\n",
      "Epoch 291/300\n",
      "Average training loss: 0.003793813040273057\n",
      "Average test loss: 140.56860534606707\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003812530566006899\n",
      "Average test loss: 1.2511450558797352\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0038712004319661195\n",
      "Average test loss: 169.42805003506277\n",
      "Epoch 294/300\n",
      "Average training loss: 0.003924573683076435\n",
      "Average test loss: 1.1573141767680646\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0038169434987422495\n",
      "Average test loss: 0.009161939122196702\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0037719373235272037\n",
      "Average test loss: 141.32825967858898\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003841527452485429\n",
      "Average test loss: 3.86568544385665\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0038123182403958507\n",
      "Average test loss: 0.1293035464005338\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0037958631134695476\n",
      "Average test loss: 1.6703379358102879\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0037733497886608043\n",
      "Average test loss: 3.171961514055729\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06798084217641089\n",
      "Average test loss: 0.14966166119608615\n",
      "Epoch 2/300\n",
      "Average training loss: 0.009473260491258568\n",
      "Average test loss: 0.01174638010147545\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007679955214675929\n",
      "Average test loss: 0.006962073693258895\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006800985555267996\n",
      "Average test loss: 0.006418050414158238\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006265058516628212\n",
      "Average test loss: 0.007907553683552476\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005892276582204634\n",
      "Average test loss: 0.005591708349270953\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005579182253115707\n",
      "Average test loss: 0.0053703905718608036\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005309157954115007\n",
      "Average test loss: 0.005787229281332758\n",
      "Epoch 9/300\n",
      "Average training loss: 0.005076260284417205\n",
      "Average test loss: 0.015338009983301162\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0048564268781079185\n",
      "Average test loss: 0.051681956350803374\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004671897648523251\n",
      "Average test loss: 0.004835506911079089\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0044946684288895795\n",
      "Average test loss: 0.015271050267749362\n",
      "Epoch 13/300\n",
      "Average training loss: 0.00435925600077543\n",
      "Average test loss: 0.061559837324751746\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0042263860737697945\n",
      "Average test loss: 0.0166344475241171\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0040974182256807885\n",
      "Average test loss: 0.0068465809639957215\n",
      "Epoch 16/300\n",
      "Average training loss: 0.003979140701807208\n",
      "Average test loss: 2.2826753851307764\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0038585724162144793\n",
      "Average test loss: 0.005483865272667673\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003771091466769576\n",
      "Average test loss: 0.0037876077780707013\n",
      "Epoch 19/300\n",
      "Average training loss: 0.003683742508292198\n",
      "Average test loss: 0.003686761362892058\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003634633829196294\n",
      "Average test loss: 0.004192693821464976\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003542677032864756\n",
      "Average test loss: 0.027961018609917825\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0035046318301724063\n",
      "Average test loss: 0.6551823470344146\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0034374823421239853\n",
      "Average test loss: 0.0034450470407803853\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00341574475955632\n",
      "Average test loss: 0.016290155368132723\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0033437929726723168\n",
      "Average test loss: 27.94251256652342\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003299654883746472\n",
      "Average test loss: 0.003380728983423776\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006277344494023257\n",
      "Average test loss: 0.3899748126914104\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003903732512974077\n",
      "Average test loss: 0.04226562990744909\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0035710384342819454\n",
      "Average test loss: 0.12266400860829486\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0034249795905003945\n",
      "Average test loss: 0.006220439358096984\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0033414203921953837\n",
      "Average test loss: 245.7578449642441\n",
      "Epoch 32/300\n",
      "Average training loss: 0.003300783570648895\n",
      "Average test loss: 7.123836391078101\n",
      "Epoch 33/300\n",
      "Average training loss: 0.003259571415475673\n",
      "Average test loss: 0.4215349449486368\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0032144795945949025\n",
      "Average test loss: 10.037880411997438\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0032087322591493527\n",
      "Average test loss: 0.003556624693589078\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003225613030915459\n",
      "Average test loss: 6.759194104628844\n",
      "Epoch 37/300\n",
      "Average training loss: 0.003549232941534784\n",
      "Average test loss: 0.007611560136079788\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003174507270463639\n",
      "Average test loss: 0.5450334101357601\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003137346033420828\n",
      "Average test loss: 15.382388997404112\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003119446258991957\n",
      "Average test loss: 6.553707818132308\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003104627514258027\n",
      "Average test loss: 0.011475772892642353\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0030779576026317146\n",
      "Average test loss: 25.717653741972313\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003062705069159468\n",
      "Average test loss: 60.87498033418175\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0030337264904131494\n",
      "Average test loss: 8497.57331291128\n",
      "Epoch 45/300\n",
      "Average training loss: 0.003011510230600834\n",
      "Average test loss: 1568.9723854297126\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003009337384874622\n",
      "Average test loss: 1887.9557281551188\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0030093902128024235\n",
      "Average test loss: 0.4898134942462461\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002959932387289074\n",
      "Average test loss: 312.6733590538195\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0029368920949184234\n",
      "Average test loss: 113.16896957341478\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0044072635666363765\n",
      "Average test loss: 1.6917246514664341\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0032045095495672688\n",
      "Average test loss: 0.6898677528206673\n",
      "Epoch 52/300\n",
      "Average training loss: 0.003057270617948638\n",
      "Average test loss: 186.61729141400755\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0030141852802286545\n",
      "Average test loss: 1.7171546038339536\n",
      "Epoch 54/300\n",
      "Average training loss: 0.002974284706430303\n",
      "Average test loss: 5.256455330344124\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002944759085981382\n",
      "Average test loss: 417.622649448699\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0029283117805090214\n",
      "Average test loss: 2285.5613230272024\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002907295862833659\n",
      "Average test loss: 1717.4516574378792\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002900536233869692\n",
      "Average test loss: 48465.09683240006\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0028935405740307435\n",
      "Average test loss: 115067.93353148934\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002881093297774593\n",
      "Average test loss: 1880.4506356993827\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0028693674858659506\n",
      "Average test loss: 4.133822972772436\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0028789723898387616\n",
      "Average test loss: 19862.581716413046\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0028579769502911304\n",
      "Average test loss: 27320.87367513206\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002842824216932058\n",
      "Average test loss: 772.181281753973\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002834206914736165\n",
      "Average test loss: 1142.1270456110199\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0028132503241714503\n",
      "Average test loss: 12390.019310634889\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0027934335877911913\n",
      "Average test loss: 2151.8231173168206\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002810912799090147\n",
      "Average test loss: 34785.612179391304\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0027756993402209547\n",
      "Average test loss: 3196.873140877025\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002770634863111708\n",
      "Average test loss: 2477.3201688525223\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0027524141885547173\n",
      "Average test loss: 64.7499092235205\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003294893478353818\n",
      "Average test loss: 0.10097451301250193\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0032311023232630557\n",
      "Average test loss: 840.2629057807683\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002939967882922954\n",
      "Average test loss: 158.7165200344531\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0028383446484804152\n",
      "Average test loss: 1410.110260084178\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002786866697275804\n",
      "Average test loss: 176.46232622355802\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0027629437645276386\n",
      "Average test loss: 1631.9305752479434\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002833209276199341\n",
      "Average test loss: 12.957153072191609\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002770343785070711\n",
      "Average test loss: 7.443458330223958\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0027419240745819276\n",
      "Average test loss: 11271.9021715002\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0027388509875163436\n",
      "Average test loss: 10.256854977502798\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0028366669048037795\n",
      "Average test loss: 24.321958325328925\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0027057535335835484\n",
      "Average test loss: 6456.459990234375\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0026891823804212943\n",
      "Average test loss: 2223.8475606776874\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0026754896615942317\n",
      "Average test loss: 683660.1550876602\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002712642932517661\n",
      "Average test loss: 2.248170471549655\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0026699240228368177\n",
      "Average test loss: 17.982673459510423\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002654162644615604\n",
      "Average test loss: 24.98706853245054\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0026372261194305287\n",
      "Average test loss: 189471.6095268865\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0026313209729269147\n",
      "Average test loss: 4.696213327202946\n",
      "Epoch 91/300\n",
      "Average training loss: 0.002616294484378563\n",
      "Average test loss: 3723641.967143449\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002604709607652492\n",
      "Average test loss: 26784.471409858354\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0025891675293031667\n",
      "Average test loss: 4618.731174926042\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0025993543981264037\n",
      "Average test loss: 24244.59003257004\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00257791860970772\n",
      "Average test loss: 120714.5302692573\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0025999809282107487\n",
      "Average test loss: 67417.75177606803\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002551233703787956\n",
      "Average test loss: 8287029.217162677\n",
      "Epoch 98/300\n",
      "Average training loss: 0.002557770246018966\n",
      "Average test loss: 511742.70190314983\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0025481970415761074\n",
      "Average test loss: 179.67013811636053\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0025265793497156767\n",
      "Average test loss: 165998.69709963625\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002515272634000414\n",
      "Average test loss: 17351.45765881031\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0025362671388106215\n",
      "Average test loss: 4897.270601756651\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0024919181359517907\n",
      "Average test loss: 291.0810818135136\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0024953125988443695\n",
      "Average test loss: 27500.77372746665\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002483510298343996\n",
      "Average test loss: 23708.71117732576\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0024741348740127352\n",
      "Average test loss: 289855241.4019938\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002495897195612391\n",
      "Average test loss: 0.003025732008740306\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00247777121534778\n",
      "Average test loss: 5305128.425111111\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0024867814659244483\n",
      "Average test loss: 18.1376301274846\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002474751674880584\n",
      "Average test loss: 2291.1663159059917\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0024381961593818334\n",
      "Average test loss: 535.3012038839468\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0030301046576350927\n",
      "Average test loss: 286762.1005109581\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0029312254077651432\n",
      "Average test loss: 489079.82526831364\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0026180935081922345\n",
      "Average test loss: 811436.5243686483\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0025395688528401985\n",
      "Average test loss: 16206.544243487626\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0024893169663846495\n",
      "Average test loss: 77224.30232492332\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0024625190486096673\n",
      "Average test loss: 845289.6556779257\n",
      "Epoch 118/300\n",
      "Average training loss: 0.002441471530538466\n",
      "Average test loss: 4743266.914241006\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002441565853440099\n",
      "Average test loss: 2716.826845173379\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002431985698743827\n",
      "Average test loss: 14426469.286307395\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002423891451002823\n",
      "Average test loss: 678328.9921297499\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0024143925675501427\n",
      "Average test loss: 1.1647605784355353\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0024145190358782808\n",
      "Average test loss: 9965.334285646955\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0024072916900946036\n",
      "Average test loss: 63.400269068634344\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0023696243821953733\n",
      "Average test loss: 112.16160168356117\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0024408263296095863\n",
      "Average test loss: 1.566753897914456\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0023860470469420155\n",
      "Average test loss: 886.1605885980559\n",
      "Epoch 128/300\n",
      "Average training loss: 0.002362795762717724\n",
      "Average test loss: 0.6373823029200236\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0023485606064399082\n",
      "Average test loss: 360952.88235839526\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0028482350013736223\n",
      "Average test loss: 4.726019170088279\n",
      "Epoch 131/300\n",
      "Average training loss: 0.00240436110790405\n",
      "Average test loss: 334853.2600597214\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002407845855380098\n",
      "Average test loss: 231965.3734671224\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0024667739073435464\n",
      "Average test loss: 3017.575278725557\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002371978022158146\n",
      "Average test loss: 582594.2815172925\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0023476486010476946\n",
      "Average test loss: 9.012013538423306\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0025025962804340655\n",
      "Average test loss: 1152.802786963569\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0025093625953627957\n",
      "Average test loss: 2814271.8723172336\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002441221027738518\n",
      "Average test loss: 2163459.100284811\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0023977778842672708\n",
      "Average test loss: 1002264.1129149954\n",
      "Epoch 140/300\n",
      "Average training loss: 0.002395395794055528\n",
      "Average test loss: 1121908.3682567626\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0023638262200272745\n",
      "Average test loss: 467140.85477980116\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0026309744233472478\n",
      "Average test loss: 2.340460967340403\n",
      "Epoch 143/300\n",
      "Average training loss: 0.002377333563235071\n",
      "Average test loss: 941.3422526995649\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0023524757869955565\n",
      "Average test loss: 3394.359124473387\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002345994881147312\n",
      "Average test loss: 181.44503698621762\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002382247353800469\n",
      "Average test loss: 3.2995840337801106\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0024039961819847426\n",
      "Average test loss: 39.77222840890019\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0023353366105713777\n",
      "Average test loss: 144922.8757449717\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0023191463918321663\n",
      "Average test loss: 3630.0518377098565\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00231759213635491\n",
      "Average test loss: 41886.966383760344\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002314392250755595\n",
      "Average test loss: 65932.63573008265\n",
      "Epoch 152/300\n",
      "Average training loss: 0.002293623818705479\n",
      "Average test loss: 196129.06245377605\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0023022741900963917\n",
      "Average test loss: 46497.124932964776\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0023009074091290436\n",
      "Average test loss: 162.2390264004647\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0024904311592173246\n",
      "Average test loss: 31669.059176546718\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002334328518766496\n",
      "Average test loss: 2862.4530641022943\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002295294813811779\n",
      "Average test loss: 3969.0684939308762\n",
      "Epoch 158/300\n",
      "Average training loss: 0.002288587852070729\n",
      "Average test loss: 54239.044585557334\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0022819767952379252\n",
      "Average test loss: 10446.485415819026\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0022768139428355626\n",
      "Average test loss: 12.162626272577793\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0031417322028428315\n",
      "Average test loss: 2397727.9610557393\n",
      "Epoch 162/300\n",
      "Average training loss: 0.002508899024584227\n",
      "Average test loss: 66083922.15880202\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002362997748889029\n",
      "Average test loss: 72.37954161741129\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0022851963030795256\n",
      "Average test loss: 201.66210791130695\n",
      "Epoch 165/300\n",
      "Average training loss: 0.002286834165867832\n",
      "Average test loss: 151802.87635468773\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0022848002227644124\n",
      "Average test loss: 517.4092243369744\n",
      "Epoch 167/300\n",
      "Average training loss: 0.002294257048931387\n",
      "Average test loss: 370.94950824713874\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0022699703477943936\n",
      "Average test loss: 0.0032498263414535255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002287719624117017\n",
      "Average test loss: 157.46429053859495\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0022646022193754715\n",
      "Average test loss: 13.766667117509577\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002276770887275537\n",
      "Average test loss: 3431059.3212765954\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0022542115894870624\n",
      "Average test loss: 338266.97744492954\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0022418067302140926\n",
      "Average test loss: 2027.8966127802357\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002509200275254746\n",
      "Average test loss: 222.132159617876\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0023256799448281525\n",
      "Average test loss: 342.3111624685493\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0022743418363647327\n",
      "Average test loss: 9650.978007001602\n",
      "Epoch 177/300\n",
      "Average training loss: 0.00231137438201242\n",
      "Average test loss: 49088.26604636293\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0022269861933050883\n",
      "Average test loss: 14585.205273467967\n",
      "Epoch 179/300\n",
      "Average training loss: 0.002217033247773846\n",
      "Average test loss: 133.1848627844908\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0022852680841460825\n",
      "Average test loss: 62588.59720853538\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0022051825788285996\n",
      "Average test loss: 75931803.40088889\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0022988014670295848\n",
      "Average test loss: 7.612409214973864\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0023219721058590544\n",
      "Average test loss: 9311.024564012392\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0022823845996624895\n",
      "Average test loss: 134.45033954127837\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0022445573549096784\n",
      "Average test loss: 578.1909556634699\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0022056423841665188\n",
      "Average test loss: 80.22117031928731\n",
      "Epoch 187/300\n",
      "Average training loss: 0.00226618410491695\n",
      "Average test loss: 145.82644723660644\n",
      "Epoch 188/300\n",
      "Average training loss: 0.00225388217303488\n",
      "Average test loss: 1233701.3609756103\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0024214274417608977\n",
      "Average test loss: 2895.6433616682684\n",
      "Epoch 190/300\n",
      "Average training loss: 0.002326083421293232\n",
      "Average test loss: 15.694586805838263\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0022651135525148776\n",
      "Average test loss: 94.58754724598221\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002286395359163483\n",
      "Average test loss: 366.94912141911476\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0022606154212521184\n",
      "Average test loss: 27097.953056437425\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0022111600633296703\n",
      "Average test loss: 9312282.444909979\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002217110838120182\n",
      "Average test loss: 1983.4805299882987\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0021783152096387414\n",
      "Average test loss: 9312.254667751846\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0021696609122057757\n",
      "Average test loss: 117979.42115999968\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0021621953099966048\n",
      "Average test loss: 169.09879106634855\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002163101590341992\n",
      "Average test loss: 4515.356789334801\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0021422385912802486\n",
      "Average test loss: 0.014682542449070348\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002195118167334133\n",
      "Average test loss: 45.345300617188215\n",
      "Epoch 202/300\n",
      "Average training loss: 0.002206686702867349\n",
      "Average test loss: 840.8666076745093\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0023067995993834405\n",
      "Average test loss: 7025.655003089603\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0023183485695885287\n",
      "Average test loss: 9135.513705714953\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0022706835750076508\n",
      "Average test loss: 1150165.7754460524\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0021791305900033977\n",
      "Average test loss: 2674.2925310672863\n",
      "Epoch 207/300\n",
      "Average training loss: 0.002162329698395398\n",
      "Average test loss: 1465889.266040395\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0021819168242315453\n",
      "Average test loss: 1202.763520270494\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0022034232995162407\n",
      "Average test loss: 623369763.8542222\n",
      "Epoch 210/300\n",
      "Average training loss: 0.002309274008290635\n",
      "Average test loss: 1162.6194911865969\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002165014127165907\n",
      "Average test loss: 17148.278169617697\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0021647896280305255\n",
      "Average test loss: 69097.32720924301\n",
      "Epoch 213/300\n",
      "Average training loss: 0.002152759284401933\n",
      "Average test loss: 13565956.928983679\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0021537019764590595\n",
      "Average test loss: 907273.9392763169\n",
      "Epoch 215/300\n",
      "Average training loss: 0.002128692068573501\n",
      "Average test loss: 625194.0306579884\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002140146022248599\n",
      "Average test loss: 427443.5718114959\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0021812327704909774\n",
      "Average test loss: 392510048.62844443\n",
      "Epoch 218/300\n",
      "Average training loss: 0.002166463071687354\n",
      "Average test loss: 25844.17452466083\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0021481350196732416\n",
      "Average test loss: 142980.39061751598\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0021546843711079825\n",
      "Average test loss: 98756826.2006335\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0021479102176510624\n",
      "Average test loss: 111531837.76111111\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0022252337369653916\n",
      "Average test loss: 8.57610928834271\n",
      "Epoch 223/300\n",
      "Average training loss: 0.002114390642175244\n",
      "Average test loss: 357526.6909076236\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002114144190318055\n",
      "Average test loss: 0.7011605369134082\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0021196924071345065\n",
      "Average test loss: 45260.11765394076\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0021531295254826545\n",
      "Average test loss: 9023360.202212099\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0021366311568352912\n",
      "Average test loss: 11214601.117436912\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002100624371113049\n",
      "Average test loss: 2706339.8692135415\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002110831548356348\n",
      "Average test loss: 199390.7941311591\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0021009570678903\n",
      "Average test loss: 592621.148333972\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0020898291538986894\n",
      "Average test loss: 2301594.981252649\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0020848606552721725\n",
      "Average test loss: 249545.3173113641\n",
      "Epoch 233/300\n",
      "Average training loss: 0.002081391090941098\n",
      "Average test loss: 149422690.12337667\n",
      "Epoch 234/300\n",
      "Average training loss: 0.002077614003688925\n",
      "Average test loss: 8782790.925175129\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0020873310511103933\n",
      "Average test loss: 5175472.987072846\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002084540239121351\n",
      "Average test loss: 79788.40560112764\n",
      "Epoch 237/300\n",
      "Average training loss: 0.002108905504975054\n",
      "Average test loss: 409.6341873247963\n",
      "Epoch 238/300\n",
      "Average training loss: 0.002083929104523526\n",
      "Average test loss: 11975.65532846922\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002090360328141186\n",
      "Average test loss: 80301.83732312263\n",
      "Epoch 240/300\n",
      "Average training loss: 0.002070437524674667\n",
      "Average test loss: 43791.42546903062\n",
      "Epoch 241/300\n",
      "Average training loss: 0.002069348191118075\n",
      "Average test loss: 796.8530737389003\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002053170533022947\n",
      "Average test loss: 130867.80335937232\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002076092618207137\n",
      "Average test loss: 206568.65152539514\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002055436005298462\n",
      "Average test loss: 1059707.4287610152\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0020457986989575955\n",
      "Average test loss: 45166004.58218913\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0033960539750340912\n",
      "Average test loss: 4440.6343666235025\n",
      "Epoch 247/300\n",
      "Average training loss: 0.002736768379807472\n",
      "Average test loss: 230091.74629551894\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002546256898384955\n",
      "Average test loss: 65870.88088583754\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0022763459525174563\n",
      "Average test loss: 10907.46290953084\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0021409162491973902\n",
      "Average test loss: 7714.858883962065\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0021099145635962487\n",
      "Average test loss: 69867.30425826687\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002093674582118789\n",
      "Average test loss: 40958.77565013591\n",
      "Epoch 253/300\n",
      "Average training loss: 0.002089524955281781\n",
      "Average test loss: 48367.823994309605\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0020774416563411552\n",
      "Average test loss: 78086.74412522861\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002090659672808316\n",
      "Average test loss: 155382522.85441646\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0021020342734538846\n",
      "Average test loss: 3276859.4468321186\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0021195280347019434\n",
      "Average test loss: 9462954.628810506\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0022169186307324303\n",
      "Average test loss: 18893469.285396613\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0021690189426557885\n",
      "Average test loss: 136271.75311948397\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0020588550517956416\n",
      "Average test loss: 149862.01685708298\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002066559394614564\n",
      "Average test loss: 37358012.29583434\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0022761794303854306\n",
      "Average test loss: 6.493096046475487\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0020672934869718222\n",
      "Average test loss: 210.7319228154959\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0020503047265940243\n",
      "Average test loss: 15.83465951540114\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0020411771194388468\n",
      "Average test loss: 110265.05276794288\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002039160313705603\n",
      "Average test loss: 3725782.9372222223\n",
      "Epoch 267/300\n",
      "Average training loss: 0.002053750537025432\n",
      "Average test loss: 3615.9053976692535\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0020691417147301965\n",
      "Average test loss: 117.89054710896562\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0020744371465924714\n",
      "Average test loss: 4044.9016142001683\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0020484341360214684\n",
      "Average test loss: 1442968.366357639\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0020331466370779607\n",
      "Average test loss: 211.296733005093\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0020367044961700836\n",
      "Average test loss: 378254.11646043375\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0020683536091819403\n",
      "Average test loss: 81395.24257039942\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002627746027169956\n",
      "Average test loss: 47366.087109433975\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0020948377959430216\n",
      "Average test loss: 347842.04354309983\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0020308386424763335\n",
      "Average test loss: 2695288.881137057\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002039056462339229\n",
      "Average test loss: 79002032.96324067\n",
      "Epoch 278/300\n",
      "Average training loss: 0.002033526973074509\n",
      "Average test loss: 408881.4514755554\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0020349978282012873\n",
      "Average test loss: 26154312.171965815\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0020465172882088356\n",
      "Average test loss: 62022.94440790041\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0020329466017170086\n",
      "Average test loss: 29.30983091904368\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0020387290436774492\n",
      "Average test loss: 1891.7732369799232\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0020284259182711444\n",
      "Average test loss: 135442.39909974817\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0021716289817252095\n",
      "Average test loss: 77.49970469665176\n",
      "Epoch 285/300\n",
      "Average training loss: 0.002308915249796377\n",
      "Average test loss: 131.77369445691755\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0020735351677156157\n",
      "Average test loss: 10737.676630873795\n",
      "Epoch 287/300\n",
      "Average training loss: 0.002033064826702078\n",
      "Average test loss: 480737.84430685226\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002035778218259414\n",
      "Average test loss: 13415878.30550509\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0020452999450887243\n",
      "Average test loss: 228197.32970292706\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0020225431898401843\n",
      "Average test loss: 50018.575958238536\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00200898519748201\n",
      "Average test loss: 47701.091978922304\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002016179016687804\n",
      "Average test loss: 1495.5728287990662\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002023232875391841\n",
      "Average test loss: 132162.9535819077\n",
      "Epoch 294/300\n",
      "Average training loss: 0.002015218672239118\n",
      "Average test loss: 23254.996463378124\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0020082867675874794\n",
      "Average test loss: 199510.06900211828\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002044812285134362\n",
      "Average test loss: 140.16640823369556\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0020071189581520026\n",
      "Average test loss: 6393250.966781952\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0019954615160822867\n",
      "Average test loss: 265235.09751969477\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0020164645028611025\n",
      "Average test loss: 3335533.3083410556\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0019946719008601373\n",
      "Average test loss: 932473.4952274306\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05800551179879242\n",
      "Average test loss: 0.023474753895567524\n",
      "Epoch 2/300\n",
      "Average training loss: 0.007330105289402936\n",
      "Average test loss: 0.006576767058008247\n",
      "Epoch 3/300\n",
      "Average training loss: 0.006091523884071244\n",
      "Average test loss: 0.005830192116813527\n",
      "Epoch 4/300\n",
      "Average training loss: 0.005408633271025287\n",
      "Average test loss: 0.004929251573565933\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0049299746615191304\n",
      "Average test loss: 0.008943298913124535\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004493647509151035\n",
      "Average test loss: 0.0041045926792754065\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004159446476648251\n",
      "Average test loss: 0.007578830679257711\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0038870393404116235\n",
      "Average test loss: 0.17524451341500713\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0036691368632018567\n",
      "Average test loss: 0.7227599835122624\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0034930885701129834\n",
      "Average test loss: 0.09155097052951654\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0033353862998386223\n",
      "Average test loss: 0.005301718482954634\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003181506470673614\n",
      "Average test loss: 0.19987341232101122\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0030570849308537112\n",
      "Average test loss: 0.010648122496489021\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002944885236935483\n",
      "Average test loss: 0.7443466103043821\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0028312824194629987\n",
      "Average test loss: 0.17454493506501118\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0027384990422676006\n",
      "Average test loss: 0.30737834300928646\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0026595271955140764\n",
      "Average test loss: 0.1212109217285696\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0025897358874272967\n",
      "Average test loss: 0.5849473776655892\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0025306325163692235\n",
      "Average test loss: 75.3707421874884\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0024605283975187276\n",
      "Average test loss: 0.01728189755934808\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002400014432768027\n",
      "Average test loss: 12.140186608627646\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0023557414044108656\n",
      "Average test loss: 4640.092162418747\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0023134871028984588\n",
      "Average test loss: 24.063489617501283\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0022647783688993918\n",
      "Average test loss: 10.91551882770906\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002220455767483347\n",
      "Average test loss: 154.50248489416143\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002202854068121976\n",
      "Average test loss: 0.24589736443406177\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0021828502393845053\n",
      "Average test loss: 305.1938783375919\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002122219875247942\n",
      "Average test loss: 0.40938282732727627\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004425931963345243\n",
      "Average test loss: 0.11160045573653446\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0025752241654942434\n",
      "Average test loss: 1.0076020328634314\n",
      "Epoch 31/300\n",
      "Average training loss: 0.002371927141936289\n",
      "Average test loss: 8.336541700034505\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0022580293108605677\n",
      "Average test loss: 181.2604484123778\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0022095327327648798\n",
      "Average test loss: 0.7101543830815289\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00215518225905382\n",
      "Average test loss: 24.717159006459017\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0021687526361395914\n",
      "Average test loss: 306.9062161028054\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0021202947383539544\n",
      "Average test loss: 32017.7526425905\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0020839014639043147\n",
      "Average test loss: 197.33259222136437\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0021089271656754946\n",
      "Average test loss: 13.397838612419243\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002101838925646411\n",
      "Average test loss: 7267.495200850712\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002059951850730512\n",
      "Average test loss: 37960.03587922758\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009400975439180103\n",
      "Average test loss: 2.8217022653329704\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0033522937349561186\n",
      "Average test loss: 0.002721811063380705\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0028441813602629634\n",
      "Average test loss: 0.08862681758838395\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002666364956440197\n",
      "Average test loss: 208.80101954515942\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0025255774423066112\n",
      "Average test loss: 0.021659880806381503\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002497823245409462\n",
      "Average test loss: 190.72774098127996\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00246729459023724\n",
      "Average test loss: 500.4518071156433\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0023456619784442915\n",
      "Average test loss: 1.4586344204553299\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0031201102821570305\n",
      "Average test loss: 4.917145979588231\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0023383114294459424\n",
      "Average test loss: 782.5124841715239\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002272987621008522\n",
      "Average test loss: 0.17687878039324034\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0022220972255907125\n",
      "Average test loss: 1053753.13425\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0021888440751160186\n",
      "Average test loss: 2138.2099521655896\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0025948774894285534\n",
      "Average test loss: 2.013479600937002\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0023283092313342623\n",
      "Average test loss: 12.115005277772124\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0021522236649567884\n",
      "Average test loss: 1696.9879583839327\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002159477803338733\n",
      "Average test loss: 6574.1268519023415\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0021870182839532695\n",
      "Average test loss: 1043.2801855043867\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0022125510696528686\n",
      "Average test loss: 11.990190357524074\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0021585879952957234\n",
      "Average test loss: 101.53325517864525\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002575015431270003\n",
      "Average test loss: 0.08129824071004987\n",
      "Epoch 62/300\n",
      "Average training loss: 0.00222444010205153\n",
      "Average test loss: 0.0022662947626991402\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0022096421294328238\n",
      "Average test loss: 2466.196748533618\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002110787652018997\n",
      "Average test loss: 2.784234576791525\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002047997892834246\n",
      "Average test loss: 272235.1296527778\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0020156831292228567\n",
      "Average test loss: 0.020442567195743323\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002015927554625604\n",
      "Average test loss: 2.469747182148819\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0027937187370326785\n",
      "Average test loss: 0.016901737424027587\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00207711157689078\n",
      "Average test loss: 0.5023903250346581\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0020128172202449708\n",
      "Average test loss: 8.369564158293937\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0019847879570184484\n",
      "Average test loss: 7.926279700581398\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0019664462608181767\n",
      "Average test loss: 20.100606075363853\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002002509426842961\n",
      "Average test loss: 0.008030187408543296\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002069594929719137\n",
      "Average test loss: 559.5571649780273\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002048731635324657\n",
      "Average test loss: 0.0024700446646246646\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0021377887481616605\n",
      "Average test loss: 15.78696887258523\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002080746963620186\n",
      "Average test loss: 0.35216772995226914\n",
      "Epoch 78/300\n",
      "Average training loss: 0.001954388085442285\n",
      "Average test loss: 0.10188763138982984\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0019371105855744747\n",
      "Average test loss: 13.909973216864797\n",
      "Epoch 80/300\n",
      "Average training loss: 0.00191920843679044\n",
      "Average test loss: 31.82265021085377\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002400795020783941\n",
      "Average test loss: 5.269252963130673\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0020454735159873964\n",
      "Average test loss: 13.247380940027535\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002003426605525116\n",
      "Average test loss: 4.956031974057357\n",
      "Epoch 84/300\n",
      "Average training loss: 0.001984216945556303\n",
      "Average test loss: 1.5522122978290749\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002036097570012013\n",
      "Average test loss: 0.0024832562444110713\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0020181490197363826\n",
      "Average test loss: 0.015698529220496613\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0019787264585288036\n",
      "Average test loss: 0.3856143424337109\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0019520232108318142\n",
      "Average test loss: 0.28340374392902273\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0019055267622073492\n",
      "Average test loss: 0.04791911922002004\n",
      "Epoch 90/300\n",
      "Average training loss: 0.001936641626784371\n",
      "Average test loss: 0.003587170346743531\n",
      "Epoch 91/300\n",
      "Average training loss: 0.001970255001551575\n",
      "Average test loss: 0.09675817530995441\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0019755254155024884\n",
      "Average test loss: 0.0028190770194762285\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0019012636416074302\n",
      "Average test loss: 6.966907379625572\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0018913421842993961\n",
      "Average test loss: 7952.470573755903\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0018807137712008423\n",
      "Average test loss: 0.03534943373128772\n",
      "Epoch 96/300\n",
      "Average training loss: 0.001900200937046773\n",
      "Average test loss: 22.482421495772897\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0018534725681982106\n",
      "Average test loss: 36.80124031363262\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0020741545013669466\n",
      "Average test loss: 2896.647973152125\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0018752126495043438\n",
      "Average test loss: 16.68420067979354\n",
      "Epoch 100/300\n",
      "Average training loss: 0.001838083138482438\n",
      "Average test loss: 1674.4973922012753\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0018256248706537816\n",
      "Average test loss: 0.003793647574260831\n",
      "Epoch 102/300\n",
      "Average training loss: 0.001822286777353535\n",
      "Average test loss: 3494.09286423688\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0018251727127159636\n",
      "Average test loss: 0.002239799317179455\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0018290120830966366\n",
      "Average test loss: 1036.152485213624\n",
      "Epoch 105/300\n",
      "Average training loss: 0.001805861953025063\n",
      "Average test loss: 92.366168856889\n",
      "Epoch 106/300\n",
      "Average training loss: 0.001844231882546511\n",
      "Average test loss: 0.32881511753352566\n",
      "Epoch 107/300\n",
      "Average training loss: 0.001815472283711036\n",
      "Average test loss: 884.335972609986\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0017915860151665078\n",
      "Average test loss: 2517.1080643867494\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0018280630982998344\n",
      "Average test loss: 133.5107713112318\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0017961873054607874\n",
      "Average test loss: 292.68838206039555\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0017560282999442684\n",
      "Average test loss: 13935.87241414139\n",
      "Epoch 112/300\n",
      "Average training loss: 0.001759828681126237\n",
      "Average test loss: 239.75852153266635\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0017541764771772756\n",
      "Average test loss: 4152.644000301866\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0017892481901993355\n",
      "Average test loss: 44.59990231031842\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0017528208694938157\n",
      "Average test loss: 78.04453348805838\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0017412435855302547\n",
      "Average test loss: 5639.130499688411\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0018661426708309187\n",
      "Average test loss: 0.011704891810245399\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0017357127419155504\n",
      "Average test loss: 1.898918748519487\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0017238644907871883\n",
      "Average test loss: 569.1148887533518\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0017068177855884036\n",
      "Average test loss: 25257.73870733421\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0024296891687230933\n",
      "Average test loss: 8.616481445736355\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002065800283725063\n",
      "Average test loss: 46.02938649640067\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0018645449976126354\n",
      "Average test loss: 27.62210018786664\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0017921314026332564\n",
      "Average test loss: 87.4625363556577\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0017515141393782364\n",
      "Average test loss: 1.9704178281007334\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0017324389630100794\n",
      "Average test loss: 209.7829006396081\n",
      "Epoch 127/300\n",
      "Average training loss: 0.001718140091229644\n",
      "Average test loss: 208.83514481963817\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0017137965310571922\n",
      "Average test loss: 734009.8356658368\n",
      "Epoch 129/300\n",
      "Average training loss: 0.001698548415572279\n",
      "Average test loss: 147.2342562398091\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0016944983234215114\n",
      "Average test loss: 6386858.600593991\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0016946971183642745\n",
      "Average test loss: 1498.4461458400644\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0017157499478715988\n",
      "Average test loss: 3217.8006896334414\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0016815105718043116\n",
      "Average test loss: 62039.40542225893\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0017045845202066832\n",
      "Average test loss: 823311.1852638889\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0017053466193998853\n",
      "Average test loss: 12121.34026925739\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0018237841739836666\n",
      "Average test loss: 1.099055232670365\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0017442515210972893\n",
      "Average test loss: 3297.7339522108427\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0016893442842281527\n",
      "Average test loss: 8.497710601896047\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0016608232201801407\n",
      "Average test loss: 8424.520717563004\n",
      "Epoch 140/300\n",
      "Average training loss: 0.001660277060336537\n",
      "Average test loss: 795.9964244956598\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0016716522343663705\n",
      "Average test loss: 16595.010684645316\n",
      "Epoch 142/300\n",
      "Average training loss: 0.001655027715799709\n",
      "Average test loss: 8677.271353417447\n",
      "Epoch 143/300\n",
      "Average training loss: 0.001643982121306989\n",
      "Average test loss: 7990.344032513724\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0016727472097716398\n",
      "Average test loss: 3638.063282066606\n",
      "Epoch 145/300\n",
      "Average training loss: 0.001645602914504707\n",
      "Average test loss: 1740985.4707948938\n",
      "Epoch 146/300\n",
      "Average training loss: 0.001629572678771284\n",
      "Average test loss: 796.1435734065486\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0016251511114339034\n",
      "Average test loss: 98740.06625549788\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0016564540214215717\n",
      "Average test loss: 33.38577413383954\n",
      "Epoch 149/300\n",
      "Average training loss: 0.00166970020884441\n",
      "Average test loss: 3478.538721534793\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0016714572820605504\n",
      "Average test loss: 3290.8871600912876\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001621316499490705\n",
      "Average test loss: 188.89821140489127\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0016306353379040956\n",
      "Average test loss: 4864.549809376149\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0016512886465837558\n",
      "Average test loss: 0.11907400988911589\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0016358341480501824\n",
      "Average test loss: 3383.9235022322728\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0016121955741610792\n",
      "Average test loss: 7190.885030161587\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0017738850373360845\n",
      "Average test loss: 5.586167405653331\n",
      "Epoch 157/300\n",
      "Average training loss: 0.001659377858456638\n",
      "Average test loss: 32521.27789438896\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001618195773412784\n",
      "Average test loss: 149.34321690072284\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0016044238620541162\n",
      "Average test loss: 2848.354446642558\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0016033793409458465\n",
      "Average test loss: 1096727.292195004\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0015967889961062205\n",
      "Average test loss: 10044.525714584403\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0015873100261928307\n",
      "Average test loss: 5636.622706397068\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0016035463505735\n",
      "Average test loss: 9887.71356009547\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0017827639714297322\n",
      "Average test loss: 41.495486750114296\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0015891611060748498\n",
      "Average test loss: 390.3387088253804\n",
      "Epoch 166/300\n",
      "Average training loss: 0.001578939750066234\n",
      "Average test loss: 121.38119048695877\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0015695225665759709\n",
      "Average test loss: 13655.80945589439\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0015687763620581892\n",
      "Average test loss: 3371.4547078106198\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0015748043481467499\n",
      "Average test loss: 22868.351219180473\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0015777650997042655\n",
      "Average test loss: 4010.2611391486253\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0015797086629188723\n",
      "Average test loss: 7.45866560721563\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0015698419336436524\n",
      "Average test loss: 2584.093805716659\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0015494433605215616\n",
      "Average test loss: 52.32949666475029\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0015609294418245554\n",
      "Average test loss: 5816.859158064015\n",
      "Epoch 175/300\n",
      "Average training loss: 0.001557592843245301\n",
      "Average test loss: 121573.85975623915\n",
      "Epoch 176/300\n",
      "Average training loss: 0.001559361238963902\n",
      "Average test loss: 364.7578414261159\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0015421277390172084\n",
      "Average test loss: 438.3936845769187\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0015311229144119554\n",
      "Average test loss: 8094.495672524737\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0015352451825100515\n",
      "Average test loss: 130398.68850991863\n",
      "Epoch 180/300\n",
      "Average training loss: 0.001535599205022057\n",
      "Average test loss: 159372.83978472222\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0015217787538551622\n",
      "Average test loss: 3898.9762071778873\n",
      "Epoch 182/300\n",
      "Average training loss: 0.001541661699405975\n",
      "Average test loss: 21127.055930314993\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0015097394844310152\n",
      "Average test loss: 82.38244909556738\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0015141052667879396\n",
      "Average test loss: 1646.782064078182\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0015344779349656568\n",
      "Average test loss: 437.39513374388423\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0015147705835600693\n",
      "Average test loss: 1907.8744327294053\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0015281541779016456\n",
      "Average test loss: 0.013589940504481396\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0014953180944754017\n",
      "Average test loss: 19844.88598844864\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0014855022558735477\n",
      "Average test loss: 28552.47640418026\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001501927534635696\n",
      "Average test loss: 166.4541641489855\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0014937602873477671\n",
      "Average test loss: 56522.18113627399\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0014951846534386277\n",
      "Average test loss: 43290.03741487522\n",
      "Epoch 193/300\n",
      "Average training loss: 0.001486257730051875\n",
      "Average test loss: 37.85420532519722\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0014888672510989839\n",
      "Average test loss: 10550.068214952615\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0014881305133716929\n",
      "Average test loss: 38051.56970604741\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0014851889462313718\n",
      "Average test loss: 1816.6951988989442\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0015024167509335611\n",
      "Average test loss: 16505.816343745344\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0014755330481566488\n",
      "Average test loss: 0.4496918588866376\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0014720586139398317\n",
      "Average test loss: 103.86708852976798\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0014743547578238779\n",
      "Average test loss: 3465.649561169867\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0014709631550229257\n",
      "Average test loss: 42405.201016096435\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0014668223944803078\n",
      "Average test loss: 2485.9176659046625\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0014598577770197558\n",
      "Average test loss: 64518.34267374321\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0014567995416000486\n",
      "Average test loss: 225.53048557354987\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0014776522208833033\n",
      "Average test loss: 5736.447131340611\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0014571163277659152\n",
      "Average test loss: 345917.9778904021\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0014504899211848776\n",
      "Average test loss: 2580.8251712261545\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0015788884246721864\n",
      "Average test loss: 1794.645389499853\n",
      "Epoch 209/300\n",
      "Average training loss: 0.001488892500495745\n",
      "Average test loss: 275.0346939718914\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0014533100821491744\n",
      "Average test loss: 398.03020285413965\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0014605610583805376\n",
      "Average test loss: 826.4425505362427\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0014582988109646572\n",
      "Average test loss: 0.6115924506303337\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0014530978586731686\n",
      "Average test loss: 9360.518242065464\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0014420345335577926\n",
      "Average test loss: 7057.462390051941\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0014366952335048053\n",
      "Average test loss: 173.3508433433706\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001442375449463725\n",
      "Average test loss: 2311.92979308822\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0014364178485961423\n",
      "Average test loss: 3010659.725129674\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0014501786258899504\n",
      "Average test loss: 185373.51662117438\n",
      "Epoch 219/300\n",
      "Average training loss: 0.001427080084466272\n",
      "Average test loss: 471315.61445284367\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0014431533712583283\n",
      "Average test loss: 25279.781205512154\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0014373892122465702\n",
      "Average test loss: 37.982989467415024\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0014360807641512817\n",
      "Average test loss: 322.1004026942253\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0014364975423862537\n",
      "Average test loss: 93393.62220766579\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0014382323497492406\n",
      "Average test loss: 2572.1773785273426\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0014103048922907975\n",
      "Average test loss: 88546.14657734743\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0014276773303540217\n",
      "Average test loss: 28065.532668621367\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0014068304081964824\n",
      "Average test loss: 1236.5699192867535\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0014336663966791497\n",
      "Average test loss: 321.7575988783182\n",
      "Epoch 229/300\n",
      "Average training loss: 0.001419081539536516\n",
      "Average test loss: 65.11293461721142\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0014055101809402307\n",
      "Average test loss: 200.39610752391403\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0014381860138641463\n",
      "Average test loss: 4604.8946773615635\n",
      "Epoch 232/300\n",
      "Average training loss: 0.001412636703501145\n",
      "Average test loss: 6519.188620369093\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0013994053091026014\n",
      "Average test loss: 131.6535890136895\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0014070305365862117\n",
      "Average test loss: 64.15929334496055\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0013946266177420814\n",
      "Average test loss: 35.200766156958935\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0014073450117268497\n",
      "Average test loss: 404.2616360487388\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0014049863475892278\n",
      "Average test loss: 0.08902788183010286\n",
      "Epoch 238/300\n",
      "Average training loss: 0.001426367752874891\n",
      "Average test loss: 0.5295262126239637\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0014223596594399876\n",
      "Average test loss: 34.28991746818968\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0014211381675882473\n",
      "Average test loss: 5.3711306942784125\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0013965848720529012\n",
      "Average test loss: 2.8791935983633414\n",
      "Epoch 242/300\n",
      "Average training loss: 0.001390439598976324\n",
      "Average test loss: 205.82152128240963\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0013990267472755579\n",
      "Average test loss: 1689.5059408926509\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0014621870587062504\n",
      "Average test loss: 24269.43386740619\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0014598699230700732\n",
      "Average test loss: 1452743.7044178294\n",
      "Epoch 246/300\n",
      "Average training loss: 0.001387671960828205\n",
      "Average test loss: 266495.5052357763\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0013901389528893763\n",
      "Average test loss: 0.846296432989132\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0013779333056882024\n",
      "Average test loss: 56927.85503515625\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0013858785928330488\n",
      "Average test loss: 96339.8772595821\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0013965309988707305\n",
      "Average test loss: 1085.4187037598374\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0013798047937452792\n",
      "Average test loss: 16889.857934666532\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0013947474080034429\n",
      "Average test loss: 25790.55932378107\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0014049453360752927\n",
      "Average test loss: 29102.947230681122\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0013803393157819908\n",
      "Average test loss: 371305.5617099609\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0013691919506010081\n",
      "Average test loss: 93890.68530647096\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0014135867497987217\n",
      "Average test loss: 10719.9794789458\n",
      "Epoch 257/300\n",
      "Average training loss: 0.001360093664067487\n",
      "Average test loss: 84489.64739233658\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0013576504309765166\n",
      "Average test loss: 59752.32637460796\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0013696403036721877\n",
      "Average test loss: 63.77618287876704\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0013650620135789117\n",
      "Average test loss: 5251.6137901909115\n",
      "Epoch 261/300\n",
      "Average training loss: 0.001380634680079917\n",
      "Average test loss: 1685.8891435449075\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0013598822866463\n",
      "Average test loss: 68669.90397585901\n",
      "Epoch 263/300\n",
      "Average training loss: 0.001351176753329734\n",
      "Average test loss: 110793.6712734188\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0013608552939775916\n",
      "Average test loss: 0.40957117238971924\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0013709521140489313\n",
      "Average test loss: 3647.88203310927\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0013569971529973877\n",
      "Average test loss: 82.21616370856648\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0013713949661081036\n",
      "Average test loss: 1999.1399042956145\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0014208978775681722\n",
      "Average test loss: 3.6762748375773016\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0013880862515523202\n",
      "Average test loss: 0.03297741155378107\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0013584253332681127\n",
      "Average test loss: 215.59973352727957\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0013581754382078846\n",
      "Average test loss: 392.2964409959349\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0013650204378904567\n",
      "Average test loss: 749.9553761612624\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00135297980842491\n",
      "Average test loss: 24.009429332451273\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0014171933017464147\n",
      "Average test loss: 418.53638072039496\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0013513300132213368\n",
      "Average test loss: 95.29249138903452\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0013503682990041045\n",
      "Average test loss: 720.8307833964585\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0013467081583415468\n",
      "Average test loss: 3923.6631643548294\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0013454122341548403\n",
      "Average test loss: 461.59101813847326\n",
      "Epoch 279/300\n",
      "Average training loss: 0.001353194559096462\n",
      "Average test loss: 2987.3896752149726\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0013432971988287237\n",
      "Average test loss: 35807.402870207356\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0013648615412207112\n",
      "Average test loss: 6210.895874403051\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0013451647829885284\n",
      "Average test loss: 2856.18952806499\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0013274565083492133\n",
      "Average test loss: 2240.815580108573\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0013362636073595948\n",
      "Average test loss: 772.295940477819\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0013402175162401464\n",
      "Average test loss: 24326.655790224308\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0013358846403037508\n",
      "Average test loss: 3372801.8756306283\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0013312263307679031\n",
      "Average test loss: 874.3049016736851\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0013309874047214786\n",
      "Average test loss: 4.805427564234783\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0013413597206688589\n",
      "Average test loss: 83581.70639299872\n",
      "Epoch 290/300\n",
      "Average training loss: 0.11705576646679805\n",
      "Average test loss: 0.41342383814520306\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0030679302180392875\n",
      "Average test loss: 0.44073340067660643\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0027367145381867884\n",
      "Average test loss: 22.24915098423759\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0025582155976444482\n",
      "Average test loss: 15.702013785580794\n",
      "Epoch 294/300\n",
      "Average training loss: 0.002440282589652472\n",
      "Average test loss: 1.5692398854165028\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0023309230342921285\n",
      "Average test loss: 114.50188803173933\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002258404530998733\n",
      "Average test loss: 274.50292816659476\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0021869451217353345\n",
      "Average test loss: 8.545261596250038\n",
      "Epoch 298/300\n",
      "Average training loss: 0.002121132007489602\n",
      "Average test loss: 4.5526787136743465\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0020724378306832577\n",
      "Average test loss: 107.69885585751219\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0020213543101110392\n",
      "Average test loss: 16404.342690305122\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06964273548374574\n",
      "Average test loss: 0.0587073301370773\n",
      "Epoch 2/300\n",
      "Average training loss: 0.006095329942388667\n",
      "Average test loss: 12.596767594224877\n",
      "Epoch 3/300\n",
      "Average training loss: 0.004932065717255076\n",
      "Average test loss: 0.004873814871327745\n",
      "Epoch 4/300\n",
      "Average training loss: 0.004347175129585796\n",
      "Average test loss: 0.0038984448317852284\n",
      "Epoch 5/300\n",
      "Average training loss: 0.00394353426165051\n",
      "Average test loss: 0.005203839973443084\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0036341523341834546\n",
      "Average test loss: 0.0035841788297726047\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0033858329204635488\n",
      "Average test loss: 0.2852372010234329\n",
      "Epoch 8/300\n",
      "Average training loss: 0.003173181872607933\n",
      "Average test loss: 0.07067210506958266\n",
      "Epoch 9/300\n",
      "Average training loss: 0.002999560041146146\n",
      "Average test loss: 0.007656012484596835\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0028561410187847086\n",
      "Average test loss: 0.48394666966464783\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0027186594906573494\n",
      "Average test loss: 0.0026101163967202105\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0025950243123289613\n",
      "Average test loss: 1.13348094866342\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0024946668737878402\n",
      "Average test loss: 0.00971567942160699\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002414318861025903\n",
      "Average test loss: 0.046438953582611346\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0023089738589608007\n",
      "Average test loss: 0.8193717348981234\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0022494039753865862\n",
      "Average test loss: 22.657873199596175\n",
      "Epoch 17/300\n",
      "Average training loss: 0.002178141193257438\n",
      "Average test loss: 0.023564243565003078\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0021056966743328506\n",
      "Average test loss: 1079.459617537703\n",
      "Epoch 19/300\n",
      "Average training loss: 0.002049571548381613\n",
      "Average test loss: 2425.193358339098\n",
      "Epoch 20/300\n",
      "Average training loss: 0.001996098737542828\n",
      "Average test loss: 114.91020946314269\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0019449418122983642\n",
      "Average test loss: 17.153312617683575\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0019090178807576498\n",
      "Average test loss: 40.32408326428735\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0018574013587915235\n",
      "Average test loss: 10.485690541315462\n",
      "Epoch 24/300\n",
      "Average training loss: 0.001833272146474984\n",
      "Average test loss: 1382.6384228010913\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0017788525096451243\n",
      "Average test loss: 0.011182773329938452\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0017322088598998057\n",
      "Average test loss: 26.796536047009543\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0016940783518883918\n",
      "Average test loss: 18.61411507597152\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0016624864102858636\n",
      "Average test loss: 2.4361965676645436\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0016217382175640928\n",
      "Average test loss: 35.44678205275743\n",
      "Epoch 30/300\n",
      "Average training loss: 0.001636287582003408\n",
      "Average test loss: 0.1607448108833697\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0015766629052037994\n",
      "Average test loss: 8.315246824781514\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0015418359369246497\n",
      "Average test loss: 33.74628243436085\n",
      "Epoch 33/300\n",
      "Average training loss: 0.001544209526437852\n",
      "Average test loss: 0.11307848959799029\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0015010545308598213\n",
      "Average test loss: 3.4281127268270484\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0014644604557090336\n",
      "Average test loss: 2633.4797086289477\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0014607005972001289\n",
      "Average test loss: 0.0015573017371611462\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0014247228106380337\n",
      "Average test loss: 51.49548141821515\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0014989722599792813\n",
      "Average test loss: 44.75591463056041\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0014370096488338377\n",
      "Average test loss: 0.00814236275292933\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0013984571336251167\n",
      "Average test loss: 24.032579673010854\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0014011912077354887\n",
      "Average test loss: 0.1318244598467524\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0013622007376721335\n",
      "Average test loss: 380.8350066457279\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0014738663094532158\n",
      "Average test loss: 0.007802258962765336\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0013440579449137053\n",
      "Average test loss: 0.015731302063498234\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0013271650107991365\n",
      "Average test loss: 0.5847346936368073\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0013231629787219894\n",
      "Average test loss: 332.2112641979936\n",
      "Epoch 47/300\n",
      "Average training loss: 0.001321786034697046\n",
      "Average test loss: 38.958030965110495\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002131532755576902\n",
      "Average test loss: 0.0017991263025647236\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0015791574802715331\n",
      "Average test loss: 25.807739515474996\n",
      "Epoch 50/300\n",
      "Average training loss: 0.001428051070931057\n",
      "Average test loss: 96.53365891121783\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0015024210900689164\n",
      "Average test loss: 0.09697789864873306\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0014060193167792427\n",
      "Average test loss: 2.470492064870066\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0014090620105465252\n",
      "Average test loss: 141.26725867524618\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0013561048488029176\n",
      "Average test loss: 110.33713148940645\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0013461740037633313\n",
      "Average test loss: 0.9873634835831407\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0013212785987804334\n",
      "Average test loss: 199.54982557261073\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0013111935435897774\n",
      "Average test loss: 0.9398828362404472\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0012970202832172314\n",
      "Average test loss: 215.06994373909373\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0012828125396432976\n",
      "Average test loss: 1.4190341223859124\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0013591520269918772\n",
      "Average test loss: 0.07626204846551021\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0013527955807124575\n",
      "Average test loss: 365.3217514717761\n",
      "Epoch 62/300\n",
      "Average training loss: 0.001276434963931226\n",
      "Average test loss: 0.2834192177933744\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0012518155019109448\n",
      "Average test loss: 8969.419614130105\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0012420457077936993\n",
      "Average test loss: 47.94404442029943\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0012292563290231759\n",
      "Average test loss: 644.3874305396097\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0012142584895094236\n",
      "Average test loss: 3.0186615769013554\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0012408855270283918\n",
      "Average test loss: 1.5356574691217393\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0012079285065540008\n",
      "Average test loss: 1492.0169055391534\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005927994531786276\n",
      "Average test loss: 2.886498491548002\n",
      "Epoch 70/300\n",
      "Average training loss: 0.001761901580211189\n",
      "Average test loss: 116.58737447272406\n",
      "Epoch 71/300\n",
      "Average training loss: 0.001592195078006221\n",
      "Average test loss: 87.63394814399386\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0015057519713623656\n",
      "Average test loss: 16.024867032474113\n",
      "Epoch 73/300\n",
      "Average training loss: 0.001440922481421795\n",
      "Average test loss: 0.9369156595170498\n",
      "Epoch 74/300\n",
      "Average training loss: 0.001406755068546368\n",
      "Average test loss: 1.0262753252569172\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0013691506275079317\n",
      "Average test loss: 46.914188405496795\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0013347880061094959\n",
      "Average test loss: 236.42981175420206\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0013171642746569383\n",
      "Average test loss: 8379.263536743403\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0012991441821472512\n",
      "Average test loss: 5.967699895947861\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0012788659832957718\n",
      "Average test loss: 18.923979590343517\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0012801951240334246\n",
      "Average test loss: 560.1026004061417\n",
      "Epoch 81/300\n",
      "Average training loss: 0.001261225070649137\n",
      "Average test loss: 26.90972991219784\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0012498877484144436\n",
      "Average test loss: 0.037574030046972136\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0012456581078780194\n",
      "Average test loss: 324.81810235128467\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0012222813702084951\n",
      "Average test loss: 93.17278985593686\n",
      "Epoch 85/300\n",
      "Average training loss: 0.001215173624507669\n",
      "Average test loss: 107.01717117310388\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0012530015251185332\n",
      "Average test loss: 49300.768582704746\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0013268085677797595\n",
      "Average test loss: 6.87312404719854\n",
      "Epoch 88/300\n",
      "Average training loss: 0.001200923909743627\n",
      "Average test loss: 0.0032158093688388666\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0012036149256552258\n",
      "Average test loss: 1674.692567536838\n",
      "Epoch 90/300\n",
      "Average training loss: 0.001210057131583906\n",
      "Average test loss: 1.7571653854724847\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0011814292308667468\n",
      "Average test loss: 54.175728903591526\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0012234613113105298\n",
      "Average test loss: 264.28677749172476\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0011714590352235568\n",
      "Average test loss: 24028.993665093316\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0011794038532922666\n",
      "Average test loss: 609.3772134847953\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0011771899713203312\n",
      "Average test loss: 1899.4563484945152\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0011972224038456463\n",
      "Average test loss: 0.20964466550490923\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0012021825909614562\n",
      "Average test loss: 7.293411249445958\n",
      "Epoch 98/300\n",
      "Average training loss: 0.001201859549412297\n",
      "Average test loss: 1.709071649746452\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0014558955893541376\n",
      "Average test loss: 1.533476289573229\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0012400092978754806\n",
      "Average test loss: 14.795404004160728\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001191681699672093\n",
      "Average test loss: 331.33862325020755\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0011708226727528705\n",
      "Average test loss: 4.056251268335618\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0011589784649097257\n",
      "Average test loss: 215.8631765132298\n",
      "Epoch 104/300\n",
      "Average training loss: 0.001163131926415695\n",
      "Average test loss: 27.854847438390056\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0011324401972297993\n",
      "Average test loss: 108.26242651090006\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0011442791076583996\n",
      "Average test loss: 10.097409062736563\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0011436580977299147\n",
      "Average test loss: 294250.5634583333\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0011280898712575436\n",
      "Average test loss: 195.9157210991606\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0011326079928419656\n",
      "Average test loss: 25.23777601009773\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0011165380127107103\n",
      "Average test loss: 1.1536041549460756\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0011122793892088035\n",
      "Average test loss: 898.4339645618676\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0011308022738537855\n",
      "Average test loss: 5.140313730159567\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0011469255382609035\n",
      "Average test loss: 198.99731158759496\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0011182592805061076\n",
      "Average test loss: 66.75882896550331\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0011078542869331109\n",
      "Average test loss: 2141508.169277778\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0011199322731958495\n",
      "Average test loss: 27632.802639019745\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0010854340173924962\n",
      "Average test loss: 678.159969026438\n",
      "Epoch 118/300\n",
      "Average training loss: 0.001099649374011076\n",
      "Average test loss: 131.8984245656406\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0010903367877213492\n",
      "Average test loss: 2373.6958668697303\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0013757570027891134\n",
      "Average test loss: 0.004054622069932521\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0011352312222847507\n",
      "Average test loss: 0.6114330850789944\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0011221536344124211\n",
      "Average test loss: 0.06706437970366742\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0010963267386477026\n",
      "Average test loss: 887.7487062574397\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0010949677181844083\n",
      "Average test loss: 0.08253777832724155\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0010787494311937026\n",
      "Average test loss: 4.162464490009265\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0010848859668605858\n",
      "Average test loss: 256.679059748448\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0010838869156108962\n",
      "Average test loss: 2.6988909666296093\n",
      "Epoch 128/300\n",
      "Average training loss: 0.001064184547867626\n",
      "Average test loss: 457.6686661596128\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0010710832396936085\n",
      "Average test loss: 0.005316280987734596\n",
      "Epoch 130/300\n",
      "Average training loss: 0.001095733885342876\n",
      "Average test loss: 26.541603180050437\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0010879812376159762\n",
      "Average test loss: 11.12595262690799\n",
      "Epoch 132/300\n",
      "Average training loss: 0.001067916746923907\n",
      "Average test loss: 0.3876009813522299\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0011010516192246642\n",
      "Average test loss: 0.006531057965527806\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0010493236717561054\n",
      "Average test loss: 3.5719232676312225\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0010424278008544612\n",
      "Average test loss: 19109.89837887127\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0016986673859258493\n",
      "Average test loss: 68.36722849629685\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0013490418519100382\n",
      "Average test loss: 390.3059175197243\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0012135827428040404\n",
      "Average test loss: 1077.8897583889398\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0011687698946851823\n",
      "Average test loss: 28.30222579647042\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0011514584257060455\n",
      "Average test loss: 0.0698580304744343\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001846783613993062\n",
      "Average test loss: 0.25617476073652506\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0014459528557749259\n",
      "Average test loss: 12.167005093876117\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0013341860035434366\n",
      "Average test loss: 0.6414358247212237\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0012775295057540967\n",
      "Average test loss: 0.2255495629492733\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0012418355878649487\n",
      "Average test loss: 0.9834824412859355\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0012180496953013871\n",
      "Average test loss: 13.796431177055464\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0011850888565596606\n",
      "Average test loss: 45.367884466632376\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0011614106677265632\n",
      "Average test loss: 1.5635659221181024\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0011397274735176729\n",
      "Average test loss: 285.94532921335434\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0011387616281604601\n",
      "Average test loss: 0.5285866654301062\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0011003216300159693\n",
      "Average test loss: 10.366359610541103\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0010983440072482658\n",
      "Average test loss: 2.5175689114996542\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0010872454024954802\n",
      "Average test loss: 5.932746395411487\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0010917862279651066\n",
      "Average test loss: 21.31158114345413\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0010897823870182038\n",
      "Average test loss: 0.2157961984626535\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0010828569166155325\n",
      "Average test loss: 13.168995184808141\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0011205205097277132\n",
      "Average test loss: 18963.46223290063\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0010693972903407282\n",
      "Average test loss: 0.04328869593039983\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0010589945463256704\n",
      "Average test loss: 0.3238143033844212\n",
      "Epoch 160/300\n",
      "Average training loss: 0.001057780261306713\n",
      "Average test loss: 8.925637529652565\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0010508254337021046\n",
      "Average test loss: 12.064653923689905\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0010507377317278748\n",
      "Average test loss: 0.0021148891417930526\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001062625274579558\n",
      "Average test loss: 0.03080764100038343\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0011556716828813984\n",
      "Average test loss: 0.022759547367691993\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0010469642482284042\n",
      "Average test loss: 0.02278459319141176\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0010385859138332307\n",
      "Average test loss: 1.9501488679817154\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0010392753656746612\n",
      "Average test loss: 12285.523078038275\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0010212114412441022\n",
      "Average test loss: 416.40856380090037\n",
      "Epoch 169/300\n",
      "Average training loss: 0.001027672321949568\n",
      "Average test loss: 11.7992698553159\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0010564045592521627\n",
      "Average test loss: 201.11968040732583\n",
      "Epoch 171/300\n",
      "Average training loss: 0.001012747973151919\n",
      "Average test loss: 0.01782191547068457\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0010048906419736644\n",
      "Average test loss: 18787.092487162274\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0010109141245484353\n",
      "Average test loss: 1.742180851124227\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0009919876228604051\n",
      "Average test loss: 58.76715317577786\n",
      "Epoch 175/300\n",
      "Average training loss: 0.001081479507084522\n",
      "Average test loss: 0.835481205985571\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0009964888205544816\n",
      "Average test loss: 394.5029877317713\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0010039547024708656\n",
      "Average test loss: 5234.686469018972\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0010412483899854123\n",
      "Average test loss: 483.7307518782566\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0010364124532788992\n",
      "Average test loss: 1.5704446707684547\n",
      "Epoch 180/300\n",
      "Average training loss: 0.000981727720496969\n",
      "Average test loss: 51.10823921452711\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00098164683378612\n",
      "Average test loss: 209.91259174754347\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0009893770264979037\n",
      "Average test loss: 587.3544274247879\n",
      "Epoch 183/300\n",
      "Average training loss: 0.00103077403212794\n",
      "Average test loss: 132.51668484725016\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0009828195380978287\n",
      "Average test loss: 179.6972669641331\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0009913864053992761\n",
      "Average test loss: 83.23733060997353\n",
      "Epoch 186/300\n",
      "Average training loss: 0.000977432723208848\n",
      "Average test loss: 0.021607640281319618\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0009858220235651567\n",
      "Average test loss: 75.8120261265172\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0009761094386275444\n",
      "Average test loss: 312.32230982159524\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0009666458514208595\n",
      "Average test loss: 11.22808791303107\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0009724726937193837\n",
      "Average test loss: 5613.005183088006\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0009676347735027472\n",
      "Average test loss: 2410.2262543579905\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0009634902715786464\n",
      "Average test loss: 160.11825437620283\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0009497220232151448\n",
      "Average test loss: 30.929518747378967\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0009531874189981156\n",
      "Average test loss: 294.02059583182216\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0009618738868480755\n",
      "Average test loss: 4.684444734433666\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0009864492731479306\n",
      "Average test loss: 0.004347773228978945\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0009570581124992006\n",
      "Average test loss: 11.463460096659553\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0009371369147362808\n",
      "Average test loss: 148.939757353824\n",
      "Epoch 199/300\n",
      "Average training loss: 0.000937558678444475\n",
      "Average test loss: 2074.611006286621\n",
      "Epoch 200/300\n",
      "Average training loss: 0.000939018764688323\n",
      "Average test loss: 27.61162831024391\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0009416348064421779\n",
      "Average test loss: 1853.2319603324802\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0010436937080489264\n",
      "Average test loss: 1.0568212177885903\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0010358226356717447\n",
      "Average test loss: 3.4389084341807497\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0009421532384326889\n",
      "Average test loss: 5.069154468153552\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0009330413631784419\n",
      "Average test loss: 681.3410206853727\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0009237599259552857\n",
      "Average test loss: 0.014720862917705543\n",
      "Epoch 207/300\n",
      "Average training loss: 0.000938965894954486\n",
      "Average test loss: 914.1402277473198\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0011017751756331158\n",
      "Average test loss: 101735.6991374849\n",
      "Epoch 209/300\n",
      "Average training loss: 0.000951283450341887\n",
      "Average test loss: 124.00016283268647\n",
      "Epoch 210/300\n",
      "Average training loss: 0.000932880587513662\n",
      "Average test loss: 21659.259651159755\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0009224290198956926\n",
      "Average test loss: 20138.407303430322\n",
      "Epoch 212/300\n",
      "Average training loss: 0.000928682578417162\n",
      "Average test loss: 1002.0900735676549\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0009358940568959548\n",
      "Average test loss: 211.15469404099798\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0010256336102158659\n",
      "Average test loss: 1.9553689262638283\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0009680197770810789\n",
      "Average test loss: 1887479.663999411\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0009616423315472073\n",
      "Average test loss: 9.82473252740999\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0010081219803541898\n",
      "Average test loss: 7.969278252071804\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0009476979051509665\n",
      "Average test loss: 19.854343798071145\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0009506586385994322\n",
      "Average test loss: 0.06713016273588356\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0009499209261913267\n",
      "Average test loss: 8.035569946716022\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0009994494845676754\n",
      "Average test loss: 3570.2906189737187\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0009205642539179987\n",
      "Average test loss: 193.71320066294487\n",
      "Epoch 223/300\n",
      "Average training loss: 0.001046601733089321\n",
      "Average test loss: 34.30578030501927\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0009407836676885684\n",
      "Average test loss: 19091.47843937128\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0009636249364767637\n",
      "Average test loss: 17.013931685590908\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0009292477319637934\n",
      "Average test loss: 14.84708405462756\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0009294307744130492\n",
      "Average test loss: 1548.279003411596\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0009230120973661542\n",
      "Average test loss: 54.49605066795151\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0009154771999973389\n",
      "Average test loss: 22.25626072800211\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00094532986678597\n",
      "Average test loss: 43.97421458706674\n",
      "Epoch 231/300\n",
      "Average training loss: 0.000902123137170242\n",
      "Average test loss: 65.85961376637619\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0009100304320971999\n",
      "Average test loss: 1661.501112077358\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0009128588356284631\n",
      "Average test loss: 9195.878026574675\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0008988261065031919\n",
      "Average test loss: 1.4347581412220995\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0009161251930312978\n",
      "Average test loss: 2.4838659185764587\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0008915223101050489\n",
      "Average test loss: 32993.5500212932\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0009247653908096254\n",
      "Average test loss: 10.429808508760969\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0009018390314756996\n",
      "Average test loss: 4.447237808689061\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0008987204635308849\n",
      "Average test loss: 1.3922116693674276\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0009031313858512375\n",
      "Average test loss: 468.51528027585067\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0009087368545846806\n",
      "Average test loss: 16.97717794798501\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0009030548855921047\n",
      "Average test loss: 0.1367318305182788\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0010221447413787245\n",
      "Average test loss: 157.74046285878535\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0010145738123295209\n",
      "Average test loss: 0.001621280761746069\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0009917388432141808\n",
      "Average test loss: 0.087718217615866\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0009439470123810072\n",
      "Average test loss: 0.0045299086546939284\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0009169931973641118\n",
      "Average test loss: 0.014907459278694458\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0009182067401707172\n",
      "Average test loss: 0.027847363454393214\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0009103543465025723\n",
      "Average test loss: 0.0029351230576220486\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0009027363516183363\n",
      "Average test loss: 0.8586652765290604\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0009257008159636623\n",
      "Average test loss: 0.02057621065982514\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0009081097815392746\n",
      "Average test loss: 15.41579948778347\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0009465178077419599\n",
      "Average test loss: 825.4263489875719\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0009023409470294912\n",
      "Average test loss: 265.3101194035469\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0008810929567553103\n",
      "Average test loss: 15.484155080670698\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0009112105069475041\n",
      "Average test loss: 2.9540197608070446\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0010202893044075204\n",
      "Average test loss: 0.29453373904940156\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0009257440069276425\n",
      "Average test loss: 0.0234452727890263\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0008830023752525449\n",
      "Average test loss: 109.74879666009421\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0008808507020067837\n",
      "Average test loss: 152.2458395937528\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0008818458989262581\n",
      "Average test loss: 2.9938879259577433\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0008690018581433428\n",
      "Average test loss: 239.10282302531363\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00087085476046842\n",
      "Average test loss: 147.98068378711957\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0008764064334229462\n",
      "Average test loss: 17059.54476014689\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0008737940659177386\n",
      "Average test loss: 848.9354935828677\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0009500867549536956\n",
      "Average test loss: 13.460460663850936\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0008727804316311247\n",
      "Average test loss: 3.248707185138431\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0008581860593209664\n",
      "Average test loss: 16.666707962598238\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0008535153910828134\n",
      "Average test loss: 52.83200222356773\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0008531221419365869\n",
      "Average test loss: 861.6997363783075\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0008672789734167358\n",
      "Average test loss: 0.5594117918282541\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0008547549149952829\n",
      "Average test loss: 41.77054671629901\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0008533888155490988\n",
      "Average test loss: 5.890810985092488\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0008520266778974069\n",
      "Average test loss: 842.5692980297754\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0008473359481948945\n",
      "Average test loss: 110.99491370591791\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0008443884259193308\n",
      "Average test loss: 1202.8485339753288\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0008558895822303991\n",
      "Average test loss: 3502.3922311881706\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0008455242324723965\n",
      "Average test loss: 26.512438866603397\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0008534822458297842\n",
      "Average test loss: 28.698531824605332\n",
      "Epoch 280/300\n",
      "Average training loss: 0.000845368003162245\n",
      "Average test loss: 56.658671167348\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0008450177629064355\n",
      "Average test loss: 1.061192193161696\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0009259171208056311\n",
      "Average test loss: 0.837423396907902\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0009185544927604497\n",
      "Average test loss: 1.7065413764727613\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0009015515855410033\n",
      "Average test loss: 9.28829419717027\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0008889526860374543\n",
      "Average test loss: 917.4775862685036\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0008636165349226859\n",
      "Average test loss: 0.01286847572018289\n",
      "Epoch 287/300\n",
      "Average training loss: 0.000864160906833907\n",
      "Average test loss: 0.0014981806975685888\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0008598065505114695\n",
      "Average test loss: 308.6815962083605\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0008534815138619808\n",
      "Average test loss: 1.4109229666983916\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0008726101367113491\n",
      "Average test loss: 0.6021590549751288\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0008520901739700801\n",
      "Average test loss: 0.04797195438212819\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0008619656843754153\n",
      "Average test loss: 0.3985861309737795\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0008564454197055763\n",
      "Average test loss: 822.7104302842882\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0008517133712044192\n",
      "Average test loss: 649.6048246290419\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0008636320827839275\n",
      "Average test loss: 546.0354336964798\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0008529876776867442\n",
      "Average test loss: 18.28784934323985\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0009074949513499936\n",
      "Average test loss: 0.14184061082866456\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0008588955935815143\n",
      "Average test loss: 42.35582643975152\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0008610925770157741\n",
      "Average test loss: 18.193881690290965\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0008563919577540623\n",
      "Average test loss: 235152.22801280685\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.01/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 4.74\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 5.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 8.75\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 11.86\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 20.56\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 14.78\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 22.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 14.36\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 23.04\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 19.08\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 23.54\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 17.16\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 9.11\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 13.77\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 14.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 19.71\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 21.74\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 14.00\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 22.08\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.96\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 13.82\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 21.53\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 23.57\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.68\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 0.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 3.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 8.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 13.20\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 14.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 17.94\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 16.49\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 20.42\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.35\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 21.23\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.69\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 2.89\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 5.55\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 10.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 13.55\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 18.00\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 23.12\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 18.56\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 20.09\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.52\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.66\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.34\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.65\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.17\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.22889834962785244\n",
      "Average test loss: 0.01605848488708337\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015127818225986427\n",
      "Average test loss: 0.03040643851624595\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013071574239267244\n",
      "Average test loss: 0.011299628311561213\n",
      "Epoch 4/300\n",
      "Average training loss: 0.011944857103129229\n",
      "Average test loss: 0.05596518149309688\n",
      "Epoch 5/300\n",
      "Average training loss: 0.010855406017767058\n",
      "Average test loss: 0.013229539595544338\n",
      "Epoch 6/300\n",
      "Average training loss: 0.009742928014861213\n",
      "Average test loss: 0.009616250503394338\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009103108912706374\n",
      "Average test loss: 0.601445934795671\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008728728805565172\n",
      "Average test loss: 0.19242968818545342\n",
      "Epoch 9/300\n",
      "Average training loss: 0.008457384222497543\n",
      "Average test loss: 0.018018843962086573\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008188242892424265\n",
      "Average test loss: 0.008396946568869882\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007930383273296886\n",
      "Average test loss: 0.008856384017815192\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007765555673589309\n",
      "Average test loss: 0.326608392490281\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007351974667774306\n",
      "Average test loss: 2.197953889735871\n",
      "Epoch 14/300\n",
      "Average training loss: 0.007122382717000114\n",
      "Average test loss: 0.006833318687147564\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04181567461457517\n",
      "Average test loss: 0.009185629204743438\n",
      "Epoch 16/300\n",
      "Average training loss: 0.008822341713640425\n",
      "Average test loss: 0.010381124462518427\n",
      "Epoch 17/300\n",
      "Average training loss: 0.008175014969375399\n",
      "Average test loss: 2.1397386369539633\n",
      "Epoch 18/300\n",
      "Average training loss: 0.007572165907257133\n",
      "Average test loss: 0.026016314273079235\n",
      "Epoch 19/300\n",
      "Average training loss: 0.007232487409479088\n",
      "Average test loss: 0.6811762263211939\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006997664228909546\n",
      "Average test loss: 0.7157925566467974\n",
      "Epoch 21/300\n",
      "Average training loss: 0.006912995676613516\n",
      "Average test loss: 0.8490623290638128\n",
      "Epoch 22/300\n",
      "Average training loss: 0.006809870321303606\n",
      "Average test loss: 4.062571612354782\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007033186367402474\n",
      "Average test loss: 0.5791898956331942\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006639775643746058\n",
      "Average test loss: 10.94676482760244\n",
      "Epoch 25/300\n",
      "Average training loss: 0.006456564469055997\n",
      "Average test loss: 0.006586978699184127\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0063718073434299895\n",
      "Average test loss: 0.31222088079982335\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006194155116875966\n",
      "Average test loss: 0.0061007672895987825\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006311507725467284\n",
      "Average test loss: 11.380242729183701\n",
      "Epoch 29/300\n",
      "Average training loss: 0.006172990122810006\n",
      "Average test loss: 0.2955158311157591\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005966809357206027\n",
      "Average test loss: 0.005862175041602718\n",
      "Epoch 31/300\n",
      "Average training loss: 0.005840523117946254\n",
      "Average test loss: 0.012372731071793371\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005821636085295015\n",
      "Average test loss: 133.152122127273\n",
      "Epoch 33/300\n",
      "Average training loss: 0.00577403125539422\n",
      "Average test loss: 6.806707154398163\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006050331224997838\n",
      "Average test loss: 0.00808008272614744\n",
      "Epoch 35/300\n",
      "Average training loss: 0.00609811737967862\n",
      "Average test loss: 0.009455204220281706\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005679782874882221\n",
      "Average test loss: 21.82437405660583\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005572803107400735\n",
      "Average test loss: 2.3477810253269142\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005513726968111263\n",
      "Average test loss: 0.015561416551056835\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005439086962077353\n",
      "Average test loss: 0.0055244164131581786\n",
      "Epoch 40/300\n",
      "Average training loss: 0.00909799907853206\n",
      "Average test loss: 0.006595939223137167\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0060333521970444256\n",
      "Average test loss: 0.26896842718455527\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005670650425056617\n",
      "Average test loss: 0.3838521664506859\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005533423514001899\n",
      "Average test loss: 0.011132509648385976\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005435207401505775\n",
      "Average test loss: 3.1333757530707453\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005381883821553654\n",
      "Average test loss: 0.013707995745870801\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005340777301126056\n",
      "Average test loss: 183.54447228695616\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005335919953054852\n",
      "Average test loss: 0.006972611322999001\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00531151658627722\n",
      "Average test loss: 0.11469216887321737\n",
      "Epoch 49/300\n",
      "Average training loss: 0.005723011819852723\n",
      "Average test loss: 0.005716929074997703\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005322801717039612\n",
      "Average test loss: 0.020676991565359962\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00528529840376642\n",
      "Average test loss: 0.01445485293534067\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005366342656314373\n",
      "Average test loss: 8988.481093088785\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005299557134509087\n",
      "Average test loss: 0.009514440486828486\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005211363634715477\n",
      "Average test loss: 0.8137339782168468\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005192870171119769\n",
      "Average test loss: 16.331624755072927\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005174088738651739\n",
      "Average test loss: 7.881377802213033\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005178454192562236\n",
      "Average test loss: 0.31197572479521235\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005132405304246478\n",
      "Average test loss: 5.7437741859571805\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005098770993865198\n",
      "Average test loss: 0.9845188126352926\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0051121590721110504\n",
      "Average test loss: 6.314618066443544\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0057391618395017255\n",
      "Average test loss: 0.0054425837935672865\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0051558215918226374\n",
      "Average test loss: 7.137022942620433\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005100348801662525\n",
      "Average test loss: 1.8462086997980045\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005048940420978599\n",
      "Average test loss: 4.472587487224489\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0050263166655268934\n",
      "Average test loss: 0.009702373838259113\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0050216374500758115\n",
      "Average test loss: 7.281381983776887\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0049821229858530896\n",
      "Average test loss: 84.48214962620206\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004963048796686861\n",
      "Average test loss: 0.007841346338391304\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0049385649537046755\n",
      "Average test loss: 80.34872369647688\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004971263724482722\n",
      "Average test loss: 1055.844244273113\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004908028807491064\n",
      "Average test loss: 41.736875680999624\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00492983331779639\n",
      "Average test loss: 5.255149532851246\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004906520756582419\n",
      "Average test loss: 48.851988323405386\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004998574195222722\n",
      "Average test loss: 1168.6262034076055\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004860195213721858\n",
      "Average test loss: 0.019405183817363447\n",
      "Epoch 76/300\n",
      "Average training loss: 0.004815400608297851\n",
      "Average test loss: 10.501142130119932\n",
      "Epoch 77/300\n",
      "Average training loss: 0.005872606720361445\n",
      "Average test loss: 0.005298450925697883\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004905097317778402\n",
      "Average test loss: 812.3426767811875\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004807472109380696\n",
      "Average test loss: 63.690314462174975\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004771360575738881\n",
      "Average test loss: 1.5551792657996217\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004750873860178723\n",
      "Average test loss: 0.023023570867669253\n",
      "Epoch 82/300\n",
      "Average training loss: 0.004766521563132604\n",
      "Average test loss: 0.01887785352725122\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004765942526774274\n",
      "Average test loss: 10.506727781611598\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0049142810329794884\n",
      "Average test loss: 61.25219231012588\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004731159112312727\n",
      "Average test loss: 2.7177080575856896\n",
      "Epoch 86/300\n",
      "Average training loss: 0.005146599070065551\n",
      "Average test loss: 2.4577738193050025\n",
      "Epoch 87/300\n",
      "Average training loss: 0.005065936527732346\n",
      "Average test loss: 1.5478654994650018\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004819036336822642\n",
      "Average test loss: 0.25620929175780877\n",
      "Epoch 89/300\n",
      "Average training loss: 0.004718857346723477\n",
      "Average test loss: 0.03897037120660146\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004672533118890392\n",
      "Average test loss: 1.7233578275868462\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0046865768378807435\n",
      "Average test loss: 3.9697512279566793\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004651403935419188\n",
      "Average test loss: 0.5697688179309998\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004628091236783398\n",
      "Average test loss: 57.98235776117113\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0047072623177535\n",
      "Average test loss: 174.1357055818124\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005131513519212603\n",
      "Average test loss: 0.005375967020375861\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004685925060676204\n",
      "Average test loss: 0.022333120598147314\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004605070263147354\n",
      "Average test loss: 29.096528615925045\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004580549991793103\n",
      "Average test loss: 2.2849989361746443\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004556886416342523\n",
      "Average test loss: 4481.752290917294\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00455177435113324\n",
      "Average test loss: 0.6919833827482329\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005884658981114626\n",
      "Average test loss: 34462.213128244\n",
      "Epoch 102/300\n",
      "Average training loss: 0.00476186353713274\n",
      "Average test loss: 2979.6740591577745\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004662425126466486\n",
      "Average test loss: 2130339.2652759887\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004622539324478971\n",
      "Average test loss: 1522086.2894458699\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004612045148594512\n",
      "Average test loss: 5335.6049075077935\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0046513449313739935\n",
      "Average test loss: 586.1035578782526\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004676348769416412\n",
      "Average test loss: 16.433942892251743\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004759991129653321\n",
      "Average test loss: 5.117391399336772\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00460404808115628\n",
      "Average test loss: 298.73194130515554\n",
      "Epoch 110/300\n",
      "Average training loss: 0.004594008084386587\n",
      "Average test loss: 0.5273216717988253\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005753813250611225\n",
      "Average test loss: 96.12060000367298\n",
      "Epoch 112/300\n",
      "Average training loss: 0.005050400090921257\n",
      "Average test loss: 8.638164705623769\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004685463608139091\n",
      "Average test loss: 7749.985658311632\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004676171433594492\n",
      "Average test loss: 10032.228812164403\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004683946348726749\n",
      "Average test loss: 45.78000138233954\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004597491368237469\n",
      "Average test loss: 8.207252750941448\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004591639077911774\n",
      "Average test loss: 1.0404322271843751\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004528918745203151\n",
      "Average test loss: 133.5038311740276\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004514569306539164\n",
      "Average test loss: 299.0809247914499\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004488979822645585\n",
      "Average test loss: 88846.88285698826\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004477483781261576\n",
      "Average test loss: 271240.46544177254\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004464488675610887\n",
      "Average test loss: 10370.135741875476\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004448889435579379\n",
      "Average test loss: 234.5764553353571\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0044813272290759616\n",
      "Average test loss: 20184.879233746575\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004466268674367004\n",
      "Average test loss: 442.2188992740446\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0044390139873656965\n",
      "Average test loss: 2.218697868509011\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004486604425228305\n",
      "Average test loss: 13848.721259865482\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004431995564243859\n",
      "Average test loss: 144.5741292923875\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004397345556153192\n",
      "Average test loss: 945.1595904654347\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004413829196037518\n",
      "Average test loss: 21.9221663719515\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004376553707445661\n",
      "Average test loss: 6.537566387293653\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004368705941157208\n",
      "Average test loss: 18092.13492083233\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004349953083942334\n",
      "Average test loss: 0.66089362515178\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0043293984849005936\n",
      "Average test loss: 7.501134261315896\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004291205479246047\n",
      "Average test loss: 211.43725081041953\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0044194125528964736\n",
      "Average test loss: 48.47129786232776\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004374328501936462\n",
      "Average test loss: 29.768741028635866\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004348408716420333\n",
      "Average test loss: 14.628681046107577\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004356476424882809\n",
      "Average test loss: 6.4616986938921945\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004312890096671051\n",
      "Average test loss: 2.0795148085852464\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0043195290288163555\n",
      "Average test loss: 38.40760702691972\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004318214341377219\n",
      "Average test loss: 71.45947150820163\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0042739040321773955\n",
      "Average test loss: 402.4258450829453\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00430669761242138\n",
      "Average test loss: 42102.3810231577\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004294486423540447\n",
      "Average test loss: 307.8065842825046\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0043220430585659214\n",
      "Average test loss: 391834.79107648646\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00425230091313521\n",
      "Average test loss: 853.1445495084027\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00423179992992017\n",
      "Average test loss: 180142.99969354743\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004243440681033665\n",
      "Average test loss: 9.158473535304267\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004249008651615845\n",
      "Average test loss: 35734.122080237285\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004229978024752603\n",
      "Average test loss: 40142.12596338841\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004233738522976637\n",
      "Average test loss: 4453.117471218533\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004213030137949519\n",
      "Average test loss: 0.7705224067912334\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004191955250170496\n",
      "Average test loss: 19.047718549417127\n",
      "Epoch 155/300\n",
      "Average training loss: 0.00426047977929314\n",
      "Average test loss: 4718.384139730152\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004251876713501083\n",
      "Average test loss: 569.7529640117784\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004313526308370961\n",
      "Average test loss: 2.200048285604351\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004332561532656352\n",
      "Average test loss: 6.088985069426811\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00429020038826598\n",
      "Average test loss: 23.399241103990626\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004275165982544422\n",
      "Average test loss: 5.884440386626042\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004255074615693755\n",
      "Average test loss: 642442.7632954355\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004234207801934745\n",
      "Average test loss: 8.706049574674832\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004349305729692181\n",
      "Average test loss: 29973.929529131372\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004222590313189559\n",
      "Average test loss: 32915.32472817928\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004161920165436136\n",
      "Average test loss: 291.29871597030177\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004372481689271\n",
      "Average test loss: 1697.3792930343084\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004074809788001908\n",
      "Average test loss: 56.29514903661609\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004052944424665636\n",
      "Average test loss: 0.4258089790811969\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004038034792368611\n",
      "Average test loss: 655079.3615809259\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004067509960590137\n",
      "Average test loss: 0.04521125464472506\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004043764686004983\n",
      "Average test loss: 0.3629432918416957\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0046411042883992195\n",
      "Average test loss: 79.0568408372667\n",
      "Epoch 173/300\n",
      "Average training loss: 0.005647777441889048\n",
      "Average test loss: 1.616003766576449\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004600766263074345\n",
      "Average test loss: 4114.685954161416\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004545839774939749\n",
      "Average test loss: 726.8850217670993\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004382977878053983\n",
      "Average test loss: 127.037494967305\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004349054238862462\n",
      "Average test loss: 4.355839329568876\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00423002980545991\n",
      "Average test loss: 2191.2042660191123\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004234347745776176\n",
      "Average test loss: 6987.934688954554\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0041248251690218845\n",
      "Average test loss: 6578.689339727155\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00411243165946669\n",
      "Average test loss: 5843171.804501561\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00419311105646193\n",
      "Average test loss: 568008.5148442222\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004156982433878713\n",
      "Average test loss: 15518924.649324449\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00419493062959777\n",
      "Average test loss: 256989.198810933\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004187383765561713\n",
      "Average test loss: 24027.54150108013\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004246423135201136\n",
      "Average test loss: 336.0308523180236\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004216368230266703\n",
      "Average test loss: 27505.81387940553\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004420543977576826\n",
      "Average test loss: 10792.239525833444\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004230885112037262\n",
      "Average test loss: 692745.407641493\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0041404077551431126\n",
      "Average test loss: 278215991518880.66\n",
      "Epoch 191/300\n",
      "Average training loss: 0.006537888062910901\n",
      "Average test loss: 639.9407946393937\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004866720648689402\n",
      "Average test loss: 10.673807275750985\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004779228846025136\n",
      "Average test loss: 0.05128857471379969\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004428905127363073\n",
      "Average test loss: 12.044916221040404\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004332978295783202\n",
      "Average test loss: 53.60502366470339\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004221191783332162\n",
      "Average test loss: 598.4231933551298\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004179892597306106\n",
      "Average test loss: 3.9226969637204374\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004169223863217566\n",
      "Average test loss: 6990.879110018252\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004188037094556623\n",
      "Average test loss: 46523.71073031247\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004241405947133898\n",
      "Average test loss: 64587.53723296173\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004138034864225321\n",
      "Average test loss: 0.2028983466008471\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004091498676687479\n",
      "Average test loss: 14746.663918921497\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004125853172813852\n",
      "Average test loss: 207.47006982097008\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004152785853793224\n",
      "Average test loss: 8846.947390258034\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0041897463796453345\n",
      "Average test loss: 1048836.15085947\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004107608250238829\n",
      "Average test loss: 52014.59699932399\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004069162753514118\n",
      "Average test loss: 384.43405361087457\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004093867297594746\n",
      "Average test loss: 0.5607796267088917\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004081616536610657\n",
      "Average test loss: 9.879836097059979\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004028577789250348\n",
      "Average test loss: 69534.1633563335\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004022231832767526\n",
      "Average test loss: 5901.321244178315\n",
      "Epoch 212/300\n",
      "Average training loss: 0.003969359402441316\n",
      "Average test loss: 22723.08621667139\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004070722122780151\n",
      "Average test loss: 97837796.6792685\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0039816635453866584\n",
      "Average test loss: 1222155.150267008\n",
      "Epoch 215/300\n",
      "Average training loss: 0.003959434258648091\n",
      "Average test loss: 144265.1498891724\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00397510069484512\n",
      "Average test loss: 9145.376162443603\n",
      "Epoch 217/300\n",
      "Average training loss: 0.00395458916740285\n",
      "Average test loss: 1978920.046\n",
      "Epoch 218/300\n",
      "Average training loss: 0.003933446863873137\n",
      "Average test loss: 5218305.933811856\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0039279916452036965\n",
      "Average test loss: 1020914.9749448365\n",
      "Epoch 220/300\n",
      "Average training loss: 0.003915779754105541\n",
      "Average test loss: 30461.75483511177\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0039284129453202084\n",
      "Average test loss: 7947412.65484081\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00392361788244711\n",
      "Average test loss: 13.912832203458995\n",
      "Epoch 223/300\n",
      "Average training loss: 0.003908286098390818\n",
      "Average test loss: 52982.889356734435\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00391282193031576\n",
      "Average test loss: 166103.53001972154\n",
      "Epoch 225/300\n",
      "Average training loss: 0.003920168506395485\n",
      "Average test loss: 3445.7986035707468\n",
      "Epoch 226/300\n",
      "Average training loss: 0.003901303852183951\n",
      "Average test loss: 313933.0077689981\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0038893057650162114\n",
      "Average test loss: 143237.28120562335\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0038952230202655\n",
      "Average test loss: 710156.9109918645\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0038662576174570452\n",
      "Average test loss: 157751.87255306295\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004006731686906682\n",
      "Average test loss: 5048890.531291846\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0038935522225995857\n",
      "Average test loss: 28080185189917.582\n",
      "Epoch 232/300\n",
      "Average training loss: 0.003874560124344296\n",
      "Average test loss: 1033380.2691221937\n",
      "Epoch 233/300\n",
      "Average training loss: 0.003862327745391263\n",
      "Average test loss: 35192.888857622005\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0038607736070536904\n",
      "Average test loss: 61670.70782593241\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0038478691337837114\n",
      "Average test loss: 13669359.438247886\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0038416730109602213\n",
      "Average test loss: 44.41733372918848\n",
      "Epoch 237/300\n",
      "Average training loss: 0.003831876467913389\n",
      "Average test loss: 72284.54459320547\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0038342586203167837\n",
      "Average test loss: 4981.775772255136\n",
      "Epoch 239/300\n",
      "Average training loss: 0.003818849240326219\n",
      "Average test loss: 6540049.347491587\n",
      "Epoch 240/300\n",
      "Average training loss: 0.003814675934612751\n",
      "Average test loss: 2749.297323514274\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003812881817213363\n",
      "Average test loss: 221051.02040763703\n",
      "Epoch 242/300\n",
      "Average training loss: 0.003817167776533299\n",
      "Average test loss: 1323836.4006554645\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0038191880118101833\n",
      "Average test loss: 1282914.5325457673\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0038043564456618495\n",
      "Average test loss: 794471.7756055676\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003790046160419782\n",
      "Average test loss: 137714.09549572496\n",
      "Epoch 246/300\n",
      "Average training loss: 0.003780702040841182\n",
      "Average test loss: 6825951.10546875\n",
      "Epoch 247/300\n",
      "Average training loss: 0.003785275898045964\n",
      "Average test loss: 42587463.24530377\n",
      "Epoch 248/300\n",
      "Average training loss: 0.003781479597919517\n",
      "Average test loss: 9631334.90681124\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0037807937036785813\n",
      "Average test loss: 25267567.254050165\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0037760260705318716\n",
      "Average test loss: 199447.44375282552\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0037666711465766034\n",
      "Average test loss: 5222367419.981817\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0037629604428592654\n",
      "Average test loss: 274638044.39215314\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0037516000357766944\n",
      "Average test loss: 93886370.65349366\n",
      "Epoch 254/300\n",
      "Average training loss: 0.003774616870822178\n",
      "Average test loss: 253891.8236750181\n",
      "Epoch 255/300\n",
      "Average training loss: 0.003747663201143344\n",
      "Average test loss: 169530.70761504304\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003719205703379379\n",
      "Average test loss: 300887.12739648786\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003746755882476767\n",
      "Average test loss: 12546.133237001763\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003733941939142015\n",
      "Average test loss: 6303.92291646514\n",
      "Epoch 259/300\n",
      "Average training loss: 0.003730893363141351\n",
      "Average test loss: 82434.84847362044\n",
      "Epoch 260/300\n",
      "Average training loss: 0.003737997829914093\n",
      "Average test loss: 12.631433847513463\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0037241671677264903\n",
      "Average test loss: 16492.301010215495\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0037268065870222117\n",
      "Average test loss: 22679.920925493265\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0037178177343060574\n",
      "Average test loss: 3198.3282593533704\n",
      "Epoch 264/300\n",
      "Average training loss: 0.003671562212001946\n",
      "Average test loss: 827.1824795899043\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0036534642229477565\n",
      "Average test loss: 15872.725126752022\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0036402020001163084\n",
      "Average test loss: 388306.46821292175\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0036609086241159174\n",
      "Average test loss: 190368.49553819443\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0037196276624583535\n",
      "Average test loss: 8076063.78257055\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0036922406221015585\n",
      "Average test loss: 1695392.505577399\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0037054466023627255\n",
      "Average test loss: 23333.64477447156\n",
      "Epoch 271/300\n",
      "Average training loss: 0.003675480744491021\n",
      "Average test loss: 16278.04206918871\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0036892701755795215\n",
      "Average test loss: 7948.6875148402205\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0036675828039232227\n",
      "Average test loss: 3524.2613166009623\n",
      "Epoch 274/300\n",
      "Average training loss: 0.003668071846374207\n",
      "Average test loss: 138103.85572364792\n",
      "Epoch 275/300\n",
      "Average training loss: 0.003674719549715519\n",
      "Average test loss: 29.641234723569617\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0036821436784747574\n",
      "Average test loss: 42859.13917689483\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0036563204860107766\n",
      "Average test loss: 25550319.3812559\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0036451455898996858\n",
      "Average test loss: 12538.296827142874\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0036474697180092336\n",
      "Average test loss: 182308427.7908611\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0036457323926604454\n",
      "Average test loss: 46801.03764575274\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003638563324386875\n",
      "Average test loss: 254126.77206635062\n",
      "Epoch 282/300\n",
      "Average training loss: 0.003651741381527649\n",
      "Average test loss: 5796.962601914458\n",
      "Epoch 283/300\n",
      "Average training loss: 0.003661129063202275\n",
      "Average test loss: 188670.641683208\n",
      "Epoch 284/300\n",
      "Average training loss: 0.003669313915901714\n",
      "Average test loss: 141943.38813888948\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0036356649479518336\n",
      "Average test loss: 4777.529114236431\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0036391560809893742\n",
      "Average test loss: 258.0968409125242\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0036469192021629875\n",
      "Average test loss: 8528.841021606782\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0036149206070436373\n",
      "Average test loss: 1571751.471136182\n",
      "Epoch 289/300\n",
      "Average training loss: 0.003620863994376527\n",
      "Average test loss: 347760.3967568245\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00361276677581999\n",
      "Average test loss: 55446473.8192936\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0036347029416097533\n",
      "Average test loss: 56618127.48448977\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0036276764360566936\n",
      "Average test loss: 4481.338940008188\n",
      "Epoch 293/300\n",
      "Average training loss: 0.003596930316752858\n",
      "Average test loss: 48011.8723048426\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0036449537231690353\n",
      "Average test loss: 272.9737872075025\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0036011434644460678\n",
      "Average test loss: 1566.1739022003055\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0035986525014870697\n",
      "Average test loss: 14485.674784876901\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0036020998093816967\n",
      "Average test loss: 21577.44079456363\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0035975403094457258\n",
      "Average test loss: 42.280442447341976\n",
      "Epoch 299/300\n",
      "Average training loss: 0.003602640514779422\n",
      "Average test loss: 428.9056011635032\n",
      "Epoch 300/300\n",
      "Average training loss: 0.003584244869856371\n",
      "Average test loss: 26897.490162060403\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.19832985925094948\n",
      "Average test loss: 3192.098572463016\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011511093835863802\n",
      "Average test loss: 0.10476490236363477\n",
      "Epoch 3/300\n",
      "Average training loss: 0.009014365254590909\n",
      "Average test loss: 0.007424522652394242\n",
      "Epoch 4/300\n",
      "Average training loss: 0.007658684293429057\n",
      "Average test loss: 24.16183425593045\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006981428138497803\n",
      "Average test loss: 0.006713045583003097\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006507584638065762\n",
      "Average test loss: 0.01185821390690075\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006177066698670387\n",
      "Average test loss: 0.12088624029358228\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005877192669030693\n",
      "Average test loss: 2.672979872489762\n",
      "Epoch 9/300\n",
      "Average training loss: 0.005632602575338549\n",
      "Average test loss: 0.006320233524259594\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005361867319378588\n",
      "Average test loss: 0.0346471088056763\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005137150586065319\n",
      "Average test loss: 0.01046427156858974\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004892936744623714\n",
      "Average test loss: 0.5870380240906444\n",
      "Epoch 13/300\n",
      "Average training loss: 0.00468895696890023\n",
      "Average test loss: 0.09178105460107326\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004505769598815176\n",
      "Average test loss: 0.005151128906549679\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004365029174420569\n",
      "Average test loss: 0.025744698272397122\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004220456921805938\n",
      "Average test loss: 32.0253349618829\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004104234178240101\n",
      "Average test loss: 180.02134511502584\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004047646368129386\n",
      "Average test loss: 38.690352507575106\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0038996489650259415\n",
      "Average test loss: 149.88130014978267\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003823047955830892\n",
      "Average test loss: 14.628102504129211\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003744867931223578\n",
      "Average test loss: 7.228982753816578\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003674918987063898\n",
      "Average test loss: 2.4759249070246394\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003588458582551943\n",
      "Average test loss: 902.876760995744\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0035309456675830814\n",
      "Average test loss: 4.626523247909215\n",
      "Epoch 25/300\n",
      "Average training loss: 0.003508236187406712\n",
      "Average test loss: 19146.01406737965\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014893896779666344\n",
      "Average test loss: 2.91426732773251\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006328267459654146\n",
      "Average test loss: 0.5403230763226747\n",
      "Epoch 28/300\n",
      "Average training loss: 0.004602959461510182\n",
      "Average test loss: 0.007306467392378383\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004233218685620361\n",
      "Average test loss: 0.004062598462320036\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005115370041173365\n",
      "Average test loss: 0.005030705261975527\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004320178170171049\n",
      "Average test loss: 0.004339458919026785\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004017068759641714\n",
      "Average test loss: 0.4547399218728145\n",
      "Epoch 33/300\n",
      "Average training loss: 0.003893560275435448\n",
      "Average test loss: 0.007429405365139246\n",
      "Epoch 34/300\n",
      "Average training loss: 0.003777256957358784\n",
      "Average test loss: 0.00560252056353622\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004044763236410088\n",
      "Average test loss: 0.04576362692978647\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0035997041937791637\n",
      "Average test loss: 0.4106759530471431\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0034938108641654254\n",
      "Average test loss: 5.10631360100872\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0034209968712594773\n",
      "Average test loss: 8.760537078339192\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0033622666717403465\n",
      "Average test loss: 45.765605815641166\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0033077949333108134\n",
      "Average test loss: 3.896523514860206\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003259885977539751\n",
      "Average test loss: 15.40555336340848\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0032662284918543367\n",
      "Average test loss: 48569.86428988924\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003176543512278133\n",
      "Average test loss: 0.24361541689766777\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003137189956381917\n",
      "Average test loss: 0.02553869143107699\n",
      "Epoch 45/300\n",
      "Average training loss: 0.003109946305760079\n",
      "Average test loss: 593.1779759226309\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014288710256831514\n",
      "Average test loss: 16.243050012108352\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0055459544339941606\n",
      "Average test loss: 0.3519267952216582\n",
      "Epoch 48/300\n",
      "Average training loss: 0.004525610529300239\n",
      "Average test loss: 1.040200997384472\n",
      "Epoch 49/300\n",
      "Average training loss: 0.004205061132502225\n",
      "Average test loss: 295.5482270669772\n",
      "Epoch 50/300\n",
      "Average training loss: 0.004054666548967361\n",
      "Average test loss: 0.10283559379478295\n",
      "Epoch 51/300\n",
      "Average training loss: 0.004034152311997281\n",
      "Average test loss: 0.09343637073598803\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0038838586134629116\n",
      "Average test loss: 24.83587007981042\n",
      "Epoch 53/300\n",
      "Average training loss: 0.003645984143225683\n",
      "Average test loss: 1154.3172666862838\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0035802515024940173\n",
      "Average test loss: 5669.885849283854\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003577384989708662\n",
      "Average test loss: 0.39178197420305677\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0036680234229813018\n",
      "Average test loss: 0.30877702842756277\n",
      "Epoch 57/300\n",
      "Average training loss: 0.003444740603574448\n",
      "Average test loss: 0.00428695613787406\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0033102205397768153\n",
      "Average test loss: 5571.911767650422\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003227101532949342\n",
      "Average test loss: 0.1559389677633428\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0032123637035903004\n",
      "Average test loss: 209.76009518914586\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005576783438109689\n",
      "Average test loss: 48.64274405725466\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004305424461969071\n",
      "Average test loss: 5.433875482605149\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0038675730470567942\n",
      "Average test loss: 79.06667592864298\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003388314046586553\n",
      "Average test loss: 10123.03997311409\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0032939692729463178\n",
      "Average test loss: 194461.35170831467\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0032355485134240654\n",
      "Average test loss: 887.8340427284779\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0031729045698626174\n",
      "Average test loss: 97.42433260554861\n",
      "Epoch 68/300\n",
      "Average training loss: 0.003135769606464439\n",
      "Average test loss: 1145.5732189230268\n",
      "Epoch 69/300\n",
      "Average training loss: 0.003462898345871104\n",
      "Average test loss: 242.64181186255854\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0032562159159117273\n",
      "Average test loss: 1.9815998632750578\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0032453276825447877\n",
      "Average test loss: 1248.0086778048333\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003188853058964014\n",
      "Average test loss: 1425.3228742760123\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0033182304803695945\n",
      "Average test loss: 0.2759398173975448\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0031633136171019738\n",
      "Average test loss: 0.4152378019309706\n",
      "Epoch 75/300\n",
      "Average training loss: 0.003127563683523072\n",
      "Average test loss: 0.009326986171305179\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00314483608574503\n",
      "Average test loss: 233.27138941552158\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004040131465428406\n",
      "Average test loss: 0.0873656908079154\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0032271657021095356\n",
      "Average test loss: 0.8434771520712724\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0031005257330834865\n",
      "Average test loss: 10.057611928410932\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0030153050424738064\n",
      "Average test loss: 53.77123661972914\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002997433477184839\n",
      "Average test loss: 2.465938358593318\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002992060975689027\n",
      "Average test loss: 6.032029996247341\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002974328479419152\n",
      "Average test loss: 0.1190268896093799\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0029711523968726397\n",
      "Average test loss: 0.31602748698585975\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0029365372423910433\n",
      "Average test loss: 3.606218681154152\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0030300705422543817\n",
      "Average test loss: 0.003249586307029757\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0029151719556086593\n",
      "Average test loss: 38.667673036673\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002889463877098428\n",
      "Average test loss: 0.448605225937234\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0028697071609397728\n",
      "Average test loss: 12698.564462239583\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002904270111065772\n",
      "Average test loss: 6141.72263281252\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0028493357816090185\n",
      "Average test loss: 109.65765100742877\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0028270327266719606\n",
      "Average test loss: 0.06307797023902337\n",
      "Epoch 93/300\n",
      "Average training loss: 0.003024535183898277\n",
      "Average test loss: 567.898009415409\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0028597269534236854\n",
      "Average test loss: 36.61300919103954\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002846797979229854\n",
      "Average test loss: 208181.7979625022\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002844919692724943\n",
      "Average test loss: 61.76164698892025\n",
      "Epoch 97/300\n",
      "Average training loss: 0.00277659349474642\n",
      "Average test loss: 0.0031279837026571234\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0027610620212637715\n",
      "Average test loss: 2552.7661491136055\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002757437043926782\n",
      "Average test loss: 0.21569952799855835\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0027323027269707785\n",
      "Average test loss: 5617.105602733097\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0029256287773864135\n",
      "Average test loss: 517.816042177899\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0028202054819299115\n",
      "Average test loss: 33.45820024446315\n",
      "Epoch 103/300\n",
      "Average training loss: 0.003000104939047661\n",
      "Average test loss: 135.79098198525938\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002761712677983774\n",
      "Average test loss: 93.07468623805109\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002717500504727165\n",
      "Average test loss: 130.76572528507063\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0027919065550797515\n",
      "Average test loss: 0.5867635198616319\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002689459576892356\n",
      "Average test loss: 4.199851042523152\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0026650451773570644\n",
      "Average test loss: 764.2988606138232\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002912452694442537\n",
      "Average test loss: 0.04632973311489655\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0028716554699672594\n",
      "Average test loss: 2221.6618344827775\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0027631562356319694\n",
      "Average test loss: 32.68319192181838\n",
      "Epoch 112/300\n",
      "Average training loss: 0.002695242719103893\n",
      "Average test loss: 213.1719760445669\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002682028580456972\n",
      "Average test loss: 143.41403388925062\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0026579562473214337\n",
      "Average test loss: 696.7764153126623\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0026430336218327285\n",
      "Average test loss: 6300.87586979243\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0026357778345959056\n",
      "Average test loss: 259.1786859877331\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0051988106343067355\n",
      "Average test loss: 0.4424641755500601\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0034597414625394673\n",
      "Average test loss: 0.01928519242318968\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0031992040953288474\n",
      "Average test loss: 0.004582410741804374\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0030428331235630643\n",
      "Average test loss: 272.89781789497533\n",
      "Epoch 121/300\n",
      "Average training loss: 0.003103463833530744\n",
      "Average test loss: 1.946478934418824\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002948680311648382\n",
      "Average test loss: 3.508839388681783\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0028925228944669166\n",
      "Average test loss: 2.5275731715750362\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002827041261932916\n",
      "Average test loss: 230.43553172072768\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0028130006570782928\n",
      "Average test loss: 117.85536938877404\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0027387032407439416\n",
      "Average test loss: 28.395993837369605\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0026905938616643347\n",
      "Average test loss: 6.030795959299017\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0026706617060634824\n",
      "Average test loss: 0.0615070377116402\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0026575018875818284\n",
      "Average test loss: 0.07983439348638058\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0026357119630815253\n",
      "Average test loss: 34.52945943335796\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0026248802000449763\n",
      "Average test loss: 13735.972769050872\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0026081496429526145\n",
      "Average test loss: 1716.3877903018874\n",
      "Epoch 133/300\n",
      "Average training loss: 0.002602109673536486\n",
      "Average test loss: 646.8842822937169\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0026060543954372407\n",
      "Average test loss: 52084.73378903116\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002609531843310429\n",
      "Average test loss: 1543.5306158440505\n",
      "Epoch 136/300\n",
      "Average training loss: 0.002587358463038173\n",
      "Average test loss: 25642.575659053815\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0025868766765213677\n",
      "Average test loss: 12584.791719689334\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0025729548626889785\n",
      "Average test loss: 11205.822130092598\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0025742884706705807\n",
      "Average test loss: 30731.70650511025\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0027317915996536613\n",
      "Average test loss: 6995.389759313089\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0027009047160132063\n",
      "Average test loss: 0.0963431477844715\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0026422622977859446\n",
      "Average test loss: 2334.032851440164\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0026250866752945716\n",
      "Average test loss: 2.780958065708892\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0026075162349475755\n",
      "Average test loss: 105061.50094223622\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002561035313953956\n",
      "Average test loss: 5010.487252869214\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002568251279079252\n",
      "Average test loss: 0.8992758537083865\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0025287582584553295\n",
      "Average test loss: 136704.25288163315\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002539125935691926\n",
      "Average test loss: 30941.51266909866\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002531429076153371\n",
      "Average test loss: 393863.97317961353\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002524883043227924\n",
      "Average test loss: 15982.839640709042\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0025140620632511045\n",
      "Average test loss: 6677.06398980366\n",
      "Epoch 152/300\n",
      "Average training loss: 0.002522845555096865\n",
      "Average test loss: 5025899.713166666\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0025272807938357196\n",
      "Average test loss: 610455.8777535593\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002484847762208018\n",
      "Average test loss: 12764.604554926082\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0025925395279708836\n",
      "Average test loss: 302.36140076478654\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002618594833546215\n",
      "Average test loss: 1123.3212350515491\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0025819012549602323\n",
      "Average test loss: 3.9372478428615465\n",
      "Epoch 158/300\n",
      "Average training loss: 0.002515976380349861\n",
      "Average test loss: 460.05615062642926\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0025840275964389243\n",
      "Average test loss: 22.452228721502756\n",
      "Epoch 160/300\n",
      "Average training loss: 0.002619107348016567\n",
      "Average test loss: 199.12249832837614\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0025343019218287536\n",
      "Average test loss: 0.9439641595962974\n",
      "Epoch 162/300\n",
      "Average training loss: 0.002510611864220765\n",
      "Average test loss: 4224.881906763363\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0024863110655504797\n",
      "Average test loss: 107.39830653699549\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002480080025477542\n",
      "Average test loss: 18395.30648231018\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0024681309776173697\n",
      "Average test loss: 57430.3179590822\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002481535190716386\n",
      "Average test loss: 220.10705777432852\n",
      "Epoch 167/300\n",
      "Average training loss: 0.002440674501781662\n",
      "Average test loss: 180869.11047079586\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002414005088723368\n",
      "Average test loss: 10.220432363647037\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002429965456430283\n",
      "Average test loss: 0.08249690263180269\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0024691577177080844\n",
      "Average test loss: 239.073721007064\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0025066816558440524\n",
      "Average test loss: 26.061523193746805\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0024865503985848692\n",
      "Average test loss: 619.7843691374384\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0024425051033289897\n",
      "Average test loss: 1797.2704557007046\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002453565350112816\n",
      "Average test loss: 1354.3063267615007\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0024548336579981777\n",
      "Average test loss: 189.98043386157022\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0024256497008932963\n",
      "Average test loss: 7572.101942872929\n",
      "Epoch 177/300\n",
      "Average training loss: 0.002407591650676396\n",
      "Average test loss: 3370.4997297203226\n",
      "Epoch 178/300\n",
      "Average training loss: 0.002353929304931727\n",
      "Average test loss: 140.34034930079287\n",
      "Epoch 179/300\n",
      "Average training loss: 0.002455284713043107\n",
      "Average test loss: 317.0245271307623\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0023897544665055143\n",
      "Average test loss: 146.4987573360273\n",
      "Epoch 181/300\n",
      "Average training loss: 0.002408301598288947\n",
      "Average test loss: 19.192987259669437\n",
      "Epoch 182/300\n",
      "Average training loss: 0.002385542253135807\n",
      "Average test loss: 14.92252275819911\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0023481707227312855\n",
      "Average test loss: 103.00302974793316\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002349221971506874\n",
      "Average test loss: 1180.9604313930936\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0023698252578162483\n",
      "Average test loss: 1470.4059757632117\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0023312846447030703\n",
      "Average test loss: 44921.948268581364\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0023310764962807297\n",
      "Average test loss: 881.0656414193897\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002302832262383567\n",
      "Average test loss: 11392.827325138791\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0023113993122759794\n",
      "Average test loss: 4032.8183386855985\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0023567088954150675\n",
      "Average test loss: 604.6201343645906\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002316985435369942\n",
      "Average test loss: 2862.1490542146134\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002302535436840521\n",
      "Average test loss: 208584.57587346318\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0023168050670582386\n",
      "Average test loss: 44.52643736238943\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0022929316570775376\n",
      "Average test loss: 397399.029375\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002264572834596038\n",
      "Average test loss: 133.50241038918475\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00228638133365247\n",
      "Average test loss: 3170.508319842337\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0023236136835896306\n",
      "Average test loss: 1510.9148857585176\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002272004012018442\n",
      "Average test loss: 311.722278718277\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0022711988088364404\n",
      "Average test loss: 2283.9249590094214\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002256662243563268\n",
      "Average test loss: 5210.90792060752\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0022723221505681675\n",
      "Average test loss: 2062.720607046971\n",
      "Epoch 202/300\n",
      "Average training loss: 0.002244884493243363\n",
      "Average test loss: 1906712.252472062\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0022321634758263828\n",
      "Average test loss: 462.68772754164286\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0022239740290161637\n",
      "Average test loss: 10536332.848381944\n",
      "Epoch 205/300\n",
      "Average training loss: 0.002228356480081048\n",
      "Average test loss: 16049.189981464187\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0022120833735292155\n",
      "Average test loss: 168804.7886442412\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0022168722309999995\n",
      "Average test loss: 180648.47964803939\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002273956553493109\n",
      "Average test loss: 1569.6864148055502\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0022591605542434585\n",
      "Average test loss: 14491.076239824437\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0022476806081831455\n",
      "Average test loss: 122330.18954571724\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0022670339554962185\n",
      "Average test loss: 5073.044307959412\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0022219391527275242\n",
      "Average test loss: 125.20962594405479\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0022152299617106716\n",
      "Average test loss: 226601.73925786675\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002214941592162682\n",
      "Average test loss: 83.87218328290712\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0022117534815851185\n",
      "Average test loss: 12113.637957411644\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0021940754289842316\n",
      "Average test loss: 4.308649743522621\n",
      "Epoch 217/300\n",
      "Average training loss: 0.002193020846694708\n",
      "Average test loss: 214864.90418377548\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0021805682146611312\n",
      "Average test loss: 29011.279463409297\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002192090670371221\n",
      "Average test loss: 1103415.7122265112\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002322660419055157\n",
      "Average test loss: 367.8048573853974\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0022154362690117624\n",
      "Average test loss: 1237.517374535256\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002164670788993438\n",
      "Average test loss: 4943.745099849486\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00219543284115692\n",
      "Average test loss: 188.3869165818956\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002172325117306577\n",
      "Average test loss: 348.11338314620156\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0021634341683238745\n",
      "Average test loss: 24.541613678625787\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0021542542945179673\n",
      "Average test loss: 1099.6238754546443\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002168030026368797\n",
      "Average test loss: 36483.75116429034\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002165014816655053\n",
      "Average test loss: 12881.103835317166\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002151562633406785\n",
      "Average test loss: 201541.83326305423\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0021600798174945843\n",
      "Average test loss: 27212.903419225688\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0021458556826950775\n",
      "Average test loss: 2286.893742180165\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0021427377418925366\n",
      "Average test loss: 4469.1503042555\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0021284934770729805\n",
      "Average test loss: 102338.6420536733\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0021365452830990157\n",
      "Average test loss: 763.9418803858215\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0021536362336741556\n",
      "Average test loss: 18408.906201123344\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0021853743737770452\n",
      "Average test loss: 425795.8892994348\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0021612565407736436\n",
      "Average test loss: 5276.791745083197\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0023144696855710613\n",
      "Average test loss: 13.646601208536369\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002235655079699225\n",
      "Average test loss: 1.2888829480583468\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0023197707283414072\n",
      "Average test loss: 12934.21411023656\n",
      "Epoch 241/300\n",
      "Average training loss: 0.002288371193119221\n",
      "Average test loss: 140.6764366689581\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002268728454079893\n",
      "Average test loss: 9182.594980480728\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002234768048135771\n",
      "Average test loss: 17.935196441505934\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002176367781021529\n",
      "Average test loss: 7657.526794713273\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002150851703559359\n",
      "Average test loss: 589.006964841175\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002178960772231221\n",
      "Average test loss: 12200.163102213543\n",
      "Epoch 247/300\n",
      "Average training loss: 0.002150221640554567\n",
      "Average test loss: 881176.5233240783\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0021570984137554965\n",
      "Average test loss: 4904.85659582294\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0021480455862151254\n",
      "Average test loss: 131227.83378980373\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0021302785386020937\n",
      "Average test loss: 272208.2820615845\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002128911497588787\n",
      "Average test loss: 528207.0733948239\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002125937452746762\n",
      "Average test loss: 2453739.108608479\n",
      "Epoch 253/300\n",
      "Average training loss: 0.002123661776797639\n",
      "Average test loss: 79378.3277488258\n",
      "Epoch 254/300\n",
      "Average training loss: 0.002115073462223841\n",
      "Average test loss: 1698318.423617549\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0021230276022106407\n",
      "Average test loss: 17116094.06471485\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0021105943062446186\n",
      "Average test loss: 291147.43771813647\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002141461115123497\n",
      "Average test loss: 32259.875096146385\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002128425339754257\n",
      "Average test loss: 185486.3163302207\n",
      "Epoch 259/300\n",
      "Average training loss: 0.002111324566933844\n",
      "Average test loss: 453015.63619016926\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0021102479967392155\n",
      "Average test loss: 1682167.4049144725\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002131880884990096\n",
      "Average test loss: 356928561.536\n",
      "Epoch 262/300\n",
      "Average training loss: 0.002101789749227464\n",
      "Average test loss: 885334.7512345895\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002176051945736011\n",
      "Average test loss: 0.5113963450238936\n",
      "Epoch 264/300\n",
      "Average training loss: 0.002129365729271538\n",
      "Average test loss: 2024.979349121487\n",
      "Epoch 265/300\n",
      "Average training loss: 0.002111445237468514\n",
      "Average test loss: 128121.40742100387\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0021200214494019745\n",
      "Average test loss: 19315.759064818023\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0021060790820047258\n",
      "Average test loss: 1.0350215745344757\n",
      "Epoch 268/300\n",
      "Average training loss: 0.002109919257565505\n",
      "Average test loss: 7316.754626396874\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0021449922125579583\n",
      "Average test loss: 3374.283013903002\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0024019154041177698\n",
      "Average test loss: 0.040295237900482284\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0023828847172359625\n",
      "Average test loss: 8.273330248374078\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0021648614979866477\n",
      "Average test loss: 96.50909895399543\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002130972975347605\n",
      "Average test loss: 4.745203945127419\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002154753404152062\n",
      "Average test loss: 938.6242508473138\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002137067196683751\n",
      "Average test loss: 609.6677595737725\n",
      "Epoch 276/300\n",
      "Average training loss: 0.002162771665180723\n",
      "Average test loss: 3894.965472752968\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0021283800608168047\n",
      "Average test loss: 9392.519640241795\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0021157159878768855\n",
      "Average test loss: 15.389363624652434\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002122637767965595\n",
      "Average test loss: 385.1837524250228\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0021123680807650087\n",
      "Average test loss: 818.3248252526058\n",
      "Epoch 281/300\n",
      "Average training loss: 0.002167360732021431\n",
      "Average test loss: 1167.048269762645\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002077590366618501\n",
      "Average test loss: 20.369121109211402\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002070773883412282\n",
      "Average test loss: 1562.9258501037657\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002064273037223352\n",
      "Average test loss: 673.711270425419\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0020653598812512227\n",
      "Average test loss: 45099.09277378902\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0020709530622180965\n",
      "Average test loss: 1787.1091514103869\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0020676257194330297\n",
      "Average test loss: 9349.666949554645\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002092815182482203\n",
      "Average test loss: 158600.32369972736\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0021313060282005204\n",
      "Average test loss: 522.2604227011665\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0020719985813937255\n",
      "Average test loss: 14122.912929970702\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0020627747315706477\n",
      "Average test loss: 370.0760140562488\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0020538181545626784\n",
      "Average test loss: 15564.17089038884\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00205707232767923\n",
      "Average test loss: 371836.3511528611\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0020580924019838374\n",
      "Average test loss: 1582806.6913047556\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002054528169747856\n",
      "Average test loss: 1744549.25148546\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0020380908764070934\n",
      "Average test loss: 8548.236579707507\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0020450341297934454\n",
      "Average test loss: 394786.4996284457\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0020404552784230976\n",
      "Average test loss: 1718.5013800580575\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0020601466451254154\n",
      "Average test loss: 4205.125115558901\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002061480011790991\n",
      "Average test loss: 39059.81494791328\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1603700562119484\n",
      "Average test loss: 0.013284533122347461\n",
      "Epoch 2/300\n",
      "Average training loss: 0.00801364456779427\n",
      "Average test loss: 2.313214289718204\n",
      "Epoch 3/300\n",
      "Average training loss: 0.00656038708322578\n",
      "Average test loss: 0.03226415851505266\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0057850267228980855\n",
      "Average test loss: 0.013423140550239219\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005261443774319357\n",
      "Average test loss: 128.38522128582\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004907331403758791\n",
      "Average test loss: 0.005465625787774722\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004613410087095367\n",
      "Average test loss: 9.067392932915025\n",
      "Epoch 8/300\n",
      "Average training loss: 0.004362296221570836\n",
      "Average test loss: 0.35753386546091903\n",
      "Epoch 9/300\n",
      "Average training loss: 0.004165828750365311\n",
      "Average test loss: 4.367659760697021\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003973745271356569\n",
      "Average test loss: 0.14846886642111673\n",
      "Epoch 11/300\n",
      "Average training loss: 0.003797360566341215\n",
      "Average test loss: 0.009216711976875862\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0036421179303692446\n",
      "Average test loss: 0.14410633410120177\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0035006678054730097\n",
      "Average test loss: 682.7554063433806\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0033692919131782318\n",
      "Average test loss: 0.0032426762582941186\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0032590005977286233\n",
      "Average test loss: 4.023144336337845\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0031629349475519523\n",
      "Average test loss: 0.0030256153620365593\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0030572974344508516\n",
      "Average test loss: 111.59126216855314\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0029494564346969126\n",
      "Average test loss: 1.7765666009253926\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0028704666547063325\n",
      "Average test loss: 0.8114336221540968\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0027777732896308105\n",
      "Average test loss: 0.002781636584757103\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0027026414244125285\n",
      "Average test loss: 25.94439019037452\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0026322599307944377\n",
      "Average test loss: 2.534305131756804\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0025658365317827296\n",
      "Average test loss: 0.003971931159082386\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0024840821312326522\n",
      "Average test loss: 118.47365123032199\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002436721864880787\n",
      "Average test loss: 80.35224941600941\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002402563161216676\n",
      "Average test loss: 76.2359098476544\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0023312814141520197\n",
      "Average test loss: 173.41792572344633\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002301322087024649\n",
      "Average test loss: 60.92836079381282\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0022286518290638925\n",
      "Average test loss: 123.16662601108435\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002175806251871917\n",
      "Average test loss: 124.78642323490315\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0021713581072787445\n",
      "Average test loss: 32577.535006234393\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002123214001560377\n",
      "Average test loss: 0.085080996233556\n",
      "Epoch 33/300\n",
      "Average training loss: 0.006474950103192693\n",
      "Average test loss: 0.020443310180678964\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00287621547964712\n",
      "Average test loss: 229.11146794779268\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0025279634733580882\n",
      "Average test loss: 48.651311440764204\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0023737870034658244\n",
      "Average test loss: 109.5531624832977\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0023032698689235583\n",
      "Average test loss: 46.104185980040164\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0022248204855455293\n",
      "Average test loss: 7830.364324096421\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0021841090170459615\n",
      "Average test loss: 34.32059724791017\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0022000194805570773\n",
      "Average test loss: 27.948830190541017\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0021550113331112596\n",
      "Average test loss: 2338.979519263326\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0021227893563401366\n",
      "Average test loss: 48539.0175524621\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002108146922248933\n",
      "Average test loss: 32.55408467194314\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002175272523942921\n",
      "Average test loss: 6426.64189108957\n",
      "Epoch 45/300\n",
      "Average training loss: 0.002101687955463098\n",
      "Average test loss: 10.449668885290002\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002076086572681864\n",
      "Average test loss: 3545.975678342186\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002047661226346261\n",
      "Average test loss: 2801.882267600018\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002030442819206251\n",
      "Average test loss: 476.2911328252281\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002006914753259884\n",
      "Average test loss: 8.646941915314645\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0019975558376560607\n",
      "Average test loss: 8990.818799576407\n",
      "Epoch 51/300\n",
      "Average training loss: 0.001978786788984305\n",
      "Average test loss: 74112.72181647086\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0019528104441447391\n",
      "Average test loss: 5255.888163852897\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0019319470750374925\n",
      "Average test loss: 410.0008440545137\n",
      "Epoch 54/300\n",
      "Average training loss: 0.008608303973037336\n",
      "Average test loss: 0.6308309527287881\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0029228352655967078\n",
      "Average test loss: 6.024715018942953\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002533491881357299\n",
      "Average test loss: 451.4643664901588\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002574730405997899\n",
      "Average test loss: 8204.730272198602\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0023360567088756296\n",
      "Average test loss: 1955.9610654221633\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002210271657237576\n",
      "Average test loss: 0.05094888328905735\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0021352685926896\n",
      "Average test loss: 13.989782309304301\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0020998471675233708\n",
      "Average test loss: 0.6631114692274067\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0021828444362100626\n",
      "Average test loss: 29.758181929344932\n",
      "Epoch 63/300\n",
      "Average training loss: 0.00206563858822402\n",
      "Average test loss: 12639.385659303654\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002009484163640688\n",
      "Average test loss: 1223.671986954646\n",
      "Epoch 65/300\n",
      "Average training loss: 0.001988723453030818\n",
      "Average test loss: 772.4055436351697\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0019824020222036377\n",
      "Average test loss: 5.351203394499297\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0019634194298543866\n",
      "Average test loss: 1798.294029508632\n",
      "Epoch 68/300\n",
      "Average training loss: 0.001961998830549419\n",
      "Average test loss: 1565.4466757482985\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0019421963998013073\n",
      "Average test loss: 316.11950472096623\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0019100777238814368\n",
      "Average test loss: 2241.107676868366\n",
      "Epoch 71/300\n",
      "Average training loss: 0.001910372762526903\n",
      "Average test loss: 15264.253305877566\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0018790134768933058\n",
      "Average test loss: 188232.73923754532\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0019020272551311388\n",
      "Average test loss: 1220.556361230337\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0018594288794944683\n",
      "Average test loss: 3998.3326217661765\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0018414694729985462\n",
      "Average test loss: 149.6878603422557\n",
      "Epoch 76/300\n",
      "Average training loss: 0.007246997336029178\n",
      "Average test loss: 0.09715015929192304\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0028701821693943607\n",
      "Average test loss: 81.9748634564595\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0024947708710614178\n",
      "Average test loss: 39.48239030055867\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0023325227395527893\n",
      "Average test loss: 6.757422613597992\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002188962757690913\n",
      "Average test loss: 0.47183571916694445\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0021086714383628633\n",
      "Average test loss: 104653.36612562543\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002031326302120255\n",
      "Average test loss: 671.4858992303444\n",
      "Epoch 83/300\n",
      "Average training loss: 0.001991679685914682\n",
      "Average test loss: 1668.9077565623847\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002004482627949781\n",
      "Average test loss: 4.47065645292153\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00195092332849486\n",
      "Average test loss: 0.005574011546663112\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0019322421246518692\n",
      "Average test loss: 0.7370358885614615\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0019175324948090646\n",
      "Average test loss: 1.611356787302428\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0018959530760637587\n",
      "Average test loss: 27.058986216058333\n",
      "Epoch 89/300\n",
      "Average training loss: 0.001907994325997101\n",
      "Average test loss: 8.146763984064675\n",
      "Epoch 90/300\n",
      "Average training loss: 0.001862198684985439\n",
      "Average test loss: 16524.54208225706\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0018297664258215163\n",
      "Average test loss: 109.77461262498278\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0018141330730997854\n",
      "Average test loss: 2.680468677906009\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0018038535128451056\n",
      "Average test loss: 4283.950414889483\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0019478059801169567\n",
      "Average test loss: 553.6241211018579\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0017894580877489514\n",
      "Average test loss: 23306.262339433233\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0017774049553813205\n",
      "Average test loss: 3256.339570549826\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0017669414029353194\n",
      "Average test loss: 1582.3466242326983\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0018042253424516983\n",
      "Average test loss: 6028.957393115888\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0017471737173489399\n",
      "Average test loss: 38.97269147543961\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0017364407285220093\n",
      "Average test loss: 85960.46806167721\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0017240705874024165\n",
      "Average test loss: 728539.8802310209\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0017951345889725618\n",
      "Average test loss: 11354.87048710939\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0017321307152095768\n",
      "Average test loss: 56.41497224084122\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0017511889396442308\n",
      "Average test loss: 5809.963373727311\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0017514929903878105\n",
      "Average test loss: 0.02089956150203943\n",
      "Epoch 106/300\n",
      "Average training loss: 0.001678180317290955\n",
      "Average test loss: 21.961594092351177\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0016562072317012483\n",
      "Average test loss: 63022.54032013623\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0017216408912920289\n",
      "Average test loss: 31.655301848756668\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0018118766896012756\n",
      "Average test loss: 10.953903636977076\n",
      "Epoch 110/300\n",
      "Average training loss: 0.00186085577443656\n",
      "Average test loss: 0.17655644191873984\n",
      "Epoch 111/300\n",
      "Average training loss: 0.001732781125439538\n",
      "Average test loss: 223.6342001923869\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0017037584260106086\n",
      "Average test loss: 968.804751247531\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0016946001892081566\n",
      "Average test loss: 23.12658639593567\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0017507696711561746\n",
      "Average test loss: 11.606841045608123\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0016594802502335774\n",
      "Average test loss: 45989.19569442395\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004259325909324818\n",
      "Average test loss: 0.005612930559449726\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002377432164218691\n",
      "Average test loss: 0.0023617003367592893\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0021462568836286664\n",
      "Average test loss: 6.361639686985976\n",
      "Epoch 119/300\n",
      "Average training loss: 0.001987494738979472\n",
      "Average test loss: 7.210888327162299\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0018930931336556872\n",
      "Average test loss: 0.9017846185184187\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0018522227617601553\n",
      "Average test loss: 4.741014444687507\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0018297636127099395\n",
      "Average test loss: 43.53359860996074\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0018047846908577614\n",
      "Average test loss: 13737.497183986961\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0019341938239004876\n",
      "Average test loss: 21.337200048446654\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0018502051849953003\n",
      "Average test loss: 10.21657646195011\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0018206458685712682\n",
      "Average test loss: 4.775748198659883\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0017932486758670872\n",
      "Average test loss: 805.9614770999691\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0018764054684175385\n",
      "Average test loss: 0.01963151821680367\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0017560306043467588\n",
      "Average test loss: 267.27495713365823\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0017249593908588092\n",
      "Average test loss: 1031.5068514036134\n",
      "Epoch 131/300\n",
      "Average training loss: 0.001727836340976258\n",
      "Average test loss: 580.873673290992\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0016944912668938438\n",
      "Average test loss: 833.8517753286221\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0016946155198125376\n",
      "Average test loss: 721.0138480663456\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0016767606764203971\n",
      "Average test loss: 0.002005669831401772\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0017176973098475073\n",
      "Average test loss: 48.54947083170298\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0016960720231549607\n",
      "Average test loss: 437.1075598659714\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0016513084205281403\n",
      "Average test loss: 24.296550533030494\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0016284880404257112\n",
      "Average test loss: 284.7418219565056\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0016706104406880007\n",
      "Average test loss: 0.49520552394125195\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0016511892283128367\n",
      "Average test loss: 45.855960114897954\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0016124120301877459\n",
      "Average test loss: 145.44365663021856\n",
      "Epoch 142/300\n",
      "Average training loss: 0.001613109240308404\n",
      "Average test loss: 1374.6758942406345\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0015943456264212728\n",
      "Average test loss: 16288.870852650556\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001587354458661543\n",
      "Average test loss: 17922.114763744452\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0015811529691434568\n",
      "Average test loss: 71.65310568239002\n",
      "Epoch 146/300\n",
      "Average training loss: 0.001645608523653613\n",
      "Average test loss: 6573.6994766111375\n",
      "Epoch 147/300\n",
      "Average training loss: 0.001657358640184005\n",
      "Average test loss: 373.4302328182055\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0016316834308413995\n",
      "Average test loss: 19.916798037470215\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0016727641905761427\n",
      "Average test loss: 12988.304029490273\n",
      "Epoch 150/300\n",
      "Average training loss: 0.001618447692030006\n",
      "Average test loss: 9059.097024202327\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0016635350744343465\n",
      "Average test loss: 5191.716371903426\n",
      "Epoch 152/300\n",
      "Average training loss: 0.001612437061758505\n",
      "Average test loss: 166.78622993384135\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001611433738325205\n",
      "Average test loss: 5362.851298205458\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0015948872978074683\n",
      "Average test loss: 325603.43840561144\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0016006408809787697\n",
      "Average test loss: 3037.212893535083\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0015797778551156323\n",
      "Average test loss: 2526.5780731193545\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0015498026553541421\n",
      "Average test loss: 4548.120049126562\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0015459695683999195\n",
      "Average test loss: 106.36556329865185\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0015227317599993612\n",
      "Average test loss: 1872.517387399606\n",
      "Epoch 160/300\n",
      "Average training loss: 0.001526588124016093\n",
      "Average test loss: 78.67332029669359\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0015514167789887224\n",
      "Average test loss: 223056.72521354107\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0015556494484966\n",
      "Average test loss: 138.86908211335478\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0015195454827820261\n",
      "Average test loss: 205.35653144741784\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0015034908906867107\n",
      "Average test loss: 6284.201559074369\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001540934100281447\n",
      "Average test loss: 551162.3015934265\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0015226436709571216\n",
      "Average test loss: 4736045.054003038\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0015181579768864645\n",
      "Average test loss: 18465392.726930555\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0015100805391040112\n",
      "Average test loss: 124.90435839410789\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0014707959838625457\n",
      "Average test loss: 20516.756855727217\n",
      "Epoch 170/300\n",
      "Average training loss: 0.005562584621003932\n",
      "Average test loss: 505.287671828616\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0022900307023276884\n",
      "Average test loss: 2812.3961493892944\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0020694337603118684\n",
      "Average test loss: 1926.7450215509004\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0019286598124437862\n",
      "Average test loss: 3477.74605510939\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0018118223982552686\n",
      "Average test loss: 42541.60701898872\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0017222047448158264\n",
      "Average test loss: 1347.604789707515\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0016371700768876406\n",
      "Average test loss: 636.4836850115731\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0015847278955496019\n",
      "Average test loss: 19834.23186148661\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0015428979341975517\n",
      "Average test loss: 1821.3479961542996\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0015189227394552694\n",
      "Average test loss: 22592.77668570022\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0015037341420021321\n",
      "Average test loss: 228422.5440600118\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0014930515127877394\n",
      "Average test loss: 9663.691313775966\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0015088298736761015\n",
      "Average test loss: 129523.82728297933\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0014808424593259891\n",
      "Average test loss: 3883.908476306297\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0014746597670018673\n",
      "Average test loss: 19448.009311109257\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0014705539705852668\n",
      "Average test loss: 8051713618.204426\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0015704040059612858\n",
      "Average test loss: 18.90164175546428\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0015017933648907476\n",
      "Average test loss: 87.27144112989637\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0014950868272119097\n",
      "Average test loss: 3694.065007678802\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0014608144961918393\n",
      "Average test loss: 23.35241426151039\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0014549960211540263\n",
      "Average test loss: 418073.2151068517\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0014660488979683982\n",
      "Average test loss: 6509.187404014099\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0014585770854933394\n",
      "Average test loss: 22839.440914290608\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0014656542408176595\n",
      "Average test loss: 1565316.8421024708\n",
      "Epoch 194/300\n",
      "Average training loss: 0.001439862162185212\n",
      "Average test loss: 5208875.338363056\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0014334749753276508\n",
      "Average test loss: 195024171.29042688\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0014495079076538484\n",
      "Average test loss: 39126.82319001792\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0014712946553611093\n",
      "Average test loss: 18661.237315967814\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0014142332583044967\n",
      "Average test loss: 139075.46719755244\n",
      "Epoch 199/300\n",
      "Average training loss: 0.001418429939593706\n",
      "Average test loss: 870.834424011342\n",
      "Epoch 200/300\n",
      "Average training loss: 0.001413395788624055\n",
      "Average test loss: 564867.7062498496\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0014169883630755875\n",
      "Average test loss: 4459667.283846206\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0014228306639318664\n",
      "Average test loss: 51117070.12450406\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0014181802689822184\n",
      "Average test loss: 6445946.526383948\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0014051410358192193\n",
      "Average test loss: 2293191.595462131\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0014071598736465805\n",
      "Average test loss: 8074.321591805239\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00139389237580407\n",
      "Average test loss: 2334736.6970356\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0014007011270150541\n",
      "Average test loss: 433308.94224015385\n",
      "Epoch 208/300\n",
      "Average training loss: 0.001376778844330046\n",
      "Average test loss: 19992280.30426736\n",
      "Epoch 209/300\n",
      "Average training loss: 0.001382286020140681\n",
      "Average test loss: 97235.51964646849\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0013936197900523742\n",
      "Average test loss: 0.13033487235547767\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0013835924450928967\n",
      "Average test loss: 3756065.7059878632\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0013940893687928717\n",
      "Average test loss: 2.3569924420879946\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0013938043971235554\n",
      "Average test loss: 33.767443736802576\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0013666349974357419\n",
      "Average test loss: 891365.3285077065\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0013741347797008023\n",
      "Average test loss: 43722305.65141269\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001395815949059195\n",
      "Average test loss: 1081.8185053959232\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0013614742802456022\n",
      "Average test loss: 14902.31733422184\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0013505540417714252\n",
      "Average test loss: 2088.8138422364686\n",
      "Epoch 219/300\n",
      "Average training loss: 0.001370437630969617\n",
      "Average test loss: 1053.4841768211516\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0013490827354705995\n",
      "Average test loss: 44174.75509868245\n",
      "Epoch 221/300\n",
      "Average training loss: 0.001353951253100402\n",
      "Average test loss: 2687320.2909602365\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0013486974315924777\n",
      "Average test loss: 40692.711879937575\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0013661467575778563\n",
      "Average test loss: 1324541.4401495056\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0013581798284625014\n",
      "Average test loss: 5036008.134462592\n",
      "Epoch 225/300\n",
      "Average training loss: 0.001345866562281218\n",
      "Average test loss: 2063113419.0003455\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0013372144643217325\n",
      "Average test loss: 2452309.412397741\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001342761277842025\n",
      "Average test loss: 517871.16201384575\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0013329460922007759\n",
      "Average test loss: 8933534.273530915\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0014426881041791704\n",
      "Average test loss: 18379788.510534286\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0015583970919251441\n",
      "Average test loss: 69961161.99416667\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0013728018934941954\n",
      "Average test loss: 35124.20232152463\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0013601666120812298\n",
      "Average test loss: 322.6914057492949\n",
      "Epoch 233/300\n",
      "Average training loss: 0.001339038451616135\n",
      "Average test loss: 796108.0204783635\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0013359409214721786\n",
      "Average test loss: 40414.72762075346\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0013515113009553816\n",
      "Average test loss: 2059.7853509683487\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001385995962863995\n",
      "Average test loss: 854.8847591381781\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00135355993727636\n",
      "Average test loss: 0.02161034815173803\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0015347632442911465\n",
      "Average test loss: 14.541250886639787\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0014599157225133645\n",
      "Average test loss: 62.16068457342933\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0013765149184813102\n",
      "Average test loss: 1.516047092711553\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0013463221400355299\n",
      "Average test loss: 0.9873522157594562\n",
      "Epoch 242/300\n",
      "Average training loss: 0.001458553680115276\n",
      "Average test loss: 1.186212560661965\n",
      "Epoch 243/300\n",
      "Average training loss: 0.001527293825832506\n",
      "Average test loss: 33.58305278285489\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0014151686468265123\n",
      "Average test loss: 6.8309712370502655\n",
      "Epoch 245/300\n",
      "Average training loss: 0.001458393844568895\n",
      "Average test loss: 46.58411281361617\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0014529220153474145\n",
      "Average test loss: 82.73158498090588\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0016189646614301536\n",
      "Average test loss: 156.2748496840944\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0014210818938704\n",
      "Average test loss: 22.00433735003571\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0014238479320580761\n",
      "Average test loss: 0.013153308158947361\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0013531432795441812\n",
      "Average test loss: 5.656525710751613\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0013839124829197924\n",
      "Average test loss: 5.705519870161803\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0013458782164379954\n",
      "Average test loss: 303.2457035706664\n",
      "Epoch 253/300\n",
      "Average training loss: 0.001331937822823723\n",
      "Average test loss: 68.84795617671104\n",
      "Epoch 254/300\n",
      "Average training loss: 0.001329065106705659\n",
      "Average test loss: 28236.49083321615\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0013213336679877507\n",
      "Average test loss: 10553.064689764924\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0017495365680402352\n",
      "Average test loss: 936.1706706779433\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0013648592668275038\n",
      "Average test loss: 998447.3777404124\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0013084860164672136\n",
      "Average test loss: 405.06287597920624\n",
      "Epoch 259/300\n",
      "Average training loss: 0.001293544899051388\n",
      "Average test loss: 150610.5207549448\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0013009278220124543\n",
      "Average test loss: 23264.63546534303\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0013318635563676557\n",
      "Average test loss: 351.83139053368984\n",
      "Epoch 262/300\n",
      "Average training loss: 0.001327048844192177\n",
      "Average test loss: 8.866434500977396\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0013170813150289987\n",
      "Average test loss: 1.3349225935668996\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0014574182842754656\n",
      "Average test loss: 0.010304249207385712\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0013653197401306695\n",
      "Average test loss: 4515.88949424235\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0013535564962981477\n",
      "Average test loss: 644.8111055880851\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0013261477793049482\n",
      "Average test loss: 5370.373734933507\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0013199136862531305\n",
      "Average test loss: 876.5877504653765\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0013089139239034718\n",
      "Average test loss: 164382.61583041036\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0013041068783236874\n",
      "Average test loss: 102.37570218842787\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0012939402950513693\n",
      "Average test loss: 383.17849825921843\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0013126942619370917\n",
      "Average test loss: 25832.80142092879\n",
      "Epoch 273/300\n",
      "Average training loss: 0.001302756941359904\n",
      "Average test loss: 87.76462426763152\n",
      "Epoch 274/300\n",
      "Average training loss: 0.001295276791892118\n",
      "Average test loss: 19.851008209258318\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001317528105030457\n",
      "Average test loss: 118992.31031375416\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0012908639113108317\n",
      "Average test loss: 3534.1642221953116\n",
      "Epoch 277/300\n",
      "Average training loss: 0.001286359194562667\n",
      "Average test loss: 38465574.80359311\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0012839893270283937\n",
      "Average test loss: 238.26544856593944\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0012847267786144383\n",
      "Average test loss: 1812.4075074252305\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0012749406343532932\n",
      "Average test loss: 1379.7211127867795\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0012838605244437026\n",
      "Average test loss: 590245.2357892023\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001280894019951423\n",
      "Average test loss: 130861.50960027218\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0012706420958663027\n",
      "Average test loss: 88530.74754092273\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0012956518431504567\n",
      "Average test loss: 12599.388965694325\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0017974419381676448\n",
      "Average test loss: 370.1778133538502\n",
      "Epoch 286/300\n",
      "Average training loss: 0.001652520827949047\n",
      "Average test loss: 126820.21779139333\n",
      "Epoch 287/300\n",
      "Average training loss: 0.001450926969655686\n",
      "Average test loss: 2946.7691119108636\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0013682686073912514\n",
      "Average test loss: 24179.436517787137\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0013406149088922474\n",
      "Average test loss: 9139265.930143218\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0013198736165132788\n",
      "Average test loss: 5024926.326700491\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0013091904652408428\n",
      "Average test loss: 11856218.403458333\n",
      "Epoch 292/300\n",
      "Average training loss: 0.001308513893487139\n",
      "Average test loss: 847016.2882747336\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0013128312493984897\n",
      "Average test loss: 20278.68928861152\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0013116875507144465\n",
      "Average test loss: 10969.993442527615\n",
      "Epoch 295/300\n",
      "Average training loss: 0.001307213936621944\n",
      "Average test loss: 156724.3357690124\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0013194084216116204\n",
      "Average test loss: 130210.42226941508\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0012847674552144276\n",
      "Average test loss: 171046.32419062703\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0012841999859859545\n",
      "Average test loss: 37208042.3789101\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 337.39328940248157\n",
      "Average test loss: 348.6103914711194\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03700161833730009\n",
      "Average test loss: 3.935978501962291\n",
      "Epoch 3/300\n",
      "Average training loss: 0.021884116214182642\n",
      "Average test loss: 0.3488679942091306\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01586554248382648\n",
      "Average test loss: 0.011149862823386987\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012742513145009677\n",
      "Average test loss: 6.827469114758902\n",
      "Epoch 6/300\n",
      "Average training loss: 0.010778625006477038\n",
      "Average test loss: 52.299545527668876\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009424701538350847\n",
      "Average test loss: 0.01696451213045253\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008400674287643697\n",
      "Average test loss: 0.25987182804445424\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007646070792029301\n",
      "Average test loss: 0.5089518770376841\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006993191349837515\n",
      "Average test loss: 0.00646460115040342\n",
      "Epoch 11/300\n",
      "Average training loss: 0.006417433443996642\n",
      "Average test loss: 73.94747552496526\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0059065904613170355\n",
      "Average test loss: 0.19351854942035343\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005517338377734026\n",
      "Average test loss: 34.71945200109813\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005169398465504249\n",
      "Average test loss: 1.4782264547430806\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004812888133856985\n",
      "Average test loss: 0.004362014388458596\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004530371612972683\n",
      "Average test loss: 0.5445711661196417\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004262652198680573\n",
      "Average test loss: 0.10979056857029597\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004019985521419181\n",
      "Average test loss: 26.08548284210099\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0038134525778392952\n",
      "Average test loss: 0.02163204102135367\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003611785280621714\n",
      "Average test loss: 20.376038328036667\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003443763360588087\n",
      "Average test loss: 0.02268975470546219\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0032715700862722265\n",
      "Average test loss: 1.0926145109985437\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0031039957424832715\n",
      "Average test loss: 0.009715154430518548\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0029675469520605274\n",
      "Average test loss: 9.378943823581768\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0028237522995720305\n",
      "Average test loss: 0.12379491602670815\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002690506987273693\n",
      "Average test loss: 0.2752050596119629\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00257460524700582\n",
      "Average test loss: 0.01652276256846057\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002485352284481956\n",
      "Average test loss: 0.002784959054241578\n",
      "Epoch 29/300\n",
      "Average training loss: 0.002366054914581279\n",
      "Average test loss: 0.34360602371974125\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0022771807476464245\n",
      "Average test loss: 0.04361492721819216\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0022098282139955297\n",
      "Average test loss: 2316.9199944517445\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0021373632728225655\n",
      "Average test loss: 0.06071633854094479\n",
      "Epoch 33/300\n",
      "Average training loss: 0.002071425262942082\n",
      "Average test loss: 0.2290906141160263\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0020225359877157545\n",
      "Average test loss: 106.38173781748779\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0020293626259598467\n",
      "Average test loss: 0.15038042368160354\n",
      "Epoch 36/300\n",
      "Average training loss: 0.001932050936544935\n",
      "Average test loss: 167.1185475803564\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0019024450509912438\n",
      "Average test loss: 0.05716417981766992\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0018364656870770786\n",
      "Average test loss: 0.05331930762343109\n",
      "Epoch 39/300\n",
      "Average training loss: 0.001874231323496335\n",
      "Average test loss: 3.030033618259761\n",
      "Epoch 40/300\n",
      "Average training loss: 0.001795239773578942\n",
      "Average test loss: 379.81057067626716\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0017340355023948682\n",
      "Average test loss: 756.1254018640195\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0017467888460184137\n",
      "Average test loss: 1722.5450788031685\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0016879221465852525\n",
      "Average test loss: 7000.629833732289\n",
      "Epoch 44/300\n",
      "Average training loss: 0.001662611309438944\n",
      "Average test loss: 0.13148312303123788\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0016233047206575672\n",
      "Average test loss: 11006.450436022686\n",
      "Epoch 46/300\n",
      "Average training loss: 0.001605667823408213\n",
      "Average test loss: 1.7262379545979203\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0019240193055528733\n",
      "Average test loss: 63.12696671776887\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0015950534466861023\n",
      "Average test loss: 25.809709116329127\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0015756472741357153\n",
      "Average test loss: 0.22704637841797537\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0017119460347005064\n",
      "Average test loss: 0.01739536950695846\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0015551175986313157\n",
      "Average test loss: 0.19969285080147287\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0015130324734685323\n",
      "Average test loss: 36.67886326905588\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0015077454898920323\n",
      "Average test loss: 0.2013527440899569\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0015085232061230473\n",
      "Average test loss: 158.11984302543848\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0014735442978433438\n",
      "Average test loss: 5.8594744981306395\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0015195843643612333\n",
      "Average test loss: 0.008655148056232267\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0015227926155138346\n",
      "Average test loss: 4447.989497926939\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0014574451977904471\n",
      "Average test loss: 1004.2838237042597\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0014072009262421892\n",
      "Average test loss: 717.5485030356397\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0015668402850213979\n",
      "Average test loss: 4.219809370002192\n",
      "Epoch 61/300\n",
      "Average training loss: 0.001449031799721221\n",
      "Average test loss: 0.023089019257989195\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0013990743382730417\n",
      "Average test loss: 3.3693598057318273\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0015154101072500149\n",
      "Average test loss: 0.005928898671331505\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0014670759686786267\n",
      "Average test loss: 0.0020773492958396675\n",
      "Epoch 65/300\n",
      "Average training loss: 0.001420016378422992\n",
      "Average test loss: 0.13031268268823623\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0013718746923841536\n",
      "Average test loss: 1.5437794670300145\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0013963943082425329\n",
      "Average test loss: 0.027715328809598254\n",
      "Epoch 68/300\n",
      "Average training loss: 0.001348466997138328\n",
      "Average test loss: 2.979252231795134\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0013447836650949385\n",
      "Average test loss: 2.9828572650042675\n",
      "Epoch 70/300\n",
      "Average training loss: 0.001300035626317064\n",
      "Average test loss: 2.2866042225844123\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0012857473012473847\n",
      "Average test loss: 677.3017711380298\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0012703199951599041\n",
      "Average test loss: 0.03363638365993069\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0012544427801751427\n",
      "Average test loss: 223.4308237940996\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0012426143850510318\n",
      "Average test loss: 1496.8768414389076\n",
      "Epoch 75/300\n",
      "Average training loss: 0.001208393427491602\n",
      "Average test loss: 662.2313026141553\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0012081537739270264\n",
      "Average test loss: 69356.18515277778\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0011923280988509455\n",
      "Average test loss: 32673622.166202217\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0011861262278010448\n",
      "Average test loss: 363341748.5266944\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0012473479447265466\n",
      "Average test loss: 218.186755221278\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0032889143193347587\n",
      "Average test loss: 0.9495843447910415\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0027117895715766485\n",
      "Average test loss: 0.058918071546488336\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0024287957097921106\n",
      "Average test loss: 0.3366434910075946\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002250475871687134\n",
      "Average test loss: 0.7192043030899432\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0020936644369115433\n",
      "Average test loss: 0.08287036064132634\n",
      "Epoch 87/300\n",
      "Average training loss: 0.002004086993738181\n",
      "Average test loss: 0.5672160911526944\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0018971460044590962\n",
      "Average test loss: 0.10587188233838728\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0018292363887238833\n",
      "Average test loss: 36.99017632334721\n",
      "Epoch 90/300\n",
      "Average training loss: 0.001765138676079611\n",
      "Average test loss: 0.1155365954393314\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0017040239001313846\n",
      "Average test loss: 93.86771998649836\n",
      "Epoch 92/300\n",
      "Average training loss: 0.001643797441282206\n",
      "Average test loss: 26.83597115471297\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0016034886338230636\n",
      "Average test loss: 0.08141856257017288\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0015658649562133684\n",
      "Average test loss: 2.1649185323309568\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0015252556480053398\n",
      "Average test loss: 1.0221245419374771\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0014858400102497802\n",
      "Average test loss: 4.1922907778951854\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0014533939141159256\n",
      "Average test loss: 0.32233900677412747\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0014380438863299788\n",
      "Average test loss: 12.934586029384285\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0014151881371314327\n",
      "Average test loss: 121.64227370600817\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0013863232583842344\n",
      "Average test loss: 0.016348837762243218\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0013775020447663136\n",
      "Average test loss: 127.888709932492\n",
      "Epoch 102/300\n",
      "Average training loss: 0.00134400434512645\n",
      "Average test loss: 20.518629838598272\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0013224506969046262\n",
      "Average test loss: 0.009295195176887016\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0013095685242571764\n",
      "Average test loss: 0.36970124054244824\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0012923705258613659\n",
      "Average test loss: 66.5382368153851\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0012996492371377018\n",
      "Average test loss: 4.955313002074758\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0012469790125679639\n",
      "Average test loss: 0.026255926189323268\n",
      "Epoch 108/300\n",
      "Average training loss: 0.001224523772744255\n",
      "Average test loss: 10.100376119752312\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0012201262577436864\n",
      "Average test loss: 9.091210830209155\n",
      "Epoch 110/300\n",
      "Average training loss: 0.001344447005747093\n",
      "Average test loss: 0.6644469989418156\n",
      "Epoch 111/300\n",
      "Average training loss: 0.001291508760303259\n",
      "Average test loss: 0.001385901843301124\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0012854842697787615\n",
      "Average test loss: 3064.0044995461694\n",
      "Epoch 113/300\n",
      "Average training loss: 0.001289536462786297\n",
      "Average test loss: 0.7914600610844791\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0012580738950313793\n",
      "Average test loss: 11.64678673325867\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0012429674906242224\n",
      "Average test loss: 4.018670203667031\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0012235935955411858\n",
      "Average test loss: 1086.2359860654092\n",
      "Epoch 117/300\n",
      "Average training loss: 0.001241873521461255\n",
      "Average test loss: 7300.90259375\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0011987579826058612\n",
      "Average test loss: 41.203472522606454\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0011683462193339234\n",
      "Average test loss: 7.95448569166205\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0011506596748820609\n",
      "Average test loss: 728.7587806636567\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0026900488468300965\n",
      "Average test loss: 44.48314128708374\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0016042323188028402\n",
      "Average test loss: 0.7214125642250809\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0014068031449698739\n",
      "Average test loss: 2.8386584986378955\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0013598540970641705\n",
      "Average test loss: 0.9555123270186595\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0013985819996127653\n",
      "Average test loss: 284.62951993436405\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0012832193239074613\n",
      "Average test loss: 1951.2193253451578\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0012514871344901622\n",
      "Average test loss: 50.706164814183694\n",
      "Epoch 128/300\n",
      "Average training loss: 0.001218825064237333\n",
      "Average test loss: 644.7818974929075\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0012351710413479142\n",
      "Average test loss: 536.2711878398061\n",
      "Epoch 130/300\n",
      "Average training loss: 0.001433893819960455\n",
      "Average test loss: 0.07856583063138856\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0013912476094232666\n",
      "Average test loss: 4.703487368076522\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0015856144121951527\n",
      "Average test loss: 0.004476423572645419\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0014377439544639653\n",
      "Average test loss: 4.165221803095606\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0013765348179472817\n",
      "Average test loss: 3.5877478168681263\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0013188572436985042\n",
      "Average test loss: 0.001411641332320869\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0014003827850230866\n",
      "Average test loss: 0.0936999650866621\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0013779897176039715\n",
      "Average test loss: 1.9528091020079124\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0013196954600926904\n",
      "Average test loss: 38859.00742656525\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0012628755619128545\n",
      "Average test loss: 2.1203709237889075\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0012387014596412578\n",
      "Average test loss: 4164.258794681919\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0012355872100840012\n",
      "Average test loss: 13.758311883250872\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0011885684050309162\n",
      "Average test loss: 19.51334911508734\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0012029510002790225\n",
      "Average test loss: 714.0598433004767\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0011856845937048396\n",
      "Average test loss: 9.24555562723666\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0011855033421888947\n",
      "Average test loss: 1.480108252173652\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0011457331263356738\n",
      "Average test loss: 790.1173344082873\n",
      "Epoch 147/300\n",
      "Average training loss: 0.001158651711936626\n",
      "Average test loss: 0.05745463208719674\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0011486984827659196\n",
      "Average test loss: 33.35109904745449\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05319990832099898\n",
      "Average test loss: 0.05239402564449443\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00292725553487738\n",
      "Average test loss: 0.18631810235480467\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0025205010818317533\n",
      "Average test loss: 0.011763185909224882\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0022718823021277783\n",
      "Average test loss: 0.22085305021372106\n",
      "Epoch 153/300\n",
      "Average training loss: 0.002093219378549192\n",
      "Average test loss: 0.07256122601528962\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0019484725433091322\n",
      "Average test loss: 0.0018165828403499391\n",
      "Epoch 155/300\n",
      "Average training loss: 0.001815158713919421\n",
      "Average test loss: 0.028962414426936044\n",
      "Epoch 156/300\n",
      "Average training loss: 0.001719668738440507\n",
      "Average test loss: 0.012218267284954587\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0016358997360285785\n",
      "Average test loss: 0.002529676194820139\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0015785783897671434\n",
      "Average test loss: 0.06784209627999614\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0015185340695704024\n",
      "Average test loss: 0.005764263099680344\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0014767542565241455\n",
      "Average test loss: 0.003146866586887174\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0014338948757180738\n",
      "Average test loss: 0.037157277505844834\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0013992607508682543\n",
      "Average test loss: 0.0841108558600665\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001361358531905959\n",
      "Average test loss: 0.18083657171691042\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0013389333282700843\n",
      "Average test loss: 2.6710690668233568\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0013039274448528885\n",
      "Average test loss: 5.648410059875085\n",
      "Epoch 166/300\n",
      "Average training loss: 0.001276361359231588\n",
      "Average test loss: 0.45527926889310283\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0012600475238222215\n",
      "Average test loss: 10.273483073030494\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0012359172406399416\n",
      "Average test loss: 1.0708600945385793\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0012192403165002664\n",
      "Average test loss: 0.1563387963551407\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0012190936598926783\n",
      "Average test loss: 0.005359517093747854\n",
      "Epoch 171/300\n",
      "Average training loss: 0.00119817271673431\n",
      "Average test loss: 1.533841007170578\n",
      "Epoch 172/300\n",
      "Average training loss: 0.001175288082876553\n",
      "Average test loss: 6.324078649145862\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0011564558868089483\n",
      "Average test loss: 3.9003127405531703\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0011520770112466482\n",
      "Average test loss: 7.16482338875408\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0011367145979570019\n",
      "Average test loss: 53.65797553451856\n",
      "Epoch 176/300\n",
      "Average training loss: 0.001126956263350116\n",
      "Average test loss: 274.7135480652428\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0011332890463785993\n",
      "Average test loss: 0.22083712377026676\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023441569588095363\n",
      "Average test loss: 358.22154783873776\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0027173207801663214\n",
      "Average test loss: 19691.53128059896\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0021729939334715407\n",
      "Average test loss: 3.0002486119589045\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0019515213255460063\n",
      "Average test loss: 3.958433841588079\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0017840597201138734\n",
      "Average test loss: 1454.6565878391025\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0016752709528017374\n",
      "Average test loss: 457.8900030568242\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001603816400282085\n",
      "Average test loss: 118.2994029162104\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0015582780878370007\n",
      "Average training loss: 0.0014074953263625502\n",
      "Average test loss: 31.975467327147307\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0013597941166824765\n",
      "Average test loss: 69.47381419773565\n",
      "Epoch 189/300\n",
      "Average training loss: 0.001312794096923123\n",
      "Average test loss: 22.731488292053548\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0012959476697983012\n",
      "Average test loss: 0.3416028987160987\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0012508412543684245\n",
      "Average test loss: 144.02609094892185\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0012280311602064305\n",
      "Average test loss: 24.462251202723838\n",
      "Epoch 193/300\n",
      "Average training loss: 0.001225920242878298\n",
      "Average test loss: 48.334294797112335\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0012671919602693783\n",
      "Average test loss: 7136.957631888133\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0012008326983907157\n",
      "Average test loss: 371.4160606679763\n",
      "Epoch 196/300\n",
      "Average training loss: 0.001210334811773565\n",
      "Average test loss: 66359.20306765407\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0012187555536317329\n",
      "Average test loss: 286.0038787128586\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0011823137821629643\n",
      "Average test loss: 777.6885357656549\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0011555426249591013\n",
      "Average test loss: 6007.5034956645095\n",
      "Epoch 200/300\n",
      "Average training loss: 0.001144328838472979\n",
      "Average test loss: 1504.0937940524925\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0011310792988062732\n",
      "Average test loss: 59425.379945213324\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0011108792637371354\n",
      "Average test loss: 46727.4054609402\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00111279268293745\n",
      "Average test loss: 0.0231835477813147\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001175935804947383\n",
      "Average test loss: 14898.246186527394\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0011047752494406369\n",
      "Average test loss: 521.3888577337486\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0010783338015899061\n",
      "Average test loss: 3493.6187284501416\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0010779083531556857\n",
      "Average test loss: 26.378405698822192\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0010758688059221539\n",
      "Average test loss: 8748.42030694183\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0011837978428747092\n",
      "Average test loss: 350.0900155192301\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0011385678875570496\n",
      "Average test loss: 784.9602687548619\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0011408970901328657\n",
      "Average test loss: 59.27479059931181\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0011253114711079332\n",
      "Average test loss: 0.6798746286009749\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0011137297454600532\n",
      "Average test loss: 0.868211367941565\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0011017745565623045\n",
      "Average test loss: 7.421497166949842\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0010956274171670278\n",
      "Average test loss: 211.08327110036836\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0010725791069885922\n",
      "Average test loss: 9.834739422015328\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0010605457106915614\n",
      "Average test loss: 17222.795068347477\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0010852889627114765\n",
      "Average test loss: 33514.103158825324\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0010669232604818212\n",
      "Average test loss: 1579502.0672809554\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0010654252087179984\n",
      "Average test loss: 5652.372659346756\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0010520364638521439\n",
      "Average test loss: 8450.074638541633\n",
      "Epoch 222/300\n",
      "Average training loss: 0.001039277804808484\n",
      "Average test loss: 109.04433527648408\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0010290997586109572\n",
      "Average test loss: 65239.27924673748\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0010880271951254044\n",
      "Average test loss: 9.63859023701855\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0010570048028603195\n",
      "Average test loss: 577.3509255499931\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0010439066192031734\n",
      "Average test loss: 3.210476180332609\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0010273791647826631\n",
      "Average test loss: 4.218678032513708\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0010995266982354224\n",
      "Average test loss: 4.235222332946853\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0010011438052169978\n",
      "Average test loss: 8.957558837469564\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00102203194786691\n",
      "Average test loss: 0.06088720915342371\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0010089812968443665\n",
      "Average test loss: 807.5779189535806\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0009829616020433606\n",
      "Average test loss: 10.264450703766197\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0009787251027818355\n",
      "Average test loss: 3665.360654182808\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0009783680986923476\n",
      "Average test loss: 22609.316389756943\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0010802806202943125\n",
      "Average test loss: 6958455.686222223\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0009827114481271969\n",
      "Average test loss: 22668.37303751038\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0009781725256082913\n",
      "Average test loss: 8227.93047558814\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0009728334215469658\n",
      "Average test loss: 73677.40468308872\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0010047821029503313\n",
      "Average test loss: 2.202430700776995\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0009854413876827393\n",
      "Average test loss: 56.40969365532986\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0010195833756070998\n",
      "Average test loss: 40.274100049169526\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0009697614007422494\n",
      "Average test loss: 2698.516547784325\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0009584929950328337\n",
      "Average test loss: 15604.567084619728\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0009918763677900036\n",
      "Average test loss: 1.4816158651592106\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0009615038050752547\n",
      "Average test loss: 260.78892239816525\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0009753871893820663\n",
      "Average test loss: 1.9391296401406741\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0009677805958522691\n",
      "Average test loss: 151.0013000009151\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0009435596204259329\n",
      "Average test loss: 1.786659296097182\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0009439076341999073\n",
      "Average test loss: 1.6974599584323458\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0009405349136019746\n",
      "Average test loss: 39487.91116180712\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0009458592642719546\n",
      "Average test loss: 135.92651683613204\n",
      "Epoch 252/300\n",
      "Average training loss: 0.000929063349082652\n",
      "Average test loss: 47731.481484157506\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0009500919397299488\n",
      "Average test loss: 1.8923028097928811\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0009599006626134117\n",
      "Average test loss: 21.322048407365877\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0009516138363008698\n",
      "Average test loss: 2174.1357770551963\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0009206257578399447\n",
      "Average test loss: 108.06956468008975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0009305514463533958\n",
      "Average test loss: 120.76618263810656\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0009937444855459034\n",
      "Average test loss: 0.001385532851744857\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0009831276022984337\n",
      "Average test loss: 24.279119585414108\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0009499310806600584\n",
      "Average test loss: 37.69154488050772\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0011112279838158025\n",
      "Average test loss: 2.0197024388832765\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0010154940208627118\n",
      "Average test loss: 10429.850084813394\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0009221259846559001\n",
      "Average test loss: 3145.30689105334\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0009061211614559094\n",
      "Average test loss: 15.760622227621885\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0009056464691853357\n",
      "Average test loss: 33.129670447831764\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0009084935882128775\n",
      "Average test loss: 0.11237817498456894\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0009156143184647792\n",
      "Average test loss: 21.87639911236324\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0009256358375358913\n",
      "Average test loss: 0.4073789763280915\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0009035283809320794\n",
      "Average test loss: 142.96005957381178\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0009079394106132288\n",
      "Average test loss: 23.51662963687794\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0008944481010548771\n",
      "Average test loss: 497.5253243972671\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0008960664066382581\n",
      "Average test loss: 29.160075858305724\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0009058641210819284\n",
      "Average test loss: 130.5320945724911\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0008973833795947333\n",
      "Average test loss: 0.15660762579904663\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0008937741379874448\n",
      "Average test loss: 627.2104980115494\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0009000033733641935\n",
      "Average test loss: 101.17685081800032\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0008878427851531241\n",
      "Average test loss: 10643.035356557744\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0008937559453770518\n",
      "Average test loss: 53.67311580828495\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0008835156519586842\n",
      "Average test loss: 0.4656415877141472\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0008908580912587544\n",
      "Average test loss: 808.6429330028469\n",
      "Epoch 283/300\n",
      "Average training loss: 0.000873645007610321\n",
      "Average test loss: 73.71299264155162\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0008766089509137803\n",
      "Average test loss: 6.830338340109318\n",
      "Epoch 285/300\n",
      "Average training loss: 0.000872008419336958\n",
      "Average test loss: 9.078221347264531\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0008667985112923715\n",
      "Average test loss: 490.2515653507254\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0008809376528176168\n",
      "Average test loss: 277.50872577558954\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0008797920852796071\n",
      "Average test loss: 477.1599687099751\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0008701318386010826\n",
      "Average test loss: 26783.53731814236\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0008635695564250151\n",
      "Average test loss: 0.014638783728082975\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0008876667662730647\n",
      "Average test loss: 2.632420024716192\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0009777089387385382\n",
      "Average test loss: 4.388302100291269\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0009374458972468145\n",
      "Average test loss: 8.240529611606151\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0009321671379212703\n",
      "Average test loss: 15.85883666694868\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0009379193655008243\n",
      "Average test loss: 9.829565696540909\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0008987226620213025\n",
      "Average test loss: 1.9268755069110128\n",
      "Epoch 297/300\n",
      "Average training loss: 0.000858802994247526\n",
      "Average test loss: 218.10664330850108\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0008586328123799629\n",
      "Average test loss: 258.8060537452507\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0008630230223449568\n",
      "Average test loss: 5.963533441989372\n",
      "Epoch 300/300\n",
      "Average training loss: 0.000851199335573862\n",
      "Average test loss: 0.5834608170995489\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.01/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: -1.39\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -5.49\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -6.48\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -6.06\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -6.61\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -2.91\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -2.75\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -1.13\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 5.03\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 6.67\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 6.77\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 7.50\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 7.50\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 9.18\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 11.35\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 9.96\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 15.22\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 19.35\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 7.70\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 18.33\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 22.05\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 22.38\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 9.21\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 21.62\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 23.74\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 16.71\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 23.28\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 19.84\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 25.70\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.65\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 1.59\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -6.45\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -4.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -3.25\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -4.20\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -3.19\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -1.44\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 1.58\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 4.22\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 4.37\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 5.36\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 8.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 10.31\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 11.42\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 16.56\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 18.11\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 9.60\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 19.09\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 21.96\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 21.80\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 12.78\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 21.76\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 20.49\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 24.47\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 25.45\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 17.21\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 23.72\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 25.01\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 0 across 2500 images: -3.78\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -2.61\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -1.02\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -1.85\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 1.38\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 4.44\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -0.95\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 1.95\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 0.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: -6.31\n",
      "Average PSNR for Projection Layer 10 across 2500 images: -7.66\n",
      "Average PSNR for Projection Layer 11 across 2500 images: -4.92\n",
      "Average PSNR for Projection Layer 12 across 2500 images: -6.75\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 0.82\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 8.49\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 10.21\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 15.95\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 14.66\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 20.99\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 21.37\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 24.21\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 24.28\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 14.61\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 21.15\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 25.10\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 23.69\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 23.88\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.93\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.65\n",
      "Average PSNR for Projection Layer 0 across 2500 images: -3.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -2.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -4.27\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -3.77\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -8.76\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -9.21\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -9.87\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -10.78\n",
      "Average PSNR for Projection Layer 8 across 2500 images: -10.56\n",
      "Average PSNR for Projection Layer 9 across 2500 images: -11.27\n",
      "Average PSNR for Projection Layer 10 across 2500 images: -10.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: -10.54\n",
      "Average PSNR for Projection Layer 12 across 2500 images: -5.36\n",
      "Average PSNR for Projection Layer 13 across 2500 images: -0.04\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 3.48\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 9.76\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 9.87\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 15.46\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 18.02\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 21.75\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 24.41\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 16.88\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 21.11\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 23.65\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.15\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.50\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
