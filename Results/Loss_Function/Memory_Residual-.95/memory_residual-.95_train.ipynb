{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.95)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14316912149058447\n",
      "Average test loss: 0.01074129535506169\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05694065115849177\n",
      "Average test loss: 0.009512575310137536\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05094008590446578\n",
      "Average test loss: 0.010314731803205279\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04776408838894632\n",
      "Average test loss: 0.00871007666695449\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0457396307223373\n",
      "Average test loss: 0.008169197048577997\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04350577882925669\n",
      "Average test loss: 0.008784693003528648\n",
      "Epoch 7/300\n",
      "Average training loss: 0.042320618371168774\n",
      "Average test loss: 0.00852071670608388\n",
      "Epoch 8/300\n",
      "Average training loss: 0.041056111050976646\n",
      "Average test loss: 0.007928644832637576\n",
      "Epoch 9/300\n",
      "Average training loss: 0.040305623067749874\n",
      "Average test loss: 0.007830500847763485\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03946503057082494\n",
      "Average test loss: 0.008112426676683956\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03883634003996849\n",
      "Average test loss: 0.007295445192605257\n",
      "Epoch 12/300\n",
      "Average training loss: 0.038086259583632154\n",
      "Average test loss: 0.007113822343862719\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03736560320191913\n",
      "Average test loss: 0.007853543028649356\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03689383248488108\n",
      "Average test loss: 0.006961141049447987\n",
      "Epoch 15/300\n",
      "Average training loss: 0.036430853115187754\n",
      "Average test loss: 0.0072654015918572745\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0361313104695744\n",
      "Average test loss: 0.006999634109851387\n",
      "Epoch 17/300\n",
      "Average training loss: 0.035780218280024\n",
      "Average test loss: 0.006948050985733668\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03525584950380855\n",
      "Average test loss: 0.006719542881266938\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03492181796166632\n",
      "Average test loss: 0.006725191069145997\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03464240240388446\n",
      "Average test loss: 0.00673318046455582\n",
      "Epoch 21/300\n",
      "Average training loss: 0.034304534622364574\n",
      "Average test loss: 0.006744858915607135\n",
      "Epoch 22/300\n",
      "Average training loss: 0.034063857576913305\n",
      "Average test loss: 0.0064675094981988274\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03386455265018675\n",
      "Average test loss: 0.006816950757470396\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03359863406750891\n",
      "Average test loss: 0.006412076869358619\n",
      "Epoch 25/300\n",
      "Average training loss: 0.033312726467847824\n",
      "Average test loss: 0.0066256922500001056\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03323474292291535\n",
      "Average test loss: 0.0065524119151135285\n",
      "Epoch 27/300\n",
      "Average training loss: 0.032931102948056325\n",
      "Average test loss: 0.00633589901174936\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03268350770407253\n",
      "Average test loss: 0.006445953908479876\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03259700646665361\n",
      "Average test loss: 0.006339601000150045\n",
      "Epoch 30/300\n",
      "Average training loss: 0.032579799251423944\n",
      "Average test loss: 0.007535930253565312\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03226109554370244\n",
      "Average test loss: 0.006376361767244008\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03207663762238291\n",
      "Average test loss: 0.0064901762240462835\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03195801780621211\n",
      "Average test loss: 0.0061483167045646245\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03186120518379741\n",
      "Average test loss: 0.006380668582187759\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03167009761929512\n",
      "Average test loss: 0.00634616394340992\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03156574728091558\n",
      "Average test loss: 0.006529560322562853\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03148809794419342\n",
      "Average test loss: 0.006175215163578589\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03138127107421557\n",
      "Average test loss: 0.0060217331747214\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03118574757874012\n",
      "Average test loss: 0.006072920038468308\n",
      "Epoch 40/300\n",
      "Average training loss: 0.031092339808742205\n",
      "Average test loss: 0.006239437150044574\n",
      "Epoch 41/300\n",
      "Average training loss: 0.031105769841207397\n",
      "Average test loss: 0.005953936085104942\n",
      "Epoch 42/300\n",
      "Average training loss: 0.030942637883954578\n",
      "Average test loss: 0.006128474586953719\n",
      "Epoch 43/300\n",
      "Average training loss: 0.030885430329375796\n",
      "Average test loss: 0.006103696030047205\n",
      "Epoch 44/300\n",
      "Average training loss: 0.030716639988952212\n",
      "Average test loss: 0.00629533797254165\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03065646054181788\n",
      "Average test loss: 0.006186560769048002\n",
      "Epoch 46/300\n",
      "Average training loss: 0.030595570320884388\n",
      "Average test loss: 0.005993009977870518\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03060093379020691\n",
      "Average test loss: 0.00602314661981331\n",
      "Epoch 48/300\n",
      "Average training loss: 0.030515037829677263\n",
      "Average test loss: 0.007003149970538087\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03027850314312511\n",
      "Average test loss: 0.005978755338324441\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03029789239499304\n",
      "Average test loss: 0.0066032520375318\n",
      "Epoch 51/300\n",
      "Average training loss: 0.030259168720907635\n",
      "Average test loss: 0.006901372572200166\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03012724904881583\n",
      "Average test loss: 0.00606205515894625\n",
      "Epoch 53/300\n",
      "Average training loss: 0.030130902810229194\n",
      "Average test loss: 0.005973140602310498\n",
      "Epoch 54/300\n",
      "Average training loss: 0.029957814517948364\n",
      "Average test loss: 0.005948227912601497\n",
      "Epoch 55/300\n",
      "Average training loss: 0.029961544955770174\n",
      "Average test loss: 0.005906420456038581\n",
      "Epoch 56/300\n",
      "Average training loss: 0.029938207894563675\n",
      "Average test loss: 0.006333640049729083\n",
      "Epoch 57/300\n",
      "Average training loss: 0.029802196575535667\n",
      "Average test loss: 0.005907993888689412\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02977552156481478\n",
      "Average test loss: 0.006092359207570553\n",
      "Epoch 59/300\n",
      "Average training loss: 0.029684838939044212\n",
      "Average test loss: 0.005852124728676346\n",
      "Epoch 60/300\n",
      "Average training loss: 0.029657070580455993\n",
      "Average test loss: 0.005851622465584013\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02958624222046799\n",
      "Average test loss: 0.006434763941086001\n",
      "Epoch 62/300\n",
      "Average training loss: 0.029574033993813725\n",
      "Average test loss: 0.006481355784667863\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02952793416546451\n",
      "Average test loss: 0.005992018707096577\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029472585244311227\n",
      "Average test loss: 0.006241713505238294\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029503797638747428\n",
      "Average test loss: 0.05970172797309028\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07349186656541294\n",
      "Average test loss: 0.006865873107479679\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03823858383960194\n",
      "Average test loss: 0.006194207317299313\n",
      "Epoch 68/300\n",
      "Average training loss: 0.035418416178888745\n",
      "Average test loss: 0.006052868320296208\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03392390194535255\n",
      "Average test loss: 0.005987194666845931\n",
      "Epoch 70/300\n",
      "Average training loss: 0.032754466480678986\n",
      "Average test loss: 0.005902016700969802\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03195125333799256\n",
      "Average test loss: 0.0058949796598818565\n",
      "Epoch 72/300\n",
      "Average training loss: 0.031287209684650104\n",
      "Average test loss: 0.005987529264969958\n",
      "Epoch 73/300\n",
      "Average training loss: 0.030795063893000286\n",
      "Average test loss: 0.006008225960863961\n",
      "Epoch 74/300\n",
      "Average training loss: 0.030364634265502295\n",
      "Average test loss: 0.005910951929166913\n",
      "Epoch 75/300\n",
      "Average training loss: 0.030082058995962144\n",
      "Average test loss: 0.006100496679544449\n",
      "Epoch 76/300\n",
      "Average training loss: 0.029821473729279305\n",
      "Average test loss: 0.005878412988450792\n",
      "Epoch 77/300\n",
      "Average training loss: 0.029622996346818076\n",
      "Average test loss: 0.005912670815156566\n",
      "Epoch 78/300\n",
      "Average training loss: 0.029532759143246545\n",
      "Average test loss: 0.006084262767185767\n",
      "Epoch 79/300\n",
      "Average training loss: 0.029412149796883266\n",
      "Average test loss: 0.005993813447654248\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02937746058901151\n",
      "Average test loss: 0.00600401416213976\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02935266237292025\n",
      "Average test loss: 0.006014832153502438\n",
      "Epoch 82/300\n",
      "Average training loss: 0.029279974449012015\n",
      "Average test loss: 0.006014134133855502\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02916374522447586\n",
      "Average test loss: 0.006171581350680855\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02917249522606532\n",
      "Average test loss: 0.005895740263577964\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02915486267871327\n",
      "Average test loss: 0.005824538006550736\n",
      "Epoch 86/300\n",
      "Average training loss: 0.029026869154638715\n",
      "Average test loss: 0.006089691055524681\n",
      "Epoch 87/300\n",
      "Average training loss: 0.029001910517613094\n",
      "Average test loss: 0.0059232233042518295\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02892614512807793\n",
      "Average test loss: 0.005938747839381297\n",
      "Epoch 89/300\n",
      "Average training loss: 0.028938268916474448\n",
      "Average test loss: 0.005870571703132656\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02891042145424419\n",
      "Average test loss: 0.0058791802107459965\n",
      "Epoch 91/300\n",
      "Average training loss: 0.028830627673202092\n",
      "Average test loss: 0.006530192631814215\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02875617183248202\n",
      "Average test loss: 0.005865529448207882\n",
      "Epoch 93/300\n",
      "Average training loss: 0.028747511635224026\n",
      "Average test loss: 0.005830909950037797\n",
      "Epoch 94/300\n",
      "Average training loss: 0.028718834423356585\n",
      "Average test loss: 0.005884313972045978\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02865024269786146\n",
      "Average test loss: 0.006082823781503571\n",
      "Epoch 96/300\n",
      "Average training loss: 0.028596688967612055\n",
      "Average test loss: 0.005919949125084612\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02859869940744506\n",
      "Average test loss: 0.005806726502875487\n",
      "Epoch 98/300\n",
      "Average training loss: 0.028516283866431978\n",
      "Average test loss: 0.005864536733263068\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02850601315829489\n",
      "Average test loss: 0.005965057673967547\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02852100449303786\n",
      "Average test loss: 0.00584290775242779\n",
      "Epoch 101/300\n",
      "Average training loss: 0.028451584385501015\n",
      "Average test loss: 0.005841197996089856\n",
      "Epoch 102/300\n",
      "Average training loss: 0.028386594737569493\n",
      "Average test loss: 0.005820183007253541\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02837744899921947\n",
      "Average test loss: 0.005977037416564093\n",
      "Epoch 104/300\n",
      "Average training loss: 0.028367933789889017\n",
      "Average test loss: 0.005739886520637406\n",
      "Epoch 105/300\n",
      "Average training loss: 0.028295882339278856\n",
      "Average test loss: 0.005962160613801744\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02824493317057689\n",
      "Average test loss: 0.005900453177177244\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02825147800644239\n",
      "Average test loss: 0.005806982052822908\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0282420734382338\n",
      "Average test loss: 0.007432377218372292\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02818022808763716\n",
      "Average test loss: 0.006097282552884685\n",
      "Epoch 110/300\n",
      "Average training loss: 0.028169291267792385\n",
      "Average test loss: 0.005831655818969011\n",
      "Epoch 111/300\n",
      "Average training loss: 0.028117269203066828\n",
      "Average test loss: 0.00591795721070634\n",
      "Epoch 112/300\n",
      "Average training loss: 0.028063821256160738\n",
      "Average test loss: 0.00596362018212676\n",
      "Epoch 113/300\n",
      "Average training loss: 0.028080343027909595\n",
      "Average test loss: 0.006189756534993649\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028000260260370044\n",
      "Average test loss: 0.006364116096662151\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0280367009854979\n",
      "Average test loss: 0.005877502940181229\n",
      "Epoch 116/300\n",
      "Average training loss: 0.027977693541182413\n",
      "Average test loss: 0.005880602204551299\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02794353770216306\n",
      "Average test loss: 0.006225080454308126\n",
      "Epoch 118/300\n",
      "Average training loss: 0.027959619892968073\n",
      "Average test loss: 0.005884557662738694\n",
      "Epoch 119/300\n",
      "Average training loss: 0.027857956637938816\n",
      "Average test loss: 0.005920817608634631\n",
      "Epoch 120/300\n",
      "Average training loss: 0.027866437271237374\n",
      "Average test loss: 0.005882769719594055\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027873809167080457\n",
      "Average test loss: 0.006030343156307936\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02786521260274781\n",
      "Average test loss: 0.005895336111386617\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027775728861490885\n",
      "Average test loss: 0.0058986802883446215\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027770160868763924\n",
      "Average test loss: 0.006062506942285432\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02775254775583744\n",
      "Average test loss: 0.005858980293903086\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02772852466172642\n",
      "Average test loss: 0.0058886987823579045\n",
      "Epoch 127/300\n",
      "Average training loss: 0.027723967952860726\n",
      "Average test loss: 0.005998566966503858\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027713324982259008\n",
      "Average test loss: 0.005823193521135383\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02761947381330861\n",
      "Average test loss: 0.005888447457717524\n",
      "Epoch 130/300\n",
      "Average training loss: 0.027658496436145572\n",
      "Average test loss: 0.005772991595996751\n",
      "Epoch 131/300\n",
      "Average training loss: 0.027639641051491103\n",
      "Average test loss: 0.005819021792875396\n",
      "Epoch 132/300\n",
      "Average training loss: 0.027579655877417987\n",
      "Average test loss: 0.005926914448953337\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02753589756621255\n",
      "Average test loss: 0.006024686720636156\n",
      "Epoch 134/300\n",
      "Average training loss: 0.027615164582928023\n",
      "Average test loss: 0.006200929846200678\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02754834201435248\n",
      "Average test loss: 0.006040776568982336\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02751100406381819\n",
      "Average test loss: 0.00645259748150905\n",
      "Epoch 137/300\n",
      "Average training loss: 0.027469417701992724\n",
      "Average test loss: 0.005903860827287038\n",
      "Epoch 138/300\n",
      "Average training loss: 0.027474232706758712\n",
      "Average test loss: 0.005851884544309642\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0274712427639299\n",
      "Average test loss: 0.0058904699302381935\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0274364140133063\n",
      "Average test loss: 0.005969877794177996\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02744368093046877\n",
      "Average test loss: 0.006387651294469833\n",
      "Epoch 142/300\n",
      "Average training loss: 0.027455497038033273\n",
      "Average test loss: 0.005985838734027412\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02739568558997578\n",
      "Average test loss: 0.005956976878560252\n",
      "Epoch 144/300\n",
      "Average training loss: 0.027345431134104727\n",
      "Average test loss: 0.005997537690732214\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0273600415074163\n",
      "Average test loss: 0.005963760597424375\n",
      "Epoch 146/300\n",
      "Average training loss: 0.027282606828543873\n",
      "Average test loss: 0.005922059168625209\n",
      "Epoch 147/300\n",
      "Average training loss: 0.027338336893253858\n",
      "Average test loss: 0.005847703034679095\n",
      "Epoch 148/300\n",
      "Average training loss: 0.027299940465225114\n",
      "Average test loss: 0.0058474452789458965\n",
      "Epoch 149/300\n",
      "Average training loss: 0.027262444050775634\n",
      "Average test loss: 0.005882141338040431\n",
      "Epoch 150/300\n",
      "Average training loss: 0.027237235223253567\n",
      "Average test loss: 0.005875070581833522\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02723342952132225\n",
      "Average test loss: 0.007068934200538529\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02720516936480999\n",
      "Average test loss: 0.006259297101034058\n",
      "Epoch 153/300\n",
      "Average training loss: 0.027195630269745984\n",
      "Average test loss: 0.005918361545022991\n",
      "Epoch 154/300\n",
      "Average training loss: 0.027181481164362694\n",
      "Average test loss: 0.005868670550071531\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02713948736919297\n",
      "Average test loss: 0.005796436948080857\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02711530987256103\n",
      "Average test loss: 0.0058511245972994305\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02717437368300226\n",
      "Average test loss: 0.005810197519138456\n",
      "Epoch 158/300\n",
      "Average training loss: 0.027113726064562797\n",
      "Average test loss: 0.0059293333176109525\n",
      "Epoch 159/300\n",
      "Average training loss: 0.027065705441766313\n",
      "Average test loss: 0.005869068127953344\n",
      "Epoch 160/300\n",
      "Average training loss: 0.027094422020845943\n",
      "Average test loss: 0.0062879039620359735\n",
      "Epoch 161/300\n",
      "Average training loss: 0.027046940378016894\n",
      "Average test loss: 0.005994052737537357\n",
      "Epoch 162/300\n",
      "Average training loss: 0.027019386059708064\n",
      "Average test loss: 0.005928477423886458\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027061629462573265\n",
      "Average test loss: 0.00598601375768582\n",
      "Epoch 164/300\n",
      "Average training loss: 0.027032721805903645\n",
      "Average test loss: 0.006068078006307284\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02704276298979918\n",
      "Average test loss: 0.005826481550518009\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02692788763013151\n",
      "Average test loss: 0.005857006520860725\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02698389629026254\n",
      "Average test loss: 0.00607987875243028\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026949233394530085\n",
      "Average test loss: 0.005924306599216328\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02688495360314846\n",
      "Average test loss: 0.006080420692761739\n",
      "Epoch 170/300\n",
      "Average training loss: 0.026898489496774144\n",
      "Average test loss: 0.005908907843960656\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0269459770321846\n",
      "Average test loss: 0.006400826915684673\n",
      "Epoch 172/300\n",
      "Average training loss: 0.026918291840288374\n",
      "Average test loss: 0.005955093852140837\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02688912967675262\n",
      "Average test loss: 0.006003338732239273\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026878619132770433\n",
      "Average test loss: 0.00596831413730979\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02688080812162823\n",
      "Average test loss: 0.00613545034867194\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026823022526171473\n",
      "Average test loss: 0.01139389075173272\n",
      "Epoch 177/300\n",
      "Average training loss: 0.026795031895240146\n",
      "Average test loss: 0.0060109827684031595\n",
      "Epoch 178/300\n",
      "Average training loss: 0.026796146111355886\n",
      "Average test loss: 0.007515927509094278\n",
      "Epoch 179/300\n",
      "Average training loss: 0.026803916873203385\n",
      "Average test loss: 0.00589478812697861\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026829284932878283\n",
      "Average test loss: 0.006163839186231295\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0267817438741525\n",
      "Average test loss: 0.006040590919554233\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02672550723122226\n",
      "Average test loss: 0.005936537765380409\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0267174221591817\n",
      "Average test loss: 0.005840118582670887\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0267474282466703\n",
      "Average test loss: 0.006907370594226652\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026756091546681193\n",
      "Average test loss: 0.006059226115544637\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026720938904417884\n",
      "Average test loss: 0.005929601550102234\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02666901626686255\n",
      "Average test loss: 0.006839894212368462\n",
      "Epoch 188/300\n",
      "Average training loss: 0.026693491712212563\n",
      "Average test loss: 0.006186127749996053\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026647749417357973\n",
      "Average test loss: 0.006014521569841438\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0266652509652906\n",
      "Average test loss: 0.006520706235120694\n",
      "Epoch 191/300\n",
      "Average training loss: 0.026636906138724752\n",
      "Average test loss: 0.00588541722877158\n",
      "Epoch 192/300\n",
      "Average training loss: 0.026624540843897395\n",
      "Average test loss: 0.005916272340135442\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02662793745100498\n",
      "Average test loss: 0.006041667041679223\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02661057985987928\n",
      "Average test loss: 0.006110608850088384\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026582690422733625\n",
      "Average test loss: 0.006111369864601228\n",
      "Epoch 196/300\n",
      "Average training loss: 0.026545203945702978\n",
      "Average test loss: 0.0062573496653801864\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02660556353463067\n",
      "Average test loss: 0.005996893037938409\n",
      "Epoch 198/300\n",
      "Average training loss: 0.026596408183375993\n",
      "Average test loss: 0.006084370652006732\n",
      "Epoch 199/300\n",
      "Average training loss: 0.026544381744331783\n",
      "Average test loss: 0.006038887533048789\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02658012691968017\n",
      "Average test loss: 0.00594844471083747\n",
      "Epoch 201/300\n",
      "Average training loss: 0.026549611023730702\n",
      "Average test loss: 0.006169491316295332\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0264797645724482\n",
      "Average test loss: 0.00658509137108922\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0264906042036083\n",
      "Average test loss: 0.005977096099820402\n",
      "Epoch 204/300\n",
      "Average training loss: 0.026503323997060457\n",
      "Average test loss: 0.006286586141006814\n",
      "Epoch 205/300\n",
      "Average training loss: 0.026517939772870806\n",
      "Average test loss: 0.0058767974641587995\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02644236337641875\n",
      "Average test loss: 0.005967866164528661\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02643593694931931\n",
      "Average test loss: 0.0062333226878609925\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02645604861776034\n",
      "Average test loss: 0.006039849208874835\n",
      "Epoch 209/300\n",
      "Average training loss: 0.026449018009834818\n",
      "Average test loss: 0.006128800140486823\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026410766318440436\n",
      "Average test loss: 0.006176381271332502\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02640465846657753\n",
      "Average test loss: 0.0063178459823959405\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026414976278940835\n",
      "Average test loss: 0.005979595786995358\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026382686478396256\n",
      "Average test loss: 0.006192199014127255\n",
      "Epoch 214/300\n",
      "Average training loss: 0.026377879056665634\n",
      "Average test loss: 0.0059797608119746045\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0263498944491148\n",
      "Average test loss: 0.006049767742554346\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02635669579108556\n",
      "Average test loss: 0.006686458430770371\n",
      "Epoch 217/300\n",
      "Average training loss: 0.026366516136460835\n",
      "Average test loss: 0.006001843260808123\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02638750742872556\n",
      "Average test loss: 0.006219546197603146\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026362143768204584\n",
      "Average test loss: 0.005888338529401355\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02635735690097014\n",
      "Average test loss: 0.006098976663417286\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02630276661945714\n",
      "Average test loss: 0.006155636933528714\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026269382804632188\n",
      "Average test loss: 0.00627404886422058\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026339136906796033\n",
      "Average test loss: 0.0060648394467102155\n",
      "Epoch 224/300\n",
      "Average training loss: 0.026313253586490948\n",
      "Average test loss: 0.006116121349235375\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026251239142484135\n",
      "Average test loss: 0.005956320946001344\n",
      "Epoch 226/300\n",
      "Average training loss: 0.026288495293921896\n",
      "Average test loss: 0.006244079897801081\n",
      "Epoch 227/300\n",
      "Average training loss: 0.026291721074117553\n",
      "Average test loss: 0.006045287243607971\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02621375727487935\n",
      "Average test loss: 0.006112957067373726\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02623547051846981\n",
      "Average test loss: 0.006013169062634309\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026246228797568216\n",
      "Average test loss: 0.005991550725160374\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026221853350599608\n",
      "Average test loss: 0.006163057160460287\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02623078807691733\n",
      "Average test loss: 0.006028254538360569\n",
      "Epoch 233/300\n",
      "Average training loss: 0.026255382135510444\n",
      "Average test loss: 0.006200013750129276\n",
      "Epoch 234/300\n",
      "Average training loss: 0.026191740367147658\n",
      "Average test loss: 0.006332038570195437\n",
      "Epoch 235/300\n",
      "Average training loss: 0.026220694391263857\n",
      "Average test loss: 0.005888092392020755\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02620461426509751\n",
      "Average test loss: 0.005856812201854256\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02619892844557762\n",
      "Average test loss: 0.0060776924399866\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026222600648800533\n",
      "Average test loss: 0.006050874849988354\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026151734943191212\n",
      "Average test loss: 0.006776099775400427\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026123024582862853\n",
      "Average test loss: 0.0060148441998495\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026140618284543355\n",
      "Average test loss: 0.006293415843198697\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026087830260396003\n",
      "Average test loss: 0.006054620783362124\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02613775385916233\n",
      "Average test loss: 0.006072560882402791\n",
      "Epoch 244/300\n",
      "Average training loss: 0.026108358457684517\n",
      "Average test loss: 0.00598617123067379\n",
      "Epoch 245/300\n",
      "Average training loss: 0.026090709601839384\n",
      "Average test loss: 0.006081410258801447\n",
      "Epoch 246/300\n",
      "Average training loss: 0.026063209007183712\n",
      "Average test loss: 0.005964996155765322\n",
      "Epoch 247/300\n",
      "Average training loss: 0.026170193195343017\n",
      "Average test loss: 0.00609405994332499\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026025322255161073\n",
      "Average test loss: 0.006165411268671353\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026105050454537072\n",
      "Average test loss: 0.006035834811213944\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026032937851217058\n",
      "Average test loss: 0.0061729670804407865\n",
      "Epoch 251/300\n",
      "Average training loss: 0.026029586071769396\n",
      "Average test loss: 0.006019606759150823\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02614379953013526\n",
      "Average test loss: 0.00597176147542066\n",
      "Epoch 253/300\n",
      "Average training loss: 0.026018989659017985\n",
      "Average test loss: 0.005892908249050379\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026012158137228753\n",
      "Average test loss: 0.005924332251151403\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02598160811430878\n",
      "Average test loss: 0.005940740565458934\n",
      "Epoch 256/300\n",
      "Average training loss: 0.025979608388410673\n",
      "Average test loss: 0.00605271090939641\n",
      "Epoch 257/300\n",
      "Average training loss: 0.026070596170094277\n",
      "Average test loss: 0.006363442242145538\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026016751242180666\n",
      "Average test loss: 0.006186512030247185\n",
      "Epoch 259/300\n",
      "Average training loss: 0.025978483574257957\n",
      "Average test loss: 0.006256789214495156\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02598395165304343\n",
      "Average test loss: 0.005982915692859226\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02602151215242015\n",
      "Average test loss: 0.005939911889533202\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02599146707687113\n",
      "Average test loss: 0.0061298355940315455\n",
      "Epoch 263/300\n",
      "Average training loss: 0.025930962592363358\n",
      "Average test loss: 0.006578832587434186\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02598842583431138\n",
      "Average test loss: 0.006159229496286975\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025958602731426555\n",
      "Average test loss: 0.006069921905795733\n",
      "Epoch 266/300\n",
      "Average training loss: 0.025950542489687603\n",
      "Average test loss: 0.0060337045134769545\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026018427557415433\n",
      "Average test loss: 0.006050127050115002\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02592139570083883\n",
      "Average test loss: 0.006001686227404409\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025913113527827793\n",
      "Average test loss: 0.006002256916628944\n",
      "Epoch 270/300\n",
      "Average training loss: 0.025883716988894676\n",
      "Average test loss: 0.006258490615834792\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025904823776748444\n",
      "Average test loss: 0.0065284679184357325\n",
      "Epoch 272/300\n",
      "Average training loss: 0.025869252453247706\n",
      "Average test loss: 0.0062236249316483735\n",
      "Epoch 273/300\n",
      "Average training loss: 0.025928017111288178\n",
      "Average test loss: 0.006091288492911392\n",
      "Epoch 274/300\n",
      "Average training loss: 0.025855002249280612\n",
      "Average test loss: 0.005937542034520044\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02586742260058721\n",
      "Average test loss: 0.00999059261712763\n",
      "Epoch 276/300\n",
      "Average training loss: 0.025837943959567282\n",
      "Average test loss: 0.00607321038759417\n",
      "Epoch 277/300\n",
      "Average training loss: 0.025866318308644824\n",
      "Average test loss: 0.006169604037784868\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0258189796888166\n",
      "Average test loss: 0.0060615221154358655\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025889526756273377\n",
      "Average test loss: 0.0063106455918815405\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02582884706556797\n",
      "Average test loss: 0.006105135781483518\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025797023756636513\n",
      "Average test loss: 0.0060495428310500254\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02583217087884744\n",
      "Average test loss: 0.006082708510259788\n",
      "Epoch 283/300\n",
      "Average training loss: 0.025821158442232345\n",
      "Average test loss: 0.006046369175530142\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02584013955295086\n",
      "Average test loss: 0.006095492030183474\n",
      "Epoch 285/300\n",
      "Average training loss: 0.025782295073072115\n",
      "Average test loss: 0.00614447862903277\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02582505413558748\n",
      "Average test loss: 0.005981574466245042\n",
      "Epoch 287/300\n",
      "Average training loss: 0.025797221746709613\n",
      "Average test loss: 0.006160140415446626\n",
      "Epoch 288/300\n",
      "Average training loss: 0.025789029497239324\n",
      "Average test loss: 0.006135841296364864\n",
      "Epoch 289/300\n",
      "Average training loss: 0.025787535496883923\n",
      "Average test loss: 0.006178935370097558\n",
      "Epoch 290/300\n",
      "Average training loss: 0.025796577321158514\n",
      "Average test loss: 0.0060246219601896075\n",
      "Epoch 291/300\n",
      "Average training loss: 0.025757367334432072\n",
      "Average test loss: 0.00604748640416397\n",
      "Epoch 292/300\n",
      "Average training loss: 0.025825357127520772\n",
      "Average test loss: 0.006280140276998281\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025809925181998147\n",
      "Average test loss: 0.006193680660178264\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025792555222908655\n",
      "Average test loss: 0.006076921945230828\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025736365381214352\n",
      "Average test loss: 0.0061557114534080025\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025753404600752726\n",
      "Average test loss: 0.0065165828412605655\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025748435060183206\n",
      "Average test loss: 0.006143837766100963\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025689999226066802\n",
      "Average test loss: 0.006171565952814288\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025701236640413603\n",
      "Average test loss: 0.006070981179674467\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025761038459009594\n",
      "Average test loss: 0.006041193371431695\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12400182329283821\n",
      "Average test loss: 0.007657921421031157\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04499111661314964\n",
      "Average test loss: 0.012975977494484849\n",
      "Epoch 3/300\n",
      "Average training loss: 0.040003751897149616\n",
      "Average test loss: 0.006653695297737916\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03678739082482126\n",
      "Average test loss: 0.005751785876436366\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03439992550015449\n",
      "Average test loss: 0.005472673286580377\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03263527741034826\n",
      "Average test loss: 0.006867928622911374\n",
      "Epoch 7/300\n",
      "Average training loss: 0.031059476921955743\n",
      "Average test loss: 0.005458670113649633\n",
      "Epoch 8/300\n",
      "Average training loss: 0.029781940821144316\n",
      "Average test loss: 0.005199036489758227\n",
      "Epoch 9/300\n",
      "Average training loss: 0.029004887393779225\n",
      "Average test loss: 0.004844277513523896\n",
      "Epoch 10/300\n",
      "Average training loss: 0.028061673829952876\n",
      "Average test loss: 0.004882646267198854\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027346924851338068\n",
      "Average test loss: 0.004659423989554246\n",
      "Epoch 12/300\n",
      "Average training loss: 0.026761011112067433\n",
      "Average test loss: 0.004616343891041146\n",
      "Epoch 13/300\n",
      "Average training loss: 0.026162120065755315\n",
      "Average test loss: 0.0043271867053376305\n",
      "Epoch 14/300\n",
      "Average training loss: 0.025804721597168182\n",
      "Average test loss: 0.004314580392092466\n",
      "Epoch 15/300\n",
      "Average training loss: 0.025269713504446876\n",
      "Average test loss: 0.004378154849633575\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02498352929370271\n",
      "Average test loss: 0.004202867286590238\n",
      "Epoch 17/300\n",
      "Average training loss: 0.024618793447812398\n",
      "Average test loss: 0.00433655035123229\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02428783729010158\n",
      "Average test loss: 0.003986552945855591\n",
      "Epoch 19/300\n",
      "Average training loss: 0.024002160966396333\n",
      "Average test loss: 0.004481985280911128\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02371488904290729\n",
      "Average test loss: 0.003941724655321902\n",
      "Epoch 21/300\n",
      "Average training loss: 0.023596494474344785\n",
      "Average test loss: 0.0038992580502397486\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02335779380467203\n",
      "Average test loss: 0.0038521529245707726\n",
      "Epoch 23/300\n",
      "Average training loss: 0.023050597172644405\n",
      "Average test loss: 0.003811887440789077\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02293366560422712\n",
      "Average test loss: 0.0037776899513685042\n",
      "Epoch 25/300\n",
      "Average training loss: 0.022747885218924947\n",
      "Average test loss: 0.0037581364951199954\n",
      "Epoch 26/300\n",
      "Average training loss: 0.022546931207180022\n",
      "Average test loss: 0.003797060755805837\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02241394044790003\n",
      "Average test loss: 0.0037349552102386952\n",
      "Epoch 28/300\n",
      "Average training loss: 0.022334387317299843\n",
      "Average test loss: 0.004294983842306667\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02212056620253457\n",
      "Average test loss: 0.0038525695668326482\n",
      "Epoch 30/300\n",
      "Average training loss: 0.022038282392753496\n",
      "Average test loss: 0.0037798962175018256\n",
      "Epoch 31/300\n",
      "Average training loss: 0.021973644743363063\n",
      "Average test loss: 0.0036114022604500254\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02180946511692471\n",
      "Average test loss: 0.0036515194699168205\n",
      "Epoch 33/300\n",
      "Average training loss: 0.021672043207618924\n",
      "Average test loss: 0.0036006042884869707\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021624920287893878\n",
      "Average test loss: 0.0036543577450017135\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02153041196200583\n",
      "Average test loss: 0.003575606999711858\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02147037576304542\n",
      "Average test loss: 0.0038642452899366616\n",
      "Epoch 37/300\n",
      "Average training loss: 0.021336592280202443\n",
      "Average test loss: 0.00353622331077026\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02125399597320292\n",
      "Average test loss: 0.0037800470603009066\n",
      "Epoch 39/300\n",
      "Average training loss: 0.021186139131585756\n",
      "Average test loss: 0.003530665355010165\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02113203873899248\n",
      "Average test loss: 0.003572080129964484\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02108626359038883\n",
      "Average test loss: 0.0035260766272743544\n",
      "Epoch 42/300\n",
      "Average training loss: 0.021042351726028655\n",
      "Average test loss: 0.00347569575947192\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02093162479831113\n",
      "Average test loss: 0.003549268433617221\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020922273483541278\n",
      "Average test loss: 0.0036863338013903963\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020860875024563737\n",
      "Average test loss: 0.0036010284597675006\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02076310183770127\n",
      "Average test loss: 0.003442094927032789\n",
      "Epoch 47/300\n",
      "Average training loss: 0.020746711949507394\n",
      "Average test loss: 0.0034977276101708413\n",
      "Epoch 48/300\n",
      "Average training loss: 0.020688186397155126\n",
      "Average test loss: 0.003677244552514619\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020655075919297005\n",
      "Average test loss: 0.003433344739385777\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020599659563766585\n",
      "Average test loss: 0.0034471381625367534\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02054268247551388\n",
      "Average test loss: 0.0034206308176120124\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02052245949374305\n",
      "Average test loss: 0.0034110546935763623\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02045519160065386\n",
      "Average test loss: 0.003446933534617225\n",
      "Epoch 54/300\n",
      "Average training loss: 0.020431664062043033\n",
      "Average test loss: 0.0035122461867415243\n",
      "Epoch 55/300\n",
      "Average training loss: 0.020396827600068515\n",
      "Average test loss: 0.0036972262768281833\n",
      "Epoch 56/300\n",
      "Average training loss: 0.020369864034983847\n",
      "Average test loss: 0.004006078902218077\n",
      "Epoch 57/300\n",
      "Average training loss: 0.020389790491925346\n",
      "Average test loss: 0.0035697332326736714\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020267633704675568\n",
      "Average test loss: 0.0034774071702526675\n",
      "Epoch 59/300\n",
      "Average training loss: 0.020274234705501132\n",
      "Average test loss: 0.0034049331467184757\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020199039533734323\n",
      "Average test loss: 0.003431046110888322\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0201633011682166\n",
      "Average test loss: 0.0034411687445309426\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02023393998377853\n",
      "Average test loss: 0.0034296592163542907\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02012268824875355\n",
      "Average test loss: 0.003437778152111504\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02013154206507736\n",
      "Average test loss: 0.003425034769086374\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02003727262053225\n",
      "Average test loss: 0.0034747815570897525\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020053890655438104\n",
      "Average test loss: 0.0034048096568634113\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01999456930326091\n",
      "Average test loss: 0.0033794506241877874\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019995472994115616\n",
      "Average test loss: 0.0034555391466452017\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01998739733795325\n",
      "Average test loss: 0.0037506561780141457\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019921130661335256\n",
      "Average test loss: 0.003406362315846814\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01992533218363921\n",
      "Average test loss: 0.003375100589874718\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01995209185944663\n",
      "Average test loss: 0.0033588748483194247\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019833748959832722\n",
      "Average test loss: 0.0035835902090701794\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019800982382562424\n",
      "Average test loss: 0.0035173864708178573\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019810443543725544\n",
      "Average test loss: 0.0033944761616488297\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019819400327073203\n",
      "Average test loss: 0.003464783397813638\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01979930287765132\n",
      "Average test loss: 0.0033686331477430134\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019737798222237164\n",
      "Average test loss: 0.003343655846806036\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019793794258601137\n",
      "Average test loss: 0.003390518025391632\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01968815494328737\n",
      "Average test loss: 0.0034597831728557745\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019669830744465194\n",
      "Average test loss: 0.003390267692713274\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019659153890278603\n",
      "Average test loss: 0.003407647003316217\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019688973878820737\n",
      "Average test loss: 0.0033343020776907604\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01959633413785034\n",
      "Average test loss: 0.0033840912588768536\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0195783019380437\n",
      "Average test loss: 0.0035061332504782413\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01957757129602962\n",
      "Average test loss: 0.0033690703954133722\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019565675729678735\n",
      "Average test loss: 0.003386770741393169\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01955024483634366\n",
      "Average test loss: 0.0033972329929884935\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019539825504024822\n",
      "Average test loss: 0.00338157191603548\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01949043246772554\n",
      "Average test loss: 0.003603220010797183\n",
      "Epoch 91/300\n",
      "Average training loss: 0.019495000183582308\n",
      "Average test loss: 0.003401661393750045\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0194823832710584\n",
      "Average test loss: 0.0033236672658887175\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019450979135102697\n",
      "Average test loss: 0.003401554102698962\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01946496262980832\n",
      "Average test loss: 0.004043826065750586\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019402674661742315\n",
      "Average test loss: 0.003404859157072173\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019422269736727078\n",
      "Average test loss: 0.0033392737649588122\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019366544660594728\n",
      "Average test loss: 0.003455215108477407\n",
      "Epoch 98/300\n",
      "Average training loss: 0.019380478453305032\n",
      "Average test loss: 0.0033396928709828193\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01937368489967452\n",
      "Average test loss: 0.0033711002142065103\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01933061277038521\n",
      "Average test loss: 0.003361982830075754\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019366221082707247\n",
      "Average test loss: 0.0034474246442938845\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019350498574475447\n",
      "Average test loss: 0.003323632321630915\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019292061984539033\n",
      "Average test loss: 0.003478688261575169\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01927491372989284\n",
      "Average test loss: 0.003406875578065713\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019289951496654086\n",
      "Average test loss: 0.003389988991949293\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0192676486124595\n",
      "Average test loss: 0.003378380980549587\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01922528137597773\n",
      "Average test loss: 0.0035040346855918568\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01923209306266573\n",
      "Average test loss: 0.0033226080735524496\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01922600613368882\n",
      "Average test loss: 0.003368652145895693\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019170925910274188\n",
      "Average test loss: 0.0033583298501455123\n",
      "Epoch 111/300\n",
      "Average training loss: 0.019206461617516147\n",
      "Average test loss: 0.0033986098469338485\n",
      "Epoch 112/300\n",
      "Average training loss: 0.019147461005383067\n",
      "Average test loss: 0.0034917839225381614\n",
      "Epoch 113/300\n",
      "Average training loss: 0.019195636365148757\n",
      "Average test loss: 0.003485145453363657\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019137871406144567\n",
      "Average test loss: 0.003612215937839614\n",
      "Epoch 115/300\n",
      "Average training loss: 0.019170374731222788\n",
      "Average test loss: 0.0034098995559745365\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01912827957669894\n",
      "Average test loss: 0.0033950905210028094\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019111551188760335\n",
      "Average test loss: 0.0034125105877303415\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01909381921092669\n",
      "Average test loss: 0.0034639113261881803\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01906758049627145\n",
      "Average test loss: 0.0033871629358165796\n",
      "Epoch 120/300\n",
      "Average training loss: 0.019091699052188132\n",
      "Average test loss: 0.0033698055264022616\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019060533802542422\n",
      "Average test loss: 0.0033842853493988513\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01903879410525163\n",
      "Average test loss: 0.003660984509107139\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019075896483328608\n",
      "Average test loss: 0.0035672449303997887\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019014245664079985\n",
      "Average test loss: 0.0033424693281865784\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01901250494354301\n",
      "Average test loss: 0.00345625850578977\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01900878272785081\n",
      "Average test loss: 0.0035374765251245765\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019000665720966127\n",
      "Average test loss: 0.003496820047083828\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01896741076807181\n",
      "Average test loss: 0.0033973389849480656\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018970964474810493\n",
      "Average test loss: 0.0033728153937392764\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0189853952013784\n",
      "Average test loss: 0.003447056836138169\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01893901097608937\n",
      "Average test loss: 0.00344144837020172\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018953992173075675\n",
      "Average test loss: 0.0034010041993525293\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018961985738741027\n",
      "Average test loss: 0.003503709898226791\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018934022093812626\n",
      "Average test loss: 0.0033626446879158416\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018900116725100412\n",
      "Average test loss: 0.0034825190409190124\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018899063989520074\n",
      "Average test loss: 0.004010335235959954\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018874355700280933\n",
      "Average test loss: 0.0033975562387042577\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01885551572508282\n",
      "Average test loss: 0.0033764646732144885\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01887402785486645\n",
      "Average test loss: 0.0035288383403999936\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018855297192103333\n",
      "Average test loss: 0.004147712369759878\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018839454223712287\n",
      "Average test loss: 0.003378010949533847\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01885643225080437\n",
      "Average test loss: 4.732217955483331\n",
      "Epoch 143/300\n",
      "Average training loss: 0.019287194134460554\n",
      "Average test loss: 0.003357117536581225\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018760782109366523\n",
      "Average test loss: 0.003327008731663227\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01876910635497835\n",
      "Average test loss: 0.0033983436330325073\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018785026424460942\n",
      "Average test loss: 0.0033713715196483666\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018786686998274592\n",
      "Average test loss: 0.003352722238128384\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01877694877816571\n",
      "Average test loss: 0.0034669344926046\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018758372555176418\n",
      "Average test loss: 0.003435213704076078\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018746694042450853\n",
      "Average test loss: 0.0038098407845116324\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018806131288409234\n",
      "Average test loss: 0.003388503175849716\n",
      "Epoch 152/300\n",
      "Average training loss: 0.018731444660160277\n",
      "Average test loss: 0.0035353802075195643\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0187546334059702\n",
      "Average test loss: 0.0034189308091170257\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01872731146713098\n",
      "Average test loss: 0.0034646551803582245\n",
      "Epoch 155/300\n",
      "Average training loss: 0.018709595279561148\n",
      "Average test loss: 0.0033993087185339795\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018711476339234246\n",
      "Average test loss: 0.003543185522986783\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01872907330426905\n",
      "Average test loss: 0.003502742451719112\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018668400216433737\n",
      "Average test loss: 0.003451680928468704\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01871484170688523\n",
      "Average test loss: 0.0033427153988223935\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018708271423975627\n",
      "Average test loss: 0.00357374451475011\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01867435440255536\n",
      "Average test loss: 0.0034665374488880235\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01863450506826242\n",
      "Average test loss: 0.00334968299853305\n",
      "Epoch 163/300\n",
      "Average training loss: 0.018647856352229913\n",
      "Average test loss: 0.003430852805574735\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018647361944119135\n",
      "Average test loss: 0.0034249282810423108\n",
      "Epoch 165/300\n",
      "Average training loss: 0.018639663610193466\n",
      "Average test loss: 0.0036703168884333636\n",
      "Epoch 166/300\n",
      "Average training loss: 0.018645699635148047\n",
      "Average test loss: 0.003442384258740478\n",
      "Epoch 167/300\n",
      "Average training loss: 0.018633807819750575\n",
      "Average test loss: 0.003411300572670168\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01860792345305284\n",
      "Average test loss: 0.0033899160923643243\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01862426073352496\n",
      "Average test loss: 0.0034285622864133783\n",
      "Epoch 170/300\n",
      "Average training loss: 0.018627971043189365\n",
      "Average test loss: 0.0033837464983678526\n",
      "Epoch 171/300\n",
      "Average training loss: 0.018578482415941025\n",
      "Average test loss: 0.0033637289918131297\n",
      "Epoch 172/300\n",
      "Average training loss: 0.018578433816631636\n",
      "Average test loss: 0.0034763653768847386\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01858989751338959\n",
      "Average test loss: 0.003469886059562365\n",
      "Epoch 174/300\n",
      "Average training loss: 0.018580338201589053\n",
      "Average test loss: 0.00412280528454317\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0185891535282135\n",
      "Average test loss: 0.0034702618405636813\n",
      "Epoch 176/300\n",
      "Average training loss: 0.018529485372205574\n",
      "Average test loss: 0.0034144708450055783\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018544063768453068\n",
      "Average test loss: 0.003392495550215244\n",
      "Epoch 178/300\n",
      "Average training loss: 0.018540026826990977\n",
      "Average test loss: 0.0034689182765367957\n",
      "Epoch 179/300\n",
      "Average training loss: 0.018509476744466357\n",
      "Average test loss: 0.003423894227792819\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01853567591143979\n",
      "Average test loss: 0.003434444195901354\n",
      "Epoch 181/300\n",
      "Average training loss: 0.018537106970945995\n",
      "Average test loss: 0.0035529342749052576\n",
      "Epoch 182/300\n",
      "Average training loss: 0.018496739662355845\n",
      "Average test loss: 0.0035392435933980677\n",
      "Epoch 183/300\n",
      "Average training loss: 0.018491104985276858\n",
      "Average test loss: 0.00345476490424739\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01850110670344697\n",
      "Average test loss: 0.003453028915863898\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01848791037003199\n",
      "Average test loss: 0.0034358294165382783\n",
      "Epoch 186/300\n",
      "Average training loss: 0.018518421191308233\n",
      "Average test loss: 0.0034915844197902415\n",
      "Epoch 187/300\n",
      "Average training loss: 0.018442722835474545\n",
      "Average test loss: 0.0036323609734988877\n",
      "Epoch 188/300\n",
      "Average training loss: 0.018470420519510904\n",
      "Average test loss: 0.003792617806750867\n",
      "Epoch 189/300\n",
      "Average training loss: 0.018451989693774117\n",
      "Average test loss: 0.003505944174196985\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018461151167750357\n",
      "Average test loss: 0.003416421923579441\n",
      "Epoch 191/300\n",
      "Average training loss: 0.018447204255395467\n",
      "Average test loss: 0.0034859964735805987\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01843480998608801\n",
      "Average test loss: 0.0034136983249336483\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018442830060919124\n",
      "Average test loss: 0.003518873714738422\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01844096194869942\n",
      "Average test loss: 0.0034168017808761863\n",
      "Epoch 195/300\n",
      "Average training loss: 0.018422786508997282\n",
      "Average test loss: 0.003489080985180206\n",
      "Epoch 196/300\n",
      "Average training loss: 0.018440350010991098\n",
      "Average test loss: 0.0035402506271170245\n",
      "Epoch 197/300\n",
      "Average training loss: 0.018419305724402268\n",
      "Average test loss: 0.0036220076257983845\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0184382520566384\n",
      "Average test loss: 0.003628611224806971\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018409746714764173\n",
      "Average test loss: 0.0034583818061898154\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018376153570082453\n",
      "Average test loss: 0.0033868236601766613\n",
      "Epoch 201/300\n",
      "Average training loss: 0.018379448658890195\n",
      "Average test loss: 0.0033838150130791798\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01837271278599898\n",
      "Average test loss: 0.003534608715524276\n",
      "Epoch 203/300\n",
      "Average training loss: 0.018390120993057885\n",
      "Average test loss: 0.0034304077099594804\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01837521932191319\n",
      "Average test loss: 0.0034801834411919115\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018350289371278552\n",
      "Average test loss: 0.0034650272582140234\n",
      "Epoch 206/300\n",
      "Average training loss: 0.018367698672744962\n",
      "Average test loss: 0.0036634896513488557\n",
      "Epoch 207/300\n",
      "Average training loss: 0.018340332986580003\n",
      "Average test loss: 0.003538232011306617\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01833565688961082\n",
      "Average test loss: 0.015350325523151291\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01833237275564008\n",
      "Average test loss: 0.0034089161557041936\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01831750061776903\n",
      "Average test loss: 0.0034224512038959396\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018330699311362372\n",
      "Average test loss: 0.0034370321691450144\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018314631111092036\n",
      "Average test loss: 0.0036590440809312792\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018316780605249935\n",
      "Average test loss: 0.0033795498959306213\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01832612771789233\n",
      "Average test loss: 0.003402956670564082\n",
      "Epoch 215/300\n",
      "Average training loss: 0.018292800629304516\n",
      "Average test loss: 0.0036285898697872958\n",
      "Epoch 216/300\n",
      "Average training loss: 0.018334446326312093\n",
      "Average test loss: 0.0035125263496819468\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01829501144008504\n",
      "Average test loss: 0.003846433903918498\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018262534588575362\n",
      "Average test loss: 0.0034321516731546987\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018278308663103315\n",
      "Average test loss: 0.0034849356673657896\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01825769896639718\n",
      "Average test loss: 0.0033824115857068035\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01826325355966886\n",
      "Average test loss: 0.0034144389753540355\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018266274655858675\n",
      "Average test loss: 0.0036004891232069994\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018231202491455607\n",
      "Average test loss: 0.003473753678508931\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018256869037946065\n",
      "Average test loss: 0.003503317470351855\n",
      "Epoch 225/300\n",
      "Average training loss: 0.018244754749867653\n",
      "Average test loss: 0.0034522251851028867\n",
      "Epoch 226/300\n",
      "Average training loss: 0.018235372819834287\n",
      "Average test loss: 0.0034915245667927796\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018245452158153055\n",
      "Average test loss: 0.003599018171015713\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01823245624370045\n",
      "Average test loss: 0.003662396660281552\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018243976960579555\n",
      "Average test loss: 0.0034393617109292085\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01821638364593188\n",
      "Average test loss: 0.0034546542589863143\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018228908399740854\n",
      "Average test loss: 0.0035672363891369767\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01820452430844307\n",
      "Average test loss: 0.0034616521745920183\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018217938809759086\n",
      "Average test loss: 0.003512531589716673\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018194959541161855\n",
      "Average test loss: 0.0034135957294040256\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018196559655997487\n",
      "Average test loss: 0.003857768214825127\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01820183724992805\n",
      "Average test loss: 0.0037473025773134497\n",
      "Epoch 237/300\n",
      "Average training loss: 0.018196065354678367\n",
      "Average test loss: 0.0035090440170218546\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01814831950598293\n",
      "Average test loss: 0.0036219999502516455\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018155122631953822\n",
      "Average test loss: 0.0034712924510240553\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018185650661587716\n",
      "Average test loss: 0.0034883914262884195\n",
      "Epoch 241/300\n",
      "Average training loss: 0.018177048802375792\n",
      "Average test loss: 0.0036309299945003455\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018239695957965322\n",
      "Average test loss: 0.003520886297855112\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01815197768807411\n",
      "Average test loss: 0.0033791925646364687\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018172893807291986\n",
      "Average test loss: 0.0035357524804357026\n",
      "Epoch 245/300\n",
      "Average training loss: 0.018144885488682323\n",
      "Average test loss: 0.0035040938773502907\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018157791329754723\n",
      "Average test loss: 0.0034676328806413546\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018106986853811475\n",
      "Average test loss: 0.0035248297995163335\n",
      "Epoch 248/300\n",
      "Average training loss: 0.018124941022031837\n",
      "Average test loss: 0.003548529883225759\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018108089826173252\n",
      "Average test loss: 0.004540526733630233\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01814094133178393\n",
      "Average test loss: 0.0035214044919444454\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018116919790705045\n",
      "Average test loss: 0.00350487124795715\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01810878515574667\n",
      "Average test loss: 0.003725134769661559\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018093567164407836\n",
      "Average test loss: 0.003498561785245935\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01809566785891851\n",
      "Average test loss: 0.0035226833319498435\n",
      "Epoch 255/300\n",
      "Average training loss: 0.018101264104247092\n",
      "Average test loss: 0.0035231250193383957\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018104384475284153\n",
      "Average test loss: 0.003560849455702636\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018089198978410825\n",
      "Average test loss: 0.003468509618192911\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01809014912860261\n",
      "Average test loss: 0.003434940478247073\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018073174559407764\n",
      "Average test loss: 0.0035575932818982334\n",
      "Epoch 260/300\n",
      "Average training loss: 0.018080011932386293\n",
      "Average test loss: 0.0034642777546412414\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018087952649427783\n",
      "Average test loss: 0.0038027872641881308\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0180536148250103\n",
      "Average test loss: 0.0036419504582881927\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01806055187847879\n",
      "Average test loss: 0.003610180613481336\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018048693714870348\n",
      "Average test loss: 0.003522370203087727\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018079434396492112\n",
      "Average test loss: 0.0034742527860734196\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018047991699642604\n",
      "Average test loss: 0.0034967273647586506\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018026893478300835\n",
      "Average test loss: 0.0034966618102043865\n",
      "Epoch 268/300\n",
      "Average training loss: 0.018038712945249346\n",
      "Average test loss: 0.0034560999990337427\n",
      "Epoch 269/300\n",
      "Average training loss: 0.018017031179534064\n",
      "Average test loss: 0.003484886445932918\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018012042353550594\n",
      "Average test loss: 0.003623806914521588\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018038728502061632\n",
      "Average test loss: 0.003733217505324218\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0180292399889893\n",
      "Average test loss: 0.0034514420732027953\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018027122682995268\n",
      "Average test loss: 0.004427475099762281\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0180045495049821\n",
      "Average test loss: 0.003450395258557465\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01800845552649763\n",
      "Average test loss: 0.0034512640931126143\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018005985519952243\n",
      "Average test loss: 0.0035866852388199834\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018024232930607266\n",
      "Average test loss: 0.00357605006876919\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017988661170005797\n",
      "Average test loss: 0.0035849636671029857\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018010833273331323\n",
      "Average test loss: 0.0036288788684954248\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01798510636223687\n",
      "Average test loss: 0.003652910727386673\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01798578302313884\n",
      "Average test loss: 0.00350904793292284\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017989932720859844\n",
      "Average test loss: 0.0035581858216060533\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01800532332724995\n",
      "Average test loss: 0.0035381514185832607\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01798623028728697\n",
      "Average test loss: 0.0036105871515141595\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01796587662647168\n",
      "Average test loss: 0.003743178728967905\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017968288886878226\n",
      "Average test loss: 0.0034510317765590218\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0179464520447784\n",
      "Average test loss: 0.005516034311718411\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01797204563352797\n",
      "Average test loss: 0.003493542912105719\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017967604731520016\n",
      "Average test loss: 0.0035473105216191876\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01798727433052328\n",
      "Average test loss: 0.003644252613600757\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017948023370570606\n",
      "Average test loss: 0.0035034378721482224\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01798020754671759\n",
      "Average test loss: 0.0037402286264631482\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017971672374341222\n",
      "Average test loss: 0.0034405463099893598\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017957062867780527\n",
      "Average test loss: 0.0035483324902339116\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017928695969283582\n",
      "Average test loss: 0.0038641898340235153\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017934619528551896\n",
      "Average test loss: 0.0038623201166176134\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017927114203572275\n",
      "Average test loss: 0.0036099903262737723\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01791341689725717\n",
      "Average test loss: 0.0037013184099147717\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017922871184017922\n",
      "Average test loss: 0.003567392094888621\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017937910813424322\n",
      "Average test loss: 0.00351633007845117\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11009274417493078\n",
      "Average test loss: 0.006046008138193024\n",
      "Epoch 2/300\n",
      "Average training loss: 0.037392744024594624\n",
      "Average test loss: 0.004855763058902489\n",
      "Epoch 3/300\n",
      "Average training loss: 0.032789739039209155\n",
      "Average test loss: 0.004365183880345689\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03014853292869197\n",
      "Average test loss: 0.004291341289463971\n",
      "Epoch 5/300\n",
      "Average training loss: 0.028068052735593582\n",
      "Average test loss: 0.004178690707517994\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02651843406094445\n",
      "Average test loss: 0.003796901773247454\n",
      "Epoch 7/300\n",
      "Average training loss: 0.025010093679030735\n",
      "Average test loss: 0.0036851790994405745\n",
      "Epoch 8/300\n",
      "Average training loss: 0.023939451257387796\n",
      "Average test loss: 0.00364533674199548\n",
      "Epoch 9/300\n",
      "Average training loss: 0.023104471511311002\n",
      "Average test loss: 0.0037987129932476413\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022290188863873482\n",
      "Average test loss: 0.0034825742671059238\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021666555931998625\n",
      "Average test loss: 0.0035079537853598595\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02112255409359932\n",
      "Average test loss: 0.0033382696737017897\n",
      "Epoch 13/300\n",
      "Average training loss: 0.020630907658073638\n",
      "Average test loss: 0.003183763985418611\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02019585320353508\n",
      "Average test loss: 0.002967245747645696\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01976292389134566\n",
      "Average test loss: 0.003334209167295032\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01953263224992487\n",
      "Average test loss: 0.003102905972550313\n",
      "Epoch 17/300\n",
      "Average training loss: 0.019127271779709392\n",
      "Average test loss: 0.0027959816718680993\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018861329388287332\n",
      "Average test loss: 0.0027555602957598036\n",
      "Epoch 19/300\n",
      "Average training loss: 0.018617956717809043\n",
      "Average test loss: 0.002813205473952823\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018419184118509294\n",
      "Average test loss: 0.002727616991226872\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01821783973938889\n",
      "Average test loss: 0.002591141697226299\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018056357335713175\n",
      "Average test loss: 0.0027335270386603145\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017901459475358327\n",
      "Average test loss: 0.0025843376440720426\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017727408637603123\n",
      "Average test loss: 0.002550416730137335\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017577593978080484\n",
      "Average test loss: 0.002521568612299032\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01747461302744018\n",
      "Average test loss: 0.0025042236989570987\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017395426823033227\n",
      "Average test loss: 0.0024890485850887164\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01727249941892094\n",
      "Average test loss: 0.0024698147809960776\n",
      "Epoch 29/300\n",
      "Average training loss: 0.017169095213214556\n",
      "Average test loss: 0.0028017086937195726\n",
      "Epoch 30/300\n",
      "Average training loss: 0.017078403148386212\n",
      "Average test loss: 0.00259718658340474\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016998811976777183\n",
      "Average test loss: 0.0024184589085893497\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016903250550230346\n",
      "Average test loss: 0.0024983070308549535\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016812627070479924\n",
      "Average test loss: 0.002403552429854042\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01674778584473663\n",
      "Average test loss: 0.002398522833776143\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016657861199643877\n",
      "Average test loss: 0.002415977419250541\n",
      "Epoch 36/300\n",
      "Average training loss: 0.016603937667277125\n",
      "Average test loss: 0.002515421068916718\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01655618394911289\n",
      "Average test loss: 0.0024312911623468\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01649117088317871\n",
      "Average test loss: 0.002373743637568421\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016426997462080584\n",
      "Average test loss: 0.0023339783166431718\n",
      "Epoch 40/300\n",
      "Average training loss: 0.016379079615076383\n",
      "Average test loss: 0.0023656956056753793\n",
      "Epoch 41/300\n",
      "Average training loss: 0.016355499469571644\n",
      "Average test loss: 0.0023265567274971142\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01630632675025198\n",
      "Average test loss: 0.002602375351099504\n",
      "Epoch 43/300\n",
      "Average training loss: 0.016250112961563798\n",
      "Average test loss: 0.0023202480162597366\n",
      "Epoch 44/300\n",
      "Average training loss: 0.016216518686877356\n",
      "Average test loss: 0.0023936066697869034\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016172315155466396\n",
      "Average test loss: 0.0024157338701188565\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016121168859303\n",
      "Average test loss: 0.002327817936324411\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01608566973772314\n",
      "Average test loss: 0.002397188435619076\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016063505465785662\n",
      "Average test loss: 0.0022981693401104876\n",
      "Epoch 49/300\n",
      "Average training loss: 0.016032598342332574\n",
      "Average test loss: 0.0023354411297995185\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015995849579572678\n",
      "Average test loss: 0.002340126895449228\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015948799155652522\n",
      "Average test loss: 0.002288667591702607\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01591927946027782\n",
      "Average test loss: 0.00231842306152814\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01588929829498132\n",
      "Average test loss: 0.002288922829967406\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015821791561113463\n",
      "Average test loss: 0.002374735641707149\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015830807919303577\n",
      "Average test loss: 0.0023449047449976205\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015780327669448323\n",
      "Average test loss: 0.0022961545356859764\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015793352632058992\n",
      "Average test loss: 0.002380005006781883\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01572619832224316\n",
      "Average test loss: 0.0022677125407175886\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015690723767711058\n",
      "Average test loss: 0.002302328511244721\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015674662984079785\n",
      "Average test loss: 0.0023424030448206596\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015662474314371743\n",
      "Average test loss: 0.002287497318039338\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015614944477048184\n",
      "Average test loss: 0.0023001874341732928\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015614858150482178\n",
      "Average test loss: 0.002270210076019996\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01558438473029269\n",
      "Average test loss: 0.0023606345179594227\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015546585573090447\n",
      "Average test loss: 0.002275970292588075\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015531333282589913\n",
      "Average test loss: 0.0023221597677717605\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015541232859094938\n",
      "Average test loss: 0.002294286989296476\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015495286410053571\n",
      "Average test loss: 0.002271478424055709\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015481801864173677\n",
      "Average test loss: 0.002488283193359772\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015456920411851672\n",
      "Average test loss: 0.0022670894428673716\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015442001347740491\n",
      "Average test loss: 0.0022665693501217497\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015426629495289591\n",
      "Average test loss: 0.0023608352401190335\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015396190563009844\n",
      "Average test loss: 0.003199350334910883\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015390477281477716\n",
      "Average test loss: 0.0022786145609700018\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015361445660392443\n",
      "Average test loss: 0.0023172936205648714\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015319799256821474\n",
      "Average test loss: 0.002293275450459785\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01534525372253524\n",
      "Average test loss: 0.002335962777129478\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015299926920897431\n",
      "Average test loss: 0.0023120346205929916\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01529748647660017\n",
      "Average test loss: 0.002287856275940107\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015286805170277754\n",
      "Average test loss: 0.0025841231702102554\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015246111505561405\n",
      "Average test loss: 0.0026611968502402304\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015274047414461771\n",
      "Average test loss: 0.002289545833754043\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015213863357901574\n",
      "Average test loss: 0.002327021634309656\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01521693707836999\n",
      "Average test loss: 0.0022727067466411324\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015193716348873244\n",
      "Average test loss: 0.0023582213448567523\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015173347884582149\n",
      "Average test loss: 0.0024324758543322485\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015171272099018097\n",
      "Average test loss: 0.0022593627211948235\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015152778977321254\n",
      "Average test loss: 0.002256973912732469\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015151682068076398\n",
      "Average test loss: 0.0023303489627109633\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015115891357262929\n",
      "Average test loss: 0.002390687764932712\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015100182499322627\n",
      "Average test loss: 0.0023257879974941413\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015095353634821044\n",
      "Average test loss: 0.0022764676484382813\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015078083217144012\n",
      "Average test loss: 0.0022652664556064538\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015077543357180225\n",
      "Average test loss: 0.002250112617181407\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015059816787640254\n",
      "Average test loss: 0.002360364539755715\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015030136247475942\n",
      "Average test loss: 0.0022734980872935718\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015025595728721884\n",
      "Average test loss: 0.0022577792627529966\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015026691286100282\n",
      "Average test loss: 0.0023188688084483148\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014988953654964766\n",
      "Average test loss: 0.0022705525539608467\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014990226674411031\n",
      "Average test loss: 0.002213380561313695\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014992520327369372\n",
      "Average test loss: 0.0023055860416756734\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014974017057981756\n",
      "Average test loss: 0.002303740762795011\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014940258069170846\n",
      "Average test loss: 0.0024241903339409167\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014930792482362853\n",
      "Average test loss: 0.0022429964772115152\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014948880440658993\n",
      "Average test loss: 0.0022916870473159686\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014942089572548866\n",
      "Average test loss: 0.002325809754638208\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014917379365199141\n",
      "Average test loss: 0.0023600324255724746\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014916610106825828\n",
      "Average test loss: 0.0022579181057711443\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01487439277023077\n",
      "Average test loss: 0.0022960041659987637\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014918889666597048\n",
      "Average test loss: 0.0022369644737078084\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014840471064050993\n",
      "Average test loss: 0.0022518001643733847\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014844531094862355\n",
      "Average test loss: 0.0023581228471464583\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014871487822797564\n",
      "Average test loss: 0.0023543941016412446\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014842207660277685\n",
      "Average test loss: 0.0029016655871851576\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014842752919428878\n",
      "Average test loss: 0.0023150304046769938\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01480198016928302\n",
      "Average test loss: 0.0022736946961118116\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014818829240070448\n",
      "Average test loss: 0.0023652191083464356\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014794207848608494\n",
      "Average test loss: 0.0023799006189737055\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01480428357836273\n",
      "Average test loss: 0.0023318044665373034\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014771089285612106\n",
      "Average test loss: 0.0023314040739917095\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014774643246498373\n",
      "Average test loss: 0.0024215896083042026\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014747147315906154\n",
      "Average test loss: 0.002554846098439561\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014778395687540372\n",
      "Average test loss: 0.002257085498318904\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014739202182326052\n",
      "Average test loss: 0.0022550394423305987\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014723206448886129\n",
      "Average test loss: 0.002263221813986699\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014731742694973946\n",
      "Average test loss: 0.0022435117377382187\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014703826801644431\n",
      "Average test loss: 0.0022401334887577427\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014719388427005873\n",
      "Average test loss: 0.0023683207454159856\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0147603929920329\n",
      "Average test loss: 0.0022992539548625547\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014690238207578659\n",
      "Average test loss: 0.0033169809147301646\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014671048699981636\n",
      "Average test loss: 0.002252563533683618\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01468562317142884\n",
      "Average test loss: 0.002241323937351505\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01467157169431448\n",
      "Average test loss: 0.0025266355456163487\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01466109128544728\n",
      "Average test loss: 0.0022266001771721576\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014649418618943956\n",
      "Average test loss: 0.0023836441116614473\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014653596685164505\n",
      "Average test loss: 0.002320026177292069\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014643381640315057\n",
      "Average test loss: 0.0022189863046838177\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014617625382211473\n",
      "Average test loss: 0.00227856448209948\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014626631278958585\n",
      "Average test loss: 0.002314583498156733\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01462655433267355\n",
      "Average test loss: 0.0022956507257703277\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014589045195943779\n",
      "Average test loss: 0.002249993572425511\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01459901009251674\n",
      "Average test loss: 0.002273958779250582\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014586023395260175\n",
      "Average test loss: 0.00229523583004872\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01457152893477016\n",
      "Average test loss: 0.002288110331735677\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01458484334912565\n",
      "Average test loss: 0.0022833432135068708\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014574859780569872\n",
      "Average test loss: 0.002337459977509247\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014576327631043063\n",
      "Average test loss: 0.002286764104333189\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014556869544916683\n",
      "Average test loss: 0.002327787049114704\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014567156930764517\n",
      "Average test loss: 0.002343336563557386\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014528248224821355\n",
      "Average test loss: 0.0022758994547443256\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014533427995940049\n",
      "Average test loss: 0.0022886352497670385\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014523701918621857\n",
      "Average test loss: 0.0022910785948236784\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014531594201922416\n",
      "Average test loss: 0.0031625765872498354\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014518199681407875\n",
      "Average test loss: 0.0023200848187423413\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014509312142928442\n",
      "Average test loss: 0.002319443941530254\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014504406607813306\n",
      "Average test loss: 0.002264393940774931\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014484667133953836\n",
      "Average test loss: 0.002247383469301793\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014481632181339793\n",
      "Average test loss: 0.0022811134511397943\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014503856871691015\n",
      "Average test loss: 0.002278945624931819\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014471912206874953\n",
      "Average test loss: 0.0022958831052399343\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01449506720734967\n",
      "Average test loss: 0.0022339862204260297\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014460190016362402\n",
      "Average test loss: 0.0022883324629316727\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014448136839601729\n",
      "Average test loss: 0.003622656106327971\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014446361720561981\n",
      "Average test loss: 0.0022879667356610297\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014488054506480694\n",
      "Average test loss: 0.002309745731556581\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014424759086635378\n",
      "Average test loss: 0.002243941299099889\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01441487924920188\n",
      "Average test loss: 0.0023180551211246184\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014425381100840039\n",
      "Average test loss: 0.002316085851854748\n",
      "Epoch 169/300\n",
      "Average training loss: 0.014413386970758437\n",
      "Average test loss: 0.0023315233174297546\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014413646320501964\n",
      "Average test loss: 0.002349342258647084\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014407640924884214\n",
      "Average test loss: 0.0022544384030625223\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01439536265283823\n",
      "Average test loss: 0.0023053238104201027\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014403888722260793\n",
      "Average test loss: 0.00226155049726367\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014398231140441364\n",
      "Average test loss: 0.0023583370319878063\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014392288284997145\n",
      "Average test loss: 0.0023327817120071914\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014380786328679986\n",
      "Average test loss: 0.0022525057723331784\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014359777190619046\n",
      "Average test loss: 0.002299780560243461\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014368276085290644\n",
      "Average test loss: 0.0023344394295579858\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014363063972029421\n",
      "Average test loss: 0.0023665133958889377\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014348384259475601\n",
      "Average test loss: 0.0023435687665931053\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014356468710634444\n",
      "Average test loss: 0.0023307444146937794\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014367371156811715\n",
      "Average test loss: 0.002325170522348748\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01432401268515322\n",
      "Average test loss: 0.0022761513747067913\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014332960144513183\n",
      "Average test loss: 0.0022868722930757535\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014322113758160008\n",
      "Average test loss: 0.0023615615928752553\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014321198264757792\n",
      "Average test loss: 0.002370152098644111\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01431226956513193\n",
      "Average test loss: 0.0022441403941354816\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014318051513698365\n",
      "Average test loss: 0.0022853389030529393\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014288578343060281\n",
      "Average test loss: 0.0023355405927739208\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014313238316939937\n",
      "Average test loss: 0.0022652903885270158\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01429034660425451\n",
      "Average test loss: 0.002309889497235417\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014311045386725002\n",
      "Average test loss: 0.0025879646907043126\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014286982571913137\n",
      "Average test loss: 0.0022690794341680078\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01428256843984127\n",
      "Average test loss: 0.0022895775151749453\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014309713871942626\n",
      "Average test loss: 0.002391532502654526\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01426448669615719\n",
      "Average test loss: 0.0023359353558884727\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014251836290789976\n",
      "Average test loss: 0.002361928067687485\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0142646014119188\n",
      "Average test loss: 0.0023491580134464636\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014248437310258547\n",
      "Average test loss: 0.012817832428548072\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014263219363987447\n",
      "Average test loss: 0.0022838871294839514\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014258963120066458\n",
      "Average test loss: 0.002399006652128365\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014228932776384883\n",
      "Average test loss: 0.00237523512625032\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01424710343281428\n",
      "Average test loss: 0.0022866552985376784\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014244595513575606\n",
      "Average test loss: 0.0024599642443160217\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014244585286411975\n",
      "Average test loss: 0.002327107241480715\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014222011500762568\n",
      "Average test loss: 0.0022707657081385453\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014226382152073913\n",
      "Average test loss: 0.002440304028491179\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0142122100541989\n",
      "Average test loss: 0.0022689982073174583\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014199487110806836\n",
      "Average test loss: 0.0023181890087823074\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014215347559087807\n",
      "Average test loss: 0.002313533360345496\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01419023226449887\n",
      "Average test loss: 0.002336112175964647\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01419885746969117\n",
      "Average test loss: 0.002254310570864214\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014209017415841421\n",
      "Average test loss: 0.0022589804410106606\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014184138214422597\n",
      "Average test loss: 0.002509867822337482\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014181155760255125\n",
      "Average test loss: 0.0022619090514878434\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014181855363150437\n",
      "Average test loss: 0.0023332905951473446\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014191161736018127\n",
      "Average test loss: 0.002338433485892084\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014158408910036087\n",
      "Average test loss: 0.0023273218925007515\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014155281301173899\n",
      "Average test loss: 0.0024704432628220984\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014173446241352293\n",
      "Average test loss: 0.0023796197765817246\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014156611959139505\n",
      "Average test loss: 0.002377768285779489\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014176199800438351\n",
      "Average test loss: 0.0024287882559001447\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014143149566319254\n",
      "Average test loss: 0.0022919891977475747\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014145512610673905\n",
      "Average test loss: 0.002361520158333911\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014145793112615745\n",
      "Average test loss: 0.0023368442787064445\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014132528223097324\n",
      "Average test loss: 0.002342634957904617\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014143987903992334\n",
      "Average test loss: 0.0022552813345359433\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01414605665455262\n",
      "Average test loss: 0.0023109392008433738\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01411465886566374\n",
      "Average test loss: 0.0023535594859470923\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014135273988876079\n",
      "Average test loss: 0.0023085008745806087\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014111381533245246\n",
      "Average test loss: 0.00225127905420959\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014108310651448039\n",
      "Average test loss: 0.0022888479808138478\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014120429946316614\n",
      "Average test loss: 0.0023264707995371684\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014110644873645571\n",
      "Average test loss: 0.002301147029010786\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014114573307335376\n",
      "Average test loss: 0.0023016193006187677\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014092348682383696\n",
      "Average test loss: 0.002350237010874682\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014081785294744704\n",
      "Average test loss: 0.00238708992385202\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014097160710228814\n",
      "Average test loss: 0.002452489257272747\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014086850025587612\n",
      "Average test loss: 0.0024689327677090965\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014062257510092524\n",
      "Average test loss: 0.0024782474122734535\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014087436772882938\n",
      "Average test loss: 0.00225875964988437\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014068163606027763\n",
      "Average test loss: 0.0024525786209851504\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0140932174788581\n",
      "Average test loss: 0.0023289359598937963\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014068071392675241\n",
      "Average test loss: 0.0023404504112485383\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014069584356413947\n",
      "Average test loss: 0.0023216303889122274\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014066660153369109\n",
      "Average test loss: 0.0023260638709697457\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014058957724107637\n",
      "Average test loss: 0.002445271583584448\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01404099767903487\n",
      "Average test loss: 0.002395886747683916\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01404144028822581\n",
      "Average test loss: 0.0023210466883869635\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014056449588802125\n",
      "Average test loss: 0.002380505809456938\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01404094656722413\n",
      "Average test loss: 0.002291723376346959\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014032815443972746\n",
      "Average test loss: 0.002324365593286024\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014032564798990885\n",
      "Average test loss: 0.0023473369506084255\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014047535229060385\n",
      "Average test loss: 0.002304012896079156\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014036366129914919\n",
      "Average test loss: 0.002284788453227116\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014027731622258822\n",
      "Average test loss: 0.002341258056461811\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01402483697483937\n",
      "Average test loss: 0.0023746370387574037\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014028564420011309\n",
      "Average test loss: 0.0023768473031620186\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014016977647940318\n",
      "Average test loss: 0.0023125985596949856\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014022608790960577\n",
      "Average test loss: 0.0022974622795979183\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014014999518791834\n",
      "Average test loss: 0.0023277389688624276\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014001554942793317\n",
      "Average test loss: 0.002325784698335661\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014004134193890624\n",
      "Average test loss: 0.0023074125844157406\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013996850432621109\n",
      "Average test loss: 0.0023890467902852428\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014011975686583253\n",
      "Average test loss: 0.0024327606967546873\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014003377727336353\n",
      "Average test loss: 0.002455358165419764\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01397576384080781\n",
      "Average test loss: 0.002385191857297387\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013979360957112576\n",
      "Average test loss: 0.002350719964545634\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013986882044209375\n",
      "Average test loss: 0.0022990998066961766\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013992128965755303\n",
      "Average test loss: 0.0023379032529062694\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013992439138392607\n",
      "Average test loss: 0.0023481543246242735\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01396182760844628\n",
      "Average test loss: 0.0023475274506749378\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013981504192782773\n",
      "Average test loss: 0.0023810400718616113\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013979813951585028\n",
      "Average test loss: 0.0023862323870675432\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01396296871950229\n",
      "Average test loss: 0.0023583207410863703\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01394398748709096\n",
      "Average test loss: 0.002370542547148135\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01395277072985967\n",
      "Average test loss: 0.0023858264620519347\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013969459585017628\n",
      "Average test loss: 0.002405812036453022\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013958601129551728\n",
      "Average test loss: 0.0023565208286874823\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01393853882782989\n",
      "Average test loss: 0.0023197114794618555\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013946026057004929\n",
      "Average test loss: 0.002330215694796708\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01395726484970914\n",
      "Average test loss: 0.002498007196105189\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013930781056483587\n",
      "Average test loss: 0.0023301244297375284\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013932797682782013\n",
      "Average test loss: 0.0023302409250496162\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013917053186231189\n",
      "Average test loss: 0.002313344583329227\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01392957593086693\n",
      "Average test loss: 0.00236192007155882\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013939675257437759\n",
      "Average test loss: 0.0023146160699220168\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013928182718654473\n",
      "Average test loss: 0.002369131600484252\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013931785018907654\n",
      "Average test loss: 0.0023831269056018857\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01393049384901921\n",
      "Average test loss: 0.002354360710001654\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01391198358601994\n",
      "Average test loss: 0.0023134617890334793\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01389138171904617\n",
      "Average test loss: 0.0023161325050103996\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013902905742327372\n",
      "Average test loss: 0.002317012301335732\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013894349538617665\n",
      "Average test loss: 0.002352313172072172\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013920733348362975\n",
      "Average test loss: 0.0023789125920997724\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013891114191048675\n",
      "Average test loss: 0.0023422959074378014\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013901715073320602\n",
      "Average test loss: 0.0023564672803299295\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013883093312382699\n",
      "Average test loss: 0.0023370360393698015\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013876824425326454\n",
      "Average test loss: 0.0024223464276227685\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013889171100325054\n",
      "Average test loss: 0.0022845855803332394\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.0997482028371758\n",
      "Average test loss: 0.005002172764390707\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03211482605172528\n",
      "Average test loss: 0.004061516341235903\n",
      "Epoch 3/300\n",
      "Average training loss: 0.027806580404440562\n",
      "Average test loss: 0.0041898612684259815\n",
      "Epoch 4/300\n",
      "Average training loss: 0.025437571682863765\n",
      "Average test loss: 0.003530215359189444\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02387005817062325\n",
      "Average test loss: 0.002990089263146122\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022159501704904768\n",
      "Average test loss: 0.0030493193991068335\n",
      "Epoch 7/300\n",
      "Average training loss: 0.021054164969258837\n",
      "Average test loss: 0.002934150493807263\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01991706087771389\n",
      "Average test loss: 0.0028528058868315487\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01910243481894334\n",
      "Average test loss: 0.0026411209994306167\n",
      "Epoch 10/300\n",
      "Average training loss: 0.018403401566876306\n",
      "Average test loss: 0.0025629959091958074\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0178721854372157\n",
      "Average test loss: 0.00241759400938948\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017361219339900546\n",
      "Average test loss: 0.002479323629496826\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016953157375256222\n",
      "Average test loss: 0.002248649195043577\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01663101706157128\n",
      "Average test loss: 0.002567581745899386\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016260821698440446\n",
      "Average test loss: 0.0023845622727854386\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015973911537064445\n",
      "Average test loss: 0.00211817679192043\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01577043752041128\n",
      "Average test loss: 0.0020747452084388996\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015464511825806564\n",
      "Average test loss: 0.002055693125973145\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015302883626686203\n",
      "Average test loss: 0.0020401906881274447\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01513623928030332\n",
      "Average test loss: 0.001902181455762022\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014944008765121301\n",
      "Average test loss: 0.0019621119574747153\n",
      "Epoch 22/300\n",
      "Average training loss: 0.014839446812868117\n",
      "Average test loss: 0.0019110296592116357\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014659737678865591\n",
      "Average test loss: 0.001889648019750085\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014570539545681742\n",
      "Average test loss: 0.001986403887056642\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014453059644334847\n",
      "Average test loss: 0.001882393561510576\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014353303132785692\n",
      "Average test loss: 0.0018946527443412277\n",
      "Epoch 27/300\n",
      "Average training loss: 0.014216914088361792\n",
      "Average test loss: 0.0018024994745436643\n",
      "Epoch 28/300\n",
      "Average training loss: 0.014189808242850833\n",
      "Average test loss: 0.0019234903131922086\n",
      "Epoch 29/300\n",
      "Average training loss: 0.014053976003494527\n",
      "Average test loss: 0.0017614139872085717\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013987160931858752\n",
      "Average test loss: 0.0017737616880072488\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013926316229005654\n",
      "Average test loss: 0.0017144861926014225\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013837958071794774\n",
      "Average test loss: 0.0017488586572516295\n",
      "Epoch 33/300\n",
      "Average training loss: 0.013795042744113339\n",
      "Average test loss: 0.0017502441816031932\n",
      "Epoch 34/300\n",
      "Average training loss: 0.013768643269108401\n",
      "Average test loss: 0.001737936564307246\n",
      "Epoch 35/300\n",
      "Average training loss: 0.013703298448688454\n",
      "Average test loss: 0.0017309635447131262\n",
      "Epoch 36/300\n",
      "Average training loss: 0.013612672952314218\n",
      "Average test loss: 0.001703456317178077\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013587768539786339\n",
      "Average test loss: 0.001696796742681828\n",
      "Epoch 38/300\n",
      "Average training loss: 0.013509027774963114\n",
      "Average test loss: 0.0017257939087640909\n",
      "Epoch 39/300\n",
      "Average training loss: 0.013505174602899286\n",
      "Average test loss: 0.0017553728334605693\n",
      "Epoch 40/300\n",
      "Average training loss: 0.013474266583720843\n",
      "Average test loss: 0.0017048071616639693\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013398449996279345\n",
      "Average test loss: 0.0016605452702691157\n",
      "Epoch 42/300\n",
      "Average training loss: 0.013362284601562554\n",
      "Average test loss: 0.001686051367989017\n",
      "Epoch 43/300\n",
      "Average training loss: 0.013367127247154713\n",
      "Average test loss: 0.001698812471392254\n",
      "Epoch 44/300\n",
      "Average training loss: 0.013290330880218082\n",
      "Average test loss: 0.0016519229182352622\n",
      "Epoch 45/300\n",
      "Average training loss: 0.013311989509397083\n",
      "Average test loss: 0.0018747237635155518\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013247646001478037\n",
      "Average test loss: 0.0017143735403401983\n",
      "Epoch 47/300\n",
      "Average training loss: 0.013187038200596968\n",
      "Average test loss: 0.0016368631281786495\n",
      "Epoch 48/300\n",
      "Average training loss: 0.013166903860867024\n",
      "Average test loss: 0.001653389727593296\n",
      "Epoch 49/300\n",
      "Average training loss: 0.013137322083115579\n",
      "Average test loss: 0.0016670382785507374\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01312955014490419\n",
      "Average test loss: 0.0016322053246614005\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013101032999654611\n",
      "Average test loss: 0.0016516816917185983\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013043604476584328\n",
      "Average test loss: 0.0016586910999483533\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013063325496183501\n",
      "Average test loss: 0.001631851009817587\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013006834617919393\n",
      "Average test loss: 0.001734747101759745\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012994711045175791\n",
      "Average test loss: 0.0017035708846524357\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012963778850932917\n",
      "Average test loss: 0.0016496763920618427\n",
      "Epoch 57/300\n",
      "Average training loss: 0.012931382957432005\n",
      "Average test loss: 0.0016568150617596177\n",
      "Epoch 58/300\n",
      "Average training loss: 0.012901926712857352\n",
      "Average test loss: 0.0016413649277140696\n",
      "Epoch 59/300\n",
      "Average training loss: 0.012914689058230983\n",
      "Average test loss: 0.0016475285975676443\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01287093251115746\n",
      "Average test loss: 0.0017224553332974514\n",
      "Epoch 61/300\n",
      "Average training loss: 0.012852217348913352\n",
      "Average test loss: 0.0017355046526839335\n",
      "Epoch 62/300\n",
      "Average training loss: 0.012843744257258044\n",
      "Average test loss: 0.0017135271467268467\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012808435320854187\n",
      "Average test loss: 0.0016504291318770912\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012807398043572903\n",
      "Average test loss: 0.0016856733550214104\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012760685954656866\n",
      "Average test loss: 0.0016286180249104898\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012777215447690752\n",
      "Average test loss: 0.001609933282352156\n",
      "Epoch 67/300\n",
      "Average training loss: 0.012739514213469293\n",
      "Average test loss: 0.001867632401900159\n",
      "Epoch 68/300\n",
      "Average training loss: 0.012719169667197598\n",
      "Average test loss: 0.0017713112437890636\n",
      "Epoch 69/300\n",
      "Average training loss: 0.012704490724537108\n",
      "Average test loss: 0.0016468632771737045\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012693043275839753\n",
      "Average test loss: 0.001642182879563835\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01267189952896701\n",
      "Average test loss: 0.0016715916089920534\n",
      "Epoch 72/300\n",
      "Average training loss: 0.012658936219082938\n",
      "Average test loss: 0.0016255824746977952\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012651626043021679\n",
      "Average test loss: 0.0016207451064450045\n",
      "Epoch 74/300\n",
      "Average training loss: 0.012648618234528435\n",
      "Average test loss: 0.0017115045755894647\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01260733648058441\n",
      "Average test loss: 0.0016675425529893901\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012605102935598956\n",
      "Average test loss: 0.0016752947731357483\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012588923268847996\n",
      "Average test loss: 0.0016281067709335022\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012564203559524484\n",
      "Average test loss: 0.0017088884680221477\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012551142557627624\n",
      "Average test loss: 0.0016401766660726733\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012527154548300638\n",
      "Average test loss: 0.0017141649414681725\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012522942831118902\n",
      "Average test loss: 0.0016241558444582754\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012520389975772964\n",
      "Average test loss: 0.0016042844266113308\n",
      "Epoch 83/300\n",
      "Average training loss: 0.012482849568128587\n",
      "Average test loss: 0.0016251204783717791\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012507090339230167\n",
      "Average test loss: 0.001670326566323638\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012490246397753556\n",
      "Average test loss: 0.0016086938116285537\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012443308533893692\n",
      "Average test loss: 0.0016021997600586878\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012444525964558124\n",
      "Average test loss: 0.001730755952393843\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012440022789769703\n",
      "Average test loss: 0.001616564332611031\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012428824111819267\n",
      "Average test loss: 0.0016984339863475827\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012409535797105894\n",
      "Average test loss: 0.005571592906696929\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01242641114277972\n",
      "Average test loss: 0.0016540856065435541\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012380532644689083\n",
      "Average test loss: 0.0016749165245435304\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012373098603553243\n",
      "Average test loss: 0.0016170158487641148\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01236295172986057\n",
      "Average test loss: 0.0016174755750430956\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01236296907853749\n",
      "Average test loss: 0.0016709821965131494\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012351174312333266\n",
      "Average test loss: 0.0018482417942335208\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01235547864354319\n",
      "Average test loss: 0.0016240433048870827\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012337097787194782\n",
      "Average test loss: 0.0016451160326186153\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012312387146883541\n",
      "Average test loss: 0.00170768521570911\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012292199106266102\n",
      "Average test loss: 0.001639284072443843\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012295489825308323\n",
      "Average test loss: 0.0016050590050096312\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012305123672717147\n",
      "Average test loss: 0.0016295423547012938\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012273937025831806\n",
      "Average test loss: 0.0015897027231338951\n",
      "Epoch 104/300\n",
      "Average training loss: 0.012268754572504096\n",
      "Average test loss: 0.001679907820911871\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012273486516541905\n",
      "Average test loss: 0.001593900129199028\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012265324734979205\n",
      "Average test loss: 0.0016408168509499067\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012230960463484129\n",
      "Average test loss: 0.0015921101643600398\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01223125760588381\n",
      "Average test loss: 0.0016220774747845198\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012219274625182151\n",
      "Average test loss: 0.001594820779023899\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012197527895371119\n",
      "Average test loss: 0.0019203847990267807\n",
      "Epoch 111/300\n",
      "Average training loss: 0.012226248472101159\n",
      "Average test loss: 0.0017773892625959384\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012205925453040336\n",
      "Average test loss: 0.0016937663750723005\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012197279340691036\n",
      "Average test loss: 0.0016013360013150506\n",
      "Epoch 114/300\n",
      "Average training loss: 0.012185119397938251\n",
      "Average test loss: 0.001643700192268524\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01217071630143457\n",
      "Average test loss: 0.0016059399710761175\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012171985084811847\n",
      "Average test loss: 0.001765102621167898\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01215431759837601\n",
      "Average test loss: 0.001677916965343886\n",
      "Epoch 118/300\n",
      "Average training loss: 0.012163465021799009\n",
      "Average test loss: 0.00161920594310181\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012133460445536507\n",
      "Average test loss: 0.001613948110697998\n",
      "Epoch 120/300\n",
      "Average training loss: 0.012126188399891059\n",
      "Average test loss: 0.0016270414950946967\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012121545881860787\n",
      "Average test loss: 0.0016791375561410354\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012130842148429818\n",
      "Average test loss: 0.0032676156514013805\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012140069065822495\n",
      "Average test loss: 0.0016645212921624383\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012091510724690226\n",
      "Average test loss: 0.0015807090384057827\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012094348959624768\n",
      "Average test loss: 0.0015969601697805856\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012091425915559133\n",
      "Average test loss: 0.0016299781496119168\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012076444396542178\n",
      "Average test loss: 0.001645140601735976\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012055847022268508\n",
      "Average test loss: 0.0016904103834078543\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012075083812905682\n",
      "Average test loss: 0.0016920473135800826\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012071055797239145\n",
      "Average test loss: 0.0016124772197670407\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012024087819788191\n",
      "Average test loss: 0.0016249698927212092\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012029910350839296\n",
      "Average test loss: 0.0017039635843700833\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012029564090900951\n",
      "Average test loss: 0.0015954433256346319\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012035115314854516\n",
      "Average test loss: 0.0016570591103906432\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01202205364074972\n",
      "Average test loss: 0.001757046207992567\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012000495577024088\n",
      "Average test loss: 0.0016260142804951304\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012007392901513312\n",
      "Average test loss: 0.0016467310981824995\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011991106457180448\n",
      "Average test loss: 0.0016161003352867232\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012000246123307282\n",
      "Average test loss: 0.001770465264096856\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01199012865871191\n",
      "Average test loss: 0.0016913024393013783\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011992428391344017\n",
      "Average test loss: 0.0016704017584108645\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011988436839646763\n",
      "Average test loss: 0.0016361934402957558\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011956269231935343\n",
      "Average test loss: 0.0016056533426874215\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011970202550292014\n",
      "Average test loss: 0.001596579604368243\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011950788136157724\n",
      "Average test loss: 0.0016088953453840481\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011955722554690308\n",
      "Average test loss: 0.001637933714936177\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011935493754016029\n",
      "Average test loss: 0.001625458321430617\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011937669684489569\n",
      "Average test loss: 0.0016275081280618907\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011929291653964254\n",
      "Average test loss: 0.0016053999658260081\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011927263421316942\n",
      "Average test loss: 0.0016255189271436798\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011901510860357019\n",
      "Average test loss: 0.0016418920263854993\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011924075550503201\n",
      "Average test loss: 0.001602494930331078\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01190053158501784\n",
      "Average test loss: 0.0016068011126998398\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01190394908686479\n",
      "Average test loss: 0.001607956029681696\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011892458661562867\n",
      "Average test loss: 0.0019799953833636312\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011892136313849026\n",
      "Average test loss: 0.0016453154922152558\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011883233407305347\n",
      "Average test loss: 0.0016401165347132418\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011875552701453369\n",
      "Average test loss: 0.0017624300689333015\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011872612215992477\n",
      "Average test loss: 0.0016843071804485388\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011851207973228561\n",
      "Average test loss: 0.0016588399226052893\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011868743676278326\n",
      "Average test loss: 0.0016033863127231597\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011843568700883123\n",
      "Average test loss: 0.0016215996902642978\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011841479991045263\n",
      "Average test loss: 0.0016974679903230734\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011851722041765848\n",
      "Average test loss: 0.0017978911165975862\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011830402350260151\n",
      "Average test loss: 0.0016669127572741774\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011834978178143501\n",
      "Average test loss: 0.001717880870629516\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011830457899305556\n",
      "Average test loss: 0.0017514173525075118\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011847102397845852\n",
      "Average test loss: 0.0016517773823191723\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011833429556753901\n",
      "Average test loss: 0.0016641696881916788\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011794543561008241\n",
      "Average test loss: 0.0016997973510167665\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011806328064037693\n",
      "Average test loss: 0.001754347487559749\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011812906553347905\n",
      "Average test loss: 0.0016189446071576742\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011789832169810931\n",
      "Average test loss: 0.0016709434267961317\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011791123689876662\n",
      "Average test loss: 0.0016325667330788242\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011778114033242066\n",
      "Average test loss: 0.0016306366997046603\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011776850872569614\n",
      "Average test loss: 0.0016716873657165302\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011786072132488092\n",
      "Average test loss: 0.0016614137155314286\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011759820682307085\n",
      "Average test loss: 0.0017033351685644851\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011766581107344892\n",
      "Average test loss: 0.0016778944821821318\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011769598205056455\n",
      "Average test loss: 0.0016281038187961612\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011758379102581077\n",
      "Average test loss: 0.0016601003282186058\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011745181527402665\n",
      "Average test loss: 0.0016316581854803694\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01175247471779585\n",
      "Average test loss: 0.0016304365955293178\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011748195649849044\n",
      "Average test loss: 0.0016809622947540548\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011723333553307586\n",
      "Average test loss: 0.0017581772123359972\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011743623438808653\n",
      "Average test loss: 0.0016380549481966428\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011733215528229873\n",
      "Average test loss: 0.001621330357570615\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011710961870849133\n",
      "Average test loss: 0.0019223817155903412\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011711186673078272\n",
      "Average test loss: 0.0016355205196887255\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01170277467618386\n",
      "Average test loss: 0.0016806945246126918\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011703332687417667\n",
      "Average test loss: 0.0016570447488791413\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011698013019230631\n",
      "Average test loss: 0.0017241208896868758\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011707415061692397\n",
      "Average test loss: 0.0016002467286048664\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011691420862244235\n",
      "Average test loss: 0.001653440745547414\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011698924595283138\n",
      "Average test loss: 0.001621680319516195\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011671739365491603\n",
      "Average test loss: 0.0016809465690102014\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011683183963927958\n",
      "Average test loss: 0.0016490431930869817\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011682616099715232\n",
      "Average test loss: 0.0016251167569102513\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011673818317552407\n",
      "Average test loss: 0.0024696974489423966\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011674128202100596\n",
      "Average test loss: 0.0016926452667038474\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011660791310171286\n",
      "Average test loss: 0.0016490535505322946\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011658548106749853\n",
      "Average test loss: 0.0016064259609621433\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011666424615515604\n",
      "Average test loss: 0.0017147449831374818\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011659046529895729\n",
      "Average test loss: 0.0016634692955348226\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011643957629799842\n",
      "Average test loss: 0.0016616292187116213\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011651808731257915\n",
      "Average test loss: 0.0016596256918791268\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011654626940687498\n",
      "Average test loss: 0.0016627018784897195\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011627506963494752\n",
      "Average test loss: 0.0016701573806090486\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011651318603919611\n",
      "Average test loss: 0.0016944302457074325\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011630325539244545\n",
      "Average test loss: 0.0017200234005641606\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011611956351333195\n",
      "Average test loss: 0.0017046593721542094\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01161639070096943\n",
      "Average test loss: 0.0016381310177966952\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011649493044449224\n",
      "Average test loss: 0.0016351121221669018\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011621771848864025\n",
      "Average test loss: 0.001613993309231268\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01163212872379356\n",
      "Average test loss: 0.0016366479670008023\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011603985104295942\n",
      "Average test loss: 0.0016527409040265613\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011607647151582771\n",
      "Average test loss: 0.0016134870895701979\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011586982296572792\n",
      "Average test loss: 0.0016459638213531839\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011586791246301598\n",
      "Average test loss: 0.001675224932945437\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011600484949847062\n",
      "Average test loss: 0.0016944364687013957\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011597311154835754\n",
      "Average test loss: 0.001740234368894663\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011585632795261012\n",
      "Average test loss: 0.0016505914863405957\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011564523216750887\n",
      "Average test loss: 0.0016632465871257915\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011588555129037963\n",
      "Average test loss: 0.0017031558047359188\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011578918595280912\n",
      "Average test loss: 0.0016560276399056116\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011574699236700932\n",
      "Average test loss: 0.0018020279866953692\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011568928071194225\n",
      "Average test loss: 0.001689885025843978\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011566452295415931\n",
      "Average test loss: 0.0016340778093371126\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011569285800059637\n",
      "Average test loss: 0.0016372619783505797\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011558949830217494\n",
      "Average test loss: 0.0017002557046814925\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011557635765108798\n",
      "Average test loss: 0.0016952778390712207\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011538484913607438\n",
      "Average test loss: 0.0016696588615369465\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01154882290876574\n",
      "Average test loss: 0.0016916934705028931\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011555341207318836\n",
      "Average test loss: 0.001605229046092265\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011539222242931526\n",
      "Average test loss: 0.001688284075094594\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011543926374779807\n",
      "Average test loss: 0.0016856514712692136\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01152655442804098\n",
      "Average test loss: 0.0016800618506968021\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01152252523187134\n",
      "Average test loss: 0.001652375233359635\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011511702300773726\n",
      "Average test loss: 0.001685753021389246\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011527512019707097\n",
      "Average test loss: 0.0016493815833495723\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01151799333592256\n",
      "Average test loss: 0.001706128508473436\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011511162073247963\n",
      "Average test loss: 0.013426366587479909\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011510874413781695\n",
      "Average test loss: 0.0016732705823249287\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011505904912120767\n",
      "Average test loss: 0.0017365896453460058\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011501380100018449\n",
      "Average test loss: 0.0016790242286192046\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0115015125133925\n",
      "Average test loss: 0.0016764505316710307\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011487495601177216\n",
      "Average test loss: 0.001699153549865716\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011484387293457986\n",
      "Average test loss: 0.0016587815345782374\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01149832179148992\n",
      "Average test loss: 0.0016549591070765422\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011490154448482726\n",
      "Average test loss: 0.001831178424983389\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011493301333652603\n",
      "Average test loss: 0.0016969401747402217\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011490728570355309\n",
      "Average test loss: 0.001680126601209243\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011481376761363613\n",
      "Average test loss: 0.001765223901718855\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011481912449830108\n",
      "Average test loss: 0.0016565973334428338\n",
      "Epoch 255/300\n",
      "Average training loss: 0.011480364902979798\n",
      "Average test loss: 0.0016446804576036003\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011460278879437182\n",
      "Average test loss: 0.0016787190983175403\n",
      "Epoch 257/300\n",
      "Average training loss: 0.011457771378258864\n",
      "Average test loss: 0.0018210538228352864\n",
      "Epoch 258/300\n",
      "Average training loss: 0.011463007288674514\n",
      "Average test loss: 0.0016789877290527026\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011479103844198917\n",
      "Average test loss: 0.001658004431778358\n",
      "Epoch 260/300\n",
      "Average training loss: 0.011446246975825893\n",
      "Average test loss: 0.0016824143442014854\n",
      "Epoch 261/300\n",
      "Average training loss: 0.011455946346951856\n",
      "Average test loss: 0.0017103076730337408\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011460155583090253\n",
      "Average test loss: 0.0016329531766888167\n",
      "Epoch 263/300\n",
      "Average training loss: 0.011462259620428085\n",
      "Average test loss: 0.0016222365643415186\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011443992691735426\n",
      "Average test loss: 0.0016791577053566773\n",
      "Epoch 265/300\n",
      "Average training loss: 0.011439572895566623\n",
      "Average test loss: 0.0016881027626287606\n",
      "Epoch 266/300\n",
      "Average training loss: 0.011443798465861214\n",
      "Average test loss: 0.0016481033260416654\n",
      "Epoch 267/300\n",
      "Average training loss: 0.011449804343283177\n",
      "Average test loss: 0.0017690914903456965\n",
      "Epoch 268/300\n",
      "Average training loss: 0.011432424452569749\n",
      "Average test loss: 0.0016568977435429892\n",
      "Epoch 269/300\n",
      "Average training loss: 0.011423360927237405\n",
      "Average test loss: 0.0016426344451804956\n",
      "Epoch 270/300\n",
      "Average training loss: 0.011420798742108875\n",
      "Average test loss: 0.0016531311774419413\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01141890212562349\n",
      "Average test loss: 0.0016565884763581884\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01141578090770377\n",
      "Average test loss: 0.0016451669191527697\n",
      "Epoch 273/300\n",
      "Average training loss: 0.011448120281100272\n",
      "Average test loss: 0.0016389560667384002\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01140755211810271\n",
      "Average test loss: 0.0017178124506026507\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01141039645506276\n",
      "Average test loss: 0.0016820792614792785\n",
      "Epoch 276/300\n",
      "Average training loss: 0.011428104104267226\n",
      "Average test loss: 0.0029963575191795824\n",
      "Epoch 277/300\n",
      "Average training loss: 0.011413890250027179\n",
      "Average test loss: 0.0016691495745132367\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01141017714391152\n",
      "Average test loss: 0.0017355341924768354\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011401915416121483\n",
      "Average test loss: 0.001638505657720897\n",
      "Epoch 280/300\n",
      "Average training loss: 0.011390644277963373\n",
      "Average test loss: 0.0016390832418368923\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01139463329480754\n",
      "Average test loss: 0.0017518866574391724\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01140418291836977\n",
      "Average test loss: 0.0017242954412682188\n",
      "Epoch 283/300\n",
      "Average training loss: 0.011394227950937218\n",
      "Average test loss: 0.001665895398085316\n",
      "Epoch 284/300\n",
      "Average training loss: 0.011381524084342851\n",
      "Average test loss: 0.001648350906610075\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011388874173164368\n",
      "Average test loss: 0.0017117108278390434\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01138639206522041\n",
      "Average test loss: 0.0016318918215110897\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011375406698220306\n",
      "Average test loss: 0.0016705570613137549\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011373907938599586\n",
      "Average test loss: 0.00170803989097476\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011380781648887528\n",
      "Average test loss: 0.0016430314739959108\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0113772878655129\n",
      "Average test loss: 0.001693411762929625\n",
      "Epoch 291/300\n",
      "Average training loss: 0.011366038195788861\n",
      "Average test loss: 0.0016207545083016157\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011367018631762928\n",
      "Average test loss: 0.0016834483905177977\n",
      "Epoch 293/300\n",
      "Average training loss: 0.011374053435193168\n",
      "Average test loss: 0.0017301910596175325\n",
      "Epoch 294/300\n",
      "Average training loss: 0.011349686133364836\n",
      "Average test loss: 0.0016675156189335718\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011366988935404354\n",
      "Average test loss: 0.0016825243330871066\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011365783479478624\n",
      "Average test loss: 0.0016824892724139823\n",
      "Epoch 297/300\n",
      "Average training loss: 0.011349657654762268\n",
      "Average test loss: 0.0017573878208382262\n",
      "Epoch 298/300\n",
      "Average training loss: 0.011362751862241163\n",
      "Average test loss: 0.0016803521636045641\n",
      "Epoch 299/300\n",
      "Average training loss: 0.011354039560589526\n",
      "Average test loss: 0.001777121305051777\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011342224809030691\n",
      "Average test loss: 0.0016692890077829362\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.95/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.24\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.09\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.82\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.35\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.15\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.73\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.54\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.2626326500044929\n",
      "Average test loss: 0.012253559834427304\n",
      "Epoch 2/300\n",
      "Average training loss: 0.3988052312268151\n",
      "Average test loss: 0.011395342321031624\n",
      "Epoch 3/300\n",
      "Average training loss: 0.2723632157113817\n",
      "Average test loss: 0.008712582376268175\n",
      "Epoch 4/300\n",
      "Average training loss: 0.21562258932325576\n",
      "Average test loss: 0.008479778146164284\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1820763879219691\n",
      "Average test loss: 0.008569168916179074\n",
      "Epoch 6/300\n",
      "Average training loss: 0.16079515609476303\n",
      "Average test loss: 0.00791606018692255\n",
      "Epoch 7/300\n",
      "Average training loss: 0.14637966907024383\n",
      "Average test loss: 0.00874165272878276\n",
      "Epoch 8/300\n",
      "Average training loss: 0.132724093053076\n",
      "Average test loss: 0.007495870624565416\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1256494268046485\n",
      "Average test loss: 0.007909528161502546\n",
      "Epoch 10/300\n",
      "Average training loss: 0.11788896197742886\n",
      "Average test loss: 0.006858113085230192\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11103702874978383\n",
      "Average test loss: 0.007373210372610225\n",
      "Epoch 12/300\n",
      "Average training loss: 0.10549439744816887\n",
      "Average test loss: 0.007861085296918948\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10143188344107734\n",
      "Average test loss: 0.008090980047567023\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09910234980450736\n",
      "Average test loss: 0.006774246188915438\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09510765490929286\n",
      "Average test loss: 0.009528354830212063\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09158979233105978\n",
      "Average test loss: 0.006367322761151525\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08956632991631826\n",
      "Average test loss: 0.006807547873506944\n",
      "Epoch 18/300\n",
      "Average training loss: 0.08666663599014282\n",
      "Average test loss: 0.006402471484409439\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08411891610754861\n",
      "Average test loss: 0.006201797854155302\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08222894273201624\n",
      "Average test loss: 0.005890783334771792\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08016217194663154\n",
      "Average test loss: 0.015171778426402145\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07892557613717185\n",
      "Average test loss: 0.006053461487508483\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07741662486394246\n",
      "Average test loss: 0.00577904484503799\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07579660690493054\n",
      "Average test loss: 0.005948894429951906\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0746817427608702\n",
      "Average test loss: 0.0055975920905669534\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0734886877073182\n",
      "Average test loss: 0.005722379333029191\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0724225771493382\n",
      "Average test loss: 0.005763717552440034\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07147398430109024\n",
      "Average test loss: 0.005645475353217787\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07142972726954354\n",
      "Average test loss: 0.005556317980504698\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06984376725223329\n",
      "Average test loss: 0.006044671677259935\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06897786257664362\n",
      "Average test loss: 0.005517103161248896\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06828060394525529\n",
      "Average test loss: 0.005515498185737266\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0674631397359901\n",
      "Average test loss: 0.005402503186215957\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06699039589034186\n",
      "Average test loss: 0.00544384952303436\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06723232856061724\n",
      "Average test loss: 0.0057098103327055776\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06577769637770123\n",
      "Average test loss: 0.0053634802192035645\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06516224589943886\n",
      "Average test loss: 0.005386259348028236\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06468510354889764\n",
      "Average test loss: 0.005421465527266264\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06453386135233773\n",
      "Average test loss: 0.00546992479554481\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06445828902059131\n",
      "Average test loss: 0.00675717878383067\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06342072264353434\n",
      "Average test loss: 0.005327219961418046\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06308864029248555\n",
      "Average test loss: 0.005382641042686171\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06267194676068094\n",
      "Average test loss: 0.0060805744872325\n",
      "Epoch 44/300\n",
      "Average training loss: 0.062292628758483466\n",
      "Average test loss: 0.005868585042655468\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0616332768201828\n",
      "Average test loss: 0.0053060136309100525\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06126518443557951\n",
      "Average test loss: 0.0053076547715399\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06097778447137939\n",
      "Average test loss: 0.005263912760549121\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06094471887747447\n",
      "Average test loss: 0.03370571829047468\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06038548120525148\n",
      "Average test loss: 0.009595434769160218\n",
      "Epoch 50/300\n",
      "Average training loss: 0.060066535393397014\n",
      "Average test loss: 0.0057586500168674525\n",
      "Epoch 51/300\n",
      "Average training loss: 0.059876280503140554\n",
      "Average test loss: 0.0057071944127480186\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05984732086790932\n",
      "Average test loss: 0.005476065112277865\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05921522342165311\n",
      "Average test loss: 0.00581115709286597\n",
      "Epoch 54/300\n",
      "Average training loss: 0.059127356790834\n",
      "Average test loss: 0.005325672444783979\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05878500740064515\n",
      "Average test loss: 0.005285186048597098\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0587387688126829\n",
      "Average test loss: 0.00613335009995434\n",
      "Epoch 57/300\n",
      "Average training loss: 0.058732618305418226\n",
      "Average test loss: 0.00783625482271115\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05809062096145418\n",
      "Average test loss: 0.005323304178400172\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05955012822813458\n",
      "Average test loss: 0.005657807487994432\n",
      "Epoch 60/300\n",
      "Average training loss: 0.17718177571892738\n",
      "Average test loss: 0.005935869376485547\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08540037856499354\n",
      "Average test loss: 0.0055539909021721946\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07498010802931256\n",
      "Average test loss: 0.005533552168558041\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07116045113404591\n",
      "Average test loss: 0.005535808140618934\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06872623674737083\n",
      "Average test loss: 0.00546601062019666\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06703922626045015\n",
      "Average test loss: 0.0052582534452279405\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06555038385258781\n",
      "Average test loss: 0.005278850951956378\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06435559720463223\n",
      "Average test loss: 0.005252703785068459\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06334667417738173\n",
      "Average test loss: 0.005748865594052606\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06231127550866869\n",
      "Average test loss: 0.005300188056296773\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06147430358661546\n",
      "Average test loss: 0.005264564097548524\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06066368585824967\n",
      "Average test loss: 0.005298906214949157\n",
      "Epoch 72/300\n",
      "Average training loss: 0.061929619544082215\n",
      "Average test loss: 0.005258764487173822\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05957018957204289\n",
      "Average test loss: 0.005225169229424662\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05914276650879118\n",
      "Average test loss: 0.005268128798653682\n",
      "Epoch 75/300\n",
      "Average training loss: 0.058551984598239265\n",
      "Average test loss: 0.005484440063436826\n",
      "Epoch 76/300\n",
      "Average training loss: 0.058305935773584575\n",
      "Average test loss: 0.005266117772294416\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05801038661930296\n",
      "Average test loss: 0.005333206805917952\n",
      "Epoch 78/300\n",
      "Average training loss: 0.057937839064333176\n",
      "Average test loss: 0.005201329541703065\n",
      "Epoch 79/300\n",
      "Average training loss: 0.057556119110849166\n",
      "Average test loss: 0.005297464769747522\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05735859919918908\n",
      "Average test loss: 0.005549441448516316\n",
      "Epoch 81/300\n",
      "Average training loss: 16.517192749573123\n",
      "Average test loss: 0.0298071458786726\n",
      "Epoch 82/300\n",
      "Average training loss: 4.63993774541219\n",
      "Average test loss: 0.016328840307063528\n",
      "Epoch 83/300\n",
      "Average training loss: 3.424210010316637\n",
      "Average test loss: 0.011764764740235276\n",
      "Epoch 84/300\n",
      "Average training loss: 2.8166672253078886\n",
      "Average test loss: 0.010126014760798878\n",
      "Epoch 85/300\n",
      "Average training loss: 2.3699322058359784\n",
      "Average test loss: 0.00975335431098938\n",
      "Epoch 86/300\n",
      "Average training loss: 2.0189778820673623\n",
      "Average test loss: 0.008987437836825847\n",
      "Epoch 87/300\n",
      "Average training loss: 1.7319158273273043\n",
      "Average test loss: 0.00808658089571529\n",
      "Epoch 88/300\n",
      "Average training loss: 1.490071851518419\n",
      "Average test loss: 0.00988088086909718\n",
      "Epoch 89/300\n",
      "Average training loss: 1.2778614902496337\n",
      "Average test loss: 0.007524804627729787\n",
      "Epoch 90/300\n",
      "Average training loss: 1.085856707043118\n",
      "Average test loss: 0.01674577384442091\n",
      "Epoch 91/300\n",
      "Average training loss: 0.9153201859792074\n",
      "Average test loss: 0.014950512056135469\n",
      "Epoch 92/300\n",
      "Average training loss: 0.7588999441464742\n",
      "Average test loss: 0.006598780697832505\n",
      "Epoch 93/300\n",
      "Average training loss: 0.6208210582203335\n",
      "Average test loss: 0.006443551498154799\n",
      "Epoch 94/300\n",
      "Average training loss: 0.5019673515160878\n",
      "Average test loss: 0.006661051943898201\n",
      "Epoch 95/300\n",
      "Average training loss: 0.3974142251809438\n",
      "Average test loss: 0.006269200510448879\n",
      "Epoch 96/300\n",
      "Average training loss: 0.304324740436342\n",
      "Average test loss: 0.0060589596927165985\n",
      "Epoch 97/300\n",
      "Average training loss: 0.22996110320091248\n",
      "Average test loss: 0.006103771030488941\n",
      "Epoch 98/300\n",
      "Average training loss: 0.17823437637752956\n",
      "Average test loss: 0.005902728916456302\n",
      "Epoch 99/300\n",
      "Average training loss: 0.14406437238057454\n",
      "Average test loss: 0.005779056785007318\n",
      "Epoch 100/300\n",
      "Average training loss: 0.121578732377953\n",
      "Average test loss: 0.00565837069062723\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10692263076702754\n",
      "Average test loss: 0.005599065210256312\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09709786144892375\n",
      "Average test loss: 0.005543061799059312\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08983185901906755\n",
      "Average test loss: 0.005955411621679862\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08444958939154942\n",
      "Average test loss: 0.005496834419667721\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08048050424787734\n",
      "Average test loss: 0.00541862357656161\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07699750034676658\n",
      "Average test loss: 0.005699613015684816\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07425092632902994\n",
      "Average test loss: 0.005364711756507556\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07182569251126714\n",
      "Average test loss: 0.0057505698800086975\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06990557868613137\n",
      "Average test loss: 0.005374212433894476\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06832168108224869\n",
      "Average test loss: 0.005288012250016133\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06670099320179887\n",
      "Average test loss: 0.0056518069356679915\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06533999177482393\n",
      "Average test loss: 0.005306832765953408\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06387874749302865\n",
      "Average test loss: 0.00522740742812554\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06258102603753408\n",
      "Average test loss: 0.005178220674602522\n",
      "Epoch 115/300\n",
      "Average training loss: 0.061335928168561725\n",
      "Average test loss: 0.005554124388843775\n",
      "Epoch 116/300\n",
      "Average training loss: 0.060226191464397644\n",
      "Average test loss: 0.0051977437991234995\n",
      "Epoch 117/300\n",
      "Average training loss: 0.059391296138366065\n",
      "Average test loss: 0.005688448267264498\n",
      "Epoch 118/300\n",
      "Average training loss: 0.058839903907643425\n",
      "Average test loss: 0.005209722730434603\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05836392242709796\n",
      "Average test loss: 0.005461790879567464\n",
      "Epoch 120/300\n",
      "Average training loss: 0.058035969134834074\n",
      "Average test loss: 0.005442759014873041\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05818518593907356\n",
      "Average test loss: 0.005385931822781761\n",
      "Epoch 122/300\n",
      "Average training loss: 0.057561418536636565\n",
      "Average test loss: 0.0052442709008852645\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05730145131879383\n",
      "Average test loss: 0.005998945131897926\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05693818153275384\n",
      "Average test loss: 0.005390273435248269\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05686574217345979\n",
      "Average test loss: 0.005217749776111709\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0565567562646336\n",
      "Average test loss: 0.005283531681944927\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05657701827751266\n",
      "Average test loss: 0.005344647801998589\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05642104312777519\n",
      "Average test loss: 0.005261056131372849\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05665378955337736\n",
      "Average test loss: 0.005438854437735345\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05715062808328205\n",
      "Average test loss: 0.007948029508193333\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05598575188053979\n",
      "Average test loss: 0.005311497616685099\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05557368214593993\n",
      "Average test loss: 0.005669544788698355\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05560537381635772\n",
      "Average test loss: 0.005501094376875295\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05552624269988802\n",
      "Average test loss: 0.005199821124474207\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05521602329942915\n",
      "Average test loss: 0.005440390684538417\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05513420987129211\n",
      "Average test loss: 0.005911086976528168\n",
      "Epoch 137/300\n",
      "Average training loss: 0.054977107346057895\n",
      "Average test loss: 0.0052252036713891555\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05485671184460322\n",
      "Average test loss: 0.005466437788059314\n",
      "Epoch 139/300\n",
      "Average training loss: 0.054745468662844764\n",
      "Average test loss: 0.005344005257719093\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05479283509320683\n",
      "Average test loss: 0.0054454870265391135\n",
      "Epoch 141/300\n",
      "Average training loss: 0.054302183048592675\n",
      "Average test loss: 0.005256487182444996\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05428468598922094\n",
      "Average test loss: 0.00543969831822647\n",
      "Epoch 143/300\n",
      "Average training loss: 0.054160176346699396\n",
      "Average test loss: 0.005221560267524587\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05414724343683985\n",
      "Average test loss: 0.0053783549045523\n",
      "Epoch 145/300\n",
      "Average training loss: 0.053911792116032704\n",
      "Average test loss: 0.0054143432151112295\n",
      "Epoch 146/300\n",
      "Average training loss: 0.053709373527103\n",
      "Average test loss: 0.005287458192557096\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05366338067253431\n",
      "Average test loss: 0.005444764716343747\n",
      "Epoch 148/300\n",
      "Average training loss: 0.053405422578255334\n",
      "Average test loss: 0.005372527767800623\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05371931526064873\n",
      "Average test loss: 0.005307462417831024\n",
      "Epoch 150/300\n",
      "Average training loss: 0.053219772547483446\n",
      "Average test loss: 0.005400087001630001\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05330679448114501\n",
      "Average test loss: 0.005426966449452771\n",
      "Epoch 152/300\n",
      "Average training loss: 0.052953085958957674\n",
      "Average test loss: 0.005385247438732121\n",
      "Epoch 153/300\n",
      "Average training loss: 0.052821184900071885\n",
      "Average test loss: 0.005411940687232547\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05621644972761472\n",
      "Average test loss: 0.005254350087294976\n",
      "Epoch 155/300\n",
      "Average training loss: 7.9516272660526965\n",
      "Average test loss: 0.022462038149436314\n",
      "Epoch 156/300\n",
      "Average training loss: 4.15872744263543\n",
      "Average test loss: 0.012686309489111106\n",
      "Epoch 157/300\n",
      "Average training loss: 3.260400995042589\n",
      "Average test loss: 0.009453223694529798\n",
      "Epoch 158/300\n",
      "Average training loss: 2.596776717927721\n",
      "Average test loss: 0.011921574696070619\n",
      "Epoch 159/300\n",
      "Average training loss: 1.9864469071494208\n",
      "Average test loss: 0.008375475123524666\n",
      "Epoch 160/300\n",
      "Average training loss: 1.489738452381558\n",
      "Average test loss: 0.009163821025027169\n",
      "Epoch 161/300\n",
      "Average training loss: 1.1674560237460667\n",
      "Average test loss: 0.008107407266894977\n",
      "Epoch 162/300\n",
      "Average training loss: 0.934632581392924\n",
      "Average test loss: 0.007687775767925713\n",
      "Epoch 163/300\n",
      "Average training loss: 0.7542567190594144\n",
      "Average test loss: 0.006911948911017842\n",
      "Epoch 164/300\n",
      "Average training loss: 0.6137506425645616\n",
      "Average test loss: 0.006715045833753215\n",
      "Epoch 165/300\n",
      "Average training loss: 0.5022570729785495\n",
      "Average test loss: 0.007056869542433156\n",
      "Epoch 166/300\n",
      "Average training loss: 0.4095608835750156\n",
      "Average test loss: 0.006595493161843883\n",
      "Epoch 167/300\n",
      "Average training loss: 0.3330712635252211\n",
      "Average test loss: 0.006171900470637613\n",
      "Epoch 168/300\n",
      "Average training loss: 0.27428951289918685\n",
      "Average test loss: 0.00605096141455902\n",
      "Epoch 169/300\n",
      "Average training loss: 0.22883484489387937\n",
      "Average test loss: 0.0064464283962216646\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1953773695892758\n",
      "Average test loss: 0.006574597960131036\n",
      "Epoch 171/300\n",
      "Average training loss: 0.16958100483152602\n",
      "Average test loss: 0.005722417170802752\n",
      "Epoch 172/300\n",
      "Average training loss: 0.1505218216445711\n",
      "Average test loss: 0.006905137697855631\n",
      "Epoch 173/300\n",
      "Average training loss: 0.13469450064500174\n",
      "Average test loss: 0.009316090548411012\n",
      "Epoch 174/300\n",
      "Average training loss: 0.12317977718512217\n",
      "Average test loss: 0.013978883516457347\n",
      "Epoch 175/300\n",
      "Average training loss: 0.11204866870244344\n",
      "Average test loss: 0.005642196496327718\n",
      "Epoch 176/300\n",
      "Average training loss: 0.102861656943957\n",
      "Average test loss: 0.005452944794048865\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09511453490124809\n",
      "Average test loss: 0.005487604142063194\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08886870559718874\n",
      "Average test loss: 0.007720014019144906\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0827817323870129\n",
      "Average test loss: 0.0053156969228552444\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07787544754478666\n",
      "Average test loss: 0.005601868701477846\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07375701161225637\n",
      "Average test loss: 0.005282368818091022\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07122557993067635\n",
      "Average test loss: 0.0053033976409998206\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06908495780825614\n",
      "Average test loss: 0.0051866419443653685\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06742379199465116\n",
      "Average test loss: 0.005187919768608279\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06582226233349905\n",
      "Average test loss: 0.00519009148163928\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0643253857228491\n",
      "Average test loss: 0.005383893189330895\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06279269985026784\n",
      "Average test loss: 0.005626281161275175\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06137211792998844\n",
      "Average test loss: 0.005208245334111982\n",
      "Epoch 189/300\n",
      "Average training loss: 0.059920948571628996\n",
      "Average test loss: 0.0052815641495916575\n",
      "Epoch 190/300\n",
      "Average training loss: 0.058632736249102486\n",
      "Average test loss: 0.005391452819522884\n",
      "Epoch 191/300\n",
      "Average training loss: 7.460292242450847\n",
      "Average test loss: 0.016227750447061328\n",
      "Epoch 192/300\n",
      "Average training loss: 4.537590643141005\n",
      "Average test loss: 0.013120613512065676\n",
      "Epoch 193/300\n",
      "Average training loss: 3.597213018629286\n",
      "Average test loss: 0.011172427836391661\n",
      "Epoch 194/300\n",
      "Average training loss: 2.9073171910179987\n",
      "Average test loss: 0.009048955320484109\n",
      "Epoch 195/300\n",
      "Average training loss: 2.3341834470960827\n",
      "Average test loss: 0.008713081222441462\n",
      "Epoch 196/300\n",
      "Average training loss: 1.8516396154827541\n",
      "Average test loss: 0.00797357819477717\n",
      "Epoch 197/300\n",
      "Average training loss: 1.363571249432034\n",
      "Average test loss: 0.008143674637708398\n",
      "Epoch 198/300\n",
      "Average training loss: 2.950607617378235\n",
      "Average test loss: 0.01114799462010463\n",
      "Epoch 199/300\n",
      "Average training loss: 2.0528282188839384\n",
      "Average test loss: 0.009996988599499067\n",
      "Epoch 200/300\n",
      "Average training loss: 1.411090071572198\n",
      "Average test loss: 0.008157497087286579\n",
      "Epoch 201/300\n",
      "Average training loss: 1.0112704903284708\n",
      "Average test loss: 0.008009972046646807\n",
      "Epoch 202/300\n",
      "Average training loss: 0.7359152633878919\n",
      "Average test loss: 0.006997415051692062\n",
      "Epoch 203/300\n",
      "Average training loss: 0.5434238887892829\n",
      "Average test loss: 0.006774248011824157\n",
      "Epoch 204/300\n",
      "Average training loss: 0.4201983389324612\n",
      "Average test loss: 0.006430550051232179\n",
      "Epoch 205/300\n",
      "Average training loss: 0.3357197580337524\n",
      "Average test loss: 0.006462029876394404\n",
      "Epoch 206/300\n",
      "Average training loss: 0.2699870503875944\n",
      "Average test loss: 0.006080444315241443\n",
      "Epoch 207/300\n",
      "Average training loss: 0.21527941102451748\n",
      "Average test loss: 0.010055994504027896\n",
      "Epoch 208/300\n",
      "Average training loss: 0.17536775143941244\n",
      "Average test loss: 0.005952240833805667\n",
      "Epoch 209/300\n",
      "Average training loss: 0.14760629226101768\n",
      "Average test loss: 0.0058520667018989725\n",
      "Epoch 210/300\n",
      "Average training loss: 0.12823580908775328\n",
      "Average test loss: 0.005632493334097995\n",
      "Epoch 211/300\n",
      "Average training loss: 0.11414112885130777\n",
      "Average test loss: 0.0055622254113356275\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10307975885603163\n",
      "Average test loss: 0.0055057507153186535\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09458371358447605\n",
      "Average test loss: 0.005484398682912191\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0882719272242652\n",
      "Average test loss: 0.0053760922008918395\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0833352628979418\n",
      "Average test loss: 0.005406544215977192\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07953760827912225\n",
      "Average test loss: 0.005421733948091666\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07650320029921003\n",
      "Average test loss: 0.005303600035607815\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0738186763326327\n",
      "Average test loss: 0.005307087819195456\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07152936738729478\n",
      "Average test loss: 0.005428491397036447\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0696360942920049\n",
      "Average test loss: 0.005256593290302488\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06801237091753218\n",
      "Average test loss: 0.005256535801622603\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06623123751746283\n",
      "Average test loss: 0.005261301054308812\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06486797856622271\n",
      "Average test loss: 0.005141799723522531\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06336923917796877\n",
      "Average test loss: 0.005173367504858309\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06226234411199887\n",
      "Average test loss: 0.005150825811343061\n",
      "Epoch 226/300\n",
      "Average training loss: 0.060799163450797396\n",
      "Average test loss: 0.005693306140187713\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05979729531208674\n",
      "Average test loss: 0.005299622648706039\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05846799933579233\n",
      "Average test loss: 0.005658902521762583\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05752935428089566\n",
      "Average test loss: 0.006085824038419459\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05670883300238185\n",
      "Average test loss: 0.005183160389877028\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05608880464898215\n",
      "Average test loss: 0.0052825347342424925\n",
      "Epoch 232/300\n",
      "Average training loss: 512.0473124696083\n",
      "Average test loss: 0.7446588670412699\n",
      "Epoch 233/300\n",
      "Average training loss: 4.496634428024292\n",
      "Average test loss: 0.050118331890967156\n",
      "Epoch 234/300\n",
      "Average training loss: 2.8556292239295114\n",
      "Average test loss: 0.013101609572768212\n",
      "Epoch 235/300\n",
      "Average training loss: 2.340282405429416\n",
      "Average test loss: 0.009807746686041355\n",
      "Epoch 236/300\n",
      "Average training loss: 1.8766230059729683\n",
      "Average test loss: 0.009653623908758164\n",
      "Epoch 237/300\n",
      "Average training loss: 1.5536879274580213\n",
      "Average test loss: 0.009331836130470037\n",
      "Epoch 238/300\n",
      "Average training loss: 1.3634110850228203\n",
      "Average test loss: 0.008052546794215838\n",
      "Epoch 239/300\n",
      "Average training loss: 1.20404670270284\n",
      "Average test loss: 0.007988644344939125\n",
      "Epoch 240/300\n",
      "Average training loss: 1.0578701939053006\n",
      "Average test loss: 0.007765104226354095\n",
      "Epoch 241/300\n",
      "Average training loss: 0.9278596815533108\n",
      "Average test loss: 0.00829919628550609\n",
      "Epoch 242/300\n",
      "Average training loss: 0.8078995996581183\n",
      "Average test loss: 0.006954542206807269\n",
      "Epoch 243/300\n",
      "Average training loss: 0.6963049716419644\n",
      "Average test loss: 0.007397401351895597\n",
      "Epoch 244/300\n",
      "Average training loss: 0.5937282525698344\n",
      "Average test loss: 0.00686973172509008\n",
      "Epoch 245/300\n",
      "Average training loss: 0.5023583297199673\n",
      "Average test loss: 0.006646466849578751\n",
      "Epoch 246/300\n",
      "Average training loss: 0.4268249901930491\n",
      "Average test loss: 0.006723979061262475\n",
      "Epoch 247/300\n",
      "Average training loss: 0.36689227843284605\n",
      "Average test loss: 0.3452240450249778\n",
      "Epoch 248/300\n",
      "Average training loss: 0.3213534660074446\n",
      "Average test loss: 0.0063579385454456015\n",
      "Epoch 249/300\n",
      "Average training loss: 0.28463028433587817\n",
      "Average test loss: 0.007223772782418463\n",
      "Epoch 250/300\n",
      "Average training loss: 0.2536829577287038\n",
      "Average test loss: 0.005910263290835751\n",
      "Epoch 251/300\n",
      "Average training loss: 0.22679718277189467\n",
      "Average test loss: 0.00615506121640404\n",
      "Epoch 252/300\n",
      "Average training loss: 0.20270298975043827\n",
      "Average test loss: 0.006016535511447324\n",
      "Epoch 253/300\n",
      "Average training loss: 0.18112181107203165\n",
      "Average test loss: 0.0058080458463066155\n",
      "Epoch 254/300\n",
      "Average training loss: 0.16179841192563374\n",
      "Average test loss: 0.00560095790359709\n",
      "Epoch 255/300\n",
      "Average training loss: 0.14436521221531762\n",
      "Average test loss: 0.00555186121000184\n",
      "Epoch 256/300\n",
      "Average training loss: 0.12930588240093654\n",
      "Average test loss: 0.0059166154952512845\n",
      "Epoch 257/300\n",
      "Average training loss: 0.11698533984687594\n",
      "Average test loss: 0.006123735013935301\n",
      "Epoch 258/300\n",
      "Average training loss: 0.10692377205689749\n",
      "Average test loss: 0.005780673070500294\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0987580447991689\n",
      "Average test loss: 0.01871805581947168\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09212886024183697\n",
      "Average test loss: 0.005381117906007502\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08674051643742456\n",
      "Average test loss: 0.005299333259049389\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08245717997021146\n",
      "Average test loss: 0.005802867581446966\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0789085077047348\n",
      "Average test loss: 0.005530448498825232\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07606989334026973\n",
      "Average test loss: 0.006676256706317266\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07379168208440146\n",
      "Average test loss: 0.005265859883278609\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0715872412191497\n",
      "Average test loss: 0.005356478890197145\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0698738092382749\n",
      "Average test loss: 0.005198715125521024\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06834261726670796\n",
      "Average test loss: 0.005345142422864834\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06703632101747725\n",
      "Average test loss: 0.005382546465843916\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06574207820163833\n",
      "Average test loss: 0.005345725487089819\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06462367747889625\n",
      "Average test loss: 0.00551831055060029\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06378075206279754\n",
      "Average test loss: 0.0060398090870844\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06272184767987993\n",
      "Average test loss: 0.005181755237488283\n",
      "Epoch 274/300\n",
      "Average training loss: 0.062037384129232834\n",
      "Average test loss: 0.006501156473325358\n",
      "Epoch 275/300\n",
      "Average training loss: 0.061234451899925865\n",
      "Average test loss: 0.005226388592686918\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06069617732365926\n",
      "Average test loss: 0.005165326395382484\n",
      "Epoch 277/300\n",
      "Average training loss: 0.059857524696323605\n",
      "Average test loss: 0.005194297002421485\n",
      "Epoch 278/300\n",
      "Average training loss: 0.059226917299959395\n",
      "Average test loss: 0.005163498022903999\n",
      "Epoch 279/300\n",
      "Average training loss: 0.058611364162630505\n",
      "Average test loss: 0.005250685557309124\n",
      "Epoch 280/300\n",
      "Average training loss: 0.058182226734028925\n",
      "Average test loss: 0.005621677733543847\n",
      "Epoch 281/300\n",
      "Average training loss: 0.057354889505439335\n",
      "Average test loss: 0.005517178957247072\n",
      "Epoch 282/300\n",
      "Average training loss: 0.056938402748770184\n",
      "Average test loss: 0.0053351091514858935\n",
      "Epoch 283/300\n",
      "Average training loss: 0.056421288162469864\n",
      "Average test loss: 0.0051772713652915425\n",
      "Epoch 284/300\n",
      "Average training loss: 0.056731666737132605\n",
      "Average test loss: 0.005863912780872649\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0554005022711224\n",
      "Average test loss: 0.005205429910785622\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05521054800351461\n",
      "Average test loss: 0.005261876167936458\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05473098336325751\n",
      "Average test loss: 0.005302540431213048\n",
      "Epoch 288/300\n",
      "Average training loss: 0.054668709218502046\n",
      "Average test loss: 0.005479768601970541\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05423476842376921\n",
      "Average test loss: 0.005369078810430235\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05402260156803661\n",
      "Average test loss: 0.005230144353376495\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05398386486702495\n",
      "Average test loss: 0.005453300292707152\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05351340449187491\n",
      "Average test loss: 0.005403544387469689\n",
      "Epoch 293/300\n",
      "Average training loss: 0.053420892857842975\n",
      "Average test loss: 0.005333625768207842\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05335358811749352\n",
      "Average test loss: 0.005273451416442792\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05302467970715629\n",
      "Average test loss: 0.005311636912326018\n",
      "Epoch 296/300\n",
      "Average training loss: 0.052914324978987376\n",
      "Average test loss: 0.005399275797108809\n",
      "Epoch 297/300\n",
      "Average training loss: 0.052683390501472686\n",
      "Average test loss: 0.005426673105193509\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05258709462152587\n",
      "Average test loss: 0.005452598568879896\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05242682004306051\n",
      "Average test loss: 0.005475613072100613\n",
      "Epoch 300/300\n",
      "Average training loss: 0.052186813122696345\n",
      "Average test loss: 0.006262022463397847\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.2154428197013007\n",
      "Average test loss: 0.007946923221978876\n",
      "Epoch 2/300\n",
      "Average training loss: 0.37770235964987015\n",
      "Average test loss: 0.006418480982797014\n",
      "Epoch 3/300\n",
      "Average training loss: 0.24747457181082833\n",
      "Average test loss: 0.006053762045999368\n",
      "Epoch 4/300\n",
      "Average training loss: 0.18882806045479245\n",
      "Average test loss: 0.005384417083114386\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1537513636747996\n",
      "Average test loss: 0.005167683694511652\n",
      "Epoch 6/300\n",
      "Average training loss: 0.13061279686292013\n",
      "Average test loss: 0.005510912779304716\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11450695661703746\n",
      "Average test loss: 0.004756676046798626\n",
      "Epoch 8/300\n",
      "Average training loss: 0.103047929306825\n",
      "Average test loss: 0.004610801562252972\n",
      "Epoch 9/300\n",
      "Average training loss: 0.09408359797795614\n",
      "Average test loss: 0.004726656758122974\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08750035154157215\n",
      "Average test loss: 0.004925696869277292\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08154434803459379\n",
      "Average test loss: 0.004295606957541572\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0768330274687873\n",
      "Average test loss: 0.009345285611020195\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07280844113892979\n",
      "Average test loss: 0.004887468143469758\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06977035989032851\n",
      "Average test loss: 0.004352173932310608\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06650757740272416\n",
      "Average test loss: 0.004276582355714507\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06428252903620402\n",
      "Average test loss: 0.004053737630860673\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06195708920227157\n",
      "Average test loss: 0.003814404801362091\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06018627185291714\n",
      "Average test loss: 0.0035078442705174287\n",
      "Epoch 19/300\n",
      "Average training loss: 0.058107136024369135\n",
      "Average test loss: 0.0034528121087286207\n",
      "Epoch 20/300\n",
      "Average training loss: 0.056427420255210664\n",
      "Average test loss: 0.0036498520897908344\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05440762145651711\n",
      "Average test loss: 0.0034478697797490493\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05277086159255769\n",
      "Average test loss: 0.007658064520193471\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0516171905166573\n",
      "Average test loss: 0.0032930297712898916\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0502874553932084\n",
      "Average test loss: 0.003409671135660675\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04880291808644931\n",
      "Average test loss: 0.003409876943462425\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0474759877572457\n",
      "Average test loss: 0.003306815495921506\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04644673772652944\n",
      "Average test loss: 0.0031394313513818713\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04536683145165443\n",
      "Average test loss: 0.003266911658975813\n",
      "Epoch 29/300\n",
      "Average training loss: 0.044335188928577636\n",
      "Average test loss: 0.0031162261851131914\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0436114373240206\n",
      "Average test loss: 0.0030837212420172164\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0426241256263521\n",
      "Average test loss: 0.0032170525905158786\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04240489078230328\n",
      "Average test loss: 0.0037727409075531695\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04167021068268352\n",
      "Average test loss: 0.0040021651755604475\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04115755559338464\n",
      "Average test loss: 0.0034830997072988084\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04289366414811876\n",
      "Average test loss: 0.003324789828931292\n",
      "Epoch 36/300\n",
      "Average training loss: 0.041690365311172276\n",
      "Average test loss: 0.0031647471423364346\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04098400644130177\n",
      "Average test loss: 0.003401237782297863\n",
      "Epoch 38/300\n",
      "Average training loss: 0.040978922936651445\n",
      "Average test loss: 0.003128510318075617\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04004221306244532\n",
      "Average test loss: 0.0032311620857152674\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03935218499600887\n",
      "Average test loss: 0.0030218040266384682\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03959541231724951\n",
      "Average test loss: 0.0029910636943661505\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03916735188828574\n",
      "Average test loss: 0.0030172279795838727\n",
      "Epoch 43/300\n",
      "Average training loss: 0.038654373705387116\n",
      "Average test loss: 0.0029735222156676983\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03837087407377031\n",
      "Average test loss: 0.004295537437001864\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03831422790222698\n",
      "Average test loss: 0.0032588884577982955\n",
      "Epoch 46/300\n",
      "Average training loss: 0.037911438756518895\n",
      "Average test loss: 0.002970646215809716\n",
      "Epoch 47/300\n",
      "Average training loss: 0.037692732613947656\n",
      "Average test loss: 0.0029838370006117555\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03766307558781571\n",
      "Average test loss: 0.003089303941776355\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03836439910531044\n",
      "Average test loss: 0.0030039125511215794\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03724009514682823\n",
      "Average test loss: 0.0029884420914782417\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03693712248901526\n",
      "Average test loss: 0.0029796694128049746\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0368521630399757\n",
      "Average test loss: 0.0029548368209766016\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03678654038243823\n",
      "Average test loss: 0.002955779790878296\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03649992255038685\n",
      "Average test loss: 0.002951233407275544\n",
      "Epoch 55/300\n",
      "Average training loss: 0.036316516680849925\n",
      "Average test loss: 0.0029293619162506527\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03613686953981717\n",
      "Average test loss: 0.002935234611440036\n",
      "Epoch 57/300\n",
      "Average training loss: 0.036023294195532796\n",
      "Average test loss: 0.0030478548958069746\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03582596205671628\n",
      "Average test loss: 0.16947824322515065\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03573358612259229\n",
      "Average test loss: 0.003293420331759585\n",
      "Epoch 60/300\n",
      "Average training loss: 0.035578848587142095\n",
      "Average test loss: 0.0031050761267542837\n",
      "Epoch 61/300\n",
      "Average training loss: 0.035408638255463705\n",
      "Average test loss: 0.0032297829824189344\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06879050283961825\n",
      "Average test loss: 0.0031150089158780044\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04351854412092103\n",
      "Average test loss: 0.0029890394125961597\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04113016592462858\n",
      "Average test loss: 0.0029656156616078484\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03974027240938611\n",
      "Average test loss: 0.0029545794166624547\n",
      "Epoch 66/300\n",
      "Average training loss: 0.038799858629703525\n",
      "Average test loss: 0.0029330041841086414\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03791530440251033\n",
      "Average test loss: 0.002975501980011662\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03758905329141352\n",
      "Average test loss: 0.002940366781420178\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03708435390724076\n",
      "Average test loss: 0.004834191003607379\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03638518530295955\n",
      "Average test loss: 0.0031006073467433454\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03603628222809897\n",
      "Average test loss: 0.003867164746754699\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03577761866317855\n",
      "Average test loss: 0.002904971279617813\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03544086645957496\n",
      "Average test loss: 0.0030762471266918712\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03524236598445309\n",
      "Average test loss: 0.0029259570632129907\n",
      "Epoch 75/300\n",
      "Average training loss: 0.036683633637097146\n",
      "Average test loss: 0.0029856415608276923\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03518762167460389\n",
      "Average test loss: 0.007643129818969302\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03484732556508647\n",
      "Average test loss: 0.0029081125760244\n",
      "Epoch 78/300\n",
      "Average training loss: 0.034794274936119717\n",
      "Average test loss: 0.002947191016541587\n",
      "Epoch 79/300\n",
      "Average training loss: 0.034678688214884866\n",
      "Average test loss: 0.003055219049875935\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03459398882256614\n",
      "Average test loss: 0.0032622662679188783\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03452918690608607\n",
      "Average test loss: 0.0034371374232901467\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0343726452589035\n",
      "Average test loss: 0.003220665997101201\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03469256124893824\n",
      "Average test loss: 0.0031096116525845393\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03428560185763571\n",
      "Average test loss: 0.0029095537037485174\n",
      "Epoch 85/300\n",
      "Average training loss: 0.034937206513351866\n",
      "Average test loss: 0.002995484583700697\n",
      "Epoch 86/300\n",
      "Average training loss: 16.047120567798615\n",
      "Average test loss: 0.013982928252882427\n",
      "Epoch 87/300\n",
      "Average training loss: 2.302477789878845\n",
      "Average test loss: 0.005939661051250166\n",
      "Epoch 88/300\n",
      "Average training loss: 1.276025726424323\n",
      "Average test loss: 0.005098871073375146\n",
      "Epoch 89/300\n",
      "Average training loss: 0.9097930373615689\n",
      "Average test loss: 0.005404595418522755\n",
      "Epoch 90/300\n",
      "Average training loss: 0.6614902309311761\n",
      "Average test loss: 0.004714045600137777\n",
      "Epoch 91/300\n",
      "Average training loss: 0.5036925668716431\n",
      "Average test loss: 0.00513271111001571\n",
      "Epoch 92/300\n",
      "Average training loss: 0.389663399193022\n",
      "Average test loss: 0.00417788574203021\n",
      "Epoch 93/300\n",
      "Average training loss: 0.30245565819740294\n",
      "Average test loss: 0.003916947204412685\n",
      "Epoch 94/300\n",
      "Average training loss: 0.23300950793425243\n",
      "Average test loss: 0.004011722060334351\n",
      "Epoch 95/300\n",
      "Average training loss: 0.17872952106263904\n",
      "Average test loss: 0.0037644975195742316\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1419374282360077\n",
      "Average test loss: 0.0042572246980335975\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11741773533821107\n",
      "Average test loss: 0.0036647404117716685\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10104565088616477\n",
      "Average test loss: 0.0047715852035002575\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08815105274650786\n",
      "Average test loss: 0.0034104054936518273\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07646301506625282\n",
      "Average test loss: 0.0033366137685047256\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06872427505585882\n",
      "Average test loss: 0.0033660983079009587\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06357283441887962\n",
      "Average test loss: 0.0031881870441138743\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05953128672308392\n",
      "Average test loss: 0.0032556135565456417\n",
      "Epoch 104/300\n",
      "Average training loss: 0.056159041265646616\n",
      "Average test loss: 0.0032160766530368065\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0529918977883127\n",
      "Average test loss: 0.0031150765725307993\n",
      "Epoch 106/300\n",
      "Average training loss: 0.050077961517704855\n",
      "Average test loss: 0.003546539895133012\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04769943779706955\n",
      "Average test loss: 0.0030972918158190116\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0460036482612292\n",
      "Average test loss: 0.0030431850312484634\n",
      "Epoch 109/300\n",
      "Average training loss: 0.044530410832828944\n",
      "Average test loss: 0.002998400136621462\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04327653289503521\n",
      "Average test loss: 0.0029794587490873203\n",
      "Epoch 111/300\n",
      "Average training loss: 0.042272740648852454\n",
      "Average test loss: 0.0029419930941528743\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04116645836830139\n",
      "Average test loss: 0.002942282765482863\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04041565594739384\n",
      "Average test loss: 0.0029393802045120133\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03956629542675283\n",
      "Average test loss: 0.002953536515434583\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03879052976436085\n",
      "Average test loss: 0.0029607879722283945\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03811933538648817\n",
      "Average test loss: 0.0029513151889873877\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03751529258489609\n",
      "Average test loss: 0.0029223165556581483\n",
      "Epoch 118/300\n",
      "Average training loss: 0.036992340786589514\n",
      "Average test loss: 0.0029762865773712596\n",
      "Epoch 119/300\n",
      "Average training loss: 0.036488626774814396\n",
      "Average test loss: 0.0029005208681854937\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03598751656545533\n",
      "Average test loss: 0.0030717096695055566\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03570600667430295\n",
      "Average test loss: 0.0031847647631333934\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03552625579304165\n",
      "Average test loss: 0.0029317228748566575\n",
      "Epoch 123/300\n",
      "Average training loss: 0.035137437141603896\n",
      "Average test loss: 0.0028995915818959476\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03481691093908416\n",
      "Average test loss: 0.0029265846210635373\n",
      "Epoch 125/300\n",
      "Average training loss: 0.034669523548748755\n",
      "Average test loss: 0.003000301248497433\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03448741616143121\n",
      "Average test loss: 0.0029587647587888772\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03437512075404326\n",
      "Average test loss: 0.002931141458451748\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03420707737406095\n",
      "Average test loss: 0.004276519267923302\n",
      "Epoch 129/300\n",
      "Average training loss: 0.034058929001291594\n",
      "Average test loss: 0.0030379677340388296\n",
      "Epoch 130/300\n",
      "Average training loss: 0.034046850936280355\n",
      "Average test loss: 0.0029653232677115332\n",
      "Epoch 131/300\n",
      "Average training loss: 0.033804081304205785\n",
      "Average test loss: 0.0029402886757420167\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03370996435483297\n",
      "Average test loss: 0.002984501444424192\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03369709333777428\n",
      "Average test loss: 0.0029543511296312016\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03359424859947628\n",
      "Average test loss: 0.003081759968151649\n",
      "Epoch 135/300\n",
      "Average training loss: 0.033437760627932016\n",
      "Average test loss: 0.002941833594813943\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03322843058241738\n",
      "Average test loss: 0.003030782540846202\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03334583850536082\n",
      "Average test loss: 0.0034019743951244487\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03321798278888067\n",
      "Average test loss: 0.002952636370849278\n",
      "Epoch 139/300\n",
      "Average training loss: 0.033039165844519935\n",
      "Average test loss: 0.003044325420839919\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03307382159597344\n",
      "Average test loss: 0.0032255689120954936\n",
      "Epoch 141/300\n",
      "Average training loss: 0.032842671379446985\n",
      "Average test loss: 0.0030761027518245907\n",
      "Epoch 142/300\n",
      "Average training loss: 0.032852845105859965\n",
      "Average test loss: 0.002952756703313854\n",
      "Epoch 143/300\n",
      "Average training loss: 0.032655211301313505\n",
      "Average test loss: 0.07597455780208111\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03256216186616156\n",
      "Average test loss: 0.0030799803123292\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0325346634603209\n",
      "Average test loss: 0.003025018102903333\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03240055395497216\n",
      "Average test loss: 0.0029563746615830395\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03261499151090781\n",
      "Average test loss: 0.0029750092145469456\n",
      "Epoch 148/300\n",
      "Average training loss: 0.2973357278638416\n",
      "Average test loss: 0.0039559982952972255\n",
      "Epoch 149/300\n",
      "Average training loss: 0.11475978246662352\n",
      "Average test loss: 0.0032971513087136878\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0711359986199273\n",
      "Average test loss: 0.0031594782051526838\n",
      "Epoch 151/300\n",
      "Average training loss: 0.057659292916456856\n",
      "Average test loss: 0.003100546220938365\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05186610284116533\n",
      "Average test loss: 0.0030600367376787795\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04853208238548703\n",
      "Average test loss: 0.0029970534276217223\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04615217168463601\n",
      "Average test loss: 0.0030192802680863275\n",
      "Epoch 155/300\n",
      "Average training loss: 0.044391439689530264\n",
      "Average test loss: 0.00840806076096164\n",
      "Epoch 156/300\n",
      "Average training loss: 0.042892148190074496\n",
      "Average test loss: 0.003045162719157007\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04160631747047106\n",
      "Average test loss: 0.0029632124917374717\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04047027654118008\n",
      "Average test loss: 0.0030089987727502984\n",
      "Epoch 159/300\n",
      "Average training loss: 0.039389039397239686\n",
      "Average test loss: 0.0029112480106867024\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03851778932081328\n",
      "Average test loss: 0.002891289092703826\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03770287853479386\n",
      "Average test loss: 0.0029317480163234804\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03699126463466221\n",
      "Average test loss: 0.003001357326284051\n",
      "Epoch 163/300\n",
      "Average training loss: 0.036386709885464774\n",
      "Average test loss: 0.0029345526744922004\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03579917276733451\n",
      "Average test loss: 0.0029199014444732004\n",
      "Epoch 165/300\n",
      "Average training loss: 0.035286748533447584\n",
      "Average test loss: 0.0029398442109425864\n",
      "Epoch 166/300\n",
      "Average training loss: 0.034878143140011364\n",
      "Average test loss: 0.003009187669803699\n",
      "Epoch 167/300\n",
      "Average training loss: 0.034463919076654644\n",
      "Average test loss: 0.0048481086306273935\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03427884409825007\n",
      "Average test loss: 0.003005597801051206\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03382944378587935\n",
      "Average test loss: 0.0028977791580061117\n",
      "Epoch 170/300\n",
      "Average training loss: 0.033578775563173825\n",
      "Average test loss: 0.0029415003698733118\n",
      "Epoch 171/300\n",
      "Average training loss: 0.033298814902702964\n",
      "Average test loss: 0.003162846672038237\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03311193775137265\n",
      "Average test loss: 0.0033056437079277304\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0329720077448421\n",
      "Average test loss: 0.003487161507209142\n",
      "Epoch 174/300\n",
      "Average training loss: 0.032836740689145195\n",
      "Average test loss: 0.0029746521160834366\n",
      "Epoch 175/300\n",
      "Average training loss: 0.032670685357517666\n",
      "Average test loss: 0.0029624813217669728\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03249292284912533\n",
      "Average test loss: 0.0029190496291137402\n",
      "Epoch 177/300\n",
      "Average training loss: 0.032365181926223965\n",
      "Average test loss: 0.002933391316793859\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03234500236478117\n",
      "Average test loss: 0.002987079705629084\n",
      "Epoch 179/300\n",
      "Average training loss: 0.032167958388725916\n",
      "Average test loss: 0.0030044453444166316\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03203438746266895\n",
      "Average test loss: 0.002962185532268551\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03202345848414633\n",
      "Average test loss: 0.0029671139588786496\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0319603133317497\n",
      "Average test loss: 0.0029828399192127915\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0317957783424192\n",
      "Average test loss: 0.0030148307213352784\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0318211710287465\n",
      "Average test loss: 0.0030920971311214898\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03221543357272943\n",
      "Average test loss: 0.016022293774618043\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03478209634621938\n",
      "Average test loss: 0.0029637594607969126\n",
      "Epoch 187/300\n",
      "Average training loss: 0.031890480116009715\n",
      "Average test loss: 0.0029887630311358306\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03145986888806025\n",
      "Average test loss: 0.0029615948386490345\n",
      "Epoch 189/300\n",
      "Average training loss: 0.031367168315582804\n",
      "Average test loss: 0.0030123760124875438\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031267060269912085\n",
      "Average test loss: 0.003052711084485054\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03128226519458824\n",
      "Average test loss: 0.003045678228346838\n",
      "Epoch 192/300\n",
      "Average training loss: 0.031373008166750274\n",
      "Average test loss: 0.004061124067339632\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03121944836775462\n",
      "Average test loss: 0.0029984716224587624\n",
      "Epoch 194/300\n",
      "Average training loss: 0.031131425244940652\n",
      "Average test loss: 0.003023723479360342\n",
      "Epoch 195/300\n",
      "Average training loss: 0.031023513125048745\n",
      "Average test loss: 0.0029886086881160735\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0313325640824106\n",
      "Average test loss: 0.0030200171158131627\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03154340035716693\n",
      "Average test loss: 0.0031191996956864993\n",
      "Epoch 198/300\n",
      "Average training loss: 0.031199738843573466\n",
      "Average test loss: 0.0034541874354084333\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03140883847574393\n",
      "Average test loss: 0.003031339901809891\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03093117962612046\n",
      "Average test loss: 0.0030419801188011964\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030755302265286447\n",
      "Average test loss: 0.0035956039109991656\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03077240839269426\n",
      "Average test loss: 0.0029713255821002854\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030705339911911222\n",
      "Average test loss: 0.003015789947575993\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030641562233368554\n",
      "Average test loss: 0.0029997555061967835\n",
      "Epoch 205/300\n",
      "Average training loss: 0.030600337270233365\n",
      "Average test loss: 0.003082213815508617\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030531619247463015\n",
      "Average test loss: 0.0031013885330822733\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030522108912467958\n",
      "Average test loss: 0.003033039840973086\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030463666083084213\n",
      "Average test loss: 0.003053534854617384\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030461463534169727\n",
      "Average test loss: 0.0034479495299359164\n",
      "Epoch 210/300\n",
      "Average training loss: 0.030344273976153798\n",
      "Average test loss: 0.0038036228070656456\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030298637517624433\n",
      "Average test loss: 0.0030741587798628544\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030273292576273283\n",
      "Average test loss: 0.0030680728395366006\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030234683109654322\n",
      "Average test loss: 0.0030398085840667288\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03068270756635401\n",
      "Average test loss: 0.0030193143549064796\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03131469777888722\n",
      "Average test loss: 0.0030014514041443664\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030190892602006594\n",
      "Average test loss: 0.004121425740938219\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030166548086537254\n",
      "Average test loss: 0.0030058489988247553\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030106432005763052\n",
      "Average test loss: 0.003255924458718962\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02999270183013545\n",
      "Average test loss: 0.0031358780111703606\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02988878927793768\n",
      "Average test loss: 0.0031480827203227415\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029950318887829782\n",
      "Average test loss: 0.003021940357983112\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0299534240944518\n",
      "Average test loss: 0.003085213125165966\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0299567858427763\n",
      "Average test loss: 0.0031651764414790604\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02993737934033076\n",
      "Average test loss: 0.003026628096691436\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030572855985826916\n",
      "Average test loss: 0.004192376704265674\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030027571669883197\n",
      "Average test loss: 0.003143131537569894\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02981927150322331\n",
      "Average test loss: 0.0031119543448504474\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02971263192428483\n",
      "Average test loss: 0.003142124886934956\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029720048755407333\n",
      "Average test loss: 0.003867194606612126\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02972267805536588\n",
      "Average test loss: 0.00309477278466026\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02961676134003533\n",
      "Average test loss: 0.0034190985303786067\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029630135774612428\n",
      "Average test loss: 0.008110266543096966\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02975779483550125\n",
      "Average test loss: 0.0034648102381163175\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029521705740027958\n",
      "Average test loss: 0.0032462655218938987\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029568240899178717\n",
      "Average test loss: 0.0031953421137813063\n",
      "Epoch 236/300\n",
      "Average training loss: 0.029504378577073415\n",
      "Average test loss: 0.003041540601187282\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02944196400543054\n",
      "Average test loss: 0.003060814878799849\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02944636109802458\n",
      "Average test loss: 0.003382995927085479\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029469649369517962\n",
      "Average test loss: 0.003066539384631647\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02935564759042528\n",
      "Average test loss: 0.003088663580516974\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029388255624307528\n",
      "Average test loss: 0.0031845272096494835\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029344936543040805\n",
      "Average test loss: 0.003073209506769975\n",
      "Epoch 243/300\n",
      "Average training loss: 0.029362273760967783\n",
      "Average test loss: 0.003359163527687391\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029340797495510842\n",
      "Average test loss: 0.005750166364014149\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02932880666024155\n",
      "Average test loss: 0.0030613874521934325\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030108336079451774\n",
      "Average test loss: 0.0030654113760424984\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02950404518014855\n",
      "Average test loss: 0.0031025151113669076\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02916116154856152\n",
      "Average test loss: 0.0036659783215986356\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029036375410026974\n",
      "Average test loss: 0.003098841108588709\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02922234081228574\n",
      "Average test loss: 0.003962000211286876\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02905974134968387\n",
      "Average test loss: 0.004008057230876552\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02908776016202238\n",
      "Average test loss: 0.0031761425965362126\n",
      "Epoch 253/300\n",
      "Average training loss: 0.029056216960151992\n",
      "Average test loss: 0.0031156699363556173\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029085362003909217\n",
      "Average test loss: 0.003090844957985812\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029048954827917948\n",
      "Average test loss: 0.0030886133826441235\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02900943645172649\n",
      "Average test loss: 0.0031801267645011344\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029043402885397276\n",
      "Average test loss: 0.003114546637154288\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02901367504398028\n",
      "Average test loss: 0.07074955037236214\n",
      "Epoch 259/300\n",
      "Average training loss: 0.029118074806200135\n",
      "Average test loss: 0.0031486821095976563\n",
      "Epoch 260/300\n",
      "Average training loss: 0.028958017978403302\n",
      "Average test loss: 0.003280073591818412\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02893815160459942\n",
      "Average test loss: 0.0030942696082509225\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029022999619444212\n",
      "Average test loss: 0.00311676844726834\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02907826205756929\n",
      "Average test loss: 0.0032211269748707613\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02886997965640492\n",
      "Average test loss: 0.003157910145405266\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02884872787197431\n",
      "Average test loss: 0.0041866887497405214\n",
      "Epoch 266/300\n",
      "Average training loss: 0.028822079315781592\n",
      "Average test loss: 0.003618714448064566\n",
      "Epoch 267/300\n",
      "Average training loss: 0.028794340764482815\n",
      "Average test loss: 0.003207854854149951\n",
      "Epoch 268/300\n",
      "Average training loss: 0.028803613800141546\n",
      "Average test loss: 0.003136216495600012\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02872229135533174\n",
      "Average test loss: 0.0031797868599080376\n",
      "Epoch 270/300\n",
      "Average training loss: 0.028696984517905446\n",
      "Average test loss: 0.003604176101171308\n",
      "Epoch 271/300\n",
      "Average training loss: 0.028815930974152353\n",
      "Average test loss: 0.003106540395153893\n",
      "Epoch 272/300\n",
      "Average training loss: 0.028730911019775603\n",
      "Average test loss: 0.003048309190198779\n",
      "Epoch 273/300\n",
      "Average training loss: 0.028663465582662157\n",
      "Average test loss: 0.00340646617507769\n",
      "Epoch 274/300\n",
      "Average training loss: 0.028700449294514128\n",
      "Average test loss: 0.009779811597532696\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02917247476014826\n",
      "Average test loss: 0.06212596458858914\n",
      "Epoch 276/300\n",
      "Average training loss: 0.028806477150983282\n",
      "Average test loss: 0.003136307220078177\n",
      "Epoch 277/300\n",
      "Average training loss: 0.028502187750405736\n",
      "Average test loss: 0.004684483230734865\n",
      "Epoch 278/300\n",
      "Average training loss: 0.028541903187831244\n",
      "Average test loss: 0.0031216620749069585\n",
      "Epoch 279/300\n",
      "Average training loss: 0.028583064541220664\n",
      "Average test loss: 0.0031551521635717816\n",
      "Epoch 280/300\n",
      "Average training loss: 0.028611377960277927\n",
      "Average test loss: 0.0031799304305265346\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02858009829123815\n",
      "Average test loss: 0.0031121884019424517\n",
      "Epoch 282/300\n",
      "Average training loss: 0.028511419950260058\n",
      "Average test loss: 0.0031086237666507564\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02854802973816792\n",
      "Average test loss: 0.003256549936408798\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02845966045392884\n",
      "Average test loss: 0.0031713468258579573\n",
      "Epoch 285/300\n",
      "Average training loss: 0.028443883427315287\n",
      "Average test loss: 0.0031649136453246077\n",
      "Epoch 286/300\n",
      "Average training loss: 0.028475260514352056\n",
      "Average test loss: 0.0031071837078779935\n",
      "Epoch 287/300\n",
      "Average training loss: 0.028483617714709707\n",
      "Average test loss: 0.010414480787184504\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02844231145746178\n",
      "Average test loss: 0.00316405982669029\n",
      "Epoch 289/300\n",
      "Average training loss: 0.028377641543745995\n",
      "Average test loss: 0.0031802841303870083\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02904200967649619\n",
      "Average test loss: 0.003073751406951083\n",
      "Epoch 291/300\n",
      "Average training loss: 0.028559979105989137\n",
      "Average test loss: 0.004026428204236759\n",
      "Epoch 292/300\n",
      "Average training loss: 0.028376261704497866\n",
      "Average test loss: 0.0033896925314846967\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02837108477867312\n",
      "Average test loss: 0.004510495929254426\n",
      "Epoch 294/300\n",
      "Average training loss: 0.028349410224292014\n",
      "Average test loss: 0.0031056612545831337\n",
      "Epoch 295/300\n",
      "Average training loss: 0.028323783774342803\n",
      "Average test loss: 0.0032184803150594236\n",
      "Epoch 296/300\n",
      "Average training loss: 0.028503125654326545\n",
      "Average test loss: 0.0031551756550454433\n",
      "Epoch 297/300\n",
      "Average training loss: 0.028182329474223985\n",
      "Average test loss: 0.003631891556084156\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02834941428237491\n",
      "Average test loss: 0.0031261428453856046\n",
      "Epoch 299/300\n",
      "Average training loss: 0.028204633557134203\n",
      "Average test loss: 0.0030963821845750015\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02824885191188918\n",
      "Average test loss: 0.0031476711061679654\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.0270281874868605\n",
      "Average test loss: 0.005594357303447194\n",
      "Epoch 2/300\n",
      "Average training loss: 0.3475361102686988\n",
      "Average test loss: 0.004682327488230335\n",
      "Epoch 3/300\n",
      "Average training loss: 0.2329940894179874\n",
      "Average test loss: 0.0042469424290789495\n",
      "Epoch 4/300\n",
      "Average training loss: 0.17545650272899205\n",
      "Average test loss: 0.0038688497559891807\n",
      "Epoch 5/300\n",
      "Average training loss: 0.14031017206774818\n",
      "Average test loss: 0.00378321428803934\n",
      "Epoch 6/300\n",
      "Average training loss: 0.11753417099846734\n",
      "Average test loss: 0.0034884706136460108\n",
      "Epoch 7/300\n",
      "Average training loss: 0.10072412263684803\n",
      "Average test loss: 0.003455561015341017\n",
      "Epoch 8/300\n",
      "Average training loss: 0.08885198489162657\n",
      "Average test loss: 0.0036639715811858575\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0795066411362754\n",
      "Average test loss: 0.0033160922821197244\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0723446169230673\n",
      "Average test loss: 0.0030878366958349943\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06664558570252524\n",
      "Average test loss: 0.003505451866115133\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06195860336886512\n",
      "Average test loss: 0.003881515211115281\n",
      "Epoch 13/300\n",
      "Average training loss: 0.058289849705166284\n",
      "Average test loss: 0.0031020420763848556\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05522871311505636\n",
      "Average test loss: 0.003196045045223501\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05229645624756813\n",
      "Average test loss: 0.002712885876910554\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04957028301225768\n",
      "Average test loss: 0.002616548039433029\n",
      "Epoch 17/300\n",
      "Average training loss: 0.047577022597193715\n",
      "Average test loss: 0.0024243489619758395\n",
      "Epoch 18/300\n",
      "Average training loss: 0.045835691990123856\n",
      "Average test loss: 0.0027053574745853743\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04368489877714051\n",
      "Average test loss: 0.002555900994895233\n",
      "Epoch 20/300\n",
      "Average training loss: 0.041960737152232065\n",
      "Average test loss: 0.0032573621233718264\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04100170313649707\n",
      "Average test loss: 0.002961649591310157\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03966579636600282\n",
      "Average test loss: 0.0023386790514406232\n",
      "Epoch 23/300\n",
      "Average training loss: 0.037866134332285986\n",
      "Average test loss: 0.0021472642038845354\n",
      "Epoch 24/300\n",
      "Average training loss: 0.036999018967151645\n",
      "Average test loss: 0.002312964140954945\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03551312538319164\n",
      "Average test loss: 0.002387416833701233\n",
      "Epoch 26/300\n",
      "Average training loss: 0.034627478202184044\n",
      "Average test loss: 0.0021672627225311266\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03413437020281951\n",
      "Average test loss: 0.002466657523272766\n",
      "Epoch 28/300\n",
      "Average training loss: 0.032735897165205745\n",
      "Average test loss: 0.0021219432349834175\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03214348798659113\n",
      "Average test loss: 0.002086375562681092\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04552170532610681\n",
      "Average test loss: 0.002260528273983962\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03436657259199354\n",
      "Average test loss: 0.0021288288065956697\n",
      "Epoch 32/300\n",
      "Average training loss: 0.032773584693670275\n",
      "Average test loss: 0.002125546025733153\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03158388458357917\n",
      "Average test loss: 0.002118859822551409\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03137512284848425\n",
      "Average test loss: 0.0022054170914408235\n",
      "Epoch 35/300\n",
      "Average training loss: 0.030801169261336327\n",
      "Average test loss: 0.0022165230011774433\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03015963386495908\n",
      "Average test loss: 0.002024057784014278\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0297728760474258\n",
      "Average test loss: 0.002055031148820288\n",
      "Epoch 38/300\n",
      "Average training loss: 0.030193909703029525\n",
      "Average test loss: 0.003407139574694965\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02934730143017239\n",
      "Average test loss: 0.0023044738386654193\n",
      "Epoch 40/300\n",
      "Average training loss: 0.028783687902821434\n",
      "Average test loss: 0.0020567376390099525\n",
      "Epoch 41/300\n",
      "Average training loss: 0.028660139525930085\n",
      "Average test loss: 0.0019688076378984584\n",
      "Epoch 42/300\n",
      "Average training loss: 0.028325533060563936\n",
      "Average test loss: 0.001942108677700162\n",
      "Epoch 43/300\n",
      "Average training loss: 0.028095966789457534\n",
      "Average test loss: 0.0019593048633800613\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03154974013070266\n",
      "Average test loss: 0.0020394769611044063\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02840589897003439\n",
      "Average test loss: 0.0019606114868074656\n",
      "Epoch 46/300\n",
      "Average training loss: 0.027951775757802857\n",
      "Average test loss: 0.001977189145464864\n",
      "Epoch 47/300\n",
      "Average training loss: 0.027620962897936502\n",
      "Average test loss: 0.0019337187539268698\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02731104256874985\n",
      "Average test loss: 0.001973743046530419\n",
      "Epoch 49/300\n",
      "Average training loss: 0.027182219905985728\n",
      "Average test loss: 0.0020142014851379725\n",
      "Epoch 50/300\n",
      "Average training loss: 0.027003834656543203\n",
      "Average test loss: 0.001967680300482445\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02689393172164758\n",
      "Average test loss: 0.001940754417123066\n",
      "Epoch 52/300\n",
      "Average training loss: 0.026815026887589032\n",
      "Average test loss: 0.00206257316759891\n",
      "Epoch 53/300\n",
      "Average training loss: 0.026465666188134087\n",
      "Average test loss: 0.0020712278369400235\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02625787334309684\n",
      "Average test loss: 0.001943551216688421\n",
      "Epoch 55/300\n",
      "Average training loss: 0.026062046739790175\n",
      "Average test loss: 0.001983540415763855\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02589177531335089\n",
      "Average test loss: 0.001927310514367289\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025959203074375787\n",
      "Average test loss: 0.002022427271844612\n",
      "Epoch 58/300\n",
      "Average training loss: 0.025869950936900244\n",
      "Average test loss: 0.0021151842110686834\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02769078575240241\n",
      "Average test loss: 0.0019514688599026866\n",
      "Epoch 60/300\n",
      "Average training loss: 0.025719834339287546\n",
      "Average test loss: 0.0019564036487912137\n",
      "Epoch 61/300\n",
      "Average training loss: 0.025522056332892843\n",
      "Average test loss: 0.0018809930063370202\n",
      "Epoch 62/300\n",
      "Average training loss: 0.025236696165468957\n",
      "Average test loss: 0.0019044503420591354\n",
      "Epoch 63/300\n",
      "Average training loss: 0.025149660837319163\n",
      "Average test loss: 0.002017242708760831\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024995921169718105\n",
      "Average test loss: 0.001880336770373914\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024995523756576907\n",
      "Average test loss: 0.004009253237189518\n",
      "Epoch 66/300\n",
      "Average training loss: 0.024835234145323435\n",
      "Average test loss: 0.005628321399291356\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024791252261234655\n",
      "Average test loss: 0.0019350312035530805\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024671045574876996\n",
      "Average test loss: 0.003925731906460391\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02451310516728295\n",
      "Average test loss: 0.005282587268700202\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024798555594351557\n",
      "Average test loss: 0.0018660220034006569\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02512362541920609\n",
      "Average test loss: 0.0019456737169788944\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024280462730262013\n",
      "Average test loss: 0.001961985751469102\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024146697603993947\n",
      "Average test loss: 0.007307398755517271\n",
      "Epoch 74/300\n",
      "Average training loss: 0.024193396722277005\n",
      "Average test loss: 0.001898062715306878\n",
      "Epoch 75/300\n",
      "Average training loss: 0.024161193177103996\n",
      "Average test loss: 0.0026002178316315017\n",
      "Epoch 76/300\n",
      "Average training loss: 0.023924143084221416\n",
      "Average test loss: 0.0019416860337886546\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02384989839129978\n",
      "Average test loss: 0.001999133875593543\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023735123240285448\n",
      "Average test loss: 0.007977183095282978\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02373651009135776\n",
      "Average test loss: 0.001866295784815318\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023587833328379524\n",
      "Average test loss: 0.0018615319702981247\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023618826559848257\n",
      "Average test loss: 0.0020601201712464293\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023650574813286463\n",
      "Average test loss: 0.0021157441266501944\n",
      "Epoch 83/300\n",
      "Average training loss: 0.023539053986469903\n",
      "Average test loss: 0.002154365670763784\n",
      "Epoch 84/300\n",
      "Average training loss: 0.023256172272066274\n",
      "Average test loss: 0.0019595309934682314\n",
      "Epoch 85/300\n",
      "Average training loss: 0.023216960716578694\n",
      "Average test loss: 0.0018474171434839566\n",
      "Epoch 86/300\n",
      "Average training loss: 0.023539643852247133\n",
      "Average test loss: 0.0018597562464161051\n",
      "Epoch 87/300\n",
      "Average training loss: 0.023726687400705285\n",
      "Average test loss: 0.0018577301942019\n",
      "Epoch 88/300\n",
      "Average training loss: 0.023068581591877673\n",
      "Average test loss: 0.0018931351074328025\n",
      "Epoch 89/300\n",
      "Average training loss: 0.022908238874541388\n",
      "Average test loss: 0.0018696654465877348\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023049410802622638\n",
      "Average test loss: 0.0021313222806072897\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022867365229460927\n",
      "Average test loss: 0.0019757816390030916\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022811202241314783\n",
      "Average test loss: 0.0018584608679844273\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022775951786173716\n",
      "Average test loss: 0.0019552287828798094\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022779014479782846\n",
      "Average test loss: 0.00190096946609103\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022955343605743514\n",
      "Average test loss: 0.0018642797159651916\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022569298141532475\n",
      "Average test loss: 0.001885386327592035\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02248790922926532\n",
      "Average test loss: 0.0022582246780188547\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022600809400280315\n",
      "Average test loss: 0.0035949875356422532\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02244422726167573\n",
      "Average test loss: 0.0018669066429138183\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022482518702745438\n",
      "Average test loss: 0.0018630185096214216\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02242262720813354\n",
      "Average test loss: 0.001900835373128454\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02233540460632907\n",
      "Average test loss: 0.001942605210468173\n",
      "Epoch 103/300\n",
      "Average training loss: 0.022215621665120124\n",
      "Average test loss: 0.001892666495974279\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022247348078423077\n",
      "Average test loss: 0.0018730797815757494\n",
      "Epoch 105/300\n",
      "Average training loss: 0.022464355380998717\n",
      "Average test loss: 0.001869831254498826\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02215494064324432\n",
      "Average test loss: 0.0018971569070385561\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02210194147626559\n",
      "Average test loss: 0.0019159998167306185\n",
      "Epoch 108/300\n",
      "Average training loss: 0.022169390542639625\n",
      "Average test loss: 0.00202553849046429\n",
      "Epoch 109/300\n",
      "Average training loss: 0.021953685238957407\n",
      "Average test loss: 0.0019023032640624377\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021893778127928577\n",
      "Average test loss: 0.0021428276031381552\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022004263351360957\n",
      "Average test loss: 0.0018885627643515667\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02185612067249086\n",
      "Average test loss: 0.001918269504689508\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021924949276778433\n",
      "Average test loss: 0.0019155514968766107\n",
      "Epoch 114/300\n",
      "Average training loss: 0.021735397688216633\n",
      "Average test loss: 0.0018804883544022838\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0217490464647611\n",
      "Average test loss: 0.002023011637541155\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021689705524179672\n",
      "Average test loss: 0.0018893291488703754\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02190371721320682\n",
      "Average test loss: 0.0019492192306659287\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021663043667872747\n",
      "Average test loss: 0.0018884608253008789\n",
      "Epoch 119/300\n",
      "Average training loss: 0.021659337803721427\n",
      "Average test loss: 0.002072458513081074\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0215364894742767\n",
      "Average test loss: 0.0019013937449910575\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021513309905926387\n",
      "Average test loss: 0.001899002994514174\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021595469476448165\n",
      "Average test loss: 0.0029438512041750882\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02159473546511597\n",
      "Average test loss: 0.0018997794435256057\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02175120691458384\n",
      "Average test loss: 0.0021007696561929252\n",
      "Epoch 125/300\n",
      "Average training loss: 0.021476824358105658\n",
      "Average test loss: 0.002373141836374998\n",
      "Epoch 126/300\n",
      "Average training loss: 0.021364330960644617\n",
      "Average test loss: 0.0019140538478063212\n",
      "Epoch 127/300\n",
      "Average training loss: 0.021435110819008615\n",
      "Average test loss: 0.001923667367754711\n",
      "Epoch 128/300\n",
      "Average training loss: 0.021253321907586522\n",
      "Average test loss: 0.0021742854724741646\n",
      "Epoch 129/300\n",
      "Average training loss: 0.021362892551554574\n",
      "Average test loss: 0.001895346824804114\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021179125029179784\n",
      "Average test loss: 0.00205448658992019\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021313941286669837\n",
      "Average test loss: 0.001906084985161821\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021350327080322636\n",
      "Average test loss: 0.021257907511666417\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021250358543462224\n",
      "Average test loss: 0.0019023234384755294\n",
      "Epoch 134/300\n",
      "Average training loss: 0.021150417311324013\n",
      "Average test loss: 0.0021674474626779557\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021091860070824624\n",
      "Average test loss: 0.002290464953829845\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02111431163052718\n",
      "Average test loss: 0.002184739279250304\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021189715526170202\n",
      "Average test loss: 0.0019671219792217016\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02124814945211013\n",
      "Average test loss: 0.0019866000554627844\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021162531312968996\n",
      "Average test loss: 0.001955150170251727\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021154825632770855\n",
      "Average test loss: 0.013524785547827682\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02096075490779347\n",
      "Average test loss: 0.001971950949480136\n",
      "Epoch 142/300\n",
      "Average training loss: 0.020918716286619503\n",
      "Average test loss: 0.001969643661752343\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021015973922279146\n",
      "Average test loss: 0.0024873767043981287\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02090523625910282\n",
      "Average test loss: 0.0020971745263992083\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02089292642639743\n",
      "Average test loss: 0.00204172433892058\n",
      "Epoch 146/300\n",
      "Average training loss: 0.020909909692075516\n",
      "Average test loss: 0.0021405037869181897\n",
      "Epoch 147/300\n",
      "Average training loss: 0.020869147731198204\n",
      "Average test loss: 0.0019840427434278858\n",
      "Epoch 148/300\n",
      "Average training loss: 0.020860919846428766\n",
      "Average test loss: 0.0027737586356492505\n",
      "Epoch 149/300\n",
      "Average training loss: 0.020918037593364715\n",
      "Average test loss: 0.001929348655251993\n",
      "Epoch 150/300\n",
      "Average training loss: 0.020944927154315843\n",
      "Average test loss: 0.0026837840127862164\n",
      "Epoch 151/300\n",
      "Average training loss: 0.020879899190531837\n",
      "Average test loss: 0.0019574092444446353\n",
      "Epoch 152/300\n",
      "Average training loss: 0.020700856246882015\n",
      "Average test loss: 0.002530968094865481\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02077164338860247\n",
      "Average test loss: 0.0019654701124462814\n",
      "Epoch 154/300\n",
      "Average training loss: 0.020825456564625103\n",
      "Average test loss: 0.001935564191908472\n",
      "Epoch 155/300\n",
      "Average training loss: 0.020802424730526077\n",
      "Average test loss: 0.0021700854947169623\n",
      "Epoch 156/300\n",
      "Average training loss: 0.020838861521747378\n",
      "Average test loss: 0.002067907533297936\n",
      "Epoch 157/300\n",
      "Average training loss: 0.020593037143349646\n",
      "Average test loss: 0.0030525350057416492\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02059310674170653\n",
      "Average test loss: 0.002036773247644305\n",
      "Epoch 159/300\n",
      "Average training loss: 0.020593062027460998\n",
      "Average test loss: 0.04046442912270625\n",
      "Epoch 160/300\n",
      "Average training loss: 0.020606433918078742\n",
      "Average test loss: 0.001977615510010057\n",
      "Epoch 161/300\n",
      "Average training loss: 0.020672918612758318\n",
      "Average test loss: 0.0019822288817829556\n",
      "Epoch 162/300\n",
      "Average training loss: 0.020530776381492616\n",
      "Average test loss: 0.0019728332731044954\n",
      "Epoch 163/300\n",
      "Average training loss: 0.020656200973523987\n",
      "Average test loss: 0.0019272482048513161\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02060532211843464\n",
      "Average test loss: 0.0022024307784934837\n",
      "Epoch 165/300\n",
      "Average training loss: 0.020828475518359077\n",
      "Average test loss: 0.0020141058577638534\n",
      "Epoch 166/300\n",
      "Average training loss: 0.020401997827821306\n",
      "Average test loss: 0.0019649961832910777\n",
      "Epoch 167/300\n",
      "Average training loss: 0.020488697909646563\n",
      "Average test loss: 0.0020300246151164175\n",
      "Epoch 168/300\n",
      "Average training loss: 0.020417325847678715\n",
      "Average test loss: 0.001995962348042263\n",
      "Epoch 169/300\n",
      "Average training loss: 0.020529612840877638\n",
      "Average test loss: 0.0020170766540492575\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02036011513405376\n",
      "Average test loss: 0.002161529969320529\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02055281490749783\n",
      "Average test loss: 0.0019566756784915925\n",
      "Epoch 172/300\n",
      "Average training loss: 0.020463175002071593\n",
      "Average test loss: 0.0019796776788102255\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02037053152339326\n",
      "Average test loss: 0.00206918673362169\n",
      "Epoch 174/300\n",
      "Average training loss: 0.020358736627631716\n",
      "Average test loss: 0.002384248891017503\n",
      "Epoch 175/300\n",
      "Average training loss: 0.020406622990965843\n",
      "Average test loss: 0.0020917921695444317\n",
      "Epoch 176/300\n",
      "Average training loss: 0.020490892410278322\n",
      "Average test loss: 0.002016651501879096\n",
      "Epoch 177/300\n",
      "Average training loss: 0.020605020809504722\n",
      "Average test loss: 0.0020505802603438495\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02022587656478087\n",
      "Average test loss: 0.002088878806059559\n",
      "Epoch 179/300\n",
      "Average training loss: 0.020332864976591532\n",
      "Average test loss: 0.0021334167420864105\n",
      "Epoch 180/300\n",
      "Average training loss: 0.020286881120668517\n",
      "Average test loss: 0.01038884064141247\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02029805300798681\n",
      "Average test loss: 0.002615516085488101\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02026480431854725\n",
      "Average test loss: 0.0029113941627244154\n",
      "Epoch 183/300\n",
      "Average training loss: 0.020204595304197735\n",
      "Average test loss: 0.0020002700571074253\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02034038300977813\n",
      "Average test loss: 0.0034658191599365737\n",
      "Epoch 185/300\n",
      "Average training loss: 0.020334271495540936\n",
      "Average test loss: 0.002017506376736694\n",
      "Epoch 186/300\n",
      "Average training loss: 0.020211673319339752\n",
      "Average test loss: 0.0020170128586598566\n",
      "Epoch 187/300\n",
      "Average training loss: 0.020140594825148583\n",
      "Average test loss: 0.0019866103332282768\n",
      "Epoch 188/300\n",
      "Average training loss: 0.020132828219069376\n",
      "Average test loss: 0.001985821961528725\n",
      "Epoch 189/300\n",
      "Average training loss: 0.020276904066403707\n",
      "Average test loss: 0.0020986433293566937\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020294866838388975\n",
      "Average test loss: 0.0031809045891794895\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02005528607633379\n",
      "Average test loss: 0.003797736698658102\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02015081293715371\n",
      "Average test loss: 0.002045961337784926\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0200521097994513\n",
      "Average test loss: 0.002056282199919224\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02035696861313449\n",
      "Average test loss: 0.001967002631061607\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020034943347175916\n",
      "Average test loss: 0.0028482221458107234\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020216264670093853\n",
      "Average test loss: 0.0021437226036149595\n",
      "Epoch 197/300\n",
      "Average training loss: 0.020190306102236114\n",
      "Average test loss: 0.001994748923099703\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020031849505172835\n",
      "Average test loss: 0.002070452533144918\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02005025738394923\n",
      "Average test loss: 0.004716342662357622\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020046882854567634\n",
      "Average test loss: 0.0021987970624532966\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019982942567931282\n",
      "Average test loss: 0.0020737519319065745\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020070403120583957\n",
      "Average test loss: 0.005179953164524502\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020025660262339644\n",
      "Average test loss: 0.002080384838860482\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019998434275388718\n",
      "Average test loss: 0.0019791304090370733\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019994956359267235\n",
      "Average test loss: 0.0019929902565975982\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02040347346332338\n",
      "Average test loss: 0.0021259968431873453\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01997789979726076\n",
      "Average test loss: 0.0058844536162085005\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01987063259548611\n",
      "Average test loss: 0.001957243801198072\n",
      "Epoch 209/300\n",
      "Average training loss: 0.020467906585998005\n",
      "Average test loss: 0.0019756225755231246\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01982938247256809\n",
      "Average test loss: 0.0019996460032545857\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019860865833030806\n",
      "Average test loss: 0.0019833281616576845\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01987440603143639\n",
      "Average test loss: 0.0020133068201442558\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019944523312979273\n",
      "Average test loss: 0.0029961098507046697\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01994771326912774\n",
      "Average test loss: 0.0020480709565389487\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01997304996020264\n",
      "Average test loss: 0.002049255899567571\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01984602934287654\n",
      "Average test loss: 0.0019923162834925784\n",
      "Epoch 217/300\n",
      "Average training loss: 0.019820920358101526\n",
      "Average test loss: 0.0021008151221192546\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019889909856849245\n",
      "Average test loss: 0.002107631662239631\n",
      "Epoch 219/300\n",
      "Average training loss: 0.019807888343102403\n",
      "Average test loss: 0.0020040628239512445\n",
      "Epoch 220/300\n",
      "Average training loss: 0.019878520063228076\n",
      "Average test loss: 0.002037082922955354\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019857643105917505\n",
      "Average test loss: 0.0020328757013711664\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019791086946924528\n",
      "Average test loss: 0.0020350753178613053\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019888023932774862\n",
      "Average test loss: 0.002003215909625093\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019725835939248402\n",
      "Average test loss: 0.0020521840173751116\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019788556206557485\n",
      "Average test loss: 0.00215131479770773\n",
      "Epoch 226/300\n",
      "Average training loss: 0.019758475777175693\n",
      "Average test loss: 0.0020814434020883506\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019803050188554657\n",
      "Average test loss: 0.001999220807933145\n",
      "Epoch 228/300\n",
      "Average training loss: 0.019692915255824724\n",
      "Average test loss: 0.0021076576554526884\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01994223177515798\n",
      "Average test loss: 0.0020334744225773546\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019679366577002737\n",
      "Average test loss: 0.002003607976560791\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01971144542263614\n",
      "Average test loss: 0.0019890069239255456\n",
      "Epoch 232/300\n",
      "Average training loss: 0.019692579766114552\n",
      "Average test loss: 0.0020097844364742436\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019701394177145427\n",
      "Average test loss: 0.0021189776875285638\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01968276601533095\n",
      "Average test loss: 0.002012767950176365\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01965791887210475\n",
      "Average test loss: 0.0020131429404848153\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019717537187867694\n",
      "Average test loss: 0.00211715648504388\n",
      "Epoch 237/300\n",
      "Average training loss: 0.019655305669539505\n",
      "Average test loss: 0.0020948253460228443\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019652691981858678\n",
      "Average test loss: 0.0020176030505034658\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01985646725197633\n",
      "Average test loss: 0.0021330711881940565\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01959122734434075\n",
      "Average test loss: 0.0020079802783826988\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01958808434340689\n",
      "Average test loss: 0.00210447063172857\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01966208088232411\n",
      "Average test loss: 0.002010854039651652\n",
      "Epoch 243/300\n",
      "Average training loss: 0.019584029239084987\n",
      "Average test loss: 0.002099995178584423\n",
      "Epoch 244/300\n",
      "Average training loss: 0.019838793971472316\n",
      "Average test loss: 0.00363492208884822\n",
      "Epoch 245/300\n",
      "Average training loss: 0.019614348833759624\n",
      "Average test loss: 0.003238319443538785\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01951970182110866\n",
      "Average test loss: 0.0021503264169312184\n",
      "Epoch 247/300\n",
      "Average training loss: 0.019605148850215807\n",
      "Average test loss: 0.002030340945865545\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01957738028301133\n",
      "Average test loss: 0.0020756239872425793\n",
      "Epoch 249/300\n",
      "Average training loss: 0.019598775683177843\n",
      "Average test loss: 0.00227125403450595\n",
      "Epoch 250/300\n",
      "Average training loss: 0.019576078252659904\n",
      "Average test loss: 0.0020252069609446657\n",
      "Epoch 251/300\n",
      "Average training loss: 0.019523566539088884\n",
      "Average test loss: 0.002054828458569116\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01957809287475215\n",
      "Average test loss: 0.002059647789535423\n",
      "Epoch 253/300\n",
      "Average training loss: 0.019499581603540313\n",
      "Average test loss: 0.002010461891328709\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019596461966633796\n",
      "Average test loss: 0.3497176295088397\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01958551519612471\n",
      "Average test loss: 0.0023879001086784734\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01942465470896827\n",
      "Average test loss: 0.0022130192269881567\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019532167280713716\n",
      "Average test loss: 0.0023401054522643487\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01958930024007956\n",
      "Average test loss: 0.0020912193097174167\n",
      "Epoch 259/300\n",
      "Average training loss: 0.019415108434028095\n",
      "Average test loss: 0.00203899870119575\n",
      "Epoch 260/300\n",
      "Average training loss: 0.019467720202273792\n",
      "Average test loss: 0.002465600085341268\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01947119197746118\n",
      "Average test loss: 0.002008853027390109\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01946040926873684\n",
      "Average test loss: 0.0026997118045886358\n",
      "Epoch 263/300\n",
      "Average training loss: 0.019484101883239215\n",
      "Average test loss: 0.002060757273601161\n",
      "Epoch 264/300\n",
      "Average training loss: 0.019433740066157446\n",
      "Average test loss: 0.0020471713641244506\n",
      "Epoch 265/300\n",
      "Average training loss: 0.019441126147905986\n",
      "Average test loss: 0.0020109803795607555\n",
      "Epoch 266/300\n",
      "Average training loss: 0.019510681427187388\n",
      "Average test loss: 0.0020189638760768707\n",
      "Epoch 267/300\n",
      "Average training loss: 0.019384986533059013\n",
      "Average test loss: 0.002061981208415495\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01949723890092638\n",
      "Average test loss: 0.002046018817772468\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0194337128897508\n",
      "Average test loss: 0.0020490374396451645\n",
      "Epoch 270/300\n",
      "Average training loss: 0.019419474528895486\n",
      "Average test loss: 0.0023837947468790745\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01936462414761384\n",
      "Average test loss: 0.0023745827683144143\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01939463815010256\n",
      "Average test loss: 0.002938095057900581\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01938883249130514\n",
      "Average test loss: 0.0021850106858958803\n",
      "Epoch 274/300\n",
      "Average training loss: 0.019398191480173004\n",
      "Average test loss: 0.002241304061479039\n",
      "Epoch 275/300\n",
      "Average training loss: 0.019395714779694876\n",
      "Average test loss: 0.005129523117509153\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01937609933813413\n",
      "Average test loss: 0.002081718839808471\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021209921199414464\n",
      "Average test loss: 0.0020419514957401488\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01939889101518525\n",
      "Average test loss: 0.002083989561224977\n",
      "Epoch 279/300\n",
      "Average training loss: 0.019223764755659633\n",
      "Average test loss: 0.0032725565309325855\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019297743486033546\n",
      "Average test loss: 0.0020522833137462536\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01931323567695088\n",
      "Average test loss: 0.002065511535956628\n",
      "Epoch 282/300\n",
      "Average training loss: 0.019294410932395194\n",
      "Average test loss: 0.0023247255231771203\n",
      "Epoch 283/300\n",
      "Average training loss: 0.019287872528036435\n",
      "Average test loss: 0.002083816786814067\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019291283920407295\n",
      "Average test loss: 0.002001017180788848\n",
      "Epoch 285/300\n",
      "Average training loss: 0.019394129988220002\n",
      "Average test loss: 0.002280861269682646\n",
      "Epoch 286/300\n",
      "Average training loss: 0.019350111915005578\n",
      "Average test loss: 0.0033374187468240657\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01928183590537972\n",
      "Average test loss: 0.0023558431282225583\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01923812599480152\n",
      "Average test loss: 0.0020800235924414464\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01928935071494844\n",
      "Average test loss: 0.0020989623843795722\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01923793038063579\n",
      "Average test loss: 0.0021014937922979393\n",
      "Epoch 291/300\n",
      "Average training loss: 0.019311910457081263\n",
      "Average test loss: 0.0020159069689818553\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0192538466586007\n",
      "Average test loss: 0.002033073390937514\n",
      "Epoch 293/300\n",
      "Average training loss: 0.019236106110943688\n",
      "Average test loss: 0.002019914637423224\n",
      "Epoch 294/300\n",
      "Average training loss: 0.019389186087581847\n",
      "Average test loss: 0.0023721073475769824\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0193048440731234\n",
      "Average test loss: 0.0020227201861432857\n",
      "Epoch 296/300\n",
      "Average training loss: 0.019284453461567562\n",
      "Average test loss: 0.0021597919629679787\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019269229369031058\n",
      "Average test loss: 0.017268383506271575\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019327003441751003\n",
      "Average test loss: 0.002042941854697549\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01924270348582003\n",
      "Average test loss: 0.002534121073885924\n",
      "Epoch 300/300\n",
      "Average training loss: 0.019224390824635822\n",
      "Average test loss: 0.002214536314830184\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.0205921543969048\n",
      "Average test loss: 0.005004176876197259\n",
      "Epoch 2/300\n",
      "Average training loss: 0.31290331178241304\n",
      "Average test loss: 0.003957672016488181\n",
      "Epoch 3/300\n",
      "Average training loss: 0.2086296560631858\n",
      "Average test loss: 0.0034527895220865805\n",
      "Epoch 4/300\n",
      "Average training loss: 0.15715948593616486\n",
      "Average test loss: 0.003325783991151386\n",
      "Epoch 5/300\n",
      "Average training loss: 0.12612812815772162\n",
      "Average test loss: 0.0029612216556237805\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1043965293235249\n",
      "Average test loss: 0.00310667805125316\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08915927251180013\n",
      "Average test loss: 0.005089648589905765\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07776612505647872\n",
      "Average test loss: 0.0032544528637081386\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06891487861341901\n",
      "Average test loss: 0.0025501211835071446\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06257035695181952\n",
      "Average test loss: 0.00347669036520852\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05737268858816889\n",
      "Average test loss: 0.002488880636791388\n",
      "Epoch 12/300\n",
      "Average training loss: 0.053261632127894296\n",
      "Average test loss: 0.002212947749532759\n",
      "Epoch 13/300\n",
      "Average training loss: 0.049639913039075\n",
      "Average test loss: 0.0022219646230546963\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04679141302406788\n",
      "Average test loss: 0.0033170374412503506\n",
      "Epoch 15/300\n",
      "Average training loss: 0.044254378447930016\n",
      "Average test loss: 0.002050700957990355\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04227772883574168\n",
      "Average test loss: 0.002373102310217089\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04001469588941998\n",
      "Average test loss: 0.0020834235679358243\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03861318950851758\n",
      "Average test loss: 0.0020256337597966195\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03688137327631315\n",
      "Average test loss: 0.002392193176059259\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03538344821333885\n",
      "Average test loss: 0.0023723716319849096\n",
      "Epoch 21/300\n",
      "Average training loss: 0.034071385502815243\n",
      "Average test loss: 0.002397293502671851\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03291785689857271\n",
      "Average test loss: 0.001931867571030226\n",
      "Epoch 23/300\n",
      "Average training loss: 0.031766512049569026\n",
      "Average test loss: 0.002374932220723066\n",
      "Epoch 24/300\n",
      "Average training loss: 0.030463831735981835\n",
      "Average test loss: 0.0017595683499756786\n",
      "Epoch 25/300\n",
      "Average training loss: 0.029598157074716355\n",
      "Average test loss: 0.0016591091200502382\n",
      "Epoch 26/300\n",
      "Average training loss: 0.028483815459741487\n",
      "Average test loss: 0.0019135176117221514\n",
      "Epoch 27/300\n",
      "Average training loss: 0.027804689819614092\n",
      "Average test loss: 0.0016394360787752602\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02644788736436102\n",
      "Average test loss: 0.0016818968461205562\n",
      "Epoch 29/300\n",
      "Average training loss: 0.025513030303849115\n",
      "Average test loss: 0.0014906117600492306\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024752973549895817\n",
      "Average test loss: 0.0014719772409233782\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02416902839806345\n",
      "Average test loss: 0.0014468908267509606\n",
      "Epoch 32/300\n",
      "Average training loss: 0.023504276759094663\n",
      "Average test loss: 0.0014750922792073752\n",
      "Epoch 33/300\n",
      "Average training loss: 0.023420076396730213\n",
      "Average test loss: 0.0014711103096811309\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0226015347548657\n",
      "Average test loss: 0.0014117913838062022\n",
      "Epoch 35/300\n",
      "Average training loss: 0.022249185271561146\n",
      "Average test loss: 0.0015951808091873923\n",
      "Epoch 36/300\n",
      "Average training loss: 0.022817706720696555\n",
      "Average test loss: 0.001387074298846225\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02163014573024379\n",
      "Average test loss: 0.0017100880564086967\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02135541056593259\n",
      "Average test loss: 0.0013708901702322895\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02115023016764058\n",
      "Average test loss: 0.00135679615702894\n",
      "Epoch 40/300\n",
      "Average training loss: 0.021095844336681897\n",
      "Average test loss: 0.0015122438522262706\n",
      "Epoch 41/300\n",
      "Average training loss: 0.020805981642670102\n",
      "Average test loss: 0.001379688844498661\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020505609753231208\n",
      "Average test loss: 0.0013321936836259232\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020350840303632947\n",
      "Average test loss: 0.0013449356525929437\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02022716310289171\n",
      "Average test loss: 0.001657605765490896\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01998208967513508\n",
      "Average test loss: 0.001375389961939719\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019747762536009152\n",
      "Average test loss: 0.001507635861945649\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019581987242731785\n",
      "Average test loss: 0.001367817161190841\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01952943259312047\n",
      "Average test loss: 0.0015310740708890888\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0192601154566639\n",
      "Average test loss: 0.0016455340609471832\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019227496273815634\n",
      "Average test loss: 0.0019485911250942283\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019004477436343828\n",
      "Average test loss: 0.001784175580781367\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018840243183904225\n",
      "Average test loss: 0.00140456932120853\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018770460358924337\n",
      "Average test loss: 0.001291774307274156\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018583113776312932\n",
      "Average test loss: 0.0013730339729744528\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018487759379876983\n",
      "Average test loss: 0.0012721405381129847\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018608935756815805\n",
      "Average test loss: 0.0012759962087083195\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01823043889635139\n",
      "Average test loss: 0.0012862687202998334\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01822040271262328\n",
      "Average test loss: 0.004976343765026993\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01818396164311303\n",
      "Average test loss: 0.0013295496329665184\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01801995096769598\n",
      "Average test loss: 0.0012711042751454645\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01802234826485316\n",
      "Average test loss: 0.001542452198970649\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018948618289497163\n",
      "Average test loss: 0.00170535725603501\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017972019960482914\n",
      "Average test loss: 0.0012715760346295106\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017747793856594297\n",
      "Average test loss: 0.0013708588576151265\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017675890882809957\n",
      "Average test loss: 0.0013046820730798774\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017658146780398155\n",
      "Average test loss: 0.0012468143272110158\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0177405154986514\n",
      "Average test loss: 0.0012557287843277057\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017536580943399004\n",
      "Average test loss: 0.0012448182674124836\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017528018567297195\n",
      "Average test loss: 0.0014476596949001153\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017517540441619024\n",
      "Average test loss: 0.0019036207913110652\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017524169165227147\n",
      "Average test loss: 0.0012917327924838497\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0173436412844393\n",
      "Average test loss: 0.0012619423211241761\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017419245843258168\n",
      "Average test loss: 0.0013112422701799208\n",
      "Epoch 74/300\n",
      "Average training loss: 0.017431099789010154\n",
      "Average test loss: 0.001252358880928821\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01718445072405868\n",
      "Average test loss: 0.0014820102510145969\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01714759853647815\n",
      "Average test loss: 0.0014802299489577611\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017137647294335895\n",
      "Average test loss: 0.0018786694622702068\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017072541693846383\n",
      "Average test loss: 0.0012742566195213132\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0170391217586067\n",
      "Average test loss: 0.0012661421766711606\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01710889583826065\n",
      "Average test loss: 0.0012942725934812593\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01706328061636951\n",
      "Average test loss: 0.06726096690901452\n",
      "Epoch 82/300\n",
      "Average training loss: 0.016906105923155945\n",
      "Average test loss: 0.0012554509215470817\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01679391333543592\n",
      "Average test loss: 0.0013279545600008633\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01682772460911009\n",
      "Average test loss: 0.00128454007094519\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01679571110672421\n",
      "Average test loss: 0.0012961400849744677\n",
      "Epoch 86/300\n",
      "Average training loss: 0.016724533390667703\n",
      "Average test loss: 0.0018280532701561848\n",
      "Epoch 87/300\n",
      "Average training loss: 0.016872893794543212\n",
      "Average test loss: 0.0012814576415758993\n",
      "Epoch 88/300\n",
      "Average training loss: 0.016911212681896156\n",
      "Average test loss: 0.0012732820457054509\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016771167109409967\n",
      "Average test loss: 0.0014567300998088385\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016769595452480846\n",
      "Average test loss: 0.0012530589536246326\n",
      "Epoch 91/300\n",
      "Average training loss: 0.016438360715077982\n",
      "Average test loss: 0.0012718792147934437\n",
      "Epoch 92/300\n",
      "Average training loss: 0.016467001972926987\n",
      "Average test loss: 0.0014101976559807857\n",
      "Epoch 93/300\n",
      "Average training loss: 0.016526689533558157\n",
      "Average test loss: 0.0012891723510498802\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01645077566554149\n",
      "Average test loss: 0.0013418193658192952\n",
      "Epoch 95/300\n",
      "Average training loss: 0.016398900183538595\n",
      "Average test loss: 0.001285836215544906\n",
      "Epoch 96/300\n",
      "Average training loss: 0.016464890660511124\n",
      "Average test loss: 0.01860076659462518\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016466105949547557\n",
      "Average test loss: 0.001293696769202749\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016292965341773297\n",
      "Average test loss: 0.0012798300071929893\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016363026993142234\n",
      "Average test loss: 0.24454602913475698\n",
      "Epoch 100/300\n",
      "Average training loss: 0.016296774926284948\n",
      "Average test loss: 0.0013018432091921569\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01629701450963815\n",
      "Average test loss: 0.001264299348100192\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01618253086010615\n",
      "Average test loss: 0.0013139579669675893\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016324900333252217\n",
      "Average test loss: 0.0012898856199656924\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016218869479166138\n",
      "Average test loss: 0.001341313813916511\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01627719989170631\n",
      "Average test loss: 0.0012788448604858585\n",
      "Epoch 106/300\n",
      "Average training loss: 0.016065778346525297\n",
      "Average test loss: 0.001306313541200426\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01607607164978981\n",
      "Average test loss: 0.0014225582640825045\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016065195625854862\n",
      "Average test loss: 0.0013320116864310371\n",
      "Epoch 109/300\n",
      "Average training loss: 0.016004040305813153\n",
      "Average test loss: 0.001306838367289553\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016074186060991553\n",
      "Average test loss: 0.001331786319354756\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01609757727964057\n",
      "Average test loss: 0.0012737920636104213\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015930921824855938\n",
      "Average test loss: 0.001313515125049485\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0159199202756087\n",
      "Average test loss: 0.00239737727596528\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015966490123834876\n",
      "Average test loss: 0.0012841602986575001\n",
      "Epoch 115/300\n",
      "Average training loss: 0.015945593814055126\n",
      "Average test loss: 0.0024338533945071203\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01584160330394904\n",
      "Average test loss: 0.04781435917814573\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015987331004606354\n",
      "Average test loss: 0.0017318028705194592\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015857629031770758\n",
      "Average test loss: 0.0016212310695813761\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01585305554419756\n",
      "Average test loss: 0.0013109606922500664\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01577151218139463\n",
      "Average test loss: 0.001302003304887977\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015827790777716372\n",
      "Average test loss: 0.0014096168162715105\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01576034616927306\n",
      "Average test loss: 0.0013524702313459582\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015740565543373424\n",
      "Average test loss: 0.0014663056851261192\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01575649524728457\n",
      "Average test loss: 0.0014494246447251903\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01569661498732037\n",
      "Average test loss: 0.0014195112260058523\n",
      "Epoch 126/300\n",
      "Average training loss: 0.015739556877977318\n",
      "Average test loss: 0.0019300709244691662\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01617326743652423\n",
      "Average test loss: 0.0013175461278814408\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01568916111522251\n",
      "Average test loss: 0.0012771130599495437\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015560329641732904\n",
      "Average test loss: 0.0013156736211126878\n",
      "Epoch 130/300\n",
      "Average training loss: 0.015574897482163376\n",
      "Average test loss: 0.001298968281182978\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015645525914927325\n",
      "Average test loss: 0.0013567211444282697\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01559601137952672\n",
      "Average test loss: 0.002043909449337257\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0155470889210701\n",
      "Average test loss: 0.0012995772665470012\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015507806003093719\n",
      "Average test loss: 0.0013410379292132953\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01554801866826084\n",
      "Average test loss: 0.001376615939868821\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015492806292242475\n",
      "Average test loss: 0.0030586883819972476\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015538388896319602\n",
      "Average test loss: 0.0013054040774392586\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015480061589843696\n",
      "Average test loss: 0.001311789758089516\n",
      "Epoch 139/300\n",
      "Average training loss: 0.015501336562964652\n",
      "Average test loss: 0.0012978376963486275\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01546131601350175\n",
      "Average test loss: 0.0013419415588594146\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015466880131098959\n",
      "Average test loss: 0.002679674025107589\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015432998629079925\n",
      "Average test loss: 0.0012922299501693082\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01542466377052996\n",
      "Average test loss: 0.0013523771942903597\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01532522387471464\n",
      "Average test loss: 0.0012864095969125629\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01548663681662745\n",
      "Average test loss: 0.0013315388542703456\n",
      "Epoch 146/300\n",
      "Average training loss: 0.015373462175329526\n",
      "Average test loss: 0.0013137418768472142\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015595262244343757\n",
      "Average test loss: 0.0012848257114075952\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015220255351728863\n",
      "Average test loss: 0.001323691270624598\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015283019088208675\n",
      "Average test loss: 0.0013957995283934805\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015419489300085439\n",
      "Average test loss: 0.001346030675340444\n",
      "Epoch 151/300\n",
      "Average training loss: 0.015262942196594345\n",
      "Average test loss: 0.0013598299246902266\n",
      "Epoch 152/300\n",
      "Average training loss: 0.015279993577135933\n",
      "Average test loss: 0.0013131122096545166\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015278283814589183\n",
      "Average test loss: 0.0013125348180118535\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015271218709647656\n",
      "Average test loss: 0.0013217105491914684\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015213893103102843\n",
      "Average test loss: 0.36334794824322064\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01525909742878543\n",
      "Average test loss: 0.0015545963608763285\n",
      "Epoch 157/300\n",
      "Average training loss: 0.015221717742582162\n",
      "Average test loss: 0.0013438191105508142\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01524507436901331\n",
      "Average test loss: 0.001317371535839306\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015158876395059957\n",
      "Average test loss: 0.0013061070683826174\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015193218268454075\n",
      "Average test loss: 0.0013084606263372633\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015188438079423374\n",
      "Average test loss: 0.00133526350516412\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015315053034987714\n",
      "Average test loss: 0.00134075483886732\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015149939500623278\n",
      "Average test loss: 516.6106236707899\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019568603388137287\n",
      "Average test loss: 0.0012834257214951019\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015168983075353835\n",
      "Average test loss: 0.001306341859822472\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015055208787322045\n",
      "Average test loss: 0.001371770181796617\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015003911120196183\n",
      "Average test loss: 0.0013611289040806392\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015023713374303447\n",
      "Average test loss: 0.001350203018428551\n",
      "Epoch 169/300\n",
      "Average training loss: 0.015139125391840935\n",
      "Average test loss: 0.0013443552414990133\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015053027462628153\n",
      "Average test loss: 0.0030116711789742114\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015065546162426472\n",
      "Average test loss: 0.0015828869700845744\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015116526071396139\n",
      "Average test loss: 0.0013441630782973434\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01505093984471427\n",
      "Average test loss: 0.0013877362074951331\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015048571882148584\n",
      "Average test loss: 0.0021646188038090867\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015082004523939556\n",
      "Average test loss: 0.001348769532309638\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015121329504582616\n",
      "Average test loss: 0.001331299568216006\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015091741850806608\n",
      "Average test loss: 0.0013476086841482256\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015016478487186962\n",
      "Average test loss: 0.0013850805339817372\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014956131252149741\n",
      "Average test loss: 0.0013127468603973587\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014973665982484818\n",
      "Average test loss: 0.0013202208946976397\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015028293677502209\n",
      "Average test loss: 0.0017249739896506072\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015042101325260269\n",
      "Average test loss: 0.0013330484728018442\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015027085276113615\n",
      "Average test loss: 0.0013578560210557448\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014889434592591391\n",
      "Average test loss: 0.0013644825356184608\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015342344709568554\n",
      "Average test loss: 0.0016201688584147227\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015104976284007232\n",
      "Average test loss: 0.0013507650670491987\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01488878788219558\n",
      "Average test loss: 0.0016454481623238988\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015018272346092595\n",
      "Average test loss: 0.0019138948855300744\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01492741747200489\n",
      "Average test loss: 0.0013178659333433543\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014821517112354438\n",
      "Average test loss: 0.0013484987393021584\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01484902799460623\n",
      "Average test loss: 0.007668450192444855\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015020557112163968\n",
      "Average test loss: 0.014496679400404295\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014890212983720832\n",
      "Average test loss: 0.0013092004322550364\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014822339855134488\n",
      "Average test loss: 0.0013799456428322527\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014956112719244427\n",
      "Average test loss: 0.0014404690003850394\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014859823911554283\n",
      "Average test loss: 0.001392581193190482\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014856855997608767\n",
      "Average test loss: 0.0013432530586918196\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014859743336836498\n",
      "Average test loss: 0.0016387469979623954\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014787281839383973\n",
      "Average test loss: 0.0015308199087157846\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014844310520423783\n",
      "Average test loss: 0.0015099247566734751\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014838246245351103\n",
      "Average test loss: 0.0013435019069454736\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014781724558108382\n",
      "Average test loss: 0.0013453536356488863\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014806905527909597\n",
      "Average test loss: 0.0013800897639658716\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014771598276164796\n",
      "Average test loss: 0.0014261416385674642\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01473314007702801\n",
      "Average test loss: 0.001347818210410575\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014811352109743488\n",
      "Average test loss: 0.0013585802668498622\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014819019827577802\n",
      "Average test loss: 0.001482426905590627\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014745822153157659\n",
      "Average test loss: 0.0015591847892954117\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01478850694414642\n",
      "Average test loss: 0.0013615626227110624\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014682795657051934\n",
      "Average test loss: 0.0013649790939978428\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016111319335798423\n",
      "Average test loss: 0.0019312728150851198\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016945864358709917\n",
      "Average test loss: 0.001356345739008652\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014887867596414354\n",
      "Average test loss: 0.0013641466192073292\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014884169681204689\n",
      "Average test loss: 0.01452267971676257\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0146710673885213\n",
      "Average test loss: 0.0013547554071992635\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014662729320426782\n",
      "Average test loss: 0.0014075673169249461\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014669541768729687\n",
      "Average test loss: 0.001358788398353176\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014754417780372831\n",
      "Average test loss: 0.0013828993178386655\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014711246486339305\n",
      "Average test loss: 0.0013940043213466803\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014661019421286054\n",
      "Average test loss: 0.0013249025388310353\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014781141029463873\n",
      "Average test loss: 0.0029900761428806517\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014817602689067523\n",
      "Average test loss: 0.0013445669592668612\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014575083185401228\n",
      "Average test loss: 0.0013848700151882238\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014652948992119896\n",
      "Average test loss: 0.0014822979465954835\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01468173880626758\n",
      "Average test loss: 0.0013668904357279341\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014692254083024131\n",
      "Average test loss: 0.0013553638781110447\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014591320482393106\n",
      "Average test loss: 0.0013646897165001267\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014588123153481219\n",
      "Average test loss: 0.0014666869156062603\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01462633736514383\n",
      "Average test loss: 0.001531295316086875\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014593607747720347\n",
      "Average test loss: 0.0014858970853189627\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014545429685049587\n",
      "Average test loss: 0.0014038879293948412\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01469388355811437\n",
      "Average test loss: 0.0013812589161098004\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014551887337532309\n",
      "Average test loss: 0.0013993523196420736\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014682119498650233\n",
      "Average test loss: 0.001401460982962615\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014528653384082848\n",
      "Average test loss: 0.0013532983604818583\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014544968347582551\n",
      "Average test loss: 0.0014016856937151816\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014595368168420261\n",
      "Average test loss: 0.0013865872725533942\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014541554109917747\n",
      "Average test loss: 0.0013231961178696817\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01452684320592218\n",
      "Average test loss: 0.0013542499467730522\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014666953684555159\n",
      "Average test loss: 0.0018067639538397393\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014502798017528322\n",
      "Average test loss: 0.001362577970346643\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014520886248598496\n",
      "Average test loss: 0.0014127776769196822\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014536110308435228\n",
      "Average test loss: 0.0013753156697688003\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014519962585634656\n",
      "Average test loss: 0.0013549518044003181\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014561273014379872\n",
      "Average test loss: 0.001416555326225029\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014501905388302273\n",
      "Average test loss: 0.001348034876657443\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014467682673699326\n",
      "Average test loss: 0.0014591050648854837\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014933043787876766\n",
      "Average test loss: 0.0026401781959252226\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014400478678445022\n",
      "Average test loss: 0.0013428971980594926\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014422624449763033\n",
      "Average test loss: 0.0013543245699256658\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014619615756803088\n",
      "Average test loss: 0.0014521975600057177\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014411622033350998\n",
      "Average test loss: 0.00133512947599714\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014413213990628719\n",
      "Average test loss: 0.0013442391032973925\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014438580233189795\n",
      "Average test loss: 0.0013834615831987726\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014674836716718143\n",
      "Average test loss: 0.0013621278502460982\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014413590259022183\n",
      "Average test loss: 0.0014080021959832973\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014406188686688741\n",
      "Average test loss: 0.001473636403783328\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0143619712781575\n",
      "Average test loss: 0.001372326430761152\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014411313883960247\n",
      "Average test loss: 0.0013812112966552377\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014553768083453178\n",
      "Average test loss: 0.0013312755620314015\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014372493526173962\n",
      "Average test loss: 0.001456701239777936\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014437929237882296\n",
      "Average test loss: 0.0014202183012643622\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014411832697689534\n",
      "Average test loss: 0.0013711967232326666\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014339636872212092\n",
      "Average test loss: 0.005074909735884931\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014427838986118635\n",
      "Average test loss: 0.0014232892798156373\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01439717719041639\n",
      "Average test loss: 0.0015279377623357707\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01434577166206307\n",
      "Average test loss: 0.0013710859338235523\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014395145331819853\n",
      "Average test loss: 0.0014666245195807683\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014418857870002588\n",
      "Average test loss: 0.001369968337420788\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01433174920330445\n",
      "Average test loss: 0.0013698626462784079\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014331154192487398\n",
      "Average test loss: 0.0017977686806892356\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0143264240241713\n",
      "Average test loss: 0.0022196588481052056\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014589259058237075\n",
      "Average test loss: 0.0014267310098641448\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014339765868253179\n",
      "Average test loss: 0.0013746181986191207\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014537879308064779\n",
      "Average test loss: 0.0014710007874915998\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014295777897040049\n",
      "Average test loss: 0.0017221853186686834\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01426501099103027\n",
      "Average test loss: 0.0014590815659612417\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014347578040427631\n",
      "Average test loss: 0.0013888916769582364\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01434422073347701\n",
      "Average test loss: 0.0015407968817485703\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014282027442422178\n",
      "Average test loss: 0.0013592589608290128\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014322772501243485\n",
      "Average test loss: 0.0013585897975394296\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014387333432833353\n",
      "Average test loss: 0.0013654209699274764\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014274740780393282\n",
      "Average test loss: 0.002623843096041431\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014330279679761993\n",
      "Average test loss: 0.0013706875491059489\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014253684063752492\n",
      "Average test loss: 0.0013824115955374306\n",
      "Epoch 286/300\n",
      "Average training loss: 0.014256391079061561\n",
      "Average test loss: 0.0016259219861692853\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014247554802232318\n",
      "Average test loss: 0.001356758896364934\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014452104552752441\n",
      "Average test loss: 0.0013861003989974658\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014282302771177556\n",
      "Average test loss: 0.0013667730866000055\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014263809491362836\n",
      "Average test loss: 0.0014259539703942007\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014205727304021518\n",
      "Average test loss: 0.0017675557606336143\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014286322750979\n",
      "Average test loss: 0.0014499664888199832\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014233543876144622\n",
      "Average test loss: 0.001387042367261731\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014256315430833234\n",
      "Average test loss: 0.0013945155460387468\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014204452882210413\n",
      "Average test loss: 0.001421039197128266\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014300918260382281\n",
      "Average test loss: 0.001391032442657484\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014217127856281068\n",
      "Average test loss: 0.0013867803296177751\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014503289351032839\n",
      "Average test loss: 0.0013799081517176495\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014187910573350058\n",
      "Average test loss: 0.0013979801634947458\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014209701023168033\n",
      "Average test loss: 0.0014382527699280116\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.95/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 20.94\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.65\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.25\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.63\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.98\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.46\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.14\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.56\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.04\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.23\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.45\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.98\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.57\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.69\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.78\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.64\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.09\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.51\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.56\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.72\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.76\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.66\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.30\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.82\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.13\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.31\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.65\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.80\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 34.10\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.29\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.241038898468018\n",
      "Average test loss: 0.012649842966761854\n",
      "Epoch 2/300\n",
      "Average training loss: 3.111465766482883\n",
      "Average test loss: 0.010525324526760313\n",
      "Epoch 3/300\n",
      "Average training loss: 2.0874420726564193\n",
      "Average test loss: 0.009288394568694963\n",
      "Epoch 4/300\n",
      "Average training loss: 1.640495528539022\n",
      "Average test loss: 0.012621214129030705\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2406745145585802\n",
      "Average test loss: 0.009478633851640755\n",
      "Epoch 6/300\n",
      "Average training loss: 1.0609697233835855\n",
      "Average test loss: 0.008060714396337668\n",
      "Epoch 7/300\n",
      "Average training loss: 0.9229660255114237\n",
      "Average test loss: 0.008758121523592207\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7725803299480014\n",
      "Average test loss: 0.00759229829783241\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6221109498871698\n",
      "Average test loss: 0.008291088712298209\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5073970946735806\n",
      "Average test loss: 0.007405626239875953\n",
      "Epoch 11/300\n",
      "Average training loss: 0.42425492056210834\n",
      "Average test loss: 0.007283552621801694\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3620675363805559\n",
      "Average test loss: 0.006809723537829188\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3098001736005147\n",
      "Average test loss: 0.0071741142893830935\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2720405189461178\n",
      "Average test loss: 0.007136337228119374\n",
      "Epoch 15/300\n",
      "Average training loss: 0.24135937596691978\n",
      "Average test loss: 0.007524236669970884\n",
      "Epoch 16/300\n",
      "Average training loss: 0.21783209726545547\n",
      "Average test loss: 0.00805640137526724\n",
      "Epoch 17/300\n",
      "Average training loss: 0.19993382722801634\n",
      "Average test loss: 0.006365250003420644\n",
      "Epoch 18/300\n",
      "Average training loss: 0.18644352614879608\n",
      "Average test loss: 0.0065118632569081255\n",
      "Epoch 19/300\n",
      "Average training loss: 0.17341496472888523\n",
      "Average test loss: 0.006275463772316774\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1632387801276313\n",
      "Average test loss: 0.00590937071873082\n",
      "Epoch 21/300\n",
      "Average training loss: 0.15566230800416733\n",
      "Average test loss: 0.005959250681930118\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1478207008573744\n",
      "Average test loss: 0.005896536410682731\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1405445231993993\n",
      "Average test loss: 0.006144821549662285\n",
      "Epoch 24/300\n",
      "Average training loss: 0.13618306057982973\n",
      "Average test loss: 0.005874598162869613\n",
      "Epoch 25/300\n",
      "Average training loss: 0.13061414453718398\n",
      "Average test loss: 0.0060833719066447686\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12667895152833727\n",
      "Average test loss: 0.006209425287528171\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12207766686545478\n",
      "Average test loss: 0.005714970449192657\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11862398205863105\n",
      "Average test loss: 0.007021803275992473\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11551819366216659\n",
      "Average test loss: 0.005703783045212428\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11329426938295364\n",
      "Average test loss: 0.005544036772102118\n",
      "Epoch 31/300\n",
      "Average training loss: 0.11031084597773022\n",
      "Average test loss: 0.005847608285231723\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10849066227674484\n",
      "Average test loss: 0.0054650208685133195\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10601935850249397\n",
      "Average test loss: 0.005373848201500045\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10349574901660283\n",
      "Average test loss: 0.10568167622884114\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10119959951109356\n",
      "Average test loss: 0.005274687125037114\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10052714971701304\n",
      "Average test loss: 0.005274728218300475\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09870215343766742\n",
      "Average test loss: 0.0058125690370798115\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09676694728268517\n",
      "Average test loss: 0.005531095060623354\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0954137789607048\n",
      "Average test loss: 0.005324727861003744\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09545952139960395\n",
      "Average test loss: 0.005282872607724534\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0933145835465855\n",
      "Average test loss: 0.026571419220831657\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09290604639715619\n",
      "Average test loss: 0.005209437087385191\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09144774036275016\n",
      "Average test loss: 0.005346940227266815\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0904881973332829\n",
      "Average test loss: 0.013217068736751875\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09041718623373243\n",
      "Average test loss: 0.017340832829475402\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08876783377594417\n",
      "Average test loss: 0.005120799908207523\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08859807475407919\n",
      "Average test loss: 0.0052092884907292\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09147184987862905\n",
      "Average test loss: 0.005139547301663293\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08711189412408404\n",
      "Average test loss: 14.896362242804633\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08643506616685126\n",
      "Average test loss: 0.005123367079844078\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08596131002902985\n",
      "Average test loss: 0.005050518352538347\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08508731895022922\n",
      "Average test loss: 0.023045989957534603\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08462889303763707\n",
      "Average test loss: 0.0054767143842246796\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08452599183056089\n",
      "Average test loss: 0.008606568915976418\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08454746087392172\n",
      "Average test loss: 0.006767890732321475\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08306679926978217\n",
      "Average test loss: 0.0143432375697626\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08236176393098302\n",
      "Average test loss: 0.00498593625633253\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08234675790203942\n",
      "Average test loss: 0.006330237522721291\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08167911537488301\n",
      "Average test loss: 0.005005968003637261\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08109970477554533\n",
      "Average test loss: 0.005075301753357053\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08080673110485077\n",
      "Average test loss: 0.00508007042730848\n",
      "Epoch 62/300\n",
      "Average training loss: 0.079983451899555\n",
      "Average test loss: 0.00505679895190729\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08653848702377744\n",
      "Average test loss: 0.005311211198568344\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07968232154846192\n",
      "Average test loss: 0.005056125291933616\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07889034838146633\n",
      "Average test loss: 0.005861532407502333\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0785484773947133\n",
      "Average test loss: 0.005081235024250216\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0784120917585161\n",
      "Average test loss: 0.00521955797324578\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07847344460090001\n",
      "Average test loss: 0.005102847935424911\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07791946318745613\n",
      "Average test loss: 0.005815343703246779\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07705843175119824\n",
      "Average test loss: 0.0055031683974795876\n",
      "Epoch 71/300\n",
      "Average training loss: 1.2901614521609412\n",
      "Average test loss: 0.009308000269035499\n",
      "Epoch 72/300\n",
      "Average training loss: 0.41984496053059894\n",
      "Average test loss: 0.1184539765069882\n",
      "Epoch 73/300\n",
      "Average training loss: 0.29074741678767735\n",
      "Average test loss: 0.11431643447114362\n",
      "Epoch 74/300\n",
      "Average training loss: 0.2333734867705239\n",
      "Average test loss: 0.0054364739223900765\n",
      "Epoch 75/300\n",
      "Average training loss: 0.19472324250804054\n",
      "Average test loss: 0.005423242806974385\n",
      "Epoch 76/300\n",
      "Average training loss: 0.16604307121700712\n",
      "Average test loss: 0.006447066803980205\n",
      "Epoch 77/300\n",
      "Average training loss: 0.14315274326006572\n",
      "Average test loss: 4.669964523060454\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1251229557858573\n",
      "Average test loss: 0.005699377385278543\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11095473952425851\n",
      "Average test loss: 176.73162396861116\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10526443449656168\n",
      "Average test loss: 0.007077224823128846\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10104201573133469\n",
      "Average test loss: 0.022140495201779735\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0978518809609943\n",
      "Average test loss: 0.006134580394460095\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0951769385933876\n",
      "Average test loss: 0.008210286344091098\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09301498177978727\n",
      "Average test loss: 0.005191900023983584\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09096175783872604\n",
      "Average test loss: 0.005337874099612236\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08907667626937231\n",
      "Average test loss: 0.005050263134969605\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08700086041953829\n",
      "Average test loss: 0.0050945825189765955\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08528871758116616\n",
      "Average test loss: 0.005599534751226504\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08442648471064038\n",
      "Average test loss: 0.005407881778561407\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08402378377318383\n",
      "Average test loss: 0.005059460527780983\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08181636024183697\n",
      "Average test loss: 0.01837072784577807\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08077308112382889\n",
      "Average test loss: 0.0053371749723123176\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0815210154917505\n",
      "Average test loss: 0.005168280641237894\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07909084788295957\n",
      "Average test loss: 0.005129701492687066\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07832790362834931\n",
      "Average test loss: 77811.65512760417\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07806909376382828\n",
      "Average test loss: 0.005117674707538552\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07830199212498135\n",
      "Average test loss: 0.0051005191391126975\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07733291824327575\n",
      "Average test loss: 0.0050401341699891624\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07695534725983938\n",
      "Average test loss: 0.00555648624110553\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07768143006828096\n",
      "Average test loss: 0.0052629018171379965\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07632276558213764\n",
      "Average test loss: 0.18425811674031947\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07632467263274723\n",
      "Average test loss: 0.005044885288510058\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07587438789341185\n",
      "Average test loss: 0.007907308716740873\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07578747544686\n",
      "Average test loss: 31.252834647284615\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07617292192247178\n",
      "Average test loss: 0.00528846242858304\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07490984804762735\n",
      "Average test loss: 0.005197986458324724\n",
      "Epoch 107/300\n",
      "Average training loss: 0.075690220952034\n",
      "Average test loss: 0.011256452586087916\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07390549416674508\n",
      "Average test loss: 0.005232402229888571\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07379875227146679\n",
      "Average test loss: 0.005247007966041565\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07406777328252792\n",
      "Average test loss: 0.014752903609226148\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07648286920785904\n",
      "Average test loss: 0.005134300753060314\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07361823369397058\n",
      "Average test loss: 0.009619795743376017\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07257210385799408\n",
      "Average test loss: 0.0053884899309939805\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07282724304331674\n",
      "Average test loss: 0.006300294273015526\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07571523666381835\n",
      "Average test loss: 0.005558188492018315\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07235248919990328\n",
      "Average test loss: 0.005106327464183172\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07158445028464\n",
      "Average test loss: 0.005450003587537342\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07253860901461708\n",
      "Average test loss: 0.005299611924423112\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07175080960988998\n",
      "Average test loss: 0.005894505039271381\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07225473597314623\n",
      "Average test loss: 0.005172907473312484\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07094584716359774\n",
      "Average test loss: 0.005149848535656929\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07076658270756403\n",
      "Average test loss: 0.005385445110499859\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07139106123977237\n",
      "Average test loss: 0.005096767242170042\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07066299236483044\n",
      "Average test loss: 0.006060528183976809\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07084862756066852\n",
      "Average test loss: 0.005561021643380324\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07220600697729322\n",
      "Average test loss: 0.0052084341798391606\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06989994646443261\n",
      "Average test loss: 0.0051789207785493795\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0724198772708575\n",
      "Average test loss: 0.00511324253719714\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06900239878892898\n",
      "Average test loss: 0.0052469866875973015\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06923335922757784\n",
      "Average test loss: 0.007601515234758456\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06914563077688217\n",
      "Average test loss: 0.00532513311629494\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06957346983088387\n",
      "Average test loss: 0.019234211698174478\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06889471015665266\n",
      "Average test loss: 0.005154093609915839\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0687365121907658\n",
      "Average test loss: 0.00533925876683659\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07057939696974225\n",
      "Average test loss: 0.005332396366529994\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07058995258145861\n",
      "Average test loss: 0.0050084673691954875\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0787828708158599\n",
      "Average test loss: 0.005135784088737435\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07006272485521105\n",
      "Average test loss: 0.005076766008304225\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06804949430624643\n",
      "Average test loss: 0.007043387124935786\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06763521858387524\n",
      "Average test loss: 0.005609433480021026\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06800451516111691\n",
      "Average test loss: 0.005690237681070963\n",
      "Epoch 142/300\n",
      "Average training loss: 0.08635407880942027\n",
      "Average test loss: 0.005167512515766753\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0815093785127004\n",
      "Average test loss: 0.0050702527633143795\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07381852857934104\n",
      "Average test loss: 0.005799413058078951\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07105745429462856\n",
      "Average test loss: 0.0056038813421295745\n",
      "Epoch 146/300\n",
      "Average training loss: 253.8185762898922\n",
      "Average test loss: 8.18027307899793\n",
      "Epoch 147/300\n",
      "Average training loss: 8.16829382748074\n",
      "Average test loss: 0.0363632600373692\n",
      "Epoch 148/300\n",
      "Average training loss: 6.686650383843316\n",
      "Average test loss: 0.010536990259256628\n",
      "Epoch 149/300\n",
      "Average training loss: 5.787815210554335\n",
      "Average test loss: 0.009278146158903837\n",
      "Epoch 150/300\n",
      "Average training loss: 5.074046451144748\n",
      "Average test loss: 0.008896576723290814\n",
      "Epoch 151/300\n",
      "Average training loss: 4.506882861243354\n",
      "Average test loss: 0.010459971224268278\n",
      "Epoch 152/300\n",
      "Average training loss: 4.022215662426419\n",
      "Average test loss: 0.013270268465909693\n",
      "Epoch 153/300\n",
      "Average training loss: 3.595171062257555\n",
      "Average test loss: 0.007660250508536895\n",
      "Epoch 154/300\n",
      "Average training loss: 3.2210031102498373\n",
      "Average test loss: 0.00739494459455212\n",
      "Epoch 155/300\n",
      "Average training loss: 2.8846936172909206\n",
      "Average test loss: 0.013135685533285141\n",
      "Epoch 156/300\n",
      "Average training loss: 2.6216493979560003\n",
      "Average test loss: 0.006543294494350751\n",
      "Epoch 157/300\n",
      "Average training loss: 2.4070887463887534\n",
      "Average test loss: 0.009077898326019447\n",
      "Epoch 158/300\n",
      "Average training loss: 2.21128895272149\n",
      "Average test loss: 0.009616085485451752\n",
      "Epoch 159/300\n",
      "Average training loss: 2.0301501410802207\n",
      "Average test loss: 0.006157951400511795\n",
      "Epoch 160/300\n",
      "Average training loss: 1.8635836279127334\n",
      "Average test loss: 0.007103045048813025\n",
      "Epoch 161/300\n",
      "Average training loss: 1.7065508705774943\n",
      "Average test loss: 0.008623274320943487\n",
      "Epoch 162/300\n",
      "Average training loss: 1.5528683038287692\n",
      "Average test loss: 0.006780891772773531\n",
      "Epoch 163/300\n",
      "Average training loss: 1.4031453959147135\n",
      "Average test loss: 0.021120580951372783\n",
      "Epoch 164/300\n",
      "Average training loss: 1.2515791088740031\n",
      "Average test loss: 0.005661089793675475\n",
      "Epoch 165/300\n",
      "Average training loss: 1.1109254322052002\n",
      "Average test loss: 0.005719997480511665\n",
      "Epoch 166/300\n",
      "Average training loss: 0.9564212015999688\n",
      "Average test loss: 0.005721041349073251\n",
      "Epoch 167/300\n",
      "Average training loss: 0.8114079389572143\n",
      "Average test loss: 0.00564652383637925\n",
      "Epoch 168/300\n",
      "Average training loss: 0.6692851786613464\n",
      "Average test loss: 0.0060491911706825096\n",
      "Epoch 169/300\n",
      "Average training loss: 0.5521516718334621\n",
      "Average test loss: 0.006189045135552684\n",
      "Epoch 170/300\n",
      "Average training loss: 0.47723793392711217\n",
      "Average test loss: 0.00613728752773669\n",
      "Epoch 171/300\n",
      "Average training loss: 0.37741915249824526\n",
      "Average test loss: 0.005649621888167328\n",
      "Epoch 172/300\n",
      "Average training loss: 0.31586873806847465\n",
      "Average test loss: 0.005347077104366488\n",
      "Epoch 173/300\n",
      "Average training loss: 0.2709851479795244\n",
      "Average test loss: 0.02844480279708902\n",
      "Epoch 174/300\n",
      "Average training loss: 0.23761903921763103\n",
      "Average test loss: 0.010282448235485289\n",
      "Epoch 175/300\n",
      "Average training loss: 0.2115175768799252\n",
      "Average test loss: 0.005176255647506979\n",
      "Epoch 176/300\n",
      "Average training loss: 0.18890365821785396\n",
      "Average test loss: 0.005386910941451788\n",
      "Epoch 177/300\n",
      "Average training loss: 0.17032307501633961\n",
      "Average test loss: 0.005358389475279384\n",
      "Epoch 178/300\n",
      "Average training loss: 0.1549192597468694\n",
      "Average test loss: 0.005102597331835164\n",
      "Epoch 179/300\n",
      "Average training loss: 0.14112764631377325\n",
      "Average test loss: 0.005138326805912786\n",
      "Epoch 180/300\n",
      "Average training loss: 0.13017859326468573\n",
      "Average test loss: 0.00509308094282945\n",
      "Epoch 181/300\n",
      "Average training loss: 0.12102040757073297\n",
      "Average test loss: 0.0057864696391754684\n",
      "Epoch 182/300\n",
      "Average training loss: 0.11344491493039661\n",
      "Average test loss: 0.005029658365994692\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10664743411540985\n",
      "Average test loss: 4.050016645219591\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10303829872608185\n",
      "Average test loss: 0.005108455092335741\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09802756409512625\n",
      "Average test loss: 0.005054880688587824\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0939655107723342\n",
      "Average test loss: 0.005026641178462241\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09111796344651116\n",
      "Average test loss: 0.00506342403507895\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08894170412752364\n",
      "Average test loss: 0.006319744529823462\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08633813665972816\n",
      "Average test loss: 0.028367137524816725\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08506291353040271\n",
      "Average test loss: 0.004964383309086164\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08289640209741063\n",
      "Average test loss: 0.005018674050354295\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08275403126080831\n",
      "Average test loss: 0.005191144918815957\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08029974923531215\n",
      "Average test loss: 0.005001359290960762\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07948536188734902\n",
      "Average test loss: 0.00522575335825483\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07794307931264241\n",
      "Average test loss: 0.00514081678705083\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07751730723513497\n",
      "Average test loss: 0.0051274607814848425\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07522163718938828\n",
      "Average test loss: 0.0050387306478288435\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0754566669927703\n",
      "Average test loss: 0.005160522805319892\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07218858950667911\n",
      "Average test loss: 0.006894342246154944\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07211364759339227\n",
      "Average test loss: 0.011752553610338105\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07057958176732063\n",
      "Average test loss: 0.005205829674584998\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07099281540513039\n",
      "Average test loss: 0.005910337258130312\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06930904922220442\n",
      "Average test loss: 0.0051283152440769805\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06906532486279805\n",
      "Average test loss: 0.00535482725335492\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07004127091169357\n",
      "Average test loss: 9.439095684876044\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06874632081720564\n",
      "Average test loss: 0.005246043799031112\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06842027760545413\n",
      "Average test loss: 0.005393440972599718\n",
      "Epoch 208/300\n",
      "Average training loss: 0.1143657355705897\n",
      "Average test loss: 0.005183259846849574\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08238845807976193\n",
      "Average test loss: 0.005015190820520122\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07701700799663862\n",
      "Average test loss: 0.005371484886440966\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07404363830222024\n",
      "Average test loss: 0.30616192355420857\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07147843764887915\n",
      "Average test loss: 0.012488455878363716\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06948105717698733\n",
      "Average test loss: 0.0052040640231635835\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06926919238434898\n",
      "Average test loss: 0.009176139928400516\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0676900906264782\n",
      "Average test loss: 0.013602155783110194\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06795877397391531\n",
      "Average test loss: 0.005447700123406118\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06727123829391267\n",
      "Average test loss: 0.0614574600789282\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06746631150775485\n",
      "Average test loss: 0.006813822440803051\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06726544709338082\n",
      "Average test loss: 0.005538524765935209\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06793920346101125\n",
      "Average test loss: 0.0051221328013473085\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06690542470084296\n",
      "Average test loss: 0.019664150747987958\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06694829039772351\n",
      "Average test loss: 0.02329591343055169\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06642700905601183\n",
      "Average test loss: 0.0057895895453790825\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07003420376777648\n",
      "Average test loss: 0.005991279922839668\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06588529531823265\n",
      "Average test loss: 0.40773388196362387\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06591791857613458\n",
      "Average test loss: 0.01079505133959982\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06657833600044251\n",
      "Average test loss: 0.0051648809235129095\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06569730162951681\n",
      "Average test loss: 0.005951236567977402\n",
      "Epoch 229/300\n",
      "Average training loss: 3912.0283929982384\n",
      "Average test loss: 554.0171442320016\n",
      "Epoch 230/300\n",
      "Average training loss: 6.901820843166775\n",
      "Average test loss: 72285.94829285648\n",
      "Epoch 231/300\n",
      "Average training loss: 6.124272985670301\n",
      "Average test loss: 260.1433365671204\n",
      "Epoch 232/300\n",
      "Average training loss: 5.469262829250759\n",
      "Average test loss: 178198.95708923653\n",
      "Epoch 233/300\n",
      "Average training loss: 4.912708468119304\n",
      "Average test loss: 60.65570569742057\n",
      "Epoch 234/300\n",
      "Average training loss: 4.457793782975939\n",
      "Average test loss: 0.05177546966572603\n",
      "Epoch 235/300\n",
      "Average training loss: 4.074243923611111\n",
      "Average test loss: 162.32044219013923\n",
      "Epoch 236/300\n",
      "Average training loss: 3.7760969083574083\n",
      "Average test loss: 717.0909128624896\n",
      "Epoch 237/300\n",
      "Average training loss: 3.479911610921224\n",
      "Average test loss: 0.010671969779663617\n",
      "Epoch 238/300\n",
      "Average training loss: 3.2046901940239803\n",
      "Average test loss: 0.006718089687741465\n",
      "Epoch 239/300\n",
      "Average training loss: 2.9436621873643665\n",
      "Average test loss: 7.214132300994049\n",
      "Epoch 240/300\n",
      "Average training loss: 2.700217860751682\n",
      "Average test loss: 0.006368625776221355\n",
      "Epoch 241/300\n",
      "Average training loss: 2.4775533091227215\n",
      "Average test loss: 33190.00397511122\n",
      "Epoch 242/300\n",
      "Average training loss: 2.2810020224253336\n",
      "Average test loss: 0.11014480087368024\n",
      "Epoch 243/300\n",
      "Average training loss: 2.10780483033922\n",
      "Average test loss: 0.027169157650735642\n",
      "Epoch 244/300\n",
      "Average training loss: 1.9570705869462754\n",
      "Average test loss: 0.005910810212708182\n",
      "Epoch 245/300\n",
      "Average training loss: 1.8124965059492324\n",
      "Average test loss: 21.446978333738528\n",
      "Epoch 246/300\n",
      "Average training loss: 1.6760747109519112\n",
      "Average test loss: 109.4687233308355\n",
      "Epoch 247/300\n",
      "Average training loss: 1.5389448939429389\n",
      "Average test loss: 0.013255676978164248\n",
      "Epoch 248/300\n",
      "Average training loss: 1.3962702119615342\n",
      "Average test loss: 6.802597681509538\n",
      "Epoch 249/300\n",
      "Average training loss: 1.2524733320871988\n",
      "Average test loss: 0.11609462311532762\n",
      "Epoch 250/300\n",
      "Average training loss: 1.1099453253216214\n",
      "Average test loss: 0.0055091553727785745\n",
      "Epoch 251/300\n",
      "Average training loss: 0.9798410284784105\n",
      "Average test loss: 0.005402355604494612\n",
      "Epoch 252/300\n",
      "Average training loss: 0.866309433195326\n",
      "Average test loss: 0.005542834678043922\n",
      "Epoch 253/300\n",
      "Average training loss: 0.7632142789098951\n",
      "Average test loss: 0.007982075538072321\n",
      "Epoch 254/300\n",
      "Average training loss: 0.6708387026786804\n",
      "Average test loss: 0.005314817537450128\n",
      "Epoch 255/300\n",
      "Average training loss: 0.58710776064131\n",
      "Average test loss: 0.005796252554696468\n",
      "Epoch 256/300\n",
      "Average training loss: 0.5139643443690406\n",
      "Average test loss: 0.0055517449374828075\n",
      "Epoch 257/300\n",
      "Average training loss: 0.4474748992390103\n",
      "Average test loss: 0.005765194469028049\n",
      "Epoch 258/300\n",
      "Average training loss: 0.3874104365242852\n",
      "Average test loss: 0.005196399075703489\n",
      "Epoch 259/300\n",
      "Average training loss: 0.3292724293337928\n",
      "Average test loss: 1.4709799630273548\n",
      "Epoch 260/300\n",
      "Average training loss: 0.27781392804781596\n",
      "Average test loss: 0.005309787882284985\n",
      "Epoch 261/300\n",
      "Average training loss: 0.2282425734731886\n",
      "Average test loss: 0.005360678700937165\n",
      "Epoch 262/300\n",
      "Average training loss: 0.18790146932337018\n",
      "Average test loss: 0.010614902738067838\n",
      "Epoch 263/300\n",
      "Average training loss: 0.1575125268432829\n",
      "Average test loss: 0.006444184494101339\n",
      "Epoch 264/300\n",
      "Average training loss: 0.13900454636414847\n",
      "Average test loss: 0.005387938393072949\n",
      "Epoch 265/300\n",
      "Average training loss: 0.12695449021789762\n",
      "Average test loss: 0.005090537888722288\n",
      "Epoch 266/300\n",
      "Average training loss: 0.17605734703938167\n",
      "Average test loss: 0.008694332806600464\n",
      "Epoch 267/300\n",
      "Average training loss: 0.12096355745527479\n",
      "Average test loss: 0.0161993773844507\n",
      "Epoch 268/300\n",
      "Average training loss: 0.10778906133439806\n",
      "Average test loss: 0.00577650255502926\n",
      "Epoch 269/300\n",
      "Average training loss: 0.10173974517318937\n",
      "Average test loss: 0.015422761453522576\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09709481369786792\n",
      "Average test loss: 0.0052141891966263456\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09363526399930318\n",
      "Average test loss: 0.005056749808705515\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09031965933243434\n",
      "Average test loss: 0.0050844572436892325\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0877375676035881\n",
      "Average test loss: 0.005064867006407844\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08625017559197214\n",
      "Average test loss: 0.005190328383197387\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08413072189688682\n",
      "Average test loss: 0.005110589985218313\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0825035007794698\n",
      "Average test loss: 0.005092615952301357\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08110030469629499\n",
      "Average test loss: 0.005054241981771257\n",
      "Epoch 278/300\n",
      "Average training loss: 0.10954641709725062\n",
      "Average test loss: 0.0050136805656883455\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08131384334961574\n",
      "Average test loss: 0.005010564815666941\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07832855786879857\n",
      "Average test loss: 0.005232348519480891\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07664135689867867\n",
      "Average test loss: 0.005295925551818477\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07520611621273889\n",
      "Average test loss: 0.005082570189403163\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07407345628738403\n",
      "Average test loss: 0.029423510018736124\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07425333526068263\n",
      "Average test loss: 0.005296196464449167\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07170574889249272\n",
      "Average test loss: 0.005136709394554297\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0706900527411037\n",
      "Average test loss: 0.1971112923853927\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07024842311938603\n",
      "Average test loss: 0.005336127687659529\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06959027732743157\n",
      "Average test loss: 0.005148424446996715\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06906002376145787\n",
      "Average test loss: 0.010407586771580908\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06915799630350537\n",
      "Average test loss: 0.01921162497997284\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0680711899863349\n",
      "Average test loss: 0.05907722229758899\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07108294841647148\n",
      "Average test loss: 0.005324149278716908\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06751257344749238\n",
      "Average test loss: 0.005242417386008633\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06716003688838747\n",
      "Average test loss: 0.005201248270769914\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06687135840124554\n",
      "Average test loss: 0.005316522772527403\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06754734402563836\n",
      "Average test loss: 0.005186284729176097\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06802765373720063\n",
      "Average test loss: 0.005177135285403993\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06597877617014779\n",
      "Average test loss: 0.005829437989327643\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07236602980560726\n",
      "Average test loss: 0.005813987168172995\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06755149115456476\n",
      "Average test loss: 0.005200971010658476\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7.461524402830336\n",
      "Average test loss: 0.12332175039582782\n",
      "Epoch 2/300\n",
      "Average training loss: 2.266946799490187\n",
      "Average test loss: 0.007048188363098436\n",
      "Epoch 3/300\n",
      "Average training loss: 1.5422870398627386\n",
      "Average test loss: 0.005863354854285717\n",
      "Epoch 4/300\n",
      "Average training loss: 1.2213174878226387\n",
      "Average test loss: 0.00712855039909482\n",
      "Epoch 5/300\n",
      "Average training loss: 0.9322611861758762\n",
      "Average test loss: 0.0052881156048840946\n",
      "Epoch 6/300\n",
      "Average training loss: 0.7937334607972039\n",
      "Average test loss: 0.005334849539109402\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6272777523464627\n",
      "Average test loss: 0.005343894843839936\n",
      "Epoch 8/300\n",
      "Average training loss: 0.5170996480517918\n",
      "Average test loss: 0.004880850909070836\n",
      "Epoch 9/300\n",
      "Average training loss: 0.41186052407158746\n",
      "Average test loss: 0.004591482248778145\n",
      "Epoch 10/300\n",
      "Average training loss: 0.3287532911565569\n",
      "Average test loss: 0.004421153494467338\n",
      "Epoch 11/300\n",
      "Average training loss: 0.272031602329678\n",
      "Average test loss: 0.004458529061741299\n",
      "Epoch 12/300\n",
      "Average training loss: 0.22949090569549135\n",
      "Average test loss: 0.004120058355646001\n",
      "Epoch 13/300\n",
      "Average training loss: 0.19895612688859304\n",
      "Average test loss: 0.003951702971839242\n",
      "Epoch 14/300\n",
      "Average training loss: 0.17569433416260613\n",
      "Average test loss: 0.00396723304895891\n",
      "Epoch 15/300\n",
      "Average training loss: 0.15726484031147428\n",
      "Average test loss: 0.0038991728340172106\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1440718513197369\n",
      "Average test loss: 0.003966627265844081\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1308133181201087\n",
      "Average test loss: 0.00384457876574662\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12054491134484609\n",
      "Average test loss: 0.0037518286286956732\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11154246118333605\n",
      "Average test loss: 0.00429420332196686\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10461882983313667\n",
      "Average test loss: 0.003541633040540748\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09842789338032405\n",
      "Average test loss: 0.0034258582877616088\n",
      "Epoch 22/300\n",
      "Average training loss: 0.09323273114363352\n",
      "Average test loss: 0.0033292077998113303\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0878327001200782\n",
      "Average test loss: 0.00357616867373387\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08545491232474645\n",
      "Average test loss: 0.003374276371051868\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08144170065058602\n",
      "Average test loss: 0.0032342167095177703\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07892399470673667\n",
      "Average test loss: 0.003372255825334125\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07571821214093102\n",
      "Average test loss: 0.0035192692014906143\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07358020591073566\n",
      "Average test loss: 0.0031507300873183544\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07107809247573217\n",
      "Average test loss: 0.003144670637531413\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06933439081907272\n",
      "Average test loss: 0.003328775118208594\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06800773856374953\n",
      "Average test loss: 0.003599718263372779\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06569841086202198\n",
      "Average test loss: 0.0031830030975656376\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06431367419825659\n",
      "Average test loss: 0.0030739945384363333\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06291439350446065\n",
      "Average test loss: 0.003043025037480725\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06183293278018633\n",
      "Average test loss: 0.0030490307913472255\n",
      "Epoch 36/300\n",
      "Average training loss: 0.060145606997940276\n",
      "Average test loss: 0.0030074726295553976\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05918633753392431\n",
      "Average test loss: 0.003141696845077806\n",
      "Epoch 38/300\n",
      "Average training loss: 0.058008856399191754\n",
      "Average test loss: 0.00321350831248694\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05671951331363784\n",
      "Average test loss: 0.0029900621482067636\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05560854058464368\n",
      "Average test loss: 0.002958486112455527\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05468041830923822\n",
      "Average test loss: 0.00400308871227834\n",
      "Epoch 42/300\n",
      "Average training loss: 0.055985109160343806\n",
      "Average test loss: 0.003221458355585734\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05340240461958779\n",
      "Average test loss: 0.30203887410130764\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05248176289929284\n",
      "Average test loss: 0.003271451063868072\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05227600176797973\n",
      "Average test loss: 0.0028832577340718772\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05128409025404188\n",
      "Average test loss: 0.0030084972529568605\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05088760666383638\n",
      "Average test loss: 0.003599659312930372\n",
      "Epoch 48/300\n",
      "Average training loss: 0.051153610597054165\n",
      "Average test loss: 0.0029662300993998844\n",
      "Epoch 49/300\n",
      "Average training loss: 1.0974087688558631\n",
      "Average test loss: 2.6703078879233866\n",
      "Epoch 50/300\n",
      "Average training loss: 0.3413444912963443\n",
      "Average test loss: 0.9116638071040313\n",
      "Epoch 51/300\n",
      "Average training loss: 0.21017115132013955\n",
      "Average test loss: 43.41392472572956\n",
      "Epoch 52/300\n",
      "Average training loss: 0.15332212890519037\n",
      "Average test loss: 1.9321902417519854\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12104523062705994\n",
      "Average test loss: 1.8748964267522097\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09995065383778678\n",
      "Average test loss: 5.57233067400588\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08761595335933897\n",
      "Average test loss: 0.0031830554082989695\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07819538927740521\n",
      "Average test loss: 0.0032701521207475\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07145170194572872\n",
      "Average test loss: 0.0030957420907086796\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06778449288341734\n",
      "Average test loss: 0.0031617152134163513\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06499846262070867\n",
      "Average test loss: 0.0030989832664943405\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06300282827681965\n",
      "Average test loss: 0.0029684395011928344\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06129559780160586\n",
      "Average test loss: 0.002949701411649585\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05973869710498386\n",
      "Average test loss: 0.003226200055744913\n",
      "Epoch 63/300\n",
      "Average training loss: 0.058551768865850236\n",
      "Average test loss: 0.0029969455419729153\n",
      "Epoch 64/300\n",
      "Average training loss: 0.057348081595367854\n",
      "Average test loss: 0.0029205350155631703\n",
      "Epoch 65/300\n",
      "Average training loss: 0.056236562344763014\n",
      "Average test loss: 0.0029942408696644837\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05571330607930819\n",
      "Average test loss: 0.006118996242268218\n",
      "Epoch 67/300\n",
      "Average training loss: 0.054437614695893394\n",
      "Average test loss: 0.0028755662486784988\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0539443664037519\n",
      "Average test loss: 0.0032475145065950024\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05281921565201547\n",
      "Average test loss: 0.002930422104688154\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05225463768839836\n",
      "Average test loss: 0.0029727213143681485\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05224423666795095\n",
      "Average test loss: 0.004735175305977463\n",
      "Epoch 72/300\n",
      "Average training loss: 0.050952341990338434\n",
      "Average test loss: 0.002837011985687746\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05917110905713505\n",
      "Average test loss: 0.0029348433481322395\n",
      "Epoch 74/300\n",
      "Average training loss: 78.78243873224656\n",
      "Average test loss: 24379.640600504557\n",
      "Epoch 75/300\n",
      "Average training loss: 5.512511550479465\n",
      "Average test loss: 0.05746925452724099\n",
      "Epoch 76/300\n",
      "Average training loss: 3.6666849835713706\n",
      "Average test loss: 7.100012896019551\n",
      "Epoch 77/300\n",
      "Average training loss: 3.0340022604200576\n",
      "Average test loss: 0.018821927039987512\n",
      "Epoch 78/300\n",
      "Average training loss: 2.6316164162953695\n",
      "Average test loss: 0.004364114097423023\n",
      "Epoch 79/300\n",
      "Average training loss: 2.3203605183495415\n",
      "Average test loss: 0.0050797692268259\n",
      "Epoch 80/300\n",
      "Average training loss: 2.0332027778625488\n",
      "Average test loss: 0.024840686104156906\n",
      "Epoch 81/300\n",
      "Average training loss: 1.7660100251303779\n",
      "Average test loss: 0.03926578138396144\n",
      "Epoch 82/300\n",
      "Average training loss: 1.541720320807563\n",
      "Average test loss: 0.010108857850233714\n",
      "Epoch 83/300\n",
      "Average training loss: 1.3449178811179268\n",
      "Average test loss: 0.008132350502742662\n",
      "Epoch 84/300\n",
      "Average training loss: 1.1462565998501248\n",
      "Average test loss: 0.003756085405954056\n",
      "Epoch 85/300\n",
      "Average training loss: 0.9794628637101915\n",
      "Average test loss: 0.0035199246779084206\n",
      "Epoch 86/300\n",
      "Average training loss: 0.8351304522620308\n",
      "Average test loss: 0.007805116959330108\n",
      "Epoch 87/300\n",
      "Average training loss: 0.7073361746999952\n",
      "Average test loss: 0.0036638396868689193\n",
      "Epoch 88/300\n",
      "Average training loss: 0.6039533486366272\n",
      "Average test loss: 0.003320427896661891\n",
      "Epoch 89/300\n",
      "Average training loss: 0.511318934334649\n",
      "Average test loss: 0.004260638302606013\n",
      "Epoch 90/300\n",
      "Average training loss: 0.4169973536597358\n",
      "Average test loss: 0.005019807579616706\n",
      "Epoch 91/300\n",
      "Average training loss: 0.3432033350467682\n",
      "Average test loss: 0.0032555252446068656\n",
      "Epoch 92/300\n",
      "Average training loss: 0.283175179693434\n",
      "Average test loss: 0.0031871032247112858\n",
      "Epoch 93/300\n",
      "Average training loss: 0.2304272096024619\n",
      "Average test loss: 0.003715443901096781\n",
      "Epoch 94/300\n",
      "Average training loss: 0.18820032336976794\n",
      "Average test loss: 0.0031371041109992397\n",
      "Epoch 95/300\n",
      "Average training loss: 0.15855495362149344\n",
      "Average test loss: 0.005419040270563629\n",
      "Epoch 96/300\n",
      "Average training loss: 0.13632475201951133\n",
      "Average test loss: 0.005201617120040788\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11970727942387263\n",
      "Average test loss: 0.010519079096615315\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10675275837050544\n",
      "Average test loss: 0.004341884898435738\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0967310808300972\n",
      "Average test loss: 0.0033063039345045883\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08877332643005582\n",
      "Average test loss: 0.0029727208639184636\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08186394664314058\n",
      "Average test loss: 0.013637008565581508\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07647454786300659\n",
      "Average test loss: 0.009592625560031997\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07224332583612866\n",
      "Average test loss: 0.0029874034101764362\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0685762768652704\n",
      "Average test loss: 0.004079918270930648\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06526303564177618\n",
      "Average test loss: 0.0029230539951887397\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06245927541454633\n",
      "Average test loss: 0.0030167534603840777\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05967982810735702\n",
      "Average test loss: 0.0029917445197287533\n",
      "Epoch 108/300\n",
      "Average training loss: 0.058053277754121356\n",
      "Average test loss: 0.0029740983337784807\n",
      "Epoch 109/300\n",
      "Average training loss: 0.056874378701051075\n",
      "Average test loss: 0.0028791151386168266\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05588722753193644\n",
      "Average test loss: 0.003371888557035062\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05523527746068107\n",
      "Average test loss: 0.0028695075377408\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05446630193458663\n",
      "Average test loss: 0.0029255338354657093\n",
      "Epoch 113/300\n",
      "Average training loss: 0.053659057676792146\n",
      "Average test loss: 0.003030877483387788\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05306863028142187\n",
      "Average test loss: 0.002917274910128779\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05237411520878474\n",
      "Average test loss: 0.0028888345181735027\n",
      "Epoch 116/300\n",
      "Average training loss: 0.051995189679993524\n",
      "Average test loss: 0.0033223030852774778\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05170492492781745\n",
      "Average test loss: 0.003255981742714842\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05127352453602685\n",
      "Average test loss: 0.0030656875080118575\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05066757333609793\n",
      "Average test loss: 0.0032433564687768617\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05046526821123229\n",
      "Average test loss: 0.002793075733818114\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05031088761819733\n",
      "Average test loss: 0.002842355422468649\n",
      "Epoch 122/300\n",
      "Average training loss: 0.049952346394459404\n",
      "Average test loss: 0.0032488197133772904\n",
      "Epoch 123/300\n",
      "Average training loss: 0.049596250183052484\n",
      "Average test loss: 0.0031138573626263272\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04924114467038049\n",
      "Average test loss: 0.004162988003343344\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04944506483276685\n",
      "Average test loss: 0.0028527627968125873\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0494487150642607\n",
      "Average test loss: 0.0028181650081856384\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04832547855708334\n",
      "Average test loss: 0.0028508366437421905\n",
      "Epoch 128/300\n",
      "Average training loss: 0.048174283749527404\n",
      "Average test loss: 0.0039045953096614942\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04796665869487657\n",
      "Average test loss: 0.0029110650320847828\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04765001748336686\n",
      "Average test loss: 0.002887017267859644\n",
      "Epoch 131/300\n",
      "Average training loss: 0.049178953025076126\n",
      "Average test loss: 0.003673738584957189\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04781047413415379\n",
      "Average test loss: 0.002814670160205828\n",
      "Epoch 133/300\n",
      "Average training loss: 0.047108569304148355\n",
      "Average test loss: 0.003113274746470981\n",
      "Epoch 134/300\n",
      "Average training loss: 0.046502776354551315\n",
      "Average test loss: 0.0035843119240469403\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04634629938999812\n",
      "Average test loss: 0.002867166163606776\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04699790171451039\n",
      "Average test loss: 0.0029346970648815233\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04603737653626336\n",
      "Average test loss: 0.0027927879217184252\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04578416202134556\n",
      "Average test loss: 0.0027870673769050175\n",
      "Epoch 139/300\n",
      "Average training loss: 0.045481003950039546\n",
      "Average test loss: 0.010270990969406233\n",
      "Epoch 140/300\n",
      "Average training loss: 0.045206271411644086\n",
      "Average test loss: 0.003165636701302396\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04510627692606714\n",
      "Average test loss: 0.0027917651027027104\n",
      "Epoch 142/300\n",
      "Average training loss: 0.044660008072853086\n",
      "Average test loss: 0.002900044586095545\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04454967620472113\n",
      "Average test loss: 0.0028280525476568276\n",
      "Epoch 144/300\n",
      "Average training loss: 0.044546694808536104\n",
      "Average test loss: 0.003064069316825933\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0438259517169661\n",
      "Average test loss: 0.0029255742999828526\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04372109681036737\n",
      "Average test loss: 0.003019606900297933\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04393350841932826\n",
      "Average test loss: 0.0029271645757059255\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04335374856988589\n",
      "Average test loss: 0.002898103014048603\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04316670917802387\n",
      "Average test loss: 0.08431364206804169\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04459531307220459\n",
      "Average test loss: 0.002901589556493693\n",
      "Epoch 151/300\n",
      "Average training loss: 0.2216604501373238\n",
      "Average test loss: 132.83191148212387\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08355494852198495\n",
      "Average test loss: 0.0031693128500547675\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06372957405116823\n",
      "Average test loss: 0.0030042269563095437\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05797489628030194\n",
      "Average test loss: 0.00287361556208796\n",
      "Epoch 155/300\n",
      "Average training loss: 0.054702357328600355\n",
      "Average test loss: 0.003318425447576576\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0523830565445953\n",
      "Average test loss: 0.004339416651676098\n",
      "Epoch 157/300\n",
      "Average training loss: 0.050779713600873945\n",
      "Average test loss: 0.004089296255674627\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04938878620829847\n",
      "Average test loss: 0.0028178804382267926\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04817152572671572\n",
      "Average test loss: 0.0028527452235834467\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04713036800755395\n",
      "Average test loss: 0.002833560058226188\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04634363380405638\n",
      "Average test loss: 0.002872020075822042\n",
      "Epoch 162/300\n",
      "Average training loss: 0.048796461999416355\n",
      "Average test loss: 0.00279639944092681\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04528185633818309\n",
      "Average test loss: 0.0027723519226743117\n",
      "Epoch 164/300\n",
      "Average training loss: 0.044547606925169625\n",
      "Average test loss: 0.002898933525507649\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0442114608420266\n",
      "Average test loss: 0.0030765770158420005\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04403849303391245\n",
      "Average test loss: 0.0031008604456567103\n",
      "Epoch 167/300\n",
      "Average training loss: 0.043030026717318426\n",
      "Average test loss: 0.0028153838291764257\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0438148609995842\n",
      "Average test loss: 0.0028644410218629573\n",
      "Epoch 169/300\n",
      "Average training loss: 0.4167537428836028\n",
      "Average test loss: 26.713028877069966\n",
      "Epoch 170/300\n",
      "Average training loss: 0.4052886319557826\n",
      "Average test loss: 0.003912841841578483\n",
      "Epoch 171/300\n",
      "Average training loss: 0.15718082092867958\n",
      "Average test loss: 0.0045348227216551705\n",
      "Epoch 172/300\n",
      "Average training loss: 0.10803778056965933\n",
      "Average test loss: 0.0035475107398298053\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08844765674405627\n",
      "Average test loss: 0.2214958301625318\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0778735605875651\n",
      "Average test loss: 0.004282589037592212\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07094466414054235\n",
      "Average test loss: 0.014269991404687364\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0660597476694319\n",
      "Average test loss: 0.0031495767068117856\n",
      "Epoch 177/300\n",
      "Average training loss: 0.062265920258230634\n",
      "Average test loss: 0.0029546681503868767\n",
      "Epoch 178/300\n",
      "Average training loss: 0.059230293856726755\n",
      "Average test loss: 0.007848689173037807\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05646131763524479\n",
      "Average test loss: 0.003514286560937762\n",
      "Epoch 180/300\n",
      "Average training loss: 0.054849226935042274\n",
      "Average test loss: 0.0028062462382432486\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05308830267853207\n",
      "Average test loss: 0.0028896186088936196\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05201820610960325\n",
      "Average test loss: 0.002852683937590983\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05081149860554271\n",
      "Average test loss: 0.00435813801901208\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04945393184820811\n",
      "Average test loss: 0.007620577009187804\n",
      "Epoch 185/300\n",
      "Average training loss: 0.048634814596838424\n",
      "Average test loss: 0.006377083408335845\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04767958330114683\n",
      "Average test loss: 0.0029432421318358847\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0469176689154572\n",
      "Average test loss: 0.003005702419620421\n",
      "Epoch 188/300\n",
      "Average training loss: 0.046082398964299096\n",
      "Average test loss: 0.0029150529619720246\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04538640003734165\n",
      "Average test loss: 0.00287714856200748\n",
      "Epoch 190/300\n",
      "Average training loss: 0.045705298086007436\n",
      "Average test loss: 0.003027325096229712\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04447625370489226\n",
      "Average test loss: 0.0028177996851089927\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04410481317506896\n",
      "Average test loss: 0.0028223233771406944\n",
      "Epoch 193/300\n",
      "Average training loss: 0.046663410309288236\n",
      "Average test loss: 0.0028974719506998856\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04437873543633355\n",
      "Average test loss: 0.011787677908109294\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04323197588324547\n",
      "Average test loss: 0.002814663833628098\n",
      "Epoch 196/300\n",
      "Average training loss: 0.042953268617391585\n",
      "Average test loss: 0.0028299355489305325\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04295564932955636\n",
      "Average test loss: 0.003704150311648846\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04268263263172573\n",
      "Average test loss: 0.002844254026396407\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04255635201268726\n",
      "Average test loss: 0.003124353794174062\n",
      "Epoch 200/300\n",
      "Average training loss: 0.042380424002806345\n",
      "Average test loss: 0.0028584502732588186\n",
      "Epoch 201/300\n",
      "Average training loss: 0.042794076217545406\n",
      "Average test loss: 0.018857683178658286\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04181965390344461\n",
      "Average test loss: 74.6591889292399\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04976770760946803\n",
      "Average test loss: 0.002842453917074535\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04185646687613593\n",
      "Average test loss: 0.0030972634363505575\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04136151193910175\n",
      "Average test loss: 0.0028597124610096215\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0415138418028752\n",
      "Average test loss: 0.00283095492153532\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04103840945826637\n",
      "Average test loss: 0.002839351758878264\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04117444864577717\n",
      "Average test loss: 0.0030867521698690124\n",
      "Epoch 209/300\n",
      "Average training loss: 0.041260620799329545\n",
      "Average test loss: 0.002903917852582203\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04098137059807777\n",
      "Average test loss: 0.002905727373229133\n",
      "Epoch 211/300\n",
      "Average training loss: 0.040790084024270375\n",
      "Average test loss: 0.0030494237020611764\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04054227001965046\n",
      "Average test loss: 0.003008188704856568\n",
      "Epoch 213/300\n",
      "Average training loss: 0.040599826018015546\n",
      "Average test loss: 0.009494161870744493\n",
      "Epoch 214/300\n",
      "Average training loss: 0.041730703274408976\n",
      "Average test loss: 0.0028983489816180536\n",
      "Epoch 215/300\n",
      "Average training loss: 0.039853706455892986\n",
      "Average test loss: 0.003084769311050574\n",
      "Epoch 216/300\n",
      "Average training loss: 0.040020768268240824\n",
      "Average test loss: 0.002822444862375657\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04000221515695254\n",
      "Average test loss: 0.0028491598800238637\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03958073159886731\n",
      "Average test loss: 0.0029071163953178457\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03965941147671805\n",
      "Average test loss: 0.0029024604712095526\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04006762892421749\n",
      "Average test loss: 0.0029168074801564216\n",
      "Epoch 221/300\n",
      "Average training loss: 0.039274243709113865\n",
      "Average test loss: 0.019751333127419154\n",
      "Epoch 222/300\n",
      "Average training loss: 0.039242512088682915\n",
      "Average test loss: 0.0028980598727034197\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03906903980175654\n",
      "Average test loss: 0.0033849610417253442\n",
      "Epoch 224/300\n",
      "Average training loss: 0.038749901708629395\n",
      "Average test loss: 0.009044161953032016\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04130429382125537\n",
      "Average test loss: 0.0029659261722117664\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03888177850180202\n",
      "Average test loss: 0.002881317168060276\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03900629214280182\n",
      "Average test loss: 0.004239500515576866\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10681517512599627\n",
      "Average test loss: 0.0032077386582063305\n",
      "Epoch 229/300\n",
      "Average training loss: 0.062413307872083455\n",
      "Average test loss: 0.002848449784434504\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05188435636957486\n",
      "Average test loss: 0.0028710493495066962\n",
      "Epoch 231/300\n",
      "Average training loss: 0.047118690066867405\n",
      "Average test loss: 0.0028437603700699077\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04457228545513418\n",
      "Average test loss: 0.002887145166699257\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04283421695894665\n",
      "Average test loss: 0.003031567136860556\n",
      "Epoch 234/300\n",
      "Average training loss: 0.041448823250002334\n",
      "Average test loss: 0.002864181755317582\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04053643205430772\n",
      "Average test loss: 0.0028533670900182592\n",
      "Epoch 236/300\n",
      "Average training loss: 0.039871792991956076\n",
      "Average test loss: 0.003024526464856333\n",
      "Epoch 237/300\n",
      "Average training loss: 0.040717159383826786\n",
      "Average test loss: 0.0037963353681067626\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03910867811739445\n",
      "Average test loss: 0.002844890263552467\n",
      "Epoch 239/300\n",
      "Average training loss: 0.038882798749539586\n",
      "Average test loss: 0.0029532669205218553\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03884810477495194\n",
      "Average test loss: 0.0028724319812738234\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03870226012004746\n",
      "Average test loss: 0.0029126765405138332\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03844335774580638\n",
      "Average test loss: 0.0036602734016875425\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03836031780971421\n",
      "Average test loss: 0.0028894502735800215\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03994549627436532\n",
      "Average test loss: 0.002879165180855327\n",
      "Epoch 245/300\n",
      "Average training loss: 0.038195805145634545\n",
      "Average test loss: 0.003011719997144408\n",
      "Epoch 246/300\n",
      "Average training loss: 0.038163356128666136\n",
      "Average test loss: 0.018756519995215867\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03816748208469815\n",
      "Average test loss: 0.0030990868268741503\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03818089921606912\n",
      "Average test loss: 1717.1830671657985\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0420074205721418\n",
      "Average test loss: 0.0030122408515049354\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03769669912589921\n",
      "Average test loss: 0.00306791854939527\n",
      "Epoch 251/300\n",
      "Average training loss: 0.037399012403355705\n",
      "Average test loss: 0.00302605169152634\n",
      "Epoch 252/300\n",
      "Average training loss: 0.037710242003202436\n",
      "Average test loss: 0.003673731279869874\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03737910460101233\n",
      "Average test loss: 0.0034092242339005072\n",
      "Epoch 254/300\n",
      "Average training loss: 0.037909617606136536\n",
      "Average test loss: 0.004563661352627807\n",
      "Epoch 255/300\n",
      "Average training loss: 0.038291899929444\n",
      "Average test loss: 0.0029344965565121835\n",
      "Epoch 256/300\n",
      "Average training loss: 0.037167412824100916\n",
      "Average test loss: 0.0029725488695419495\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03735572062432766\n",
      "Average test loss: 0.0031216886745144925\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03728825733231174\n",
      "Average test loss: 0.0035570733816259435\n",
      "Epoch 259/300\n",
      "Average training loss: 0.038222075555059645\n",
      "Average test loss: 0.003126245566798995\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03914216077658865\n",
      "Average test loss: 0.0029855250430603822\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03778505373332235\n",
      "Average test loss: 0.0044076945173243684\n",
      "Epoch 262/300\n",
      "Average training loss: 0.037149512744612166\n",
      "Average test loss: 0.0030141124083764024\n",
      "Epoch 263/300\n",
      "Average training loss: 0.036640709554155666\n",
      "Average test loss: 0.0029678460359573366\n",
      "Epoch 264/300\n",
      "Average training loss: 0.036892819374799726\n",
      "Average test loss: 0.0030550286082757843\n",
      "Epoch 265/300\n",
      "Average training loss: 0.037031420595116084\n",
      "Average test loss: 0.0034890490996961794\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03665923495093981\n",
      "Average test loss: 0.002939621770340535\n",
      "Epoch 267/300\n",
      "Average training loss: 53.318913403093816\n",
      "Average test loss: 0.006187727947615915\n",
      "Epoch 268/300\n",
      "Average training loss: 1.299506505118476\n",
      "Average test loss: 0.004162298010041317\n",
      "Epoch 269/300\n",
      "Average training loss: 0.840769895447625\n",
      "Average test loss: 0.0038149164447353946\n",
      "Epoch 270/300\n",
      "Average training loss: 0.6772608854505751\n",
      "Average test loss: 0.059924015471918717\n",
      "Epoch 271/300\n",
      "Average training loss: 0.5815625606642829\n",
      "Average test loss: 0.01909525366582804\n",
      "Epoch 272/300\n",
      "Average training loss: 0.5071250845591228\n",
      "Average test loss: 0.0189417211521003\n",
      "Epoch 273/300\n",
      "Average training loss: 0.43979244746102225\n",
      "Average test loss: 130.73276885190606\n",
      "Epoch 274/300\n",
      "Average training loss: 0.3746818919711643\n",
      "Average test loss: 0.09798546606664442\n",
      "Epoch 275/300\n",
      "Average training loss: 0.31114669259389244\n",
      "Average test loss: 0.020776917361964783\n",
      "Epoch 276/300\n",
      "Average training loss: 0.262439399295383\n",
      "Average test loss: 0.05022606733110216\n",
      "Epoch 277/300\n",
      "Average training loss: 0.22884745101133983\n",
      "Average test loss: 0.0033006035925613507\n",
      "Epoch 278/300\n",
      "Average training loss: 0.2004247328042984\n",
      "Average test loss: 0.023306379817840128\n",
      "Epoch 279/300\n",
      "Average training loss: 0.17433144272698298\n",
      "Average test loss: 0.0029874785256882508\n",
      "Epoch 280/300\n",
      "Average training loss: 0.14938125207689074\n",
      "Average test loss: 0.004074101526083218\n",
      "Epoch 281/300\n",
      "Average training loss: 0.12749236083692975\n",
      "Average test loss: 0.0029357216503057217\n",
      "Epoch 282/300\n",
      "Average training loss: 0.10958407252364688\n",
      "Average test loss: 1.1098717717064752\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09552064273092482\n",
      "Average test loss: 5.160311062706842\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08497814300325182\n",
      "Average test loss: 0.003420217312251528\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07578414495123757\n",
      "Average test loss: 0.0032817282339351046\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06948642611503601\n",
      "Average test loss: 0.005164787911706501\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06472219385041131\n",
      "Average test loss: 0.007140528986851374\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06119483414623472\n",
      "Average test loss: 0.0029158358491129343\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05764113333821297\n",
      "Average test loss: 0.0037694562232742706\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05434545186824269\n",
      "Average test loss: 0.002945997318563362\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05208849506245719\n",
      "Average test loss: 0.0033700227526326973\n",
      "Epoch 292/300\n",
      "Average training loss: 0.12041251374615564\n",
      "Average test loss: 1.9634430346637963\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06617533082432217\n",
      "Average test loss: 0.0029776194603699777\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05395991832680173\n",
      "Average test loss: 0.00283665456995368\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05082833445734448\n",
      "Average test loss: 0.0028498758510169054\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04862350159221225\n",
      "Average test loss: 0.0028674244071460434\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04724745884868834\n",
      "Average test loss: 1148675.5434027778\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04665287825299634\n",
      "Average test loss: 0.0028653818373050957\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0452460372183058\n",
      "Average test loss: 0.002819561515417364\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0435984291864766\n",
      "Average test loss: 0.002836272646776504\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.9549360620710585\n",
      "Average test loss: 0.006717753454214997\n",
      "Epoch 2/300\n",
      "Average training loss: 2.1812082654105294\n",
      "Average test loss: 0.24046025349696476\n",
      "Epoch 3/300\n",
      "Average training loss: 1.5667898745006985\n",
      "Average test loss: 0.00466651228028867\n",
      "Epoch 4/300\n",
      "Average training loss: 1.2064580428865221\n",
      "Average test loss: 0.004401391194719407\n",
      "Epoch 5/300\n",
      "Average training loss: 0.986123403761122\n",
      "Average test loss: 0.0952136577355365\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8102813681496515\n",
      "Average test loss: 0.003879728020272321\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6873784053590563\n",
      "Average test loss: 0.008331985509230031\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6145467163721721\n",
      "Average test loss: 0.0037661015395489003\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5565869813495212\n",
      "Average test loss: 0.004173090169413222\n",
      "Epoch 10/300\n",
      "Average training loss: 0.46111002855830724\n",
      "Average test loss: 0.0033373735522230466\n",
      "Epoch 11/300\n",
      "Average training loss: 0.3982790528933207\n",
      "Average test loss: 0.0032217455584969785\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3369784857696957\n",
      "Average test loss: 0.003244229487246937\n",
      "Epoch 13/300\n",
      "Average training loss: 0.2861028779612647\n",
      "Average test loss: 0.005242959635953108\n",
      "Epoch 14/300\n",
      "Average training loss: 0.24070682741536034\n",
      "Average test loss: 0.0032016229331493376\n",
      "Epoch 15/300\n",
      "Average training loss: 0.20509174443615807\n",
      "Average test loss: 0.002997927030755414\n",
      "Epoch 16/300\n",
      "Average training loss: 0.17746391389105054\n",
      "Average test loss: 0.002721965663342012\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1546923809316423\n",
      "Average test loss: 0.002977843021353086\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13595361988412008\n",
      "Average test loss: 0.006205361559987068\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12082700171735551\n",
      "Average test loss: 0.00249653216637671\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10830351626873017\n",
      "Average test loss: 0.002409793690674835\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09850602910253736\n",
      "Average test loss: 0.0025290619497083957\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0899687683780988\n",
      "Average test loss: 0.002448488914097349\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08269176498055458\n",
      "Average test loss: 0.0023113365969103243\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07678722986910078\n",
      "Average test loss: 0.0025993979822637307\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07170099951161278\n",
      "Average test loss: 0.0022669623535540367\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06701941303412119\n",
      "Average test loss: 0.0025298712332215575\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06363237708806992\n",
      "Average test loss: 0.00220261959783319\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05992897629737854\n",
      "Average test loss: 0.002166257525690728\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05729403487179015\n",
      "Average test loss: 0.0021696828932811817\n",
      "Epoch 30/300\n",
      "Average training loss: 0.054535238828923964\n",
      "Average test loss: 0.010229463025927544\n",
      "Epoch 31/300\n",
      "Average training loss: 0.052497518989774915\n",
      "Average test loss: 0.002159483223532637\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05336966521872415\n",
      "Average test loss: 0.0019959902971362076\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04896970467766126\n",
      "Average test loss: 0.002039385155774653\n",
      "Epoch 34/300\n",
      "Average training loss: 0.047601546969678665\n",
      "Average test loss: 0.001984878223285907\n",
      "Epoch 35/300\n",
      "Average training loss: 0.045842926277054684\n",
      "Average test loss: 0.0019481089688423608\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04427261408170064\n",
      "Average test loss: 0.00224159647586445\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04332556304997868\n",
      "Average test loss: 0.0020496466170168586\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04218215285076035\n",
      "Average test loss: 0.0021144568538293243\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04116791937086317\n",
      "Average test loss: 0.0020930773224681616\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04170448351568646\n",
      "Average test loss: 0.0019530680501419638\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04462546020746231\n",
      "Average test loss: 0.001939285305949549\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03989254067838192\n",
      "Average test loss: 0.002354222172457311\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03885137705008189\n",
      "Average test loss: 0.0018660773307912879\n",
      "Epoch 44/300\n",
      "Average training loss: 0.038585300748546915\n",
      "Average test loss: 0.002724932720263799\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03769711272087362\n",
      "Average test loss: 0.02742360893388589\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03648021068506771\n",
      "Average test loss: 0.0019312384472125106\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09591727110412386\n",
      "Average test loss: 0.0020940036270767452\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04579728052020073\n",
      "Average test loss: 0.002006393710565236\n",
      "Epoch 49/300\n",
      "Average training loss: 0.042185032576322556\n",
      "Average test loss: 0.001933980819872684\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04015771580239137\n",
      "Average test loss: 0.0019308154755789373\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03892797280020184\n",
      "Average test loss: 0.0019326246235933568\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03789184755086899\n",
      "Average test loss: 0.00189412009012368\n",
      "Epoch 53/300\n",
      "Average training loss: 0.037230362728238105\n",
      "Average test loss: 0.0018509286236949266\n",
      "Epoch 54/300\n",
      "Average training loss: 0.036619105299313864\n",
      "Average test loss: 0.0018678195566559832\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03599859847790665\n",
      "Average test loss: 0.0019276218033499187\n",
      "Epoch 56/300\n",
      "Average training loss: 0.035575812776883445\n",
      "Average test loss: 0.0018381478070384927\n",
      "Epoch 57/300\n",
      "Average training loss: 0.035252650163239904\n",
      "Average test loss: 0.001883476423099637\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03489670588903957\n",
      "Average test loss: 0.001810568939273556\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03468308292826017\n",
      "Average test loss: 0.0018346679740481907\n",
      "Epoch 60/300\n",
      "Average training loss: 0.034253082672754925\n",
      "Average test loss: 0.1617215484538012\n",
      "Epoch 61/300\n",
      "Average training loss: 0.034730546611878604\n",
      "Average test loss: 0.0018500101909869246\n",
      "Epoch 62/300\n",
      "Average training loss: 0.19504926923248503\n",
      "Average test loss: 0.00210276943196853\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05399448419279523\n",
      "Average test loss: 0.0019629518327613673\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04679135478205151\n",
      "Average test loss: 0.0019062438606181078\n",
      "Epoch 65/300\n",
      "Average training loss: 0.043646020303169886\n",
      "Average test loss: 0.0018811634511997303\n",
      "Epoch 66/300\n",
      "Average training loss: 0.041460680044359634\n",
      "Average test loss: 0.001852537026732332\n",
      "Epoch 67/300\n",
      "Average training loss: 0.039840856071975496\n",
      "Average test loss: 0.0018678230590497454\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03859517061048084\n",
      "Average test loss: 0.001846938249655068\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03756674706604746\n",
      "Average test loss: 0.0021060509845200514\n",
      "Epoch 70/300\n",
      "Average training loss: 0.036880005314946174\n",
      "Average test loss: 0.001935699991778367\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03622456260522207\n",
      "Average test loss: 0.0019257170851859782\n",
      "Epoch 72/300\n",
      "Average training loss: 0.035816703577836354\n",
      "Average test loss: 0.0018736748518422247\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03521696399980121\n",
      "Average test loss: 0.0018386158956628707\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03496831913623545\n",
      "Average test loss: 0.0019593893095023102\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03471343262990316\n",
      "Average test loss: 0.001799691915926006\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03410277613335186\n",
      "Average test loss: 0.0018909192259112995\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0337003689010938\n",
      "Average test loss: 0.0028644030594991315\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0336293948425187\n",
      "Average test loss: 0.0018162751149179207\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03380835027330452\n",
      "Average test loss: 0.0018122816293810805\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03288138005634149\n",
      "Average test loss: 0.057562166013651425\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03272416279216608\n",
      "Average test loss: 0.0017762560614695152\n",
      "Epoch 82/300\n",
      "Average training loss: 0.032495099107424416\n",
      "Average test loss: 0.20637728969090516\n",
      "Epoch 83/300\n",
      "Average training loss: 0.032407655184467636\n",
      "Average test loss: 0.001790016174937288\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03197636617223422\n",
      "Average test loss: 0.0017946301161622007\n",
      "Epoch 85/300\n",
      "Average training loss: 0.031718640971514916\n",
      "Average test loss: 0.019664299490551155\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03148584001428551\n",
      "Average test loss: 0.0017858184076224764\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03126040847433938\n",
      "Average test loss: 0.001904214604654246\n",
      "Epoch 88/300\n",
      "Average training loss: 0.031144862833950255\n",
      "Average test loss: 0.0018398646421523557\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03077544340822432\n",
      "Average test loss: 0.00255639351821608\n",
      "Epoch 90/300\n",
      "Average training loss: 0.030564068469736312\n",
      "Average test loss: 0.0017932444812936916\n",
      "Epoch 91/300\n",
      "Average training loss: 0.030403872112433117\n",
      "Average test loss: 0.004449180512378613\n",
      "Epoch 92/300\n",
      "Average training loss: 0.030185146363245118\n",
      "Average test loss: 0.0017614616870673166\n",
      "Epoch 93/300\n",
      "Average training loss: 0.030103671921624078\n",
      "Average test loss: 0.004837289627434479\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02961204169028335\n",
      "Average test loss: 0.0017927655519710648\n",
      "Epoch 95/300\n",
      "Average training loss: 0.029951845405830277\n",
      "Average test loss: 0.002312577983157502\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03233947624266147\n",
      "Average test loss: 0.0027753806149380077\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03152026447488202\n",
      "Average test loss: 0.003955245304024882\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02991469677289327\n",
      "Average test loss: 0.0017648218804970383\n",
      "Epoch 99/300\n",
      "Average training loss: 0.030532446281777488\n",
      "Average test loss: 0.0017667168801029523\n",
      "Epoch 100/300\n",
      "Average training loss: 0.029633733246061538\n",
      "Average test loss: 0.0018111932505336073\n",
      "Epoch 101/300\n",
      "Average training loss: 0.029117996181050935\n",
      "Average test loss: 0.0018087159464549687\n",
      "Epoch 102/300\n",
      "Average training loss: 0.028863793328404427\n",
      "Average test loss: 0.0017705978780157036\n",
      "Epoch 103/300\n",
      "Average training loss: 0.028666951504018573\n",
      "Average test loss: 0.001788762417828871\n",
      "Epoch 104/300\n",
      "Average training loss: 0.028702462141712508\n",
      "Average test loss: 0.0018057680435271727\n",
      "Epoch 105/300\n",
      "Average training loss: 0.028465769595570036\n",
      "Average test loss: 0.0018796344217326907\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02827151489092244\n",
      "Average test loss: 0.001818453874439001\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02817570056517919\n",
      "Average test loss: 0.0017574401717219087\n",
      "Epoch 108/300\n",
      "Average training loss: 0.028520883681045637\n",
      "Average test loss: 0.0018058215141710308\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02789502998524242\n",
      "Average test loss: 0.0018219017037707898\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027802716228697035\n",
      "Average test loss: 0.0029232214784456626\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02757940669854482\n",
      "Average test loss: 0.0018519295727213224\n",
      "Epoch 112/300\n",
      "Average training loss: 0.027624239310622214\n",
      "Average test loss: 0.0018469088441795773\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027366800722148685\n",
      "Average test loss: 0.0023705750637584263\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02732550068034066\n",
      "Average test loss: 0.005009014386062821\n",
      "Epoch 115/300\n",
      "Average training loss: 0.026972669882906808\n",
      "Average test loss: 0.006671800568906797\n",
      "Epoch 116/300\n",
      "Average training loss: 0.026967655362354386\n",
      "Average test loss: 0.0017911141742434768\n",
      "Epoch 117/300\n",
      "Average training loss: 0.026810674324631693\n",
      "Average test loss: 0.002669184500661989\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02676655229429404\n",
      "Average test loss: 0.004956912203381459\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02666240978737672\n",
      "Average test loss: 0.0018112692576315668\n",
      "Epoch 120/300\n",
      "Average training loss: 0.026301329826315244\n",
      "Average test loss: 0.001863954881620076\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02850508845018016\n",
      "Average test loss: 0.0018555683023813699\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02631728336877293\n",
      "Average test loss: 0.001895787346487244\n",
      "Epoch 123/300\n",
      "Average training loss: 0.026228726448284254\n",
      "Average test loss: 0.0018759931198631724\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02614390602376726\n",
      "Average test loss: 0.0019093203025145663\n",
      "Epoch 125/300\n",
      "Average training loss: 0.025968388944864272\n",
      "Average test loss: 0.001791892831110292\n",
      "Epoch 126/300\n",
      "Average training loss: 0.025852556579642826\n",
      "Average test loss: 0.002050256976029939\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025868162378668784\n",
      "Average test loss: 0.0018345236803094546\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025776846365796195\n",
      "Average test loss: 0.0018475754561109675\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02579175466299057\n",
      "Average test loss: 0.0077543938590420615\n",
      "Epoch 130/300\n",
      "Average training loss: 0.028441919565200806\n",
      "Average test loss: 0.0018170123482123017\n",
      "Epoch 131/300\n",
      "Average training loss: 0.027930218091441524\n",
      "Average test loss: 0.0018586332531025011\n",
      "Epoch 132/300\n",
      "Average training loss: 0.026238851606845855\n",
      "Average test loss: 0.001827482130481965\n",
      "Epoch 133/300\n",
      "Average training loss: 0.025457734304997655\n",
      "Average test loss: 0.0018578595584258437\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02556903635793262\n",
      "Average test loss: 0.0019235115894633862\n",
      "Epoch 135/300\n",
      "Average training loss: 0.025203310133682356\n",
      "Average test loss: 0.0018585376375251346\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02540892783800761\n",
      "Average test loss: 0.07733402109642823\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02527974570956495\n",
      "Average test loss: 0.001923884911255704\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02520353752043512\n",
      "Average test loss: 0.003063172019397219\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02511757614215215\n",
      "Average test loss: 0.0018162224700467454\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02479688944750362\n",
      "Average test loss: 0.0018814000698427358\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02496400559445222\n",
      "Average test loss: 0.001868305446476572\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02516827198035187\n",
      "Average test loss: 0.001869065815789832\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024709027174446316\n",
      "Average test loss: 0.001879559643411388\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024732032977872423\n",
      "Average test loss: 0.0019269629635123742\n",
      "Epoch 145/300\n",
      "Average training loss: 0.024584643027848668\n",
      "Average test loss: 0.0018842833203574021\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024621318507525657\n",
      "Average test loss: 0.002738995799380872\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024497402722636858\n",
      "Average test loss: 0.0018737028574364054\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02435086835258537\n",
      "Average test loss: 0.002040478093549609\n",
      "Epoch 149/300\n",
      "Average training loss: 0.029136820380886396\n",
      "Average test loss: 0.001871500386701276\n",
      "Epoch 150/300\n",
      "Average training loss: 0.024556967669063144\n",
      "Average test loss: 0.0018508877663148775\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02472864625023471\n",
      "Average test loss: 0.03710401644143793\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024041829783055517\n",
      "Average test loss: 0.001869394410918984\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024171219042605822\n",
      "Average test loss: 0.0020478052554859056\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024136296386520067\n",
      "Average test loss: 0.0020245682793772883\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024309420436620714\n",
      "Average test loss: 0.001857916762224502\n",
      "Epoch 156/300\n",
      "Average training loss: 0.023952026502953635\n",
      "Average test loss: 0.0025368530923086737\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024182123881247308\n",
      "Average test loss: 0.13472540929582383\n",
      "Epoch 158/300\n",
      "Average training loss: 0.024137151724762388\n",
      "Average test loss: 0.002353039998561144\n",
      "Epoch 159/300\n",
      "Average training loss: 0.024833350665039486\n",
      "Average test loss: 0.0021321670934557916\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02409538664917151\n",
      "Average test loss: 0.0018317117152942552\n",
      "Epoch 161/300\n",
      "Average training loss: 0.024044104918009707\n",
      "Average test loss: 0.0019185289510836204\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024124588494499524\n",
      "Average test loss: 0.0019071144855891664\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0237956177542607\n",
      "Average test loss: 0.002118618177767429\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02365654454794195\n",
      "Average test loss: 0.0018819668334391381\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023457632347113556\n",
      "Average test loss: 0.0026888432417892746\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024245478803084957\n",
      "Average test loss: 0.001996445327790247\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024017506524920462\n",
      "Average test loss: 0.0036358757286022105\n",
      "Epoch 168/300\n",
      "Average training loss: 0.023422827278574306\n",
      "Average test loss: 0.0019497648514807224\n",
      "Epoch 169/300\n",
      "Average training loss: 0.023400409718354543\n",
      "Average test loss: 0.0025274058473813863\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023916750197609266\n",
      "Average test loss: 0.03437453867660628\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02483966347409619\n",
      "Average test loss: 0.001916070429608226\n",
      "Epoch 172/300\n",
      "Average training loss: 0.023227239809102483\n",
      "Average test loss: 0.0022748354052503903\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02330822370118565\n",
      "Average test loss: 0.0019586016670283344\n",
      "Epoch 174/300\n",
      "Average training loss: 0.023474159942732917\n",
      "Average test loss: 0.0018789237580365604\n",
      "Epoch 175/300\n",
      "Average training loss: 0.023238408707910115\n",
      "Average test loss: 0.003755118601024151\n",
      "Epoch 176/300\n",
      "Average training loss: 0.023372736252016493\n",
      "Average test loss: 0.0019330172677420908\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023102133886681663\n",
      "Average test loss: 0.001898690953436825\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023953998120294678\n",
      "Average test loss: 0.0024460380954874888\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0230310205237733\n",
      "Average test loss: 0.001911215058631367\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02298251419597202\n",
      "Average test loss: 0.001942392960501214\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02340455918179618\n",
      "Average test loss: 0.004650339464243087\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02335019914805889\n",
      "Average test loss: 0.0019917651216706466\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02327580645845996\n",
      "Average test loss: 0.0019373304111262163\n",
      "Epoch 184/300\n",
      "Average training loss: 0.024229422928558456\n",
      "Average test loss: 0.0019710034917419157\n",
      "Epoch 185/300\n",
      "Average training loss: 0.022787727057933808\n",
      "Average test loss: 0.0020738932742840714\n",
      "Epoch 186/300\n",
      "Average training loss: 0.022840151285131772\n",
      "Average test loss: 0.006405764646828175\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02309209451576074\n",
      "Average test loss: 0.002485178355438014\n",
      "Epoch 188/300\n",
      "Average training loss: 0.022811097181505625\n",
      "Average test loss: 0.007212763983549343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.022709584413303268\n",
      "Average test loss: 0.008350952035850949\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023295346072978444\n",
      "Average test loss: 2.181003768088089\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023256949269109303\n",
      "Average test loss: 0.002110662544353141\n",
      "Epoch 192/300\n",
      "Average training loss: 0.022614004542430243\n",
      "Average test loss: 0.0019163001078284449\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02267450762208965\n",
      "Average test loss: 0.0020971671582923996\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022677052969733873\n",
      "Average test loss: 0.0019712106422003772\n",
      "Epoch 195/300\n",
      "Average training loss: 0.022611501168873574\n",
      "Average test loss: 0.002436090732096798\n",
      "Epoch 196/300\n",
      "Average training loss: 0.022815438146392503\n",
      "Average test loss: 0.0019941225409921674\n",
      "Epoch 197/300\n",
      "Average training loss: 0.022544340958197913\n",
      "Average test loss: 0.002008322541601956\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02335685770213604\n",
      "Average test loss: 0.04256029050217734\n",
      "Epoch 199/300\n",
      "Average training loss: 0.022639331912000973\n",
      "Average test loss: 0.008755581201778518\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02249815251926581\n",
      "Average test loss: 0.0019325661150117715\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022548301739825142\n",
      "Average test loss: 0.0019296657923195098\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02248776442143652\n",
      "Average test loss: 0.002549937114533451\n",
      "Epoch 203/300\n",
      "Average training loss: 0.022459827823771372\n",
      "Average test loss: 0.007125573606954681\n",
      "Epoch 204/300\n",
      "Average training loss: 0.022331820570760302\n",
      "Average test loss: 0.0019108900514741738\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02238921526985036\n",
      "Average test loss: 0.005999863814976481\n",
      "Epoch 206/300\n",
      "Average training loss: 0.024050635192129348\n",
      "Average test loss: 0.002607912137483557\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022072895157668325\n",
      "Average test loss: 0.001979456990129418\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022791406272186173\n",
      "Average test loss: 0.001889223704321517\n",
      "Epoch 209/300\n",
      "Average training loss: 0.022181942853662703\n",
      "Average test loss: 0.00225372018189066\n",
      "Epoch 210/300\n",
      "Average training loss: 0.022148144601119888\n",
      "Average test loss: 0.002002821439463231\n",
      "Epoch 211/300\n",
      "Average training loss: 0.024203016302651828\n",
      "Average test loss: 0.0032632861245009636\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02209542450143231\n",
      "Average test loss: 0.002063958670426574\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022336906493537957\n",
      "Average test loss: 0.0023044565789815453\n",
      "Epoch 214/300\n",
      "Average training loss: 0.022385339508454005\n",
      "Average test loss: 0.001954903792589903\n",
      "Epoch 215/300\n",
      "Average training loss: 0.022223115732272467\n",
      "Average test loss: 0.003663245152268145\n",
      "Epoch 216/300\n",
      "Average training loss: 0.022914414876865016\n",
      "Average test loss: 0.0019660573029476737\n",
      "Epoch 217/300\n",
      "Average training loss: 0.022032494005229736\n",
      "Average test loss: 0.023733194426943858\n",
      "Epoch 218/300\n",
      "Average training loss: 0.022018810853362082\n",
      "Average test loss: 0.0020762917631202272\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02239306801226404\n",
      "Average test loss: 0.001964384610661202\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02258074652651946\n",
      "Average test loss: 0.0027466581836342813\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021967201406757036\n",
      "Average test loss: 0.002153076876782709\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022778429506553545\n",
      "Average test loss: 0.001963553841950165\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02181116759777069\n",
      "Average test loss: 0.007471092198457983\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022528057750728397\n",
      "Average test loss: 0.0024976564861006207\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023128005986412365\n",
      "Average test loss: 0.0019904973742862543\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021718203200234307\n",
      "Average test loss: 0.0025976550507669648\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023312257316377427\n",
      "Average test loss: 0.01267502097454336\n",
      "Epoch 228/300\n",
      "Average training loss: 0.021661790748437246\n",
      "Average test loss: 0.0019680387131455874\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023460371389985083\n",
      "Average test loss: 0.0019160468780125182\n",
      "Epoch 230/300\n",
      "Average training loss: 0.021773711557189622\n",
      "Average test loss: 0.0019649923044360346\n",
      "Epoch 231/300\n",
      "Average training loss: 0.021950364301602045\n",
      "Average test loss: 0.001909199479346474\n",
      "Epoch 232/300\n",
      "Average training loss: 0.021582234703832203\n",
      "Average test loss: 0.0110302807589372\n",
      "Epoch 233/300\n",
      "Average training loss: 0.021919008534815577\n",
      "Average test loss: 0.001919434825786286\n",
      "Epoch 234/300\n",
      "Average training loss: 0.021834831916623644\n",
      "Average test loss: 0.0020022877256075542\n",
      "Epoch 235/300\n",
      "Average training loss: 0.021944709597362413\n",
      "Average test loss: 0.0019359758055458465\n",
      "Epoch 236/300\n",
      "Average training loss: 0.021778335858550338\n",
      "Average test loss: 0.003033836774010625\n",
      "Epoch 237/300\n",
      "Average training loss: 0.021764727126393053\n",
      "Average test loss: 0.0019437307297873\n",
      "Epoch 238/300\n",
      "Average training loss: 0.021689290488759675\n",
      "Average test loss: 0.0020149589845289786\n",
      "Epoch 239/300\n",
      "Average training loss: 0.022040678206417294\n",
      "Average test loss: 0.0019084414889415104\n",
      "Epoch 240/300\n",
      "Average training loss: 0.021560128763318063\n",
      "Average test loss: 0.002163737893001073\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02167300091104375\n",
      "Average test loss: 0.0019406845735179053\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021716821567879782\n",
      "Average test loss: 0.002057758748738302\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021829586561355325\n",
      "Average test loss: 0.002308468666031129\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021891435553630195\n",
      "Average test loss: 0.0019055651585157547\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022954413365986612\n",
      "Average test loss: 0.0037708971628712283\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02134794245991442\n",
      "Average test loss: 0.0033965020415683586\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02150787310136689\n",
      "Average test loss: 0.0019358581891283392\n",
      "Epoch 248/300\n",
      "Average training loss: 0.021485925008853275\n",
      "Average test loss: 0.001980939956692358\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03804593348668681\n",
      "Average test loss: 0.001866698286185662\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02920162088341183\n",
      "Average test loss: 0.0018281362406495545\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02473218453096019\n",
      "Average test loss: 0.002004830405012601\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02292602067357964\n",
      "Average test loss: 0.0019333817267583476\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022104587483737205\n",
      "Average test loss: 0.0019779026600428755\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02167173239092032\n",
      "Average test loss: 0.0019414538252684806\n",
      "Epoch 255/300\n",
      "Average training loss: 0.022050480872392655\n",
      "Average test loss: 0.0019989492963585587\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021363253167933888\n",
      "Average test loss: 0.002182755638534824\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02121377504037486\n",
      "Average test loss: 0.0026014842442754244\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021320462375879288\n",
      "Average test loss: 0.0019429426427102752\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021592915544907252\n",
      "Average test loss: 0.001950566509531604\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021299030469523535\n",
      "Average test loss: 0.0020645638830545875\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02141946499215232\n",
      "Average test loss: 0.00197780788058622\n",
      "Epoch 262/300\n",
      "Average training loss: 0.021515804159972403\n",
      "Average test loss: 0.001959267005738285\n",
      "Epoch 263/300\n",
      "Average training loss: 0.021228883122404417\n",
      "Average test loss: 0.0019770858958363534\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023221517536375257\n",
      "Average test loss: 0.0019442833233624696\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021129027020600107\n",
      "Average test loss: 0.0021866808607139522\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021178879984551004\n",
      "Average test loss: 0.001950268213947614\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02130345795551936\n",
      "Average test loss: 0.0020678518751843108\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02141170355015331\n",
      "Average test loss: 0.0019424649154146513\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021241644147369596\n",
      "Average test loss: 0.0034701852707399263\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021526280437906584\n",
      "Average test loss: 0.002189006337275108\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021465658519003127\n",
      "Average test loss: 0.001978749767566721\n",
      "Epoch 272/300\n",
      "Average training loss: 0.021638149754868614\n",
      "Average test loss: 0.0020392512650125555\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021127152823739583\n",
      "Average test loss: 0.0022146104752189585\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02127784187760618\n",
      "Average test loss: 0.0019713020398178034\n",
      "Epoch 275/300\n",
      "Average training loss: 0.021196170664495893\n",
      "Average test loss: 0.0020083075080894763\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021503536189595857\n",
      "Average test loss: 0.0019726678364806703\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021186192706227302\n",
      "Average test loss: 0.0019758562239714796\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020959392280214362\n",
      "Average test loss: 0.00199594195621709\n",
      "Epoch 279/300\n",
      "Average training loss: 0.021632473678224618\n",
      "Average test loss: 0.0024819484282698895\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022211155128147867\n",
      "Average test loss: 0.0022164410482057267\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02092411814464463\n",
      "Average test loss: 0.0019503428716626432\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020940094118316967\n",
      "Average test loss: 0.0019869771391774218\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021028669568399588\n",
      "Average test loss: 0.003047645261304246\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022653665271070267\n",
      "Average test loss: 0.002014435551543203\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020813356856505077\n",
      "Average test loss: 0.002341468050248093\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021116453335516983\n",
      "Average test loss: 0.002004898066528969\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02098412363562319\n",
      "Average test loss: 0.0019910381234561404\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02094365960525142\n",
      "Average test loss: 0.0023478652389927043\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02094490554266506\n",
      "Average test loss: 0.0066900314887364705\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022587571973601978\n",
      "Average test loss: 0.0019599581037958464\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020772194383045037\n",
      "Average test loss: 0.2569804380999671\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022014277594784897\n",
      "Average test loss: 0.0053104082565340735\n",
      "Epoch 293/300\n",
      "Average training loss: 0.020968052266372574\n",
      "Average test loss: 0.0020738803760872947\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02064317877093951\n",
      "Average test loss: 0.007913677642639312\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020795115888118745\n",
      "Average test loss: 0.06319734856237968\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021010684417353737\n",
      "Average test loss: 0.001971175849541194\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021007880224121943\n",
      "Average test loss: 0.0019722096593015725\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020819683833254707\n",
      "Average test loss: 0.0020164793700807623\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02071563286913766\n",
      "Average test loss: 0.0032559797457522815\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02134936364657349\n",
      "Average test loss: 0.021383324661188655\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 667.4304571961297\n",
      "Average test loss: 157.7786328115728\n",
      "Epoch 2/300\n",
      "Average training loss: 7.244967467414008\n",
      "Average test loss: 3.686711843654513\n",
      "Epoch 3/300\n",
      "Average training loss: 5.834597970750597\n",
      "Average test loss: 0.09187695945468213\n",
      "Epoch 4/300\n",
      "Average training loss: 4.606450876871745\n",
      "Average test loss: 0.008526267834835583\n",
      "Epoch 5/300\n",
      "Average training loss: 3.9594794182247584\n",
      "Average test loss: 1.1667565699310767\n",
      "Epoch 6/300\n",
      "Average training loss: 3.5988239055209688\n",
      "Average test loss: 2.2410588306652177\n",
      "Epoch 7/300\n",
      "Average training loss: 2.5496351511213513\n",
      "Average test loss: 0.006003714690605799\n",
      "Epoch 8/300\n",
      "Average training loss: 2.111950093375312\n",
      "Average test loss: 0.00667511178387536\n",
      "Epoch 9/300\n",
      "Average training loss: 2.196182601292928\n",
      "Average test loss: 0.006132604817549387\n",
      "Epoch 10/300\n",
      "Average training loss: 1.8729107477400038\n",
      "Average test loss: 0.004324111248883936\n",
      "Epoch 11/300\n",
      "Average training loss: 1.4721391341951158\n",
      "Average test loss: 0.22973623870644305\n",
      "Epoch 12/300\n",
      "Average training loss: 1.2602703754636977\n",
      "Average test loss: 0.004425001685404115\n",
      "Epoch 13/300\n",
      "Average training loss: 1.1933416471481324\n",
      "Average test loss: 0.009658842018081083\n",
      "Epoch 14/300\n",
      "Average training loss: 0.9982655242284139\n",
      "Average test loss: 0.0036140519463353686\n",
      "Epoch 15/300\n",
      "Average training loss: 0.8443255582915412\n",
      "Average test loss: 0.003330446493708425\n",
      "Epoch 16/300\n",
      "Average training loss: 0.7411066572401259\n",
      "Average test loss: 0.0036326677238361703\n",
      "Epoch 17/300\n",
      "Average training loss: 0.6440554858313666\n",
      "Average test loss: 0.0031372464570320314\n",
      "Epoch 18/300\n",
      "Average training loss: 0.5637681610849169\n",
      "Average test loss: 0.00297898577257163\n",
      "Epoch 19/300\n",
      "Average training loss: 0.49568728454907734\n",
      "Average test loss: 0.003149089364334941\n",
      "Epoch 20/300\n",
      "Average training loss: 0.43688563452826606\n",
      "Average test loss: 0.002869763589774569\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3845915829605526\n",
      "Average test loss: 0.002692254372355011\n",
      "Epoch 22/300\n",
      "Average training loss: 0.339752880970637\n",
      "Average test loss: 0.0026423573820955224\n",
      "Epoch 23/300\n",
      "Average training loss: 0.3002550635602739\n",
      "Average test loss: 0.0028332183812227515\n",
      "Epoch 24/300\n",
      "Average training loss: 0.26653533250755734\n",
      "Average test loss: 0.0025087906202922263\n",
      "Epoch 25/300\n",
      "Average training loss: 0.23627312401930492\n",
      "Average test loss: 0.00245375469699502\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2088357967933019\n",
      "Average test loss: 0.0029396751910034154\n",
      "Epoch 27/300\n",
      "Average training loss: 0.18434945935673183\n",
      "Average test loss: 0.002162904412071738\n",
      "Epoch 28/300\n",
      "Average training loss: 0.16262831165393193\n",
      "Average test loss: 0.0024198340498324897\n",
      "Epoch 29/300\n",
      "Average training loss: 0.14396407184335921\n",
      "Average test loss: 0.0021609130915668275\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12736019849777222\n",
      "Average test loss: 0.0021468173447582455\n",
      "Epoch 31/300\n",
      "Average training loss: 0.11245246756076813\n",
      "Average test loss: 0.002180937811939253\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1003476250105434\n",
      "Average test loss: 0.001993399888690975\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09011285427543853\n",
      "Average test loss: 0.0018557217616794839\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08171500384807587\n",
      "Average test loss: 0.002097719116239912\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07348719824022718\n",
      "Average test loss: 0.001885217239252395\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06774144201146232\n",
      "Average test loss: 0.0017833889528281158\n",
      "Epoch 37/300\n",
      "Average training loss: 0.061836806032392715\n",
      "Average test loss: 0.00177793231440915\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06323499055703481\n",
      "Average test loss: 0.003138944138876266\n",
      "Epoch 39/300\n",
      "Average training loss: 0.054751928587754566\n",
      "Average test loss: 0.0017633085744455457\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05082705030838648\n",
      "Average test loss: 0.0024270867038932113\n",
      "Epoch 41/300\n",
      "Average training loss: 0.049254578444692824\n",
      "Average test loss: 0.003229534395453003\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04921324737204446\n",
      "Average test loss: 0.0018802851130151085\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0754882758624024\n",
      "Average test loss: 0.003995571841796239\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05782651893794537\n",
      "Average test loss: 0.0017886709097979798\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04639318546983931\n",
      "Average test loss: 0.0016873993408969706\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0429532761954599\n",
      "Average test loss: 0.0017254125792015758\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05410468432307243\n",
      "Average test loss: 0.001886132939822144\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04035362551278538\n",
      "Average test loss: 0.0017677007435510557\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0390477779077159\n",
      "Average test loss: 0.0017937660415967306\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03703658512896962\n",
      "Average test loss: 0.0019859043864740265\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03569847321179178\n",
      "Average test loss: 0.001535022478343712\n",
      "Epoch 52/300\n",
      "Average training loss: 0.035114014100697304\n",
      "Average test loss: 0.002176570555350433\n",
      "Epoch 53/300\n",
      "Average training loss: 0.034205105821291604\n",
      "Average test loss: 0.0025847677049330537\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0329956164509058\n",
      "Average test loss: 0.00161319272675448\n",
      "Epoch 55/300\n",
      "Average training loss: 0.032502970293164256\n",
      "Average test loss: 0.0030284651399900514\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03680982162223922\n",
      "Average test loss: 0.0015377198051040371\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0310858981443776\n",
      "Average test loss: 0.0016829319391399621\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03034073852499326\n",
      "Average test loss: 0.0014920168882235885\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02919518718123436\n",
      "Average test loss: 0.0014718351292734345\n",
      "Epoch 60/300\n",
      "Average training loss: 0.028856679457757207\n",
      "Average test loss: 0.0016108451714325282\n",
      "Epoch 61/300\n",
      "Average training loss: 0.027841752254300647\n",
      "Average test loss: 0.0013795506699631611\n",
      "Epoch 62/300\n",
      "Average training loss: 0.027186590605311922\n",
      "Average test loss: 0.0019192633924798833\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02716036183304257\n",
      "Average test loss: 0.0014127719215013914\n",
      "Epoch 64/300\n",
      "Average training loss: 0.028269680912295977\n",
      "Average test loss: 0.0013490867637511756\n",
      "Epoch 65/300\n",
      "Average training loss: 0.025786663504938283\n",
      "Average test loss: 0.005474696716914574\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02552531819873386\n",
      "Average test loss: 0.0013577860360965133\n",
      "Epoch 67/300\n",
      "Average training loss: 0.025139368888404633\n",
      "Average test loss: 0.001318377201962802\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024816796178619066\n",
      "Average test loss: 0.001313489692699578\n",
      "Epoch 69/300\n",
      "Average training loss: 0.025020813064442742\n",
      "Average test loss: 0.0012950934457282225\n",
      "Epoch 70/300\n",
      "Average training loss: 0.025708354226417013\n",
      "Average test loss: 0.0013021297191476656\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0247221127251784\n",
      "Average test loss: 0.0013456365029431052\n",
      "Epoch 72/300\n",
      "Average training loss: 0.023957676601078774\n",
      "Average test loss: 0.0013720159644467964\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02444623485373126\n",
      "Average test loss: 0.0013047932570593225\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02371312920914756\n",
      "Average test loss: 0.0012629317388766343\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0230426490654548\n",
      "Average test loss: 0.0012644492818766999\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02316207165684965\n",
      "Average test loss: 0.0015319782050016025\n",
      "Epoch 77/300\n",
      "Average training loss: 0.023187473573618465\n",
      "Average test loss: 0.0012687963642593887\n",
      "Epoch 78/300\n",
      "Average training loss: 0.022592091855075623\n",
      "Average test loss: 0.0025793572436604234\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02239994010825952\n",
      "Average test loss: 0.005546543937797348\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02230984273304542\n",
      "Average test loss: 0.0013336143305318223\n",
      "Epoch 81/300\n",
      "Average training loss: 0.022281707912683486\n",
      "Average test loss: 0.001325698129211863\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022144446714056864\n",
      "Average test loss: 0.001413443411907388\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02181693625781271\n",
      "Average test loss: 0.0012314626611769199\n",
      "Epoch 84/300\n",
      "Average training loss: 0.022053657902611625\n",
      "Average test loss: 0.001454069520605521\n",
      "Epoch 85/300\n",
      "Average training loss: 0.021185697242617606\n",
      "Average test loss: 0.0012424986801213688\n",
      "Epoch 86/300\n",
      "Average training loss: 0.021303733420868713\n",
      "Average test loss: 0.0012216981758053104\n",
      "Epoch 87/300\n",
      "Average training loss: 0.021651043249501123\n",
      "Average test loss: 0.0035252304987774953\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020951852772798804\n",
      "Average test loss: 0.0013776614049242602\n",
      "Epoch 89/300\n",
      "Average training loss: 0.021156865348418555\n",
      "Average test loss: 0.0024693190844522583\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02106916508740849\n",
      "Average test loss: 0.0012051605923722188\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020371767264273432\n",
      "Average test loss: 0.0017876152429833181\n",
      "Epoch 92/300\n",
      "Average training loss: 0.020733756778968705\n",
      "Average test loss: 0.0012780080187237924\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02006507931235764\n",
      "Average test loss: 0.001232144997196479\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02023304134607315\n",
      "Average test loss: 11056.731020833333\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020513349039687052\n",
      "Average test loss: 0.0016198897310015228\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01993147497375806\n",
      "Average test loss: 0.003947834247930182\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02030272911157873\n",
      "Average test loss: 0.0012166800655217635\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01972212467673752\n",
      "Average test loss: 0.0017617911694881817\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019606889622079002\n",
      "Average test loss: 0.0029880008122159375\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019147764648000398\n",
      "Average test loss: 0.0012226504247325161\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019828069734076657\n",
      "Average test loss: 0.0016588258339713018\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01940057419406043\n",
      "Average test loss: 0.001463150193914771\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018868902537557814\n",
      "Average test loss: 0.0012897288935879866\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018907934313019115\n",
      "Average test loss: 0.006609693470100562\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018971513632271026\n",
      "Average test loss: 0.001301428862226506\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018624986976385118\n",
      "Average test loss: 0.0012374775427290135\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0186285051604112\n",
      "Average test loss: 0.0012104382757614884\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018925260345141092\n",
      "Average test loss: 0.001264359893794689\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018742744024429055\n",
      "Average test loss: 0.0012866751087001627\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018472301627198855\n",
      "Average test loss: 0.0012627328795691332\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01810311227457391\n",
      "Average test loss: 0.0012712502415395445\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018182123735547066\n",
      "Average test loss: 0.00163967540829132\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01822298187845283\n",
      "Average test loss: 0.0012682609897520806\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019114567870895067\n",
      "Average test loss: 0.0054422657239354315\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017701632464925447\n",
      "Average test loss: 0.0013261508466675877\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03941623457769553\n",
      "Average test loss: 0.0013047186152802574\n",
      "Epoch 117/300\n",
      "Average training loss: 0.024611064515180057\n",
      "Average test loss: 0.007957670190061132\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021834322144587835\n",
      "Average test loss: 0.0026699584819790388\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02040266753733158\n",
      "Average test loss: 0.0012629255497207245\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01949852032131619\n",
      "Average test loss: 0.0013058146335598495\n",
      "Epoch 121/300\n",
      "Average training loss: 0.018673966026968428\n",
      "Average test loss: 0.0013239420340913864\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018354914534423086\n",
      "Average test loss: 0.0014014990116573043\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017987744366957083\n",
      "Average test loss: 0.0013261996671143504\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018117551035351225\n",
      "Average test loss: 0.0030829039683772457\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01772075998203622\n",
      "Average test loss: 0.0012532412636921638\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018115010364188087\n",
      "Average test loss: 0.0012360288249328732\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017716850209567283\n",
      "Average test loss: 0.0012652797680348159\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018290833745565678\n",
      "Average test loss: 0.001262023075018078\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01756212583515379\n",
      "Average test loss: 0.0013038911915694675\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01749607990682125\n",
      "Average test loss: 0.0023793779112812547\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01738809015436305\n",
      "Average test loss: 0.0018018642658781674\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01739012317276663\n",
      "Average test loss: 0.0012558716647326947\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01755462560471561\n",
      "Average test loss: 0.001485924201293124\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017264739328788387\n",
      "Average test loss: 0.003655925319219629\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01722242633998394\n",
      "Average test loss: 0.0012526555554423894\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019844490194486246\n",
      "Average test loss: 0.001334273147603704\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017147115944988196\n",
      "Average test loss: 0.06133045141730044\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017594866916537284\n",
      "Average test loss: 0.0023608094594544833\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017221384975645276\n",
      "Average test loss: 0.001299600210144288\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016775167293018766\n",
      "Average test loss: 0.0012813219686763154\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01716059168510967\n",
      "Average test loss: 0.0012937840426133738\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017006591776178942\n",
      "Average test loss: 0.0014421186865203911\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01693309055103196\n",
      "Average test loss: 0.0016626716275802917\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017100268670254284\n",
      "Average test loss: 0.001288128317333758\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01669917662938436\n",
      "Average test loss: 0.0015671674178706276\n",
      "Epoch 146/300\n",
      "Average training loss: 0.020554204042587017\n",
      "Average test loss: 0.0015528206809734305\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01725308371749189\n",
      "Average test loss: 0.0012517379357789953\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01655693946033716\n",
      "Average test loss: 0.0012850387073639366\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017026753501759636\n",
      "Average test loss: 0.001494682125850684\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01656890806224611\n",
      "Average test loss: 0.0012931035266568263\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016839609472288027\n",
      "Average test loss: 0.008174670220249229\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017050915396875806\n",
      "Average test loss: 0.001335173468829857\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016417626012530592\n",
      "Average test loss: 0.0013767147672673066\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017170382972392772\n",
      "Average test loss: 0.0013249788000765773\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016690727031065358\n",
      "Average test loss: 0.0013090123350007667\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016459551578594578\n",
      "Average test loss: 0.0013305840194225312\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016473875734541153\n",
      "Average test loss: 0.0013145097518960636\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017890228547983698\n",
      "Average test loss: 0.001490318512233595\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016125380702316762\n",
      "Average test loss: 0.0013318058696782423\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016022735698355568\n",
      "Average test loss: 0.0018383797739321987\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0162070475137896\n",
      "Average test loss: 0.0013098173229437735\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01609605173352692\n",
      "Average test loss: 0.00130682960856292\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01636035211963786\n",
      "Average test loss: 0.0013867575318242114\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01940975808352232\n",
      "Average test loss: 0.0013533390656941466\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016088303219113084\n",
      "Average test loss: 0.0013748264531087545\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0158155745110578\n",
      "Average test loss: 0.0013183436040870018\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016012210632363955\n",
      "Average test loss: 0.0015427161920815707\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016474728744063114\n",
      "Average test loss: 0.003867330169926087\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01596051846444607\n",
      "Average test loss: 0.0023404713950844276\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016782749517096414\n",
      "Average test loss: 0.002305868280948036\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015816674042079183\n",
      "Average test loss: 0.0016876727984183365\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015908067052563032\n",
      "Average test loss: 0.0013053982394954397\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016144045410884753\n",
      "Average test loss: 0.001316395165424587\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01777307877772384\n",
      "Average test loss: 0.09830645274122556\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015978469969497786\n",
      "Average test loss: 0.0012748837436859807\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01565272351519929\n",
      "Average test loss: 0.0014289328779818282\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015778058376577166\n",
      "Average test loss: 0.0013118657739832997\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016276555505063798\n",
      "Average test loss: 0.0012959585804492235\n",
      "Epoch 179/300\n",
      "Average training loss: 0.016243504418267146\n",
      "Average test loss: 0.0025232434231374\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015755945917632845\n",
      "Average test loss: 0.0023593252058037454\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01570416364239322\n",
      "Average test loss: 0.0012707887635462814\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015703996662464408\n",
      "Average test loss: 0.0013006903264257642\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01625003091659811\n",
      "Average test loss: 0.07968578850560717\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015664581143193773\n",
      "Average test loss: 0.024298613977515034\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015482343286275863\n",
      "Average test loss: 0.0012806873716827896\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016259118549525737\n",
      "Average test loss: 0.0015711979773930377\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015557739897734589\n",
      "Average test loss: 0.0013677684441726241\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015393203559021155\n",
      "Average test loss: 0.0016052176074849234\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016108643904742266\n",
      "Average test loss: 0.0015053522152205308\n",
      "Epoch 190/300\n",
      "Average training loss: 0.029879962497287325\n",
      "Average test loss: 0.0013386538135301735\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02169231098724736\n",
      "Average test loss: 0.0012929893408177627\n",
      "Epoch 192/300\n",
      "Average training loss: 0.018740013529857\n",
      "Average test loss: 0.0016688829842540953\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01710619472298357\n",
      "Average test loss: 0.0013585303216758701\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01617337228440576\n",
      "Average test loss: 0.0014093824686068628\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015695729246570006\n",
      "Average test loss: 0.0016451506496717532\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015971898600459097\n",
      "Average test loss: 0.0028414897666209273\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015457911574178272\n",
      "Average test loss: 0.001296661691636675\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015423872617383799\n",
      "Average test loss: 0.0013579670981917944\n",
      "Epoch 199/300\n",
      "Average training loss: 0.015392747979197237\n",
      "Average test loss: 0.002531806539951099\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015366901867919497\n",
      "Average test loss: 211460371.9537778\n",
      "Epoch 201/300\n",
      "Average training loss: 0.015420142696135574\n",
      "Average test loss: 0.001404919950188034\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01545209910803371\n",
      "Average test loss: 0.001514064116610421\n",
      "Epoch 203/300\n",
      "Average training loss: 0.015564115041659938\n",
      "Average test loss: 0.0013212496133314239\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015337177955442005\n",
      "Average test loss: 7.626922752221425\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015777874734666613\n",
      "Average test loss: 0.001321556113505115\n",
      "Epoch 206/300\n",
      "Average training loss: 0.015421556528243754\n",
      "Average test loss: 0.0013863504400683773\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015493439882993698\n",
      "Average test loss: 0.001580191095928765\n",
      "Epoch 208/300\n",
      "Average training loss: 0.015201393876638677\n",
      "Average test loss: 0.0013182517602108419\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015619474026064077\n",
      "Average test loss: 0.0012981789101743036\n",
      "Epoch 210/300\n",
      "Average training loss: 0.015352647178702885\n",
      "Average test loss: 0.001492686990958949\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015382403458158176\n",
      "Average test loss: 0.0020125388597241707\n",
      "Epoch 212/300\n",
      "Average training loss: 0.015227916513052251\n",
      "Average test loss: 0.0014052801361928383\n",
      "Epoch 213/300\n",
      "Average training loss: 0.015220978894995319\n",
      "Average test loss: 44.45448028288947\n",
      "Epoch 214/300\n",
      "Average training loss: 0.015267231868373024\n",
      "Average test loss: 0.0012951304900149505\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015001142028305266\n",
      "Average test loss: 0.0014779732853381171\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015580183871918254\n",
      "Average test loss: 0.0025556917095349895\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015043130487203599\n",
      "Average test loss: 0.001309734696832796\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014966974572175079\n",
      "Average test loss: 0.0013865971121316155\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015321753674911127\n",
      "Average test loss: 0.0013376884154147571\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017899653549823497\n",
      "Average test loss: 0.0016856844454175895\n",
      "Epoch 221/300\n",
      "Average training loss: 0.015148128781053755\n",
      "Average test loss: 0.0014394046726843549\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014828583831588428\n",
      "Average test loss: 0.0013436963197050824\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01757157001727157\n",
      "Average test loss: 0.00203945745424264\n",
      "Epoch 224/300\n",
      "Average training loss: 0.015553667989869912\n",
      "Average test loss: 0.0014274478273259268\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014821755554113122\n",
      "Average test loss: 0.0015441157184541226\n",
      "Epoch 226/300\n",
      "Average training loss: 0.015675087897313967\n",
      "Average test loss: 0.0018759086286235186\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014859435314933458\n",
      "Average test loss: 0.007162152719994386\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01516364494462808\n",
      "Average test loss: 0.0013857634162219863\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015008489635255601\n",
      "Average test loss: 0.0012865669103339313\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014765877847042348\n",
      "Average test loss: 0.001358879765521528\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014905103450020154\n",
      "Average test loss: 0.0013231849311333564\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016144079871475696\n",
      "Average test loss: 0.0013439657183157074\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01566705382284191\n",
      "Average test loss: 0.0013059627204719518\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014594833523862892\n",
      "Average test loss: 0.001340805407613516\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01488655920409494\n",
      "Average test loss: 0.0013409004470126497\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014829219645924038\n",
      "Average test loss: 0.0016289422631056772\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015993915788001484\n",
      "Average test loss: 0.0013093229497058525\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015579850380619367\n",
      "Average test loss: 0.001342013753950596\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014594651646084256\n",
      "Average test loss: 0.0013837080761376355\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014657992647753821\n",
      "Average test loss: 0.00130384782805211\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014630487617519167\n",
      "Average test loss: 0.0013801422321961985\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014835096187061733\n",
      "Average test loss: 0.0013579103223358591\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014813473345504867\n",
      "Average test loss: 0.0016570090199303296\n",
      "Epoch 244/300\n",
      "Average training loss: 0.015046522878938252\n",
      "Average test loss: 0.0013252482447876697\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01456594399197234\n",
      "Average test loss: 0.0021957255811947914\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0160487119588587\n",
      "Average test loss: 0.0017294446910834974\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014933304220438004\n",
      "Average test loss: 0.0017483358595313297\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014875048565367857\n",
      "Average test loss: 0.0013655844553787674\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014985696414278613\n",
      "Average test loss: 0.002735218228565322\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014469204031758838\n",
      "Average test loss: 0.0013539812992482136\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014474185523059633\n",
      "Average test loss: 0.001377278881561425\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01497778099609746\n",
      "Average test loss: 0.002191058229551547\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014673620865576798\n",
      "Average test loss: 0.001662089091208246\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01458652039948437\n",
      "Average test loss: 0.0013328875282572375\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014649698106778993\n",
      "Average test loss: 0.014317083524333106\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016357654741240873\n",
      "Average test loss: 0.0017237180220997996\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01559595326334238\n",
      "Average test loss: 0.0013407461803096037\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014398411096798049\n",
      "Average test loss: 0.0013400627173897294\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014395984023809432\n",
      "Average test loss: 0.0013844564570527937\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014951485408676995\n",
      "Average test loss: 0.010470585439768103\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014495162068141831\n",
      "Average test loss: 0.001342416419958075\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014333279185824924\n",
      "Average test loss: 0.0013452291607649789\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015644328848355347\n",
      "Average test loss: 0.0013561711082648899\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014278940466543039\n",
      "Average test loss: 0.0013836724388723571\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01428312471260627\n",
      "Average test loss: 0.001326166976760659\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01432342462324434\n",
      "Average test loss: 0.0013606502416854103\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01451768235862255\n",
      "Average test loss: 0.001387209069604675\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01458153860270977\n",
      "Average test loss: 86.05197502983941\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015227370308505165\n",
      "Average test loss: 0.0013828978112174406\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014385991657773654\n",
      "Average test loss: 0.0017017257553007868\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01469278965642055\n",
      "Average test loss: 0.0013475635688131055\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014484461572435168\n",
      "Average test loss: 0.0013513825265690684\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01555502821918991\n",
      "Average test loss: 0.0013764211990767055\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014682761386864715\n",
      "Average test loss: 0.0013260347277133\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014226204495463106\n",
      "Average test loss: 0.001491668884952863\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014172589157190588\n",
      "Average test loss: 0.001327693090153237\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014369386388195885\n",
      "Average test loss: 0.0013252220569281942\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01564409154239628\n",
      "Average test loss: 0.0013413881400807035\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01529654807349046\n",
      "Average test loss: 0.0013258520742464397\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01407843374626504\n",
      "Average test loss: 0.0013679071276241706\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014111232076254155\n",
      "Average test loss: 0.0015570103083219792\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014232229734460512\n",
      "Average test loss: 0.0020193133587017653\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014336832582950591\n",
      "Average test loss: 0.001401438588793907\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014423963313301404\n",
      "Average test loss: 0.001408353867319723\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014172056641843584\n",
      "Average test loss: 0.0013721372772836024\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015304408629735311\n",
      "Average test loss: 0.001684741003645791\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01573620627158218\n",
      "Average test loss: 0.001315051248607536\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014478948555058904\n",
      "Average test loss: 0.005827198489465647\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014122754652467038\n",
      "Average test loss: 0.00133837936570247\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014062115743756294\n",
      "Average test loss: 0.0014761868774270017\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014472955115139484\n",
      "Average test loss: 0.001632255844357941\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01406483010533783\n",
      "Average test loss: 0.0014125512347867091\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014445664967927667\n",
      "Average test loss: 0.0033404083982523946\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0146044775231017\n",
      "Average test loss: 0.001350496606539107\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014023019704553816\n",
      "Average test loss: 0.0013703608920073345\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014430129340953297\n",
      "Average test loss: 0.0028439645282406774\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013978398407498996\n",
      "Average test loss: 0.001480743520686196\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015429319572945436\n",
      "Average test loss: 0.0021050983487317957\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014850565047727691\n",
      "Average test loss: 0.0014635786117158003\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014240469684203465\n",
      "Average test loss: 0.0013390328718556298\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.95/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 19.47\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.51\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.47\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.11\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 23.92\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.40\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.90\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.17\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.31\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.64\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.81\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.99\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.22\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.24\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.38\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.51\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.00\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.15\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.42\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.41\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.59\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.60\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.78\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.38\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.76\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.04\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.60\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.60\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.85\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.10\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.61\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.91\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.93\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.82\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.09\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.27\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.45\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.62\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.79\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.89\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.85\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.92\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.43\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.82\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.24\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.24\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.39\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.86\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.79\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.12\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.45\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.99\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.16\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.30\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.21\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.15\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.24\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.60\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.88\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 34.00\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.97\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 34.17\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 34.41\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 34.39\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.36\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
