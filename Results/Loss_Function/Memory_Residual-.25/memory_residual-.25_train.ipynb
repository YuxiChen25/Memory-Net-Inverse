{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.25)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.042960980539520584\n",
      "Average test loss: 0.011390960566699505\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015603592342800563\n",
      "Average test loss: 0.009589469021807115\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013865230699380239\n",
      "Average test loss: 0.009184941369626257\n",
      "Epoch 4/300\n",
      "Average training loss: 0.012974060365723239\n",
      "Average test loss: 0.008502535868436098\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012336067604521911\n",
      "Average test loss: 0.008531822625133726\n",
      "Epoch 6/300\n",
      "Average training loss: 0.011760513368580076\n",
      "Average test loss: 0.008800127218580908\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011184872746467591\n",
      "Average test loss: 0.007853030977149804\n",
      "Epoch 8/300\n",
      "Average training loss: 0.010775478868848748\n",
      "Average test loss: 0.007616426826351219\n",
      "Epoch 9/300\n",
      "Average training loss: 0.010497133646574286\n",
      "Average test loss: 0.0076063157659437925\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01029369692173269\n",
      "Average test loss: 0.007452190695537461\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010086696498923832\n",
      "Average test loss: 0.007220578935825162\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009863335620197984\n",
      "Average test loss: 0.007211361983170112\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009655350039402644\n",
      "Average test loss: 0.007076773994084862\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009505887332061926\n",
      "Average test loss: 0.007162480382869641\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009403020896845393\n",
      "Average test loss: 0.007294305114282502\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009223131209611893\n",
      "Average test loss: 0.006955695314125882\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009142205474277337\n",
      "Average test loss: 0.007070132483624749\n",
      "Epoch 18/300\n",
      "Average training loss: 0.008994394834670755\n",
      "Average test loss: 0.006694881401956081\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008852760758664873\n",
      "Average test loss: 0.006750763682027658\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00876785900650753\n",
      "Average test loss: 0.006650164729605119\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00866091535654333\n",
      "Average test loss: 0.006584709900534815\n",
      "Epoch 22/300\n",
      "Average training loss: 0.008570311259892252\n",
      "Average test loss: 0.006940703773250182\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008504688902861542\n",
      "Average test loss: 0.006569684228963322\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008382629590729872\n",
      "Average test loss: 0.006465333043701119\n",
      "Epoch 25/300\n",
      "Average training loss: 0.008349959267510308\n",
      "Average test loss: 0.006490379240363837\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008267775995035967\n",
      "Average test loss: 0.006407074376112885\n",
      "Epoch 27/300\n",
      "Average training loss: 0.008187770236697462\n",
      "Average test loss: 0.0064937354429728455\n",
      "Epoch 28/300\n",
      "Average training loss: 0.008116869223614534\n",
      "Average test loss: 0.006521123100486067\n",
      "Epoch 29/300\n",
      "Average training loss: 0.008057243670854303\n",
      "Average test loss: 0.006613067394329442\n",
      "Epoch 30/300\n",
      "Average training loss: 0.008045411457618078\n",
      "Average test loss: 0.007463371968103779\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007932256333943871\n",
      "Average test loss: 0.006346082984987233\n",
      "Epoch 32/300\n",
      "Average training loss: 0.007880133246382077\n",
      "Average test loss: 0.006279783475730155\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007843387581821945\n",
      "Average test loss: 0.006592109336621231\n",
      "Epoch 34/300\n",
      "Average training loss: 0.007842388684550922\n",
      "Average test loss: 0.006480219753252136\n",
      "Epoch 35/300\n",
      "Average training loss: 0.007734633241262701\n",
      "Average test loss: 0.006334596217506461\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007699225438137849\n",
      "Average test loss: 0.006515539414766762\n",
      "Epoch 37/300\n",
      "Average training loss: 0.007636561519569821\n",
      "Average test loss: 0.006269549200932185\n",
      "Epoch 38/300\n",
      "Average training loss: 0.007616295359614823\n",
      "Average test loss: 0.006263526805159118\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007569360299242867\n",
      "Average test loss: 0.0063756154278914135\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0075488286105295024\n",
      "Average test loss: 0.006829286475562387\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00748708702954981\n",
      "Average test loss: 0.006242657599349817\n",
      "Epoch 42/300\n",
      "Average training loss: 0.007468271833327081\n",
      "Average test loss: 0.006311241405705611\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007447558906757169\n",
      "Average test loss: 0.006206089797947142\n",
      "Epoch 44/300\n",
      "Average training loss: 0.00739150891204675\n",
      "Average test loss: 0.006320797438422839\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007365157413399882\n",
      "Average test loss: 0.006188722857998477\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0073439253725939325\n",
      "Average test loss: 0.006494584568258789\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007326520228137573\n",
      "Average test loss: 0.0064001688307358156\n",
      "Epoch 48/300\n",
      "Average training loss: 0.007289548124704096\n",
      "Average test loss: 0.0063427635460264154\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007225034786181317\n",
      "Average test loss: 0.006343536525136894\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007236525843540827\n",
      "Average test loss: 0.0063981056834260626\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007174811902973387\n",
      "Average test loss: 0.006282556769334608\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007165543506956763\n",
      "Average test loss: 0.006356453195214272\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007161499198526144\n",
      "Average test loss: 0.006423150293529033\n",
      "Epoch 54/300\n",
      "Average training loss: 0.007106219065272146\n",
      "Average test loss: 0.006229760485390822\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007084363623625702\n",
      "Average test loss: 0.006411620508465502\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007090637449589041\n",
      "Average test loss: 0.0062039831653237345\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0070335642612642715\n",
      "Average test loss: 0.006328343510627747\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007050467746332288\n",
      "Average test loss: 0.006489056217380696\n",
      "Epoch 59/300\n",
      "Average training loss: 0.006986746818655067\n",
      "Average test loss: 0.006674030183917946\n",
      "Epoch 60/300\n",
      "Average training loss: 0.00697485574003723\n",
      "Average test loss: 0.006243586314635144\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006954365645017889\n",
      "Average test loss: 0.006279553997847769\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006967604495584965\n",
      "Average test loss: 0.006304437892304527\n",
      "Epoch 63/300\n",
      "Average training loss: 0.006932197947882943\n",
      "Average test loss: 0.0062922406440807715\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006906852586401833\n",
      "Average test loss: 0.006169541529483265\n",
      "Epoch 65/300\n",
      "Average training loss: 0.006922083862539795\n",
      "Average test loss: 0.034190563864178125\n",
      "Epoch 66/300\n",
      "Average training loss: 0.006903429558707608\n",
      "Average test loss: 0.006232386807600657\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00684609207097027\n",
      "Average test loss: 0.0062348944006694685\n",
      "Epoch 68/300\n",
      "Average training loss: 0.006832650112609069\n",
      "Average test loss: 0.0062737902365624905\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006816967933128277\n",
      "Average test loss: 0.006192301192631324\n",
      "Epoch 70/300\n",
      "Average training loss: 0.006812251848479112\n",
      "Average test loss: 0.006253306440181202\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0067750594963630045\n",
      "Average test loss: 0.006317507409387165\n",
      "Epoch 72/300\n",
      "Average training loss: 0.006782880656421185\n",
      "Average test loss: 0.006576919534968005\n",
      "Epoch 73/300\n",
      "Average training loss: 0.006758704848173592\n",
      "Average test loss: 0.006229795124795702\n",
      "Epoch 74/300\n",
      "Average training loss: 0.006738689058356815\n",
      "Average test loss: 0.006176787081484994\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0067111480687227516\n",
      "Average test loss: 0.006141563144408994\n",
      "Epoch 76/300\n",
      "Average training loss: 0.006712522226903174\n",
      "Average test loss: 0.006175526753895813\n",
      "Epoch 77/300\n",
      "Average training loss: 0.006693279942704571\n",
      "Average test loss: 0.006316550510625045\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006710608533687061\n",
      "Average test loss: 0.006331654784787032\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006683605640712712\n",
      "Average test loss: 0.006232879083189699\n",
      "Epoch 80/300\n",
      "Average training loss: 0.00665094183302588\n",
      "Average test loss: 0.006439316438717975\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006647424368394746\n",
      "Average test loss: 0.007121122261302339\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006680725387814972\n",
      "Average test loss: 0.006513433530926704\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006603284447971317\n",
      "Average test loss: 0.006166794863839945\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006624002027428812\n",
      "Average test loss: 0.006284071136265993\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00661367722062601\n",
      "Average test loss: 0.00623526051061021\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006593461955587069\n",
      "Average test loss: 0.006231292229145765\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006585277959704399\n",
      "Average test loss: 0.006388424417624871\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0065577198250426185\n",
      "Average test loss: 0.006469836970998181\n",
      "Epoch 89/300\n",
      "Average training loss: 0.006579051275634103\n",
      "Average test loss: 0.006662706904941135\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006546581044793129\n",
      "Average test loss: 0.006231195184919569\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006550281518449386\n",
      "Average test loss: 0.006296523615717888\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006503463076634539\n",
      "Average test loss: 0.006339350437952413\n",
      "Epoch 93/300\n",
      "Average training loss: 0.006522210287758045\n",
      "Average test loss: 0.0062772017336554\n",
      "Epoch 94/300\n",
      "Average training loss: 0.006497397298614184\n",
      "Average test loss: 0.006254839743177096\n",
      "Epoch 95/300\n",
      "Average training loss: 0.006492207663340701\n",
      "Average test loss: 0.006375345487975412\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006481873274263409\n",
      "Average test loss: 0.006204285595979956\n",
      "Epoch 97/300\n",
      "Average training loss: 0.006491515893489123\n",
      "Average test loss: 0.006227178356299798\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0064605391816132595\n",
      "Average test loss: 0.0062136623221966955\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006456435908873876\n",
      "Average test loss: 0.00629970838336481\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0064631348595851\n",
      "Average test loss: 0.006290099509474304\n",
      "Epoch 101/300\n",
      "Average training loss: 0.006442739866673946\n",
      "Average test loss: 0.006271759918994374\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006472782018697924\n",
      "Average test loss: 0.00628742608634962\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0063901708246105245\n",
      "Average test loss: 0.006272645504110389\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006402580606026782\n",
      "Average test loss: 0.006210304533441862\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006395663254376915\n",
      "Average test loss: 0.006268721973316537\n",
      "Epoch 106/300\n",
      "Average training loss: 0.006412446399529775\n",
      "Average test loss: 0.006523914957212077\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006376065537333489\n",
      "Average test loss: 0.006352494550247987\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006592372390545077\n",
      "Average test loss: 0.006874894761376911\n",
      "Epoch 109/300\n",
      "Average training loss: 0.006336967223634323\n",
      "Average test loss: 0.006433157368252675\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006351136645095216\n",
      "Average test loss: 0.006268471114751366\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0063367648774551024\n",
      "Average test loss: 0.006219933213873042\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006310354183117549\n",
      "Average test loss: 0.006314666912787491\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0063134766009946665\n",
      "Average test loss: 0.0063646825063559745\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006327468242496252\n",
      "Average test loss: 0.006376547635429435\n",
      "Epoch 119/300\n",
      "Average training loss: 0.006272511147376564\n",
      "Average test loss: 0.006347739421659046\n",
      "Epoch 120/300\n",
      "Average training loss: 0.006279635744376315\n",
      "Average test loss: 0.006256829941438304\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006289739405943288\n",
      "Average test loss: 0.006284751628835996\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006291832536872891\n",
      "Average test loss: 0.006425504773027367\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006282891223000155\n",
      "Average test loss: 0.0063191080362432536\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006246767835898532\n",
      "Average test loss: 0.006389313084383805\n",
      "Epoch 125/300\n",
      "Average training loss: 0.006253334810750352\n",
      "Average test loss: 0.00631172141722507\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0062497419764598215\n",
      "Average test loss: 0.006323294897046354\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006239922505699926\n",
      "Average test loss: 0.0063834385292397605\n",
      "Epoch 128/300\n",
      "Average training loss: 0.006243078996737798\n",
      "Average test loss: 0.006503281136353811\n",
      "Epoch 129/300\n",
      "Average training loss: 0.006215926514731513\n",
      "Average test loss: 0.00630889818403456\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006231528011461099\n",
      "Average test loss: 0.006327166023767657\n",
      "Epoch 131/300\n",
      "Average training loss: 0.006234037332236767\n",
      "Average test loss: 0.006298693011618323\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006206972266650862\n",
      "Average test loss: 0.006607411671015952\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006194766718894244\n",
      "Average test loss: 0.006399992816150188\n",
      "Epoch 134/300\n",
      "Average training loss: 0.006190720033314493\n",
      "Average test loss: 0.006174052966551648\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006219622591717376\n",
      "Average test loss: 0.006275714532368713\n",
      "Epoch 136/300\n",
      "Average training loss: 0.006171161769578854\n",
      "Average test loss: 0.006487206396543317\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006182393558323384\n",
      "Average test loss: 0.00658990560265051\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006275560648904907\n",
      "Average test loss: 0.006234176898168193\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006148115339378516\n",
      "Average test loss: 0.006331848552243577\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006151804871443245\n",
      "Average test loss: 0.0064043722256190245\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006163936274747054\n",
      "Average test loss: 0.0073717201981279585\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006183465044531557\n",
      "Average test loss: 0.00649067076916496\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006152317913870017\n",
      "Average test loss: 0.006424237238036262\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00612167234511839\n",
      "Average test loss: 0.006293281595740053\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0061461955888403785\n",
      "Average test loss: 0.006521624777466059\n",
      "Epoch 146/300\n",
      "Average training loss: 0.006139154873374435\n",
      "Average test loss: 0.006382574571917455\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006125631266170078\n",
      "Average test loss: 0.006294974584960276\n",
      "Epoch 148/300\n",
      "Average training loss: 0.006126278205878205\n",
      "Average test loss: 0.006423540980865558\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006118465536998378\n",
      "Average test loss: 0.006274415315439303\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006106184829854303\n",
      "Average test loss: 0.0061894291283355815\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0060886179647511906\n",
      "Average test loss: 0.006966665588319302\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00608949903688497\n",
      "Average test loss: 0.006694762025028467\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006089263290166855\n",
      "Average test loss: 0.006689540362192525\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006114026919421222\n",
      "Average test loss: 0.006516669294072522\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006088415455073118\n",
      "Average test loss: 0.006266345881339576\n",
      "Epoch 156/300\n",
      "Average training loss: 0.006042753100395202\n",
      "Average test loss: 0.006426853159649504\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006034320705880721\n",
      "Average test loss: 0.006328941436691417\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006055127354131804\n",
      "Average test loss: 0.006339174717664719\n",
      "Epoch 164/300\n",
      "Average training loss: 0.006039121663818757\n",
      "Average test loss: 0.006514007468190458\n",
      "Epoch 165/300\n",
      "Average training loss: 0.006034706593387657\n",
      "Average test loss: 0.006172760385606024\n",
      "Epoch 166/300\n",
      "Average training loss: 0.006040371789286534\n",
      "Average test loss: 0.006451011579483748\n",
      "Epoch 167/300\n",
      "Average training loss: 0.006024618378116025\n",
      "Average test loss: 0.006473944945467843\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006004755589283175\n",
      "Average test loss: 0.006424274705350399\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00600876587919063\n",
      "Average test loss: 0.0064384858260552085\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00600417490883006\n",
      "Average test loss: 0.0064504889237384\n",
      "Epoch 171/300\n",
      "Average training loss: 0.005999776800887452\n",
      "Average test loss: 0.006354901344825824\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00602050395268533\n",
      "Average test loss: 0.006405699404163493\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00601418591166536\n",
      "Average test loss: 0.006363771888944838\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00599666275911861\n",
      "Average test loss: 0.0065209598156313105\n",
      "Epoch 175/300\n",
      "Average training loss: 0.005977460955166154\n",
      "Average test loss: 0.00640657939761877\n",
      "Epoch 176/300\n",
      "Average training loss: 0.005988216903474596\n",
      "Average test loss: 0.0070968388385242885\n",
      "Epoch 177/300\n",
      "Average training loss: 0.00596127779657642\n",
      "Average test loss: 0.007211093501912223\n",
      "Epoch 178/300\n",
      "Average training loss: 0.005966748826205731\n",
      "Average test loss: 0.006614261077716946\n",
      "Epoch 179/300\n",
      "Average training loss: 0.005958214823156595\n",
      "Average test loss: 0.006347333668006791\n",
      "Epoch 180/300\n",
      "Average training loss: 0.00597726462326116\n",
      "Average test loss: 0.006577657501316733\n",
      "Epoch 181/300\n",
      "Average training loss: 0.005976695254858997\n",
      "Average test loss: 0.0063930428222649625\n",
      "Epoch 182/300\n",
      "Average training loss: 0.005959308864755763\n",
      "Average test loss: 0.006518505185635554\n",
      "Epoch 183/300\n",
      "Average training loss: 0.005966750536113978\n",
      "Average test loss: 0.0062616988776458635\n",
      "Epoch 184/300\n",
      "Average training loss: 0.005954161499523454\n",
      "Average test loss: 0.007122792425254981\n",
      "Epoch 185/300\n",
      "Average training loss: 0.005983458770645989\n",
      "Average test loss: 0.006577418827762207\n",
      "Epoch 186/300\n",
      "Average training loss: 0.005934002080311378\n",
      "Average test loss: 0.0064650229451557\n",
      "Epoch 187/300\n",
      "Average training loss: 0.005927664313465357\n",
      "Average test loss: 0.0064284251535104385\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0059176026545465\n",
      "Average test loss: 0.006360551820033126\n",
      "Epoch 189/300\n",
      "Average training loss: 0.005935193253060182\n",
      "Average test loss: 0.006600555082990064\n",
      "Epoch 190/300\n",
      "Average training loss: 0.005947596567786402\n",
      "Average test loss: 0.006506000733209981\n",
      "Epoch 191/300\n",
      "Average training loss: 0.005920343805518416\n",
      "Average test loss: 0.006309081009692616\n",
      "Epoch 192/300\n",
      "Average training loss: 0.005913612509767214\n",
      "Average test loss: 0.006520516883581877\n",
      "Epoch 193/300\n",
      "Average training loss: 0.005908775286955966\n",
      "Average test loss: 0.00638025862392452\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00600318143558171\n",
      "Average test loss: 0.0064636246123247676\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005874984480026695\n",
      "Average test loss: 0.006450591521544589\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005871429550564951\n",
      "Average test loss: 0.006507042974647549\n",
      "Epoch 197/300\n",
      "Average training loss: 0.005893266402598884\n",
      "Average test loss: 0.00642742996621463\n",
      "Epoch 198/300\n",
      "Average training loss: 0.005892983334759871\n",
      "Average test loss: 0.006966723066237238\n",
      "Epoch 199/300\n",
      "Average training loss: 0.005888076461851597\n",
      "Average test loss: 0.006517334665689203\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00591238842656215\n",
      "Average test loss: 0.006614686680750714\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0058927886647482716\n",
      "Average test loss: 0.0064779294576081965\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00587839834847384\n",
      "Average test loss: 0.00631241538334224\n",
      "Epoch 203/300\n",
      "Average training loss: 0.005882160176419549\n",
      "Average test loss: 0.006319523389554686\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005920428706540002\n",
      "Average test loss: 0.006651478122505877\n",
      "Epoch 205/300\n",
      "Average training loss: 0.005874156731698248\n",
      "Average test loss: 0.006441913371284803\n",
      "Epoch 206/300\n",
      "Average training loss: 0.005873633297781149\n",
      "Average test loss: 0.006362266324460506\n",
      "Epoch 207/300\n",
      "Average training loss: 0.005846779037680891\n",
      "Average test loss: 0.006389879848394129\n",
      "Epoch 208/300\n",
      "Average training loss: 0.005865126251760456\n",
      "Average test loss: 0.0064320309178696735\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0058613103036251335\n",
      "Average test loss: 0.006977949074986908\n",
      "Epoch 210/300\n",
      "Average training loss: 0.005847480706042713\n",
      "Average test loss: 0.006508403811189864\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0058578680600557065\n",
      "Average test loss: 0.0069763505156669355\n",
      "Epoch 212/300\n",
      "Average training loss: 0.005846461995608277\n",
      "Average test loss: 0.006380313834382428\n",
      "Epoch 213/300\n",
      "Average training loss: 0.005865953867634137\n",
      "Average test loss: 0.006644448449628221\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0058351107454962204\n",
      "Average test loss: 0.006327689981708924\n",
      "Epoch 215/300\n",
      "Average training loss: 0.005837803115861283\n",
      "Average test loss: 0.006630392600678736\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005831434571080737\n",
      "Average test loss: 0.006751444569478432\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005838508195347256\n",
      "Average test loss: 0.006455440349462959\n",
      "Epoch 218/300\n",
      "Average training loss: 0.005826488396359814\n",
      "Average test loss: 0.0064714463568396035\n",
      "Epoch 219/300\n",
      "Average training loss: 0.005829467435263925\n",
      "Average test loss: 0.006324954742358791\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0058223499268707304\n",
      "Average test loss: 0.0063873114047778975\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0058481930995153055\n",
      "Average test loss: 0.007053083657804463\n",
      "Epoch 222/300\n",
      "Average training loss: 0.005806178591731522\n",
      "Average test loss: 0.006558986713695857\n",
      "Epoch 223/300\n",
      "Average training loss: 0.005794403162267473\n",
      "Average test loss: 0.006472176945043935\n",
      "Epoch 224/300\n",
      "Average training loss: 0.005817299065904485\n",
      "Average test loss: 0.006593671537521812\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0058116347926358385\n",
      "Average test loss: 0.006359769540942377\n",
      "Epoch 226/300\n",
      "Average training loss: 0.005806237710432874\n",
      "Average test loss: 0.0069365448479851086\n",
      "Epoch 227/300\n",
      "Average training loss: 0.005799017389201456\n",
      "Average test loss: 0.006827015803092056\n",
      "Epoch 228/300\n",
      "Average training loss: 0.005817788870384296\n",
      "Average test loss: 0.006536211863574054\n",
      "Epoch 229/300\n",
      "Average training loss: 0.005764211042473714\n",
      "Average test loss: 0.006372030122412576\n",
      "Epoch 230/300\n",
      "Average training loss: 0.005783106382522318\n",
      "Average test loss: 0.006324724454846647\n",
      "Epoch 231/300\n",
      "Average training loss: 0.005804299381872018\n",
      "Average test loss: 0.006729333681364854\n",
      "Epoch 232/300\n",
      "Average training loss: 0.005824766255501244\n",
      "Average test loss: 0.006669284112751484\n",
      "Epoch 233/300\n",
      "Average training loss: 0.005797613222565915\n",
      "Average test loss: 0.0066349871572520995\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0057892309294806585\n",
      "Average test loss: 0.00638412285471956\n",
      "Epoch 235/300\n",
      "Average training loss: 0.005766029530101352\n",
      "Average test loss: 0.00634579996806052\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0057816813844773505\n",
      "Average test loss: 0.006330728432370557\n",
      "Epoch 237/300\n",
      "Average training loss: 0.005783846443312036\n",
      "Average test loss: 0.006509752358827326\n",
      "Epoch 238/300\n",
      "Average training loss: 0.005785641846557458\n",
      "Average test loss: 0.0064836598220798704\n",
      "Epoch 239/300\n",
      "Average training loss: 0.005774733865840567\n",
      "Average test loss: 0.00650911547905869\n",
      "Epoch 240/300\n",
      "Average training loss: 0.005755838869346512\n",
      "Average test loss: 0.006479745368162791\n",
      "Epoch 241/300\n",
      "Average training loss: 0.005767836102181011\n",
      "Average test loss: 0.006560210280948215\n",
      "Epoch 242/300\n",
      "Average training loss: 0.005777521020008458\n",
      "Average test loss: 0.006420408805211385\n",
      "Epoch 243/300\n",
      "Average training loss: 0.005759247315012746\n",
      "Average test loss: 0.0066335889038940275\n",
      "Epoch 244/300\n",
      "Average training loss: 0.005734665701372756\n",
      "Average test loss: 0.0065110065080225465\n",
      "Epoch 245/300\n",
      "Average training loss: 0.005736487006147702\n",
      "Average test loss: 0.0064754316413568125\n",
      "Epoch 246/300\n",
      "Average training loss: 0.005754095579600996\n",
      "Average test loss: 0.006411835595137543\n",
      "Epoch 247/300\n",
      "Average training loss: 0.005754438415997558\n",
      "Average test loss: 0.006453598828779327\n",
      "Epoch 248/300\n",
      "Average training loss: 0.005768225762165255\n",
      "Average test loss: 0.006481733666525946\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0057313334217502015\n",
      "Average test loss: 0.006435540527519252\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005753077398985625\n",
      "Average test loss: 0.006631502473519908\n",
      "Epoch 251/300\n",
      "Average training loss: 0.005748543300976356\n",
      "Average test loss: 0.006520099108417829\n",
      "Epoch 252/300\n",
      "Average training loss: 0.005721083936591943\n",
      "Average test loss: 0.006517403319891956\n",
      "Epoch 253/300\n",
      "Average training loss: 0.005734173843430148\n",
      "Average test loss: 0.006356954351067543\n",
      "Epoch 254/300\n",
      "Average training loss: 0.005731389724546009\n",
      "Average test loss: 0.006370690599911743\n",
      "Epoch 255/300\n",
      "Average training loss: 0.005725193162345224\n",
      "Average test loss: 0.006412815057155159\n",
      "Epoch 256/300\n",
      "Average training loss: 0.005708884319083558\n",
      "Average test loss: 0.00645029477112823\n",
      "Epoch 257/300\n",
      "Average training loss: 0.005717052936553955\n",
      "Average test loss: 0.00913584350877338\n",
      "Epoch 258/300\n",
      "Average training loss: 0.005733185614769657\n",
      "Average test loss: 0.006677295091665454\n",
      "Epoch 259/300\n",
      "Average training loss: 0.005685328622245126\n",
      "Average test loss: 0.006683176912367344\n",
      "Epoch 260/300\n",
      "Average training loss: 0.005724617353743977\n",
      "Average test loss: 0.00637777906200952\n",
      "Epoch 261/300\n",
      "Average training loss: 0.005709824478874604\n",
      "Average test loss: 0.006520067176471154\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00571957032299704\n",
      "Average test loss: 0.006792027585208416\n",
      "Epoch 263/300\n",
      "Average training loss: 0.005697215422987938\n",
      "Average test loss: 0.00647226427992185\n",
      "Epoch 264/300\n",
      "Average training loss: 0.005716209834234582\n",
      "Average test loss: 0.0066252590161230826\n",
      "Epoch 265/300\n",
      "Average training loss: 0.005738234374672174\n",
      "Average test loss: 0.006459931391394801\n",
      "Epoch 266/300\n",
      "Average training loss: 0.005689534310665396\n",
      "Average test loss: 0.006485427002939913\n",
      "Epoch 267/300\n",
      "Average training loss: 0.00570582941836781\n",
      "Average test loss: 0.006611821913056903\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0057050464666552015\n",
      "Average test loss: 0.006419680354909764\n",
      "Epoch 269/300\n",
      "Average training loss: 0.005678191920949353\n",
      "Average test loss: 0.006467091910541058\n",
      "Epoch 270/300\n",
      "Average training loss: 0.005688743573509985\n",
      "Average test loss: 0.00646159610317813\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0056857371015681165\n",
      "Average test loss: 0.006473864359160264\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005696939609116978\n",
      "Average test loss: 0.0066511818990111354\n",
      "Epoch 273/300\n",
      "Average training loss: 0.005693860125624471\n",
      "Average test loss: 0.006658333328862985\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005674170511464278\n",
      "Average test loss: 0.006335457132508358\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005681605731447538\n",
      "Average test loss: 0.006713936245275868\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0056611663488050305\n",
      "Average test loss: 0.006644366207222144\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005667907634543048\n",
      "Average test loss: 0.006792720661395126\n",
      "Epoch 278/300\n",
      "Average training loss: 0.005666164851023091\n",
      "Average test loss: 0.006355846874001953\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0056783931776881215\n",
      "Average test loss: 0.006657268962098493\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005663455709815026\n",
      "Average test loss: 0.006353795983104242\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0056462988071143624\n",
      "Average test loss: 0.0064226677575045165\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005669824247972833\n",
      "Average test loss: 0.0065212768200371\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0056574695458014804\n",
      "Average test loss: 0.006556162900394864\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0056593115838865435\n",
      "Average test loss: 0.006369982059217161\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00564353669145041\n",
      "Average test loss: 0.006461903129186895\n",
      "Epoch 286/300\n",
      "Average training loss: 0.005640406511310074\n",
      "Average test loss: 0.006564669270896249\n",
      "Epoch 287/300\n",
      "Average training loss: 0.005681626227166918\n",
      "Average test loss: 0.006547618186722199\n",
      "Epoch 288/300\n",
      "Average training loss: 0.00565794065532585\n",
      "Average test loss: 0.0065423840930064515\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0056417156979441645\n",
      "Average test loss: 0.006537562860796849\n",
      "Epoch 290/300\n",
      "Average training loss: 0.005658277179631922\n",
      "Average test loss: 0.006456249768121375\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00563438143250015\n",
      "Average test loss: 0.00650143689620826\n",
      "Epoch 292/300\n",
      "Average training loss: 0.005635749612417486\n",
      "Average test loss: 0.0064843443081610734\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005657761406567361\n",
      "Average test loss: 0.0068295346014201645\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0056462037786841396\n",
      "Average test loss: 0.006440248166107469\n",
      "Epoch 295/300\n",
      "Average training loss: 0.005630067994611131\n",
      "Average test loss: 0.006484625914444526\n",
      "Epoch 296/300\n",
      "Average training loss: 0.005698217649426725\n",
      "Average test loss: 0.007161131112939782\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005609175962706407\n",
      "Average test loss: 0.006552220485276646\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005636463933520847\n",
      "Average test loss: 0.0063609135287503405\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0056046311644216374\n",
      "Average test loss: 0.006444382941557301\n",
      "Epoch 300/300\n",
      "Average training loss: 0.005605433437973261\n",
      "Average test loss: 0.0064829332952698075\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.03504501152866416\n",
      "Average test loss: 0.008361009639170434\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011038466663824187\n",
      "Average test loss: 0.008112559916244613\n",
      "Epoch 3/300\n",
      "Average training loss: 0.009674454809890853\n",
      "Average test loss: 0.0064535817876458165\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008884876362979412\n",
      "Average test loss: 0.005913022154321273\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008402367815375329\n",
      "Average test loss: 0.005423184226784441\n",
      "Epoch 6/300\n",
      "Average training loss: 0.008042369974984063\n",
      "Average test loss: 0.005648678319321738\n",
      "Epoch 7/300\n",
      "Average training loss: 0.007617036047908995\n",
      "Average test loss: 0.006447404278649224\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007213759547720353\n",
      "Average test loss: 0.0050000580996274946\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006994222019281652\n",
      "Average test loss: 0.005048927814596229\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006680336727864213\n",
      "Average test loss: 0.004748984483174152\n",
      "Epoch 11/300\n",
      "Average training loss: 0.006481207612074083\n",
      "Average test loss: 0.004581794936416877\n",
      "Epoch 12/300\n",
      "Average training loss: 0.006300688626037704\n",
      "Average test loss: 0.004595084975163142\n",
      "Epoch 13/300\n",
      "Average training loss: 0.006128697583658828\n",
      "Average test loss: 0.00441115612226228\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006017768844548199\n",
      "Average test loss: 0.004374709366924233\n",
      "Epoch 15/300\n",
      "Average training loss: 0.00589349222348796\n",
      "Average test loss: 0.004288249430971013\n",
      "Epoch 16/300\n",
      "Average training loss: 0.005780062128686243\n",
      "Average test loss: 0.004324735653690166\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00567830024804506\n",
      "Average test loss: 0.004190694490447641\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00557347152961625\n",
      "Average test loss: 0.004154566048334043\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005495632467791438\n",
      "Average test loss: 0.004781247320895393\n",
      "Epoch 20/300\n",
      "Average training loss: 0.005395275661514865\n",
      "Average test loss: 0.004169338694877095\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0053133336924430395\n",
      "Average test loss: 0.0040635432592696615\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0052526120795971815\n",
      "Average test loss: 0.003950326583865616\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00517281378288236\n",
      "Average test loss: 0.003959670754563477\n",
      "Epoch 24/300\n",
      "Average training loss: 0.005110386712683572\n",
      "Average test loss: 0.0038916975137674144\n",
      "Epoch 25/300\n",
      "Average training loss: 0.005027711772256427\n",
      "Average test loss: 0.003915329967108038\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004989277643462022\n",
      "Average test loss: 0.0037739410296910338\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004945665745064616\n",
      "Average test loss: 0.0038110704612400797\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0048842984553840426\n",
      "Average test loss: 0.00392335910619133\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004833572883158922\n",
      "Average test loss: 0.0038166025885277323\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004798929130037626\n",
      "Average test loss: 0.0037267080106669\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004750511524991857\n",
      "Average test loss: 0.003686953351005084\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004711695858587822\n",
      "Average test loss: 0.003697798986815744\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004678765731139315\n",
      "Average test loss: 0.003696676471994983\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004641695680510666\n",
      "Average test loss: 0.003777759405473868\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004626671368463172\n",
      "Average test loss: 0.003703685734627975\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0045937905237078665\n",
      "Average test loss: 0.003963729864814215\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0045568282761507565\n",
      "Average test loss: 0.0036869463860574694\n",
      "Epoch 38/300\n",
      "Average training loss: 0.004531377378437254\n",
      "Average test loss: 0.0038146684823764697\n",
      "Epoch 39/300\n",
      "Average training loss: 0.004516282651987341\n",
      "Average test loss: 0.0036679996703233983\n",
      "Epoch 40/300\n",
      "Average training loss: 0.004504114093052017\n",
      "Average test loss: 0.0038425934536175594\n",
      "Epoch 41/300\n",
      "Average training loss: 0.004465219766315487\n",
      "Average test loss: 0.0036140985099805726\n",
      "Epoch 42/300\n",
      "Average training loss: 0.004453196143110593\n",
      "Average test loss: 0.003606025685659713\n",
      "Epoch 43/300\n",
      "Average training loss: 0.004431101214347614\n",
      "Average test loss: 0.003660275721301635\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004422717089868254\n",
      "Average test loss: 0.0036248436114854283\n",
      "Epoch 45/300\n",
      "Average training loss: 0.004400787567098935\n",
      "Average test loss: 0.003617937505038248\n",
      "Epoch 46/300\n",
      "Average training loss: 0.004369220049017006\n",
      "Average test loss: 0.0036826502631107966\n",
      "Epoch 47/300\n",
      "Average training loss: 0.004365032215913137\n",
      "Average test loss: 0.0036064155612968735\n",
      "Epoch 48/300\n",
      "Average training loss: 0.004330906701998578\n",
      "Average test loss: 0.00369100823191305\n",
      "Epoch 49/300\n",
      "Average training loss: 0.004340774811804294\n",
      "Average test loss: 0.003602039988256163\n",
      "Epoch 50/300\n",
      "Average training loss: 0.004366283085197211\n",
      "Average test loss: 0.003601266105348865\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00438306935918\n",
      "Average test loss: 0.00360674309813314\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004281790504025089\n",
      "Average test loss: 0.003567672766331169\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0042622145153582095\n",
      "Average test loss: 0.0036538362939738567\n",
      "Epoch 54/300\n",
      "Average training loss: 0.004251925366620223\n",
      "Average test loss: 0.0036609068889584804\n",
      "Epoch 55/300\n",
      "Average training loss: 0.004241839812861549\n",
      "Average test loss: 0.0036608501043584613\n",
      "Epoch 56/300\n",
      "Average training loss: 0.004253807021925847\n",
      "Average test loss: 0.004196173025502099\n",
      "Epoch 57/300\n",
      "Average training loss: 0.004236913652883635\n",
      "Average test loss: 0.0037913224175572394\n",
      "Epoch 58/300\n",
      "Average training loss: 0.004194631822200285\n",
      "Average test loss: 0.003569436764758494\n",
      "Epoch 59/300\n",
      "Average training loss: 0.004196628920526968\n",
      "Average test loss: 0.003799011095530457\n",
      "Epoch 60/300\n",
      "Average training loss: 0.004199588446981377\n",
      "Average test loss: 0.0036706125371985966\n",
      "Epoch 61/300\n",
      "Average training loss: 0.004193471220425434\n",
      "Average test loss: 0.0035386742336882483\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004226535072136256\n",
      "Average test loss: 0.0035921953353616927\n",
      "Epoch 63/300\n",
      "Average training loss: 0.004153334814641211\n",
      "Average test loss: 0.003836721623109447\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0041614849273529314\n",
      "Average test loss: 0.003619012188580301\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004126362053056558\n",
      "Average test loss: 0.0035409445361130765\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004137178994922174\n",
      "Average test loss: 0.0036233743619587685\n",
      "Epoch 67/300\n",
      "Average training loss: 0.004120449114176962\n",
      "Average test loss: 0.003581007670611143\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004111783623281452\n",
      "Average test loss: 0.003618245794127385\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004116300469885269\n",
      "Average test loss: 0.004159358107795318\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004091741086087293\n",
      "Average test loss: 0.0037954698838293553\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004088526528328657\n",
      "Average test loss: 0.003593299980378813\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00409401804746853\n",
      "Average test loss: 0.0035123511105775835\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004065573985377948\n",
      "Average test loss: 0.0039414812322292065\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004063289214960403\n",
      "Average test loss: 0.0037059358370800815\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004052064205623335\n",
      "Average test loss: 0.003605380277045899\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0041072506668667\n",
      "Average test loss: 0.003542402431161867\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004015282354835007\n",
      "Average test loss: 0.003558608769439161\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004026113808568981\n",
      "Average test loss: 0.0035265729216237864\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004039847974561982\n",
      "Average test loss: 0.003631977626846896\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0040331579502671955\n",
      "Average test loss: 0.0037858688268396588\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004012235910735197\n",
      "Average test loss: 0.003633845323075851\n",
      "Epoch 82/300\n",
      "Average training loss: 0.003993581036312713\n",
      "Average test loss: 0.003651872923390733\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003998747675990065\n",
      "Average test loss: 0.0035637859337859686\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004001480780955818\n",
      "Average test loss: 0.003558631750237611\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00398887547892001\n",
      "Average test loss: 0.00360114554150237\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003983071130183008\n",
      "Average test loss: 0.0036986968105451926\n",
      "Epoch 87/300\n",
      "Average training loss: 0.003976460779499677\n",
      "Average test loss: 0.003635262850465046\n",
      "Epoch 88/300\n",
      "Average training loss: 0.003974028297182587\n",
      "Average test loss: 0.0036475924795700445\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003954216725089484\n",
      "Average test loss: 0.0036846758214135965\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003939529787335131\n",
      "Average test loss: 0.003695463910698891\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003948224996113115\n",
      "Average test loss: 0.003585099565693074\n",
      "Epoch 92/300\n",
      "Average training loss: 0.003931969897614585\n",
      "Average test loss: 0.0035615258237553966\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0039325687641070944\n",
      "Average test loss: 0.003574677999648783\n",
      "Epoch 94/300\n",
      "Average training loss: 0.003941399015486241\n",
      "Average test loss: 0.0036554773749990594\n",
      "Epoch 95/300\n",
      "Average training loss: 0.003926107744789786\n",
      "Average test loss: 0.0035954957629243532\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00392570913127727\n",
      "Average test loss: 0.003671439566028615\n",
      "Epoch 97/300\n",
      "Average training loss: 0.003907975967973471\n",
      "Average test loss: 0.003740191826596856\n",
      "Epoch 98/300\n",
      "Average training loss: 0.003909344841622644\n",
      "Average test loss: 0.003592223316638006\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0039014040637347434\n",
      "Average test loss: 0.0035719755560987524\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003893547058933311\n",
      "Average test loss: 0.003603574785300427\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0038981104377243253\n",
      "Average test loss: 0.0037114614334164396\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0039012933634221553\n",
      "Average test loss: 0.003657346852330698\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0039299365869826745\n",
      "Average test loss: 0.0036372492081589167\n",
      "Epoch 104/300\n",
      "Average training loss: 0.003874466634665926\n",
      "Average test loss: 0.0035823267089823882\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0038657693622840775\n",
      "Average test loss: 0.00364877025431229\n",
      "Epoch 106/300\n",
      "Average training loss: 0.00386253656902247\n",
      "Average test loss: 0.0035513814251042076\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0038615601418746843\n",
      "Average test loss: 0.0036140347011387347\n",
      "Epoch 108/300\n",
      "Average training loss: 0.003849418697671758\n",
      "Average test loss: 0.003564636826515198\n",
      "Epoch 109/300\n",
      "Average training loss: 0.003869955025613308\n",
      "Average test loss: 0.0035759104148795208\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003847959713389476\n",
      "Average test loss: 0.0035455985772940846\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0038556528358409804\n",
      "Average test loss: 0.0035991121732319394\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0038285256144073273\n",
      "Average test loss: 0.0037250914588156676\n",
      "Epoch 113/300\n",
      "Average training loss: 0.003842440639105108\n",
      "Average test loss: 0.0037166054745515187\n",
      "Epoch 114/300\n",
      "Average training loss: 0.003837359816249874\n",
      "Average test loss: 0.0037046371075428195\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00382824130790929\n",
      "Average test loss: 0.003559490907523367\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0038351982451147503\n",
      "Average test loss: 0.0036066502957708304\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0038214583744605383\n",
      "Average test loss: 0.0036506230021930404\n",
      "Epoch 118/300\n",
      "Average training loss: 0.003806236939918664\n",
      "Average test loss: 0.003645011620803012\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0038106299833291107\n",
      "Average test loss: 0.0036983031928539277\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0038608066675563653\n",
      "Average test loss: 0.0036053863471994796\n",
      "Epoch 121/300\n",
      "Average training loss: 0.003791274602835377\n",
      "Average test loss: 0.003703449265824424\n",
      "Epoch 122/300\n",
      "Average training loss: 0.003791353736900621\n",
      "Average test loss: 0.003911581766688162\n",
      "Epoch 123/300\n",
      "Average training loss: 0.003792333933007386\n",
      "Average test loss: 0.003671725836892923\n",
      "Epoch 124/300\n",
      "Average training loss: 0.003784303068079882\n",
      "Average test loss: 0.0035679844406743843\n",
      "Epoch 125/300\n",
      "Average training loss: 0.003784698160779145\n",
      "Average test loss: 0.0035531347286370065\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0037932669596953523\n",
      "Average test loss: 0.0036360027831461696\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0037791049426628485\n",
      "Average test loss: 0.0037287196307960483\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003779801459776031\n",
      "Average test loss: 0.0035613823162598743\n",
      "Epoch 129/300\n",
      "Average training loss: 0.003770439288475447\n",
      "Average test loss: 0.003592403576398889\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0037750135680867567\n",
      "Average test loss: 0.003698780777553717\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0037549102927247682\n",
      "Average test loss: 0.003622079886082146\n",
      "Epoch 132/300\n",
      "Average training loss: 0.003764216689392924\n",
      "Average test loss: 0.003613835132163432\n",
      "Epoch 133/300\n",
      "Average training loss: 0.003768907875650459\n",
      "Average test loss: 0.0036267955410811637\n",
      "Epoch 134/300\n",
      "Average training loss: 0.003758052744385269\n",
      "Average test loss: 0.0036109865858323046\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0037480788146042163\n",
      "Average test loss: 0.0035735816129793723\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0037439991066025363\n",
      "Average test loss: 0.003788376421150234\n",
      "Epoch 137/300\n",
      "Average training loss: 0.003749461809173226\n",
      "Average test loss: 0.0036056454059564405\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0037354680384612746\n",
      "Average test loss: 0.003674361922260788\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003756805226827661\n",
      "Average test loss: 0.0039035636894404887\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0037411570391721197\n",
      "Average test loss: 0.004177466224051184\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0037240550383511517\n",
      "Average test loss: 0.003720854557429751\n",
      "Epoch 142/300\n",
      "Average training loss: 0.003729838891575734\n",
      "Average test loss: 3.49631308333079\n",
      "Epoch 143/300\n",
      "Average training loss: 0.004007325567719009\n",
      "Average test loss: 0.0036575207004530563\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0036885949307017855\n",
      "Average test loss: 0.0035666000263558494\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003689062076103356\n",
      "Average test loss: 0.003717986452082793\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0036977796372440128\n",
      "Average test loss: 0.0035895332836856446\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0037038003665705523\n",
      "Average test loss: 0.003615681861837705\n",
      "Epoch 148/300\n",
      "Average training loss: 0.003722152833516399\n",
      "Average test loss: 0.004078289435141616\n",
      "Epoch 149/300\n",
      "Average training loss: 0.003709725645888183\n",
      "Average test loss: 0.0037055869917902683\n",
      "Epoch 150/300\n",
      "Average training loss: 0.003706902789986796\n",
      "Average test loss: 0.0037392154495335286\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0037082855924963952\n",
      "Average test loss: 0.003617263476467795\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00369073536247015\n",
      "Average test loss: 0.003588470614411765\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0036966057183841863\n",
      "Average test loss: 0.003636623600290881\n",
      "Epoch 154/300\n",
      "Average training loss: 0.003700104779253403\n",
      "Average test loss: 0.00369681481934256\n",
      "Epoch 155/300\n",
      "Average training loss: 0.003702594484306044\n",
      "Average test loss: 0.0036729895640164613\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00370236796947817\n",
      "Average test loss: 0.0037332208688474365\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0037025913807253044\n",
      "Average test loss: 0.003598700771522191\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0036731472977747518\n",
      "Average test loss: 0.003644594371318817\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0036910437328947915\n",
      "Average test loss: 0.0035547263564334975\n",
      "Epoch 160/300\n",
      "Average training loss: 0.003690802446463042\n",
      "Average test loss: 0.003652380394852824\n",
      "Epoch 161/300\n",
      "Average training loss: 0.003682892400564419\n",
      "Average test loss: 0.0036993810335795084\n",
      "Epoch 162/300\n",
      "Average training loss: 0.003672571496003204\n",
      "Average test loss: 0.0035587047539237474\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0036810766365379093\n",
      "Average test loss: 0.0036062232189708284\n",
      "Epoch 164/300\n",
      "Average training loss: 0.003662657019578748\n",
      "Average test loss: 0.0036542906388640404\n",
      "Epoch 165/300\n",
      "Average training loss: 0.003654624961730507\n",
      "Average test loss: 0.0038518562258945573\n",
      "Epoch 166/300\n",
      "Average training loss: 0.003663985868088073\n",
      "Average test loss: 0.003842737310462528\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0036581851972473994\n",
      "Average test loss: 0.00364432875563701\n",
      "Epoch 168/300\n",
      "Average training loss: 0.003667851644878586\n",
      "Average test loss: 0.0037243140662709872\n",
      "Epoch 169/300\n",
      "Average training loss: 0.003680847067799833\n",
      "Average test loss: 0.0036972306823978823\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0036770988704843653\n",
      "Average test loss: 0.0036487984615895482\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0036422189227822755\n",
      "Average test loss: 0.003613433967447943\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00365266237490707\n",
      "Average test loss: 0.003727195801627305\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0036532580589668617\n",
      "Average test loss: 0.003692717914779981\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0036466061944762864\n",
      "Average test loss: 0.003692422362665335\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0036444571870896552\n",
      "Average test loss: 0.00373375605005357\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0036356736458837986\n",
      "Average test loss: 0.003722781121937765\n",
      "Epoch 177/300\n",
      "Average training loss: 0.003629611649653978\n",
      "Average test loss: 0.003660156800101201\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0036367166930188735\n",
      "Average test loss: 0.0037980709802359343\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0036454660315066575\n",
      "Average test loss: 0.003692823689844873\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0036288497023698354\n",
      "Average test loss: 0.003667462614261442\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0036374839680890243\n",
      "Average test loss: 0.0038045875421828692\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0036188349516855347\n",
      "Average test loss: 0.003616952281859186\n",
      "Epoch 183/300\n",
      "Average training loss: 0.003623882472515106\n",
      "Average test loss: 0.0037101264368328783\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0036173156241161956\n",
      "Average test loss: 0.00373677880047924\n",
      "Epoch 185/300\n",
      "Average training loss: 0.003618634217729171\n",
      "Average test loss: 0.0037260556481778623\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00361587842926383\n",
      "Average test loss: 0.0038402930063505965\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0036101222454259793\n",
      "Average test loss: 0.0037710757193466027\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003611320647721489\n",
      "Average test loss: 0.0038659361108309693\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0036059412240154213\n",
      "Average test loss: 0.0036550322715193035\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0036099097575578425\n",
      "Average test loss: 0.0036137813503543535\n",
      "Epoch 191/300\n",
      "Average training loss: 0.003606841321206755\n",
      "Average test loss: 0.003685947812679741\n",
      "Epoch 192/300\n",
      "Average training loss: 0.003599619377611412\n",
      "Average test loss: 0.0036483003513680563\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0036081284027960564\n",
      "Average test loss: 0.003701108252422677\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0036022445629868244\n",
      "Average test loss: 0.003761932970955968\n",
      "Epoch 195/300\n",
      "Average training loss: 0.003602170716557238\n",
      "Average test loss: 0.0037649510473840766\n",
      "Epoch 196/300\n",
      "Average training loss: 0.003591392680381735\n",
      "Average test loss: 0.0036557279862463473\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0035937896312938796\n",
      "Average test loss: 0.0037389112864103582\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0035952913800461426\n",
      "Average test loss: 0.0037176084464622867\n",
      "Epoch 199/300\n",
      "Average training loss: 0.003591201830034455\n",
      "Average test loss: 0.003756650461297896\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0035864342974705828\n",
      "Average test loss: 0.0036366101037710904\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0035915870111849573\n",
      "Average test loss: 0.0036584465083562665\n",
      "Epoch 202/300\n",
      "Average training loss: 0.003592051274039679\n",
      "Average test loss: 0.00367848838865757\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0035865808280391826\n",
      "Average test loss: 0.003701562377727694\n",
      "Epoch 204/300\n",
      "Average training loss: 0.003573404211964872\n",
      "Average test loss: 0.003788516279309988\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0035801403543187513\n",
      "Average test loss: 0.0036332260051535237\n",
      "Epoch 206/300\n",
      "Average training loss: 0.003584988763762845\n",
      "Average test loss: 0.0037839034603287776\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0035682598178585372\n",
      "Average test loss: 0.003638683052112659\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003573734075658851\n",
      "Average test loss: 0.00490698970357577\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0035801269300282\n",
      "Average test loss: 0.003725492627877328\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0035528273592806523\n",
      "Average test loss: 0.003619034998739759\n",
      "Epoch 211/300\n",
      "Average training loss: 0.003566521234603392\n",
      "Average test loss: 0.0037745890062716274\n",
      "Epoch 212/300\n",
      "Average training loss: 0.003579682315389315\n",
      "Average test loss: 0.0036591975568897196\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0035624485978235803\n",
      "Average test loss: 0.0036579025785128277\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0035584461887677513\n",
      "Average test loss: 0.0036827821719149748\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0035509416026373703\n",
      "Average test loss: 0.0037745342045608496\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0035649824656753077\n",
      "Average test loss: 0.0037980591472652225\n",
      "Epoch 217/300\n",
      "Average training loss: 0.003558635314098663\n",
      "Average test loss: 0.0037714777739925515\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0035581016532248923\n",
      "Average test loss: 0.003672897282987833\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0035486562007831205\n",
      "Average test loss: 0.0036828881077882315\n",
      "Epoch 220/300\n",
      "Average training loss: 0.003548177554168635\n",
      "Average test loss: 0.0036069516502320767\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00355736525149809\n",
      "Average test loss: 0.003649941943999794\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0035569538527892696\n",
      "Average test loss: 0.004304969533450074\n",
      "Epoch 223/300\n",
      "Average training loss: 0.003540634844245182\n",
      "Average test loss: 0.003915882301620311\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00353786315355036\n",
      "Average test loss: 0.0037053699478920964\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0035439238670385547\n",
      "Average test loss: 0.0036811595348020393\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00355038696134256\n",
      "Average test loss: 0.004080212133626143\n",
      "Epoch 227/300\n",
      "Average training loss: 0.003527168048131797\n",
      "Average test loss: 0.0037283395466705164\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0035478517591125435\n",
      "Average test loss: 0.004050700333590309\n",
      "Epoch 229/300\n",
      "Average training loss: 0.003538779249828723\n",
      "Average test loss: 0.0037035509389307763\n",
      "Epoch 230/300\n",
      "Average training loss: 0.003536931392426292\n",
      "Average test loss: 0.0036606611830906736\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0035396892432537344\n",
      "Average test loss: 0.003884315762047966\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0035348410933381982\n",
      "Average test loss: 0.0036939569775842956\n",
      "Epoch 233/300\n",
      "Average training loss: 0.003518905819290214\n",
      "Average test loss: 0.0037584518645372655\n",
      "Epoch 234/300\n",
      "Average training loss: 0.003525034562788076\n",
      "Average test loss: 0.0036885482859280376\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0035316362066401376\n",
      "Average test loss: 0.0038250855662756494\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0035177123091287084\n",
      "Average test loss: 0.003820531263947487\n",
      "Epoch 237/300\n",
      "Average training loss: 0.003535799778583977\n",
      "Average test loss: 0.0037004291891223856\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0035101992059499024\n",
      "Average test loss: 0.0038219814385390943\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0035168287307024\n",
      "Average test loss: 0.003665918569597933\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0035108777168724272\n",
      "Average test loss: 0.003640639601275325\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003530711015065511\n",
      "Average test loss: 0.00370556259362234\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0035343245406531626\n",
      "Average test loss: 0.003765693996515539\n",
      "Epoch 243/300\n",
      "Average training loss: 0.003506720985389418\n",
      "Average test loss: 0.0036742541802426178\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0035008864319986768\n",
      "Average test loss: 0.004042242905745903\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0035132627965261537\n",
      "Average test loss: 0.003838874120575686\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0035192552444835504\n",
      "Average test loss: 0.003681225150409672\n",
      "Epoch 247/300\n",
      "Average training loss: 0.003510164512321353\n",
      "Average test loss: 0.003806096690810389\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0035140270217218334\n",
      "Average test loss: 0.0038636613862795964\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0034937229375872347\n",
      "Average test loss: 0.003878433612278766\n",
      "Epoch 250/300\n",
      "Average training loss: 0.003511821938885583\n",
      "Average test loss: 0.003683348784223199\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0034991334869215887\n",
      "Average test loss: 0.0038775994578997292\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0034993886991093555\n",
      "Average test loss: 0.0036813143969823916\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0034923338583774037\n",
      "Average test loss: 0.0036809964469737477\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0034944665429906714\n",
      "Average test loss: 0.0037043941898478403\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0034897469468414782\n",
      "Average test loss: 0.0037916719714800517\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003488087092215816\n",
      "Average test loss: 0.004053376169875264\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003487057330293788\n",
      "Average test loss: 0.0037220544053448573\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003487341217075785\n",
      "Average test loss: 0.003701586744644576\n",
      "Epoch 259/300\n",
      "Average training loss: 0.003512126264679763\n",
      "Average test loss: 0.0038579996948440868\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0034914474834998447\n",
      "Average test loss: 0.003678499060165551\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0034897370167697467\n",
      "Average test loss: 0.003893624860379431\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0034774119042687945\n",
      "Average test loss: 0.003749198062552346\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0034794855862855913\n",
      "Average test loss: 0.0037989814525677098\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0034892085807191\n",
      "Average test loss: 0.0037400630542801486\n",
      "Epoch 265/300\n",
      "Average training loss: 0.003509385579990016\n",
      "Average test loss: 0.0037401400790032413\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0034829760175198315\n",
      "Average test loss: 0.003665383295880424\n",
      "Epoch 267/300\n",
      "Average training loss: 0.003469305067633589\n",
      "Average test loss: 0.003864197220860256\n",
      "Epoch 268/300\n",
      "Average training loss: 0.003488134757305185\n",
      "Average test loss: 0.0037040790963090127\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0034663534123036597\n",
      "Average test loss: 0.0037592471312317584\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0034759618886229067\n",
      "Average test loss: 0.003796251372537679\n",
      "Epoch 271/300\n",
      "Average training loss: 0.003531354256388214\n",
      "Average test loss: 0.0038167367683102685\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0034654933468749125\n",
      "Average test loss: 0.0037250814982172516\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0034725242551002238\n",
      "Average test loss: 0.0038467555451724266\n",
      "Epoch 274/300\n",
      "Average training loss: 0.003461229823115799\n",
      "Average test loss: 0.003879486519222458\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0034679130098472038\n",
      "Average test loss: 0.003700353220105171\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0034696267007125747\n",
      "Average test loss: 0.0037796010081138877\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0034693303271714185\n",
      "Average test loss: 0.0037290699109435083\n",
      "Epoch 278/300\n",
      "Average training loss: 0.003466734485493766\n",
      "Average test loss: 0.003786052342504263\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0034575585553215608\n",
      "Average test loss: 0.00372597129498091\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0034659191167189015\n",
      "Average test loss: 0.0038788048705706993\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0034601706167062124\n",
      "Average test loss: 0.003716378722132908\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0034705923191375204\n",
      "Average test loss: 0.004066255175405079\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0034662700345118843\n",
      "Average test loss: 0.004150193039948741\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0034614478041314414\n",
      "Average test loss: 0.003827271287846896\n",
      "Epoch 285/300\n",
      "Average training loss: 0.003461072240024805\n",
      "Average test loss: 0.003994452976104286\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0034475184693518613\n",
      "Average test loss: 0.003698729127438532\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0034578986476278967\n",
      "Average test loss: 0.00394117181830936\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0034574908603810604\n",
      "Average test loss: 0.00405197946768668\n",
      "Epoch 289/300\n",
      "Average training loss: 0.003454157510565387\n",
      "Average test loss: 0.0038774731138514146\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003451410373051961\n",
      "Average test loss: 0.0039053548473036953\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00344760910090473\n",
      "Average test loss: 0.0036978757936093542\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003451130339047975\n",
      "Average test loss: 0.0038057851545098757\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0034573700815025304\n",
      "Average test loss: 0.0037003367361095217\n",
      "Epoch 294/300\n",
      "Average training loss: 0.003443187188150154\n",
      "Average test loss: 0.003951098780044251\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0034448583006030983\n",
      "Average test loss: 0.0038979100096556874\n",
      "Epoch 296/300\n",
      "Average training loss: 0.003445614948661791\n",
      "Average test loss: 0.003781872197985649\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0034363973010331393\n",
      "Average test loss: 0.003770172560173604\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0034439287943144638\n",
      "Average test loss: 0.0047201254090501205\n",
      "Epoch 299/300\n",
      "Average training loss: 0.003449589587127169\n",
      "Average test loss: 0.0038528107172913022\n",
      "Epoch 300/300\n",
      "Average training loss: 0.003452373286295268\n",
      "Average test loss: 0.0037129654621498452\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.03107669001734919\n",
      "Average test loss: 0.006083368426395787\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008594641559653811\n",
      "Average test loss: 0.005373768844745225\n",
      "Epoch 3/300\n",
      "Average training loss: 0.00739484284321467\n",
      "Average test loss: 0.004823833275172445\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006758593890815973\n",
      "Average test loss: 0.0043006287711775965\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006318770456231302\n",
      "Average test loss: 0.004304970375779602\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005949365481320355\n",
      "Average test loss: 0.0039111949842837125\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005657862040317721\n",
      "Average test loss: 0.003893445973801944\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005402612486647235\n",
      "Average test loss: 0.003692661484910382\n",
      "Epoch 9/300\n",
      "Average training loss: 0.005128127665983306\n",
      "Average test loss: 0.0036894495582415\n",
      "Epoch 10/300\n",
      "Average training loss: 0.004918784507446819\n",
      "Average test loss: 0.003476506633684039\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004739847025109662\n",
      "Average test loss: 0.0036637413625915847\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004541383176628087\n",
      "Average test loss: 0.0033730702754110097\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004411092992665039\n",
      "Average test loss: 0.0030797753855586053\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004294926143768761\n",
      "Average test loss: 0.003011367797644602\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0041751243128544755\n",
      "Average test loss: 0.0053018829594883654\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0040771233157979115\n",
      "Average test loss: 0.0031040347421334847\n",
      "Epoch 17/300\n",
      "Average training loss: 0.003984454850355784\n",
      "Average test loss: 0.0028655925556603404\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003890668901304404\n",
      "Average test loss: 0.0028648080648854377\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0037927461506591905\n",
      "Average test loss: 0.002760142996079392\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0037271150319526593\n",
      "Average test loss: 0.002763223339906997\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003661559627908799\n",
      "Average test loss: 0.002663142561705576\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0036090603025837075\n",
      "Average test loss: 0.002823463350534439\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003570638015245398\n",
      "Average test loss: 0.0026070986539125443\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0035091677012128964\n",
      "Average test loss: 0.0026316400917453897\n",
      "Epoch 25/300\n",
      "Average training loss: 0.003471925504298674\n",
      "Average test loss: 0.002555499996472564\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003430563607149654\n",
      "Average test loss: 0.0026085854267908467\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0033916652492351003\n",
      "Average test loss: 0.002543894953197903\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003358227247786191\n",
      "Average test loss: 0.002537121429728965\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0033271142387141786\n",
      "Average test loss: 0.002603191290671627\n",
      "Epoch 30/300\n",
      "Average training loss: 0.003302018922236231\n",
      "Average test loss: 0.0025468641960372526\n",
      "Epoch 31/300\n",
      "Average training loss: 0.003271522767220934\n",
      "Average test loss: 0.002499814422387216\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0032664168102459774\n",
      "Average test loss: 0.0025249690817048154\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0032155463182263902\n",
      "Average test loss: 0.0024867861083605223\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00320418371198078\n",
      "Average test loss: 0.002479816103561057\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0031842094670153326\n",
      "Average test loss: 0.0024516306800974738\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0031940583241068655\n",
      "Average test loss: 0.002505308491902219\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0032383405040535664\n",
      "Average test loss: 0.002455200785977973\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0031484433751967217\n",
      "Average test loss: 0.002447790061020189\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003113458811822865\n",
      "Average test loss: 0.0024408531975415016\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003100761666273077\n",
      "Average test loss: 0.0024281416601604888\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003091032698336575\n",
      "Average test loss: 0.0023903712381919224\n",
      "Epoch 42/300\n",
      "Average training loss: 0.003082655409350991\n",
      "Average test loss: 0.0025462047617054646\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0030727546026723253\n",
      "Average test loss: 0.002413947887925638\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003071770918659038\n",
      "Average test loss: 0.0024675017969889774\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0030653365053650407\n",
      "Average test loss: 0.0025862891173197163\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003033033926246895\n",
      "Average test loss: 0.0024306887495848866\n",
      "Epoch 47/300\n",
      "Average training loss: 0.003017125317826867\n",
      "Average test loss: 0.0024538977448311117\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0030059723048988314\n",
      "Average test loss: 0.002445691430527303\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0030023235647628705\n",
      "Average test loss: 0.002410158669265608\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0030007580135845476\n",
      "Average test loss: 0.002457755564608508\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002987999234555496\n",
      "Average test loss: 0.002402500265277922\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0029913446613483957\n",
      "Average test loss: 0.0024013042872150737\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0030064551697836983\n",
      "Average test loss: 0.002375219134406911\n",
      "Epoch 54/300\n",
      "Average training loss: 0.002966256523297893\n",
      "Average test loss: 0.002370253149833944\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002944913126735224\n",
      "Average test loss: 0.002546235956872503\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002925799660798576\n",
      "Average test loss: 0.002432727474305365\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002931880693675743\n",
      "Average test loss: 0.0024006227896445327\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0029122847800867424\n",
      "Average test loss: 0.002352029216889706\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0029075149142493806\n",
      "Average test loss: 0.0024423150033172636\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002912735021362702\n",
      "Average test loss: 0.002667326734608246\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0029028713380297023\n",
      "Average test loss: 0.002397876387875941\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002910676584061649\n",
      "Average test loss: 0.0024502147709329924\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0028849927834752533\n",
      "Average test loss: 0.0024228848539706734\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002880516428500414\n",
      "Average test loss: 0.002419830925969614\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002884694623036517\n",
      "Average test loss: 0.002397153906731142\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00287714003233446\n",
      "Average test loss: 0.0024042656562394565\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0028607268488655487\n",
      "Average test loss: 0.0023701445332003964\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0028454584773215984\n",
      "Average test loss: 0.0023965827785432338\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0028471063023640048\n",
      "Average test loss: 0.002538205671434601\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0028435409671316546\n",
      "Average test loss: 0.0023903954825881457\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0028503427039831878\n",
      "Average test loss: 0.002372027642611\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002848972898390558\n",
      "Average test loss: 0.0024318793149044116\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002821260458479325\n",
      "Average test loss: 0.0027127465336687037\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002812618528596229\n",
      "Average test loss: 0.0024016710068616603\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0028134677161773046\n",
      "Average test loss: 0.0023863242798381383\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002801090953250726\n",
      "Average test loss: 0.0024149486666752234\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0028061391526005334\n",
      "Average test loss: 0.002390872976432244\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0028029821989023025\n",
      "Average test loss: 0.002408847811528378\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002802338733855221\n",
      "Average test loss: 0.002527919421179427\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002793907496664259\n",
      "Average test loss: 0.0023921107933339144\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002780266165319416\n",
      "Average test loss: 0.002571320177987218\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002779800453223288\n",
      "Average test loss: 0.002337145651276741\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002795429735134045\n",
      "Average test loss: 0.002474461797831787\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002794447143458658\n",
      "Average test loss: 0.0024561492446810006\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0027669333935611776\n",
      "Average test loss: 0.002424132565450337\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0027540752196477517\n",
      "Average test loss: 0.002571461899413003\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0027589476445896758\n",
      "Average test loss: 0.0024294224528388845\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0027522543020960357\n",
      "Average test loss: 0.0023785789602746568\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0027578737646755244\n",
      "Average test loss: 0.0024238851921012006\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002759179390759932\n",
      "Average test loss: 0.0026617515207164816\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0027532770538495647\n",
      "Average test loss: 0.0024149089054101044\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0027378232553601266\n",
      "Average test loss: 0.0023831608868721457\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0027341931119768156\n",
      "Average test loss: 0.0024299926976155905\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0027402319736364814\n",
      "Average test loss: 0.002383340417303973\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002727805117559102\n",
      "Average test loss: 0.002446101332290305\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002728106834822231\n",
      "Average test loss: 0.002424530207696888\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002710342576934232\n",
      "Average test loss: 0.0023980747759342193\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0027179820177455744\n",
      "Average test loss: 0.002512574864137504\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0027112393969049055\n",
      "Average test loss: 0.0024252878795895313\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0027192376245641046\n",
      "Average test loss: 0.0023419101530065137\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002719386508067449\n",
      "Average test loss: 0.002452868425908188\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0027148527354001997\n",
      "Average test loss: 0.0023838096090282005\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0027048284843977956\n",
      "Average test loss: 0.002470319496157269\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0026874666679650547\n",
      "Average test loss: 0.002381181080101265\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0026981703220970103\n",
      "Average test loss: 0.002403007134381268\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002703665503611167\n",
      "Average test loss: 0.0024097207960569198\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002691792321494884\n",
      "Average test loss: 0.0023856399373875722\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0026867305684006876\n",
      "Average test loss: 0.0024330556790033977\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002675998508102364\n",
      "Average test loss: 0.0023982295139382283\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002674926542987426\n",
      "Average test loss: 0.0024066687747836112\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002667762324420942\n",
      "Average test loss: 0.002393660857859585\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0026680209181374976\n",
      "Average test loss: 0.002442067271305455\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002671209541252918\n",
      "Average test loss: 0.0024457219818400013\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0026878345186511676\n",
      "Average test loss: 0.003012048692132036\n",
      "Epoch 115/300\n",
      "Average training loss: 0.002652227216710647\n",
      "Average test loss: 0.0024029239893166555\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0026661034433378113\n",
      "Average test loss: 0.0024242172454380325\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0026481628409690326\n",
      "Average test loss: 0.002375793591245181\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0026551396902650595\n",
      "Average test loss: 0.0028251126948744057\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002656863855404986\n",
      "Average test loss: 0.002434826972997851\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0026464069253868527\n",
      "Average test loss: 0.002509141450126966\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002643222281709313\n",
      "Average test loss: 0.002435515779691438\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0026396129427270756\n",
      "Average test loss: 0.0026996615504225096\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002639799219659633\n",
      "Average test loss: 0.0023870030012913047\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0026424467522237036\n",
      "Average test loss: 0.0023496621418744325\n",
      "Epoch 125/300\n",
      "Average training loss: 0.002633508695082532\n",
      "Average test loss: 0.0024840742795107264\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0026411038931045266\n",
      "Average test loss: 0.0023591447225254445\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002631657636827893\n",
      "Average test loss: 0.002367974008950922\n",
      "Epoch 128/300\n",
      "Average training loss: 0.002626294191616277\n",
      "Average test loss: 0.002469359157193038\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0026476671923365857\n",
      "Average test loss: 0.0024191713059941926\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0026270318306568594\n",
      "Average test loss: 0.00253443196705646\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0026190623520976966\n",
      "Average test loss: 0.0023775732312351465\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0026087824538764026\n",
      "Average test loss: 0.0024194317418668007\n",
      "Epoch 133/300\n",
      "Average training loss: 0.002615571118477318\n",
      "Average test loss: 0.0028176606549984878\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002614354874524805\n",
      "Average test loss: 0.0023917003257407084\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002613527890915672\n",
      "Average test loss: 0.002524598524802261\n",
      "Epoch 136/300\n",
      "Average training loss: 0.002606983924491538\n",
      "Average test loss: 0.002460557766051756\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002626105158072379\n",
      "Average test loss: 0.002439723587905367\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00261038924422529\n",
      "Average test loss: 0.0024185989027221996\n",
      "Epoch 139/300\n",
      "Average training loss: 0.002597737248366078\n",
      "Average test loss: 0.0024688635137346056\n",
      "Epoch 140/300\n",
      "Average training loss: 0.002597170301092168\n",
      "Average test loss: 0.0024798035580250954\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0025912829474028613\n",
      "Average test loss: 0.0023966032028612163\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0025912139281216596\n",
      "Average test loss: 0.002510869035911229\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0025915512539860276\n",
      "Average test loss: 0.0026099800169467925\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0025899125091317626\n",
      "Average test loss: 0.0024228891782048676\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002588871914272507\n",
      "Average test loss: 0.002446612829135524\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002586528049988879\n",
      "Average test loss: 0.0025378665886819363\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0025860372552027307\n",
      "Average test loss: 0.0024678888192607297\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002589728163969186\n",
      "Average test loss: 0.0025349451502164203\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0025822224902609984\n",
      "Average test loss: 0.002431947955240806\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002578832450012366\n",
      "Average test loss: 0.002413589084934857\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002575540215501355\n",
      "Average test loss: 0.002480449387596713\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0025702357016917734\n",
      "Average test loss: 0.0024681599992844795\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0025740442344297965\n",
      "Average test loss: 0.002621791350344817\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0025758689989646274\n",
      "Average test loss: 0.0024130810857233074\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002570971939505802\n",
      "Average test loss: 0.002378355153111948\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0025682600972553094\n",
      "Average test loss: 0.0024284301200467678\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0025700386729505327\n",
      "Average test loss: 0.002403086059002413\n",
      "Epoch 158/300\n",
      "Average training loss: 0.002564962527404229\n",
      "Average test loss: 0.0024834212942255866\n",
      "Epoch 159/300\n",
      "Average training loss: 0.002562491312623024\n",
      "Average test loss: 0.002407226579884688\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0025612320515016715\n",
      "Average test loss: 0.002481555355919732\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0025583692226145\n",
      "Average test loss: 0.0024084191761083074\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0025548212579968904\n",
      "Average test loss: 0.0024454302440087\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00254479709536665\n",
      "Average test loss: 0.09728776297966639\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0025660688040985\n",
      "Average test loss: 0.0024340095898757377\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0025497599152020283\n",
      "Average test loss: 0.0024280957646874916\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002544093013430635\n",
      "Average test loss: 0.002420920931837625\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0025456176853428285\n",
      "Average test loss: 0.002451001677248213\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0025570162646472456\n",
      "Average test loss: 0.0024790090380443467\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0025497298766341474\n",
      "Average test loss: 0.0025296561184028786\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0025398913309392004\n",
      "Average test loss: 0.002546318071790867\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002537083686639865\n",
      "Average test loss: 0.0024163740892998047\n",
      "Epoch 172/300\n",
      "Average training loss: 0.002527348982377185\n",
      "Average test loss: 0.0024747342663920587\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0025401668393363556\n",
      "Average test loss: 0.002421939810117086\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0025389435161939926\n",
      "Average test loss: 0.0025228176431523427\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0025318657131865622\n",
      "Average test loss: 0.0024881081920531062\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002538188811391592\n",
      "Average test loss: 0.0024218289477543698\n",
      "Epoch 177/300\n",
      "Average training loss: 0.002532181589346793\n",
      "Average test loss: 0.0024616906113094756\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0025200908813211654\n",
      "Average test loss: 0.0024893571746846042\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0025287005686097674\n",
      "Average test loss: 0.0025692227511770194\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0025312531698081227\n",
      "Average test loss: 0.0024943907426463233\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0025298521764990354\n",
      "Average test loss: 0.0025182275370591216\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0025226093334042364\n",
      "Average test loss: 0.002467471690641509\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0025099071473297144\n",
      "Average test loss: 0.002415282185189426\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002521204978745017\n",
      "Average test loss: 0.0024937303538123765\n",
      "Epoch 185/300\n",
      "Average training loss: 0.002513968712339799\n",
      "Average test loss: 0.0025683816776921353\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0025103646427806882\n",
      "Average test loss: 0.002552083128442367\n",
      "Epoch 187/300\n",
      "Average training loss: 0.002508468600196971\n",
      "Average test loss: 0.0024207740244973036\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002513254214492109\n",
      "Average test loss: 0.002658378325816658\n",
      "Epoch 189/300\n",
      "Average training loss: 0.002513224094795684\n",
      "Average test loss: 0.0024556278970299496\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0025129008661541673\n",
      "Average test loss: 0.002491267775495847\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002508590818900201\n",
      "Average test loss: 0.002631852963939309\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0025075061596516106\n",
      "Average test loss: 0.002425466666722463\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00250499547045264\n",
      "Average test loss: 0.002440014188695285\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0025032146726217536\n",
      "Average test loss: 0.0024880960956215857\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002503692373426424\n",
      "Average test loss: 0.002470935225279795\n",
      "Epoch 196/300\n",
      "Average training loss: 0.002501408654679027\n",
      "Average test loss: 0.0025699543451062506\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0024938929030257793\n",
      "Average test loss: 0.002553199569384257\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002498472524392936\n",
      "Average test loss: 0.002480945726649629\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002501261680904362\n",
      "Average test loss: 0.019629157654941082\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0024935378953814505\n",
      "Average test loss: 0.002528342672934135\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0025168211532549728\n",
      "Average test loss: 0.002552602917990751\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0024894748843378492\n",
      "Average test loss: 0.0025829561170604494\n",
      "Epoch 203/300\n",
      "Average training loss: 0.002496518122446206\n",
      "Average test loss: 0.0024764942065295245\n",
      "Epoch 204/300\n",
      "Average training loss: 0.002487330820411444\n",
      "Average test loss: 0.0024996336775107517\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0024911835975944996\n",
      "Average test loss: 0.002550479408353567\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0024851890088369447\n",
      "Average test loss: 0.0024214929675476417\n",
      "Epoch 207/300\n",
      "Average training loss: 0.002491114274702138\n",
      "Average test loss: 0.0033360556931131416\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0024871145108093817\n",
      "Average test loss: 0.0024400382795267636\n",
      "Epoch 209/300\n",
      "Average training loss: 0.002487599909512533\n",
      "Average test loss: 0.0024414880774501295\n",
      "Epoch 210/300\n",
      "Average training loss: 0.002480448129897316\n",
      "Average test loss: 0.0024793545367817085\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0024757172773695654\n",
      "Average test loss: 0.002505518122679657\n",
      "Epoch 212/300\n",
      "Average training loss: 0.002475696184155014\n",
      "Average test loss: 0.002422854962034358\n",
      "Epoch 213/300\n",
      "Average training loss: 0.002478361425300439\n",
      "Average test loss: 0.002416979092069798\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002479972734633419\n",
      "Average test loss: 0.0025615251577562758\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0024726889851606555\n",
      "Average test loss: 0.002415048068596257\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0024765295260068443\n",
      "Average test loss: 0.002493906957614753\n",
      "Epoch 217/300\n",
      "Average training loss: 0.002473291475118862\n",
      "Average test loss: 0.0025033167606840533\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0024702784814354447\n",
      "Average test loss: 0.002466462945876022\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0024646772758828268\n",
      "Average test loss: 0.0025439147609803412\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0024807823347962566\n",
      "Average test loss: 0.002465104583857788\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0024699698307861884\n",
      "Average test loss: 0.003107813642670711\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002472258184933\n",
      "Average test loss: 0.0026169891697872018\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0024671109885805185\n",
      "Average test loss: 0.002440672395957841\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0024628447506369814\n",
      "Average test loss: 0.002553550224337313\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0024662804438008203\n",
      "Average test loss: 0.0024506496004760266\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002465009023745855\n",
      "Average test loss: 0.0025200634811901384\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0024651455856445764\n",
      "Average test loss: 0.002427542380678157\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0024683151920843454\n",
      "Average test loss: 0.0025156596975608003\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002459059222497874\n",
      "Average test loss: 0.002536568540872799\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0024602130533506474\n",
      "Average test loss: 0.002534453391821848\n",
      "Epoch 231/300\n",
      "Average training loss: 0.00245714879863792\n",
      "Average test loss: 0.002460025160262982\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002481364800491267\n",
      "Average test loss: 0.0024302506715887123\n",
      "Epoch 233/300\n",
      "Average training loss: 0.002451856329002314\n",
      "Average test loss: 0.0025019514721093907\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0024496795011477337\n",
      "Average test loss: 0.0024637928621636497\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0024536489422122636\n",
      "Average test loss: 0.0024637028516994583\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002445372419224845\n",
      "Average test loss: 0.0024906225202398166\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0024470121717701356\n",
      "Average test loss: 0.0025409193687761823\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00245183076016191\n",
      "Average test loss: 0.002634758019923336\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002447228979733255\n",
      "Average test loss: 0.002503225737561782\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0024466116850574812\n",
      "Average test loss: 0.0026070226455728214\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0024562500139905345\n",
      "Average test loss: 0.002465420648248659\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0024440823532640935\n",
      "Average test loss: 0.0025633085812959405\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0024426210201862787\n",
      "Average test loss: 0.0025909797580291826\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0024440263760172657\n",
      "Average test loss: 0.00245304142880357\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002458486624682943\n",
      "Average test loss: 0.002492733276345664\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0024404435958713293\n",
      "Average test loss: 0.00253571808234685\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0024372002830108006\n",
      "Average test loss: 0.002871035204268992\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0024347979687154293\n",
      "Average test loss: 0.002563540465094977\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0024464714924494427\n",
      "Average test loss: 0.002483108717844718\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0024413309519489604\n",
      "Average test loss: 0.002540739005845454\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0024341568052768706\n",
      "Average test loss: 0.002534997465709845\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0024317846654189957\n",
      "Average test loss: 0.0025106204729527233\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0024321970641613005\n",
      "Average test loss: 0.00251369125665062\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0024413689165893528\n",
      "Average test loss: 0.0025111856344673367\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0024356998122400707\n",
      "Average test loss: 0.002445079644314117\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002433987336543699\n",
      "Average test loss: 0.0025805806124375927\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0024411960157255334\n",
      "Average test loss: 0.00255930725319518\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0024388745731363693\n",
      "Average test loss: 0.002501663663321071\n",
      "Epoch 259/300\n",
      "Average training loss: 0.002423065995797515\n",
      "Average test loss: 0.002458773687895801\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0024327684487733575\n",
      "Average test loss: 0.0025050625848687356\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002430641470890906\n",
      "Average test loss: 0.002477069822036558\n",
      "Epoch 262/300\n",
      "Average training loss: 0.002428373484561841\n",
      "Average test loss: 0.002518778292255269\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0024356293140186205\n",
      "Average test loss: 0.0024767337448688017\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0024293968027664556\n",
      "Average test loss: 0.002455688008210725\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0024224932233078613\n",
      "Average test loss: 0.0024598695013879073\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002425300441061457\n",
      "Average test loss: 0.002444621245480246\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0024221766693517566\n",
      "Average test loss: 0.002564060793982612\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0024198133417715627\n",
      "Average test loss: 0.0025227771064059604\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0024218149901264244\n",
      "Average test loss: 0.002465735355599059\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0024214268508884642\n",
      "Average test loss: 0.0025431565613382393\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0024203823811064164\n",
      "Average test loss: 0.002564469289655487\n",
      "Epoch 272/300\n",
      "Average training loss: 0.002412747684452269\n",
      "Average test loss: 0.0024948058194584318\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0024170653726905585\n",
      "Average test loss: 0.0025613745304031506\n",
      "Epoch 274/300\n",
      "Average training loss: 0.00241825602369176\n",
      "Average test loss: 0.002732056148143278\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002418535504490137\n",
      "Average test loss: 0.002458131123955051\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0024091536809379855\n",
      "Average test loss: 0.002543489706081649\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002416206582552857\n",
      "Average test loss: 0.0025344847293777596\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0024218041530499855\n",
      "Average test loss: 0.0025907436224321526\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002417256466216511\n",
      "Average test loss: 0.0025359451300981974\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002405875041666958\n",
      "Average test loss: 0.002516495558536715\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0024078671601083543\n",
      "Average test loss: 0.0024696452267881897\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0024179801237252025\n",
      "Average test loss: 0.0026785467591964535\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0024080879280550613\n",
      "Average test loss: 0.0025035892228285474\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0024159342719035017\n",
      "Average test loss: 0.002467620121108161\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00240620816561083\n",
      "Average test loss: 0.0024661264460947778\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0024083364616251652\n",
      "Average test loss: 0.002525120607473784\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0024078945596184994\n",
      "Average test loss: 0.0024743168617909154\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002395243874647551\n",
      "Average test loss: 0.0024655349229772885\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0024076523973296086\n",
      "Average test loss: 0.0024668035529967812\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002401358660000066\n",
      "Average test loss: 0.0025090784889956314\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002400646266837915\n",
      "Average test loss: 0.0024651168713139163\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002400207453303867\n",
      "Average test loss: 0.0025249045165255664\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002396224299652709\n",
      "Average test loss: 0.0025176896003799307\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00240164826106694\n",
      "Average test loss: 0.00252689152273039\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0023993614514668782\n",
      "Average test loss: 0.002563001619444953\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0023972552886439694\n",
      "Average test loss: 0.0025035714662323396\n",
      "Epoch 297/300\n",
      "Average training loss: 0.002397117966165145\n",
      "Average test loss: 0.0025868427970757086\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0023955651488569048\n",
      "Average test loss: 0.002516076031140983\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0023948867534183795\n",
      "Average test loss: 0.003045266514023145\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002390141305099759\n",
      "Average test loss: 0.0024663772483666736\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02712129410604636\n",
      "Average test loss: 0.0051287858697275325\n",
      "Epoch 2/300\n",
      "Average training loss: 0.006861723796361022\n",
      "Average test loss: 0.004069858143313064\n",
      "Epoch 3/300\n",
      "Average training loss: 0.005810292112330595\n",
      "Average test loss: 0.003762441481774052\n",
      "Epoch 4/300\n",
      "Average training loss: 0.005249856231941117\n",
      "Average test loss: 0.0035454189305504165\n",
      "Epoch 5/300\n",
      "Average training loss: 0.004875135734677315\n",
      "Average test loss: 0.003110676767097579\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004553702738549974\n",
      "Average test loss: 0.003030741015656127\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004337688296205467\n",
      "Average test loss: 0.0030743175579441917\n",
      "Epoch 8/300\n",
      "Average training loss: 0.004068885901528928\n",
      "Average test loss: 0.0028244996677256294\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0038695306190186076\n",
      "Average test loss: 0.00273707429257532\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003684201576229599\n",
      "Average test loss: 0.0026386303568465843\n",
      "Epoch 11/300\n",
      "Average training loss: 0.003520028456631634\n",
      "Average test loss: 0.002383803768290414\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003391656956531935\n",
      "Average test loss: 0.0025366200125879713\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0032662591851419873\n",
      "Average test loss: 0.002250323243232237\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0031596851570324766\n",
      "Average test loss: 0.0022209194652322265\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0030541502537412774\n",
      "Average test loss: 0.0021713157076802517\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0029891309715393516\n",
      "Average test loss: 0.002132686923361487\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0029093290470126603\n",
      "Average test loss: 0.00207824036375516\n",
      "Epoch 18/300\n",
      "Average training loss: 0.002831576985617479\n",
      "Average test loss: 0.002033390299313598\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0027922636821038193\n",
      "Average test loss: 0.002027834840118885\n",
      "Epoch 20/300\n",
      "Average training loss: 0.002725412968132231\n",
      "Average test loss: 0.001958346221389042\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0026836808063089848\n",
      "Average test loss: 0.001946027802924315\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0026403196547180414\n",
      "Average test loss: 0.0019911522558993763\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0025951835124029053\n",
      "Average test loss: 0.0018848678746985065\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002563040993383361\n",
      "Average test loss: 0.0020165527121474344\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0025464367299444144\n",
      "Average test loss: 0.0018659923422253795\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002498808117583394\n",
      "Average test loss: 0.001853911828249693\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0024755419977009296\n",
      "Average test loss: 0.001869394273393684\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002462627291886343\n",
      "Average test loss: 0.0018890068464808994\n",
      "Epoch 29/300\n",
      "Average training loss: 0.002427339639721645\n",
      "Average test loss: 0.0018597476162637273\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002419484976472126\n",
      "Average test loss: 0.0018601417744325266\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0023901879835046\n",
      "Average test loss: 0.0017589701554841465\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0023664578586402867\n",
      "Average test loss: 0.0017879972537565563\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0023551988644111488\n",
      "Average test loss: 0.0017856049227217834\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002351270399056375\n",
      "Average test loss: 0.0018525918936356902\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0023316706331032847\n",
      "Average test loss: 0.0018012669326530563\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002322070937189791\n",
      "Average test loss: 0.0017340101754913728\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0023021706075718007\n",
      "Average test loss: 0.001751301576486892\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0022853779654122062\n",
      "Average test loss: 0.0017694871597405937\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00227671523950994\n",
      "Average test loss: 0.0017580569742454424\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002267968232639962\n",
      "Average test loss: 0.0017346812198973365\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0022638703961339263\n",
      "Average test loss: 0.0017128695936666596\n",
      "Epoch 42/300\n",
      "Average training loss: 0.00225623694786595\n",
      "Average test loss: 0.0017336655045962995\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0022444241856121355\n",
      "Average test loss: 0.001695910340278513\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002229962856715752\n",
      "Average test loss: 0.0016976251791541775\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0022260828554216358\n",
      "Average test loss: 0.001726873662840161\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002213649698843559\n",
      "Average test loss: 0.0017579110361014803\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002205820205828382\n",
      "Average test loss: 0.0017087917469648852\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0022133437517202563\n",
      "Average test loss: 0.0016850684336904022\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002196847329123153\n",
      "Average test loss: 0.0017032049908820126\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0021794025357812644\n",
      "Average test loss: 0.0017009541006344888\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00217674531829026\n",
      "Average test loss: 0.0017261464669265682\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0021735295687491697\n",
      "Average test loss: 0.0017079812807755338\n",
      "Epoch 53/300\n",
      "Average training loss: 0.002167835184476442\n",
      "Average test loss: 0.0016899245743536287\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0021505801972622673\n",
      "Average test loss: 0.001724271738384333\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002159522894511206\n",
      "Average test loss: 0.0017659639624050922\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0021505964549465312\n",
      "Average test loss: 0.0016966320814357863\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0021520928250004846\n",
      "Average test loss: 0.0016936395101042258\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0021458898953472575\n",
      "Average test loss: 0.001711945254355669\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002134270614013076\n",
      "Average test loss: 0.0016996756362625295\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0021290490630393225\n",
      "Average test loss: 0.0018167480350368553\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0021242634766838616\n",
      "Average test loss: 0.0017551435957559281\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0021160255532918703\n",
      "Average test loss: 0.0021913932176927725\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0021186506458454662\n",
      "Average test loss: 0.0018051949344161484\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0021219270519084402\n",
      "Average test loss: 0.0017756046327865786\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0021219582254480983\n",
      "Average test loss: 0.0016760175892462333\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0021022619153890343\n",
      "Average test loss: 0.0016702071288600565\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0021072810959691804\n",
      "Average test loss: 0.0017332315689159764\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002093530636591216\n",
      "Average test loss: 0.0016811383198946715\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0020883460586062735\n",
      "Average test loss: 0.0017450015971230135\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0021152079757303\n",
      "Average test loss: 0.0017036125159098042\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0020765841715037823\n",
      "Average test loss: 0.0016944463479643067\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0020754991312407784\n",
      "Average test loss: 0.001703497836780217\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0020761691337037418\n",
      "Average test loss: 0.0017546662320382894\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002073607923049066\n",
      "Average test loss: 0.0017464688948045175\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0020620207699636618\n",
      "Average test loss: 0.001770726938214567\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0020564431781984038\n",
      "Average test loss: 0.0016685765713660254\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0020629432269682487\n",
      "Average test loss: 0.0017299522948968742\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002053738061959545\n",
      "Average test loss: 0.0017093179900613095\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002061074808653858\n",
      "Average test loss: 0.0017186862687683768\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0020550547333227265\n",
      "Average test loss: 0.0018031584883315696\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002054115960891876\n",
      "Average test loss: 0.0017412911551073194\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002049684722804361\n",
      "Average test loss: 0.0016598355664561191\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002035983684265779\n",
      "Average test loss: 0.0016887802485790518\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0020347324658392205\n",
      "Average test loss: 0.0018243552804407146\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0020393138898329604\n",
      "Average test loss: 0.00166902297290249\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002055079810735252\n",
      "Average test loss: 0.0016653072088956833\n",
      "Epoch 87/300\n",
      "Average training loss: 0.002024920569939746\n",
      "Average test loss: 0.0017964695576164458\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0020236842160423598\n",
      "Average test loss: 0.0016569502984897957\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0020209018281764453\n",
      "Average test loss: 0.0017320375711553627\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0020138717719043295\n",
      "Average test loss: 0.0022010660854478678\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0020212239764837757\n",
      "Average test loss: 0.0017343605876796776\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0020107818147581486\n",
      "Average test loss: 0.0017100778278998203\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0020136459403567843\n",
      "Average test loss: 0.0016801086484144131\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0020158589647875894\n",
      "Average test loss: 0.0017684479138503472\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002009978399456789\n",
      "Average test loss: 0.0017470049214445883\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0019995375267333453\n",
      "Average test loss: 0.0017056915595506628\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002006413714339336\n",
      "Average test loss: 0.0017152846503174968\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0020033052688361043\n",
      "Average test loss: 0.0017330838043449653\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002004239925907718\n",
      "Average test loss: 0.0018168994794703192\n",
      "Epoch 100/300\n",
      "Average training loss: 0.001990437686132888\n",
      "Average test loss: 0.0017170701769904958\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001998334461305704\n",
      "Average test loss: 0.0016865558112040162\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0019890732648265032\n",
      "Average test loss: 0.0017128517630820472\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0019869059829248324\n",
      "Average test loss: 0.001704106400306854\n",
      "Epoch 104/300\n",
      "Average training loss: 0.001979371347464621\n",
      "Average test loss: 0.0017284273819790947\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0019872410929658345\n",
      "Average test loss: 0.0016696906023555332\n",
      "Epoch 106/300\n",
      "Average training loss: 0.001976194045299457\n",
      "Average test loss: 0.0017214963308845956\n",
      "Epoch 107/300\n",
      "Average training loss: 0.001978564479491777\n",
      "Average test loss: 0.0016833964558318258\n",
      "Epoch 108/300\n",
      "Average training loss: 0.001985605125108527\n",
      "Average test loss: 0.0016754764264656438\n",
      "Epoch 109/300\n",
      "Average training loss: 0.001964377575036552\n",
      "Average test loss: 0.0016597156083832183\n",
      "Epoch 110/300\n",
      "Average training loss: 0.001967640792226626\n",
      "Average test loss: 0.0018944512240381705\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00196849266336196\n",
      "Average test loss: 0.0018416130521024267\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0019642393531070813\n",
      "Average test loss: 0.001767276070598099\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0019638541614015897\n",
      "Average test loss: 0.0016966621917155055\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0019635461372219856\n",
      "Average test loss: 0.0017451313758889834\n",
      "Epoch 115/300\n",
      "Average training loss: 0.001962187701008386\n",
      "Average test loss: 0.0016759204564409124\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0019620322309848335\n",
      "Average test loss: 0.00180405328857402\n",
      "Epoch 117/300\n",
      "Average training loss: 0.001970017832083007\n",
      "Average test loss: 0.001729705387312505\n",
      "Epoch 118/300\n",
      "Average training loss: 0.001976243322404722\n",
      "Average test loss: 0.0017412564868314398\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0019543737346927326\n",
      "Average test loss: 0.0017214344456377957\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0019462938785759938\n",
      "Average test loss: 0.0016810034183371398\n",
      "Epoch 121/300\n",
      "Average training loss: 0.001940447668544948\n",
      "Average test loss: 0.0017176845653189554\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0019456107419812017\n",
      "Average test loss: 0.04021041843874587\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0021280853268173007\n",
      "Average test loss: 0.0017591958368817966\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0019270694866362545\n",
      "Average test loss: 0.0016746044537673394\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0019257828956469893\n",
      "Average test loss: 0.0017046422733821803\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0019306311456279623\n",
      "Average test loss: 0.0017132084862225584\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0019395853486946888\n",
      "Average test loss: 0.0017302601480235656\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0019333379072033698\n",
      "Average test loss: 0.0017716309162788093\n",
      "Epoch 129/300\n",
      "Average training loss: 0.001930301425150699\n",
      "Average test loss: 0.0018173614095689523\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019337695363081163\n",
      "Average test loss: 0.0017208756187723742\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0019240612218984299\n",
      "Average test loss: 0.0017216262314468623\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0019314285992748207\n",
      "Average test loss: 0.0017588207953506045\n",
      "Epoch 133/300\n",
      "Average training loss: 0.001919496280244655\n",
      "Average test loss: 0.0016992538975965645\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0019299526499170396\n",
      "Average test loss: 0.0017518574575790102\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0019392988422057694\n",
      "Average test loss: 0.0017908964551364382\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0019191825642353958\n",
      "Average test loss: 0.001719245760805077\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0019174557189560598\n",
      "Average test loss: 0.0017471393237097396\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0019179305610143476\n",
      "Average test loss: 0.001727829437288973\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0019137454602039523\n",
      "Average test loss: 0.0017612882384823428\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0019157350549681318\n",
      "Average test loss: 0.0017880734364605613\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001915909418111874\n",
      "Average test loss: 0.0017304366634537777\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0019205404789083534\n",
      "Average test loss: 0.0017563026323914527\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0019149157892291745\n",
      "Average test loss: 0.0016891721141421134\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0019207885644088188\n",
      "Average test loss: 0.0017003739471029905\n",
      "Epoch 145/300\n",
      "Average training loss: 0.001906685527621044\n",
      "Average test loss: 0.0016943060751590463\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0019063710220572022\n",
      "Average test loss: 0.001747526966035366\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0019018334326230818\n",
      "Average test loss: 0.0017315253981699547\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0019015125926170084\n",
      "Average test loss: 0.001701080257486966\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0019007741519146495\n",
      "Average test loss: 0.0017009904995146726\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018976626675575971\n",
      "Average test loss: 0.001746191110048029\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0019011376719507906\n",
      "Average test loss: 0.0017455450639956528\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0019107932425621484\n",
      "Average test loss: 0.0016881952450300255\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001896956731358336\n",
      "Average test loss: 0.0017169030667800041\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0018916966418425242\n",
      "Average test loss: 0.0016993238215024273\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0018944244178839855\n",
      "Average test loss: 0.0023571520768519904\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0018916995285285843\n",
      "Average test loss: 0.0017648996298925744\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0018891286573683222\n",
      "Average test loss: 0.0017730293315317896\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001893866086171733\n",
      "Average test loss: 0.0017975400156444974\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0018847841015085579\n",
      "Average test loss: 0.0017630035398114058\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0018885453052611815\n",
      "Average test loss: 0.0017450264737837843\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018883372641478975\n",
      "Average test loss: 0.001734888941877418\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0018803077775778041\n",
      "Average test loss: 0.0016982115430550443\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0018847279927382866\n",
      "Average test loss: 0.0017718507319481836\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0018810717045432991\n",
      "Average test loss: 0.0017431026437423295\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001875351604902082\n",
      "Average test loss: 0.0017488944625688924\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0018821654102454584\n",
      "Average test loss: 0.0017900135624739858\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0018761414108383987\n",
      "Average test loss: 0.0017108705985463327\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0018870319896895024\n",
      "Average test loss: 0.001771568145395981\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0019103755830890603\n",
      "Average test loss: 0.0017511707034168972\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0018649504168166055\n",
      "Average test loss: 0.0017774110956945352\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0018679173938516115\n",
      "Average test loss: 0.0017476108286322818\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0018676338188557161\n",
      "Average test loss: 0.0017370369254300993\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001869466360554927\n",
      "Average test loss: 0.001796274034306407\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0018669282676031193\n",
      "Average test loss: 0.0017594719796131056\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0018634855748257703\n",
      "Average test loss: 0.0017508713517131076\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0018622959771504004\n",
      "Average test loss: 0.0017428440349176527\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0018720331183738179\n",
      "Average test loss: 0.0017680938032766183\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0018643800312032302\n",
      "Average test loss: 0.0018104322274319\n",
      "Epoch 179/300\n",
      "Average training loss: 0.001870748014913665\n",
      "Average test loss: 0.0017696818626589245\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0018645011420465177\n",
      "Average test loss: 0.0017054216157024106\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0018627742856947912\n",
      "Average test loss: 0.0017695084580530724\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0018521100848706232\n",
      "Average test loss: 0.0017832356490608719\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001858726765650014\n",
      "Average test loss: 0.0017383716305096944\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0018562230575415823\n",
      "Average test loss: 0.0017685876218602062\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0018543743577061427\n",
      "Average test loss: 0.0018328491486608982\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0018633903551639782\n",
      "Average test loss: 0.001766133372257981\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0018553921510982845\n",
      "Average test loss: 0.0017560615579908093\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0018491506749350164\n",
      "Average test loss: 0.0018795455614518787\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0018513565449458029\n",
      "Average test loss: 0.0017468122229393986\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0018493133975813786\n",
      "Average test loss: 0.001820516791401638\n",
      "Epoch 191/300\n",
      "Average training loss: 0.001848758789917661\n",
      "Average test loss: 0.0018024681334694226\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0018465238168007797\n",
      "Average test loss: 0.0018194223456084728\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0018578120361392697\n",
      "Average test loss: 0.0017043465399700735\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0018470868815978368\n",
      "Average test loss: 0.001805080777655045\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0018382806313327618\n",
      "Average test loss: 0.001755028026074999\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0018444172928316726\n",
      "Average test loss: 0.0017480798488896754\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0018370395408322414\n",
      "Average test loss: 0.0017559970983614525\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0018394363350752328\n",
      "Average test loss: 0.0017584868691240749\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0018454118345139754\n",
      "Average test loss: 0.9256904167069329\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004466916586375899\n",
      "Average test loss: 0.0018646793412044644\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0023663227160771686\n",
      "Average test loss: 0.0017375297027950485\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0021643091924488543\n",
      "Average test loss: 0.0016862539666601354\n",
      "Epoch 203/300\n",
      "Average training loss: 0.002019611989147961\n",
      "Average test loss: 0.001736206610273156\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0019159008028606574\n",
      "Average test loss: 0.0017252709548920392\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0018550656447187065\n",
      "Average test loss: 0.001745768651055793\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0018263812301059564\n",
      "Average test loss: 0.0017610048914535178\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0018149218844870727\n",
      "Average test loss: 0.0017800663409547673\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0018083958832754029\n",
      "Average test loss: 0.0018134026832671629\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0018140689945883221\n",
      "Average test loss: 0.0017075948800063796\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0018156251662100355\n",
      "Average test loss: 0.0018104194075696997\n",
      "Epoch 211/300\n",
      "Average training loss: 0.001820197714596159\n",
      "Average test loss: 0.0018332333929008907\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0018295502865480053\n",
      "Average test loss: 0.0017491931076058084\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001834301930334833\n",
      "Average test loss: 0.0017671846128586267\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0018339731226571733\n",
      "Average test loss: 0.0017297382309205001\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0018338462152621812\n",
      "Average test loss: 0.0017566105580578248\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0018327534491610197\n",
      "Average test loss: 0.0017404444165941742\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0018383564298144646\n",
      "Average test loss: 0.0017211004385931624\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0018305768580693337\n",
      "Average test loss: 0.0017890626387670637\n",
      "Epoch 219/300\n",
      "Average training loss: 0.001824019998518957\n",
      "Average test loss: 0.001807015417350663\n",
      "Epoch 220/300\n",
      "Average training loss: 0.001829841078259051\n",
      "Average test loss: 0.0018891551612565914\n",
      "Epoch 221/300\n",
      "Average training loss: 0.001831869152167605\n",
      "Average test loss: 0.001811390002982484\n",
      "Epoch 222/300\n",
      "Average training loss: 0.001829683719202876\n",
      "Average test loss: 0.0017754166231801112\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0018264546917958393\n",
      "Average test loss: 0.001909729526274734\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0018309192202157444\n",
      "Average test loss: 0.001834412282022337\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0018275042556019294\n",
      "Average test loss: 0.0018057546317577363\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0018234187764529554\n",
      "Average test loss: 0.0018060771578715907\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001822665022375683\n",
      "Average test loss: 0.0017918397335128652\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001826759060120417\n",
      "Average test loss: 0.0017287455443292857\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0018265488595805234\n",
      "Average test loss: 0.0017371336629407274\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0018188832279087768\n",
      "Average test loss: 0.001856451745558944\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0018252928576742609\n",
      "Average test loss: 0.0018028227413694063\n",
      "Epoch 232/300\n",
      "Average training loss: 0.001818486945082744\n",
      "Average test loss: 0.0017935178203301297\n",
      "Epoch 233/300\n",
      "Average training loss: 0.001816905218693945\n",
      "Average test loss: 0.0017945481272828249\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0018238554091917145\n",
      "Average test loss: 0.0017354488509396713\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0018221585205238726\n",
      "Average test loss: 0.001823849200995432\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0018102094512432814\n",
      "Average test loss: 0.001784445551637974\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0018124032817367051\n",
      "Average test loss: 0.001807559878875812\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0018151376558881668\n",
      "Average test loss: 0.0017519866053221954\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0018060907787746854\n",
      "Average test loss: 0.0017987841500176323\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0018179613737803366\n",
      "Average test loss: 0.0017360737646619478\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0018124086694377992\n",
      "Average test loss: 0.0018328535182194578\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0018134091144634617\n",
      "Average test loss: 0.0034956580793692005\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0018659545006230473\n",
      "Average test loss: 0.0017808241379550763\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0018016705986422796\n",
      "Average test loss: 0.0018029942524929842\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0017983256857842208\n",
      "Average test loss: 0.0017798494034343296\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0018112471126433877\n",
      "Average test loss: 0.0017679653149098157\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0018012902602139447\n",
      "Average test loss: 0.001729314238143464\n",
      "Epoch 248/300\n",
      "Average training loss: 0.001811091880624493\n",
      "Average test loss: 0.0017528331299415893\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0018124465442573032\n",
      "Average test loss: 0.0017518762276611394\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0018006175215252572\n",
      "Average test loss: 0.0018753670815171467\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0017987185476554765\n",
      "Average test loss: 0.0018207344031996198\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0018057903329738313\n",
      "Average test loss: 0.0017448255449740422\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00180320951446063\n",
      "Average test loss: 0.0018837460766856868\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0018024399681016803\n",
      "Average test loss: 0.0017778210424714619\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0018020845881352823\n",
      "Average test loss: 0.0017415980658390456\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0018032217149933178\n",
      "Average test loss: 0.0017819665278204612\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0017996803217877944\n",
      "Average test loss: 0.0018115155779653126\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0017979198738725648\n",
      "Average test loss: 0.0018150847007830937\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0018054259151427281\n",
      "Average test loss: 0.0017644028097598089\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0017941274802303977\n",
      "Average test loss: 0.0018346061787257591\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0018076914649249778\n",
      "Average test loss: 0.0018265106938779355\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0017976890286016795\n",
      "Average test loss: 0.0017151122416059175\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0017991502491964235\n",
      "Average test loss: 0.001732270377067228\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0018021694085457258\n",
      "Average test loss: 0.0017969486256026559\n",
      "Epoch 265/300\n",
      "Average training loss: 0.001786138965955211\n",
      "Average test loss: 0.0018063641314705213\n",
      "Epoch 266/300\n",
      "Average training loss: 0.001794869925516347\n",
      "Average test loss: 0.0017414075773623254\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0017999103462530508\n",
      "Average test loss: 0.0018503745406245192\n",
      "Epoch 268/300\n",
      "Average training loss: 0.001792660748689539\n",
      "Average test loss: 0.0017600467172968718\n",
      "Epoch 269/300\n",
      "Average training loss: 0.001784180124083327\n",
      "Average test loss: 0.0017408072299634418\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0017886885081728299\n",
      "Average test loss: 0.0017552721925700703\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0017891413376977046\n",
      "Average test loss: 0.0017818637628936105\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0017900086351566845\n",
      "Average test loss: 0.0017448181455127067\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0017906655848233235\n",
      "Average test loss: 0.001741379369619406\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0017855422400558988\n",
      "Average test loss: 0.001838462306186557\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0017921344989703762\n",
      "Average test loss: 0.0017809375647662414\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0017843919014558197\n",
      "Average test loss: 0.016236831112040415\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0018707570555723375\n",
      "Average test loss: 0.001811737528691689\n",
      "Epoch 278/300\n",
      "Average training loss: 0.001773053136550718\n",
      "Average test loss: 0.0017835428403793938\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0017773109912458394\n",
      "Average test loss: 0.0017622418882739213\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0017836773589046465\n",
      "Average test loss: 0.0017565204983370172\n",
      "Epoch 281/300\n",
      "Average training loss: 0.001780797709710896\n",
      "Average test loss: 0.0018564597871154546\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001783134740880794\n",
      "Average test loss: 0.0020101411660305326\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0017839193720784452\n",
      "Average test loss: 0.001775481399355663\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0017843925741811593\n",
      "Average test loss: 0.0017708455662553508\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0017786912582814693\n",
      "Average test loss: 0.0017741174499193828\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0017809860389679671\n",
      "Average test loss: 0.0017559029582060045\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0017778218866636355\n",
      "Average test loss: 0.0018152899181263314\n",
      "Epoch 288/300\n",
      "Average training loss: 0.001786475438107219\n",
      "Average test loss: 0.001801142210761706\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0017812060144626431\n",
      "Average test loss: 0.0017642891384247277\n",
      "Epoch 290/300\n",
      "Average training loss: 0.001774475674248404\n",
      "Average test loss: 0.0017960869518833028\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0017786863207196195\n",
      "Average test loss: 0.0017258316070462266\n",
      "Epoch 292/300\n",
      "Average training loss: 0.001777431992089583\n",
      "Average test loss: 0.0017846583621576428\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0017781773367896677\n",
      "Average test loss: 0.0018079621236150463\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0017722145037518608\n",
      "Average test loss: 0.001806127888460954\n",
      "Epoch 295/300\n",
      "Average training loss: 0.001776398869438304\n",
      "Average test loss: 0.0017471929896208974\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0017808154712741573\n",
      "Average test loss: 0.0017845385664453108\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0017696819458570745\n",
      "Average test loss: 0.00208458030058278\n",
      "Epoch 298/300\n",
      "Average training loss: 0.001775834190659225\n",
      "Average test loss: 0.0018189285258866019\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0017685583904385566\n",
      "Average test loss: 0.0018775126401128041\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0017708824458014633\n",
      "Average test loss: 0.0017899272503952186\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.25/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 17.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.60\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.91\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.76\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 19.92\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.51\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.24\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.97\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.95\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.07\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10960823508765963\n",
      "Average test loss: 0.034345837516917124\n",
      "Epoch 2/300\n",
      "Average training loss: 0.019873274137576422\n",
      "Average test loss: 0.026894217398431567\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0169536135925187\n",
      "Average test loss: 0.010305846619937154\n",
      "Epoch 4/300\n",
      "Average training loss: 0.015408242400321695\n",
      "Average test loss: 0.009779982426514228\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014335512669550047\n",
      "Average test loss: 0.028426721677184106\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013501109267274538\n",
      "Average test loss: 0.015517374361554781\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01267437117960718\n",
      "Average test loss: 0.00861976852433549\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01203853105339739\n",
      "Average test loss: 0.008379941106670433\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011442527485390505\n",
      "Average test loss: 0.007679658475021521\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01083047271023194\n",
      "Average test loss: 0.007442921287069718\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010327527579334047\n",
      "Average test loss: 0.019290106796556048\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009895519076122178\n",
      "Average test loss: 0.00772581253035201\n",
      "Epoch 13/300\n",
      "Average training loss: 0.00958907667795817\n",
      "Average test loss: 0.0085582823852698\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009315353261927764\n",
      "Average test loss: 0.006585284845696556\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009104760496980615\n",
      "Average test loss: 0.0065598151870071885\n",
      "Epoch 16/300\n",
      "Average training loss: 0.008909312108324633\n",
      "Average test loss: 0.008281794503331184\n",
      "Epoch 17/300\n",
      "Average training loss: 0.008777350233660804\n",
      "Average test loss: 0.012143267857117786\n",
      "Epoch 18/300\n",
      "Average training loss: 0.008633049244681994\n",
      "Average test loss: 0.021285767363177405\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00850088091691335\n",
      "Average test loss: 0.006243421360022492\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008366043944739634\n",
      "Average test loss: 0.006164202647076713\n",
      "Epoch 21/300\n",
      "Average training loss: 0.009540624757607778\n",
      "Average test loss: 0.012009680218166775\n",
      "Epoch 22/300\n",
      "Average training loss: 0.008328065507941776\n",
      "Average test loss: 0.011159064071045981\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008127808922694789\n",
      "Average test loss: 0.006020554451478853\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008006499180363284\n",
      "Average test loss: 0.007222012415942219\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007906973265939289\n",
      "Average test loss: 0.005848783842805359\n",
      "Epoch 26/300\n",
      "Average training loss: 0.007805202912539244\n",
      "Average test loss: 0.009631340102189117\n",
      "Epoch 27/300\n",
      "Average training loss: 0.007722087572018306\n",
      "Average test loss: 0.005849153781102763\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007621135339140892\n",
      "Average test loss: 0.48254917075360815\n",
      "Epoch 29/300\n",
      "Average training loss: 0.007566964175966051\n",
      "Average test loss: 0.007556287575099204\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0075058812693589265\n",
      "Average test loss: 0.006385092303570774\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007409202883227004\n",
      "Average test loss: 0.06048684136238363\n",
      "Epoch 32/300\n",
      "Average training loss: 0.007334522883511252\n",
      "Average test loss: 1.1351938729484876\n",
      "Epoch 33/300\n",
      "Average training loss: 0.00815899646571941\n",
      "Average test loss: 0.00801979329643978\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008132902562618255\n",
      "Average test loss: 0.005710096052951283\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0074908743103345235\n",
      "Average test loss: 0.010943078000512387\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007331487310429414\n",
      "Average test loss: 0.013416369222932392\n",
      "Epoch 37/300\n",
      "Average training loss: 0.00724597341236141\n",
      "Average test loss: 0.005764548149787717\n",
      "Epoch 38/300\n",
      "Average training loss: 0.007190593504243427\n",
      "Average test loss: 0.006000882228629456\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007191408812585804\n",
      "Average test loss: 0.0065507242336041395\n",
      "Epoch 40/300\n",
      "Average training loss: 0.007171887100156811\n",
      "Average test loss: 0.005708197113540437\n",
      "Epoch 41/300\n",
      "Average training loss: 0.007204270176589489\n",
      "Average test loss: 0.37317180916501413\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0070768335124270785\n",
      "Average test loss: 0.0054280117737750214\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007207561932918098\n",
      "Average test loss: 0.007128799883027871\n",
      "Epoch 44/300\n",
      "Average training loss: 0.007056087645805544\n",
      "Average test loss: 0.005837691937055853\n",
      "Epoch 45/300\n",
      "Average training loss: 0.008204072054475545\n",
      "Average test loss: 0.005713274472289615\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0074424851420852875\n",
      "Average test loss: 0.02582639016293817\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007130923543125391\n",
      "Average test loss: 0.0057005261348353495\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0070384949321548144\n",
      "Average test loss: 0.007108776005605857\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007301019151177671\n",
      "Average test loss: 0.01339434429547853\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007047130105810033\n",
      "Average test loss: 0.22699377554655076\n",
      "Epoch 51/300\n",
      "Average training loss: 0.006934922201765908\n",
      "Average test loss: 0.16013954105559322\n",
      "Epoch 52/300\n",
      "Average training loss: 0.006992382010238038\n",
      "Average test loss: 0.005447667165348927\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0068811997415290935\n",
      "Average test loss: 0.04920685385146903\n",
      "Epoch 54/300\n",
      "Average training loss: 0.006882428248723348\n",
      "Average test loss: 0.005760688346293237\n",
      "Epoch 55/300\n",
      "Average training loss: 0.008096662138071324\n",
      "Average test loss: 0.8259927246260146\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006981149516171879\n",
      "Average test loss: 85.83429293772082\n",
      "Epoch 57/300\n",
      "Average training loss: 0.006846106663760212\n",
      "Average test loss: 0.08756216503348616\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0068414049732188386\n",
      "Average test loss: 0.3163776591949993\n",
      "Epoch 59/300\n",
      "Average training loss: 0.006764354838265313\n",
      "Average test loss: 0.005314800668093893\n",
      "Epoch 60/300\n",
      "Average training loss: 0.006736770376149151\n",
      "Average test loss: 0.42037351327513656\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00674185156615244\n",
      "Average test loss: 0.006365249552246597\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006697167472706901\n",
      "Average test loss: 0.029527995674146545\n",
      "Epoch 63/300\n",
      "Average training loss: 0.006672996932847632\n",
      "Average test loss: 0.017630903411242696\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014402775892366966\n",
      "Average test loss: 0.008286006107098527\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009416165134145154\n",
      "Average test loss: 0.037725866965535614\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00821509141392178\n",
      "Average test loss: 0.006032728026310603\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007699583469165696\n",
      "Average test loss: 0.04900103736337688\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007388909661935435\n",
      "Average test loss: 0.0689378121263451\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0071647129559682474\n",
      "Average test loss: 0.005382842956731717\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007099974299470584\n",
      "Average test loss: 0.005417793331046899\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0069043689486053255\n",
      "Average test loss: 0.005308276274965869\n",
      "Epoch 72/300\n",
      "Average training loss: 0.006807702659318845\n",
      "Average test loss: 0.0052976064023872215\n",
      "Epoch 73/300\n",
      "Average training loss: 0.006755355675601297\n",
      "Average test loss: 0.005284100830968883\n",
      "Epoch 74/300\n",
      "Average training loss: 0.006681813020673063\n",
      "Average test loss: 0.480297772463825\n",
      "Epoch 75/300\n",
      "Average training loss: 0.006657469432387087\n",
      "Average test loss: 1.5274262771507103\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0066292101475927566\n",
      "Average test loss: 5.521815093461011\n",
      "Epoch 77/300\n",
      "Average training loss: 0.007104515882415904\n",
      "Average test loss: 1.6275194523019922\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0067195061652196774\n",
      "Average test loss: 0.0052392300640543305\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006571725374708573\n",
      "Average test loss: 0.03948157971310947\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006533355075452063\n",
      "Average test loss: 0.007011199249161614\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006509725927892659\n",
      "Average test loss: 0.04986017224854893\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006508833396351999\n",
      "Average test loss: 25.879381050844987\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006495645409656896\n",
      "Average test loss: 54350.54198170351\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006477948483493593\n",
      "Average test loss: 1351.2739178500706\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006449105500761006\n",
      "Average test loss: 2284.2008481207868\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006442960262712505\n",
      "Average test loss: 0.36740860167808004\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006418873977743917\n",
      "Average test loss: 0.005478518555561702\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006431182514462206\n",
      "Average test loss: 41.972452749736604\n",
      "Epoch 89/300\n",
      "Average training loss: 0.006396537743508816\n",
      "Average test loss: 0.005269961665487951\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006648890096694231\n",
      "Average test loss: 0.005563056515322792\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00664123481967383\n",
      "Average test loss: 0.16804192404614554\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006451505253505375\n",
      "Average test loss: 0.00525107612212499\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0063420499083068635\n",
      "Average test loss: 0.005337931438866589\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0069954062894814545\n",
      "Average test loss: 0.04205510778559579\n",
      "Epoch 95/300\n",
      "Average training loss: 0.006408983159396383\n",
      "Average test loss: 0.46288030497233074\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006293827385124233\n",
      "Average test loss: 2.053928027048707\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0062741785297791165\n",
      "Average test loss: 0.7364941971914636\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006253915837655465\n",
      "Average test loss: 0.019125718219412696\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006233955658144421\n",
      "Average test loss: 0.005253064225117366\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0071880271977020635\n",
      "Average test loss: 4.924996474319034\n",
      "Epoch 101/300\n",
      "Average training loss: 0.006531253084540367\n",
      "Average test loss: 0.006006057646125555\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006319304027698106\n",
      "Average test loss: 0.006531029142853286\n",
      "Epoch 103/300\n",
      "Average training loss: 0.00621231401960055\n",
      "Average test loss: 2.962040572891633\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00622247918902172\n",
      "Average test loss: 0.00674050673097372\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006222370266500446\n",
      "Average test loss: 0.006373149728609456\n",
      "Epoch 106/300\n",
      "Average training loss: 0.006683471824973822\n",
      "Average test loss: 327.0719610785842\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0063285788156920006\n",
      "Average test loss: 0.007140850078728464\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006156903566999568\n",
      "Average test loss: 0.6500292658673392\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0061238924318717585\n",
      "Average test loss: 6.938473760863559\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006094728891220358\n",
      "Average test loss: 7.257002603802416\n",
      "Epoch 111/300\n",
      "Average training loss: 0.006069092327521907\n",
      "Average test loss: 33.836162611968405\n",
      "Epoch 112/300\n",
      "Average training loss: 0.3962112758544584\n",
      "Average test loss: 1.0792872440218926\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02218334718214141\n",
      "Average test loss: 0.009447774888740646\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01712784418546491\n",
      "Average test loss: 0.008072630893439054\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014732473166452514\n",
      "Average test loss: 0.009021457553737693\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01316865477628178\n",
      "Average test loss: 0.0648455009907484\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012128157437675528\n",
      "Average test loss: 0.00721865901350975\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011312076037128766\n",
      "Average test loss: 0.01808886104491022\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01072037792040242\n",
      "Average test loss: 0.017183102935552596\n",
      "Epoch 120/300\n",
      "Average training loss: 0.010189763758745458\n",
      "Average test loss: 0.01094437642312712\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009780170525941583\n",
      "Average test loss: 0.03444940056900184\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009409297866953744\n",
      "Average test loss: 0.009573145362238088\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009070485038061937\n",
      "Average test loss: 0.17450214953389434\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008805756743583415\n",
      "Average test loss: 0.03036216844452752\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008582606363627645\n",
      "Average test loss: 0.1824716825120979\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00835403177804417\n",
      "Average test loss: 0.017669990367359584\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00815370904819833\n",
      "Average test loss: 0.010544230246709453\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007954912100815111\n",
      "Average test loss: 0.008816064028069377\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0077856508957015146\n",
      "Average test loss: 0.052241936423712304\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00759326925004522\n",
      "Average test loss: 0.35540547954456675\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007457465605189403\n",
      "Average test loss: 0.46847067965070405\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007352477102643914\n",
      "Average test loss: 1.340212905128797\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007196119320475393\n",
      "Average test loss: 0.10379495765641332\n",
      "Epoch 134/300\n",
      "Average training loss: 0.007063318972786267\n",
      "Average test loss: 0.1045910328966048\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0069353747098810144\n",
      "Average test loss: 0.19975128956635793\n",
      "Epoch 136/300\n",
      "Average training loss: 0.006816174927891956\n",
      "Average test loss: 5.115462623619371\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006712175255848302\n",
      "Average test loss: 0.005686784019486772\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006615725676218669\n",
      "Average test loss: 0.20417403944002258\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006527200150820944\n",
      "Average test loss: 0.10472132992661662\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006441201027068827\n",
      "Average test loss: 221.69295599068536\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006397708306709925\n",
      "Average test loss: 0.005292733436243402\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006333481481091844\n",
      "Average test loss: 3.614842646802879\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006317324300193125\n",
      "Average test loss: 0.045886972103474866\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0062550773297746975\n",
      "Average test loss: 55.139322618891796\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006344524473779731\n",
      "Average test loss: 46.44942887784044\n",
      "Epoch 146/300\n",
      "Average training loss: 0.006233669725557168\n",
      "Average test loss: 24.02212424430251\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006211465130249659\n",
      "Average test loss: 561.0384921407501\n",
      "Epoch 148/300\n",
      "Average training loss: 0.006169550916386975\n",
      "Average test loss: 59.1411389572074\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0061184818252093265\n",
      "Average test loss: 0.8794478030304114\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006168261019719972\n",
      "Average test loss: 0.01336699780345791\n",
      "Epoch 151/300\n",
      "Average training loss: 0.006075675700273779\n",
      "Average test loss: 0.006691555997563733\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0063077206131484775\n",
      "Average test loss: 0.023684879642393852\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006015695304713315\n",
      "Average test loss: 0.005430723402235243\n",
      "Epoch 154/300\n",
      "Average training loss: 0.005990750359578265\n",
      "Average test loss: 0.4643521186432077\n",
      "Epoch 155/300\n",
      "Average training loss: 0.005985093030664656\n",
      "Average test loss: 1.2944013820886613\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00599679746106267\n",
      "Average test loss: 0.09607106429897248\n",
      "Epoch 157/300\n",
      "Average training loss: 0.005981735011355745\n",
      "Average test loss: 0.027322929707666237\n",
      "Epoch 158/300\n",
      "Average training loss: 0.006493748096542226\n",
      "Average test loss: 8.00552356264823\n",
      "Epoch 159/300\n",
      "Average training loss: 0.006496492680990034\n",
      "Average test loss: 0.006558731208244959\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00606156446163853\n",
      "Average test loss: 1.8889634790143206\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0059077190947201515\n",
      "Average test loss: 3.3623516131341455\n",
      "Epoch 162/300\n",
      "Average training loss: 0.005876545166389809\n",
      "Average test loss: 1.4307122926728593\n",
      "Epoch 163/300\n",
      "Average training loss: 0.005853442896571424\n",
      "Average test loss: 0.0053011918866799936\n",
      "Epoch 164/300\n",
      "Average training loss: 0.005836786141826047\n",
      "Average test loss: 2.5827410018816592\n",
      "Epoch 165/300\n",
      "Average training loss: 0.005843013118124671\n",
      "Average test loss: 1769.9381208699544\n",
      "Epoch 166/300\n",
      "Average training loss: 0.005866779282689094\n",
      "Average test loss: 149.00850573924515\n",
      "Epoch 167/300\n",
      "Average training loss: 0.005798861356245147\n",
      "Average test loss: 0.005424067855709129\n",
      "Epoch 168/300\n",
      "Average training loss: 0.005806589701523384\n",
      "Average test loss: 14.706191096610493\n",
      "Epoch 169/300\n",
      "Average training loss: 0.006099955673846934\n",
      "Average test loss: 0.005497401073988941\n",
      "Epoch 170/300\n",
      "Average training loss: 0.005887472950336006\n",
      "Average test loss: 91.70251814397176\n",
      "Epoch 171/300\n",
      "Average training loss: 0.005730809739480416\n",
      "Average test loss: 0.007287835477126969\n",
      "Epoch 172/300\n",
      "Average training loss: 0.005898027738763226\n",
      "Average test loss: 0.005433916435059574\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00591545076502694\n",
      "Average test loss: 150.01580464720726\n",
      "Epoch 174/300\n",
      "Average training loss: 0.005847356173313326\n",
      "Average test loss: 1424.0620159577188\n",
      "Epoch 175/300\n",
      "Average training loss: 0.005840948350313637\n",
      "Average test loss: 0.007761686407029629\n",
      "Epoch 176/300\n",
      "Average training loss: 0.005744303987258011\n",
      "Average test loss: 190.75186157660394\n",
      "Epoch 177/300\n",
      "Average training loss: 0.005696920605997245\n",
      "Average test loss: 0.11253474222123623\n",
      "Epoch 178/300\n",
      "Average training loss: 0.005667420333458318\n",
      "Average test loss: 1.3386458980685307\n",
      "Epoch 179/300\n",
      "Average training loss: 0.005658690160347356\n",
      "Average test loss: 0.0350852435881065\n",
      "Epoch 180/300\n",
      "Average training loss: 0.005641252571509944\n",
      "Average test loss: 0.037617693503697716\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04168839999039968\n",
      "Average test loss: 20.87004354164294\n",
      "Epoch 182/300\n",
      "Average training loss: 0.012321411107977232\n",
      "Average test loss: 2393.880513684441\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009430084610978762\n",
      "Average test loss: 3633.248423821391\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008547570057213306\n",
      "Average test loss: 85.53797149077016\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008043173164957099\n",
      "Average test loss: 226.34738858250776\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0076629354912373755\n",
      "Average test loss: 1170.9467485310659\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007330971928520335\n",
      "Average test loss: 1785.1367775018066\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007163413475371069\n",
      "Average test loss: 70078.62385476923\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007046978770858712\n",
      "Average test loss: 651.4392975398881\n",
      "Epoch 190/300\n",
      "Average training loss: 0.006778775906397237\n",
      "Average test loss: 53253.228319553666\n",
      "Epoch 191/300\n",
      "Average training loss: 0.006588943542291721\n",
      "Average test loss: 0.7147139652868112\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00634049176880055\n",
      "Average test loss: 2854.573116805656\n",
      "Epoch 193/300\n",
      "Average training loss: 0.006160615273233917\n",
      "Average test loss: 22.048501728589336\n",
      "Epoch 194/300\n",
      "Average training loss: 0.005964602182308833\n",
      "Average test loss: 605.0218109797504\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005915464438084099\n",
      "Average test loss: 13.52318345391916\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005909073640074995\n",
      "Average test loss: 26870580.304444443\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00812486669421196\n",
      "Average test loss: 1148.637793940885\n",
      "Epoch 198/300\n",
      "Average training loss: 0.006221661276701424\n",
      "Average test loss: 3.1506930617069204\n",
      "Epoch 199/300\n",
      "Average training loss: 0.005941294303784768\n",
      "Average test loss: 314.4259179075302\n",
      "Epoch 200/300\n",
      "Average training loss: 0.005847885732021597\n",
      "Average test loss: 184.45832144418029\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00574181351562341\n",
      "Average test loss: 345.7259012364712\n",
      "Epoch 202/300\n",
      "Average training loss: 0.005812528927293089\n",
      "Average test loss: 5385.087920433008\n",
      "Epoch 203/300\n",
      "Average training loss: 0.006145691703591082\n",
      "Average test loss: 0.005411558128479454\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005855114551468028\n",
      "Average test loss: 47.72495799844215\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0057534349713888435\n",
      "Average test loss: 78371.63505555555\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0056799942888319495\n",
      "Average test loss: 7.186722726266003\n",
      "Epoch 207/300\n",
      "Average training loss: 0.005643209772391452\n",
      "Average test loss: 0.11449412046041754\n",
      "Epoch 208/300\n",
      "Average training loss: 0.00563228695674075\n",
      "Average test loss: 100.88779279374414\n",
      "Epoch 209/300\n",
      "Average training loss: 0.005611119683004088\n",
      "Average test loss: 41.33333797065417\n",
      "Epoch 210/300\n",
      "Average training loss: 0.005617605086002085\n",
      "Average test loss: 1.6328094925963217\n",
      "Epoch 211/300\n",
      "Average training loss: 0.005645083702272839\n",
      "Average test loss: 14.11565325079527\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0055922836206025546\n",
      "Average test loss: 242.83965992966708\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0055700347336630025\n",
      "Average test loss: 8433.331859928769\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006099929699053367\n",
      "Average test loss: 1.9043680404312908\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00618095432780683\n",
      "Average test loss: 1.3901112213465903\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005720208611753251\n",
      "Average test loss: 1326.6751974216038\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005555876062562068\n",
      "Average test loss: 0.005695252950820658\n",
      "Epoch 218/300\n",
      "Average training loss: 0.005498493240111404\n",
      "Average test loss: 1.1377311018146574\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0055688329537709556\n",
      "Average test loss: 17.075515257674788\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0064228903092443945\n",
      "Average test loss: 14.15045126690136\n",
      "Epoch 221/300\n",
      "Average training loss: 0.005603951892919011\n",
      "Average test loss: 0.5721356173041794\n",
      "Epoch 222/300\n",
      "Average training loss: 0.005694088364226951\n",
      "Average test loss: 19904.38290967449\n",
      "Epoch 223/300\n",
      "Average training loss: 0.005556508618510432\n",
      "Average test loss: 0.005569985161638922\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0054899616411162745\n",
      "Average test loss: 0.025628467918684086\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006031435086495346\n",
      "Average test loss: 0.17612099479263027\n",
      "Epoch 226/300\n",
      "Average training loss: 0.005500633982734548\n",
      "Average test loss: 0.01374261711206701\n",
      "Epoch 227/300\n",
      "Average training loss: 0.006800221744510862\n",
      "Average test loss: 115.74006527526925\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006882636590550343\n",
      "Average test loss: 37228.220332139754\n",
      "Epoch 229/300\n",
      "Average training loss: 0.00593666458212667\n",
      "Average test loss: 39.42812260301411\n",
      "Epoch 230/300\n",
      "Average training loss: 0.006699632674455643\n",
      "Average test loss: 14682.476002066278\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0056532316054734915\n",
      "Average test loss: 0.008709577867140373\n",
      "Epoch 232/300\n",
      "Average training loss: 0.005528267639378707\n",
      "Average test loss: 1628.2426189812918\n",
      "Epoch 233/300\n",
      "Average training loss: 0.005475419131004148\n",
      "Average test loss: 1.2346526971956093\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0054394463685651625\n",
      "Average test loss: 4895.821420385544\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0058936690849562485\n",
      "Average test loss: 0.007826102294027805\n",
      "Epoch 236/300\n",
      "Average training loss: 0.00563821581709716\n",
      "Average test loss: 3.969771544614186\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0060817409244676435\n",
      "Average test loss: 0.005389115060576134\n",
      "Epoch 238/300\n",
      "Average training loss: 0.005650743348317014\n",
      "Average test loss: 1.2444515874551403\n",
      "Epoch 239/300\n",
      "Average training loss: 0.005503274150813619\n",
      "Average test loss: 10.881480677547554\n",
      "Epoch 240/300\n",
      "Average training loss: 0.005530207556154993\n",
      "Average test loss: 5.0781759857386355\n",
      "Epoch 241/300\n",
      "Average training loss: 0.005515917848381732\n",
      "Average test loss: 29.74572779305445\n",
      "Epoch 242/300\n",
      "Average training loss: 0.005486188203924232\n",
      "Average test loss: 0.005482292160391807\n",
      "Epoch 243/300\n",
      "Average training loss: 0.005417265824145741\n",
      "Average test loss: 3202.915148921675\n",
      "Epoch 244/300\n",
      "Average training loss: 0.005444354618589083\n",
      "Average test loss: 143.12824197580747\n",
      "Epoch 245/300\n",
      "Average training loss: 0.005572208854059378\n",
      "Average test loss: 159.1331450475057\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0054324120866755645\n",
      "Average test loss: 0.022353699249939785\n",
      "Epoch 247/300\n",
      "Average training loss: 0.005437734627889262\n",
      "Average test loss: 230.63983935581643\n",
      "Epoch 248/300\n",
      "Average training loss: 0.005487731657094426\n",
      "Average test loss: 0.005711389309002293\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0053760044396751454\n",
      "Average test loss: 57.85757785102063\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005388924116061794\n",
      "Average test loss: 117.75025888940361\n",
      "Epoch 251/300\n",
      "Average training loss: 0.005371229395684269\n",
      "Average test loss: 0.04704344941303134\n",
      "Epoch 252/300\n",
      "Average training loss: 0.005349579044514232\n",
      "Average test loss: 38.614529641651444\n",
      "Epoch 253/300\n",
      "Average training loss: 0.005348004286073976\n",
      "Average test loss: 465.12780062820184\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00531625565720929\n",
      "Average test loss: 0.06380297916962041\n",
      "Epoch 255/300\n",
      "Average training loss: 0.005340393577184942\n",
      "Average test loss: 155.0461455117493\n",
      "Epoch 256/300\n",
      "Average training loss: 0.00528794974171453\n",
      "Average test loss: 2097.0103700758186\n",
      "Epoch 257/300\n",
      "Average training loss: 0.005315264750685957\n",
      "Average test loss: 20.741901012813052\n",
      "Epoch 258/300\n",
      "Average training loss: 0.005286379985511303\n",
      "Average test loss: 84948.01094986341\n",
      "Epoch 259/300\n",
      "Average training loss: 0.005329062740421957\n",
      "Average test loss: 427.61914686555104\n",
      "Epoch 260/300\n",
      "Average training loss: 0.005563628571314944\n",
      "Average test loss: 42286.44138082597\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0052649857256975435\n",
      "Average test loss: 0.005583865135494206\n",
      "Epoch 262/300\n",
      "Average training loss: 0.005296504551751746\n",
      "Average test loss: 0.0091380292131669\n",
      "Epoch 263/300\n",
      "Average training loss: 0.005250061476810111\n",
      "Average test loss: 2.5188982387714915\n",
      "Epoch 264/300\n",
      "Average training loss: 0.005279419542600711\n",
      "Average test loss: 24792.904792164183\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0052303174924519325\n",
      "Average test loss: 27.72244823582636\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00522981876341833\n",
      "Average test loss: 1083.3479580124742\n",
      "Epoch 267/300\n",
      "Average training loss: 0.005188541889604595\n",
      "Average test loss: 0.5846610244059315\n",
      "Epoch 268/300\n",
      "Average training loss: 0.005186155921883053\n",
      "Average test loss: 51947.46441048177\n",
      "Epoch 269/300\n",
      "Average training loss: 0.005182926279389196\n",
      "Average test loss: 134.68426622569396\n",
      "Epoch 270/300\n",
      "Average training loss: 0.005209718390885326\n",
      "Average test loss: 7.601468592905336\n",
      "Epoch 271/300\n",
      "Average training loss: 0.005209089617762301\n",
      "Average test loss: 0.2418702883993586\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0051566926961143815\n",
      "Average test loss: 5.659195083374779\n",
      "Epoch 273/300\n",
      "Average training loss: 0.005160455293539498\n",
      "Average test loss: 49.16414500218568\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005155794327871667\n",
      "Average test loss: 0.17372076597478656\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005136515712158548\n",
      "Average test loss: 0.09514683525057302\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0051323515727288195\n",
      "Average test loss: 10.219056331000393\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0051513325683772565\n",
      "Average test loss: 1.1929409677253828\n",
      "Epoch 278/300\n",
      "Average training loss: 0.005119722762455543\n",
      "Average test loss: 32.285561414524295\n",
      "Epoch 279/300\n",
      "Average training loss: 0.005124000552420815\n",
      "Average test loss: 22.453154840028947\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005129897584931718\n",
      "Average test loss: 869.5556605148266\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00531362359970808\n",
      "Average test loss: 229.5858182419373\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005204465558959378\n",
      "Average test loss: 2908.9629463932874\n",
      "Epoch 283/300\n",
      "Average training loss: 0.005382387184434467\n",
      "Average test loss: 99688.97773770904\n",
      "Epoch 284/300\n",
      "Average training loss: 0.005174772110250261\n",
      "Average test loss: 718.9067413888131\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0051430248351146774\n",
      "Average test loss: 755.2283838582933\n",
      "Epoch 286/300\n",
      "Average training loss: 0.005097166264636649\n",
      "Average test loss: 0.04952510393742058\n",
      "Epoch 287/300\n",
      "Average training loss: 0.005085648289157285\n",
      "Average test loss: 0.5273976235969199\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0050727752794822055\n",
      "Average test loss: 3.608203125815425\n",
      "Epoch 289/300\n",
      "Average training loss: 0.005077997698138157\n",
      "Average test loss: 20615.89765833852\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0050845761626131004\n",
      "Average test loss: 14826.574306225879\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00506237269192934\n",
      "Average test loss: 0.5058598262038496\n",
      "Epoch 292/300\n",
      "Average training loss: 0.005078763039989604\n",
      "Average test loss: 4955.906276233099\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0050767144192424086\n",
      "Average test loss: 0.07037823166035943\n",
      "Epoch 294/300\n",
      "Average training loss: 0.005012754052463505\n",
      "Average test loss: 4.808670656926102\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0050079617334736716\n",
      "Average test loss: 12.286902862903558\n",
      "Epoch 296/300\n",
      "Average training loss: 0.005036646095414957\n",
      "Average test loss: 14189.178171093517\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005017482619318697\n",
      "Average test loss: 13.71902829841483\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005018360986063878\n",
      "Average test loss: 1986.0655867254966\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004992957759648561\n",
      "Average test loss: 592.983634339023\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004994292061568963\n",
      "Average test loss: 49.59290807792544\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.09166458209355673\n",
      "Average test loss: 0.03659386237296793\n",
      "Epoch 2/300\n",
      "Average training loss: 0.012466968283057213\n",
      "Average test loss: 0.01054497030377388\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01017529022279713\n",
      "Average test loss: 0.006583195886264244\n",
      "Epoch 4/300\n",
      "Average training loss: 0.009075555843611558\n",
      "Average test loss: 0.006059224363002512\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008284070034821829\n",
      "Average test loss: 0.00981393316677875\n",
      "Epoch 6/300\n",
      "Average training loss: 0.007709328201082018\n",
      "Average test loss: 0.005312572409295373\n",
      "Epoch 7/300\n",
      "Average training loss: 0.007252063101364507\n",
      "Average test loss: 0.005054801409443219\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0068837832249701025\n",
      "Average test loss: 0.00481097315169043\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006567988666809268\n",
      "Average test loss: 0.02976227988799413\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006282106541511085\n",
      "Average test loss: 0.03447889827026261\n",
      "Epoch 11/300\n",
      "Average training loss: 0.006037809770140383\n",
      "Average test loss: 0.0055547536499798295\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005778625070427854\n",
      "Average test loss: 0.01057748582959175\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005600666839629412\n",
      "Average test loss: 0.005133400621099605\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005414622767104043\n",
      "Average test loss: 0.12188390243219005\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005249306554181708\n",
      "Average test loss: 0.00799401763247119\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0051549552447266046\n",
      "Average test loss: 0.28258715113335187\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004980213972843355\n",
      "Average test loss: 0.0062162410583761\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004876630013187726\n",
      "Average test loss: 0.0036101790335443286\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00476815862622526\n",
      "Average test loss: 0.0037271381173696783\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00471726029800872\n",
      "Average test loss: 0.0034469775377462307\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0045763653678198655\n",
      "Average test loss: 0.004017739373776648\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004518059640088015\n",
      "Average test loss: 0.0073016926141248805\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004413083843886853\n",
      "Average test loss: 0.003316475755224625\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00439335708382229\n",
      "Average test loss: 0.006490276571777131\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0042924027053846256\n",
      "Average test loss: 0.1196501574565967\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004234030353215834\n",
      "Average test loss: 0.003398253578071793\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004181208201994499\n",
      "Average test loss: 0.14113547836078538\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0041447076665030586\n",
      "Average test loss: 0.003886720181753238\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004105146772538622\n",
      "Average test loss: 0.012154062574936285\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004191890328294701\n",
      "Average test loss: 0.0031671928850313027\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0040518991127610204\n",
      "Average test loss: 0.0037246054841412438\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004012273616260952\n",
      "Average test loss: 0.004753278901593553\n",
      "Epoch 33/300\n",
      "Average training loss: 0.00402815276881059\n",
      "Average test loss: 0.0032829512202491363\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00392241418796281\n",
      "Average test loss: 0.03759470536642604\n",
      "Epoch 35/300\n",
      "Average training loss: 0.003950677038687798\n",
      "Average test loss: 0.0030500983438558047\n",
      "Epoch 36/300\n",
      "Average training loss: 0.00396403818929361\n",
      "Average test loss: 0.17653879894316196\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0038773807612144284\n",
      "Average test loss: 0.01685638975496921\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003843270240558518\n",
      "Average test loss: 2.070894930622851\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0038286661623666684\n",
      "Average test loss: 1.551571838349104\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003869310255472859\n",
      "Average test loss: 0.182437340595449\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0038264470905479456\n",
      "Average test loss: 0.0030187635192026694\n",
      "Epoch 42/300\n",
      "Average training loss: 0.003751879074300329\n",
      "Average test loss: 6.319371033098963\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003914328502284156\n",
      "Average test loss: 90.62469277032051\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0037328191024975643\n",
      "Average test loss: 0.5650469168292152\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0037285907891475493\n",
      "Average test loss: 0.099410539454884\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0036784241888672114\n",
      "Average test loss: 0.1865377239247577\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0036551821274268957\n",
      "Average test loss: 0.05606772769718534\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0036593289302868977\n",
      "Average test loss: 0.04772653162189656\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0036767668289442858\n",
      "Average test loss: 0.05648366876712276\n",
      "Epoch 50/300\n",
      "Average training loss: 0.003645165574012531\n",
      "Average test loss: 1.5624395567650595\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0036632464265243876\n",
      "Average test loss: 0.371162619056801\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005767398800286982\n",
      "Average test loss: 0.019249125523699655\n",
      "Epoch 53/300\n",
      "Average training loss: 0.004445141304284334\n",
      "Average test loss: 0.003447084458751811\n",
      "Epoch 54/300\n",
      "Average training loss: 0.004053617892372939\n",
      "Average test loss: 0.0030991922022981775\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0038771516517218615\n",
      "Average test loss: 0.08788915811479092\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0037754574082791807\n",
      "Average test loss: 0.003005101252347231\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0037164088843597304\n",
      "Average test loss: 0.00308475890258948\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003707682102504704\n",
      "Average test loss: 0.02814492928981781\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0036704361699521544\n",
      "Average test loss: 0.011522046511785851\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003632751921398772\n",
      "Average test loss: 2.528068746402032\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0036030255218760833\n",
      "Average test loss: 0.00345397227919764\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0035768607459548446\n",
      "Average test loss: 0.003173003690317273\n",
      "Epoch 63/300\n",
      "Average training loss: 0.003940260555181239\n",
      "Average test loss: 2.565528658910758\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0037482252915700277\n",
      "Average test loss: 5.602657003629539\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003592519615880317\n",
      "Average test loss: 0.011416483759673106\n",
      "Epoch 66/300\n",
      "Average training loss: 0.003572110614635878\n",
      "Average test loss: 1030205.9225350388\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0036682583221958743\n",
      "Average test loss: 1.165871290039685\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0035603317163056796\n",
      "Average test loss: 0.04262864145719343\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0035144300508416363\n",
      "Average test loss: 25.033989458704987\n",
      "Epoch 70/300\n",
      "Average training loss: 0.003493346939070357\n",
      "Average test loss: 33.49250467653821\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0034837662069540885\n",
      "Average test loss: 0.011886851163374052\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004102861602480213\n",
      "Average test loss: 0.0030088115787754457\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003603957840758893\n",
      "Average test loss: 0.0030612690982719263\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0034840408948560555\n",
      "Average test loss: 0.0030823228845579757\n",
      "Epoch 75/300\n",
      "Average training loss: 0.003439923547208309\n",
      "Average test loss: 2.6769421715092743\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0034244649602721135\n",
      "Average test loss: 0.003174162096861336\n",
      "Epoch 77/300\n",
      "Average training loss: 0.003417537420367201\n",
      "Average test loss: 0.003623448038059804\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0033963227580404943\n",
      "Average test loss: 24.007972688585518\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0059273338011569445\n",
      "Average test loss: 1116.1213412963152\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006297566197812557\n",
      "Average test loss: 0.21012553686400254\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004578462029496829\n",
      "Average test loss: 1.1615708224475383\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0042299476601183415\n",
      "Average test loss: 0.051152923271887835\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004012013030962811\n",
      "Average test loss: 0.22843757621281677\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0038721191852043074\n",
      "Average test loss: 0.05267743810224864\n",
      "Epoch 85/300\n",
      "Average training loss: 0.003744824815955427\n",
      "Average test loss: 0.9744749052472826\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00361814290823208\n",
      "Average test loss: 3.3056617966774438\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0035508027455459037\n",
      "Average test loss: 45.01693948410948\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0035110784398598802\n",
      "Average test loss: 26.350554007322422\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003465179165204366\n",
      "Average test loss: 10.564759847385188\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003442631208855245\n",
      "Average test loss: 0.1317044740964969\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003421881280011601\n",
      "Average test loss: 8.24878020392855\n",
      "Epoch 92/300\n",
      "Average training loss: 0.003403827734912435\n",
      "Average test loss: 30.71352395732618\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0039524644977516595\n",
      "Average test loss: 0.013037450790612234\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0035702540425376758\n",
      "Average test loss: 2.0604294502652354\n",
      "Epoch 95/300\n",
      "Average training loss: 0.003451878485787246\n",
      "Average test loss: 0.24973717294436776\n",
      "Epoch 96/300\n",
      "Average training loss: 0.003521034705763062\n",
      "Average test loss: 0.7451531198546291\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0034442685258885223\n",
      "Average test loss: 7.470432834704303\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0034281282977511487\n",
      "Average test loss: 1.3755195848031176\n",
      "Epoch 99/300\n",
      "Average training loss: 0.003391402193966011\n",
      "Average test loss: 0.01434437892306596\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003379065874343117\n",
      "Average test loss: 0.8112420858335164\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0034133538228149217\n",
      "Average test loss: 0.3272162908506062\n",
      "Epoch 102/300\n",
      "Average training loss: 0.003357568710214562\n",
      "Average test loss: 817.2853097189634\n",
      "Epoch 103/300\n",
      "Average training loss: 0.003321034678361482\n",
      "Average test loss: 0.13906615065193426\n",
      "Epoch 104/300\n",
      "Average training loss: 0.003388096584834986\n",
      "Average test loss: 0.4690087777367897\n",
      "Epoch 105/300\n",
      "Average training loss: 0.003293118195815219\n",
      "Average test loss: 0.12058956776517961\n",
      "Epoch 106/300\n",
      "Average training loss: 0.003303351258031196\n",
      "Average test loss: 3089.5827925034596\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003302704794539346\n",
      "Average test loss: 0.0029869704339653253\n",
      "Epoch 108/300\n",
      "Average training loss: 0.003258283395941059\n",
      "Average test loss: 1495.3536304796007\n",
      "Epoch 109/300\n",
      "Average training loss: 0.003257328943452901\n",
      "Average test loss: 0.7702190336204238\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0034708993412140344\n",
      "Average test loss: 0.0703858056863149\n",
      "Epoch 111/300\n",
      "Average training loss: 0.003225883984524343\n",
      "Average test loss: 1.0477711234247933\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0032659870268156132\n",
      "Average test loss: 0.2654362461668336\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0032312392375121513\n",
      "Average test loss: 33.03630144295593\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0032876805696222516\n",
      "Average test loss: 0.6342087090013342\n",
      "Epoch 115/300\n",
      "Average training loss: 0.003169452967329158\n",
      "Average test loss: 4.352950473616521\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0031573626303838358\n",
      "Average test loss: 878.9159809357508\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003460683135729697\n",
      "Average test loss: 0.2288253407958481\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0032397090351829927\n",
      "Average test loss: 358.48568727487657\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0032031576902502115\n",
      "Average test loss: 66.45228004892253\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0031627635622604026\n",
      "Average test loss: 1741.8139633049666\n",
      "Epoch 121/300\n",
      "Average training loss: 0.003149893071502447\n",
      "Average test loss: 93099.65398772746\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0032643962589402995\n",
      "Average test loss: 0.003038207092218929\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0032069990552133984\n",
      "Average test loss: 0.0035717299837205144\n",
      "Epoch 124/300\n",
      "Average training loss: 0.003156349709878365\n",
      "Average test loss: 0.024945951206609605\n",
      "Epoch 125/300\n",
      "Average training loss: 0.003126442407982217\n",
      "Average test loss: 4.499826431840658\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00312387386109266\n",
      "Average test loss: 0.13179943683660691\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0031001519070317347\n",
      "Average test loss: 0.8997011025821169\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0031208928916603325\n",
      "Average test loss: 2901.209125665193\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0031074759186142022\n",
      "Average test loss: 0.13571180297765467\n",
      "Epoch 130/300\n",
      "Average training loss: 0.003109835079974598\n",
      "Average test loss: 838.3429269634429\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0032757996008214024\n",
      "Average test loss: 0.18530273802454272\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0030716555472463367\n",
      "Average test loss: 0.267787332902352\n",
      "Epoch 133/300\n",
      "Average training loss: 0.00310824741050601\n",
      "Average test loss: 3.9857677552782826\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0030878227462785113\n",
      "Average test loss: 4655.754738908418\n",
      "Epoch 135/300\n",
      "Average training loss: 0.003162407802210914\n",
      "Average test loss: 0.1295323263965547\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0031071568137655656\n",
      "Average test loss: 101.5668136622707\n",
      "Epoch 137/300\n",
      "Average training loss: 0.003071976912311382\n",
      "Average test loss: 1.641607672938456\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0030240397616806956\n",
      "Average test loss: 2.574219115909603\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003019976132445865\n",
      "Average test loss: 4.348901501857158\n",
      "Epoch 140/300\n",
      "Average training loss: 0.003048151807031698\n",
      "Average test loss: 1.489176235260235\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0029963317941874264\n",
      "Average test loss: 22.28923126955165\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0030008391071524886\n",
      "Average test loss: 0.0030512365963723926\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0030051988992426133\n",
      "Average test loss: 1.4774424653570686\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0030002996559358305\n",
      "Average test loss: 8.00871362260729\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003005448863738113\n",
      "Average test loss: 18.925557178787265\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002977861675537295\n",
      "Average test loss: 1.804715654007987\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0029871779899630284\n",
      "Average test loss: 0.007966296620046099\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002978693803047968\n",
      "Average test loss: 2392.2384811162437\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0029638324663456943\n",
      "Average test loss: 0.3187984628898816\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002965589169619812\n",
      "Average test loss: 2.475853669569103\n",
      "Epoch 151/300\n",
      "Average training loss: 0.003209102655657464\n",
      "Average test loss: 80.80972007540406\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0029607147797942163\n",
      "Average test loss: 0.0033355999098469815\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0032511140815913676\n",
      "Average test loss: 0.37981434754613375\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0029698319921476974\n",
      "Average test loss: 14.540528135225177\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0029866365099118814\n",
      "Average test loss: 0.014960245630807346\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0029375421340680783\n",
      "Average test loss: 69.52947421205872\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0029166713775032097\n",
      "Average test loss: 0.26033641102910043\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0029606699181927575\n",
      "Average test loss: 19352.94185655594\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0028984536890768343\n",
      "Average test loss: 2.824607736568484\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0029123332038935687\n",
      "Average test loss: 5394.798809489845\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0028991482471012408\n",
      "Average test loss: 163.56266124083163\n",
      "Epoch 162/300\n",
      "Average training loss: 0.00289208905874855\n",
      "Average test loss: 15.549308542602178\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002897329942633708\n",
      "Average test loss: 18149.27911008962\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002920131718325946\n",
      "Average test loss: 0.3894064871126579\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0028830265185485282\n",
      "Average test loss: 1.2744838612104457\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0028732113022771144\n",
      "Average test loss: 25.098911126652524\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0028495295534117352\n",
      "Average test loss: 3.0529448528039373\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0028496115607106022\n",
      "Average test loss: 18081.409857253424\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00285600565539466\n",
      "Average test loss: 0.0031222986427860127\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002854875662881467\n",
      "Average test loss: 520464.3612157495\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0028781783916056155\n",
      "Average test loss: 42.01485492704312\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0032130911969062356\n",
      "Average test loss: 41.04500492078397\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0029961302764713762\n",
      "Average test loss: 0.0693135738161703\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0029455472686224513\n",
      "Average test loss: 0.007363681259668535\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0028890398749046855\n",
      "Average test loss: 3.582991796365215\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0029253295457197562\n",
      "Average test loss: 1.3954980312786582\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0028720361880130237\n",
      "Average test loss: 48.409060933728185\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00286778902531498\n",
      "Average test loss: 0.229416332859339\n",
      "Epoch 179/300\n",
      "Average training loss: 0.002826866936766439\n",
      "Average test loss: 13244.267427521369\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0028402584003698493\n",
      "Average test loss: 1.3435715951936114\n",
      "Epoch 181/300\n",
      "Average training loss: 0.002832929048480259\n",
      "Average test loss: 0.0033432938895291754\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0028365564315269392\n",
      "Average test loss: 0.8867847478669136\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0028307951539754865\n",
      "Average test loss: 0.15203333623127804\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0028065319758736424\n",
      "Average test loss: 3105.5332233751087\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0028481636914528077\n",
      "Average test loss: 4.806959900864297\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0028099625029911596\n",
      "Average test loss: 3367.963080832839\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0028157884433037705\n",
      "Average test loss: 3315.064386650921\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002799338564897577\n",
      "Average test loss: 0.004460577012764083\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0027936354463713037\n",
      "Average test loss: 49117.14373707411\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0027955550079544387\n",
      "Average test loss: 1069.354892096296\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002808428087685671\n",
      "Average test loss: 4.081919857246181\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002777329191358553\n",
      "Average test loss: 0.03005559318719639\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0030089881618817647\n",
      "Average test loss: 0.22942528874033855\n",
      "Epoch 194/300\n",
      "Average training loss: 0.002807220643800166\n",
      "Average test loss: 0.17912448310520915\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002789654265261359\n",
      "Average test loss: 681.4132758895738\n",
      "Epoch 196/300\n",
      "Average training loss: 0.002793063472956419\n",
      "Average test loss: 37.4510936571757\n",
      "Epoch 197/300\n",
      "Average training loss: 0.002756253847438428\n",
      "Average test loss: 44.94410986820815\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00276883137681418\n",
      "Average test loss: 387.45706813134944\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002809553982069095\n",
      "Average test loss: 0.05892667317162785\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002768993290969067\n",
      "Average test loss: 254.99991610155587\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0027709682749377357\n",
      "Average test loss: 25.134727543372247\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0027823857004857728\n",
      "Average test loss: 0.3817368013560772\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0027645536029918326\n",
      "Average test loss: 0.0031339099156773753\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0027411545697185727\n",
      "Average test loss: 0.004632789475532869\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0027454779884881443\n",
      "Average test loss: 1.6351533446669992\n",
      "Epoch 206/300\n",
      "Average training loss: 0.002731507618808084\n",
      "Average test loss: 0.003343312363243765\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0027203729852206177\n",
      "Average test loss: 6679.740265724503\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002757339986248149\n",
      "Average test loss: 0.2712562550128334\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0027660237275477914\n",
      "Average test loss: 175458.70563194444\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0027063648083971606\n",
      "Average test loss: 230.41936971775567\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002704024926241901\n",
      "Average test loss: 259.55154284468296\n",
      "Epoch 212/300\n",
      "Average training loss: 0.002735357913085156\n",
      "Average test loss: 1.9213393426736196\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0027148856905599436\n",
      "Average test loss: 57.97063152287321\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0027152342918432422\n",
      "Average test loss: 325.40906372602285\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0026885130589620935\n",
      "Average test loss: 64.23066267674582\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002771122093208962\n",
      "Average test loss: 305.1660151979128\n",
      "Epoch 217/300\n",
      "Average training loss: 0.002741940443507499\n",
      "Average test loss: 7.092353602627913\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0027550615266793304\n",
      "Average test loss: 485.7942539315025\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0027025081851623123\n",
      "Average test loss: 0.004081101932045486\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002699276225020488\n",
      "Average test loss: 82.55540400930329\n",
      "Epoch 221/300\n",
      "Average training loss: 0.002686244008441766\n",
      "Average test loss: 60.55190370733539\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0026824579909443856\n",
      "Average test loss: 211.02267131129963\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0026916208821866246\n",
      "Average test loss: 0.19020576205021805\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002683463193062279\n",
      "Average test loss: 1.072345234879189\n",
      "Epoch 225/300\n",
      "Average training loss: 0.00267461050943368\n",
      "Average test loss: 1.5720628205210798\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0026670875559664433\n",
      "Average test loss: 115.49754942036876\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002686035189570652\n",
      "Average test loss: 27.927536718365218\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0027006020467314454\n",
      "Average test loss: 0.256576155455783\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002656504121919473\n",
      "Average test loss: 8070.268563957362\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0026606860854145554\n",
      "Average test loss: 694.0377095404731\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0026507187974121834\n",
      "Average test loss: 1.0258645755737605\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0026425244810266628\n",
      "Average test loss: 146.98499768840438\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0026472585865606863\n",
      "Average test loss: 0.20878237709237468\n",
      "Epoch 234/300\n",
      "Average training loss: 0.002652679129296707\n",
      "Average test loss: 0.1133124824108349\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0026328900708920424\n",
      "Average test loss: 2.7994273655911286\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002632996891314785\n",
      "Average test loss: 2.8767446134198043\n",
      "Epoch 237/300\n",
      "Average training loss: 0.002637597990118795\n",
      "Average test loss: 10.974625465434872\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0026302134727852213\n",
      "Average test loss: 6.0505563091014825\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002622524837238921\n",
      "Average test loss: 0.747956568951408\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0026989961904991004\n",
      "Average test loss: 0.026420901277826894\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0026117467031710677\n",
      "Average test loss: 57443.18643485832\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002616208891280823\n",
      "Average test loss: 3.598534320626822\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002609689898788929\n",
      "Average test loss: 5.616803347207606\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002621516161287824\n",
      "Average test loss: 0.004832982026040554\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002623041186067793\n",
      "Average test loss: 9.320601746189926\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002605127204209566\n",
      "Average test loss: 8295.944265332326\n",
      "Epoch 247/300\n",
      "Average training loss: 0.002616249152769645\n",
      "Average test loss: 0.004805414766487148\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002598134446889162\n",
      "Average test loss: 2.606534853599138\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0025893364770130977\n",
      "Average test loss: 6.169789758754066\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0027712775839285717\n",
      "Average test loss: 0.06631661582158671\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0026333344959550433\n",
      "Average test loss: 2.5425673614674142\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002626420521073871\n",
      "Average test loss: 0.3781423985515204\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0026037060194131403\n",
      "Average test loss: 1178616.9559908165\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0026059079027424257\n",
      "Average test loss: 0.5729439040645957\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002612552826706734\n",
      "Average test loss: 4.718294778118531\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002652772704553273\n",
      "Average test loss: 26824863.776812136\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0026675664573493933\n",
      "Average test loss: 1758.297370880348\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0026021634681771197\n",
      "Average test loss: 2.500252674996025\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0026110625360161064\n",
      "Average test loss: 61957.549511609075\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0026233788482430908\n",
      "Average test loss: 0.48810670697978803\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0026099843666371374\n",
      "Average test loss: 18.101299959483246\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0025765440937959485\n",
      "Average test loss: 10931.054639512804\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0026513108836693894\n",
      "Average test loss: 6.0674221452714665\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0026475605768048103\n",
      "Average test loss: 0.03717711542919278\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002644799925800827\n",
      "Average test loss: 36.78086406561567\n",
      "Epoch 270/300\n",
      "Average training loss: 0.002588204281611575\n",
      "Average test loss: 34413.89570042996\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0025923524745222596\n",
      "Average test loss: 7.650782353651399\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0025957090800835026\n",
      "Average test loss: 5.758260215861102\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002595116826498674\n",
      "Average test loss: 288.02229554773703\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0026149156940066154\n",
      "Average test loss: 0.6311315235868097\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0025923554295053087\n",
      "Average test loss: 17858.936642642297\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0025684470091429023\n",
      "Average test loss: 320.5750127853693\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002565055689050092\n",
      "Average test loss: 644.9716633920074\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0025704795097311337\n",
      "Average test loss: 163.47404754225497\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0025879109072395497\n",
      "Average test loss: 0.010438840416363544\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0025678055819330944\n",
      "Average test loss: 31.312391026405823\n",
      "Epoch 281/300\n",
      "Average training loss: 0.002567023999989033\n",
      "Average test loss: 0.0032457207370963363\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0025447055304216016\n",
      "Average test loss: 0.02024433348224395\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0025498184711775845\n",
      "Average test loss: 1.0211993249480924\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002568984111977948\n",
      "Average test loss: 1.6479932982896766\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0025868927898506323\n",
      "Average test loss: 3.159151698483361\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002543069253572159\n",
      "Average test loss: 7.356113376937807\n",
      "Epoch 287/300\n",
      "Average training loss: 0.002616516570043233\n",
      "Average test loss: 5417.701001856192\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0027898534970978895\n",
      "Average test loss: 6621.062121321523\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002565481118030018\n",
      "Average test loss: 4.352616841712759\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002551721175097757\n",
      "Average test loss: 15.582533468673626\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002556370505442222\n",
      "Average test loss: 0.36364562890513075\n",
      "Epoch 294/300\n",
      "Average training loss: 0.002537840084483226\n",
      "Average test loss: 1546969.3858952457\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0025255810922632615\n",
      "Average test loss: 4925.377003718041\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002535011332171659\n",
      "Average test loss: 0.9910851823083229\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0025522611923515797\n",
      "Average test loss: 6502.145938139939\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0025342464279383423\n",
      "Average test loss: 1035.2617033515655\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0025308929216116666\n",
      "Average test loss: 234599.90501433946\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0025318556475556558\n",
      "Average test loss: 5146920.451026757\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.07791197649389506\n",
      "Average test loss: 0.007714698375099235\n",
      "Epoch 2/300\n",
      "Average training loss: 0.009791196192304293\n",
      "Average test loss: 0.005461514233301083\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007938231488896741\n",
      "Average test loss: 0.004875970514698161\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006988496007190811\n",
      "Average test loss: 0.0044917654006017575\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0063613608334627415\n",
      "Average test loss: 0.004235674864716\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005903218714313375\n",
      "Average test loss: 0.0039455666575166914\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005525778179367383\n",
      "Average test loss: 0.003791068091160721\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0052074843707184\n",
      "Average test loss: 0.0040512985274609595\n",
      "Epoch 9/300\n",
      "Average training loss: 0.004914500747703844\n",
      "Average test loss: 0.0054771254054374165\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0046638235900965\n",
      "Average test loss: 0.0035121057718578313\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0044363633626037175\n",
      "Average test loss: 0.003142203890201118\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004247549230025874\n",
      "Average test loss: 0.003301950838830736\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004077932961905996\n",
      "Average test loss: 0.0030137287410390047\n",
      "Epoch 14/300\n",
      "Average training loss: 0.003928599872936805\n",
      "Average test loss: 0.003943021625901262\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003784959618623058\n",
      "Average test loss: 0.002937267738290959\n",
      "Epoch 16/300\n",
      "Average training loss: 0.003655389621025986\n",
      "Average test loss: 0.0033807931521700487\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0035352683399493498\n",
      "Average test loss: 0.002607410680709614\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003435385982816418\n",
      "Average test loss: 0.0025469874216036666\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0033360727433529164\n",
      "Average test loss: 0.002552805054311951\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0032539819843239253\n",
      "Average test loss: 0.0025929004088458087\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0031864252417451804\n",
      "Average test loss: 0.004755244894470606\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003131383982176582\n",
      "Average test loss: 0.15937649511131977\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0038605267175783713\n",
      "Average test loss: 0.003389776349481609\n",
      "Epoch 24/300\n",
      "Average training loss: 0.003127810988885661\n",
      "Average test loss: 0.002886192907889684\n",
      "Epoch 25/300\n",
      "Average training loss: 0.003019242845268713\n",
      "Average test loss: 0.8835794802241855\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0029620416888760195\n",
      "Average test loss: 0.002247961429051227\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0029436231445935036\n",
      "Average test loss: 0.7874345320595635\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0028983273261951074\n",
      "Average test loss: 0.01116879293281171\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0028582428319172725\n",
      "Average test loss: 0.0027531953422973553\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0028341444952206477\n",
      "Average test loss: 0.02651483965665102\n",
      "Epoch 31/300\n",
      "Average training loss: 0.002769802871056729\n",
      "Average test loss: 0.021472159568634297\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0027335938724378744\n",
      "Average test loss: 0.012460091417034467\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0026502193551924493\n",
      "Average test loss: 0.05727732905828291\n",
      "Epoch 37/300\n",
      "Average training loss: 0.002615349171890153\n",
      "Average test loss: 0.023144276870621577\n",
      "Epoch 38/300\n",
      "Average training loss: 0.002587751935339636\n",
      "Average test loss: 0.002059343737239639\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005004961448721587\n",
      "Average test loss: 43.01147361152205\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0034779762180729046\n",
      "Average test loss: 2.069704052981403\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003054575873538852\n",
      "Average test loss: 0.2337460137349036\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0029047016650438308\n",
      "Average test loss: 0.0022547074239701033\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0028036813406894603\n",
      "Average test loss: 0.010141459242751201\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002737666532811191\n",
      "Average test loss: 25.318495114595112\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0026812352411862877\n",
      "Average test loss: 0.003755915456554956\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0026465131621807815\n",
      "Average test loss: 1.677492852334347\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0026369553431868555\n",
      "Average test loss: 0.06750821576422701\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002584345010833608\n",
      "Average test loss: 0.15518906012508604\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0025560805045275223\n",
      "Average test loss: 5.229106024742126\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002550988520185153\n",
      "Average test loss: 0.01963231494008667\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0025307060525649122\n",
      "Average test loss: 0.0019808061932110126\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002551515526448687\n",
      "Average test loss: 3.311506044122908\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0024958253916766908\n",
      "Average test loss: 0.006243989457169341\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0024938824795600442\n",
      "Average test loss: 0.004894244226730532\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0025206209958220523\n",
      "Average test loss: 0.03776940478467279\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002465379553226133\n",
      "Average test loss: 0.0106084650254084\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0024198598253230255\n",
      "Average test loss: 0.0605727777596977\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002421708493803938\n",
      "Average test loss: 0.27002481172730525\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0024140023921305933\n",
      "Average test loss: 11.49268787470729\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002377148367568023\n",
      "Average test loss: 0.02457840683600969\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002360832367920213\n",
      "Average test loss: 1.9943385716560813\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0024131189557827183\n",
      "Average test loss: 0.33543613841964137\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0023513588387933044\n",
      "Average test loss: 0.1560881864970757\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002329694975581434\n",
      "Average test loss: 2.2953683884922\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0023449378153309226\n",
      "Average test loss: 10.934476484089055\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002504420780680246\n",
      "Average test loss: 16.454302551085128\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0024344670002659162\n",
      "Average test loss: 1.3492087662793282\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0023152076229453087\n",
      "Average test loss: 0.002456149286073115\n",
      "Epoch 71/300\n",
      "Average training loss: 0.002284727085174786\n",
      "Average test loss: 1.9873367834387141\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0022665453302777474\n",
      "Average test loss: 0.003731422068654663\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002273111262462205\n",
      "Average test loss: 4.224216819906608\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002267549243238237\n",
      "Average test loss: 0.25195201134247086\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002241263438430097\n",
      "Average test loss: 0.002630411455614699\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0022515491044355763\n",
      "Average test loss: 29.31371244774262\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0022727195411506628\n",
      "Average test loss: 44.187451721689975\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002217278439965513\n",
      "Average test loss: 90.09498817562643\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002184019249967403\n",
      "Average test loss: 111.71195472293098\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002183563144463632\n",
      "Average test loss: 7.081139023697211\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0021811915745751725\n",
      "Average test loss: 0.08953992133918735\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0022339276946667166\n",
      "Average test loss: 0.2605638387252887\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002165373891798986\n",
      "Average test loss: 3.61022507859187\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0021513677773376304\n",
      "Average test loss: 0.18962213307867448\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0021545204213923876\n",
      "Average test loss: 1.3752657889107036\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002126061717255248\n",
      "Average test loss: 0.1487840869947233\n",
      "Epoch 89/300\n",
      "Average training loss: 0.002166868155925638\n",
      "Average test loss: 0.8132366805424293\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002118814891618159\n",
      "Average test loss: 110.41194585151226\n",
      "Epoch 91/300\n",
      "Average training loss: 0.002098803776419825\n",
      "Average test loss: 3.513700061979807\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002109825481350223\n",
      "Average test loss: 0.007153881454529862\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002094890748357607\n",
      "Average test loss: 0.1542671835720539\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002141874419318305\n",
      "Average test loss: 0.8537800680630737\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0021262618088059957\n",
      "Average test loss: 387.94694266053125\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002123086038976908\n",
      "Average test loss: 8.28897502027866\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0020791509217686125\n",
      "Average test loss: 72231.32711295362\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0020685064143811664\n",
      "Average training loss: 0.002087405114124219\n",
      "Average test loss: 0.00203339916964372\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002063587365930693\n",
      "Average test loss: 128.0504403372498\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0020969796913365524\n",
      "Average test loss: 0.0019399792852604555\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0020363807320180867\n",
      "Average test loss: 3.127378522334827\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0020324612060147857\n",
      "Average test loss: 3.0159007848617105\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0021003918474954033\n",
      "Average test loss: 0.012947414345625375\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0020854744417592884\n",
      "Average test loss: 386.24488172354467\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0020229949617965354\n",
      "Average test loss: 2.217941009061411\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002026027749085592\n",
      "Average test loss: 73.28519755113001\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002004591976240691\n",
      "Average test loss: 69.50656939431673\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0020025738092760243\n",
      "Average test loss: 1397.8750770382765\n",
      "Epoch 112/300\n",
      "Average training loss: 0.001977954435369207\n",
      "Average test loss: 0.31300722323813374\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0021356012677359914\n",
      "Average test loss: 9.585113623210125\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0021248405520907705\n",
      "Average test loss: 0.002071082767823504\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0019953435006447966\n",
      "Average test loss: 0.038808853091465104\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0019732192997924157\n",
      "Average test loss: 12.274206919562072\n",
      "Epoch 117/300\n",
      "Average training loss: 0.00195828697250949\n",
      "Average test loss: 0.01768670362047851\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0019746933383867145\n",
      "Average test loss: 0.5958482393494083\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0019888297172470223\n",
      "Average test loss: 3.101713997676762\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0019528497951105236\n",
      "Average test loss: 1.2563841447368678\n",
      "Epoch 121/300\n",
      "Average training loss: 0.001976953639338414\n",
      "Average test loss: 3692.0960961880237\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002422340609650645\n",
      "Average test loss: 0.009781061533321108\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0020400757459509704\n",
      "Average test loss: 0.0019382395101711154\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0019395315883060297\n",
      "Average test loss: 0.0026046589602612786\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0019198875066099895\n",
      "Average test loss: 2.0675522833193343\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0019230527578749592\n",
      "Average test loss: 0.002753801262213124\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0019031172242636482\n",
      "Average test loss: 0.8023161111173944\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0019049086891528633\n",
      "Average test loss: 3.355165957197961\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0019198024969341026\n",
      "Average test loss: 29.314680872447788\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019142696909192536\n",
      "Average test loss: 0.039853078821467026\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002127971491879887\n",
      "Average test loss: 0.03721467591159874\n",
      "Epoch 132/300\n",
      "Average training loss: 0.001955998432305124\n",
      "Average test loss: 0.06025923393666744\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0019501621048483584\n",
      "Average test loss: 0.27960611948557196\n",
      "Epoch 134/300\n",
      "Average training loss: 0.001931930427853432\n",
      "Average test loss: 40.38556544727749\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0018963627217130528\n",
      "Average test loss: 2.8204253785601923\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0019239475957842337\n",
      "Average test loss: 0.01997921734675765\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018914806904892128\n",
      "Average test loss: 1.1944277916558914\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0018992378865886066\n",
      "Average test loss: 0.01706259705271158\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0019017068553302023\n",
      "Average test loss: 0.15751633438799117\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00187563972061293\n",
      "Average test loss: 0.3120297039879693\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001897366906516254\n",
      "Average test loss: 0.5787466155017416\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0018658323753625154\n",
      "Average test loss: 44.8710890730487\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0018850874761119484\n",
      "Average test loss: 13.920427840898952\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0018509573929218783\n",
      "Average test loss: 45.524445001715584\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0018670960631635453\n",
      "Average test loss: 0.09665294646223387\n",
      "Epoch 149/300\n",
      "Average training loss: 0.001872080899361107\n",
      "Average test loss: 39.82685564376993\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018810745511824887\n",
      "Average test loss: 4632.786256767034\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001862410143845611\n",
      "Average test loss: 0.5095957355950441\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0018354668508594235\n",
      "Average test loss: 0.7563577613858506\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0018448653949631585\n",
      "Average test loss: 0.0021345236760874587\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0018836677914692297\n",
      "Average test loss: 0.7094575399739875\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0018321311907428834\n",
      "Average test loss: 4.169084695023381\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0018042073055273956\n",
      "Average test loss: 0.0707511441177792\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0018174141617491842\n",
      "Average test loss: 167.23477787982176\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0019285217998549341\n",
      "Average test loss: 0.8127489426616166\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0018359288729520307\n",
      "Average test loss: 4.237103359960019\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00180226953762273\n",
      "Average test loss: 2.419786956863685\n",
      "Epoch 161/300\n",
      "Average training loss: 0.001834512848717471\n",
      "Average test loss: 0.30127472869969074\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0018112655273742147\n",
      "Average test loss: 0.02940817689233356\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0018276259060949088\n",
      "Average test loss: 0.09093962014404436\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0018182633677497507\n",
      "Average test loss: 0.8095631954231196\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001819270726138105\n",
      "Average test loss: 31.021465612651575\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0018320333367834488\n",
      "Average test loss: 0.03832901687288864\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0018178450338956383\n",
      "Average test loss: 0.18951682337249318\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0017947386842634943\n",
      "Average test loss: 2.398094973449388\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0017740663522854447\n",
      "Average test loss: 0.09871215960755944\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0017722796375552812\n",
      "Average test loss: 6.680553034643849\n",
      "Epoch 171/300\n",
      "Average training loss: 0.001776520259367923\n",
      "Average test loss: 11.50907804302209\n",
      "Epoch 172/300\n",
      "Average training loss: 0.002144489984752403\n",
      "Average test loss: 2285976.348\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0018105674978966513\n",
      "Average test loss: 3.449062915939424\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0017650807249463267\n",
      "Average test loss: 1.335762010961771\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0017402085092746548\n",
      "Average test loss: 12.67868383208745\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0017414933352524209\n",
      "Average test loss: 1616.0347313667999\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0017496158105010787\n",
      "Average test loss: 10154.48523518426\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0017465421684707205\n",
      "Average test loss: 0.0022918544670359957\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001751333638922208\n",
      "Average test loss: 1485.9375260074262\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0017767289607889122\n",
      "Average test loss: 75.57328375984211\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0017330704392451378\n",
      "Average test loss: 0.9814009369444102\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0017399827489215467\n",
      "Average test loss: 44.29459765862508\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0017550324023597771\n",
      "Average test loss: 0.04625669981828994\n",
      "Epoch 186/300\n",
      "Average training loss: 0.001726936718242036\n",
      "Average test loss: 149.98070555133788\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0017411445262324478\n",
      "Average test loss: 29.708427985074618\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0017586100078705285\n",
      "Average test loss: 0.0020402105167094202\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00172231032140553\n",
      "Average test loss: 19813.551895399305\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0017217903148589863\n",
      "Average test loss: 79.64040592909564\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0017403091441632972\n",
      "Average test loss: 2.8211187813282015\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0017357176919985148\n",
      "Average test loss: 0.5102518057800933\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0017191940432207452\n",
      "Average test loss: 0.5472346013507081\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0017079652550940712\n",
      "Average test loss: 0.7488208101673258\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0017111832635063264\n",
      "Average test loss: 1320.0289314585345\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0017153495808856355\n",
      "Average test loss: 333677.9976306745\n",
      "Epoch 197/300\n",
      "Average training loss: 0.001717717986450427\n",
      "Average test loss: 1.0387375847357843\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0017464168345969584\n",
      "Average test loss: 2335.4739515065735\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0017025876925844286\n",
      "Average test loss: 683.4980018446181\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0016830425799513856\n",
      "Average test loss: 0.15897858696317094\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0016870660585247808\n",
      "Average test loss: 0.7153050854884916\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001685948775563803\n",
      "Average test loss: 73.77925857709596\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0017167444890365004\n",
      "Average test loss: 0.026585846331591407\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0018615272294522987\n",
      "Average test loss: 1.6143137556061977\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0017260067481547595\n",
      "Average test loss: 0.0032046583994395205\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0017190807200968266\n",
      "Average test loss: 1.186905085058883\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0017017648408396376\n",
      "Average test loss: 0.023267964609381225\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0016961100166663528\n",
      "Average test loss: 18.56311043149781\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0017056467161617346\n",
      "Average test loss: 3.9266802049008094\n",
      "Epoch 212/300\n",
      "Average training loss: 0.001701768573270076\n",
      "Average test loss: 0.012759679810661409\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0017414942723181512\n",
      "Average test loss: 0.07423472359114223\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0017254639075448117\n",
      "Average test loss: 0.31214053284697646\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001679654693334467\n",
      "Average test loss: 1103.1181195908455\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0016631698051674498\n",
      "Average test loss: 559.7034953926545\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0016859224447980524\n",
      "Average test loss: 3.4254621518850326\n",
      "Epoch 218/300\n",
      "Average training loss: 0.001746456758843528\n",
      "Average test loss: 0.745408580587142\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0016946151326927873\n",
      "Average test loss: 0.03435025130605532\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0017023074932189452\n",
      "Average test loss: 0.9251417354486055\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0017050503939390182\n",
      "Average test loss: 297.4176149698049\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0017071171581952108\n",
      "Average test loss: 0.9978154976512823\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0016891756784170866\n",
      "Average test loss: 8.769454765485392\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0016672371505863136\n",
      "Average test loss: 17.90272252978136\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0016924224436903994\n",
      "Average test loss: 1.3081501657126677\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0016766929425713088\n",
      "Average test loss: 0.4916008772734139\n",
      "Epoch 229/300\n",
      "Average training loss: 0.001687011104904943\n",
      "Average test loss: 0.7671468761993779\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0016704158400081925\n",
      "Average test loss: 0.04582716316957441\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0016747235238759055\n",
      "Average test loss: 66.02368057390986\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0017013312220159505\n",
      "Average test loss: 32.034240819671915\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0016806751147119534\n",
      "Average test loss: 0.07990453221524756\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0016613062757791744\n",
      "Average test loss: 0.09713128851395514\n",
      "Epoch 235/300\n",
      "Average training loss: 0.001762214950699773\n",
      "Average test loss: 0.8567142189818745\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001645856822013027\n",
      "Average test loss: 1042.8104098334657\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001635933198966086\n",
      "Average test loss: 3.1948333831106623\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0016823910668285356\n",
      "Average test loss: 8.881225878948138\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0016506278089558086\n",
      "Average test loss: 0.497811923599595\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0016456155141608582\n",
      "Average test loss: 2.3627317136492993\n",
      "Epoch 243/300\n",
      "Average training loss: 0.001682350175972614\n",
      "Average test loss: 369.1575020695792\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0016832919930004412\n",
      "Average test loss: 64.04578857791341\n",
      "Epoch 245/300\n",
      "Average training loss: 0.001698618325818744\n",
      "Average test loss: 39.289565712874136\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0017025466894523965\n",
      "Average test loss: 6.103284081224766\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0016783371487011512\n",
      "Average test loss: 0.0027457821029755803\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0016742450375523833\n",
      "Average test loss: 3.0991422331879535\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0016804760576536258\n",
      "Average test loss: 1.0431138347761912\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0016554609729597966\n",
      "Average test loss: 0.06969789874139759\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0016530091437614626\n",
      "Average test loss: 7.242944893134137\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0016412330489191745\n",
      "Average test loss: 17.416825431653194\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0016396418458057775\n",
      "Average test loss: 1.0888745880803714\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0016482821779532566\n",
      "Average test loss: 60.56385558263461\n",
      "Epoch 255/300\n",
      "Average training loss: 0.001645254362995426\n",
      "Average test loss: 203.41085758809413\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0016573066232312057\n",
      "Average test loss: 8.52131417597913\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0016299503358701865\n",
      "Average test loss: 2.890679908331173\n",
      "Epoch 258/300\n",
      "Average training loss: 0.001656883163481123\n",
      "Average test loss: 84.68162118676884\n",
      "Epoch 259/300\n",
      "Average training loss: 0.001640709519593252\n",
      "Average test loss: 0.09401745031815437\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0016647847709763382\n",
      "Average test loss: 2.5903726581136386\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0016588716459357076\n",
      "Average test loss: 0.046757263424495855\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0016423296444118022\n",
      "Average test loss: 0.002094796363574763\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0016900100299260682\n",
      "Average test loss: 19.460539253118448\n",
      "Epoch 266/300\n",
      "Average training loss: 0.001636561165874203\n",
      "Average test loss: 0.007191173628283044\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0016217020387864775\n",
      "Average test loss: 2.362418041576528\n",
      "Epoch 268/300\n",
      "Average training loss: 0.001620341494989892\n",
      "Average test loss: 0.02838223995992707\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0016652948723898994\n",
      "Average test loss: 0.0022143331720597213\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0017343254066589807\n",
      "Average test loss: 0.0066139877463380495\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0016791330702188943\n",
      "Average test loss: 0.07157390877149171\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0016411663860393068\n",
      "Average test loss: 0.922723249591887\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0016356172073218557\n",
      "Average test loss: 0.01080162121127877\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0016255244492656654\n",
      "Average test loss: 1.6789398853133122\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001623415095317695\n",
      "Average test loss: 0.33638403510923187\n",
      "Epoch 276/300\n",
      "Average training loss: 0.001641258869320154\n",
      "Average test loss: 0.1624619283825159\n",
      "Epoch 277/300\n",
      "Average training loss: 0.001630175061006513\n",
      "Average test loss: 26.736574928384687\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0016122646246933274\n",
      "Average test loss: 0.037199549007746906\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0016703065739323696\n",
      "Average test loss: 2.198668166894052\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0016495662495079968\n",
      "Average test loss: 143.39155862282797\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00161037703210281\n",
      "Average test loss: 5.340657296054583\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0016054645406289234\n",
      "Average test loss: 0.7246244443282485\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0016213590564827123\n",
      "Average test loss: 0.17272906473237606\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0016029917676415708\n",
      "Average test loss: 1476.6182611085405\n",
      "Epoch 287/300\n",
      "Average training loss: 0.001624368741383983\n",
      "Average test loss: 0.08384452073358827\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0015942449706296126\n",
      "Average test loss: 0.006747487439773977\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0017327442380289236\n",
      "Average test loss: 0.6525049603707674\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0017045607298819556\n",
      "Average test loss: 0.002808903102349076\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0016814314610221319\n",
      "Average test loss: 0.008420187619618243\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0016404884536233213\n",
      "Average test loss: 0.09355092700405253\n",
      "Epoch 293/300\n",
      "Average training loss: 0.001619237227158414\n",
      "Average test loss: 1.0817491067333354\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0016532627422776488\n",
      "Average test loss: 0.003315189867073463\n",
      "Epoch 295/300\n",
      "Average training loss: 0.001627572258727418\n",
      "Average test loss: 7.024056757953431\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0015984754587213199\n",
      "Average test loss: 27.43810045649194\n",
      "Epoch 297/300\n",
      "Average training loss: 0.001598341331506769\n",
      "Average test loss: 17.045362298429012\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00165291113127023\n",
      "Average test loss: 3.0771806689194507\n",
      "Epoch 299/300\n",
      "Average training loss: 0.001609167761893736\n",
      "Average test loss: 11.449993733839856\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0016222469387575985\n",
      "Average test loss: 168.56544396662568\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.09134561406655443\n",
      "Average test loss: 0.014647730830228991\n",
      "Epoch 2/300\n",
      "Average training loss: 0.00867199577142795\n",
      "Average test loss: 0.6195324044062032\n",
      "Epoch 3/300\n",
      "Average training loss: 0.006787409150352081\n",
      "Average test loss: 0.00405250357877877\n",
      "Epoch 4/300\n",
      "Average training loss: 0.005925191094891893\n",
      "Average test loss: 0.0037480900945762795\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005373645570129156\n",
      "Average test loss: 0.0035054610992471375\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004956937318460809\n",
      "Average test loss: 0.0033220325784964694\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004632096300522487\n",
      "Average test loss: 0.0247296019875341\n",
      "Epoch 8/300\n",
      "Average test loss: 0.021247992869052622\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0036920994230442575\n",
      "Average test loss: 0.0025438049050668875\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0035236688318351903\n",
      "Average test loss: 0.008519673093946443\n",
      "Epoch 13/300\n",
      "Average training loss: 0.003370252528745267\n",
      "Average test loss: 0.003327893807242314\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0032476780715708932\n",
      "Average test loss: 0.002402107127631704\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0031196154927213985\n",
      "Average test loss: 0.008265256769541237\n",
      "Epoch 16/300\n",
      "Average training loss: 0.003016657833216919\n",
      "Average test loss: 0.07846930422302749\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0029110835674736233\n",
      "Average test loss: 0.0020779539984133507\n",
      "Epoch 18/300\n",
      "Average training loss: 0.002596290530016025\n",
      "Average test loss: 0.1492173937668817\n",
      "Epoch 22/300\n",
      "Average training loss: 0.002549473726294107\n",
      "Average test loss: 0.05012780841460658\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0024640645916677185\n",
      "Average test loss: 0.11292446002167546\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002432004216023617\n",
      "Average test loss: 0.017758695678578483\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002349565497599542\n",
      "Average test loss: 0.001728826231116222\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002298291705341803\n",
      "Average test loss: 0.015499739640288883\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0022495806890850266\n",
      "Average test loss: 0.23866348367639714\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0022407766421222025\n",
      "Average test loss: 0.08931909711658954\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0022353283600467775\n",
      "Average test loss: 11.845384234404813\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002118579561304715\n",
      "Average test loss: 0.35120158825214537\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0020775110681230825\n",
      "Average test loss: 0.0029470584992733265\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002010869919322431\n",
      "Average test loss: 1.2622753753467566\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0021162261654519374\n",
      "Average test loss: 0.001569918010280364\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0020044147042143677\n",
      "Average test loss: 0.300920829217881\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0019286810678119462\n",
      "Average test loss: 0.008743975129392413\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0018914549294859171\n",
      "Average test loss: 0.16219972329752314\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0020840952709938088\n",
      "Average test loss: 0.002200541030615568\n",
      "Epoch 38/300\n",
      "Average training loss: 0.002063159766710467\n",
      "Average test loss: 0.06516278648583425\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0018956333676146137\n",
      "Average test loss: 0.010606873442315394\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0018688772534951568\n",
      "Average test loss: 7.275549672484398\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0018454101647043394\n",
      "Average test loss: 0.0018851281601107782\n",
      "Epoch 42/300\n",
      "Average training loss: 0.001817968787314991\n",
      "Average test loss: 2.7131756815674404\n",
      "Epoch 43/300\n",
      "Average training loss: 0.001801312315174275\n",
      "Average test loss: 0.013708650607615709\n",
      "Epoch 44/300\n",
      "Average training loss: 0.001798516239453521\n",
      "Average test loss: 5.951025190664248\n",
      "Epoch 47/300\n",
      "Average training loss: 0.001775126071439849\n",
      "Average test loss: 63.84563662676348\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0017164755529827543\n",
      "Average test loss: 0.0014936104239378538\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0017005292211866214\n",
      "Average test loss: 0.017372593482076707\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0016692407040132416\n",
      "Average test loss: 2.735943896510535\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0017360724558432897\n",
      "Average test loss: 0.0030784060353827147\n",
      "Epoch 52/300\n",
      "Average training loss: 0.001698308061911828\n",
      "Average test loss: 0.06991763438822494\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0016480576926842332\n",
      "Average test loss: 0.43865456359336774\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0016412200767339932\n",
      "Average test loss: 5.484032932457411\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0016267234625087844\n",
      "Average test loss: 34.01708432511613\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0016084584281262425\n",
      "Average test loss: 0.34139870188385246\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0016817221217271354\n",
      "Average test loss: 0.011960129181543986\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0015908656374861796\n",
      "Average test loss: 0.05742303010303941\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0015964674363430176\n",
      "Average test loss: 0.0014146043906609217\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0015856830384582282\n",
      "Average test loss: 21.076649662993436\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0015586056971094674\n",
      "Average test loss: 89.73359171982689\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0016343573986863096\n",
      "Average test loss: 0.0013495467356923554\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0015245996063782108\n",
      "Average test loss: 0.675210621111095\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0015155703865198625\n",
      "Average test loss: 0.014283111214534277\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0015190996611490846\n",
      "Average test loss: 219.46815850716416\n",
      "Epoch 66/300\n",
      "Average training loss: 0.001571636623599463\n",
      "Average test loss: 0.10360247728880495\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0015095032864353723\n",
      "Average test loss: 0.012302331463537283\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0014795448301980893\n",
      "Average test loss: 4.409202769928508\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0014813907659716077\n",
      "Average training loss: 0.0014434110204585725\n",
      "Average test loss: 0.006232269175350666\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0014341502843631638\n",
      "Average test loss: 0.001478798836780091\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0015354741531320745\n",
      "Average test loss: 37.17212689015352\n",
      "Epoch 75/300\n",
      "Average training loss: 0.001455174013454881\n",
      "Average test loss: 1.4590393185963233\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0014346481561660766\n",
      "Average test loss: 236.35023806084527\n",
      "Epoch 77/300\n",
      "Average training loss: 0.001414712181314826\n",
      "Average test loss: 0.08356027746221258\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0014049511312817535\n",
      "Average test loss: 0.3909168338887393\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0019866357443647253\n",
      "Average test loss: 0.0026594780599698423\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0014758922687421242\n",
      "Average test loss: 60.39810559074581\n",
      "Epoch 81/300\n",
      "Average training loss: 0.001412212506764465\n",
      "Average test loss: 9.963402336255959\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0013965690519867672\n",
      "Average test loss: 1.7310821666469176\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0013885835549897617\n",
      "Average test loss: 0.034998219889795616\n",
      "Epoch 84/300\n",
      "Average training loss: 0.001379069664205114\n",
      "Average test loss: 0.1366871453696448\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0013818744814230335\n",
      "Average test loss: 1.0095139885563404\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0013697760989889503\n",
      "Average test loss: 0.04994338079128\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0013827683604322374\n",
      "Average test loss: 0.006089386134977556\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0014177246848121286\n",
      "Average test loss: 0.0021491931724465555\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0014085759439816077\n",
      "Average test loss: 0.04140645117022925\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0013657708687500822\n",
      "Average test loss: 0.011445579810792373\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0014598846577314867\n",
      "Average test loss: 0.17425675855628733\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0013661389773090681\n",
      "Average test loss: 0.5475641324656705\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0013403672029574712\n",
      "Average test loss: 0.12612034319920673\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0013320658230740164\n",
      "Average test loss: 0.38883535418328313\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0013897532482321064\n",
      "Average test loss: 0.0014385772024591763\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0013515230126471983\n",
      "Average test loss: 143.35163800896538\n",
      "Epoch 99/300\n",
      "Average training loss: 0.001324435684002108\n",
      "Average test loss: 1.0395615992012124\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0013181329184832672\n",
      "Average test loss: 1.0058628479277507\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0013047766905898849\n",
      "Average test loss: 51.48465375581674\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0013085190787290534\n",
      "Average test loss: 0.5186566779601077\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0018767214008710452\n",
      "Average test loss: 0.009742686831599309\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0014283260635824667\n",
      "Average test loss: 0.05562905766732163\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0013370668127511939\n",
      "Average test loss: 3.7248067254357866\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0013022738449896376\n",
      "Average test loss: 0.5049823600935439\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0012983573569605748\n",
      "Average test loss: 0.08765476297959686\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0012995048401256402\n",
      "Average test loss: 0.030480556251688135\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0013268878342997696\n",
      "Average test loss: 4.3318474727583425\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0013062731102108955\n",
      "Average test loss: 1.1664666183723345\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0012714588380315238\n",
      "Average test loss: 63.6352086045027\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0012696177488606838\n",
      "Average test loss: 16.81810511194335\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0012789978762674662\n",
      "Average test loss: 292.8070199686686\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0012690295919569002\n",
      "Average test loss: 6262.02167640296\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0012730924977610509\n",
      "Average test loss: 0.0039899903531703685\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0012661959350936942\n",
      "Average test loss: 6.894372580255175\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0012629467853241497\n",
      "Average test loss: 0.003059644812407593\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0012325413888320327\n",
      "Average test loss: 1086.354058135447\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0012511738025479846\n",
      "Average test loss: 35.24449097964809\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0012378628254454168\n",
      "Average test loss: 92.53793517956137\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0012482103920645185\n",
      "Average test loss: 0.05780648502397041\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0012495777547462947\n",
      "Average test loss: 0.03928395199113422\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0012443513242113922\n",
      "Average test loss: 0.015692032425560887\n",
      "Epoch 129/300\n",
      "Average training loss: 0.001223353992216289\n",
      "Average test loss: 2.5474071037001496\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0012149770801059075\n",
      "Average test loss: 2.633503129109637\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0012091462279980381\n",
      "Average test loss: 26.5501613012054\n",
      "Epoch 132/300\n",
      "Average training loss: 0.001218332252258228\n",
      "Average test loss: 0.21648350288429194\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0012244919190804165\n",
      "Average test loss: 0.0021982387678387266\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0012030924991187123\n",
      "Average test loss: 0.009688681929475732\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0012007545760522285\n",
      "Average test loss: 1.0008812180538145\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0012220105864107608\n",
      "Average test loss: 0.0026234845897803705\n",
      "Epoch 137/300\n",
      "Average training loss: 0.001336913939151499\n",
      "Average test loss: 0.0018257159986015824\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0012772960752869646\n",
      "Average test loss: 0.1923138397153881\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0012228057752880785\n",
      "Average test loss: 676.6241324259672\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0014591403223781123\n",
      "Average test loss: 0.03541585093860825\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0012980032609775662\n",
      "Average test loss: 10.931497958135274\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0012463740817167692\n",
      "Average test loss: 9.800721925152672\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0012186567367364963\n",
      "Average test loss: 0.06429320833583672\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001216987499863737\n",
      "Average test loss: 0.002522937617161208\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0012133650551032688\n",
      "Average test loss: 0.025757515020668507\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0011864922290874852\n",
      "Average test loss: 10.687965218608579\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0011950360955670476\n",
      "Average test loss: 133.2282332454208\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0011825632823424207\n",
      "Average test loss: 0.29744355772254577\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001264059009340902\n",
      "Average test loss: 82.79902182597584\n",
      "Epoch 152/300\n",
      "Average training loss: 0.001177719749096367\n",
      "Average test loss: 230.99956721052342\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0011725590092440447\n",
      "Average test loss: 1253.502868232963\n",
      "Epoch 154/300\n",
      "Average training loss: 0.001162090040743351\n",
      "Average test loss: 1.8694467970815798\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0011752152661275532\n",
      "Average test loss: 25735242.733333334\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0011836497260050642\n",
      "Average test loss: 0.001418658213276002\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0011628133832580513\n",
      "Average test loss: 0.0059687070795852275\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0011771453736970823\n",
      "Average test loss: 0.15485499102622272\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0011506425873376428\n",
      "Average test loss: 9.051488303807874\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0011520437019773655\n",
      "Average test loss: 0.8049707361780521\n",
      "Epoch 161/300\n",
      "Average training loss: 0.001145759913449486\n",
      "Average test loss: 0.4949535620717539\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0013127907653235726\n",
      "Average test loss: 0.0014031374234085281\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0012216848599620992\n",
      "Average test loss: 141.91852481944176\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0012110178238815732\n",
      "Average test loss: 127.823239498078\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0011947295574678315\n",
      "Average test loss: 5.9938489230386915\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0011808143466090163\n",
      "Average test loss: 0.09032207120675594\n",
      "Epoch 167/300\n",
      "Average test loss: 1.0688977457711266\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0011774011943489313\n",
      "Average test loss: 21.074965966594714\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0011977589399450356\n",
      "Average test loss: 0.0024979704786092043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0011970659984379178\n",
      "Average test loss: 0.1647948746283849\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001183833121942977\n",
      "Average test loss: 0.7530555757193101\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0011785592878651287\n",
      "Average test loss: 0.5059046983958946\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0011710460645457108\n",
      "Average test loss: 22.166600912648356\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0011655506445094944\n",
      "Average test loss: 0.6314506191180812\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0011526716536738807\n",
      "Average test loss: 3.1750689582690184\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0011389858049030105\n",
      "Average test loss: 145.52626343795038\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0012626581583172084\n",
      "Average test loss: 0.24684123544229403\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0011678505883448655\n",
      "Average test loss: 5.745184676039757\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0011594771495502857\n",
      "Average test loss: 17.354666727109088\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0011439742177931798\n",
      "Average test loss: 4.8027983701147345\n",
      "Epoch 185/300\n",
      "Average training loss: 0.001156577560222811\n",
      "Average test loss: 249.11086040475632\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0011525020660936006\n",
      "Average test loss: 1.601373826727685\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0011182108400389552\n",
      "Average test loss: 0.06683768319421345\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0012087907851673662\n",
      "Average test loss: 1.5401606921172803\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0011745803547609183\n",
      "Average test loss: 0.15745658431419482\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0011620957704467905\n",
      "Average test loss: 20.32744964017636\n",
      "Epoch 191/300\n",
      "Average training loss: 0.001157692850774361\n",
      "Average test loss: 27.222036405976034\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0011411186189183757\n",
      "Average test loss: 1.8609055073190894\n",
      "Epoch 193/300\n",
      "Average training loss: 0.001135732884829243\n",
      "Average test loss: 1.9350430550146849\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008375657263108426\n",
      "Average test loss: 0.002153278093681567\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0028853741702106264\n",
      "Average test loss: 0.0020719803555144206\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0024929396141734387\n",
      "Average test loss: 0.0017616045231827432\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0023202096157603795\n",
      "Average test loss: 0.0016139721657252974\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002200150570935673\n",
      "Average test loss: 0.001592734604401307\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0021041811451108917\n",
      "Average test loss: 0.004621308436410294\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002025348296906385\n",
      "Average test loss: 0.001620952336014145\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0019433152550417515\n",
      "Average test loss: 0.0049090443758500945\n",
      "Epoch 202/300\n",
      "Average training loss: 0.001873153106826875\n",
      "Average test loss: 0.0015354097586952977\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0018117443799144691\n",
      "Average test loss: 0.0014149215848495563\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0017551325330924657\n",
      "Average test loss: 0.0013868391319281526\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0017034335746947261\n",
      "Average test loss: 0.0020894231023266914\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0016463130626620517\n",
      "Average test loss: 0.0013379888496258192\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0016116822072201305\n",
      "Average test loss: 0.0014305673524116476\n",
      "Epoch 208/300\n",
      "Average training loss: 0.001518461539513535\n",
      "Average test loss: 0.0017167835653656058\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0013968907619516056\n",
      "Average test loss: 0.02187141060673942\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001322185191946725\n",
      "Average test loss: 0.0911265347238837\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001311991716424624\n",
      "Average test loss: 0.010539026226330963\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0012755978886658946\n",
      "Average test loss: 2.9894218303958575\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0012242558263759647\n",
      "Average test loss: 6.623168988688125\n",
      "Epoch 222/300\n",
      "Average training loss: 0.001237586012110114\n",
      "Average test loss: 0.07897389846471035\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0012181049836799502\n",
      "Average test loss: 30.885315393198283\n",
      "Epoch 224/300\n",
      "Average training loss: 0.001215171245056101\n",
      "Average test loss: 0.6777830294227849\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0011993395912771423\n",
      "Average test loss: 32.14157781783698\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0011985103770469625\n",
      "Average test loss: 12.513715433752784\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0013122472460899088\n",
      "Average test loss: 669.0938626300551\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001218330132878489\n",
      "Average test loss: 55.39178183833096\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0011712580245609085\n",
      "Average test loss: 1.0891936683842085\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0011867877361882064\n",
      "Average test loss: 180.26626301433146\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0011736455106486877\n",
      "Average test loss: 2.3113654612939185\n",
      "Epoch 232/300\n",
      "Average training loss: 0.001172173528569854\n",
      "Average test loss: 68.70581493614128\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0011652006241492927\n",
      "Average test loss: 321.2468737459547\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0011649542316380474\n",
      "Average test loss: 4.491317551142226\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0011526206385137306\n",
      "Average test loss: 0.01022316473401669\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0012294680792838335\n",
      "Average test loss: 0.3061518962863419\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001134492993975679\n",
      "Average test loss: 0.30238258460826345\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0011667290293715066\n",
      "Average test loss: 0.13707446498299639\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0011424719438784651\n",
      "Average test loss: 0.017531569660020372\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0011554561719919244\n",
      "Average test loss: 122.30733318971097\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0011363712621645795\n",
      "Average test loss: 15.321962681071936\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0011397788604307507\n",
      "Average test loss: 0.3342974153034803\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0011343828312431773\n",
      "Average test loss: 13.234552321708865\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0011370224297667542\n",
      "Average test loss: 10.72870177948392\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0012415540004666482\n",
      "Average test loss: 0.42287571746276487\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0011339066034803788\n",
      "Average test loss: 179.96387852255089\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0011264860100216336\n",
      "Average test loss: 53.194906744215224\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0011271242948455943\n",
      "Average test loss: 0.06897704229741874\n",
      "Epoch 251/300\n",
      "Average training loss: 0.001116264850832522\n",
      "Average test loss: 6.505144235688365\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0011287747946464354\n",
      "Average test loss: 9.080925060934076\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0011199882089470824\n",
      "Average test loss: 0.09261580110796623\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0011204002329872713\n",
      "Average test loss: 29.73303991383107\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0011241971000822055\n",
      "Average test loss: 10.812511706430051\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0011120405200765364\n",
      "Average test loss: 11.150603600829426\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0011244539094364477\n",
      "Average test loss: 0.008439335125601953\n",
      "Epoch 258/300\n",
      "Average training loss: 0.001122318948722548\n",
      "Average test loss: 0.0019875790439028707\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0010863655986678268\n",
      "Average test loss: 42.980414162048866\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0011063169384789135\n",
      "Average test loss: 0.29369325228325194\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0010869710058905185\n",
      "Average test loss: 6.371740132032169\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0011806010945389668\n",
      "Average test loss: 25.58408357493331\n",
      "Epoch 263/300\n",
      "Average training loss: 0.001099292529332969\n",
      "Average test loss: 0.001945412224986487\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0011115895198244188\n",
      "Average test loss: 2.846021790427466\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0010950243688291974\n",
      "Average test loss: 0.49056913008080383\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0010870345018597113\n",
      "Average test loss: 176.6139536528728\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0010927767212916579\n",
      "Average test loss: 780.7859567295106\n",
      "Epoch 270/300\n",
      "Average training loss: 0.001083876004235612\n",
      "Average test loss: 1.24681039301068\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0010944507928151224\n",
      "Average test loss: 0.3860975049233271\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0011058585742074582\n",
      "Average test loss: 7.996172987640318\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0011047478544836244\n",
      "Average test loss: 85.76589773799975\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0010793027802267008\n",
      "Average test loss: 700.3607692246073\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0011004168906559547\n",
      "Average test loss: 18.300869524733475\n",
      "Epoch 276/300\n",
      "Average training loss: 0.001076971190671126\n",
      "Average test loss: 1737.8638927159898\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0010943432887498704\n",
      "Average test loss: 0.03135416805288858\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0010853297451717985\n",
      "Average test loss: 1.4545914785329046\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0010754440809703535\n",
      "Average test loss: 7335.1738986018345\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0010753708109777008\n",
      "Average test loss: 0.8249280029580824\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0010849313981210191\n",
      "Average test loss: 2.154601541299166\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0010718950665452413\n",
      "Average test loss: 0.5035324442986813\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0011001541305643817\n",
      "Average test loss: 1863.7009838047404\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0010690333638857636\n",
      "Average test loss: 0.017464369525512058\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0010624283266046808\n",
      "Average test loss: 1591.0391884544285\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0010793018102542393\n",
      "Average test loss: 0.025746316619424357\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0010574710141453478\n",
      "Average test loss: 139.5145195511116\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0010818711143607894\n",
      "Average test loss: 0.001452113919891417\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0010687194944669803\n",
      "Average test loss: 0.004243734352808032\n",
      "Epoch 290/300\n",
      "Average training loss: 0.001133884332763652\n",
      "Average test loss: 3882.9323856104975\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0010685184591760239\n",
      "Average test loss: 399.9244015053399\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0010623862108008729\n",
      "Average test loss: 4.1451627675021685\n",
      "Epoch 295/300\n",
      "Average training loss: 0.001050053341158976\n",
      "Average test loss: 16105.44203843322\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0010565072921518651\n",
      "Average test loss: 28.941373957453504\n",
      "Epoch 297/300\n",
      "Average training loss: 0.001057069284324017\n",
      "Average test loss: 3920.7143959960936\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0010719021165536509\n",
      "Average test loss: 106.85162569565574\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0010466355638992456\n",
      "Average test loss: 6.052168097502863\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0010574495244978202\n",
      "Average test loss: 55.65585426520805\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.25/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 9.69\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 14.31\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 17.99\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 12.86\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 19.62\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 20.76\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 11.37\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 21.54\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 22.97\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 17.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 24.15\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 23.66\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.70\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.42\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 12.67\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 14.20\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 19.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 21.50\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.26\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 17.13\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.55\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.13\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 18.28\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.70\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.25\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 8.69\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 14.64\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 18.41\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 22.43\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.99\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 15.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 20.89\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 23.90\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.90\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.62\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 4.24\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 11.08\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 15.78\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 20.54\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 20.85\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.21\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 19.49\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 22.90\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.11\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.60\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.35\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.30490926141209074\n",
      "Average test loss: 0.01574680593940947\n",
      "Epoch 2/300\n",
      "Average training loss: 0.023643638971779082\n",
      "Average test loss: 0.0601314547508955\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01837852949400743\n",
      "Average test loss: 0.010766306705772877\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01584512079589897\n",
      "Average test loss: 0.02519891009728114\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01464501158396403\n",
      "Average test loss: 0.009821665460036861\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012840660291413467\n",
      "Average test loss: 0.06966327463255988\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01214609988944398\n",
      "Average test loss: 0.0570890254245864\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011609838649630546\n",
      "Average test loss: 0.008955068062163061\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011033663280308246\n",
      "Average test loss: 0.007624070418377717\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010653545477324063\n",
      "Average test loss: 0.007634703187892835\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010440482388767932\n",
      "Average test loss: 0.00933114499102036\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01029422494272391\n",
      "Average test loss: 0.007785917248990801\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009953860943516095\n",
      "Average test loss: 0.006951013243032826\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009593368615541192\n",
      "Average test loss: 0.00726930070337322\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009405116798149215\n",
      "Average test loss: 0.0073171839540203416\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009095192601283391\n",
      "Average test loss: 0.006993716731667519\n",
      "Epoch 18/300\n",
      "Average training loss: 0.008870211760203044\n",
      "Average test loss: 0.00636565218327774\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008697777427732945\n",
      "Average test loss: 0.006515658320652114\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00877738704201248\n",
      "Average test loss: 0.0062565877396199435\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017299103673961428\n",
      "Average test loss: 0.007054843405882518\n",
      "Epoch 23/300\n",
      "Average training loss: 0.009305716371370686\n",
      "Average test loss: 0.008860335596733623\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00837001253830062\n",
      "Average test loss: 0.006184175632480118\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008233454130589962\n",
      "Average test loss: 0.006058846307711469\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00809368669655588\n",
      "Average test loss: 0.005952189387546645\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007989471431821585\n",
      "Average test loss: 0.00727255711497532\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00789806005400088\n",
      "Average test loss: 0.0062027894212967815\n",
      "Epoch 30/300\n",
      "Average training loss: 0.007818256383140881\n",
      "Average test loss: 0.005802033928119474\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0077378774359822275\n",
      "Average test loss: 0.005819482477174865\n",
      "Epoch 32/300\n",
      "Average training loss: 0.00769699981311957\n",
      "Average test loss: 1.5734355597297351\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007596252019206683\n",
      "Average test loss: 0.006027342341840267\n",
      "Epoch 34/300\n",
      "Average training loss: 0.007596468788468176\n",
      "Average test loss: 3.464167275004917\n",
      "Epoch 35/300\n",
      "Average training loss: 0.007694339647475216\n",
      "Average test loss: 0.0056369663824637735\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007526822979665465\n",
      "Average test loss: 0.005714893150246806\n",
      "Epoch 37/300\n",
      "Average training loss: 0.007340165636605687\n",
      "Average test loss: 0.007000489549504386\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0072628176237146055\n",
      "Average test loss: 0.14931049224320386\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0072554655008845855\n",
      "Average test loss: 0.008128867550442617\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009939881475849285\n",
      "Average test loss: 0.008944002769059605\n",
      "Epoch 42/300\n",
      "Average training loss: 0.007701752250807153\n",
      "Average test loss: 0.014856672635508909\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007407952505267329\n",
      "Average test loss: 0.0067619398683309555\n",
      "Epoch 44/300\n",
      "Average training loss: 0.007338498348163234\n",
      "Average test loss: 0.08852827972173691\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007566156428307295\n",
      "Average test loss: 0.008320578421983454\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007201167053232591\n",
      "Average test loss: 0.005575207060409917\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007145795511288775\n",
      "Average test loss: 0.005414529656370481\n",
      "Epoch 48/300\n",
      "Average training loss: 0.007110034925242265\n",
      "Average test loss: 0.023138381325536306\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007043963249772787\n",
      "Average test loss: 0.005694995243723194\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007081741039537721\n",
      "Average test loss: 0.007583848666399717\n",
      "Epoch 51/300\n",
      "Average training loss: 0.006916026580250926\n",
      "Average test loss: 0.009991870455029937\n",
      "Epoch 54/300\n",
      "Average training loss: 0.006876980969475375\n",
      "Average test loss: 0.27587319261994625\n",
      "Epoch 55/300\n",
      "Average training loss: 0.006856194371564521\n",
      "Average test loss: 0.396625125537316\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006864617072045803\n",
      "Average test loss: 0.008393082972202036\n",
      "Epoch 57/300\n",
      "Average training loss: 0.006784394654962752\n",
      "Average test loss: 0.005266977458364434\n",
      "Epoch 58/300\n",
      "Average training loss: 0.006959220993849966\n",
      "Average test loss: 4.960515635042968\n",
      "Epoch 59/300\n",
      "Average training loss: 0.006737909070733521\n",
      "Average test loss: 0.006262752498603529\n",
      "Epoch 60/300\n",
      "Average training loss: 0.006676115529818667\n",
      "Average test loss: 0.005317507583647967\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00664719032868743\n",
      "Average test loss: 0.007182044848385785\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006629151751183802\n",
      "Average test loss: 0.061575857950581445\n",
      "Epoch 63/300\n",
      "Average training loss: 0.006605724555750688\n",
      "Average test loss: 0.0070596461842457455\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006574520908296108\n",
      "Average test loss: 0.4215404027005037\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012864100019550986\n",
      "Average test loss: 3.7698460818396673\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007727310938967599\n",
      "Average test loss: 0.006287870061894258\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007269956453806824\n",
      "Average test loss: 0.0382018190158738\n",
      "Epoch 70/300\n",
      "Average training loss: 0.00707301109801564\n",
      "Average test loss: 0.028480801199045447\n",
      "Epoch 71/300\n",
      "Average training loss: 0.006983499931792418\n",
      "Average test loss: 0.005656283900555637\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00684711138283213\n",
      "Average test loss: 1.0071574885514047\n",
      "Epoch 73/300\n",
      "Average training loss: 0.006727133537746138\n",
      "Average test loss: 0.01838020595245891\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0066084805495209165\n",
      "Average test loss: 4.3994431581728985\n",
      "Epoch 75/300\n",
      "Average training loss: 0.006544480061779419\n",
      "Average test loss: 0.04640339303761721\n",
      "Epoch 76/300\n",
      "Average training loss: 0.006549777893142567\n",
      "Average test loss: 0.1460749356680446\n",
      "Epoch 77/300\n",
      "Average training loss: 0.006485842799560891\n",
      "Average test loss: 0.0654073814459973\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006468270596530702\n",
      "Average test loss: 133.10099606366953\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0064067705124616625\n",
      "Average test loss: 0.2816195757703649\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006420088238600227\n",
      "Average test loss: 0.005353134633352359\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006399361121571726\n",
      "Average test loss: 0.08548603302737078\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006363737207320001\n",
      "Average test loss: 0.01053327695073353\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0063359349643190704\n",
      "Average test loss: 0.009142542907347282\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006298696702967088\n",
      "Average test loss: 0.1387995447412961\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006263129082404905\n",
      "Average test loss: 3.371260768796007\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006243676564345757\n",
      "Average test loss: 0.1916435905372103\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00624425119202998\n",
      "Average test loss: 0.005180587365809414\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006249156369931168\n",
      "Average test loss: 0.009308886278006766\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006180858473396963\n",
      "Average test loss: 0.005181788893623485\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006146039384520716\n",
      "Average test loss: 0.015374284099580512\n",
      "Epoch 93/300\n",
      "Average training loss: 0.006135506470584207\n",
      "Average test loss: 3.5959608024822343\n",
      "Epoch 94/300\n",
      "Average training loss: 0.00612815465281407\n",
      "Average test loss: 1.2642557062920596\n",
      "Epoch 95/300\n",
      "Average training loss: 0.006102782977537976\n",
      "Average test loss: 4.18115859072821\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006432807015462054\n",
      "Average test loss: 0.008961684280385573\n",
      "Epoch 97/300\n",
      "Average training loss: 0.006090318827579419\n",
      "Average test loss: 18.757130631748172\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006026056617498398\n",
      "Average test loss: 47.8564610445963\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005989714735911952\n",
      "Average test loss: 30.145105629031857\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005970050668550862\n",
      "Average test loss: 1.4196116089373827\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005965939910461506\n",
      "Average test loss: 1213.177247762489\n",
      "Epoch 102/300\n",
      "Average test loss: 3.7552933957245616\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005914524346176121\n",
      "Average test loss: 0.07335552250014411\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005903776766111453\n",
      "Average test loss: 13.627188498004857\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005921338265140851\n",
      "Average test loss: 0.009391205577386751\n",
      "Epoch 107/300\n",
      "Average training loss: 0.005878331974976592\n",
      "Average test loss: 0.6698413404110405\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005850013304087851\n",
      "Average test loss: 0.1315167983836598\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005829252995964554\n",
      "Average test loss: 0.7938300615515974\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005816784704311026\n",
      "Average test loss: 1344.2135349476735\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005785814933478833\n",
      "Average test loss: 0.0295380736672216\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0076193378840883575\n",
      "Average test loss: 0.15026602475179565\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005958358921938472\n",
      "Average test loss: 10.85798945543501\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005784204300493002\n",
      "Average test loss: 0.0061863099237283075\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005727327612539132\n",
      "Average test loss: 0.3929173064761692\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0057162016063100765\n",
      "Average test loss: 0.37958747512102126\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005705875857836671\n",
      "Average test loss: 25.42181146391564\n",
      "Epoch 118/300\n",
      "Average training loss: 0.005699376496175925\n",
      "Average test loss: 0.011109092003769345\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005707253893630372\n",
      "Average test loss: 0.28051051894492574\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005706684321992927\n",
      "Average test loss: 0.06511244953009818\n",
      "Epoch 121/300\n",
      "Average training loss: 0.005672898130284415\n",
      "Average test loss: 0.005252995184726186\n",
      "Epoch 122/300\n",
      "Average training loss: 0.005643473374760813\n",
      "Average test loss: 0.07507423406342666\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006608509743793143\n",
      "Average test loss: 0.005287157646483845\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0058806696463790204\n",
      "Average test loss: 17855.978747612848\n",
      "Epoch 125/300\n",
      "Average training loss: 0.005713742923819357\n",
      "Average test loss: 0.03500742215083705\n",
      "Epoch 126/300\n",
      "Average training loss: 0.005652142271399498\n",
      "Average test loss: 0.005576688519368569\n",
      "Epoch 127/300\n",
      "Average training loss: 0.005625974524766207\n",
      "Average training loss: 0.0058410422056913375\n",
      "Average test loss: 0.005271560488475694\n",
      "Epoch 129/300\n",
      "Average training loss: 0.005648634277698066\n",
      "Average test loss: 0.007092257423533334\n",
      "Epoch 130/300\n",
      "Average training loss: 0.005595265610764424\n",
      "Average test loss: 0.005385286632925272\n",
      "Epoch 131/300\n",
      "Average training loss: 0.005582749534398317\n",
      "Average test loss: 1.8166668463084432\n",
      "Epoch 132/300\n",
      "Average training loss: 0.005563634459343221\n",
      "Average test loss: 1.9532247879157463\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0055523042476011645\n",
      "Average test loss: 0.006112018402251933\n",
      "Epoch 134/300\n",
      "Average training loss: 0.005589357034199768\n",
      "Average test loss: 0.4755564694768853\n",
      "Epoch 135/300\n",
      "Average training loss: 0.005541820129172669\n",
      "Average test loss: 28.655959163238606\n",
      "Epoch 136/300\n",
      "Average training loss: 0.005560771936343776\n",
      "Average test loss: 0.02327025443253418\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0055254243194229075\n",
      "Average test loss: 0.011340725604030821\n",
      "Epoch 138/300\n",
      "Average training loss: 0.005498574128167497\n",
      "Average test loss: 0.5980868529623581\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00575374432777365\n",
      "Average test loss: 0.005878118454582162\n",
      "Epoch 140/300\n",
      "Average training loss: 0.005551749992287821\n",
      "Average test loss: 0.0057340615449680225\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00549047130636043\n",
      "Average test loss: 0.006347751409643226\n",
      "Epoch 142/300\n",
      "Average training loss: 0.005463788341316912\n",
      "Average test loss: 0.011939807915025287\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00546196567101611\n",
      "Average test loss: 3.3674620615177684\n",
      "Epoch 144/300\n",
      "Average training loss: 0.005514980918003453\n",
      "Average test loss: 203.74901477989886\n",
      "Epoch 145/300\n",
      "Average training loss: 0.005463126098944081\n",
      "Average test loss: 109.92561663169663\n",
      "Epoch 146/300\n",
      "Average training loss: 0.005425540405636032\n",
      "Average test loss: 0.009571735572690764\n",
      "Epoch 147/300\n",
      "Average training loss: 0.005414716166340643\n",
      "Average test loss: 0.32729260626083445\n",
      "Epoch 148/300\n",
      "Average training loss: 0.005467207789835003\n",
      "Average test loss: 0.005332143740521537\n",
      "Epoch 149/300\n",
      "Average training loss: 0.005393316927055518\n",
      "Average test loss: 0.012830389050559865\n",
      "Epoch 150/300\n",
      "Average training loss: 0.005391322338746654\n",
      "Average test loss: 23.68653032055166\n",
      "Epoch 151/300\n",
      "Average training loss: 0.005401942775895198\n",
      "Average test loss: 5.266932449233201\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0053759409392045605\n",
      "Average test loss: 407.62142233645255\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0053822797000822094\n",
      "Average test loss: 0.3182364923655987\n",
      "Epoch 154/300\n",
      "Average training loss: 0.005355125536521276\n",
      "Average test loss: 0.05014751952141523\n",
      "Epoch 155/300\n",
      "Average training loss: 0.005355501068549024\n",
      "Average test loss: 0.0379345125787788\n",
      "Epoch 156/300\n",
      "Average training loss: 0.005327647428545687\n",
      "Average test loss: 2.9704073523129852\n",
      "Epoch 157/300\n",
      "Average training loss: 0.005351204785621829\n",
      "Average test loss: 8.237902958869935\n",
      "Epoch 158/300\n",
      "Average training loss: 0.005328891210258007\n",
      "Average test loss: 0.03591886718612578\n",
      "Epoch 159/300\n",
      "Average training loss: 0.005300864017258088\n",
      "Average test loss: 0.06901565725273556\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00528395710264643\n",
      "Average test loss: 0.005978377968072891\n",
      "Epoch 161/300\n",
      "Average training loss: 0.005276220909837219\n",
      "Average test loss: 0.04148626795825031\n",
      "Epoch 162/300\n",
      "Average training loss: 0.005247542736844884\n",
      "Average test loss: 0.005882721923291683\n",
      "Epoch 163/300\n",
      "Average training loss: 0.005258194219321013\n",
      "Average test loss: 4.455871453824971\n",
      "Epoch 164/300\n",
      "Average training loss: 0.005253665255175697\n",
      "Average test loss: 0.547783983030253\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00523899574081103\n",
      "Average test loss: 18.151265670465097\n",
      "Epoch 166/300\n",
      "Average training loss: 0.005269809970011314\n",
      "Average test loss: 8.408876078075833\n",
      "Epoch 167/300\n",
      "Average training loss: 0.005217881223393811\n",
      "Average test loss: 28.670730312044423\n",
      "Epoch 168/300\n",
      "Average training loss: 0.005212346349325445\n",
      "Average test loss: 0.02176589661132958\n",
      "Epoch 169/300\n",
      "Average training loss: 0.005188127768122487\n",
      "Average test loss: 496.52574996782675\n",
      "Epoch 170/300\n",
      "Average training loss: 0.005199081208142969\n",
      "Average test loss: 0.18972750502410862\n",
      "Epoch 171/300\n",
      "Average training loss: 0.005184536346544822\n",
      "Average test loss: 0.7237104032999939\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00524856697064307\n",
      "Average test loss: 146.5851107720683\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0051892401547067694\n",
      "Average test loss: 0.6676210898438262\n",
      "Epoch 174/300\n",
      "Average training loss: 0.005149627299772369\n",
      "Average test loss: 0.2742252175592714\n",
      "Epoch 175/300\n",
      "Average training loss: 0.005185865796274609\n",
      "Average test loss: 0.0065654146973457604\n",
      "Epoch 176/300\n",
      "Average training loss: 0.005144989249606927\n",
      "Average test loss: 0.05453465221863654\n",
      "Epoch 177/300\n",
      "Average training loss: 0.005132162749353382\n",
      "Average test loss: 0.07458942476121916\n",
      "Epoch 178/300\n",
      "Average training loss: 0.005134938391960329\n",
      "Average test loss: 0.6173228369239304\n",
      "Epoch 179/300\n",
      "Average training loss: 0.00514064138299889\n",
      "Average test loss: 0.6420956609232558\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0051278702604273954\n",
      "Average test loss: 1.7822961528897285\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0051215939463840595\n",
      "Average test loss: 0.0054742239560518\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0050973169818106625\n",
      "Average test loss: 0.07194811819245418\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0050823756808208095\n",
      "Average test loss: 10.184236737484733\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0050924709108140735\n",
      "Average test loss: 7.02410967124833\n",
      "Epoch 185/300\n",
      "Average training loss: 0.005097720810522636\n",
      "Average test loss: 0.005522477011299796\n",
      "Epoch 186/300\n",
      "Average training loss: 0.005161909058690071\n",
      "Average test loss: 1.466500705588609\n",
      "Epoch 187/300\n",
      "Average training loss: 0.005093259585814343\n",
      "Average test loss: 0.008116422815455332\n",
      "Epoch 188/300\n",
      "Average training loss: 0.005067186461140712\n",
      "Average test loss: 1.4945145011908478\n",
      "Epoch 189/300\n",
      "Average training loss: 0.005092390855153402\n",
      "Average test loss: 9.81442835262542\n",
      "Epoch 190/300\n",
      "Average training loss: 0.005092942915028996\n",
      "Average test loss: 0.03773966273830997\n",
      "Epoch 191/300\n",
      "Average training loss: 0.005060911078419951\n",
      "Average test loss: 0.019953608660234344\n",
      "Epoch 192/300\n",
      "Average training loss: 0.006994982991367579\n",
      "Average test loss: 0.7303629419348306\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007355795052730375\n",
      "Average test loss: 0.14920747983414265\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006114401241971387\n",
      "Average test loss: 97.02894441323893\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005505008858111169\n",
      "Average test loss: 0.005639549345605903\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005184013013624483\n",
      "Average test loss: 40.56858815765381\n",
      "Epoch 197/300\n",
      "Average training loss: 0.005068633134994242\n",
      "Average test loss: 33.13766315734718\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0050411318784786595\n",
      "Average test loss: 0.005655385432557928\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0050135312899947165\n",
      "Average test loss: 0.005609065567453702\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004997632088346614\n",
      "Average test loss: 0.007082856911338038\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0050091207942201035\n",
      "Average test loss: 0.3073138151930438\n",
      "Epoch 202/300\n",
      "Average training loss: 0.005000373117625714\n",
      "Average test loss: 0.012458297290321853\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0050675702024665144\n",
      "Average test loss: 8.380315874460257\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005017295291854276\n",
      "Average test loss: 2.0198770206885204\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004983976911753416\n",
      "Average test loss: 0.007293178412442406\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004979692266633113\n",
      "Average test loss: 4.099921457868484\n",
      "Epoch 208/300\n",
      "Average training loss: 0.005008784958057933\n",
      "Average test loss: 0.04163692540675402\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004974132576336463\n",
      "Average test loss: 0.041242344702283544\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004975380543619394\n",
      "Average test loss: 6.005101128978448\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004957442006924087\n",
      "Average test loss: 62.133292961562674\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00494955457167493\n",
      "Average test loss: 205.07703736224107\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00493510755068726\n",
      "Average test loss: 0.01574195681181219\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004942664787173271\n",
      "Average test loss: 0.006027614740447866\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0049541710437171985\n",
      "Average test loss: 0.005672755448768536\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004934872998959488\n",
      "Average test loss: 0.005798152251789967\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00493687446663777\n",
      "Average test loss: 0.32094140189016857\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004935416643404298\n",
      "Average test loss: 4.787726026493642\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004909997505446275\n",
      "Average test loss: 0.23501994694686598\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00490040425873465\n",
      "Average test loss: 392.3431358143091\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004875845750586854\n",
      "Average test loss: 473.06837688897053\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004925821265826622\n",
      "Average test loss: 0.012067073915567663\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004905561982757515\n",
      "Average test loss: 246.83038017738528\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004981814754919873\n",
      "Average test loss: 0.42859306777517003\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004852860319738587\n",
      "Average test loss: 0.42656902153624426\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004850835772024261\n",
      "Average test loss: 2382.1805188399653\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004869522149778075\n",
      "Average test loss: 0.037914362569650015\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004864180625312858\n",
      "Average test loss: 0.005460754601905743\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004840555729137526\n",
      "Average test loss: 0.007474817230883572\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0048555231475167805\n",
      "Average test loss: 67.98886327897364\n",
      "Epoch 234/300\n",
      "Average training loss: 0.005161385821799438\n",
      "Average test loss: 0.0055329435912685266\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00529739118181169\n",
      "Average test loss: 0.007763876431932052\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004843522445609172\n",
      "Average test loss: 0.005806453459792667\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004818681891179747\n",
      "Average test loss: 0.011660204908914036\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004826868918620878\n",
      "Average test loss: 0.00552702880733543\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004823325559082959\n",
      "Average test loss: 0.04868787101035317\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004814464124126567\n",
      "Average test loss: 0.6092341200278865\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004813388127419683\n",
      "Average test loss: 0.008076067522168159\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004826283169703351\n",
      "Average test loss: 0.005583200468992193\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004785736091021034\n",
      "Average test loss: 1.635486379676395\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004795376288394133\n",
      "Average test loss: 2.4242441132656403\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0048301584875832\n",
      "Average test loss: 0.007035970749540462\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004788438458823496\n",
      "Average test loss: 0.1995849336936242\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004815291078140338\n",
      "Average test loss: 1.9670216650300556\n",
      "Epoch 248/300\n",
      "Average training loss: 0.00478579192649987\n",
      "Average test loss: 9.41432279663119\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0047801577556464405\n",
      "Average test loss: 0.05352901328851779\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004781839076429605\n",
      "Average test loss: 0.014266525413013167\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004751079037371609\n",
      "Average test loss: 0.051818121350473825\n",
      "Epoch 252/300\n",
      "Average training loss: 0.005067719107700719\n",
      "Average test loss: 0.005713378062678708\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00485642631807261\n",
      "Average test loss: 0.008349215028186639\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004783238925246729\n",
      "Average test loss: 0.007065530975245767\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0047624915834102366\n",
      "Average test loss: 4.443768175092008\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004744988011403216\n",
      "Average test loss: 0.09276194535278612\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004907729936556683\n",
      "Average test loss: 0.3564904877576563\n",
      "Epoch 258/300\n",
      "Average training loss: 0.004777915183454752\n",
      "Average test loss: 0.039693750835748186\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004769017181876633\n",
      "Average test loss: 10.014437635726399\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004728194459444946\n",
      "Average test loss: 0.025042166783991786\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004738110657781363\n",
      "Average test loss: 141.84011506612103\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004711926885363128\n",
      "Average test loss: 0.20919746747778523\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004723849476625522\n",
      "Average test loss: 0.025355312623497512\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00470467088247339\n",
      "Average test loss: 0.1361726568672392\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004755047021640672\n",
      "Average test loss: 0.006489446097777949\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0047084243562486435\n",
      "Average test loss: 0.11567828773044878\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004711615638600455\n",
      "Average test loss: 0.7608017274273766\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004720029260549281\n",
      "Average test loss: 0.005661434741069873\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004690003611768285\n",
      "Average test loss: 5.607424100622535\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004699553335913353\n",
      "Average test loss: 0.005900478841322992\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004701658073812723\n",
      "Average test loss: 7.78824663151635\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004720122773614195\n",
      "Average test loss: 0.5257646296078132\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004664675840487083\n",
      "Average test loss: 0.030502411840690507\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004670821729426583\n",
      "Average test loss: 0.006008553508669138\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004657202180681957\n",
      "Average test loss: 0.005678208141691155\n",
      "Epoch 276/300\n",
      "Average training loss: 0.004668277009079853\n",
      "Average test loss: 1.1521840056313408\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004692342882355054\n",
      "Average test loss: 7553.105683241102\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004700625258187453\n",
      "Average test loss: 16.482498773088057\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004680404397762484\n",
      "Average test loss: 26825.842133419002\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005348758224811819\n",
      "Average test loss: 0.009730413989060455\n",
      "Epoch 281/300\n",
      "Average training loss: 0.004815435444522235\n",
      "Average test loss: 4.559410403658946\n",
      "Epoch 282/300\n",
      "Average training loss: 0.004899932697829273\n",
      "Average test loss: 0.012007967359489865\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004707513350579473\n",
      "Average test loss: 0.04731397456013494\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004796714569545455\n",
      "Average test loss: 0.006047920173654953\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004732979365521007\n",
      "Average test loss: 3.5980745010905797\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004692773912929826\n",
      "Average test loss: 0.007525350237886111\n",
      "Epoch 287/300\n",
      "Average training loss: 0.004729131249090036\n",
      "Average test loss: 0.005658343175219165\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004665051478892564\n",
      "Average test loss: 0.020620915465056896\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004660804791789916\n",
      "Average test loss: 0.0066237907840145955\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00464132952855693\n",
      "Average test loss: 0.005625819860233201\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004644917512519492\n",
      "Average test loss: 0.01584283757623699\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0046455127050479255\n",
      "Average test loss: 205.327374903361\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0046509057250287795\n",
      "Average test loss: 0.015964493894742596\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004639039884424872\n",
      "Average test loss: 0.012587832573387359\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004626435191267066\n",
      "Average test loss: 0.008937422919604513\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004629732785953416\n",
      "Average test loss: 0.056202701225462885\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004617605922122796\n",
      "Average test loss: 0.0057062931574053235\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004615141219149033\n",
      "Average test loss: 3.9601482402251826\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0046254823402398166\n",
      "Average test loss: 78.33075783295764\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004633593459510141\n",
      "Average test loss: 0.8086653992897934\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.25887488145629567\n",
      "Average test loss: 0.787424123081896\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015279780091510878\n",
      "Average test loss: 0.009110149888114797\n",
      "Epoch 3/300\n",
      "Average training loss: 0.012213719464010663\n",
      "Average test loss: 0.007108614623132679\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01062655656784773\n",
      "Average test loss: 0.01784604931829704\n",
      "Epoch 5/300\n",
      "Average training loss: 0.009641852175196012\n",
      "Average test loss: 0.006243055889589919\n",
      "Epoch 6/300\n",
      "Average training loss: 0.008908647478868563\n",
      "Average test loss: 0.005969663895666599\n",
      "Epoch 7/300\n",
      "Average training loss: 0.00836207252037194\n",
      "Average test loss: 0.006612197819269366\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007916008181456062\n",
      "Average test loss: 3.289163670996825\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007509380920065774\n",
      "Average test loss: 0.005100864807350768\n",
      "Epoch 10/300\n",
      "Average training loss: 0.00712590461389886\n",
      "Average test loss: 0.006480349435160558\n",
      "Epoch 11/300\n",
      "Average training loss: 0.006762843942476644\n",
      "Average test loss: 0.006071293021241824\n",
      "Epoch 12/300\n",
      "Average training loss: 0.006424600948476129\n",
      "Average test loss: 0.07670657209058603\n",
      "Epoch 13/300\n",
      "Average training loss: 0.006177914266371065\n",
      "Average test loss: 0.006412460072586934\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005966882296320465\n",
      "Average test loss: 0.0043964122492406105\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005762667676640881\n",
      "Average test loss: 0.00898116020941072\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00560178012193905\n",
      "Average test loss: 306.56116811963255\n",
      "Epoch 17/300\n",
      "Average training loss: 0.005435319972948895\n",
      "Average test loss: 72.57065037656824\n",
      "Epoch 18/300\n",
      "Average training loss: 0.005262032149152623\n",
      "Average test loss: 0.23861845796306927\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005126909461286333\n",
      "Average test loss: 2.369641527927584\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004994101090149747\n",
      "Average test loss: 5.276671783442299\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004875280411293109\n",
      "Average test loss: 1.1469597839547527\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004776168077356285\n",
      "Average test loss: 0.003522893718443811\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004639511253270838\n",
      "Average test loss: 26.700696445516414\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00453712347894907\n",
      "Average test loss: 1459.5158904190778\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004471461035311222\n",
      "Average test loss: 0.48999301800876854\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00438015428226855\n",
      "Average test loss: 2.35628572323463\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004327955310543378\n",
      "Average test loss: 0.8361062471204334\n",
      "Epoch 28/300\n",
      "Average training loss: 0.00425472988622884\n",
      "Average test loss: 0.006579271850900518\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004273838356551197\n",
      "Average test loss: 54.280376705209825\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015133170279777713\n",
      "Average test loss: 0.004329685415658686\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00572727911050121\n",
      "Average test loss: 0.004035116917143265\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005254250894197159\n",
      "Average test loss: 0.003733876683645778\n",
      "Epoch 33/300\n",
      "Average training loss: 0.005010945753918754\n",
      "Average test loss: 0.0036879154551360343\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004830979357163112\n",
      "Average test loss: 0.0034984629539151985\n",
      "Epoch 35/300\n",
      "Average training loss: 0.005018679810480939\n",
      "Average test loss: 0.0036143323673556247\n",
      "Epoch 36/300\n",
      "Average training loss: 0.00467797483669387\n",
      "Average test loss: 0.0052747517530288964\n",
      "Epoch 37/300\n",
      "Average training loss: 0.004508501100043456\n",
      "Average test loss: 0.017011178473631542\n",
      "Epoch 38/300\n",
      "Average training loss: 0.004421583390277293\n",
      "Average test loss: 0.0035705308994899193\n",
      "Epoch 39/300\n",
      "Average training loss: 0.004356370478040642\n",
      "Average test loss: 0.0032994694088896116\n",
      "Epoch 40/300\n",
      "Average training loss: 0.004303039357480076\n",
      "Average test loss: 0.0032586787500315244\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00425711344803373\n",
      "Average test loss: 0.0032498185450418127\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0042090860104395285\n",
      "Average test loss: 0.335383071737157\n",
      "Epoch 43/300\n",
      "Average training loss: 0.004161673616203997\n",
      "Average test loss: 0.0031920256455325417\n",
      "Epoch 44/300\n",
      "Average training loss: 0.008733473122947746\n",
      "Average test loss: 0.004463167308105363\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005912745500604312\n",
      "Average test loss: 0.5635837379412518\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005179576877918509\n",
      "Average test loss: 0.5396153880490198\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00489705027308729\n",
      "Average test loss: 0.03249648120916552\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0047410227577719425\n",
      "Average test loss: 0.1246290202372604\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0045996559171213045\n",
      "Average test loss: 6.947804892501897\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0044904078133404255\n",
      "Average test loss: 0.01702701001179715\n",
      "Epoch 51/300\n",
      "Average training loss: 0.004391947323663367\n",
      "Average test loss: 0.051521909108592404\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00431601139344275\n",
      "Average test loss: 275.2563108372688\n",
      "Epoch 53/300\n",
      "Average training loss: 0.004274034787383345\n",
      "Average test loss: 0.0031930738826178844\n",
      "Epoch 54/300\n",
      "Average training loss: 0.004210657016684612\n",
      "Average test loss: 87027.60783897569\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00416093840657009\n",
      "Average test loss: 0.0035061113921304545\n",
      "Epoch 56/300\n",
      "Average training loss: 0.004097420063076748\n",
      "Average test loss: 0.004473679746190707\n",
      "Epoch 57/300\n",
      "Average training loss: 0.004057287959795859\n",
      "Average test loss: 1394038.4540455164\n",
      "Epoch 58/300\n",
      "Average training loss: 0.004030141756559412\n",
      "Average test loss: 35901.84415867915\n",
      "Epoch 59/300\n",
      "Average training loss: 0.008271809890038437\n",
      "Average test loss: 66.68113984706004\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0053753267721169526\n",
      "Average test loss: 0.0442972917455352\n",
      "Epoch 61/300\n",
      "Average training loss: 0.004679896214356025\n",
      "Average test loss: 904.3468395453625\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004524061735512482\n",
      "Average test loss: 26.740577416526776\n",
      "Epoch 63/300\n",
      "Average training loss: 0.004282171448485719\n",
      "Average test loss: 1.735125525397559\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004138530490712987\n",
      "Average test loss: 1.860889905958126\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003984990170846383\n",
      "Average test loss: 3.4993782288947455\n",
      "Epoch 68/300\n",
      "Average training loss: 0.003966637032209999\n",
      "Average test loss: 0.784774095342805\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00400603710156348\n",
      "Average test loss: 726.7210079514667\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0039331130762067104\n",
      "Average test loss: 9.877305684932404\n",
      "Epoch 71/300\n",
      "Average training loss: 0.003910305777564645\n",
      "Average test loss: 111.75798901952317\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003897197222337127\n",
      "Average test loss: 349332955620.2382\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0041530904095206\n",
      "Average test loss: 0.0030237970989611413\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0038477240136514106\n",
      "Average test loss: 600.4194230565296\n",
      "Epoch 75/300\n",
      "Average training loss: 0.00824240712573131\n",
      "Average test loss: 1.7972759574155013\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0046235866240329215\n",
      "Average test loss: 0.005120784537659751\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004155660494955049\n",
      "Average test loss: 0.3149979222284423\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004029045705166128\n",
      "Average test loss: 22.53135228046092\n",
      "Epoch 80/300\n",
      "Average training loss: 0.003969569247422947\n",
      "Average test loss: 145.67531712251406\n",
      "Epoch 81/300\n",
      "Average training loss: 0.003930429697864586\n",
      "Average test loss: 1385.4791311204401\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009515250886686973\n",
      "Average test loss: 0.003865962654352188\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0051378717159645424\n",
      "Average test loss: 0.7169154527468814\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004664190452959803\n",
      "Average test loss: 0.014359727817587554\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00433442446610166\n",
      "Average test loss: 0.05612337706817521\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004183741030800674\n",
      "Average test loss: 0.02315630566711641\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004131350670423773\n",
      "Average test loss: 3639.4421283075776\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004009618397388193\n",
      "Average test loss: 0.26907471210426753\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003958976013172004\n",
      "Average test loss: 35796590435.44178\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003996702753007412\n",
      "Average test loss: 96.26393378408419\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0038999641181694136\n",
      "Average test loss: 22.494610478913618\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004020446204683847\n",
      "Average test loss: 0.09832832030372488\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004009189160333739\n",
      "Average test loss: 5.989517034909792\n",
      "Epoch 94/300\n",
      "Average training loss: 0.003843680339347985\n",
      "Average test loss: 4.475178375550442\n",
      "Epoch 95/300\n",
      "Average training loss: 0.009314175502086679\n",
      "Average test loss: 0.003827258337289095\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004775984834051794\n",
      "Average test loss: 2.59540295613971\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0040171570047322245\n",
      "Average test loss: 0.003080946921888325\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003935810090353092\n",
      "Average test loss: 0.26049719291801254\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0038947491757571695\n",
      "Average test loss: 0.03049656099246608\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0038516290731107194\n",
      "Average test loss: 0.003143908290606406\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0038153501749038695\n",
      "Average test loss: 3010.581477330577\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0038248951726903516\n",
      "Average test loss: 100.15615315984085\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0037771185895221103\n",
      "Average test loss: 392.49958449003265\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0037676257371074624\n",
      "Average test loss: 0.7717536018962662\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003913819246408012\n",
      "Average test loss: 0.13928799685380525\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003749020656570792\n",
      "Average test loss: 0.7590486532416608\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0037860537363837163\n",
      "Average test loss: 0.009711550952452753\n",
      "Epoch 112/300\n",
      "Average training loss: 0.003739786881332596\n",
      "Average test loss: 1.9847448167055846\n",
      "Epoch 113/300\n",
      "Average training loss: 0.003679856232263976\n",
      "Average test loss: 297.3800062934028\n",
      "Epoch 114/300\n",
      "Average training loss: 0.003666099409262339\n",
      "Average test loss: 3.3091163354557422\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0037422084235068822\n",
      "Average test loss: 615778.9878125\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0036934023199396\n",
      "Average test loss: 5.5576183079626\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003667378992463152\n",
      "Average test loss: 0.003121098108589649\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0038190059425930183\n",
      "Average test loss: 0.11360997742828395\n",
      "Epoch 119/300\n",
      "Average training loss: 0.003658494283962581\n",
      "Average test loss: 925932.9718888889\n",
      "Epoch 120/300\n",
      "Average training loss: 0.003553346305464705\n",
      "Average test loss: 13.649142417245441\n",
      "Epoch 123/300\n",
      "Average training loss: 0.005811033959397012\n",
      "Average test loss: 1.3686670917347075\n",
      "Epoch 124/300\n",
      "Average training loss: 0.005916209066079723\n",
      "Average test loss: 2.9481074229321544\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004455206190960275\n",
      "Average test loss: 12387.542341919381\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004148563574378689\n",
      "Average test loss: 322.8001816560982\n",
      "Epoch 127/300\n",
      "Average training loss: 0.003973776555309693\n",
      "Average test loss: 74440.99966203775\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003850839862393008\n",
      "Average test loss: 56815.84393114475\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0037507202579743332\n",
      "Average test loss: 0.12315160515490506\n",
      "Epoch 130/300\n",
      "Average training loss: 0.003785763709495465\n",
      "Average test loss: 225.26322650589526\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0036279117193900876\n",
      "Average test loss: 1352770.2860398057\n",
      "Epoch 132/300\n",
      "Average training loss: 0.003590818944490618\n",
      "Average test loss: 65108.04220762371\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0035951897309472163\n",
      "Average test loss: 21.84344845054733\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0035721810965074433\n",
      "Average test loss: 532.8343455003885\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004625621156767011\n",
      "Average test loss: 0.7662551582070688\n",
      "Epoch 136/300\n",
      "Average training loss: 0.003761753113112516\n",
      "Average test loss: 31.706700714565194\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0036257985391550595\n",
      "Average test loss: 645.0938161223265\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0035687765586707326\n",
      "Average test loss: 0.07128007875983086\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003527254050804509\n",
      "Average test loss: 1.1832084435431494\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0035124318158874907\n",
      "Average test loss: 295446177.3795556\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0036222280245274303\n",
      "Average test loss: 0.22416841912931867\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0034912372457070484\n",
      "Average test loss: 0.003912296971823606\n",
      "Epoch 143/300\n",
      "Average training loss: 0.003499236395996478\n",
      "Average test loss: 78078768.41993794\n",
      "Epoch 144/300\n",
      "Average training loss: 0.003519950369372964\n",
      "Average test loss: 1210.9658639195322\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003460999343337284\n",
      "Average test loss: 1.4729855224788189\n",
      "Epoch 146/300\n",
      "Average training loss: 0.003442810549504227\n",
      "Average test loss: 586072.6663693576\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0034966498085608087\n",
      "Average test loss: 0.0029585049307594698\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0034122253478401235\n",
      "Average test loss: 410.54674094110726\n",
      "Epoch 149/300\n",
      "Average training loss: 0.003387928595145543\n",
      "Average test loss: 17970.5002948958\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0034619231037795544\n",
      "Average test loss: 458.53629439219503\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0033800594398958814\n",
      "Average test loss: 646394420.0675555\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0033611284821397727\n",
      "Average test loss: 1073074.9180158733\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0033512458499107096\n",
      "Average test loss: 24343.22248076043\n",
      "Epoch 154/300\n",
      "Average training loss: 0.003361580280173156\n",
      "Average test loss: 1367.7361148718294\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0033269808867739305\n",
      "Average test loss: 31.469591707787995\n",
      "Epoch 156/300\n",
      "Average training loss: 0.003305031968901555\n",
      "Average test loss: 1.3403225452788174\n",
      "Epoch 157/300\n",
      "Average training loss: 0.003312212913814518\n",
      "Average test loss: 42.29388820230547\n",
      "Epoch 158/300\n",
      "Average training loss: 0.003398132609617379\n",
      "Average test loss: 0.14228455561358067\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0033038891539391544\n",
      "Average test loss: 803.074014578705\n",
      "Epoch 160/300\n",
      "Average training loss: 0.003294216562890344\n",
      "Average test loss: 875.2974174191422\n",
      "Epoch 161/300\n",
      "Average training loss: 0.003330753991794255\n",
      "Average test loss: 0.00660713676446014\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0032656809724867343\n",
      "Average test loss: 0.7399611531833393\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004148684832163983\n",
      "Average test loss: 0.09547250302404993\n",
      "Epoch 164/300\n",
      "Average training loss: 0.003426914437984427\n",
      "Average test loss: 3.0523235426694155\n",
      "Epoch 165/300\n",
      "Average training loss: 0.003261746274928252\n",
      "Average test loss: 28.471615711964045\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0032389192132072316\n",
      "Average test loss: 9299.291712015374\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0031974681216395563\n",
      "Average test loss: 9944.471753321517\n",
      "Epoch 168/300\n",
      "Average training loss: 0.003186051536558403\n",
      "Average test loss: 12.45832491622327\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0035610261232488687\n",
      "Average test loss: 11.068048291442087\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0038346086614247823\n",
      "Average test loss: 0.04585954546266132\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0033601275210579234\n",
      "Average test loss: 35.25623200271113\n",
      "Epoch 172/300\n",
      "Average training loss: 0.003263363574941953\n",
      "Average test loss: 0.14791724503454234\n",
      "Epoch 173/300\n",
      "Average training loss: 0.003217399464506242\n",
      "Average test loss: 38.7543571581499\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0031871898093571265\n",
      "Average test loss: 1.1574019675412113\n",
      "Epoch 175/300\n",
      "Average training loss: 0.003170338262907333\n",
      "Average test loss: 1011.241724364188\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0031726921372529533\n",
      "Average test loss: 4.288605192351673\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0040191992150826585\n",
      "Average test loss: 42635.5873130758\n",
      "Epoch 178/300\n",
      "Average training loss: 0.003330081426848968\n",
      "Average test loss: 24052.00937111822\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0033069348747117653\n",
      "Average test loss: 3.496979887732201\n",
      "Epoch 180/300\n",
      "Average training loss: 0.003362808417942789\n",
      "Average test loss: 0.003156010158153044\n",
      "Epoch 181/300\n",
      "Average training loss: 0.003365846834248967\n",
      "Average test loss: 74.64133737875687\n",
      "Epoch 182/300\n",
      "Average training loss: 0.003289137509548002\n",
      "Average test loss: 321.9086319601503\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0032612841510110432\n",
      "Average test loss: 224.98954619225364\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0032013962831762104\n",
      "Average test loss: 1798.9752836261723\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0032556964854399363\n",
      "Average test loss: 6462.971458141479\n",
      "Epoch 186/300\n",
      "Average training loss: 0.003202551440646251\n",
      "Average test loss: 159115.53303585033\n",
      "Epoch 187/300\n",
      "Average training loss: 0.003191803353942103\n",
      "Average test loss: 324.9389758711987\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003193529279695617\n",
      "Average test loss: 28731.021677591958\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0032568665658020313\n",
      "Average test loss: 11724075.728714729\n",
      "Epoch 190/300\n",
      "Average training loss: 0.003185227490754591\n",
      "Average test loss: 265558000.22445604\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0031571015131970246\n",
      "Average test loss: 11624.730071378111\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00315415465976629\n",
      "Average test loss: 643247.0971375284\n",
      "Epoch 193/300\n",
      "Average training loss: 0.003162267822565304\n",
      "Average test loss: 14624942.994729823\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0031516738643662797\n",
      "Average test loss: 51010056.689028464\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0031179719515558747\n",
      "Average test loss: 1172163.7906881343\n",
      "Epoch 196/300\n",
      "Average training loss: 0.003120384796212117\n",
      "Average test loss: 1418985.7928687225\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0031065409446342125\n",
      "Average test loss: 20744069.80376683\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0032596418193231026\n",
      "Average test loss: 18595.830713931675\n",
      "Epoch 199/300\n",
      "Average training loss: 0.003088048855463664\n",
      "Average test loss: 343523.5398449337\n",
      "Epoch 200/300\n",
      "Average training loss: 0.003081708015460107\n",
      "Average test loss: 26269760.553216703\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0030557100056774087\n",
      "Average test loss: 257380284.61443114\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0030729390072325867\n",
      "Average test loss: 195941282823.8249\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0031458275264336\n",
      "Average test loss: 6486.603836765834\n",
      "Epoch 204/300\n",
      "Average training loss: 0.003028176079193751\n",
      "Average test loss: 1580339.6875791065\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0030480981833404964\n",
      "Average test loss: 286779195.9251159\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0030330342762172223\n",
      "Average test loss: 2416.794244179449\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00315338661811418\n",
      "Average test loss: 303364.1885408587\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003040090818165077\n",
      "Average test loss: 68286.15015863584\n",
      "Epoch 209/300\n",
      "Average training loss: 0.003008539990418487\n",
      "Average test loss: 136950.95657491064\n",
      "Epoch 210/300\n",
      "Average training loss: 0.003006911075156596\n",
      "Average test loss: 1298.034104211327\n",
      "Epoch 211/300\n",
      "Average training loss: 0.003028893591215213\n",
      "Average test loss: 2923337.7677059374\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0032179385599576766\n",
      "Average test loss: 1210226515.2953465\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0030442637343787485\n",
      "Average test loss: 7219.442885279738\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0030255526565015315\n",
      "Average test loss: 61.23303416771276\n",
      "Epoch 215/300\n",
      "Average training loss: 0.002973214413660268\n",
      "Average test loss: 733.1722496244108\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0029774865586724548\n",
      "Average test loss: 1924.9963533387415\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0029769842630873126\n",
      "Average test loss: 621211.0115057069\n",
      "Epoch 218/300\n",
      "Average training loss: 0.002960022696397371\n",
      "Average test loss: 191.08565334714618\n",
      "Epoch 219/300\n",
      "Average training loss: 0.003036246760437886\n",
      "Average test loss: 5583.157787627291\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0036265506909953222\n",
      "Average test loss: 0.2668965312110053\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0033600565381348133\n",
      "Average test loss: 0.12142774944421318\n",
      "Epoch 222/300\n",
      "Average training loss: 0.003164365057523052\n",
      "Average test loss: 0.36174122491603095\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0031188870155149035\n",
      "Average test loss: 1013.8678427052312\n",
      "Epoch 224/300\n",
      "Average training loss: 0.003110835459911161\n",
      "Average test loss: 14586.980533912245\n",
      "Epoch 225/300\n",
      "Average training loss: 0.003142916767961449\n",
      "Average test loss: 8.685157763532466\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0034162085503339767\n",
      "Average test loss: 3.944954303174383\n",
      "Epoch 227/300\n",
      "Average training loss: 0.003047763165500429\n",
      "Average test loss: 0.01674918638666471\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002949744908966952\n",
      "Average test loss: 0.4726869719872872\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0029635220606707862\n",
      "Average test loss: 221.30339495239488\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0029914889203177556\n",
      "Average test loss: 19.946258348396256\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0031214736834582354\n",
      "Average test loss: 21569.155085985974\n",
      "Epoch 232/300\n",
      "Average training loss: 0.003008113189910849\n",
      "Average test loss: 18.208355241694385\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00305987161770463\n",
      "Average test loss: 17480646.084993325\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0030020254922823774\n",
      "Average test loss: 0.032513463030672735\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0029775403135766584\n",
      "Average test loss: 896.6209827460498\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0029773810251305503\n",
      "Average test loss: 7246.065833399119\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0030682747401297094\n",
      "Average test loss: 3.1799639086530855\n",
      "Epoch 238/300\n",
      "Average training loss: 0.003061739719990227\n",
      "Average test loss: 0.4297231393572357\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00303063564395739\n",
      "Average test loss: 42.39746525046892\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0031263721357617114\n",
      "Average test loss: 0.16794257769216267\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0030717039530475934\n",
      "Average test loss: 591.8377155202023\n",
      "Epoch 242/300\n",
      "Average training loss: 0.003105677308514714\n",
      "Average test loss: 4746660.200111111\n",
      "Epoch 243/300\n",
      "Average training loss: 0.003042273350266947\n",
      "Average test loss: 67294.47072580979\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0031090320328043567\n",
      "Average test loss: 107113.85098163127\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003085646309786373\n",
      "Average test loss: 0.15041369416730271\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002983376868483093\n",
      "Average test loss: 15.031920222075863\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0031456429736895692\n",
      "Average test loss: 2842789.2120799217\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0032713034130218955\n",
      "Average test loss: 176.30565348369214\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0029955137711432244\n",
      "Average test loss: 0.03911757560612427\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0033917380755560268\n",
      "Average test loss: 0.4306609708186653\n",
      "Epoch 251/300\n",
      "Average training loss: 0.00316513984484805\n",
      "Average test loss: 116.50973344046363\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0032225077425440153\n",
      "Average test loss: 5047.2809401700115\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0030852882400568987\n",
      "Average test loss: 38640.245395529004\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0029680120369626416\n",
      "Average test loss: 3.0820080394728318\n",
      "Epoch 255/300\n",
      "Average training loss: 0.003048175964711441\n",
      "Average test loss: 49916.997295978545\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003094615859289964\n",
      "Average test loss: 41.108748425213\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003282066904836231\n",
      "Average test loss: 7.352419505613132\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003059798729709453\n",
      "Average test loss: 4082.440900517649\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0031814582902524205\n",
      "Average test loss: 3465.653207935187\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0029641562718898057\n",
      "Average test loss: 0.0036839273961053955\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003055783100426197\n",
      "Average test loss: 2.892185124020403\n",
      "Epoch 262/300\n",
      "Average training loss: 0.003016885319103797\n",
      "Average test loss: 59767.10933242774\n",
      "Epoch 263/300\n",
      "Average training loss: 0.003015961696911189\n",
      "Average test loss: 22081.66795780716\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0030400692565987508\n",
      "Average test loss: 1.8697031491860334\n",
      "Epoch 265/300\n",
      "Average training loss: 0.002978320666278402\n",
      "Average test loss: 193.53950430322087\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0029666160272641315\n",
      "Average test loss: 13834636.673333334\n",
      "Epoch 267/300\n",
      "Average training loss: 0.003027338245883584\n",
      "Average test loss: 531.2589921460715\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0029694691693617236\n",
      "Average test loss: 10.118486379919366\n",
      "Epoch 269/300\n",
      "Average training loss: 0.003267345287526647\n",
      "Average test loss: 0.6583262116811756\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0031156945207880603\n",
      "Average test loss: 380268.77451420284\n",
      "Epoch 271/300\n",
      "Average training loss: 0.003069528038923939\n",
      "Average test loss: 388.8302473449161\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0029343038532469007\n",
      "Average test loss: 1037.8954505941845\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002905920414254069\n",
      "Average test loss: 629191.6130340315\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0029667539476520483\n",
      "Average test loss: 73177.3152851574\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0029135203640908\n",
      "Average test loss: 290.8826527833102\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0029544787922253213\n",
      "Average test loss: 434489.9682735554\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0029561701224495966\n",
      "Average test loss: 139.48133150151455\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0029494005859726003\n",
      "Average test loss: 60.553698743260156\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002921474492798249\n",
      "Average test loss: 395.5179051721402\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0028942307343499526\n",
      "Average test loss: 7556.2322340364435\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003127122089887659\n",
      "Average test loss: 96225.5316988792\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0029024329133745695\n",
      "Average test loss: 2050197.1012207642\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0029425149295065137\n",
      "Average test loss: 2.272730959625708\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0028709807377308607\n",
      "Average test loss: 280.44400646525247\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0038337388527062205\n",
      "Average test loss: 189051408402.20444\n",
      "Epoch 286/300\n",
      "Average training loss: 0.020761819699572193\n",
      "Average test loss: 1889.2245916070608\n",
      "Epoch 287/300\n",
      "Average training loss: 0.00656511545388235\n",
      "Average test loss: 9915.179541965656\n",
      "Epoch 288/300\n",
      "Average training loss: 0.005708302962283293\n",
      "Average test loss: 0.08118290824422406\n",
      "Epoch 289/300\n",
      "Average training loss: 0.005166573375877407\n",
      "Average test loss: 0.510450582323182\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004846529297530651\n",
      "Average test loss: 0.47849831743207244\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004593037791550159\n",
      "Average test loss: 0.010508069946534105\n",
      "Epoch 292/300\n",
      "Average training loss: 0.004403429060553511\n",
      "Average test loss: 2.0137300435751677\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004257992910014259\n",
      "Average test loss: 0.40290670500116216\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004119639163629877\n",
      "Average test loss: 4.451466955382791\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004047977781544129\n",
      "Average test loss: 28.050353971690768\n",
      "Epoch 296/300\n",
      "Average training loss: 0.003964820694178342\n",
      "Average test loss: 0.6145767116645972\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0039002859358572297\n",
      "Average test loss: 31531.352505011037\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005028568324529462\n",
      "Average test loss: 10311449.222206749\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0038799073048349886\n",
      "Average test loss: 35970.92064409285\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0036733912177797823\n",
      "Average test loss: 4.540928783697386\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.21148033206330405\n",
      "Average test loss: 0.008058573106096851\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011893224136696922\n",
      "Average test loss: 1.6784351149929895\n",
      "Epoch 3/300\n",
      "Average training loss: 0.009196943648159504\n",
      "Average test loss: 0.005546217269781563\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008043235540390015\n",
      "Average test loss: 0.005587353745268451\n",
      "Epoch 5/300\n",
      "Average training loss: 0.007294496066040463\n",
      "Average test loss: 1.2989449366836083\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006770342034598192\n",
      "Average test loss: 0.004508227795362472\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006345369858046373\n",
      "Average test loss: 1.5365023962656656\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0059800621759560375\n",
      "Average test loss: 0.008487461468411816\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0056647186614573\n",
      "Average test loss: 0.033891682801975145\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005379198131048017\n",
      "Average test loss: 0.004760296126620637\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005124240004767975\n",
      "Average test loss: 0.0036696775166524783\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0048977399273879\n",
      "Average test loss: 0.004409244957690438\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0047185184632738434\n",
      "Average test loss: 1.6004742146084705\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004564952053543594\n",
      "Average test loss: 0.0031727922186255454\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004395090481680301\n",
      "Average test loss: 0.4506581023401684\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004279874606885844\n",
      "Average test loss: 0.0030364216348777217\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004145746450871229\n",
      "Average test loss: 8.149402804083294\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004005124132252401\n",
      "Average test loss: 0.012896835198832883\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0038798139672726393\n",
      "Average test loss: 0.04132426388623814\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0037713383223033615\n",
      "Average test loss: 0.002691037189008461\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0036620410767694315\n",
      "Average test loss: 0.5537428771224286\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003551800304816829\n",
      "Average test loss: 0.003942579112119145\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0034651858512726097\n",
      "Average test loss: 0.0025180571023374794\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00337308704915146\n",
      "Average test loss: 9.525353715639975\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0033835631544805234\n",
      "Average test loss: 0.11631733145813147\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003188433773815632\n",
      "Average test loss: 0.025816017346663608\n",
      "Epoch 27/300\n",
      "Average training loss: 0.003102562297963434\n",
      "Average test loss: 1.6866804767630788\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0030477641680174404\n",
      "Average test loss: 0.8968369633547134\n",
      "Epoch 29/300\n",
      "Average training loss: 0.005469286890079578\n",
      "Average test loss: 41.398080530476236\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0036241542605890166\n",
      "Average test loss: 6.161438263859186\n",
      "Epoch 31/300\n",
      "Average training loss: 0.003301437954728802\n",
      "Average test loss: 213.1796820322507\n",
      "Epoch 32/300\n",
      "Average training loss: 0.003163726755935285\n",
      "Average test loss: 0.0037208233550190925\n",
      "Epoch 33/300\n",
      "Average training loss: 0.003067309149644441\n",
      "Average test loss: 0.2863079182390744\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0029870866021762294\n",
      "Average test loss: 1316.5996703604137\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0029415168938123517\n",
      "Average test loss: 906.2014558083887\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002891743395063612\n",
      "Average test loss: 28.98636282500128\n",
      "Epoch 37/300\n",
      "Average training loss: 0.002850126966420147\n",
      "Average test loss: 69.99446508336895\n",
      "Epoch 38/300\n",
      "Average training loss: 0.002824820850044489\n",
      "Average test loss: 74429.30874977948\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0027782173371977276\n",
      "Average test loss: 13027.03906801711\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002737237219595247\n",
      "Average test loss: 2050.5727135952843\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0027271113706131777\n",
      "Average test loss: 564.4932140488559\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002682093627129992\n",
      "Average test loss: 38.63580124265866\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0026524929884407258\n",
      "Average test loss: 0.014676532886508439\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003813430916517973\n",
      "Average test loss: 0.7191120708791746\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0028852814158631694\n",
      "Average test loss: 0.02293894085991714\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002732108459704452\n",
      "Average test loss: 395.3128592533213\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002689549565936128\n",
      "Average test loss: 59.450343524171245\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0028509431735922893\n",
      "Average test loss: 0.5316115085490876\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0026103120661444133\n",
      "Average test loss: 92.25571045847713\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0025777589772931404\n",
      "Average test loss: 192.2095381710257\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0025417473516944383\n",
      "Average test loss: 7.108680900275707\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00255251469111277\n",
      "Average test loss: 2.5252074479951214\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0025115839431269302\n",
      "Average test loss: 0.0019619751941516166\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0033750528811166683\n",
      "Average test loss: 14.65688488131265\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0025972444655166732\n",
      "Average test loss: 0.2665144233332119\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0025086780675790377\n",
      "Average test loss: 0.15202044731544123\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002468298044262661\n",
      "Average test loss: 6310.396344579992\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002450936376841532\n",
      "Average test loss: 0.622773106119285\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0024327720237068006\n",
      "Average test loss: 0.0616380448612488\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0024268873412575985\n",
      "Average test loss: 5775.899834782908\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0023940538830227323\n",
      "Average test loss: 41596.00698838114\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002573099404780401\n",
      "Average test loss: 0.8840634752907273\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0024055926905324062\n",
      "Average test loss: 22.382440242937868\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0023670183080765936\n",
      "Average test loss: 251.99447006922796\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002345755133467416\n",
      "Average test loss: 5313.262022685879\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0023466892815712423\n",
      "Average test loss: 0.9801994618671015\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002322632499453094\n",
      "Average test loss: 11009.411581908442\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0022980547725326484\n",
      "Average test loss: 0.025371530251680978\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0022747447009508807\n",
      "Average test loss: 543.28619830432\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0022704657645275196\n",
      "Average test loss: 38781.42792189084\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0022595522082928155\n",
      "Average test loss: 1426459.72883648\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002695856682335337\n",
      "Average test loss: 65.137700915916\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002435943157412112\n",
      "Average test loss: 1.2385720145259467\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002357078292303615\n",
      "Average test loss: 79.27572770904833\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002382835090988212\n",
      "Average test loss: 2.1871093800558397\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0022982165831037693\n",
      "Average test loss: 23.738137102168054\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0022776969293546345\n",
      "Average test loss: 69.82207778000418\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0022483721414787903\n",
      "Average test loss: 0.852327968443537\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0022401938886485165\n",
      "Average test loss: 2.4028336076000705\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002217543550973965\n",
      "Average test loss: 0.20507242817762825\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0022219502010072272\n",
      "Average test loss: 8601.741625538716\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002377388408407569\n",
      "Average test loss: 307.4978833291874\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002257615825575259\n",
      "Average test loss: 245.97537599425803\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002301000461810165\n",
      "Average test loss: 53.38485615142849\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002368400959298015\n",
      "Average test loss: 0.0056821324154734615\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002186679562036362\n",
      "Average test loss: 6.396642222332354\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0021695379367512135\n",
      "Average test loss: 0.8981639570781134\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0021105088258369102\n",
      "Average test loss: 1311.4238149835496\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0021140372495477397\n",
      "Average test loss: 2.615495590874925\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0021044058523451287\n",
      "Average test loss: 8526.50707057925\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003015191310809718\n",
      "Average test loss: 0.015627114401095443\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0024599915196498235\n",
      "Average test loss: 0.005068368336082332\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0022437046494127974\n",
      "Average test loss: 212.69689502392038\n",
      "Epoch 94/300\n",
      "Average training loss: 0.00217167684642805\n",
      "Average test loss: 2.4783386332711412\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00215792872901592\n",
      "Average test loss: 2.7040277627971436\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002103878264211946\n",
      "Average test loss: 5.052331692907545\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002080209251286255\n",
      "Average test loss: 0.10727577956176053\n",
      "Epoch 98/300\n",
      "Average training loss: 0.00217393873932047\n",
      "Average test loss: 12.831182778504159\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0021156479901530676\n",
      "Average test loss: 0.0020952718748400607\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0021972690698587234\n",
      "Average test loss: 0.6422860198546615\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002089644620815913\n",
      "Average test loss: 1.2387833023510046\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0021070855181250306\n",
      "Average test loss: 30.783431533272896\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002080773131301006\n",
      "Average test loss: 2452.4707214626737\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002066271690548294\n",
      "Average test loss: 13.589081419879157\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002038952495592336\n",
      "Average test loss: 369.3740685714941\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002038788266479969\n",
      "Average test loss: 911.7611991851885\n",
      "Epoch 107/300\n",
      "Average training loss: 0.00225956625243028\n",
      "Average test loss: 0.006344263852884372\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0021756539096434912\n",
      "Average test loss: 1825.859985951768\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0020519591896898215\n",
      "Average test loss: 0.12911435034622748\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002056084959871239\n",
      "Average test loss: 25197.6869374908\n",
      "Epoch 111/300\n",
      "Average training loss: 0.001988658220714165\n",
      "Average test loss: 367.43667547644424\n",
      "Epoch 112/300\n",
      "Average training loss: 0.002036381414367093\n",
      "Average test loss: 18.67314525460866\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002084447221416566\n",
      "Average test loss: 1.7817054739768305\n",
      "Epoch 114/300\n",
      "Average training loss: 0.002103631889137129\n",
      "Average test loss: 55.50710937458111\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00209467853160782\n",
      "Average test loss: 0.6275514127653506\n",
      "Epoch 116/300\n",
      "Average training loss: 0.00210567047757407\n",
      "Average test loss: 2.808642250576367\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0020873656840994952\n",
      "Average test loss: 0.35531918090199016\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00207538160112583\n",
      "Average test loss: 165.26667800809858\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0020277066803019907\n",
      "Average test loss: 6.774678077084737\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0020190104691104756\n",
      "Average test loss: 144.3842892200955\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002050380556844175\n",
      "Average test loss: 30.09787581669208\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002029524157030715\n",
      "Average test loss: 0.11471019857770039\n",
      "Epoch 123/300\n",
      "Average training loss: 0.001969611805967159\n",
      "Average test loss: 13925.41033838344\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0019520029168989923\n",
      "Average test loss: 41394.22395771049\n",
      "Epoch 125/300\n",
      "Average training loss: 0.001945892473993202\n",
      "Average test loss: 2995.932989077232\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0019289509065242277\n",
      "Average test loss: 2006.0142522949104\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0019589274944737554\n",
      "Average test loss: 0.06543892581450443\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0019458500464550324\n",
      "Average test loss: 285.8977997676035\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0019521783564446702\n",
      "Average test loss: 41340.49696873547\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019397246138089233\n",
      "Average test loss: 1176.4406755061098\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0019091729695598285\n",
      "Average test loss: 548.7531439700408\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0019108394139135878\n",
      "Average test loss: 30.597099395095473\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0019085197274883588\n",
      "Average test loss: 43.89555687615979\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0018984642782145077\n",
      "Average test loss: 0.0019385537670718299\n",
      "Epoch 135/300\n",
      "Average training loss: 0.001961375224507517\n",
      "Average test loss: 4038.160787413279\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0019128879337675041\n",
      "Average test loss: 1.5869228247619338\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018870597507597672\n",
      "Average test loss: 1778.7238041078488\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0019341538126269977\n",
      "Average test loss: 1840.4620568033854\n",
      "Epoch 139/300\n",
      "Average training loss: 0.001840983120103677\n",
      "Average test loss: 5.346820332699352\n",
      "Epoch 140/300\n",
      "Average training loss: 0.001825705649745133\n",
      "Average test loss: 513.890084027465\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0018230568773837553\n",
      "Average test loss: 36764.40160993392\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0018161554887062973\n",
      "Average test loss: 665679.5890294919\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0018206038538159596\n",
      "Average test loss: 406355.7733466744\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0018216784241505794\n",
      "Average test loss: 5922.612472946674\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0017965632879899608\n",
      "Average test loss: 9241536.48584375\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0017929094696624412\n",
      "Average test loss: 2.8007516467290827\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0017917706179949972\n",
      "Average test loss: 3.2171735609742917\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0017956968653533194\n",
      "Average test loss: 2.7100292187585597\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0017824350031506685\n",
      "Average test loss: 0.00907763490298142\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0017777823559525941\n",
      "Average test loss: 274.92038269121616\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001765120901581314\n",
      "Average test loss: 8447.165081020468\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0017667652036373815\n",
      "Average test loss: 11020032.304015625\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0031809331314224336\n",
      "Average test loss: 3.423680108944575\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0023853010578701894\n",
      "Average test loss: 0.15966654263271227\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0021361233128441705\n",
      "Average test loss: 1.498267415796717\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002032187749631703\n",
      "Average test loss: 44.17003210656531\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0019290337835748991\n",
      "Average test loss: 137.54679181893667\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0018829323775652381\n",
      "Average test loss: 650.3700969311719\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0018604841557227903\n",
      "Average test loss: 1436.8025392778184\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0018627084182161423\n",
      "Average test loss: 1.1357995683248672\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018602692164066765\n",
      "Average test loss: 6.067420814981477\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0018397030516838033\n",
      "Average test loss: 159.18858109377987\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0018460659271933965\n",
      "Average test loss: 0.357352518299698\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0018095396740569008\n",
      "Average test loss: 0.1353234710143879\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001826321430090401\n",
      "Average test loss: 10.95428727328529\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0018277405464193887\n",
      "Average test loss: 2.6930367262992596\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0018298800374484724\n",
      "Average test loss: 26919.24030409889\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0018265624056673711\n",
      "Average test loss: 226.12749288613847\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0018071736994509896\n",
      "Average test loss: 343.45881507325754\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0017987229664706522\n",
      "Average test loss: 4517.729500418867\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0017617814172473218\n",
      "Average test loss: 185.81474084307916\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0017410394673546155\n",
      "Average test loss: 416.3223359540417\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001762087488029566\n",
      "Average test loss: 629.3868447486145\n",
      "Epoch 174/300\n",
      "Average training loss: 0.001988700691403614\n",
      "Average test loss: 0.14439538975908525\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0019740839944117597\n",
      "Average test loss: 1.1754206315676372\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002029154820367694\n",
      "Average test loss: 0.06262181370912327\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0019564368227082823\n",
      "Average test loss: 0.015708098239782784\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0019546069260686634\n",
      "Average test loss: 0.05027640036327971\n",
      "Epoch 179/300\n",
      "Average training loss: 0.001949154798562328\n",
      "Average test loss: 0.010696892574222551\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0019104260159656405\n",
      "Average test loss: 9.556566998075384\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0018962886875702274\n",
      "Average test loss: 70.54045143251783\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0019031279637581773\n",
      "Average test loss: 7.943766294735536\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0018802183155591289\n",
      "Average test loss: 0.5443633603099733\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001796309162552158\n",
      "Average test loss: 0.09496559944945492\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0018526559785629311\n",
      "Average test loss: 1.7035611587941855\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0018181871007093124\n",
      "Average test loss: 3071.6216102350863\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0017747637950297859\n",
      "Average test loss: 736.3561172417535\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002221363106949462\n",
      "Average test loss: 0.30955797351948505\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0018821048642405206\n",
      "Average test loss: 49.17963786723703\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0017646991259728869\n",
      "Average test loss: 48276.667639156556\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0017439937679510978\n",
      "Average test loss: 0.38056893683390486\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0017580502191558481\n",
      "Average test loss: 0.005848297636749016\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0018087742790165874\n",
      "Average test loss: 2.857002602327201\n",
      "Epoch 194/300\n",
      "Average training loss: 0.001767171252415412\n",
      "Average test loss: 11428.829828821306\n",
      "Epoch 195/300\n",
      "Average training loss: 0.001840474576378862\n",
      "Average test loss: 18.86356830256101\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0017902794733850491\n",
      "Average test loss: 141.1355548705852\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0017357912728976873\n",
      "Average test loss: 0.04119142615908964\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0017228156614841686\n",
      "Average test loss: 0.008269529486178524\n",
      "Epoch 199/300\n",
      "Average training loss: 0.001761928724228508\n",
      "Average test loss: 9.998288593202002\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0017710529607203273\n",
      "Average test loss: 0.006344279565537969\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0018211318025779394\n",
      "Average test loss: 2959.7524497898066\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0018006154584387939\n",
      "Average test loss: 6952.937076052698\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0017540537917779553\n",
      "Average test loss: 21210.771801400002\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001716450500095056\n",
      "Average test loss: 135.1484012687732\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0017108461684029964\n",
      "Average test loss: 0.6076640271879733\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0017049979571666983\n",
      "Average test loss: 28.30131404595077\n",
      "Epoch 208/300\n",
      "Average training loss: 0.001726480790413916\n",
      "Average test loss: 210826.02663074917\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0016964807601438628\n",
      "Average test loss: 18.32024311545574\n",
      "Epoch 210/300\n",
      "Average training loss: 0.001696324041734139\n",
      "Average test loss: 0.003546959472613202\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0016878929901868104\n",
      "Average test loss: 0.0049639528279917106\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001681011480279267\n",
      "Average test loss: 0.0277229825767378\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0016858936575137907\n",
      "Average test loss: 5.105242684015177\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00167482046989931\n",
      "Average test loss: 0.13195286215800378\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0016902334391035968\n",
      "Average test loss: 220.0071279084931\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0016869718737693297\n",
      "Average test loss: 0.011212753559462727\n",
      "Epoch 218/300\n",
      "Average training loss: 0.001660760966853963\n",
      "Average test loss: 9.546978383954304\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0016816459403683742\n",
      "Average test loss: 0.049149664558056326\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0016637803388552532\n",
      "Average test loss: 8.647902876141584\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0016587902926322486\n",
      "Average test loss: 16.778255571468836\n",
      "Epoch 222/300\n",
      "Average training loss: 0.001690127194755607\n",
      "Average test loss: 10.110626030287085\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0016498690501062405\n",
      "Average test loss: 11412.722467673242\n",
      "Epoch 224/300\n",
      "Average training loss: 0.001642158420342538\n",
      "Average test loss: 265637.41710498574\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0016458146461389132\n",
      "Average test loss: 4529.996017920624\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001640586213933097\n",
      "Average test loss: 480.02031105017437\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0016341160101195175\n",
      "Average test loss: 2502.065721453733\n",
      "Epoch 229/300\n",
      "Average training loss: 0.001652349528339174\n",
      "Average test loss: 6545.703803863619\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0016334474700399572\n",
      "Average test loss: 56228.7796191135\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0016199601500605544\n",
      "Average test loss: 796.7801856951846\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0016387677197862002\n",
      "Average test loss: 577.7194093385074\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0016315106494973104\n",
      "Average test loss: 1748.5303560098425\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0016258508553728462\n",
      "Average test loss: 1318.7933849399756\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0016155949821695685\n",
      "Average test loss: 259.43878346264654\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0016287598840685354\n",
      "Average test loss: 761.650801503184\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001623828472673065\n",
      "Average test loss: 0.10270056642079727\n",
      "Epoch 238/300\n",
      "Average training loss: 0.001613287548845013\n",
      "Average test loss: 287.0640158345155\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0016927502350881697\n",
      "Average test loss: 142.70591642967943\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0016004528736488686\n",
      "Average test loss: 204.97338356608844\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0015932218100254735\n",
      "Average test loss: 0.6757513995860807\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0015959864597664111\n",
      "Average test loss: 799.4586804910948\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0016004105947083898\n",
      "Average test loss: 89.14873913683597\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0015982356383982632\n",
      "Average test loss: 0.16860151451810573\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0016144998020802936\n",
      "Average test loss: 699.9935615101366\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0015897734162087242\n",
      "Average test loss: 11723.038066214756\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0015946992221805784\n",
      "Average test loss: 21714.880632572018\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0015956337480909294\n",
      "Average test loss: 147.40432055742087\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0015901126276908649\n",
      "Average test loss: 0.0339288684292179\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0016042695298997892\n",
      "Average test loss: 1094.7468966067675\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0015978533247899677\n",
      "Average test loss: 10.15241391012828\n",
      "Epoch 252/300\n",
      "Average training loss: 0.001582066602797972\n",
      "Average test loss: 287.72157550866615\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0015982995961482326\n",
      "Average test loss: 502.0451683822583\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0016168343322351575\n",
      "Average test loss: 11048.263570136345\n",
      "Epoch 255/300\n",
      "Average training loss: 0.001585869530836741\n",
      "Average test loss: 1.2504349576510074\n",
      "Epoch 256/300\n",
      "Average training loss: 0.00157534403167665\n",
      "Average test loss: 31.686283057192963\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0015869202438948884\n",
      "Average test loss: 2524.900414365452\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0015792907312926318\n",
      "Average test loss: 0.3823931412326379\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0016000618430682355\n",
      "Average test loss: 48213.32978672839\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0015909430123865604\n",
      "Average test loss: 714.7109228561529\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0015706270312269529\n",
      "Average test loss: 142.63408994743395\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00156661470932886\n",
      "Average test loss: 320790.78814002365\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0015664202896878124\n",
      "Average test loss: 2.970393716633113\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0015766091866211758\n",
      "Average test loss: 3969.115808290372\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0015667140584232078\n",
      "Average test loss: 4617614.709010035\n",
      "Epoch 266/300\n",
      "Average training loss: 0.001555152633227408\n",
      "Average test loss: 24625.23384991788\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0015667932822058597\n",
      "Average test loss: 402.02844592509007\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0015590640421335896\n",
      "Average test loss: 3479.6768140492727\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0015581143979604046\n",
      "Average test loss: 445.62619922197115\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0015613214095226593\n",
      "Average test loss: 42.78514567715033\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0015540853111694257\n",
      "Average test loss: 1426.9070993675482\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0015624355679998796\n",
      "Average test loss: 49.19712532210495\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0015499209224556882\n",
      "Average test loss: 11.329926747469438\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0015540126108874877\n",
      "Average test loss: 25619.111405422635\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0015384830162963933\n",
      "Average test loss: 4.017155044430246\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0015538957663294342\n",
      "Average test loss: 262.56229086185994\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0015409901134876742\n",
      "Average test loss: 18.95253271073827\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0015563516972793474\n",
      "Average test loss: 0.7566896234042942\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0015467233759247595\n",
      "Average test loss: 192.67365670117186\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0015412675879067845\n",
      "Average test loss: 7810.963155614439\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0015516619272530079\n",
      "Average test loss: 455.4239302685393\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0015368391575498713\n",
      "Average test loss: 0.45458943091001774\n",
      "Epoch 286/300\n",
      "Average training loss: 0.001553933985117409\n",
      "Average test loss: 247.16607036009617\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0015363989215758111\n",
      "Average test loss: 2.909265595791965\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0015188860423449013\n",
      "Average test loss: 96.88454133104419\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0015307983332830998\n",
      "Average test loss: 14107.35694927352\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0015437286097763313\n",
      "Average test loss: 65860.34384418264\n",
      "Epoch 291/300\n",
      "Average training loss: 0.001517548702450262\n",
      "Average test loss: 6390188.817681039\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0015494440822965568\n",
      "Average test loss: 183.73485245545095\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0015361208834995827\n",
      "Average test loss: 251259.18581988927\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0016369969960716036\n",
      "Average test loss: 1.342495716954685\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0015639459717397889\n",
      "Average test loss: 1348750.6178373226\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0015539554171264173\n",
      "Average test loss: 3.383928985890415\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0015439872251202663\n",
      "Average test loss: 175747.74022488834\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0015318556209612225\n",
      "Average test loss: 26434.275275094595\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0015661011715936992\n",
      "Average test loss: 50.666436076821554\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06487508797976706\n",
      "Average test loss: 2.248008514811595\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04020923884709676\n",
      "Average test loss: 0.14691820384562015\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02921056912673844\n",
      "Average test loss: 0.00955608009133074\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018860124957230356\n",
      "Average test loss: 25.68233285942384\n",
      "Epoch 7/300\n",
      "Average training loss: 0.015843457221984863\n",
      "Average test loss: 0.012831027017699347\n",
      "Epoch 8/300\n",
      "Average training loss: 0.013622195364700423\n",
      "Average test loss: 0.10181372791156172\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011998798454801242\n",
      "Average test loss: 0.3039285721083482\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010693746847410996\n",
      "Average test loss: 0.005309484195378092\n",
      "Epoch 11/300\n",
      "Average training loss: 0.009640970591041777\n",
      "Average test loss: 44.771974083811045\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00874910998923911\n",
      "Average test loss: 0.027851287756115197\n",
      "Epoch 13/300\n",
      "Average training loss: 0.008013403860645162\n",
      "Average test loss: 6.999463880168067\n",
      "Epoch 14/300\n",
      "Average training loss: 0.00738690064350764\n",
      "Average test loss: 0.17118829877239963\n",
      "Epoch 15/300\n",
      "Average training loss: 0.006808088810079628\n",
      "Average test loss: 0.004015313290473488\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00634760373706619\n",
      "Average test loss: 0.15583070486121708\n",
      "Epoch 17/300\n",
      "Average training loss: 0.005940953094512224\n",
      "Average test loss: 0.023185969864328703\n",
      "Epoch 18/300\n",
      "Average training loss: 0.005564531186388598\n",
      "Average test loss: 0.40495054717030793\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005243214223533869\n",
      "Average test loss: 0.007058651905920771\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004446135674085882\n",
      "Average test loss: 0.0821011498665644\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00422252143940164\n",
      "Average test loss: 0.003929733714502719\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004022891958968507\n",
      "Average test loss: 0.7456815290235811\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0038213471811678673\n",
      "Average test loss: 0.003119156142696738\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003646647324371669\n",
      "Average test loss: 0.3317992687092887\n",
      "Epoch 27/300\n",
      "Average training loss: 0.003498755083936784\n",
      "Average test loss: 0.0028455128078866336\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003349732699079646\n",
      "Average test loss: 0.0028630124766172633\n",
      "Epoch 29/300\n",
      "Average training loss: 0.003224406588305202\n",
      "Average test loss: 0.04517667238993777\n",
      "Epoch 30/300\n",
      "Average training loss: 0.003103681682712502\n",
      "Average test loss: 0.003750492459990912\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0029956543621503643\n",
      "Average test loss: 6.890176423641543\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002900876942401131\n",
      "Average test loss: 0.051850436875389684\n",
      "Epoch 33/300\n",
      "Average training loss: 0.002823499738135272\n",
      "Average test loss: 0.019897098498315446\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002734906621277332\n",
      "Average test loss: 26.918152732049013\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0026572844942824706\n",
      "Average test loss: 0.04352906134062343\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0025917967009461587\n",
      "Average test loss: 0.014624722296992937\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0024913197530226573\n",
      "Average test loss: 0.003536386343340079\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0024049002437127962\n",
      "Average test loss: 0.34381890215269395\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002343429373577237\n",
      "Average test loss: 35.19239386537671\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0023563807790891993\n",
      "Average test loss: 5.84595549735758\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0022385989173005025\n",
      "Average test loss: 1519.3027377387152\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002202976570878592\n",
      "Average test loss: 12.959067017022933\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002378166716028419\n",
      "Average test loss: 0.04320269088157349\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0021796484351572065\n",
      "Average test loss: 165.01369929416146\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002109555107179201\n",
      "Average test loss: 3.4545786123068796\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002115480502239532\n",
      "Average test loss: 1.6169601575993002\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002092622155737546\n",
      "Average test loss: 3.503333296227062\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0020268935562214917\n",
      "Average test loss: 2.0487619173785463\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0020106095378804537\n",
      "Average test loss: 2.0709447541936403\n",
      "Epoch 51/300\n",
      "Average training loss: 0.001973903014221125\n",
      "Average test loss: 204.39558427238572\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0025572801873915726\n",
      "Average test loss: 74.41363368252914\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0019626635427897176\n",
      "Average test loss: 38.67560071941631\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0019291941112735205\n",
      "Average test loss: 3.3028883523112165\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0019286882252328925\n",
      "Average test loss: 0.004632098125914733\n",
      "Epoch 57/300\n",
      "Average training loss: 0.001873081655241549\n",
      "Average test loss: 248.75996791717452\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0018879401083621713\n",
      "Average test loss: 11.877157176636159\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0018461253664766749\n",
      "Average test loss: 0.040497099024968015\n",
      "Epoch 60/300\n",
      "Average training loss: 0.004115569533573256\n",
      "Average test loss: 0.021292965222357046\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0028056882013463313\n",
      "Average test loss: 0.01654547406650252\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002263658136750261\n",
      "Average test loss: 0.04430812196940598\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0020701951866762503\n",
      "Average test loss: 0.0632884745316373\n",
      "Epoch 64/300\n",
      "Average training loss: 0.001965753403802713\n",
      "Average test loss: 3.0589938930699394\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0019135411367234257\n",
      "Average test loss: 0.9903979670337091\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0019099544544393817\n",
      "Average test loss: 0.007249829314959546\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0017774048210639094\n",
      "Average test loss: 745.9825195217157\n",
      "Epoch 69/300\n",
      "Average training loss: 0.001753651484950549\n",
      "Average test loss: 0.3289597394242883\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0017288974623920188\n",
      "Average test loss: 24.982202395800396\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0017023438910643259\n",
      "Average test loss: 66.56741058785303\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0016726954782174694\n",
      "Average test loss: 4463.866250094944\n",
      "Epoch 73/300\n",
      "Average training loss: 0.001784991155895922\n",
      "Average test loss: 0.005193256421221627\n",
      "Epoch 74/300\n",
      "Average training loss: 0.001671379588647849\n",
      "Average test loss: 0.02479315248131752\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0016347708779697618\n",
      "Average test loss: 137.62108368621517\n",
      "Epoch 76/300\n",
      "Average training loss: 0.001786497753320469\n",
      "Average test loss: 5.2786378633611735\n",
      "Epoch 77/300\n",
      "Average training loss: 0.001730396308315297\n",
      "Average test loss: 0.006380261299924718\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0017063538579063283\n",
      "Average test loss: 0.004047677491274145\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0015961303394287825\n",
      "Average test loss: 0.1306960959215131\n",
      "Epoch 80/300\n",
      "Average training loss: 0.001546517517314189\n",
      "Average test loss: 20.587200609289937\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0015512694476379288\n",
      "Average test loss: 0.10415810780382405\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0015546004495893915\n",
      "Average test loss: 0.029854431737835207\n",
      "Epoch 84/300\n",
      "Average training loss: 0.4544663328329722\n",
      "Average test loss: 7.9407935246941115\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004143144765247901\n",
      "Average test loss: 0.005030757076210446\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0031643606207023066\n",
      "Average test loss: 0.002433499293194877\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00280813374908434\n",
      "Average test loss: 0.00408568677968449\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0025978269173453253\n",
      "Average test loss: 0.0017977490945615701\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0024547216886033614\n",
      "Average test loss: 0.0021022542578478655\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0023398347418341374\n",
      "Average test loss: 0.0021994153559207917\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0022354808820204603\n",
      "Average test loss: 0.007220119079781903\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002150161674660113\n",
      "Average test loss: 0.01141995505367716\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0020722291972488163\n",
      "Average test loss: 0.13124495346016354\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0020003337428594627\n",
      "Average test loss: 0.0038289554005281794\n",
      "Epoch 95/300\n",
      "Average training loss: 0.001923591144900355\n",
      "Average test loss: 0.06005109003517363\n",
      "Epoch 96/300\n",
      "Average training loss: 0.001873988917718331\n",
      "Average test loss: 0.1739667631122801\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0018288737513745825\n",
      "Average test loss: 0.0015812847894719905\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0017879799421255788\n",
      "Average training loss: 0.0017244458476909333\n",
      "Average test loss: 0.03718363818909145\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0016965815640158123\n",
      "Average test loss: 0.0017690670009598965\n",
      "Epoch 102/300\n",
      "Average training loss: 0.001678158255190485\n",
      "Average test loss: 0.0018669092511344287\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0016532285653261675\n",
      "Average test loss: 0.004535901130487521\n",
      "Epoch 104/300\n",
      "Average training loss: 0.001662314284282426\n",
      "Average test loss: 0.24364569708125458\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0016241047783340844\n",
      "Average test loss: 1.3358487201606233\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0016025494514033199\n",
      "Average test loss: 17.89284571716769\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0016713482432274354\n",
      "Average test loss: 0.004730629317772885\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0017007280703013141\n",
      "Average test loss: 0.0967035792561041\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0016184024933932556\n",
      "Average test loss: 0.05206494469609525\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0015581166732849345\n",
      "Average test loss: 0.0019189769816067483\n",
      "Epoch 111/300\n",
      "Average training loss: 0.001551142621630182\n",
      "Average test loss: 0.9560544851658245\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0015923441497402058\n",
      "Average test loss: 0.013108161241643958\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0015288981961914235\n",
      "Average test loss: 0.003184765097167757\n",
      "Epoch 114/300\n",
      "Average training loss: 0.001544389260932803\n",
      "Average test loss: 7.267023730622397\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0015589072403187553\n",
      "Average test loss: 0.1049974679713034\n",
      "Epoch 116/300\n",
      "Average training loss: 0.001538198014928235\n",
      "Average test loss: 0.4309102696027193\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0015209724231519633\n",
      "Average test loss: 0.008368999893880553\n",
      "Epoch 118/300\n",
      "Average training loss: 0.001546066674714287\n",
      "Average test loss: 0.0025962920527284345\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0015738414777442813\n",
      "Average test loss: 0.052847501561459564\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0015862836178599132\n",
      "Average test loss: 0.0017254125446909004\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0015771340974089173\n",
      "Average test loss: 0.0014408781375322077\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0015956705034606987\n",
      "Average test loss: 0.03093175037908885\n",
      "Epoch 123/300\n",
      "Average training loss: 0.001578792601513366\n",
      "Average test loss: 0.016344206425464816\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0015497350290210711\n",
      "Average test loss: 0.008639607581723895\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0017209308724850415\n",
      "Average test loss: 5.589049665976316\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0015319655395837294\n",
      "Average test loss: 106.02655083706189\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0014771725123541224\n",
      "Average test loss: 0.135218424229417\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0014791187302519878\n",
      "Average test loss: 3617.6658151154725\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0014621539747652907\n",
      "Average test loss: 150.8814328512748\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0014980029014663564\n",
      "Average test loss: 0.7590925916015274\n",
      "Epoch 132/300\n",
      "Average training loss: 0.001474644829519093\n",
      "Average test loss: 1731.3185266796988\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0014640949696509374\n",
      "Average test loss: 0.0014876032479935223\n",
      "Epoch 134/300\n",
      "Average training loss: 0.001438901087269187\n",
      "Average test loss: 0.1422409690650594\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0014091235269895858\n",
      "Average test loss: 44.21753917868694\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0014086142391380337\n",
      "Average test loss: 0.25209324026956326\n",
      "Epoch 137/300\n",
      "Average training loss: 0.001403139376702408\n",
      "Average test loss: 2.083804294308544\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0013906070716265175\n",
      "Average test loss: 6.48546750273928\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0013921937859720653\n",
      "Average test loss: 69.1757062277249\n",
      "Epoch 140/300\n",
      "Average training loss: 0.003231516982221769\n",
      "Average test loss: 56.81681400435418\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0018454166098187366\n",
      "Average test loss: 9.412403892738848\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0016358272031777436\n",
      "Average test loss: 11736.002257925245\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0015333588663488627\n",
      "Average test loss: 0.9757021472988029\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00148075760435313\n",
      "Average test loss: 0.004728092649330696\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0014407361324669586\n",
      "Average test loss: 165.51299968426758\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0014319999395973153\n",
      "Average test loss: 3.3650158576062985\n",
      "Epoch 147/300\n",
      "Average training loss: 0.001381522489608162\n",
      "Average test loss: 1.8130618315115572\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0013884721557713218\n",
      "Average test loss: 0.6981658600928883\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0014453400223929849\n",
      "Average test loss: 0.006583542063418362\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0014031363332437144\n",
      "Average test loss: 0.10379198585367864\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0013666654298495914\n",
      "Average test loss: 21.41521247010016\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001377301885229018\n",
      "Average test loss: 0.009547986800989344\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0013529368749716215\n",
      "Average test loss: 0.02313578729579846\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0013670936461745037\n",
      "Average test loss: 11.53380647661092\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0013755696003014842\n",
      "Average test loss: 7.023252181541278\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0013774578409372932\n",
      "Average test loss: 175.44141405760496\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0013232283607228763\n",
      "Average test loss: 1.9842472854784379\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0013256451346808009\n",
      "Average test loss: 4.658982037298485\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0013594951992337075\n",
      "Average test loss: 0.13999398286526815\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0013079230599105358\n",
      "Average test loss: 2320.0258675621662\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0013057294244257112\n",
      "Average test loss: 4036.4290093163268\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0013047785974211162\n",
      "Average test loss: 80012.69803844318\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0013094466397952703\n",
      "Average test loss: 0.6701833697218034\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0012842951101354426\n",
      "Average test loss: 0.8283123350375229\n",
      "Epoch 167/300\n",
      "Average training loss: 0.001272117702393896\n",
      "Average test loss: 3.5705959053954315\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0013190345904893345\n",
      "Average test loss: 6.866257482075857\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0012888708911422226\n",
      "Average test loss: 27.151456929976327\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0013162591194527016\n",
      "Average test loss: 7.823092464042206\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0012835008034793038\n",
      "Average test loss: 9.770602918199884\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0013604718765450848\n",
      "Average test loss: 1121.4382612249724\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0013124616583809257\n",
      "Average test loss: 0.5355056542762452\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0012487342621510227\n",
      "Average test loss: 0.04875545764093598\n",
      "Epoch 175/300\n",
      "Average training loss: 0.001279115466400981\n",
      "Average test loss: 61.06283140648736\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0012422179225832224\n",
      "Average test loss: 201.32222040207063\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0012384651505077879\n",
      "Average test loss: 0.2821748365689483\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0012574294740334154\n",
      "Average test loss: 0.0017504628093706236\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0013030941577421293\n",
      "Average test loss: 483.253842942771\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0012521012445083923\n",
      "Average test loss: 7994.876847800021\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0012345615087283983\n",
      "Average test loss: 5.6476947138019735\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0012153397468953498\n",
      "Average test loss: 0.011808458421689769\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0012013521457297934\n",
      "Average test loss: 233.42881225797493\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0012056644528897272\n",
      "Average test loss: 1413.3311298180852\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0011956257696987855\n",
      "Average test loss: 231.95868721743736\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0011873294431198802\n",
      "Average test loss: 8215.942033711812\n",
      "Epoch 187/300\n",
      "Average training loss: 0.001192625923289193\n",
      "Average test loss: 969.3532454004019\n",
      "Epoch 188/300\n",
      "Average training loss: 0.001204073233426445\n",
      "Average test loss: 4.109807809739063\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0013973237822970583\n",
      "Average test loss: 120.8358748497102\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0012637645828848085\n",
      "Average test loss: 15887.327442303022\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0012271041538980273\n",
      "Average test loss: 580.3555888370979\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001212953820100261\n",
      "Average test loss: 15676.527268771002\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0012357013469768896\n",
      "Average test loss: 31265.829187270007\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0012089645993999308\n",
      "Average test loss: 1.6075832374655745\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0011642881999723612\n",
      "Average test loss: 1.197958098692354\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0011642005919582313\n",
      "Average test loss: 4416.08402734375\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00117581919213343\n",
      "Average test loss: 0.014860237581758863\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0011793515385232038\n",
      "Average test loss: 397.91904301370306\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0011732709483347006\n",
      "Average test loss: 65014.943473221276\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0011698251662568913\n",
      "Average test loss: 19.071387713277506\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0014626283735657733\n",
      "Average test loss: 0.0081875539848374\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0012250414244416687\n",
      "Average test loss: 154.4259520773151\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0011686311008719107\n",
      "Average test loss: 0.14440311230946745\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0011546297301020887\n",
      "Average test loss: 4229.502709926786\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0011429410935896967\n",
      "Average test loss: 3.777881703400881\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0012038189153083497\n",
      "Average test loss: 715.3176058254829\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0011546207306285699\n",
      "Average test loss: 0.7012923191682332\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0011579778809100388\n",
      "Average test loss: 6516.423557589972\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0011400470814874602\n",
      "Average test loss: 45.21468535522786\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0011344777959036744\n",
      "Average test loss: 1.039972830955353\n",
      "Epoch 211/300\n",
      "Average training loss: 0.001135243183726238\n",
      "Average test loss: 29.21885142860665\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0011479375125943788\n",
      "Average test loss: 0.004206153631210327\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0011477867625653744\n",
      "Average test loss: 2751.5200578342015\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0011295056604883737\n",
      "Average test loss: 27.299985689386208\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0012029427777354915\n",
      "Average test loss: 1.100168522241215\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0011387273685799705\n",
      "Average test loss: 24.518868590248957\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0011217302224702304\n",
      "Average test loss: 136.22036481298755\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0011170663759112358\n",
      "Average test loss: 8.62695460216577\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0011112787153364884\n",
      "Average test loss: 213.34317316080507\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0011095444184934927\n",
      "Average test loss: 1.6089040011653883\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0011093543379878004\n",
      "Average test loss: 375.000642082977\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0011123714471339351\n",
      "Average test loss: 75031.82418619675\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0011019995474360056\n",
      "Average test loss: 15.76101395562581\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0011010349362881647\n",
      "Average test loss: 0.8504459063361088\n",
      "Epoch 225/300\n",
      "Average training loss: 0.001115191837410546\n",
      "Average test loss: 1860.0801368047553\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0010948010490068958\n",
      "Average test loss: 350.65705820150384\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0010949244832413065\n",
      "Average test loss: 4265.073590450888\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0011413715573855572\n",
      "Average test loss: 5.563862579147435\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0010850279607499639\n",
      "Average test loss: 1.4217484196423449\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0011017331890761852\n",
      "Average test loss: 0.0017771032452583314\n",
      "Epoch 231/300\n",
      "Average training loss: 0.001087682597546114\n",
      "Average test loss: 210.623664148964\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0010821741281284226\n",
      "Average test loss: 0.19708582459348772\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0010766991903591488\n",
      "Average test loss: 1862.0439607484457\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0012099831821396948\n",
      "Average test loss: 1391.5756383217706\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0011320666626302733\n",
      "Average test loss: 6467.338396531847\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001200417776281635\n",
      "Average test loss: 12.875879673594817\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0012101276463104618\n",
      "Average test loss: 5273.342478700498\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0012028779475949706\n",
      "Average test loss: 1.6865622571102448\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0011710167072402934\n",
      "Average test loss: 0.43251332873147397\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0011193794237139324\n",
      "Average test loss: 22.55272239324906\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0011818617107346653\n",
      "Average test loss: 402061.85006536334\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0011230038831838303\n",
      "Average test loss: 99.94454570345725\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0010849204048928288\n",
      "Average test loss: 179.38360042419575\n",
      "Epoch 244/300\n",
      "Average training loss: 0.001090837452871104\n",
      "Average test loss: 2379.513591062677\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0010928158266469835\n",
      "Average test loss: 0.006393632081233793\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0012130795896260274\n",
      "Average test loss: 34.28156157939757\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0011442891144090228\n",
      "Average test loss: 86.18404495511204\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0011196250343281362\n",
      "Average test loss: 0.00138268745980329\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0011264205880256162\n",
      "Average test loss: 0.0014645352504319615\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0011292601447138522\n",
      "Average test loss: 10.251315570065545\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0011079553927605351\n",
      "Average test loss: 0.2524233775422391\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0010857668809799684\n",
      "Average test loss: 0.48139680591225625\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0011082956839559806\n",
      "Average test loss: 48.82419308549166\n",
      "Epoch 254/300\n",
      "Average training loss: 0.001073443448688421\n",
      "Average test loss: 5400.240473036024\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0011502585681155324\n",
      "Average test loss: 568.8174126541506\n",
      "Epoch 256/300\n",
      "Average training loss: 0.001099624221213162\n",
      "Average test loss: 0.0015423374129459261\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0010871012454024618\n",
      "Average test loss: 0.2870100243619333\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0010664897417235705\n",
      "Average test loss: 0.00828896761375169\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0010577674239046044\n",
      "Average test loss: 297.4650034044128\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0010551636279043223\n",
      "Average test loss: 642.6502914362782\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0010547663657408622\n",
      "Average test loss: 8.644956702427317\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0010527459016690653\n",
      "Average test loss: 113.6655994869691\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0010781200066105358\n",
      "Average test loss: 28.72389865122156\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0010604987928850783\n",
      "Average test loss: 20.160808004150375\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0010555548392650153\n",
      "Average test loss: 130.43145894123325\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0010450960774388578\n",
      "Average test loss: 0.13315318092326117\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0010495179808284673\n",
      "Average test loss: 850.3644654279144\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0011023539386482701\n",
      "Average test loss: 520.1862462218296\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0010724882176145912\n",
      "Average test loss: 154.5665896709168\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0010493416266205411\n",
      "Average test loss: 101.51453473968225\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0010417840922665265\n",
      "Average test loss: 1193.0412777861811\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0010421805205858416\n",
      "Average test loss: 155.20741606086906\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0010543188237999049\n",
      "Average test loss: 2727.1706748498086\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0010362546724370784\n",
      "Average test loss: 0.042508195728477506\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0010334370400135716\n",
      "Average test loss: 41508.94158680556\n",
      "Epoch 276/300\n",
      "Average training loss: 0.001028304612264037\n",
      "Average test loss: 1866.2946281411755\n",
      "Epoch 277/300\n",
      "Average training loss: 0.001028278322166039\n",
      "Average test loss: 734222.4375106128\n",
      "Epoch 278/300\n",
      "Average training loss: 0.001030645936831004\n",
      "Average test loss: 2181.178174489797\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0010256601677586636\n",
      "Average test loss: 4173.66078662968\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0010267048282548786\n",
      "Average test loss: 354.9846146099733\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0010218289469161796\n",
      "Average test loss: 9563.696400972536\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001020650521800336\n",
      "Average test loss: 2637229.584485176\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0010857629280330407\n",
      "Average test loss: 31.710360021767826\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0010946285922287238\n",
      "Average test loss: 2473.2934317016725\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0010515149535818232\n",
      "Average test loss: 16.678115497313854\n",
      "Epoch 286/300\n",
      "Average training loss: 0.001045369887051897\n",
      "Average test loss: 3.1728422161147205\n",
      "Epoch 287/300\n",
      "Average training loss: 0.001069742632822858\n",
      "Average test loss: 583.9868095265396\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0010910802367143334\n",
      "Average test loss: 0.6433681033326624\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0010374582617336677\n",
      "Average test loss: 0.07528328849934042\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0010404590720103847\n",
      "Average test loss: 400.5572977454886\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0010169493742287158\n",
      "Average test loss: 32.088200774444374\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0010315281432121991\n",
      "Average test loss: 420.43637183858556\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0012287445620944102\n",
      "Average test loss: 0.10592652442471849\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0011296960208047595\n",
      "Average test loss: 704.1753171063633\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0013057632498546608\n",
      "Average test loss: 5.989024540105783\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00121406592708081\n",
      "Average test loss: 2.211982782937586\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0011009500341800352\n",
      "Average test loss: 8.475535484569354\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0010405549394587675\n",
      "Average test loss: 0.010223851124445597\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0010290009190017978\n",
      "Average test loss: 0.013164112739575407\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0010267327317140169\n",
      "Average test loss: 0.9872893623411655\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.25/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 4.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 3.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 7.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 9.05\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 5.96\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 9.35\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 8.84\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 11.05\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 12.27\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 12.45\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 17.63\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 13.88\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 20.38\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 9.04\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 17.40\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 22.31\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 22.93\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 23.26\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 7.04\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 18.66\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 22.35\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 20.75\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 23.37\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 15.81\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 23.90\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 22.72\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 25.23\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 0.57\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 1.40\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 0.73\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 0.24\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 1.13\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 4.26\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 6.83\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 8.83\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 11.45\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 10.76\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 13.47\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 15.96\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 7.98\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 18.09\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 20.76\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 21.31\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 16.18\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 23.49\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 22.40\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 13.45\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 20.60\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 21.13\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 25.13\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 19.57\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.46\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.58\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 6.45\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 8.05\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 6.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 1.71\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -0.58\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -1.51\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -1.01\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 2.79\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 6.38\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 8.59\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 10.38\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 10.38\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 12.65\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 15.79\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 18.68\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 22.62\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 14.58\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 21.56\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 24.36\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.21\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 14.74\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 22.19\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 25.56\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 25.00\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.58\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.85\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: -4.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -12.48\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -11.85\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -11.15\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -9.61\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -9.46\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -4.92\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -2.46\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 0.28\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 6.31\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 12.54\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 15.71\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 17.94\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 12.30\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 17.86\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 21.45\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 23.67\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.20\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 22.89\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.62\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 18.97\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 24.58\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.23\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.78\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.46\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
