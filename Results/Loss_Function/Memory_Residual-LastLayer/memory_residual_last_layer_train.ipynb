{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import LastLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Last Layer Loss\n",
    "loss_function = LastLayerLoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.03004941754043102\n",
      "Average test loss: 0.015506189295815096\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011308604708148373\n",
      "Average test loss: 0.010224835507157776\n",
      "Epoch 3/300\n",
      "Average training loss: 0.010054465064571963\n",
      "Average test loss: 0.011815867433117496\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00931713602029615\n",
      "Average test loss: 0.008908786571688122\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008869236811995506\n",
      "Average test loss: 0.009414727095928457\n",
      "Epoch 6/300\n",
      "Average training loss: 0.008534770578559902\n",
      "Average test loss: 0.008704908182223638\n",
      "Epoch 7/300\n",
      "Average training loss: 0.008145656006203757\n",
      "Average test loss: 0.008196017331547208\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007975266017019748\n",
      "Average test loss: 0.008011694773617718\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007722648240625858\n",
      "Average test loss: 0.007921565967301527\n",
      "Epoch 10/300\n",
      "Average training loss: 0.00755465510321988\n",
      "Average test loss: 0.0076764106841550935\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007386086344718933\n",
      "Average test loss: 0.0074515784084796905\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007218892902963691\n",
      "Average test loss: 0.007306126983629333\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007042107319252359\n",
      "Average test loss: 0.007181568483925528\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006916957241379552\n",
      "Average test loss: 0.007163263576312197\n",
      "Epoch 15/300\n",
      "Average training loss: 0.006771586065904962\n",
      "Average test loss: 0.007361106669737233\n",
      "Epoch 16/300\n",
      "Average training loss: 0.006670056212279531\n",
      "Average test loss: 0.007451428020166026\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00658042828241984\n",
      "Average test loss: 0.006882843833002779\n",
      "Epoch 18/300\n",
      "Average training loss: 0.006483848696367608\n",
      "Average test loss: 0.006810830596834421\n",
      "Epoch 19/300\n",
      "Average training loss: 0.006397575212021668\n",
      "Average test loss: 0.006653240166604519\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006335134443723493\n",
      "Average test loss: 0.007306215612838666\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00626809574249718\n",
      "Average test loss: 0.0065652584239012666\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0062066526143915125\n",
      "Average test loss: 0.006567495003756549\n",
      "Epoch 23/300\n",
      "Average training loss: 0.006161962969849507\n",
      "Average test loss: 0.006599302804304494\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006082795647697316\n",
      "Average test loss: 0.006455633894436889\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00604096142657929\n",
      "Average test loss: 0.0066754743680357935\n",
      "Epoch 26/300\n",
      "Average training loss: 0.005983999600427018\n",
      "Average test loss: 0.0064696537521150375\n",
      "Epoch 27/300\n",
      "Average training loss: 0.005962133996188641\n",
      "Average test loss: 0.0064327058957682716\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0058942506859699885\n",
      "Average test loss: 0.006434743306703038\n",
      "Epoch 29/300\n",
      "Average training loss: 0.005853709132307106\n",
      "Average test loss: 0.006669450571139653\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005832550668881999\n",
      "Average test loss: 0.0065564426100916335\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0057700531490974954\n",
      "Average test loss: 0.006439430233505037\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005739067247344388\n",
      "Average test loss: 0.00639496807836824\n",
      "Epoch 33/300\n",
      "Average training loss: 0.005695574793136782\n",
      "Average test loss: 0.006389843257764975\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005661489900201559\n",
      "Average test loss: 0.006629749844885535\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0056332252733409405\n",
      "Average test loss: 0.00668087049242523\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005608053975221183\n",
      "Average test loss: 0.006649765556057294\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005555092170834541\n",
      "Average test loss: 0.006485507090058591\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005530598762134711\n",
      "Average test loss: 0.0063880639817151755\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0055065405699941845\n",
      "Average test loss: 0.0065899859542648\n",
      "Epoch 40/300\n",
      "Average training loss: 0.005469887759950426\n",
      "Average test loss: 0.006522744459410508\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0054437028256555395\n",
      "Average test loss: 0.00637429772482978\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005416896989982989\n",
      "Average test loss: 0.006506932326489025\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005402086481038067\n",
      "Average test loss: 0.006302660153143936\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005366481362531582\n",
      "Average test loss: 0.0063823974670635325\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0053448426201939585\n",
      "Average test loss: 0.0064328382064898805\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005312798332836893\n",
      "Average test loss: 0.006567685067653656\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0053253881169690025\n",
      "Average test loss: 0.0068348012562427255\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005269454708529843\n",
      "Average test loss: 0.006385670636263159\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0052333822378681765\n",
      "Average test loss: 0.006446906339791086\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005233478793253502\n",
      "Average test loss: 0.0065548218049936826\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005226997293531895\n",
      "Average test loss: 0.006428049253506793\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005184086392737097\n",
      "Average test loss: 0.006594831580917041\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0051708500859224136\n",
      "Average test loss: 0.0063809164725244045\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005143215090864234\n",
      "Average test loss: 0.00644931769495209\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005126321196970013\n",
      "Average test loss: 0.006717849400308397\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005126991785234875\n",
      "Average test loss: 0.0067106570638716225\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005094540462311771\n",
      "Average test loss: 0.006373716895778974\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005074833308243089\n",
      "Average test loss: 0.006557965560712748\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0050583014115691185\n",
      "Average test loss: 0.006653769900815354\n",
      "Epoch 60/300\n",
      "Average training loss: 0.005049517006095913\n",
      "Average test loss: 0.006520318989952405\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005032867079600691\n",
      "Average test loss: 0.006365901176714235\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005018878114306264\n",
      "Average test loss: 0.0076940902057621215\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005014442966837022\n",
      "Average test loss: 0.006336203620251682\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004982605800032616\n",
      "Average test loss: 0.006290769450780418\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004986945692035887\n",
      "Average test loss: 0.006661167158848709\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004962065347780784\n",
      "Average test loss: 0.00646567734496461\n",
      "Epoch 67/300\n",
      "Average training loss: 0.004940600816574362\n",
      "Average test loss: 0.0064299322234259715\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004929428546792931\n",
      "Average test loss: 0.006683281364540259\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004920472602463431\n",
      "Average test loss: 0.0064089390515453286\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004905764071477784\n",
      "Average test loss: 0.006376562858207358\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004892543204128743\n",
      "Average test loss: 0.006423747753517495\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004868855677958992\n",
      "Average test loss: 0.007344380990498596\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004875806326253547\n",
      "Average test loss: 0.006414741286800967\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004852036722004413\n",
      "Average test loss: 0.006341968506988552\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004844139052348004\n",
      "Average test loss: 0.006337047656791077\n",
      "Epoch 76/300\n",
      "Average training loss: 0.004837569478485319\n",
      "Average test loss: 0.006390512411379152\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004824916424022781\n",
      "Average test loss: 0.006346156291663647\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004816636617812845\n",
      "Average test loss: 0.006533184233432015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004812534178296725\n",
      "Average test loss: 0.006412728225605355\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004792536746710539\n",
      "Average test loss: 0.006446095625145568\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004778592839837074\n",
      "Average test loss: 0.007408105711142222\n",
      "Epoch 82/300\n",
      "Average training loss: 0.004780820350059205\n",
      "Average test loss: 0.006860628091626697\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004763930927962064\n",
      "Average test loss: 0.006357340472439925\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004765527591937118\n",
      "Average test loss: 0.006382902423540751\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004748922946966357\n",
      "Average test loss: 0.006327760578857528\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004747564480122593\n",
      "Average test loss: 0.00645459536752767\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004718495262579786\n",
      "Average test loss: 0.006503056281970607\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004722362181792656\n",
      "Average test loss: 0.006893645661986536\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00471461811185711\n",
      "Average test loss: 0.006632953834616475\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004704151964849896\n",
      "Average test loss: 0.006368928377827009\n",
      "Epoch 91/300\n",
      "Average training loss: 0.004696096183525191\n",
      "Average test loss: 0.0063794800514976185\n",
      "Epoch 92/300\n",
      "Average training loss: 0.00467453558743\n",
      "Average test loss: 0.006567707769572734\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00467697918953167\n",
      "Average test loss: 0.006503096411625544\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004667377906541029\n",
      "Average test loss: 0.0065025267452001575\n",
      "Epoch 95/300\n",
      "Average training loss: 0.004657913710715042\n",
      "Average test loss: 0.006463527313950989\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004652804341581132\n",
      "Average test loss: 0.006327540917942922\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004640064576847686\n",
      "Average test loss: 0.006549692886985011\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004662148548083173\n",
      "Average test loss: 0.00641789871495631\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004612269792084893\n",
      "Average test loss: 0.006414725286679136\n",
      "Epoch 100/300\n",
      "Average training loss: 0.004619652530178427\n",
      "Average test loss: 0.006515092322395908\n",
      "Epoch 101/300\n",
      "Average training loss: 0.004607224004343152\n",
      "Average test loss: 0.0063536168883244195\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004606355735411247\n",
      "Average test loss: 0.006615296633707152\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004599812198223339\n",
      "Average test loss: 0.006394023615039057\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004587727413409286\n",
      "Average test loss: 0.006291333268913958\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004583823197417789\n",
      "Average test loss: 0.006401300855808788\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004623495308682323\n",
      "Average test loss: 0.006564071170985699\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0045627398552993935\n",
      "Average test loss: 0.006426949424462186\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004556171850197845\n",
      "Average test loss: 0.006649208988580439\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0045543402392003276\n",
      "Average test loss: 0.006347959635572301\n",
      "Epoch 110/300\n",
      "Average training loss: 0.004609379255523284\n",
      "Average test loss: 0.0064224514588713645\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0045451052110228275\n",
      "Average test loss: 0.006360417229433854\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004527752822058069\n",
      "Average test loss: 0.00648392860384451\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0045496559091326265\n",
      "Average test loss: 0.006455406753139364\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004513881562070714\n",
      "Average test loss: 0.006565283834520314\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004520766242096822\n",
      "Average test loss: 0.006469470838291778\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004516038147111733\n",
      "Average test loss: 0.006542876058982478\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004506892865937617\n",
      "Average test loss: 0.006537001127584113\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00453097504687806\n",
      "Average test loss: 0.007726955975509352\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0045322019180489915\n",
      "Average test loss: 0.006501807793974876\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0044805401542948355\n",
      "Average test loss: 0.006478634629398584\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004483090500864718\n",
      "Average test loss: 0.0064777645696368485\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0044721497425602545\n",
      "Average test loss: 0.006525243064595594\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004466680501484209\n",
      "Average test loss: 0.006647880285564396\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0044665398713615205\n",
      "Average test loss: 0.006531743250787258\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004464142575446102\n",
      "Average test loss: 0.0064339541250632865\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004462355941534042\n",
      "Average test loss: 0.00653704139466087\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004472169739090734\n",
      "Average test loss: 0.006533865313149161\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0044417203619248335\n",
      "Average test loss: 0.006626916907727719\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004445904809774624\n",
      "Average test loss: 0.006465583639426364\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004461355283442471\n",
      "Average test loss: 0.006416413953320848\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004442353362424506\n",
      "Average test loss: 0.006490063796854681\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004421259688006507\n",
      "Average test loss: 0.006451220980534951\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004432417849699656\n",
      "Average test loss: 0.0066717332038614486\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004418367111020618\n",
      "Average test loss: 0.006352786611351702\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004422724438417289\n",
      "Average test loss: 0.006534178429179721\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004405383145850566\n",
      "Average test loss: 0.0067214317309359705\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004400375154697233\n",
      "Average test loss: 0.0065096766433368125\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0044151107395688696\n",
      "Average test loss: 0.006703872136357758\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004391380598354671\n",
      "Average test loss: 0.006523660589423445\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004392483873499765\n",
      "Average test loss: 0.006633134559210804\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004407216503388352\n",
      "Average test loss: 0.0065024172473284935\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004380753177735541\n",
      "Average test loss: 0.0065762929092678755\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00437262895454963\n",
      "Average test loss: 0.006799403239455488\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00438006148652898\n",
      "Average test loss: 0.006478262039936251\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004367936132682694\n",
      "Average test loss: 0.006660338327288628\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004366098124947813\n",
      "Average test loss: 0.006750393509864807\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004406003358877367\n",
      "Average test loss: 0.006362422054012617\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004352713595454891\n",
      "Average test loss: 0.0065742382142278885\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004337516030089723\n",
      "Average test loss: 0.012050710605250465\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004362500290075938\n",
      "Average test loss: 0.0065298512660794785\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004351817324136694\n",
      "Average test loss: 0.00643350023859077\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0043325975702868565\n",
      "Average test loss: 0.006543928530481127\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004327616886132294\n",
      "Average test loss: 0.0066689147353172305\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0043461753183768855\n",
      "Average test loss: 0.006589905391136805\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004325869834257497\n",
      "Average test loss: 0.006500618611358934\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004322833296325472\n",
      "Average test loss: 0.006512992800523838\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004319302244732777\n",
      "Average test loss: 0.006580147964258988\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004320431383740571\n",
      "Average test loss: 0.006510194095472495\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004310533901883496\n",
      "Average test loss: 0.00659477616680993\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004306680036294791\n",
      "Average test loss: 0.006558234319918686\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004301891869968838\n",
      "Average test loss: 0.006970009317000707\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004299201482699977\n",
      "Average test loss: 0.006592015242824952\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0042959553889102406\n",
      "Average test loss: 0.006471375472843647\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004307128267569674\n",
      "Average test loss: 0.006866654529339738\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004291036187981566\n",
      "Average test loss: 0.006406682538075579\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004274400564945407\n",
      "Average test loss: 0.006525217189970943\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004278832058111827\n",
      "Average test loss: 0.0066212770053082045\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0042958269996775525\n",
      "Average test loss: 0.006574708106617133\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004265817485749722\n",
      "Average test loss: 0.006620930028872357\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004272029027963678\n",
      "Average test loss: 0.006644606917682621\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004275381345715788\n",
      "Average test loss: 0.006665541799532043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004272923149789373\n",
      "Average test loss: 0.006505645265595781\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004268329623258776\n",
      "Average test loss: 0.006691640358418226\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004279172671337922\n",
      "Average test loss: 0.006606323811949955\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004251486000915368\n",
      "Average test loss: 0.006601608640617794\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004245423572758833\n",
      "Average test loss: 0.006843067596356074\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0042358366424838706\n",
      "Average test loss: 0.006748008421725697\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004244301738217473\n",
      "Average test loss: 0.006739769244359599\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004251866147749954\n",
      "Average test loss: 0.00653102104117473\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0042293604262587095\n",
      "Average test loss: 0.006613589883678489\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00423590656535493\n",
      "Average test loss: 0.006520600245230728\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004227383138404952\n",
      "Average test loss: 0.006704908931420909\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004271853721804089\n",
      "Average test loss: 0.00652313171637555\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0042218448176152175\n",
      "Average test loss: 0.006746119230985641\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004221448697149754\n",
      "Average test loss: 0.006597116192181905\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004219191516646081\n",
      "Average test loss: 0.006636750062720643\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004205032514822152\n",
      "Average test loss: 0.00659139880620771\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004236171904123492\n",
      "Average test loss: 0.006571450582808919\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004206768553290103\n",
      "Average test loss: 0.006670624601344268\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0041950629920595225\n",
      "Average test loss: 0.006744235429498885\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004204708563784758\n",
      "Average test loss: 0.006573326913846864\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004196881909337309\n",
      "Average test loss: 0.006497987207439211\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004191445796233084\n",
      "Average test loss: 0.006665599624315898\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004191531359942423\n",
      "Average test loss: 0.006680362860361735\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004197805163347059\n",
      "Average test loss: 0.006505560878664255\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004184407077315781\n",
      "Average test loss: 0.0065336280034648045\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004186710886243317\n",
      "Average test loss: 0.006733157938553227\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0041876660558498565\n",
      "Average test loss: 0.006816639824046029\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004190742017701268\n",
      "Average test loss: 0.0067943248955739865\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004192052281772097\n",
      "Average test loss: 0.006627813362412982\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004179098755121231\n",
      "Average test loss: 0.006526251245290041\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00416046120143599\n",
      "Average test loss: 0.006536797787166304\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004165625239205029\n",
      "Average test loss: 0.006608837130582995\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004179228082299233\n",
      "Average test loss: 0.006632948822031418\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004170287723342578\n",
      "Average test loss: 0.006587901098860635\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004163637630227539\n",
      "Average test loss: 0.006556426175766521\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004154685123927063\n",
      "Average test loss: 0.006706809508303801\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004168005717297395\n",
      "Average test loss: 0.006567540312806765\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004224435711486472\n",
      "Average test loss: 0.006480266025496854\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0041424039010372424\n",
      "Average test loss: 0.006665097920431031\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00412929477252894\n",
      "Average test loss: 0.006819982544829448\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004131352091415061\n",
      "Average test loss: 0.0066567353374428215\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004142149209769235\n",
      "Average test loss: 0.006670933028062185\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004167987433986531\n",
      "Average test loss: 0.006566675751987431\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004130728344950411\n",
      "Average test loss: 0.006580182657059696\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004131473580582274\n",
      "Average test loss: 0.006709062128017346\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004126947928426994\n",
      "Average test loss: 0.006585080643081003\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004140715036127302\n",
      "Average test loss: 0.006594458698398537\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004123018655098147\n",
      "Average test loss: 0.006594425512684716\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004125428011847867\n",
      "Average test loss: 0.006662850802557336\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0041252193699280425\n",
      "Average test loss: 0.006643057351724969\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004113025347598725\n",
      "Average test loss: 0.006677104212550653\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00411741743526525\n",
      "Average test loss: 0.006677627284493711\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004500281616010599\n",
      "Average test loss: 0.006580613626374139\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004127061781369978\n",
      "Average test loss: 0.006563625769482719\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0040722048042549025\n",
      "Average test loss: 0.006821558074818717\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004083317196203602\n",
      "Average test loss: 0.006721332178761562\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004087079075889455\n",
      "Average test loss: 0.006953350436356333\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004093419873052173\n",
      "Average test loss: 0.0065117869915233715\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004106559344049957\n",
      "Average test loss: 0.0066146625938514865\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004131882779507173\n",
      "Average test loss: 0.006683668426755402\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004092547165850799\n",
      "Average test loss: 0.006678021843234698\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0041137884644170605\n",
      "Average test loss: 0.006532300644450717\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004087502829730511\n",
      "Average test loss: 0.00657888505483667\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004101176546265682\n",
      "Average test loss: 0.00657597086371647\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0040881808607114685\n",
      "Average test loss: 0.006528077865640323\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004090831742104557\n",
      "Average test loss: 0.008970604094366232\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0040936528373923566\n",
      "Average test loss: 0.006742018361886343\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0040800733615954715\n",
      "Average test loss: 0.006679503013690313\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004084677269061406\n",
      "Average test loss: 0.006604787836058272\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004071087925591402\n",
      "Average test loss: 0.006667556693156561\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004073047512314386\n",
      "Average test loss: 0.006534506929831372\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004075998693911565\n",
      "Average test loss: 0.006723172663400571\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004071223778029283\n",
      "Average test loss: 0.006765588196615378\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004106451876254545\n",
      "Average test loss: 0.006570976184060176\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004057004278111789\n",
      "Average test loss: 0.006662681610220008\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004061904923576448\n",
      "Average test loss: 0.00665002845807208\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004059845284869273\n",
      "Average test loss: 0.006687707942807012\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004057822969431678\n",
      "Average test loss: 0.006606251825475031\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004072038779656092\n",
      "Average test loss: 0.0067926788528760275\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004062201051869326\n",
      "Average test loss: 0.006659229500633147\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004100191188562248\n",
      "Average test loss: 0.006716033421870735\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004047210901147789\n",
      "Average test loss: 0.006635676381488641\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004048280381494098\n",
      "Average test loss: 0.006569132752716541\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004254189798608423\n",
      "Average test loss: 0.006576746096213659\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004052592534985807\n",
      "Average test loss: 0.006497074585821894\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004022225297159619\n",
      "Average test loss: 0.0067673182677891516\n",
      "Epoch 258/300\n",
      "Average training loss: 0.00402875040906171\n",
      "Average test loss: 0.0066817930311792425\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004021049723236097\n",
      "Average test loss: 0.006679421140915818\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004036396185970969\n",
      "Average test loss: 0.0077224085686935316\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0040555389647682506\n",
      "Average test loss: 0.007358002299649848\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004028728090847532\n",
      "Average test loss: 0.006661954293648402\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004037195191408197\n",
      "Average test loss: 0.006627993811335829\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004031604211777448\n",
      "Average test loss: 0.006943789238731067\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004029173792236381\n",
      "Average test loss: 0.006671867459184594\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004028668701648712\n",
      "Average test loss: 0.006614188225318988\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0040215525053855445\n",
      "Average test loss: 0.006636878015266525\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004031061600893736\n",
      "Average test loss: 0.006611282153262032\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004026829104042715\n",
      "Average test loss: 0.006606505376597246\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00401341717566053\n",
      "Average test loss: 0.006665363914850685\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004060989517304632\n",
      "Average test loss: 0.0069522877393497365\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004019385415853726\n",
      "Average test loss: 0.006750767161655757\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004007667903064026\n",
      "Average test loss: 0.006744915025101768\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004010155356799563\n",
      "Average test loss: 0.00657718250196841\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004003532656571931\n",
      "Average test loss: 0.006702073478864299\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0040016358780364195\n",
      "Average test loss: 0.006750610902077622\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004012311921765407\n",
      "Average test loss: 0.006653720273739762\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004014739599492815\n",
      "Average test loss: 0.006599871323340469\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004053945146708025\n",
      "Average test loss: 0.0069077819221549566\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004000581382049454\n",
      "Average test loss: 0.006610465744924214\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003985692026300563\n",
      "Average test loss: 0.006735395349148247\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00400073876314693\n",
      "Average test loss: 0.006716332619388898\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004001542108754317\n",
      "Average test loss: 0.006909454664008485\n",
      "Epoch 284/300\n",
      "Average training loss: 0.003993424754796757\n",
      "Average test loss: 0.006602550543016858\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0039889122098684315\n",
      "Average test loss: 0.00667912397119734\n",
      "Epoch 286/300\n",
      "Average training loss: 0.003991927023149199\n",
      "Average test loss: 0.006619322664621803\n",
      "Epoch 287/300\n",
      "Average training loss: 0.003997246590753396\n",
      "Average test loss: 0.006658153329458501\n",
      "Epoch 288/300\n",
      "Average training loss: 0.003986052607496579\n",
      "Average test loss: 0.0068114800196554925\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00398490545546843\n",
      "Average test loss: 0.006653551862471634\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003987955506684052\n",
      "Average test loss: 0.011345602944493294\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004017234299745824\n",
      "Average test loss: 0.006650173424432675\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003980085048824549\n",
      "Average test loss: 0.006818100339629584\n",
      "Epoch 293/300\n",
      "Average training loss: 0.003986116408473916\n",
      "Average test loss: 0.00706090472266078\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0039689148370590475\n",
      "Average test loss: 0.006660801761266258\n",
      "Epoch 295/300\n",
      "Average training loss: 0.003983601306668586\n",
      "Average test loss: 0.006687031552195549\n",
      "Epoch 296/300\n",
      "Average training loss: 0.003973243036617835\n",
      "Average test loss: 0.006756238641424312\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003972966051763958\n",
      "Average test loss: 0.010342059113913112\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003968385657916467\n",
      "Average test loss: 0.006716934408164687\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00396420174319711\n",
      "Average test loss: 0.006711791791435745\n",
      "Epoch 300/300\n",
      "Average training loss: 0.003961278854144944\n",
      "Average test loss: 0.006782934843252103\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02503531221797069\n",
      "Average test loss: 0.010382411677804258\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008000951567457781\n",
      "Average test loss: 0.008246525473064847\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007003653268847201\n",
      "Average test loss: 0.006915002867165539\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006422421197096507\n",
      "Average test loss: 0.006465557838893599\n",
      "Epoch 5/300\n",
      "Average training loss: 0.00602379895043042\n",
      "Average test loss: 0.00587641670058171\n",
      "Epoch 6/300\n",
      "Average training loss: 0.00570756060257554\n",
      "Average test loss: 0.0057500098989241655\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005409727720750703\n",
      "Average test loss: 0.005809876818623808\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005192654555870427\n",
      "Average test loss: 0.0054060241157809896\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0050397118346558675\n",
      "Average test loss: 0.004998333485590087\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0048617872970385684\n",
      "Average test loss: 0.004963061649766233\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004702472167296542\n",
      "Average test loss: 0.004743470590975549\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004588587975750367\n",
      "Average test loss: 0.004757688677559296\n",
      "Epoch 13/300\n",
      "Average training loss: 0.00445021591687368\n",
      "Average test loss: 0.004523810891641511\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0043661652145286405\n",
      "Average test loss: 0.004437695869968997\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004253330452160703\n",
      "Average test loss: 0.004438864799837272\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004179143256404333\n",
      "Average test loss: 0.004305120359692309\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0040830489860640634\n",
      "Average test loss: 0.004356905162334442\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00400964109868639\n",
      "Average test loss: 0.004218429255609711\n",
      "Epoch 19/300\n",
      "Average training loss: 0.003940036888130837\n",
      "Average test loss: 0.004383616897174054\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003881818476029568\n",
      "Average test loss: 0.0041789876793821654\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0038230722966707416\n",
      "Average test loss: 0.0040497634392231705\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0037710604303412966\n",
      "Average test loss: 0.003969608808557193\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0037269154934005603\n",
      "Average test loss: 0.003971244408024682\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0036670550987538365\n",
      "Average test loss: 0.003971718721919589\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0036222932698825997\n",
      "Average test loss: 0.003996514641576343\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0035859641881866586\n",
      "Average test loss: 0.0038552573836512036\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0035507746369888384\n",
      "Average test loss: 0.0039482397250831126\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0035103238007674615\n",
      "Average test loss: 0.0038753376859757636\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0034766644763035907\n",
      "Average test loss: 0.003850713826508986\n",
      "Epoch 30/300\n",
      "Average training loss: 0.00345337039584087\n",
      "Average test loss: 0.0038642936845620472\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0034218714773241016\n",
      "Average test loss: 0.0037724222886479563\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0033971246464384926\n",
      "Average test loss: 0.0038211401655442183\n",
      "Epoch 33/300\n",
      "Average training loss: 0.003356482872325513\n",
      "Average test loss: 0.003834987894528442\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0033555883100877208\n",
      "Average test loss: 0.003980048723104927\n",
      "Epoch 35/300\n",
      "Average training loss: 0.003317422893519203\n",
      "Average test loss: 0.003766892753334509\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0032988911798844736\n",
      "Average test loss: 0.003928994304604001\n",
      "Epoch 37/300\n",
      "Average training loss: 0.003270681694563892\n",
      "Average test loss: 0.00376424595196214\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0032519974029726452\n",
      "Average test loss: 0.003905668110276262\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0032384372390806674\n",
      "Average test loss: 0.0037769509522865216\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003215414812995328\n",
      "Average test loss: 0.0038943933389253087\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0032075363863259556\n",
      "Average test loss: 0.0037583428977264297\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0031891125546147426\n",
      "Average test loss: 0.003701929808904727\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003162732726169957\n",
      "Average test loss: 0.0037274004955672557\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0033367715533822776\n",
      "Average test loss: 0.0036702954380048647\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0031321316357288097\n",
      "Average test loss: 0.003777707941830158\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00311033576561345\n",
      "Average test loss: 0.0037048829247554142\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0031072250163803497\n",
      "Average test loss: 0.0037329872411986193\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003100314601014058\n",
      "Average test loss: 0.0037855898419188127\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0030878076807906232\n",
      "Average test loss: 0.0037210592083219023\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0030769102819677855\n",
      "Average test loss: 0.00375259352206356\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00443840338393218\n",
      "Average test loss: 0.00472192871156666\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004019541401623024\n",
      "Average test loss: 0.0038466635396083196\n",
      "Epoch 53/300\n",
      "Average training loss: 0.003626720575822724\n",
      "Average test loss: 0.0037019446616371474\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003466470135996739\n",
      "Average test loss: 0.00384874624096685\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0033462682689229645\n",
      "Average test loss: 0.0036523605932792026\n",
      "Epoch 56/300\n",
      "Average training loss: 0.003236950173974037\n",
      "Average test loss: 0.0037497819082604514\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0031424525164895586\n",
      "Average test loss: 0.003784535590145323\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0030787644622226554\n",
      "Average test loss: 0.0037565890894167954\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003055246901181009\n",
      "Average test loss: 0.003806238030600879\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003040573705194725\n",
      "Average test loss: 0.0036467823535203934\n",
      "Epoch 61/300\n",
      "Average training loss: 0.003034860130192505\n",
      "Average test loss: 0.0037331010467476313\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003033673161226842\n",
      "Average test loss: 0.0037311834941307705\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0030125007681134673\n",
      "Average test loss: 0.00379070429255565\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0031112683431969747\n",
      "Average test loss: 0.003781909484623207\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0030094226789143352\n",
      "Average test loss: 0.003684332630286614\n",
      "Epoch 66/300\n",
      "Average training loss: 0.003001891186874774\n",
      "Average test loss: 0.0036897717345919876\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002991700660644306\n",
      "Average test loss: 185.40677250162761\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005078617279314333\n",
      "Average test loss: 0.004044923053433498\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00369698354229331\n",
      "Average test loss: 0.004223391499784257\n",
      "Epoch 70/300\n",
      "Average training loss: 0.003458795757343372\n",
      "Average test loss: 0.00374283492565155\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0033159926539907854\n",
      "Average test loss: 0.0036742919919391472\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003196343191381958\n",
      "Average test loss: 0.003693708317147361\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003093380227478014\n",
      "Average test loss: 0.0036617643891109363\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0030219982790036333\n",
      "Average test loss: 0.0037304368851085504\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002977595865726471\n",
      "Average test loss: 0.003656720574738251\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0029585450815243853\n",
      "Average test loss: 0.0036633385761330526\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0029485030946218307\n",
      "Average test loss: 0.003694997044487132\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0029409762945854\n",
      "Average test loss: 0.0036365179247740244\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0029455945117192136\n",
      "Average test loss: 0.0037581693335539765\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002943527116647197\n",
      "Average test loss: 0.0037924100193712446\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00293639705164565\n",
      "Average test loss: 0.003739255327731371\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0029424652941524984\n",
      "Average test loss: 0.0037237925570872093\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002930536272418168\n",
      "Average test loss: 0.003662354909090532\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002925415357781781\n",
      "Average test loss: 0.0037320819190806813\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002904675541859534\n",
      "Average test loss: 0.0037274283824695483\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002902531588036153\n",
      "Average test loss: 0.003705134337147077\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0029309071383128564\n",
      "Average test loss: 0.003777412797841761\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0028759994092914794\n",
      "Average test loss: 0.0038387987626095615\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0028736164097984632\n",
      "Average test loss: 0.003766329398171769\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003007210735231638\n",
      "Average test loss: 0.0037569042951282527\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0028638332564797668\n",
      "Average test loss: 0.003676829749097427\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0028540191395829122\n",
      "Average test loss: 0.0037255215181244746\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002869448981144362\n",
      "Average test loss: 0.0037570554593371022\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0028494891139368215\n",
      "Average test loss: 0.003693332779324717\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002842746117255754\n",
      "Average test loss: 0.0037400015021363895\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0028357511781569983\n",
      "Average test loss: 0.004406600718489952\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002898490916730629\n",
      "Average test loss: 0.0037161434412830404\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0028148145843297245\n",
      "Average test loss: 0.0036346322345650857\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0028410800399465692\n",
      "Average test loss: 0.0037274516705009674\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0028058599986963803\n",
      "Average test loss: 0.0037476247631841237\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002808272892609239\n",
      "Average test loss: 0.003782479614019394\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002810487860503296\n",
      "Average test loss: 0.0037583330128755834\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0028078786354098057\n",
      "Average test loss: 0.003722288044169545\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0027945851176563235\n",
      "Average test loss: 0.0036672267975906532\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0027960290304488607\n",
      "Average test loss: 0.0037220095278074346\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0027792909178468916\n",
      "Average test loss: 0.0037810059674084186\n",
      "Epoch 107/300\n",
      "Average training loss: 0.00277556341389815\n",
      "Average test loss: 0.003691175623072518\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0027984404812256493\n",
      "Average test loss: 0.0036491329926583503\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002768602252834373\n",
      "Average test loss: 0.0037270839580645166\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0027628172741581995\n",
      "Average test loss: 0.003661632765291466\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002758050983771682\n",
      "Average test loss: 0.003771522005192108\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0027460541907283996\n",
      "Average test loss: 0.0037272366928971477\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0027624611800743473\n",
      "Average test loss: 0.003830751688530048\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0027393486632241145\n",
      "Average test loss: 0.003735823698962728\n",
      "Epoch 115/300\n",
      "Average training loss: 0.002775427874798576\n",
      "Average test loss: 0.003715238499144713\n",
      "Epoch 116/300\n",
      "Average training loss: 0.002731160648374094\n",
      "Average test loss: 0.003736884130992823\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002735567030393415\n",
      "Average test loss: 0.003850969043249885\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0027238582842465902\n",
      "Average test loss: 0.0037448243155247633\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0027267966226985057\n",
      "Average test loss: 0.0038130689702100223\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002763467234041956\n",
      "Average test loss: 0.0037213677312764857\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002723573373216722\n",
      "Average test loss: 0.003775537641512023\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0027157155219465494\n",
      "Average test loss: 0.003842182968639665\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002705262691196468\n",
      "Average test loss: 0.003883714632027679\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002769186481005616\n",
      "Average test loss: 0.003704186415920655\n",
      "Epoch 125/300\n",
      "Average training loss: 0.002686045352783468\n",
      "Average test loss: 0.0037300279926922586\n",
      "Epoch 126/300\n",
      "Average training loss: 0.002693835399010115\n",
      "Average test loss: 0.003731312696304586\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0026965921067943176\n",
      "Average test loss: 0.004071507489929596\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0026998328229205478\n",
      "Average test loss: 0.0037740360252145264\n",
      "Epoch 129/300\n",
      "Average training loss: 0.002680048590319024\n",
      "Average test loss: 0.0038012422213537823\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002686850746679637\n",
      "Average test loss: 0.0037677361166311633\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002675003159998192\n",
      "Average test loss: 0.0038642774331900808\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002678709118627012\n",
      "Average test loss: 0.0038380601637893253\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0026767682766334878\n",
      "Average test loss: 0.0038401870955195694\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0026681732899612853\n",
      "Average test loss: 0.0037541463654488325\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002659836469218135\n",
      "Average test loss: 0.0037789827821155387\n",
      "Epoch 136/300\n",
      "Average training loss: 0.002661904555848903\n",
      "Average test loss: 0.003916218262165785\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0026664371896121235\n",
      "Average test loss: 0.003713805709448126\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0026758816367429164\n",
      "Average test loss: 0.0037717459437747796\n",
      "Epoch 139/300\n",
      "Average training loss: 0.002672075702084435\n",
      "Average test loss: 0.004004534313248264\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00264693963713944\n",
      "Average test loss: 0.004024271017147435\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002640696112376948\n",
      "Average test loss: 0.003768648350197408\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002648506890775429\n",
      "Average test loss: 0.004177614832917849\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0026340055730607772\n",
      "Average test loss: 0.0037361582031266555\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0026394528386493523\n",
      "Average test loss: 0.00372321675469478\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002636632530225648\n",
      "Average test loss: 0.003825489069438643\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002629609323417147\n",
      "Average test loss: 0.0037645632456988097\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0026811020587467487\n",
      "Average test loss: 0.0037669916438559693\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0026461395629578167\n",
      "Average test loss: 0.003832682464685705\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0026213217224511833\n",
      "Average test loss: 0.0037877696580770944\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0026261366120436127\n",
      "Average test loss: 0.003855476351868775\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0026308574326750307\n",
      "Average test loss: 0.003748381591712435\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0026008439734578133\n",
      "Average test loss: 0.0037443962556620437\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0026151712704449892\n",
      "Average test loss: 0.00396944064895312\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002624672652118736\n",
      "Average test loss: 0.0038651272095739844\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002605864857426948\n",
      "Average test loss: 0.003846438547389375\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002604091666638851\n",
      "Average test loss: 0.003967750087794331\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002608969631087449\n",
      "Average test loss: 0.0037677848330802386\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0025977936656110816\n",
      "Average test loss: 0.00377026769436068\n",
      "Epoch 159/300\n",
      "Average training loss: 0.002595709966495633\n",
      "Average test loss: 0.0037624546647485758\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00259008672233257\n",
      "Average test loss: 0.00393545827228162\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0026001800381475026\n",
      "Average test loss: 0.003800967767006821\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0025810618177056315\n",
      "Average test loss: 0.003721239731957515\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0025857784187214242\n",
      "Average test loss: 0.0037995602490587367\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002590500652583109\n",
      "Average test loss: 0.003789329245686531\n",
      "Epoch 165/300\n",
      "Average training loss: 0.002581214643186993\n",
      "Average test loss: 0.0037933540375282367\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002584629951044917\n",
      "Average test loss: 0.003840342731525501\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0025772484271890587\n",
      "Average test loss: 0.0037986411590956977\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002574468668550253\n",
      "Average test loss: 0.0037619172295348512\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0025769591447379852\n",
      "Average test loss: 0.0038863534436871607\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0025796493287715645\n",
      "Average test loss: 0.0037564606968727376\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002576907048622767\n",
      "Average test loss: 0.0037422827122112114\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0026154063211547003\n",
      "Average test loss: 0.003920767707957162\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0025530454681979287\n",
      "Average test loss: 0.0038650944820708697\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0025620221065150365\n",
      "Average test loss: 0.003801080906142791\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0025639902776521113\n",
      "Average test loss: 0.003865149312135246\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002553160865894622\n",
      "Average test loss: 0.0037955898503876395\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0025538533054706124\n",
      "Average test loss: 0.0037847387173937427\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0025589807445390357\n",
      "Average test loss: 0.003864759504588114\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0025549876025567453\n",
      "Average test loss: 0.003913066428154707\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0025527421320892044\n",
      "Average test loss: 0.0038188723143604067\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0025484988335520028\n",
      "Average test loss: 0.0038004973787400457\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0025394548821366498\n",
      "Average test loss: 0.0037896110489964484\n",
      "Epoch 183/300\n",
      "Average training loss: 0.002550301303466161\n",
      "Average test loss: 0.003848928888224893\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0025423332438286807\n",
      "Average test loss: 0.0039052774844070275\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0025484723850256867\n",
      "Average test loss: 0.003821137706024779\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0025742996046319603\n",
      "Average test loss: 0.004003359045419428\n",
      "Epoch 187/300\n",
      "Average training loss: 0.002530791244780024\n",
      "Average test loss: 0.0038471697051492004\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002533942083724671\n",
      "Average test loss: 0.004097895638396343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0025255305148247217\n",
      "Average test loss: 0.0038103867876860833\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0025269807570924363\n",
      "Average test loss: 0.0037896567744513353\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0025426947853217524\n",
      "Average test loss: 0.0038477729596197606\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0025315584467930927\n",
      "Average test loss: 0.003780345349262158\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0025232331028415098\n",
      "Average test loss: 0.0038664456804593404\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00252556990687218\n",
      "Average test loss: 0.00386928091570735\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0025203411574992868\n",
      "Average test loss: 0.00390225499889089\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0025275467216140696\n",
      "Average test loss: 0.0038109514518744414\n",
      "Epoch 197/300\n",
      "Average training loss: 0.002516196980037623\n",
      "Average test loss: 0.0038901788567503293\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002520099823259645\n",
      "Average test loss: 0.003841927104112175\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002522571075086792\n",
      "Average test loss: 0.003894716155404846\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0025133957597944473\n",
      "Average test loss: 0.0037784087252285747\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0025127365994784567\n",
      "Average test loss: 0.003926158068080743\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0025081707862102324\n",
      "Average test loss: 0.0038379801228228543\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0025117808011256987\n",
      "Average test loss: 0.0038976933024823667\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0025220731006314356\n",
      "Average test loss: 0.003775907506959306\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0025067469485931925\n",
      "Average test loss: 0.003803713436341948\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0025086522085799113\n",
      "Average test loss: 0.003812731769763761\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0025031628807385764\n",
      "Average test loss: 0.0037962860823091533\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002500338022907575\n",
      "Average test loss: 0.004207815029140975\n",
      "Epoch 209/300\n",
      "Average training loss: 0.002497696104355984\n",
      "Average test loss: 0.0039215569628609556\n",
      "Epoch 210/300\n",
      "Average training loss: 0.002491882456259595\n",
      "Average test loss: 0.004314607028125061\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002600537070176668\n",
      "Average test loss: 0.003834955154193772\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0025320653553224273\n",
      "Average test loss: 0.0038002526354458596\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0024789428582621944\n",
      "Average test loss: 0.0038163918662402364\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0024833136366473304\n",
      "Average test loss: 0.0038834821718434494\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0024806913212976523\n",
      "Average test loss: 0.003951422159870465\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002486334096122947\n",
      "Average test loss: 0.003907061707228422\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0025020436403445073\n",
      "Average test loss: 0.003989881596010592\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00249467564581169\n",
      "Average test loss: 0.003925679293771585\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002478024578963717\n",
      "Average test loss: 0.004005874816742208\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0025091052365799746\n",
      "Average test loss: 0.003755839134463006\n",
      "Epoch 221/300\n",
      "Average training loss: 0.002477837754620446\n",
      "Average test loss: 0.0037763965742455587\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0024720922449810636\n",
      "Average test loss: 0.004027748463261459\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0024711794320286975\n",
      "Average test loss: 0.0038725242256704303\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0024771637953817844\n",
      "Average test loss: 0.003870089073975881\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002476806639176276\n",
      "Average test loss: 0.003789000326767564\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002508704056756364\n",
      "Average test loss: 86.92330951605902\n",
      "Epoch 227/300\n",
      "Average training loss: 0.005265977435227898\n",
      "Average test loss: 0.004015122535948952\n",
      "Epoch 228/300\n",
      "Average training loss: 0.003560166460979316\n",
      "Average test loss: 0.003760262605423729\n",
      "Epoch 229/300\n",
      "Average training loss: 0.003293003588087029\n",
      "Average test loss: 0.0036496141381147833\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0031126226909044714\n",
      "Average test loss: 0.003644757918185658\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0029541602888041073\n",
      "Average test loss: 0.003680320787347025\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002800562963510553\n",
      "Average test loss: 0.0036999770113163525\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0026566630638101035\n",
      "Average test loss: 0.003809016711595986\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0025482726773868006\n",
      "Average test loss: 0.0037262702265547384\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0024796236676888333\n",
      "Average test loss: 0.0038661497173209983\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002444507336243987\n",
      "Average test loss: 0.003862422947047485\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0024325501384834447\n",
      "Average test loss: 0.0038072652342832753\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0024273988670772974\n",
      "Average test loss: 0.003877972334002455\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002430653017014265\n",
      "Average test loss: 0.0038098743909762965\n",
      "Epoch 240/300\n",
      "Average training loss: 0.002444304786208603\n",
      "Average test loss: 0.004010313642107778\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0024674810680250328\n",
      "Average test loss: 0.0038603050390051473\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0024675934149159325\n",
      "Average test loss: 0.0038635261274046367\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002460680193785164\n",
      "Average test loss: 0.003792898404515452\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0024597822059359816\n",
      "Average test loss: 0.003919487110649546\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002455639123295744\n",
      "Average test loss: 0.0038747064361555708\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002463676765354143\n",
      "Average test loss: 0.0038235045826683443\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0024697419473280508\n",
      "Average test loss: 0.003990897308621142\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0024550038314320975\n",
      "Average test loss: 0.003932258523172802\n",
      "Epoch 249/300\n",
      "Average training loss: 0.002454687333251867\n",
      "Average test loss: 0.003896690025511715\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0028636277864376703\n",
      "Average test loss: 0.003878663429990411\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002464666315457887\n",
      "Average test loss: 0.0038940998907718394\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0024308618360923398\n",
      "Average test loss: 0.0038455909705824324\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0024324647407564853\n",
      "Average test loss: 0.0038723246498654287\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0024384474520468048\n",
      "Average test loss: 0.0038567959157129127\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002443055593719085\n",
      "Average test loss: 0.00386752096687754\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002441972191962931\n",
      "Average test loss: 0.003933181189828449\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002444985315617588\n",
      "Average test loss: 0.003912598545766539\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002442264767984549\n",
      "Average test loss: 0.003848795824787683\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0024558904161676763\n",
      "Average test loss: 0.003958243373781443\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0024432427320215436\n",
      "Average test loss: 0.0038206122676945394\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002461871056817472\n",
      "Average test loss: 0.003979784285856618\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00243970564690729\n",
      "Average test loss: 0.003957300254040294\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002433902731579211\n",
      "Average test loss: 0.0038749711840516993\n",
      "Epoch 264/300\n",
      "Average training loss: 0.002430798697595795\n",
      "Average test loss: 0.0038522683994637597\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0024400092303338977\n",
      "Average test loss: 0.0038649224680331017\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0024379036153356235\n",
      "Average test loss: 0.0037985999749766456\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0024330364047653145\n",
      "Average test loss: 0.0038420678195026188\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0024373982277595334\n",
      "Average test loss: 0.0038435643079380195\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002431815829748909\n",
      "Average test loss: 0.00393930437705583\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0024619277604959076\n",
      "Average test loss: 0.003902306797189845\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0024234443382463522\n",
      "Average test loss: 0.003910778472820918\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0025590443667024374\n",
      "Average test loss: 0.003742814912357264\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0025341454442176555\n",
      "Average test loss: 0.003840057393329011\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0024134744529922805\n",
      "Average test loss: 0.0038197170628441707\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0024086642116308214\n",
      "Average test loss: 0.0037759251794260407\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0024149945939166678\n",
      "Average test loss: 0.003950259110786849\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002419986917430328\n",
      "Average test loss: 0.003867652650301655\n",
      "Epoch 278/300\n",
      "Average training loss: 0.002415460438157121\n",
      "Average test loss: 0.003927303620510631\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002421609630601274\n",
      "Average test loss: 0.003871797467271487\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002427191282932957\n",
      "Average test loss: 0.0039030154301888413\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0024203136719556317\n",
      "Average test loss: 0.003971177337070306\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002417475604452193\n",
      "Average test loss: 0.003981752708140347\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002420508292400175\n",
      "Average test loss: 0.003938767340862089\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002411760390425722\n",
      "Average test loss: 0.004015813132334086\n",
      "Epoch 285/300\n",
      "Average training loss: 0.002413922678679228\n",
      "Average test loss: 0.004024097896491488\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0024221857941399017\n",
      "Average test loss: 0.0039045986582835514\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0024074873413062756\n",
      "Average test loss: 0.004384587891399861\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0024123406908992265\n",
      "Average test loss: 0.0038478551575293145\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002411546823465162\n",
      "Average test loss: 0.004006974043945471\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0024123963924745718\n",
      "Average test loss: 0.0040281044975337054\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002415249529812071\n",
      "Average test loss: 0.003898823372605774\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0024065571185201405\n",
      "Average test loss: 0.003965446473616693\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0024058456174615357\n",
      "Average test loss: 0.0038825420536514788\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0024027379562871322\n",
      "Average test loss: 0.003967921150434348\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002408329356461763\n",
      "Average test loss: 0.003894743658395277\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002513653492141101\n",
      "Average test loss: 0.003851183778502875\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0025179279485924378\n",
      "Average test loss: 0.00391939186056455\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0024014003179553484\n",
      "Average test loss: 0.003923205112003618\n",
      "Epoch 299/300\n",
      "Average training loss: 0.002397821858111355\n",
      "Average test loss: 0.003875318392697308\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0023881362066086797\n",
      "Average test loss: 0.0038961093075987364\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.021963886801567342\n",
      "Average test loss: 0.006627689602474372\n",
      "Epoch 2/300\n",
      "Average training loss: 0.006257642890430159\n",
      "Average test loss: 0.007985197635574473\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0053811206341617636\n",
      "Average test loss: 0.005264491019149622\n",
      "Epoch 4/300\n",
      "Average training loss: 0.004897247878834605\n",
      "Average test loss: 0.004620359918930464\n",
      "Epoch 5/300\n",
      "Average training loss: 0.004541965208947659\n",
      "Average test loss: 0.004724440988981062\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0042567782041927175\n",
      "Average test loss: 0.004016375237868892\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004023554877481527\n",
      "Average test loss: 0.003986373753390378\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0038330841747423014\n",
      "Average test loss: 0.0037477197206268707\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0036761842436260645\n",
      "Average test loss: 0.003820386881215705\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0035339032798591585\n",
      "Average test loss: 0.0036359459944069385\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0034224535673856734\n",
      "Average test loss: 0.004978731564763519\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003285227470927768\n",
      "Average test loss: 0.0034827755120479397\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0031842608551184335\n",
      "Average test loss: 0.00322512550610635\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0030887134874032604\n",
      "Average test loss: 0.0031987265437427494\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003020303681285845\n",
      "Average test loss: 0.0038201357304222055\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002947924927601384\n",
      "Average test loss: 0.0031167441134651503\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0028754424125783974\n",
      "Average test loss: 0.002949615779850218\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0028146952136109274\n",
      "Average test loss: 0.002997847464763456\n",
      "Epoch 19/300\n",
      "Average training loss: 0.002763573201994101\n",
      "Average test loss: 0.002899674020293686\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0027133420776161883\n",
      "Average test loss: 0.003390117412639989\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002674040621560481\n",
      "Average test loss: 0.0028552802443090413\n",
      "Epoch 22/300\n",
      "Average training loss: 0.002618585434431831\n",
      "Average test loss: 0.003100551014766097\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0026179547982497346\n",
      "Average test loss: 0.0027661027947647704\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002541547510979904\n",
      "Average test loss: 0.002729964455175731\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002529588292249375\n",
      "Average test loss: 0.0026529059624299405\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002482605103196369\n",
      "Average test loss: 0.002676662199613121\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00245601757015619\n",
      "Average test loss: 0.002637228830407063\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0024332186989486216\n",
      "Average test loss: 0.0026861745229818756\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0024056796230789686\n",
      "Average test loss: 0.002791374834875266\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0024360072093291417\n",
      "Average test loss: 0.0026472500490231647\n",
      "Epoch 31/300\n",
      "Average training loss: 0.002353659466115965\n",
      "Average test loss: 0.00257976183688475\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002409494095481932\n",
      "Average test loss: 0.002842423028209143\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0023375758355897334\n",
      "Average test loss: 0.0025698926413638724\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002291589769224326\n",
      "Average test loss: 0.002566783036829697\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002285248180437419\n",
      "Average test loss: 0.002566313039511442\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002283231575662891\n",
      "Average test loss: 0.002627054830599162\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0025734648632092607\n",
      "Average test loss: 0.0026455240390366977\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0022823311544747818\n",
      "Average test loss: 0.002516565961142381\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0022263363141359553\n",
      "Average test loss: 0.002531847443224655\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0022229971611458393\n",
      "Average test loss: 0.0025258050062176253\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0022156242229458356\n",
      "Average test loss: 0.0024898491036146877\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002211352866453429\n",
      "Average test loss: 0.002557022759897841\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0021941907939811546\n",
      "Average test loss: 0.002521488017299109\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0024204932756515013\n",
      "Average test loss: 0.0025428704445560773\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0021780932433903217\n",
      "Average test loss: 0.0026285056008232962\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0021630568363600307\n",
      "Average test loss: 0.002518806330238779\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0021553166794280212\n",
      "Average test loss: 0.0025294232122186158\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002175289869411952\n",
      "Average test loss: 0.0024940859139379527\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002149950817434324\n",
      "Average test loss: 0.0025022192338688505\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002156805682616929\n",
      "Average test loss: 0.0025567212909873988\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0021499802622323236\n",
      "Average test loss: 0.002507070322624511\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0021475814195970693\n",
      "Average test loss: 0.0025617896624737318\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0021224299669265747\n",
      "Average test loss: 0.0025129005058358114\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0021076041565587125\n",
      "Average test loss: 0.0035158643486599126\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0021001400088684428\n",
      "Average test loss: 0.002527082968917158\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002095283382054832\n",
      "Average test loss: 0.002555543968040082\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0021220909423298306\n",
      "Average test loss: 0.0035383813081102237\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002089621443922321\n",
      "Average test loss: 0.002468834308389988\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002064301457359559\n",
      "Average test loss: 0.002522740514948964\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002082768550246126\n",
      "Average test loss: 0.002599061842283441\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0020637788906072577\n",
      "Average test loss: 0.002528956865477893\n",
      "Epoch 62/300\n",
      "Average training loss: 0.00205087686319732\n",
      "Average test loss: 0.0025519740473892954\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0020389594031083916\n",
      "Average test loss: 0.0024925957522872423\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002068018071456916\n",
      "Average test loss: 0.0025682394471433427\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002033259592225982\n",
      "Average test loss: 0.002508064607365264\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0020592775883980924\n",
      "Average test loss: 0.0024994037594232295\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0020266426446744136\n",
      "Average test loss: 0.0024655832203312055\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002012209767165283\n",
      "Average test loss: 0.0025077301588737303\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0020175929207147825\n",
      "Average test loss: 0.0024658670471981166\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002024982893632518\n",
      "Average test loss: 0.0024980521887126897\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0021420979050712453\n",
      "Average test loss: 0.002453534895347224\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002008032209550341\n",
      "Average test loss: 0.002497072178249558\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0019862906800376043\n",
      "Average test loss: 0.0025979364686128164\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0020031552702809373\n",
      "Average test loss: 0.002502106795294417\n",
      "Epoch 75/300\n",
      "Average training loss: 0.001982561520300806\n",
      "Average test loss: 0.002516646155466636\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0019718488195083206\n",
      "Average test loss: 0.0027372262701392176\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002018818613866137\n",
      "Average test loss: 0.002734933524702986\n",
      "Epoch 78/300\n",
      "Average training loss: 0.001967552822497156\n",
      "Average test loss: 0.002694053110977014\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0019780795379645295\n",
      "Average test loss: 0.0025005080424663094\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0020247072664399944\n",
      "Average test loss: 0.0025164744295179844\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0019565409410537945\n",
      "Average test loss: 0.002566963599787818\n",
      "Epoch 82/300\n",
      "Average training loss: 0.001952473956056767\n",
      "Average test loss: 0.0037116185474312968\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0019944579482802914\n",
      "Average test loss: 0.00268316300596214\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0019503039387572143\n",
      "Average test loss: 0.0025208629760891197\n",
      "Epoch 85/300\n",
      "Average training loss: 0.001966229913963212\n",
      "Average test loss: 0.002537269751023915\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0019457525116288\n",
      "Average test loss: 0.002673108306609922\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0019338231484095256\n",
      "Average test loss: 0.0025021971420695383\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0019541934089114268\n",
      "Average test loss: 0.0024784944020211697\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0019610607479181555\n",
      "Average test loss: 0.0027944300681766536\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0019412310692585176\n",
      "Average test loss: 0.0024715383605410654\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0019230369429828393\n",
      "Average test loss: 0.0025218731694751314\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0019189335201970404\n",
      "Average test loss: 0.002491260077390406\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0019226935913579332\n",
      "Average test loss: 0.0025660123696757686\n",
      "Epoch 94/300\n",
      "Average training loss: 0.001921601934151517\n",
      "Average test loss: 0.002469777189195156\n",
      "Epoch 95/300\n",
      "Average training loss: 0.001959643870488637\n",
      "Average test loss: 0.0025652088514632647\n",
      "Epoch 96/300\n",
      "Average training loss: 0.001925491829816666\n",
      "Average test loss: 0.002500599487374226\n",
      "Epoch 97/300\n",
      "Average training loss: 0.001898593840499719\n",
      "Average test loss: 0.002485803870484233\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0018954178889592488\n",
      "Average test loss: 0.002499869245828854\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0019104043543338777\n",
      "Average test loss: 0.002480923298228946\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0019376311343577172\n",
      "Average test loss: 0.0024299796294007035\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001902845183490879\n",
      "Average test loss: 0.0027430644896295335\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0018930911010959083\n",
      "Average test loss: 0.0024935889987068045\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0019182493624587853\n",
      "Average test loss: 0.002513449031342235\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0018813887546873754\n",
      "Average test loss: 0.002493746845051646\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0019022782990295026\n",
      "Average test loss: 0.0025046994582646424\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0020537899939550293\n",
      "Average test loss: 0.1325966913236512\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0034466778124786087\n",
      "Average test loss: 0.00941054662110077\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0023856438857813678\n",
      "Average test loss: 0.002505894958972931\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002234669907225503\n",
      "Average test loss: 0.0024996951861927905\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0021264974837087924\n",
      "Average test loss: 0.0024411572627723215\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002035566188601984\n",
      "Average test loss: 0.0024289092901680206\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0019478160284666552\n",
      "Average test loss: 0.002464048901158902\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0018970039530346791\n",
      "Average test loss: 0.002457175010815263\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0018698234035530025\n",
      "Average test loss: 0.002513462898838851\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0018567131171002985\n",
      "Average test loss: 0.0024808579521874586\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0018671088500155342\n",
      "Average test loss: 0.0025595724164611763\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0018610879368045263\n",
      "Average test loss: 0.0024867281081775825\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0018850577457083595\n",
      "Average test loss: 0.0025097594377067353\n",
      "Epoch 119/300\n",
      "Average training loss: 0.001863867747079995\n",
      "Average test loss: 0.0025542846847739485\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0018675691434699628\n",
      "Average test loss: 0.0026436558705237177\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0018796716762913598\n",
      "Average test loss: 0.0025508692319401436\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0018610656139337355\n",
      "Average test loss: 0.0027370226866462164\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0018756973628575604\n",
      "Average test loss: 0.002507647847342822\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0018542294869613317\n",
      "Average test loss: 0.002472110295461284\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0018585902677021092\n",
      "Average test loss: 0.002591940254697369\n",
      "Epoch 126/300\n",
      "Average training loss: 0.001870906860050228\n",
      "Average test loss: 0.0025294708320870996\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0018594397172952692\n",
      "Average test loss: 0.0025471634132166704\n",
      "Epoch 128/300\n",
      "Average training loss: 0.001857340636663139\n",
      "Average test loss: 0.0025743913952675133\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0018611413706094027\n",
      "Average test loss: 0.0025071336809131834\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019282812738480666\n",
      "Average test loss: 0.0026284269916100633\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0018527473744211925\n",
      "Average test loss: 0.0024987732871539063\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0018257908390627968\n",
      "Average test loss: 0.002468353814548916\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0018286715927016404\n",
      "Average test loss: 0.002557668923710783\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0018395176456413335\n",
      "Average test loss: 0.0025063185015072424\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0018436054434213373\n",
      "Average test loss: 0.002597152021403114\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0018594102447645532\n",
      "Average test loss: 0.002599035361574756\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018410834445514613\n",
      "Average test loss: 0.0024846674690230026\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0018248335151001811\n",
      "Average test loss: 0.002643349695329865\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0018551194500178098\n",
      "Average test loss: 0.0026004546290884414\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0018180376265404953\n",
      "Average test loss: 0.002570947231310937\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0018113849946401186\n",
      "Average test loss: 0.00247198705168234\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0024090253475846516\n",
      "Average test loss: 0.006096829248799218\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0018614405331512291\n",
      "Average test loss: 0.002510538514289591\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0018046795801363058\n",
      "Average test loss: 0.002511591743470894\n",
      "Epoch 145/300\n",
      "Average training loss: 0.001796430214929084\n",
      "Average test loss: 0.002480493855973085\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0018136059414181444\n",
      "Average test loss: 0.0026021593132366737\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0018030704700698456\n",
      "Average test loss: 0.0025419827846603262\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0018073755930074387\n",
      "Average test loss: 0.002625202333347665\n",
      "Epoch 149/300\n",
      "Average training loss: 0.001815899070041875\n",
      "Average test loss: 0.002615430851156513\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018092728904965851\n",
      "Average test loss: 0.0025455413363460037\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001832691312353644\n",
      "Average test loss: 0.0025716745332918234\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0018023628767372833\n",
      "Average test loss: 0.0025223390647313663\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0018094683241926962\n",
      "Average test loss: 0.0025566476177838115\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0038762899063941505\n",
      "Average test loss: 0.0028331018932577635\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0025582913234829903\n",
      "Average test loss: 0.0026055289113687146\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002293361561993758\n",
      "Average test loss: 0.002516754946981867\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002167331002238724\n",
      "Average test loss: 0.0024354502140647837\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0020687542732598053\n",
      "Average test loss: 0.002495595538367828\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0019839151882463033\n",
      "Average test loss: 0.0024298134259879587\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0019588621283570923\n",
      "Average test loss: 0.0024984600450843573\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018708023346132702\n",
      "Average test loss: 0.0024643588695261215\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0018213400585162971\n",
      "Average test loss: 0.0025187224257323475\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001862601556504766\n",
      "Average test loss: 0.0027254182918825085\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0017947645124254956\n",
      "Average test loss: 0.002533680288121104\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0017838921931882699\n",
      "Average test loss: 0.002513172306223876\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0017811569976102974\n",
      "Average test loss: 0.0025027404234020246\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0018036579150292609\n",
      "Average test loss: 0.0031711430972855953\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0017905945671308373\n",
      "Average test loss: 0.0025642933510243893\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0018190716112860374\n",
      "Average test loss: 0.002577237823771106\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0017909532990306616\n",
      "Average test loss: 0.0028287463595883713\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0017880134747053186\n",
      "Average test loss: 0.002537272797173096\n",
      "Epoch 172/300\n",
      "Average training loss: 0.001788030279489855\n",
      "Average test loss: 0.0025688050438960395\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0017979937528984414\n",
      "Average test loss: 0.002489096775030096\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0017899519545543526\n",
      "Average test loss: 0.00255352950323787\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0017821337222639058\n",
      "Average test loss: 0.0025816224312616718\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0017869633751817875\n",
      "Average test loss: 0.0025092192178385125\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0018163791524453295\n",
      "Average test loss: 0.002553373882960942\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0017806241379843817\n",
      "Average test loss: 0.0025403167485362954\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0017807636433798406\n",
      "Average test loss: 0.0025858428919066988\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0018293346125218604\n",
      "Average test loss: 0.0025099261194053623\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001815121783874929\n",
      "Average test loss: 0.0025456915077649886\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0017667635237384174\n",
      "Average test loss: 0.002580980870872736\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0017662114362335867\n",
      "Average test loss: 0.0025060193188902406\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0017677219631150364\n",
      "Average test loss: 0.0026055449868241944\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0017746406108554867\n",
      "Average test loss: 0.003079200181282229\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0017778826883683603\n",
      "Average test loss: 0.0025722286670158308\n",
      "Epoch 187/300\n",
      "Average training loss: 0.001758002332618667\n",
      "Average test loss: 0.0025195943964645266\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0017817926085036662\n",
      "Average test loss: 0.0025385718225604957\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0017676089944110977\n",
      "Average test loss: 0.0025661568141852817\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001760880254726443\n",
      "Average test loss: 0.0026122365666346415\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0017678552533810336\n",
      "Average test loss: 0.00254752237122092\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0017921262834635046\n",
      "Average test loss: 0.0025614534347421594\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0017561321720066998\n",
      "Average test loss: 0.002509580432747801\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0019116130194937189\n",
      "Average test loss: 0.005711291924946838\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0017591441351299485\n",
      "Average test loss: 0.002549182784019245\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0017480734904400178\n",
      "Average test loss: 0.0026178884587975014\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0017437743433854646\n",
      "Average test loss: 0.0025959730409085753\n",
      "Epoch 198/300\n",
      "Average training loss: 0.001741070105176833\n",
      "Average test loss: 0.0025368981864303352\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0017863270190233986\n",
      "Average test loss: 0.0037286920363290444\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0017485214665325152\n",
      "Average test loss: 0.002596070286093487\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0017488774408379363\n",
      "Average test loss: 0.002627843764300148\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0017429859953828983\n",
      "Average test loss: 0.00255723237618804\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0017877426474458642\n",
      "Average test loss: 0.0025152470463265975\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0017355670284272895\n",
      "Average test loss: 0.0025606468955261838\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0018124690685007306\n",
      "Average test loss: 0.0025756223444930383\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0017374449843126866\n",
      "Average test loss: 0.0025329568152212436\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0018584749955270026\n",
      "Average test loss: 0.0025720392502844333\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0018150990205920406\n",
      "Average test loss: 0.0025078015443351534\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0017238365364157491\n",
      "Average test loss: 0.0025210470443384515\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0017210216276968518\n",
      "Average test loss: 0.0025683540350033176\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0017612045435234905\n",
      "Average test loss: 0.002607313466568788\n",
      "Epoch 212/300\n",
      "Average training loss: 0.001726767372339964\n",
      "Average test loss: 0.002492995699039764\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0017289310369847548\n",
      "Average test loss: 0.0025282642946889\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0017321611364475556\n",
      "Average test loss: 0.0027770241362353168\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001732537866777016\n",
      "Average test loss: 0.0025104293154759537\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0017437180823956927\n",
      "Average test loss: 0.0026012900380624664\n",
      "Epoch 217/300\n",
      "Average training loss: 0.001732225476246741\n",
      "Average test loss: 0.002582847611254288\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0017221744772460724\n",
      "Average test loss: 0.002744903835778435\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0017351036707663702\n",
      "Average test loss: 0.002648504221191009\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0017245088503178624\n",
      "Average test loss: 0.002558273982670572\n",
      "Epoch 221/300\n",
      "Average training loss: 0.001740601300365395\n",
      "Average test loss: 0.0025733329388830397\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0017315343458515903\n",
      "Average test loss: 0.0027433225096513826\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0017189727446271313\n",
      "Average test loss: 0.002540895377182298\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0017284916901133126\n",
      "Average test loss: 0.002612251188192103\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0017389732155327995\n",
      "Average test loss: 0.0025632858135634\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0017225279087821643\n",
      "Average test loss: 0.002673981047131949\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0017363109229546455\n",
      "Average test loss: 0.002977650713382496\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001731276182250844\n",
      "Average test loss: 0.002610623861973484\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0017287114451949796\n",
      "Average test loss: 0.002625324044376612\n",
      "Epoch 230/300\n",
      "Average training loss: 0.001778446453726954\n",
      "Average test loss: 0.002575264525703258\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0017061597881838678\n",
      "Average test loss: 0.002565418359099163\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0017078099377039406\n",
      "Average test loss: 0.002502201015750567\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0017244816364513503\n",
      "Average test loss: 0.0026317544171793592\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0017208285541791056\n",
      "Average test loss: 0.0025507484268811015\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0017040730498524175\n",
      "Average test loss: 0.0025836396703703534\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0017079690255017745\n",
      "Average test loss: 0.002588769589240352\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001710595281380746\n",
      "Average test loss: 0.002629899110127654\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0017034258280570309\n",
      "Average test loss: 0.0027427279766028124\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0017097372110519145\n",
      "Average test loss: 0.0025867054325838883\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0017169378352248005\n",
      "Average test loss: 0.0026582278391967216\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0017023772750463751\n",
      "Average test loss: 0.0025270618761165276\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0017481334931734535\n",
      "Average test loss: 0.0025573505109383\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0017350375696809756\n",
      "Average test loss: 0.0028034158166911865\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0016901849667645163\n",
      "Average test loss: 0.002606863876183828\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0016971135234667195\n",
      "Average test loss: 0.0026103906917075317\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0017606130028143525\n",
      "Average test loss: 0.0026582608121550745\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0017136064028988281\n",
      "Average test loss: 0.0027639926249782244\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0016834446020010446\n",
      "Average test loss: 0.0026801103096869257\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0016886831029421753\n",
      "Average test loss: 0.002572215564445489\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0016893002734933462\n",
      "Average test loss: 0.002650692058934106\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0017105422843661574\n",
      "Average test loss: 0.0026624329762740266\n",
      "Epoch 252/300\n",
      "Average training loss: 0.001695698386679093\n",
      "Average test loss: 0.0025852956268936395\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0016911104797489114\n",
      "Average test loss: 0.0025430330467513867\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0017002978872300851\n",
      "Average test loss: 0.002562398536958628\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0016975457450995842\n",
      "Average test loss: 0.0025657022762008838\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0017318088359509905\n",
      "Average test loss: 0.002673958838192953\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0016979898370595443\n",
      "Average test loss: 0.002512810305174854\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0016880136956978176\n",
      "Average test loss: 0.0026153551204543975\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0016790577688564857\n",
      "Average test loss: 0.0025596945521732173\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0016827849628817703\n",
      "Average test loss: 0.0025595737580830853\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0016846604623521368\n",
      "Average test loss: 0.002562768314033747\n",
      "Epoch 262/300\n",
      "Average training loss: 0.001732140847792228\n",
      "Average test loss: 0.002657023586643239\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0016814676786048545\n",
      "Average test loss: 0.0025419778452358314\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0016739403701697786\n",
      "Average test loss: 0.0026678294425623284\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0016791246900748876\n",
      "Average test loss: 0.002589996871124539\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0016876438309748967\n",
      "Average test loss: 0.002591767123796874\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0016947968441786037\n",
      "Average test loss: 0.0026819342558996543\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0016832359000626537\n",
      "Average test loss: 0.0026459297235641214\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0016926007212864028\n",
      "Average test loss: 0.00259309496358037\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0016775532971239752\n",
      "Average test loss: 0.002635048546310928\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0016756986418945922\n",
      "Average test loss: 0.002686451200602783\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0017078078704782658\n",
      "Average test loss: 0.002605654258177512\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0016719206675059265\n",
      "Average test loss: 0.002668750731067525\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0016714715482667088\n",
      "Average test loss: 0.0026551811742699808\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0016989701236080792\n",
      "Average test loss: 0.002543736747776469\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0016687658359814022\n",
      "Average test loss: 0.002679106577920417\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0016661753828326861\n",
      "Average test loss: 0.002650940624790059\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0017017706799217396\n",
      "Average test loss: 0.002706471455179983\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0016690688656849993\n",
      "Average test loss: 0.002675931862038043\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0016675311870252092\n",
      "Average test loss: 0.0025877494373255307\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0016678599879766504\n",
      "Average test loss: 0.002706126819468207\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0016879650810733438\n",
      "Average test loss: 0.0026346719854821762\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0016647574266211854\n",
      "Average test loss: 0.0031335672663731708\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0016682774560112092\n",
      "Average test loss: 0.002561619439886676\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0016620921367365453\n",
      "Average test loss: 0.0025646873023360967\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0028272434870402017\n",
      "Average test loss: 0.002716046736058262\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0022796419718199307\n",
      "Average test loss: 0.002493930629764994\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002049355246954494\n",
      "Average test loss: 0.0024983877771430547\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0019151755101564857\n",
      "Average test loss: 0.0025236238760666715\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0018132988024089072\n",
      "Average test loss: 0.0025591606497764588\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0017349695017975237\n",
      "Average test loss: 0.002572216283529997\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0016864133231962722\n",
      "Average test loss: 0.0025689264484163787\n",
      "Epoch 293/300\n",
      "Average training loss: 0.001657762975535459\n",
      "Average test loss: 0.0026123677115473484\n",
      "Epoch 294/300\n",
      "Average training loss: 0.001645602634590533\n",
      "Average test loss: 0.0026458275746554134\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0016431025799570813\n",
      "Average test loss: 0.0026792814897166357\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0016407144795068437\n",
      "Average test loss: 0.002638662873663836\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0016443779940406482\n",
      "Average test loss: 0.0025481438607805303\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0016517138708796765\n",
      "Average test loss: 0.002601008921965129\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0016617751785864433\n",
      "Average test loss: 0.002671428510919213\n",
      "Epoch 300/300\n",
      "Average training loss: 0.001661753435412215\n",
      "Average test loss: 0.0025522429855126473\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.019123689711921747\n",
      "Average test loss: 0.006108471412211656\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0050524100261843865\n",
      "Average test loss: 0.00480593146632115\n",
      "Epoch 3/300\n",
      "Average training loss: 0.004269110755787955\n",
      "Average test loss: 0.004275647206024991\n",
      "Epoch 4/300\n",
      "Average training loss: 0.003833384839610921\n",
      "Average test loss: 0.004640856097348862\n",
      "Epoch 5/300\n",
      "Average training loss: 0.003521596018017994\n",
      "Average test loss: 0.00346713300421834\n",
      "Epoch 6/300\n",
      "Average training loss: 0.00327211099728528\n",
      "Average test loss: 0.003208423368839754\n",
      "Epoch 7/300\n",
      "Average training loss: 0.003095294421952632\n",
      "Average test loss: 0.0030444527856177753\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0029277188788271614\n",
      "Average test loss: 0.0028612123417357605\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0027846455971399942\n",
      "Average test loss: 0.002742989744991064\n",
      "Epoch 10/300\n",
      "Average training loss: 0.002660219693970349\n",
      "Average test loss: 0.002721028777874178\n",
      "Epoch 11/300\n",
      "Average training loss: 0.002536437371124824\n",
      "Average test loss: 0.0025089972983631825\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0024635674347066217\n",
      "Average test loss: 0.0028282845694985654\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0023685725087092984\n",
      "Average test loss: 0.0023476919618745646\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002299067099682159\n",
      "Average test loss: 0.0025327108910100328\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0022060188873567516\n",
      "Average test loss: 0.002249360027205613\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002155637865368691\n",
      "Average test loss: 0.002250621863019963\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0021021617501974107\n",
      "Average test loss: 0.0021365414245261088\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0020470585773388543\n",
      "Average test loss: 0.0022420600240843163\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0020026357184267708\n",
      "Average test loss: 0.0021258178503356048\n",
      "Epoch 20/300\n",
      "Average training loss: 0.001965984771959484\n",
      "Average test loss: 0.002074563668316437\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0019430061894365482\n",
      "Average test loss: 0.0020258300786630975\n",
      "Epoch 22/300\n",
      "Average training loss: 0.001898137496577369\n",
      "Average test loss: 0.002029598957548539\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0018632570000158416\n",
      "Average test loss: 0.0019477921648778848\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0018346523144799802\n",
      "Average test loss: 0.0020456406289918556\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0018316347137507466\n",
      "Average test loss: 0.001916765444808536\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0017909152906181085\n",
      "Average test loss: 0.0019184263605210516\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0017701433940480152\n",
      "Average test loss: 0.0019182623198462857\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0017510772121863232\n",
      "Average test loss: 0.0020565055310726166\n",
      "Epoch 29/300\n",
      "Average training loss: 0.001729845852798058\n",
      "Average test loss: 0.0018675271082255575\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0017162891347996063\n",
      "Average test loss: 0.0019041915355871122\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0017013250889463558\n",
      "Average test loss: 0.0018170449124235246\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0016796456904460987\n",
      "Average test loss: 0.0018585584831113616\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0016770548052671883\n",
      "Average test loss: 0.0018377923402521346\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0016671060697279042\n",
      "Average test loss: 0.0019060225542634726\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0016563825664213962\n",
      "Average test loss: 0.0018535564144452413\n",
      "Epoch 36/300\n",
      "Average training loss: 0.001636368546117511\n",
      "Average test loss: 0.0018462155881441302\n",
      "Epoch 37/300\n",
      "Average training loss: 0.001634450536945628\n",
      "Average test loss: 0.001823818652683662\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0016506644157278868\n",
      "Average test loss: 0.0018036614507436753\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0016250470922225051\n",
      "Average test loss: 0.0017944965780609184\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0015991372263265981\n",
      "Average test loss: 0.0017907142537749476\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0016003789904837807\n",
      "Average test loss: 0.0017626846506156855\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0015859087363092436\n",
      "Average test loss: 0.0017961238194257022\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0015762720990719066\n",
      "Average test loss: 0.0017619269424014622\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0015731352088559005\n",
      "Average test loss: 0.0017448622439470555\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0015772919305082824\n",
      "Average test loss: 0.0017898588410268227\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0015658247580544815\n",
      "Average test loss: 0.0018054389014012283\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0015589218065142631\n",
      "Average test loss: 0.0017913799029257561\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0015749418389879996\n",
      "Average test loss: 0.0017783053811225625\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0015385505858187874\n",
      "Average test loss: 0.0017472250993467039\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0015317559394364556\n",
      "Average test loss: 0.0017495558481249545\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0015284487002839645\n",
      "Average test loss: 0.0017758068037736748\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0015247545659335124\n",
      "Average test loss: 0.001742347191191382\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0015221941059248316\n",
      "Average test loss: 0.0017418360674960746\n",
      "Epoch 54/300\n",
      "Average training loss: 0.001510586937992937\n",
      "Average test loss: 0.0018081492294246952\n",
      "Epoch 55/300\n",
      "Average training loss: 0.001521718157455325\n",
      "Average test loss: 0.001842870308086276\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0015008328555979663\n",
      "Average test loss: 0.0017449499219655991\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0015075579539148344\n",
      "Average test loss: 0.0017403896869056755\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0014964970941137936\n",
      "Average test loss: 0.001776156724948022\n",
      "Epoch 59/300\n",
      "Average training loss: 0.001494526838262876\n",
      "Average test loss: 0.001741192939070364\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0014805201897915038\n",
      "Average test loss: 0.001947790182299084\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0014903289404594236\n",
      "Average test loss: 0.001774331265129149\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0014786112745189004\n",
      "Average test loss: 0.00197194679826498\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0014811742394748662\n",
      "Average test loss: 0.001879192538973358\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0014718001359659764\n",
      "Average test loss: 0.001957174057347907\n",
      "Epoch 65/300\n",
      "Average training loss: 0.001479219120202793\n",
      "Average test loss: 0.001739050915464759\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0014753079757922226\n",
      "Average test loss: 0.0017186697675949996\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0018100371144505012\n",
      "Average test loss: 0.0017955413522819678\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0015020558364275429\n",
      "Average test loss: 0.0017500798104123937\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0014641262168685596\n",
      "Average test loss: 0.0017864646924038727\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0014502487284027868\n",
      "Average test loss: 0.0017345604106990828\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0014463560849221216\n",
      "Average test loss: 0.001728950665642818\n",
      "Epoch 72/300\n",
      "Average training loss: 0.001448021563804812\n",
      "Average test loss: 0.0017817519549280405\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0014470611210498545\n",
      "Average test loss: 0.0017371229810329775\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0014502791326699985\n",
      "Average test loss: 0.001796643785925375\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0014486274959312545\n",
      "Average test loss: 0.002567524064746168\n",
      "Epoch 76/300\n",
      "Average training loss: 0.001452958461207648\n",
      "Average test loss: 0.0017282816980861955\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0014360285099181865\n",
      "Average test loss: 0.0017671139171967904\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0014319263340180947\n",
      "Average test loss: 0.0017621715004659361\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0014305835083747904\n",
      "Average test loss: 0.0017434632931318547\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0014278066755375929\n",
      "Average test loss: 0.0018369576604002052\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0014418600999439755\n",
      "Average test loss: 0.001776721900742915\n",
      "Epoch 82/300\n",
      "Average training loss: 0.001431312313510312\n",
      "Average test loss: 0.0017312944036804968\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0014119720293415917\n",
      "Average test loss: 0.0017678544991132285\n",
      "Epoch 84/300\n",
      "Average training loss: 0.001414217040874064\n",
      "Average test loss: 0.001784778287427293\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0014103889203526908\n",
      "Average test loss: 0.0017316157968921793\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0014127105292346743\n",
      "Average test loss: 0.0017391230355327328\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0014117739955480729\n",
      "Average test loss: 0.001941065140792893\n",
      "Epoch 88/300\n",
      "Average training loss: 0.001404850056498415\n",
      "Average test loss: 0.0017433897983282804\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0014074470680207013\n",
      "Average test loss: 0.0017860176242474053\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0014011463893370496\n",
      "Average test loss: 0.0031175861064758567\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0014092105600155061\n",
      "Average test loss: 0.0017703385568327375\n",
      "Epoch 92/300\n",
      "Average training loss: 0.001392814670378963\n",
      "Average test loss: 0.0017563153508429726\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0013948987775171796\n",
      "Average test loss: 0.0017361541765017641\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0013898590871960752\n",
      "Average test loss: 0.0018093666616413328\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0013920761349921426\n",
      "Average test loss: 0.0018230940078695615\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0014068903270074062\n",
      "Average test loss: 0.001840680309985247\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0013971526071222293\n",
      "Average test loss: 0.0017946240287274122\n",
      "Epoch 98/300\n",
      "Average training loss: 0.001377031547224356\n",
      "Average test loss: 0.0017970449158714876\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0013785406230017543\n",
      "Average test loss: 0.0018571874710420768\n",
      "Epoch 100/300\n",
      "Average training loss: 0.001372327503748238\n",
      "Average test loss: 0.0017702214881363842\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001380033639052676\n",
      "Average test loss: 0.0017447958442692955\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0013736823062515922\n",
      "Average test loss: 0.0018420782608704436\n",
      "Epoch 103/300\n",
      "Average training loss: 0.001387981444183323\n",
      "Average test loss: 0.001758660583756864\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0013653714896386697\n",
      "Average test loss: 0.00178259567088551\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0013682445274252031\n",
      "Average test loss: 0.0017389484728789991\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0013710549388908679\n",
      "Average test loss: 0.0017766004227515724\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0013615325691385403\n",
      "Average test loss: 0.001739393461909559\n",
      "Epoch 108/300\n",
      "Average training loss: 0.001364343591551814\n",
      "Average test loss: 0.001750560328985254\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0013670284831896423\n",
      "Average test loss: 0.0017410093351370758\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0013597007087535328\n",
      "Average test loss: 0.0024616773181284466\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0013572242808424764\n",
      "Average test loss: 0.0018908843569871452\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0013505951354487074\n",
      "Average test loss: 0.0018303163163363933\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0013551771544540923\n",
      "Average test loss: 0.0017519840037243234\n",
      "Epoch 114/300\n",
      "Average training loss: 0.001358857471599347\n",
      "Average test loss: 0.001768162674581011\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0013543384687768089\n",
      "Average test loss: 0.001830079449340701\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0013505482340438499\n",
      "Average test loss: 0.0018308943702528874\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0013550591674736804\n",
      "Average test loss: 0.0017933647149345941\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0013386816820129752\n",
      "Average test loss: 0.0017684292853292491\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0013391325976699591\n",
      "Average test loss: 0.0017555275248984496\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0013384865581368406\n",
      "Average test loss: 0.0017653789113586148\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0013419769165209598\n",
      "Average test loss: 0.0017863090041403968\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0013388356839617093\n",
      "Average test loss: 0.0026723286339806187\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0013423243371459344\n",
      "Average test loss: 0.001847250949487918\n",
      "Epoch 124/300\n",
      "Average training loss: 0.001341297653193275\n",
      "Average test loss: 0.0017531406775944762\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0013344904650002718\n",
      "Average test loss: 0.0017922734780651\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0013292364841327071\n",
      "Average test loss: 0.0018257595444512037\n",
      "Epoch 127/300\n",
      "Average training loss: 0.001332444340404537\n",
      "Average test loss: 0.001787128674487273\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0013241786277439032\n",
      "Average test loss: 0.0018203751236821213\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0013240772035593789\n",
      "Average test loss: 0.0018480502754035922\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0013271263072060213\n",
      "Average test loss: 0.001792317908257246\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0013247547489590942\n",
      "Average test loss: 0.0017731060867922173\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0013212008605607683\n",
      "Average test loss: 0.0018199592118875849\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0013700959390650193\n",
      "Average test loss: 0.001769857733303474\n",
      "Epoch 134/300\n",
      "Average training loss: 0.001311784866421173\n",
      "Average test loss: 0.0018240205162308282\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0013132713754764862\n",
      "Average test loss: 0.0019225650420412422\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0013165123345744278\n",
      "Average test loss: 0.001846705608483818\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0013158190748654307\n",
      "Average test loss: 0.001805773376280235\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0013237749970414572\n",
      "Average test loss: 0.0017783936518761846\n",
      "Epoch 139/300\n",
      "Average training loss: 0.001311385332710213\n",
      "Average test loss: 0.0018167677409946918\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0013138859533808297\n",
      "Average test loss: 0.0018756342784812053\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001307307601802879\n",
      "Average test loss: 0.0017880910980618663\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0013221952145298322\n",
      "Average test loss: 0.0018098190766241814\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0013173451630605591\n",
      "Average test loss: 0.0017687900399582254\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0013027463251103957\n",
      "Average test loss: 0.0018021730896499422\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0013026545605518752\n",
      "Average test loss: 0.0017800336342511905\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0013010521874659592\n",
      "Average test loss: 0.0018060403255124888\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0013042469816282392\n",
      "Average test loss: 0.0017844838700774644\n",
      "Epoch 148/300\n",
      "Average training loss: 0.001309239805986484\n",
      "Average test loss: 0.0017921377211395238\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0012985195903521445\n",
      "Average test loss: 0.001768324253666732\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0012991914761563141\n",
      "Average test loss: 0.0018090681404703192\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0013133231315554843\n",
      "Average test loss: 0.0018023324927522075\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0012938924106872744\n",
      "Average test loss: 0.0017727978521337113\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0012907842121397456\n",
      "Average test loss: 0.0017724740536262593\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0012881081830710172\n",
      "Average test loss: 0.001779183001878361\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0012962421792455845\n",
      "Average test loss: 0.0022217609343222447\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0013104531383141876\n",
      "Average test loss: 0.0018221113783203893\n",
      "Epoch 157/300\n",
      "Average training loss: 0.001289167437185016\n",
      "Average test loss: 0.0018416233124832313\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001290264953341749\n",
      "Average test loss: 0.0018726813447558217\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00128702712788557\n",
      "Average test loss: 0.0018546937772383292\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0012871937851628495\n",
      "Average test loss: 0.0018273325289289157\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0012842036999969018\n",
      "Average test loss: 0.0017902592228104671\n",
      "Epoch 162/300\n",
      "Average training loss: 0.001283841415722337\n",
      "Average test loss: 0.0017643318593295084\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0012854144228622317\n",
      "Average test loss: 0.0017746939383861092\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0012815566146746277\n",
      "Average test loss: 0.0018181650128939913\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0012784923905920652\n",
      "Average test loss: 0.0018252926398482589\n",
      "Epoch 166/300\n",
      "Average training loss: 0.001292009465292924\n",
      "Average test loss: 0.001845903602739175\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0012783461151023706\n",
      "Average test loss: 0.001772253026564916\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0012807539080580076\n",
      "Average test loss: 0.001844203915860918\n",
      "Epoch 169/300\n",
      "Average training loss: 0.001279258429693679\n",
      "Average test loss: 0.0018876108324362172\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0012748949004130232\n",
      "Average test loss: 0.0018495853764729368\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0012768924372891584\n",
      "Average test loss: 0.0020093801490341625\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0012724077525652117\n",
      "Average test loss: 0.0018191160269909435\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0012783831280345718\n",
      "Average test loss: 0.0018567420571214623\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0012702190943269266\n",
      "Average test loss: 0.001821979032829404\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0012718588646708263\n",
      "Average test loss: 0.0018257534413908918\n",
      "Epoch 176/300\n",
      "Average training loss: 0.001272971328244441\n",
      "Average test loss: 0.0017896425344256892\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0012753343749791384\n",
      "Average test loss: 0.0017989085511200958\n",
      "Epoch 178/300\n",
      "Average training loss: 0.001271699336098714\n",
      "Average test loss: 0.001868430802702076\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0012710289756457012\n",
      "Average test loss: 0.001837572349442376\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0012719455194763012\n",
      "Average test loss: 0.0017862864604426755\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0012656960704674324\n",
      "Average test loss: 0.0018269964904627867\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0012640454919180936\n",
      "Average test loss: 0.0018545216934548485\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001263200412090454\n",
      "Average test loss: 0.0017989453453984526\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0012678656063766943\n",
      "Average test loss: 0.0018268299924416676\n",
      "Epoch 185/300\n",
      "Average training loss: 0.001263076447053916\n",
      "Average test loss: 0.0019786377733366357\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0012661305187890927\n",
      "Average test loss: 0.001823618468311098\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0012637530783605245\n",
      "Average test loss: 0.0018362461662747795\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0012585897783024443\n",
      "Average test loss: 0.0020876939944509004\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0012611695694633657\n",
      "Average test loss: 0.0019270641907221742\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001260339283798304\n",
      "Average test loss: 0.0019025909971031878\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0012565430659386846\n",
      "Average test loss: 0.0018394225096950928\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001254374885155509\n",
      "Average test loss: 0.00189459088527494\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0012605963919518723\n",
      "Average test loss: 0.0017971356691171725\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0012570785358548165\n",
      "Average test loss: 0.0018595843735254474\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0012507542147197658\n",
      "Average test loss: 0.0018088527652952407\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0012561595775704417\n",
      "Average test loss: 0.0018193299562359849\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0012540557890509565\n",
      "Average test loss: 0.0018730089933507972\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0012517007510695192\n",
      "Average test loss: 0.001796928659081459\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0012544636359024378\n",
      "Average test loss: 0.006379902169936233\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0016134377635704975\n",
      "Average test loss: 0.0018729300290449627\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0012597040224613415\n",
      "Average test loss: 0.0018077033035871055\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00122962266082565\n",
      "Average test loss: 0.0017978382499681579\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0012300048487053977\n",
      "Average test loss: 0.001867842398169968\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0012313230413322647\n",
      "Average test loss: 0.0018767434743543465\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0012368674026802182\n",
      "Average test loss: 0.0018154966893295446\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0012427802418565584\n",
      "Average test loss: 0.0018786426660501294\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0012494780520080691\n",
      "Average test loss: 0.0018626205296152169\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0012399178097645441\n",
      "Average test loss: 0.001846845929717852\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0012480639576291045\n",
      "Average test loss: 0.0017930550430383947\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0012443065996178323\n",
      "Average test loss: 0.001953184625133872\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0012438022489142087\n",
      "Average test loss: 0.0019038800342629353\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0012476236043084\n",
      "Average test loss: 0.001821567920036614\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001242261367643045\n",
      "Average test loss: 0.002008893685725828\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0012434806590899826\n",
      "Average test loss: 0.0017936716091094746\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001244613775673012\n",
      "Average test loss: 0.0018366067212902838\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001238158457705544\n",
      "Average test loss: 0.0018313330796857674\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0012455302368228635\n",
      "Average test loss: 0.0018194339715151323\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0012426162965906162\n",
      "Average test loss: 0.0017888867194867796\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0012349255825910303\n",
      "Average test loss: 0.001884804672251145\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0012329547688778905\n",
      "Average test loss: 0.0018593958465175496\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0012397225625399087\n",
      "Average test loss: 0.0018673302414309648\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0012399910872077776\n",
      "Average test loss: 0.001840807099516193\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0012371322631628977\n",
      "Average test loss: 0.00188462591295441\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0012388285814474026\n",
      "Average test loss: 0.001986486143949959\n",
      "Epoch 225/300\n",
      "Average training loss: 0.001235036454266972\n",
      "Average test loss: 0.0018173610149986215\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0012357582297797005\n",
      "Average test loss: 0.0018813973781135348\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0012329074118493332\n",
      "Average test loss: 0.001870280437792341\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0012358136107731197\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-LastLayer/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-LastLayer/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-LastLayer/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
