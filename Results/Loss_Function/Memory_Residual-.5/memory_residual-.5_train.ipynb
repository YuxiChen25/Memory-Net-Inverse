{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.5)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06461056099004216\n",
      "Average test loss: 0.011017959077325131\n",
      "Epoch 2/300\n",
      "Average training loss: 0.023736422512266372\n",
      "Average test loss: 0.009730628521906004\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020958261754777698\n",
      "Average test loss: 0.009536705893774828\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01958108244008488\n",
      "Average test loss: 0.008659826318009033\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01863359876142608\n",
      "Average test loss: 0.008230749202271303\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017629787125521237\n",
      "Average test loss: 0.008541456503586637\n",
      "Epoch 7/300\n",
      "Average training loss: 0.016813072623478042\n",
      "Average test loss: 0.00799544287721316\n",
      "Epoch 8/300\n",
      "Average training loss: 0.016306654901968108\n",
      "Average test loss: 0.008150928652120961\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01591106749739912\n",
      "Average test loss: 0.0075810360312461855\n",
      "Epoch 10/300\n",
      "Average training loss: 0.015566740001241365\n",
      "Average test loss: 0.007551412220630381\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01533954409095976\n",
      "Average test loss: 0.007276699588530593\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015008125661975808\n",
      "Average test loss: 0.0071250942788190315\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014696871766613589\n",
      "Average test loss: 0.007229965533647273\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014501098118722439\n",
      "Average test loss: 0.006967360733697811\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014324560778008567\n",
      "Average test loss: 0.007304160349898868\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01409837840249141\n",
      "Average test loss: 0.006975315894517634\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01398419415867991\n",
      "Average test loss: 0.006839697901573446\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01377183333122068\n",
      "Average test loss: 0.006672615528520611\n",
      "Epoch 19/300\n",
      "Average training loss: 0.013580532893538475\n",
      "Average test loss: 0.006696166111777226\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013434690992865298\n",
      "Average test loss: 0.006624461671544446\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01327283931026856\n",
      "Average test loss: 0.006541599837856161\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013161068986687395\n",
      "Average test loss: 0.006729194860077566\n",
      "Epoch 23/300\n",
      "Average training loss: 0.013049367140564653\n",
      "Average test loss: 0.006644926708605554\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012903975570367442\n",
      "Average test loss: 0.006378336193660895\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012808539137244225\n",
      "Average test loss: 0.0064710016540355155\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01272350107298957\n",
      "Average test loss: 0.006380987809764014\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012588337591124906\n",
      "Average test loss: 0.006416826555505395\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012459898372491202\n",
      "Average test loss: 0.0064584570133851636\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01239809188577864\n",
      "Average test loss: 0.006295580852776766\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012390688541034858\n",
      "Average test loss: 0.008583654603196515\n",
      "Epoch 31/300\n",
      "Average training loss: 0.012211476110749775\n",
      "Average test loss: 0.006381030691994561\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012136939771473407\n",
      "Average test loss: 0.006199773040910562\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012070334953566393\n",
      "Average test loss: 0.006179420643382602\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01201144481367535\n",
      "Average test loss: 0.006214801936927769\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011930637468894324\n",
      "Average test loss: 0.006276742681033082\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011866723716259003\n",
      "Average test loss: 0.00716412401282125\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01181467148164908\n",
      "Average test loss: 0.006090854232509931\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01174412771479951\n",
      "Average test loss: 0.0060542893004086286\n",
      "Epoch 39/300\n",
      "Average training loss: 0.011667696804222133\n",
      "Average test loss: 0.006148174923327234\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01161706092953682\n",
      "Average test loss: 0.006329093056834406\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011596481311652395\n",
      "Average test loss: 0.006034137292868561\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011528953377571372\n",
      "Average test loss: 0.006123806871473789\n",
      "Epoch 43/300\n",
      "Average training loss: 0.011510112957821951\n",
      "Average test loss: 0.006296218697809511\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01141794305625889\n",
      "Average test loss: 0.006112893883138895\n",
      "Epoch 45/300\n",
      "Average training loss: 0.011380139662987655\n",
      "Average test loss: 0.006046514000329706\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0113433260925942\n",
      "Average test loss: 0.0060355159648590615\n",
      "Epoch 47/300\n",
      "Average training loss: 0.011334858658413093\n",
      "Average test loss: 0.0061211393934985005\n",
      "Epoch 48/300\n",
      "Average training loss: 0.011298683197961913\n",
      "Average test loss: 0.0075006342505415284\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01119829966707362\n",
      "Average test loss: 0.006155815510286225\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011212788811160458\n",
      "Average test loss: 0.006240431294673019\n",
      "Epoch 51/300\n",
      "Average training loss: 0.011145884917014175\n",
      "Average test loss: 0.006337474227365521\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011111115139391687\n",
      "Average test loss: 0.00614650354327427\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011085457746767335\n",
      "Average test loss: 0.006264776311400864\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011031334586441517\n",
      "Average test loss: 0.005988628084460894\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011020963695314195\n",
      "Average test loss: 0.005973934706714418\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011015243840714296\n",
      "Average test loss: 0.0059815548621118065\n",
      "Epoch 57/300\n",
      "Average training loss: 0.010945715679062737\n",
      "Average test loss: 0.006014215179615551\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01093083562536372\n",
      "Average test loss: 0.006310836150828335\n",
      "Epoch 59/300\n",
      "Average training loss: 0.010888028736743662\n",
      "Average test loss: 0.006049677999897135\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010861525366289749\n",
      "Average test loss: 0.006048235999213325\n",
      "Epoch 61/300\n",
      "Average training loss: 0.010855550800346666\n",
      "Average test loss: 0.00600303139951494\n",
      "Epoch 62/300\n",
      "Average training loss: 0.010818535685539246\n",
      "Average test loss: 0.006568192367752393\n",
      "Epoch 63/300\n",
      "Average training loss: 0.010807894722041156\n",
      "Average test loss: 0.006066928155720234\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01077088857938846\n",
      "Average test loss: 0.006135138891223404\n",
      "Epoch 65/300\n",
      "Average training loss: 0.010782331548631192\n",
      "Average test loss: 5.408924491458469\n",
      "Epoch 66/300\n",
      "Average training loss: 0.18014086483253372\n",
      "Average test loss: 0.011607414451738199\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03374173124631246\n",
      "Average test loss: 0.00978718651086092\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02545617859893375\n",
      "Average test loss: 0.008075481855207019\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022122612790928947\n",
      "Average test loss: 0.007546395829568306\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019937991720106865\n",
      "Average test loss: 0.007535905249002907\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01855297985010677\n",
      "Average test loss: 0.00709019318388568\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017419063571426602\n",
      "Average test loss: 0.007047474653356605\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01667619249224663\n",
      "Average test loss: 0.008755958167215189\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016018617861800723\n",
      "Average test loss: 0.006532961339586311\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015304895121190282\n",
      "Average test loss: 0.007017231957366069\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014764097147517734\n",
      "Average test loss: 0.006363655436784029\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014245722276469072\n",
      "Average test loss: 0.006455089006572962\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01374619579480754\n",
      "Average test loss: 0.006359733282277982\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01337913407302565\n",
      "Average test loss: 0.0064155319213039345\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013009555162654982\n",
      "Average test loss: 0.006211563109523721\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012711914975610044\n",
      "Average test loss: 0.006053525256199969\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012445052599741354\n",
      "Average test loss: 0.007084355129135979\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01221024562832382\n",
      "Average test loss: 0.006213838067733579\n",
      "Epoch 84/300\n",
      "Average training loss: 0.011975796206957764\n",
      "Average test loss: 0.005962303575128317\n",
      "Epoch 85/300\n",
      "Average training loss: 0.011787655604382356\n",
      "Average test loss: 0.006016273440586196\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011581307470798493\n",
      "Average test loss: 0.0059632042097962565\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011383702495859729\n",
      "Average test loss: 0.006354734074738291\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011196357059809897\n",
      "Average test loss: 0.006104767757985327\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01108681194153097\n",
      "Average test loss: 0.006111392161498467\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01099091387540102\n",
      "Average test loss: 0.0059866038742992615\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010897635707424747\n",
      "Average test loss: 0.005975464369687769\n",
      "Epoch 92/300\n",
      "Average training loss: 0.010828049793011612\n",
      "Average test loss: 0.005935835729870532\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010802021426459153\n",
      "Average test loss: 0.005957634520199564\n",
      "Epoch 94/300\n",
      "Average training loss: 0.010762407989965545\n",
      "Average test loss: 0.005954841304984358\n",
      "Epoch 95/300\n",
      "Average training loss: 0.010743103862636619\n",
      "Average test loss: 0.006188966671625773\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010695860685573684\n",
      "Average test loss: 0.005990013216932615\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010655106067657471\n",
      "Average test loss: 0.0060145465359091755\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01062468078153001\n",
      "Average test loss: 0.005998353659278817\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010622199138005575\n",
      "Average test loss: 0.006015903306918011\n",
      "Epoch 100/300\n",
      "Average training loss: 0.010589733190834522\n",
      "Average test loss: 0.006029516326470507\n",
      "Epoch 101/300\n",
      "Average training loss: 0.010542339249617524\n",
      "Average test loss: 0.006166104954563909\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010518266076015102\n",
      "Average test loss: 0.005947388471000724\n",
      "Epoch 103/300\n",
      "Average training loss: 0.010476235166192055\n",
      "Average test loss: 0.005974994782358408\n",
      "Epoch 104/300\n",
      "Average training loss: 0.010463408795495828\n",
      "Average test loss: 0.006049209362930722\n",
      "Epoch 105/300\n",
      "Average training loss: 0.010463907552262147\n",
      "Average test loss: 0.006202223690847556\n",
      "Epoch 106/300\n",
      "Average training loss: 0.010413718925582039\n",
      "Average test loss: 0.006222253472854693\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01039146394862069\n",
      "Average test loss: 0.006007080922110213\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01037820982105202\n",
      "Average test loss: 0.006171008863382869\n",
      "Epoch 109/300\n",
      "Average training loss: 0.010378178406920699\n",
      "Average test loss: 0.005966366742634111\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010327351489828692\n",
      "Average test loss: 0.005984095870206753\n",
      "Epoch 111/300\n",
      "Average training loss: 0.010308793033162752\n",
      "Average test loss: 0.006012631122436788\n",
      "Epoch 112/300\n",
      "Average training loss: 0.010294900339510705\n",
      "Average test loss: 0.006055842004716396\n",
      "Epoch 113/300\n",
      "Average training loss: 0.010276079086793793\n",
      "Average test loss: 0.006254172436892986\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01024800198359622\n",
      "Average test loss: 0.006145694671405686\n",
      "Epoch 115/300\n",
      "Average training loss: 0.010257623376117813\n",
      "Average test loss: 0.006022332261419959\n",
      "Epoch 116/300\n",
      "Average training loss: 0.010219331324928336\n",
      "Average test loss: 0.005975815191037125\n",
      "Epoch 117/300\n",
      "Average training loss: 0.010198392688814137\n",
      "Average test loss: 0.006115188089923726\n",
      "Epoch 118/300\n",
      "Average training loss: 0.010180598459310002\n",
      "Average test loss: 0.006141642784906759\n",
      "Epoch 119/300\n",
      "Average training loss: 0.010163548671536975\n",
      "Average test loss: 0.006012564460436503\n",
      "Epoch 120/300\n",
      "Average training loss: 0.010160186626017094\n",
      "Average test loss: 0.006026098137514459\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01014119474382864\n",
      "Average test loss: 0.0060185156568057\n",
      "Epoch 122/300\n",
      "Average training loss: 0.010061092508335909\n",
      "Average test loss: 0.0060107666701078415\n",
      "Epoch 127/300\n",
      "Average training loss: 0.010044893080161677\n",
      "Average test loss: 0.006390118002063698\n",
      "Epoch 128/300\n",
      "Average training loss: 0.010043038355393542\n",
      "Average test loss: 0.005999478694465425\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01001870380093654\n",
      "Average test loss: 0.006119812490625514\n",
      "Epoch 130/300\n",
      "Average training loss: 0.010033542468316026\n",
      "Average test loss: 0.005964317238993115\n",
      "Epoch 131/300\n",
      "Average training loss: 0.010077973543769783\n",
      "Average test loss: 0.006013751372694969\n",
      "Epoch 132/300\n",
      "Average training loss: 0.010016026275025474\n",
      "Average test loss: 0.006092859943707784\n",
      "Epoch 133/300\n",
      "Average training loss: 0.009942963406443597\n",
      "Average test loss: 0.006000019233673811\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009959126744833258\n",
      "Average test loss: 0.006461424894630909\n",
      "Epoch 135/300\n",
      "Average training loss: 0.009942602076464229\n",
      "Average test loss: 0.006148501314636734\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009923171443243822\n",
      "Average test loss: 0.006216102658046617\n",
      "Epoch 137/300\n",
      "Average training loss: 0.009916482695274884\n",
      "Average test loss: 0.006129809063962764\n",
      "Epoch 138/300\n",
      "Average training loss: 0.009908381348682774\n",
      "Average test loss: 0.005980671476158831\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00990999101433489\n",
      "Average test loss: 0.006082462386124664\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00988949602511194\n",
      "Average test loss: 0.006017808724815647\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009874029797812304\n",
      "Average test loss: 0.006071424432512787\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00988198440687524\n",
      "Average test loss: 0.006183111038058996\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0098467856173714\n",
      "Average test loss: 0.006157913810263077\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009843704510894087\n",
      "Average test loss: 0.006071269128471613\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009830561613043149\n",
      "Average test loss: 0.006249488660444816\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009806544279886616\n",
      "Average test loss: 0.006236006924054689\n",
      "Epoch 147/300\n",
      "Average training loss: 0.009790260420077378\n",
      "Average test loss: 0.005999962606777747\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009809371242092715\n",
      "Average test loss: 0.006188445724546909\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009811400445799033\n",
      "Average test loss: 0.006076345210687981\n",
      "Epoch 150/300\n",
      "Average training loss: 0.009784064523047871\n",
      "Average test loss: 0.0060218481839531\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009788491681218148\n",
      "Average test loss: 0.007039374782393376\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009762225768632359\n",
      "Average test loss: 0.0061085150804784565\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009721703880363041\n",
      "Average test loss: 0.006149708324008518\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009740586458808846\n",
      "Average test loss: 0.006209889847371313\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009724181806047758\n",
      "Average test loss: 0.00597691716791855\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009695883189224534\n",
      "Average test loss: 0.006253491605942448\n",
      "Epoch 157/300\n",
      "Average training loss: 0.009727796658873559\n",
      "Average test loss: 0.005987752473809653\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00971792132821348\n",
      "Average test loss: 0.006075803857710627\n",
      "Epoch 159/300\n",
      "Average training loss: 0.009660362895164224\n",
      "Average test loss: 0.006044247912449969\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009669329252507951\n",
      "Average test loss: 0.006722816856371032\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009667659283512169\n",
      "Average test loss: 0.006113490694512924\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009647294277118312\n",
      "Average test loss: 0.006070161542130841\n",
      "Epoch 163/300\n",
      "Average training loss: 0.009665136805839008\n",
      "Average test loss: 0.006604250142971674\n",
      "Epoch 164/300\n",
      "Average training loss: 0.009645875686572658\n",
      "Average test loss: 0.006180962548487716\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009671070885327128\n",
      "Average test loss: 0.006063017946150568\n",
      "Epoch 166/300\n",
      "Average training loss: 0.009598885513014263\n",
      "Average test loss: 0.006132680879284938\n",
      "Epoch 167/300\n",
      "Average training loss: 0.009611612860527304\n",
      "Average test loss: 0.006144405847622289\n",
      "Epoch 168/300\n",
      "Average training loss: 0.00956368791560332\n",
      "Average test loss: 0.006116873203466336\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009568746557666196\n",
      "Average test loss: 0.006272514502207438\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00956314715825849\n",
      "Average test loss: 0.006081066246247954\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009541664621896213\n",
      "Average test loss: 0.012015682973795468\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009541021645069122\n",
      "Average test loss: 0.006124618752963013\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009532673559668992\n",
      "Average test loss: 0.006833921447603239\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009523710537287924\n",
      "Average test loss: 0.006358795669343736\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009527707936863104\n",
      "Average test loss: 0.006371194059650104\n",
      "Epoch 181/300\n",
      "Average training loss: 0.009509948508607017\n",
      "Average test loss: 0.0060979943813549145\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009506008675528897\n",
      "Average test loss: 0.00617618218726582\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009487793347073926\n",
      "Average test loss: 0.006034023247866167\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00947478112909529\n",
      "Average test loss: 0.006846212491807011\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009491480660107401\n",
      "Average test loss: 0.006208229655192958\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009477127408815754\n",
      "Average test loss: 0.006120422653026051\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009457879016796749\n",
      "Average test loss: 0.0062356089038981335\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009467580690152116\n",
      "Average test loss: 0.006199495700912343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009438010200858117\n",
      "Average test loss: 0.006163358795146148\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00944375313901239\n",
      "Average test loss: 0.006084952785736984\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009439325837625398\n",
      "Average test loss: 0.006059015752540695\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009441447576714886\n",
      "Average test loss: 0.006297518740925524\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009436579764717155\n",
      "Average test loss: 0.006113013576302263\n",
      "Epoch 194/300\n",
      "Average training loss: 0.009416360547145208\n",
      "Average test loss: 0.006151834811601374\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009404220419625442\n",
      "Average test loss: 0.006114229015385111\n",
      "Epoch 196/300\n",
      "Average training loss: 0.009386415358218882\n",
      "Average test loss: 0.006224155057635572\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009421978637576104\n",
      "Average test loss: 0.006117296030537949\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009386964166329967\n",
      "Average test loss: 0.0070282834461993645\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009386400619314776\n",
      "Average test loss: 0.006308103936413924\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009397910042769379\n",
      "Average test loss: 0.006464927681618267\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009387090660631656\n",
      "Average test loss: 0.0061783223632309175\n",
      "Epoch 202/300\n",
      "Average training loss: 0.009370324694448047\n",
      "Average test loss: 0.0061230915333661765\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009354535029994117\n",
      "Average test loss: 0.006184323155217701\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009373565334412786\n",
      "Average test loss: 0.006459049467411306\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009365843389597204\n",
      "Average test loss: 0.00618040194031265\n",
      "Epoch 206/300\n",
      "Average training loss: 0.009337624341249465\n",
      "Average test loss: 0.006075590669280953\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009350133448839188\n",
      "Average test loss: 0.006681101454628838\n",
      "Epoch 208/300\n",
      "Average training loss: 0.00933951965553893\n",
      "Average test loss: 0.006781522628747754\n",
      "Epoch 209/300\n",
      "Average training loss: 0.009338254890508121\n",
      "Average test loss: 0.006087944125549661\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009310244570175806\n",
      "Average test loss: 0.006629878624859783\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009319654975500372\n",
      "Average test loss: 0.006288075714061658\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00932112073732747\n",
      "Average test loss: 0.006115760725405481\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00930237414356735\n",
      "Average test loss: 0.006373626919256316\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0093192445859313\n",
      "Average test loss: 0.006264261120309433\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009294264120774137\n",
      "Average test loss: 0.006207196826736133\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009307800803747442\n",
      "Average test loss: 0.00632888882731398\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009272405386384991\n",
      "Average test loss: 0.006270808080004321\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009298055809405115\n",
      "Average test loss: 0.006146911602467299\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009285743334227138\n",
      "Average test loss: 0.006181886969341172\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009267196062538358\n",
      "Average test loss: 0.006258084726416402\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00926101746244563\n",
      "Average test loss: 0.006219646113614241\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009264334403806262\n",
      "Average test loss: 0.007031854453393154\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009260893303073114\n",
      "Average test loss: 0.006281580940716796\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00923926277210315\n",
      "Average test loss: 0.00629343830794096\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009258772554496924\n",
      "Average test loss: 0.006147957314633661\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009255825551019774\n",
      "Average test loss: 0.006779967892501089\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009260525928603278\n",
      "Average test loss: 0.0066516674550043215\n",
      "Epoch 228/300\n",
      "Average training loss: 0.009222139110995663\n",
      "Average test loss: 0.006175144172790977\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009209712564945221\n",
      "Average test loss: 0.006152738350960943\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00922904589606656\n",
      "Average test loss: 0.006145102919803726\n",
      "Epoch 231/300\n",
      "Average training loss: 0.009221330912576781\n",
      "Average test loss: 0.006246549227999316\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00925303233994378\n",
      "Average test loss: 0.006246811761624283\n",
      "Epoch 233/300\n",
      "Average training loss: 0.009210920996963978\n",
      "Average test loss: 0.006210745195961661\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009204674255516795\n",
      "Average test loss: 0.006306441172129578\n",
      "Epoch 235/300\n",
      "Average training loss: 0.009222018542389075\n",
      "Average test loss: 0.006075333861427175\n",
      "Epoch 236/300\n",
      "Average training loss: 0.009221520057982869\n",
      "Average test loss: 0.006042715632253223\n",
      "Epoch 237/300\n",
      "Average training loss: 0.009196706397665871\n",
      "Average test loss: 0.006364310882157749\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009201444448696243\n",
      "Average test loss: 0.006648693127764596\n",
      "Epoch 239/300\n",
      "Average training loss: 0.009188197491897476\n",
      "Average test loss: 0.006263149335152573\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009173709724512365\n",
      "Average test loss: 0.006180251023835606\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009175876912143496\n",
      "Average test loss: 0.00619209112930629\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009189444010456403\n",
      "Average test loss: 0.006342686445762713\n",
      "Epoch 243/300\n",
      "Average training loss: 0.009177673255403836\n",
      "Average test loss: 0.006405885780437125\n",
      "Epoch 244/300\n",
      "Average training loss: 0.009141600291762086\n",
      "Average test loss: 0.006171717754254739\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009140865580489238\n",
      "Average test loss: 0.006258296154853371\n",
      "Epoch 246/300\n",
      "Average training loss: 0.009159638092335727\n",
      "Average test loss: 0.006173687607463863\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009170285877254275\n",
      "Average test loss: 0.0065248155378633075\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009146779023110866\n",
      "Average test loss: 0.0062640477741758025\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009141986725231012\n",
      "Average test loss: 0.006190177374415928\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009138209863669342\n",
      "Average test loss: 0.006199715539813042\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009130885275701682\n",
      "Average test loss: 0.006345992580884033\n",
      "Epoch 252/300\n",
      "Average training loss: 0.009136371523969703\n",
      "Average test loss: 0.006187537009103431\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00912260363996029\n",
      "Average test loss: 0.006244144803533951\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00913312340196636\n",
      "Average test loss: 0.006158626846969128\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00911633187946346\n",
      "Average test loss: 0.006240294770648082\n",
      "Epoch 256/300\n",
      "Average training loss: 0.009090923932691414\n",
      "Average test loss: 0.006172555857234531\n",
      "Epoch 257/300\n",
      "Average training loss: 0.009136578116152021\n",
      "Average test loss: 0.006340069393730826\n",
      "Epoch 258/300\n",
      "Average training loss: 0.009121738581193818\n",
      "Average test loss: 0.0064412606995966695\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009067999919255575\n",
      "Average test loss: 0.006314422091262208\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009095866853992144\n",
      "Average test loss: 0.006236353708638085\n",
      "Epoch 261/300\n",
      "Average training loss: 0.009113483582105902\n",
      "Average test loss: 0.006112670057763656\n",
      "Epoch 262/300\n",
      "Average training loss: 0.009097344780961672\n",
      "Average test loss: 0.006657170411613253\n",
      "Epoch 263/300\n",
      "Average training loss: 0.009071476154857212\n",
      "Average test loss: 0.006580755845126179\n",
      "Epoch 264/300\n",
      "Average training loss: 0.009101532605787119\n",
      "Average test loss: 0.006507567594862647\n",
      "Epoch 265/300\n",
      "Average training loss: 0.009080643753210703\n",
      "Average test loss: 0.006212762506885661\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00907407832559612\n",
      "Average test loss: 0.006253915639801158\n",
      "Epoch 267/300\n",
      "Average training loss: 0.009081375675068962\n",
      "Average test loss: 0.006183636408299208\n",
      "Epoch 268/300\n",
      "Average training loss: 0.009092910945415497\n",
      "Average test loss: 0.00638243797628416\n",
      "Epoch 269/300\n",
      "Average training loss: 0.009077194559077422\n",
      "Average test loss: 0.006154236500461896\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00904667527973652\n",
      "Average test loss: 0.00628848912111587\n",
      "Epoch 271/300\n",
      "Average training loss: 0.009060441410375967\n",
      "Average test loss: 0.006138576590352589\n",
      "Epoch 272/300\n",
      "Average training loss: 0.009046559142569702\n",
      "Average test loss: 0.006325822337427073\n",
      "Epoch 273/300\n",
      "Average training loss: 0.009053255499237113\n",
      "Average test loss: 0.006292970077031189\n",
      "Epoch 274/300\n",
      "Average training loss: 0.009033110280831655\n",
      "Average test loss: 0.006192819131331312\n",
      "Epoch 275/300\n",
      "Average training loss: 0.009044146604008145\n",
      "Average test loss: 0.007381849567923281\n",
      "Epoch 276/300\n",
      "Average training loss: 0.009036790060500304\n",
      "Average test loss: 0.006433684944278664\n",
      "Epoch 277/300\n",
      "Average training loss: 0.009028783260948128\n",
      "Average test loss: 0.006816310728589694\n",
      "Epoch 278/300\n",
      "Average training loss: 0.009049447543919087\n",
      "Average test loss: 0.006291417664537827\n",
      "Epoch 279/300\n",
      "Average training loss: 0.009040072283811039\n",
      "Average test loss: 0.006487118028104305\n",
      "Epoch 280/300\n",
      "Average training loss: 0.009031400179697408\n",
      "Average test loss: 0.006131537297947539\n",
      "Epoch 281/300\n",
      "Average training loss: 0.009009745663238896\n",
      "Average test loss: 0.006279034451891979\n",
      "Epoch 282/300\n",
      "Average training loss: 0.009014023925695154\n",
      "Average test loss: 0.006273321126070287\n",
      "Epoch 283/300\n",
      "Average training loss: 0.009018661355806722\n",
      "Average test loss: 0.006232363714940018\n",
      "Epoch 284/300\n",
      "Average training loss: 0.009019746022919814\n",
      "Average test loss: 0.0063233454649647075\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008990935924980376\n",
      "Average test loss: 0.006507146631677945\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008996438082721499\n",
      "Average test loss: 0.006150439757025904\n",
      "Epoch 287/300\n",
      "Average training loss: 0.009028223680125342\n",
      "Average test loss: 0.006419021588646703\n",
      "Epoch 288/300\n",
      "Average training loss: 0.00898928157819642\n",
      "Average test loss: 0.006300473892026477\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008995737777402004\n",
      "Average test loss: 0.006297882996499539\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008972495551738475\n",
      "Average test loss: 0.0062110588625073435\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008979235041472647\n",
      "Average test loss: 0.006211127867301305\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00900234262396892\n",
      "Average test loss: 0.006355312281184727\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008989123799734645\n",
      "Average test loss: 0.006449528154813581\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008976063416235977\n",
      "Average test loss: 0.0073241009335551\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008985840658346811\n",
      "Average test loss: 0.006250451182325681\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00899492066891657\n",
      "Average test loss: 0.006555890754279163\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008963080562651157\n",
      "Average test loss: 0.006520296075691779\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008958340969350604\n",
      "Average test loss: 0.006307557216121091\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008960458067556222\n",
      "Average test loss: 0.00624988631821341\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008992297531002098\n",
      "Average test loss: 0.006288790550082922\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05374894478751553\n",
      "Average test loss: 0.007744575921032164\n",
      "Epoch 2/300\n",
      "Average training loss: 0.017206129992173778\n",
      "Average test loss: 0.00851467397974597\n",
      "Epoch 3/300\n",
      "Average training loss: 0.015153879152403937\n",
      "Average test loss: 0.006482417655487856\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013894022036757735\n",
      "Average test loss: 0.005716470633529955\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012973325507508383\n",
      "Average test loss: 0.005277584335870213\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012304375220504073\n",
      "Average test loss: 0.005489277875257863\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011663083831469218\n",
      "Average test loss: 0.005729230637765593\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011130974053508706\n",
      "Average test loss: 0.005090215266578727\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0108141211917003\n",
      "Average test loss: 0.004816319981796874\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010376494571566582\n",
      "Average test loss: 0.004679975303717786\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010072796586487028\n",
      "Average test loss: 0.004562505537644029\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009821361491249667\n",
      "Average test loss: 0.004546791481475036\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009539066210389137\n",
      "Average test loss: 0.004359719910141495\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009377911214199331\n",
      "Average test loss: 0.004435283514360587\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009144460478590594\n",
      "Average test loss: 0.004341478926233119\n",
      "Epoch 16/300\n",
      "Average training loss: 0.008998181675871214\n",
      "Average test loss: 0.004223898093319601\n",
      "Epoch 17/300\n",
      "Average training loss: 0.008824136002196207\n",
      "Average test loss: 0.004171542129375868\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00866632085873021\n",
      "Average test loss: 0.004013253495511081\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008536607211248742\n",
      "Average test loss: 0.004552202204035388\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008405029624700546\n",
      "Average test loss: 0.003933651510212156\n",
      "Epoch 21/300\n",
      "Average training loss: 0.008314705844968557\n",
      "Average test loss: 0.003973148239776492\n",
      "Epoch 22/300\n",
      "Average training loss: 0.008210303359561496\n",
      "Average test loss: 0.003846211345659362\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008091100106222762\n",
      "Average test loss: 0.0038099975996754238\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008010704478455914\n",
      "Average test loss: 0.003778215970016188\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007910367663535808\n",
      "Average test loss: 0.003827399503439665\n",
      "Epoch 26/300\n",
      "Average training loss: 0.007842466634180811\n",
      "Average test loss: 0.0037290178516672717\n",
      "Epoch 27/300\n",
      "Average training loss: 0.007773401461541653\n",
      "Average test loss: 0.0037222820305162006\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007727378745045927\n",
      "Average test loss: 0.004080439061133398\n",
      "Epoch 29/300\n",
      "Average training loss: 0.007633965192155706\n",
      "Average test loss: 0.003763966334362825\n",
      "Epoch 30/300\n",
      "Average training loss: 0.007591944274803003\n",
      "Average test loss: 0.0036348365183091827\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007540356094638507\n",
      "Average test loss: 0.0036293397948352828\n",
      "Epoch 32/300\n",
      "Average training loss: 0.007487291122890181\n",
      "Average test loss: 0.0036106322227666774\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007451022492100795\n",
      "Average test loss: 0.0037303379196673633\n",
      "Epoch 34/300\n",
      "Average training loss: 0.007398834940046072\n",
      "Average test loss: 0.0037023600842803718\n",
      "Epoch 35/300\n",
      "Average training loss: 0.007365985765225357\n",
      "Average test loss: 0.003590647183979551\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007323073895441162\n",
      "Average test loss: 0.004385640529087848\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0072776712075703675\n",
      "Average test loss: 0.003552815216903885\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0072482677867843045\n",
      "Average test loss: 0.003732624223248826\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007215928783433305\n",
      "Average test loss: 0.0036002541335506573\n",
      "Epoch 40/300\n",
      "Average training loss: 0.007192691645688481\n",
      "Average test loss: 0.0036579059118198023\n",
      "Epoch 41/300\n",
      "Average training loss: 0.007161376348800129\n",
      "Average test loss: 0.0036130131019486323\n",
      "Epoch 42/300\n",
      "Average training loss: 0.007129104408952925\n",
      "Average test loss: 0.0036106222859687274\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007098270112027724\n",
      "Average test loss: 0.003542292791936133\n",
      "Epoch 44/300\n",
      "Average training loss: 0.007090267945908838\n",
      "Average test loss: 0.003665580781383647\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007053754753950569\n",
      "Average test loss: 0.0035088754517750608\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007017358022845453\n",
      "Average test loss: 0.003491343095070786\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007012191430975994\n",
      "Average test loss: 0.0035520776816540295\n",
      "Epoch 48/300\n",
      "Average training loss: 0.006985833222253455\n",
      "Average test loss: 0.0035408993417190182\n",
      "Epoch 49/300\n",
      "Average training loss: 0.006976278043869469\n",
      "Average test loss: 0.0034877138862179384\n",
      "Epoch 50/300\n",
      "Average training loss: 0.006938040158814854\n",
      "Average test loss: 0.003603260975331068\n",
      "Epoch 51/300\n",
      "Average training loss: 0.006927407316449616\n",
      "Average test loss: 0.003474156084987852\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00692501600086689\n",
      "Average test loss: 0.00353934490101205\n",
      "Epoch 53/300\n",
      "Average training loss: 0.006924444944080379\n",
      "Average test loss: 0.003497552326362994\n",
      "Epoch 54/300\n",
      "Average training loss: 0.006886332604620192\n",
      "Average test loss: 0.0035612992884384263\n",
      "Epoch 55/300\n",
      "Average training loss: 0.006852792296144697\n",
      "Average test loss: 0.003557527294796374\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006847595593581597\n",
      "Average test loss: 0.004571677586684624\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0068394787216352095\n",
      "Average test loss: 0.0035458124321368005\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00679421953484416\n",
      "Average test loss: 0.0034888800440563096\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0067926216680142615\n",
      "Average test loss: 0.0036161248410741488\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0067818086544672645\n",
      "Average test loss: 0.0035754732791748313\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006774803872323699\n",
      "Average test loss: 0.003535354228069385\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006782469133536021\n",
      "Average test loss: 0.0034640668452613882\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0067563268380860485\n",
      "Average test loss: 0.003990105921609534\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006776367240895828\n",
      "Average test loss: 0.0035118821822106836\n",
      "Epoch 65/300\n",
      "Average training loss: 0.006682289113187127\n",
      "Average test loss: 0.0034515928452213606\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0067232817990912335\n",
      "Average test loss: 0.003416249530389905\n",
      "Epoch 67/300\n",
      "Average training loss: 0.006684946414497164\n",
      "Average test loss: 0.0035287405140697957\n",
      "Epoch 68/300\n",
      "Average training loss: 0.006676205907844835\n",
      "Average test loss: 0.003452419927550687\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006671973024391466\n",
      "Average test loss: 0.0038411111214922532\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0066458225904239545\n",
      "Average test loss: 0.0040622042914231615\n",
      "Epoch 71/300\n",
      "Average training loss: 0.006639463922629754\n",
      "Average test loss: 0.0035315999885400136\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0066482565390566985\n",
      "Average test loss: 0.0034235195401642058\n",
      "Epoch 73/300\n",
      "Average training loss: 0.006607992556773954\n",
      "Average test loss: 0.004234495501137442\n",
      "Epoch 74/300\n",
      "Average training loss: 0.006603533028728432\n",
      "Average test loss: 0.003543815799057484\n",
      "Epoch 75/300\n",
      "Average training loss: 0.006590635496709082\n",
      "Average test loss: 0.0034866265983631213\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00660792700201273\n",
      "Average test loss: 0.003711300599699219\n",
      "Epoch 77/300\n",
      "Average training loss: 0.006576544585327308\n",
      "Average test loss: 0.0034897409634043774\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006575077152086629\n",
      "Average test loss: 0.003429676975020104\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006595046188475357\n",
      "Average test loss: 0.003555855255160067\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0065684602066046665\n",
      "Average test loss: 0.0035040235473877855\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006541892948663897\n",
      "Average test loss: 0.003539353099341194\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006523379462874598\n",
      "Average test loss: 0.0034674221985042095\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006534497062365214\n",
      "Average test loss: 0.0034872642832083833\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006527041261394819\n",
      "Average test loss: 0.0034956438704911207\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006511393075188001\n",
      "Average test loss: 0.0035222668531868194\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006498313348740339\n",
      "Average test loss: 0.0036856847457173796\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006484221429046658\n",
      "Average test loss: 0.0035164845060143206\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006480751901451085\n",
      "Average test loss: 0.0035257824721435706\n",
      "Epoch 89/300\n",
      "Average training loss: 0.006478130191978481\n",
      "Average test loss: 0.0035135523531999854\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0064498104351676175\n",
      "Average test loss: 0.0035478589431279235\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006464569314072529\n",
      "Average test loss: 0.003517213906471928\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006444418927033743\n",
      "Average test loss: 0.003477382199217876\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0064455881727238495\n",
      "Average test loss: 0.0034930329364207054\n",
      "Epoch 94/300\n",
      "Average training loss: 0.006446192084915108\n",
      "Average test loss: 0.003777318566623661\n",
      "Epoch 95/300\n",
      "Average training loss: 0.006426511057962974\n",
      "Average test loss: 0.0034842732722560566\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006426512930956152\n",
      "Average test loss: 0.0034972261985143026\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0064090580398009885\n",
      "Average test loss: 0.0036642812142769496\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006418059001366297\n",
      "Average test loss: 0.0034813724437521565\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006412620652467013\n",
      "Average test loss: 0.003471216811488072\n",
      "Epoch 100/300\n",
      "Average training loss: 0.006386161196562979\n",
      "Average test loss: 0.003467719058609671\n",
      "Epoch 101/300\n",
      "Average training loss: 0.006381257524506913\n",
      "Average test loss: 0.0035479750142743192\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006380322242362632\n",
      "Average test loss: 0.0035225235786702897\n",
      "Epoch 103/300\n",
      "Average training loss: 0.006377070220394267\n",
      "Average test loss: 0.0035417863643831676\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006361675502525435\n",
      "Average test loss: 0.0035067763270603285\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006372224139670531\n",
      "Average test loss: 0.0036418501877536376\n",
      "Epoch 106/300\n",
      "Average training loss: 0.006351256100253926\n",
      "Average test loss: 0.003423097704640693\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006336669584943188\n",
      "Average test loss: 0.003713113888270325\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006357217439346844\n",
      "Average test loss: 0.0034387867624560992\n",
      "Epoch 109/300\n",
      "Average training loss: 0.006384848643922143\n",
      "Average test loss: 0.003439336341081394\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006325500918345319\n",
      "Average test loss: 0.0034312262253628836\n",
      "Epoch 111/300\n",
      "Average training loss: 0.006330056901607249\n",
      "Average test loss: 0.0034812139773534403\n",
      "Epoch 112/300\n",
      "Average training loss: 0.006311933751735423\n",
      "Average test loss: 0.003639432993820972\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0063294557419915996\n",
      "Average test loss: 0.0036344258168505296\n",
      "Epoch 114/300\n",
      "Average training loss: 0.006290015186700556\n",
      "Average test loss: 0.0036464307370285194\n",
      "Epoch 115/300\n",
      "Average training loss: 0.006315766981906361\n",
      "Average test loss: 0.003495261742009057\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006297258231788874\n",
      "Average test loss: 0.0034873161274525852\n",
      "Epoch 117/300\n",
      "Average training loss: 0.006290081299013562\n",
      "Average test loss: 0.0035369570863743624\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006271030118895901\n",
      "Average test loss: 0.003547587046606673\n",
      "Epoch 119/300\n",
      "Average training loss: 0.006276703959951798\n",
      "Average test loss: 0.0034734076716833643\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0062870161856214204\n",
      "Average test loss: 0.003487619827191035\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006277074540240897\n",
      "Average test loss: 0.003563633383769128\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006251228873514467\n",
      "Average test loss: 0.0037869327026936743\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006252242138402329\n",
      "Average test loss: 0.003660217706941896\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006241176197098361\n",
      "Average test loss: 0.003428987451725536\n",
      "Epoch 125/300\n",
      "Average training loss: 0.006250804745902618\n",
      "Average test loss: 0.0034798059219287502\n",
      "Epoch 126/300\n",
      "Average training loss: 0.006247786189119021\n",
      "Average test loss: 0.0035753257055249478\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006243183731618855\n",
      "Average test loss: 0.0035698698630763424\n",
      "Epoch 128/300\n",
      "Average training loss: 0.006235573355522421\n",
      "Average test loss: 0.0035833138393031225\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0062144456182916955\n",
      "Average test loss: 0.003545269828910629\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006219537000689242\n",
      "Average test loss: 0.003534147816399733\n",
      "Epoch 131/300\n",
      "Average training loss: 0.006204443123191595\n",
      "Average test loss: 0.003577543043014076\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006218092937022447\n",
      "Average test loss: 0.003505777184334066\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006214211578170458\n",
      "Average test loss: 0.00357714048648874\n",
      "Epoch 134/300\n",
      "Average training loss: 0.006205470314456357\n",
      "Average test loss: 0.003488012242855297\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0061944628482063615\n",
      "Average test loss: 0.0036502636907001338\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0061790779456496235\n",
      "Average test loss: 0.0039323514393634264\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006193474260469278\n",
      "Average test loss: 0.0035778734998570547\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006177498263617356\n",
      "Average test loss: 0.0035135365174048475\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006191058911797073\n",
      "Average test loss: 0.0038317246519856984\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006181575648486614\n",
      "Average test loss: 0.003951769758429792\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006172570214917262\n",
      "Average test loss: 0.0035971353734946913\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006169021729379893\n",
      "Average test loss: 0.23308188493384255\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006366548772487376\n",
      "Average test loss: 0.003483065873591436\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0061292434355451\n",
      "Average test loss: 0.0034470060562921895\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006130089462217357\n",
      "Average test loss: 0.003527142118662596\n",
      "Epoch 146/300\n",
      "Average training loss: 0.006140781459709008\n",
      "Average test loss: 0.003520678137532539\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006134931713756587\n",
      "Average test loss: 0.0035069206222477886\n",
      "Epoch 148/300\n",
      "Average training loss: 0.006178606836332215\n",
      "Average test loss: 0.004299219393895732\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0061316612176597115\n",
      "Average test loss: 0.0035345072437905603\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006124185145729118\n",
      "Average test loss: 0.003598895388758845\n",
      "Epoch 151/300\n",
      "Average training loss: 0.00614238664549258\n",
      "Average test loss: 0.003517578265733189\n",
      "Epoch 152/300\n",
      "Average training loss: 0.006110535264429119\n",
      "Average test loss: 0.0034700533202331927\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006123985696583986\n",
      "Average test loss: 0.003502858734379212\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006111651244262854\n",
      "Average test loss: 0.003666812072197596\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006115690497888459\n",
      "Average test loss: 0.003513253299312459\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0061098362799319956\n",
      "Average test loss: 0.0036298887497848934\n",
      "Epoch 157/300\n",
      "Average training loss: 0.006123344988872607\n",
      "Average test loss: 0.0036169070777379805\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00609846851684981\n",
      "Average test loss: 0.0035140341222286223\n",
      "Epoch 159/300\n",
      "Average training loss: 0.006097345959809091\n",
      "Average test loss: 0.003494527355974747\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0060986907713943056\n",
      "Average test loss: 0.003698971373339494\n",
      "Epoch 161/300\n",
      "Average training loss: 0.006103137522108025\n",
      "Average test loss: 0.003581645791729291\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006070345859974622\n",
      "Average test loss: 0.003474282236976756\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00609662862163451\n",
      "Average test loss: 0.0035187226105481387\n",
      "Epoch 164/300\n",
      "Average training loss: 0.006101805561118656\n",
      "Average test loss: 0.0035346137980620065\n",
      "Epoch 165/300\n",
      "Average training loss: 0.006086084881176551\n",
      "Average test loss: 0.0038956534632792078\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0060921032283869055\n",
      "Average test loss: 0.004005795081870423\n",
      "Epoch 167/300\n",
      "Average training loss: 0.006064568056828446\n",
      "Average test loss: 0.0036447993456903433\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006052424698654148\n",
      "Average test loss: 0.003601995011170705\n",
      "Epoch 169/300\n",
      "Average training loss: 0.006064204792181651\n",
      "Average test loss: 0.0035488580415646235\n",
      "Epoch 170/300\n",
      "Average training loss: 0.006062365092337131\n",
      "Average test loss: 0.0035249989061719843\n",
      "Epoch 171/300\n",
      "Average training loss: 0.006048753046327167\n",
      "Average test loss: 0.0034765867778203553\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006059925237877501\n",
      "Average test loss: 0.0035601362155543432\n",
      "Epoch 173/300\n",
      "Average training loss: 0.006053433527135187\n",
      "Average test loss: 0.0036130145750939845\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006060728254417579\n",
      "Average test loss: 0.0036738467841512627\n",
      "Epoch 175/300\n",
      "Average training loss: 0.006051433520598544\n",
      "Average test loss: 0.0035968103801003763\n",
      "Epoch 176/300\n",
      "Average training loss: 0.006024009054733648\n",
      "Average test loss: 0.003593724290115966\n",
      "Epoch 177/300\n",
      "Average training loss: 0.006036191929959588\n",
      "Average test loss: 0.003625794502182139\n",
      "Epoch 178/300\n",
      "Average training loss: 0.006034632168710231\n",
      "Average test loss: 0.003664666349068284\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0060493169029553736\n",
      "Average test loss: 0.0035984923276636336\n",
      "Epoch 180/300\n",
      "Average training loss: 0.006026588750382264\n",
      "Average test loss: 0.0036335556144929592\n",
      "Epoch 181/300\n",
      "Average training loss: 0.006019462107784218\n",
      "Average test loss: 0.003664095339261823\n",
      "Epoch 182/300\n",
      "Average training loss: 0.006006639786064625\n",
      "Average test loss: 0.003637275432960855\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006025019601815277\n",
      "Average test loss: 0.003609064216415087\n",
      "Epoch 184/300\n",
      "Average training loss: 0.006023476883355115\n",
      "Average test loss: 0.003586512275454071\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006011238211972846\n",
      "Average test loss: 0.003579527664101786\n",
      "Epoch 186/300\n",
      "Average training loss: 0.006007773787611061\n",
      "Average test loss: 0.003745962707946698\n",
      "Epoch 187/300\n",
      "Average training loss: 0.006002262160181999\n",
      "Average test loss: 0.0036387071228689617\n",
      "Epoch 188/300\n",
      "Average training loss: 0.00601384767351879\n",
      "Average test loss: 0.004421541860327125\n",
      "Epoch 189/300\n",
      "Average training loss: 0.005994889788329601\n",
      "Average test loss: 0.0035855916322519383\n",
      "Epoch 190/300\n",
      "Average training loss: 0.005991659340345197\n",
      "Average test loss: 0.0034989655239300596\n",
      "Epoch 191/300\n",
      "Average training loss: 0.005996395262993045\n",
      "Average test loss: 0.0035985868159267636\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0059879996917314\n",
      "Average test loss: 0.0035103156152698728\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00599512240373426\n",
      "Average test loss: 0.003609293402896987\n",
      "Epoch 194/300\n",
      "Average training loss: 0.005981250041474899\n",
      "Average test loss: 0.003592728280979726\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005992606727613343\n",
      "Average test loss: 0.003577963892577423\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005969536119451125\n",
      "Average test loss: 0.0035822047255933283\n",
      "Epoch 197/300\n",
      "Average training loss: 0.005979833168288072\n",
      "Average test loss: 0.0035817893656591576\n",
      "Epoch 198/300\n",
      "Average training loss: 0.006001964918855164\n",
      "Average test loss: 0.0035767194980548486\n",
      "Epoch 199/300\n",
      "Average training loss: 0.005974812908305063\n",
      "Average test loss: 0.003686036912931336\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0059590354400376475\n",
      "Average test loss: 0.0035176333592583735\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0059685210941566366\n",
      "Average test loss: 0.0034816225978235404\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00597254319406218\n",
      "Average test loss: 0.0035205829723013774\n",
      "Epoch 203/300\n",
      "Average training loss: 0.005965619862493541\n",
      "Average test loss: 0.003623129677441385\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005965283513069153\n",
      "Average test loss: 0.0037060195609099336\n",
      "Epoch 205/300\n",
      "Average training loss: 0.005950887316217025\n",
      "Average test loss: 0.003591801497257418\n",
      "Epoch 206/300\n",
      "Average training loss: 0.005966951413287057\n",
      "Average test loss: 0.0037130948570039536\n",
      "Epoch 207/300\n",
      "Average training loss: 0.005946139805846744\n",
      "Average test loss: 0.003509489985182881\n",
      "Epoch 208/300\n",
      "Average training loss: 0.005941271525704199\n",
      "Average test loss: 0.10197655773825115\n",
      "Epoch 209/300\n",
      "Average training loss: 0.005953328471630812\n",
      "Average test loss: 0.0035592224566886824\n",
      "Epoch 210/300\n",
      "Average training loss: 0.005943027439630694\n",
      "Average test loss: 0.0034666743425445425\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0059518654814196955\n",
      "Average test loss: 0.0035862001292407512\n",
      "Epoch 212/300\n",
      "Average training loss: 0.005944578981647889\n",
      "Average test loss: 0.003706001105200913\n",
      "Epoch 213/300\n",
      "Average training loss: 0.005943272575736046\n",
      "Average test loss: 0.0035411488455202845\n",
      "Epoch 214/300\n",
      "Average training loss: 0.005934932107312812\n",
      "Average test loss: 0.0035274351177116236\n",
      "Epoch 215/300\n",
      "Average training loss: 0.005931215717560715\n",
      "Average test loss: 0.0038114256360050705\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005940756337924136\n",
      "Average test loss: 0.003662766842999392\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005919267674287161\n",
      "Average test loss: 0.0037993603675729698\n",
      "Epoch 218/300\n",
      "Average training loss: 0.005926427291499244\n",
      "Average test loss: 0.0035861966943161357\n",
      "Epoch 219/300\n",
      "Average training loss: 0.005922257986747556\n",
      "Average test loss: 0.003526690000254247\n",
      "Epoch 220/300\n",
      "Average training loss: 0.005914137967758708\n",
      "Average test loss: 0.0034717953101628357\n",
      "Epoch 221/300\n",
      "Average training loss: 0.005927003860059712\n",
      "Average test loss: 0.0035036248051457937\n",
      "Epoch 222/300\n",
      "Average training loss: 0.005931499909609556\n",
      "Average test loss: 0.004310540615270535\n",
      "Epoch 223/300\n",
      "Average training loss: 0.005902563101301591\n",
      "Average test loss: 0.0038067101581643024\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0059093967632701\n",
      "Average test loss: 0.0035543927227457365\n",
      "Epoch 225/300\n",
      "Average training loss: 0.005897156567209297\n",
      "Average test loss: 0.003563323114481237\n",
      "Epoch 226/300\n",
      "Average training loss: 0.005916063694490326\n",
      "Average test loss: 0.0038469608268804022\n",
      "Epoch 227/300\n",
      "Average training loss: 0.005930788754175106\n",
      "Average test loss: 0.003691336451512244\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0059044985427624645\n",
      "Average test loss: 0.0039872863876322905\n",
      "Epoch 229/300\n",
      "Average training loss: 0.005905081030809217\n",
      "Average test loss: 0.003522945752988259\n",
      "Epoch 230/300\n",
      "Average training loss: 0.005884375773370266\n",
      "Average test loss: 0.0035065688267350197\n",
      "Epoch 231/300\n",
      "Average training loss: 0.00590077449215783\n",
      "Average test loss: 0.003768621339566178\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0059035244811740184\n",
      "Average test loss: 0.0035986572108748886\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0058824982539647155\n",
      "Average test loss: 0.0036328313731484943\n",
      "Epoch 234/300\n",
      "Average training loss: 0.005889157101098034\n",
      "Average test loss: 0.0036108076746265093\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00587743823023306\n",
      "Average test loss: 0.004099924964623319\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0058715456450978915\n",
      "Average test loss: 0.0037737822669247786\n",
      "Epoch 237/300\n",
      "Average training loss: 0.005905329788310661\n",
      "Average test loss: 0.003594955446612504\n",
      "Epoch 238/300\n",
      "Average training loss: 0.005858678667081727\n",
      "Average test loss: 0.0038578066155314445\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0058768984592623185\n",
      "Average test loss: 0.00363935150206089\n",
      "Epoch 240/300\n",
      "Average training loss: 0.00588340588990185\n",
      "Average test loss: 0.003574024723014898\n",
      "Epoch 241/300\n",
      "Average training loss: 0.005865892973624998\n",
      "Average test loss: 0.003817464903824859\n",
      "Epoch 242/300\n",
      "Average training loss: 0.005908240490489536\n",
      "Average test loss: 0.0036383835408422683\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0058738313822282685\n",
      "Average test loss: 0.0035001454225016965\n",
      "Epoch 244/300\n",
      "Average training loss: 0.005857836404194435\n",
      "Average test loss: 0.0037354635898437763\n",
      "Epoch 245/300\n",
      "Average training loss: 0.005873989773293336\n",
      "Average test loss: 0.0036239680166666706\n",
      "Epoch 246/300\n",
      "Average training loss: 0.005871106356796291\n",
      "Average test loss: 0.0035477133906549876\n",
      "Epoch 247/300\n",
      "Average training loss: 0.005859367755138211\n",
      "Average test loss: 0.0036471668088601697\n",
      "Epoch 248/300\n",
      "Average training loss: 0.005864858013060358\n",
      "Average test loss: 0.003665346822390954\n",
      "Epoch 249/300\n",
      "Average training loss: 0.005848411642842823\n",
      "Average test loss: 0.003646951932253109\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005867861188948154\n",
      "Average test loss: 0.0036019266220844452\n",
      "Epoch 251/300\n",
      "Average training loss: 0.005845979741464059\n",
      "Average test loss: 0.00377992848555247\n",
      "Epoch 252/300\n",
      "Average training loss: 0.005856966399898132\n",
      "Average test loss: 0.003784609151383241\n",
      "Epoch 253/300\n",
      "Average training loss: 0.005836358227249649\n",
      "Average test loss: 0.0035562668662104343\n",
      "Epoch 254/300\n",
      "Average training loss: 0.005848466753959656\n",
      "Average test loss: 0.0035864531244668694\n",
      "Epoch 255/300\n",
      "Average training loss: 0.005842860679659579\n",
      "Average test loss: 0.0036328335077398352\n",
      "Epoch 256/300\n",
      "Average training loss: 0.005848957456234429\n",
      "Average test loss: 0.0037475026469263764\n",
      "Epoch 257/300\n",
      "Average training loss: 0.005856314346194267\n",
      "Average test loss: 0.0036171886233819857\n",
      "Epoch 258/300\n",
      "Average training loss: 0.00582632324927383\n",
      "Average test loss: 0.003527465296909213\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0058483179157806766\n",
      "Average test loss: 0.003666518896818161\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00583763431923257\n",
      "Average test loss: 0.003561548314988613\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00583194473447899\n",
      "Average test loss: 0.003725851635552115\n",
      "Epoch 262/300\n",
      "Average training loss: 0.005824254479259252\n",
      "Average test loss: 0.003610912954641713\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00583442329780923\n",
      "Average test loss: 0.0037213632518218625\n",
      "Epoch 264/300\n",
      "Average training loss: 0.005823571310275131\n",
      "Average test loss: 0.0036011975523498325\n",
      "Epoch 265/300\n",
      "Average training loss: 0.005842358630978399\n",
      "Average test loss: 0.0036338202357292177\n",
      "Epoch 266/300\n",
      "Average training loss: 0.005823471208413442\n",
      "Average test loss: 0.0035280571335719693\n",
      "Epoch 267/300\n",
      "Average training loss: 0.005810524112648434\n",
      "Average test loss: 0.0037053067365454303\n",
      "Epoch 268/300\n",
      "Average training loss: 0.005848095132658879\n",
      "Average test loss: 0.00368680801987648\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0058015703674819735\n",
      "Average test loss: 0.0036047408291035228\n",
      "Epoch 270/300\n",
      "Average training loss: 0.005814276347971625\n",
      "Average test loss: 0.003934621886867616\n",
      "Epoch 271/300\n",
      "Average training loss: 0.005838554037941827\n",
      "Average test loss: 0.0036694742776453497\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005818052312566174\n",
      "Average test loss: 0.003620027932855818\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00581941098637051\n",
      "Average test loss: 0.003689634253581365\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005812906247874101\n",
      "Average test loss: 0.003518473290113939\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005813311353325844\n",
      "Average test loss: 0.0035353117225070795\n",
      "Epoch 276/300\n",
      "Average training loss: 0.005809249944984913\n",
      "Average test loss: 0.0036599210084726414\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005801557354215119\n",
      "Average test loss: 0.003627796391853028\n",
      "Epoch 278/300\n",
      "Average training loss: 0.005806331199076441\n",
      "Average test loss: 0.003825558145634002\n",
      "Epoch 279/300\n",
      "Average training loss: 0.005799240726563666\n",
      "Average test loss: 0.003598311584856775\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0057994880361689465\n",
      "Average test loss: 0.003607763210725453\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0057988701574504375\n",
      "Average test loss: 0.003656034339633253\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0058052748861826126\n",
      "Average test loss: 0.0038806421069635284\n",
      "Epoch 283/300\n",
      "Average training loss: 0.005791501524547736\n",
      "Average test loss: 0.004027312897559669\n",
      "Epoch 284/300\n",
      "Average training loss: 0.005803500199069579\n",
      "Average test loss: 0.0037165654483768676\n",
      "Epoch 285/300\n",
      "Average training loss: 0.005790480874064896\n",
      "Average test loss: 0.003832056454486317\n",
      "Epoch 286/300\n",
      "Average training loss: 0.005784380020780696\n",
      "Average test loss: 0.003551604148828321\n",
      "Epoch 287/300\n",
      "Average training loss: 0.00578375464098321\n",
      "Average test loss: 0.0036407020572159026\n",
      "Epoch 288/300\n",
      "Average training loss: 0.005786517609324721\n",
      "Average test loss: 0.0036670112922373745\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00578882471844554\n",
      "Average test loss: 0.003696531704937418\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00578624849849277\n",
      "Average test loss: 0.0037064764601074988\n",
      "Epoch 291/300\n",
      "Average training loss: 0.005780913238724073\n",
      "Average test loss: 0.0035875076771610314\n",
      "Epoch 292/300\n",
      "Average training loss: 0.005788368898961279\n",
      "Average test loss: 0.0036736364122480153\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005784601228518618\n",
      "Average test loss: 0.0035624129958450796\n",
      "Epoch 294/300\n",
      "Average training loss: 0.005773736376729276\n",
      "Average test loss: 0.0036819697976526286\n",
      "Epoch 295/300\n",
      "Average training loss: 0.005766868102467722\n",
      "Average test loss: 0.0036222521072874466\n",
      "Epoch 296/300\n",
      "Average training loss: 0.005773947119298908\n",
      "Average test loss: 0.0037056202027532788\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005762665907541911\n",
      "Average test loss: 0.0036871694564405413\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005764161722941531\n",
      "Average test loss: 0.0037628855785975853\n",
      "Epoch 299/300\n",
      "Average training loss: 0.005784923119263517\n",
      "Average test loss: 0.003642700201521317\n",
      "Epoch 300/300\n",
      "Average training loss: 0.005773712622622649\n",
      "Average test loss: 0.0036422937514467373\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04885392999152342\n",
      "Average test loss: 0.005897982626325554\n",
      "Epoch 2/300\n",
      "Average training loss: 0.013974877150522338\n",
      "Average test loss: 0.005205158977044953\n",
      "Epoch 3/300\n",
      "Average training loss: 0.011960266589290567\n",
      "Average test loss: 0.004416537556797266\n",
      "Epoch 4/300\n",
      "Average training loss: 0.010929072712030675\n",
      "Average test loss: 0.004237456274322338\n",
      "Epoch 5/300\n",
      "Average training loss: 0.010186555641392868\n",
      "Average test loss: 0.004027305060790644\n",
      "Epoch 6/300\n",
      "Average training loss: 0.00957494013508161\n",
      "Average test loss: 0.0038025732156303192\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009035928449696964\n",
      "Average test loss: 0.0037633411675277685\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008549489551120334\n",
      "Average test loss: 0.003721938668853707\n",
      "Epoch 9/300\n",
      "Average training loss: 0.00812300499694215\n",
      "Average test loss: 0.003541892986537682\n",
      "Epoch 10/300\n",
      "Average training loss: 0.00777074022922251\n",
      "Average test loss: 0.0034351027109142808\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007488302940295802\n",
      "Average test loss: 0.0034133711384816304\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007220305647701025\n",
      "Average test loss: 0.0034478927995595667\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0070108552215\n",
      "Average test loss: 0.0030523682824439473\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006821546199007167\n",
      "Average test loss: 0.002985520629833142\n",
      "Epoch 15/300\n",
      "Average training loss: 0.006647956338607603\n",
      "Average test loss: 0.004646088822848267\n",
      "Epoch 16/300\n",
      "Average training loss: 0.006510824631485674\n",
      "Average test loss: 0.0029951925604707664\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0063532921998865075\n",
      "Average test loss: 0.0028524349940319857\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0062333708053661716\n",
      "Average test loss: 0.0027898304706646337\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00611703788737456\n",
      "Average test loss: 0.00272588237375021\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006028893660753965\n",
      "Average test loss: 0.0027136124037206175\n",
      "Epoch 21/300\n",
      "Average training loss: 0.005931987525274356\n",
      "Average test loss: 0.002640878674884637\n",
      "Epoch 22/300\n",
      "Average training loss: 0.005860723803854651\n",
      "Average test loss: 0.002722994919659363\n",
      "Epoch 23/300\n",
      "Average training loss: 0.005798712895562251\n",
      "Average test loss: 0.0025797265768051146\n",
      "Epoch 24/300\n",
      "Average training loss: 0.005721922759794527\n",
      "Average test loss: 0.0025833928020050126\n",
      "Epoch 25/300\n",
      "Average training loss: 0.005652714075727595\n",
      "Average test loss: 0.002499495802861121\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00560524569824338\n",
      "Average test loss: 0.002548270779972275\n",
      "Epoch 27/300\n",
      "Average training loss: 0.005565638066579898\n",
      "Average test loss: 0.002502998872970541\n",
      "Epoch 28/300\n",
      "Average training loss: 0.005500373269948695\n",
      "Average test loss: 0.0024719550803096756\n",
      "Epoch 29/300\n",
      "Average training loss: 0.005466216724779871\n",
      "Average test loss: 0.0025933325559728674\n",
      "Epoch 30/300\n",
      "Average training loss: 0.005427126605063677\n",
      "Average test loss: 0.002581311093022426\n",
      "Epoch 31/300\n",
      "Average training loss: 0.005388840436521504\n",
      "Average test loss: 0.0024486067940791448\n",
      "Epoch 32/300\n",
      "Average training loss: 0.00535161340319448\n",
      "Average test loss: 0.0024750102475906413\n",
      "Epoch 33/300\n",
      "Average training loss: 0.005310576917396652\n",
      "Average test loss: 0.002415554145868454\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0052858897803558244\n",
      "Average test loss: 0.002491572775774532\n",
      "Epoch 35/300\n",
      "Average training loss: 0.005259577314472861\n",
      "Average test loss: 0.002394236803882652\n",
      "Epoch 36/300\n",
      "Average training loss: 0.005226654903342326\n",
      "Average test loss: 0.0024682769059307047\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0052156891292995874\n",
      "Average test loss: 0.0024651964627620247\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005181100148707628\n",
      "Average test loss: 0.0023622606311821275\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005157476315067874\n",
      "Average test loss: 0.002394101174341308\n",
      "Epoch 40/300\n",
      "Average training loss: 0.005143030407114162\n",
      "Average test loss: 0.0023961959220468996\n",
      "Epoch 41/300\n",
      "Average training loss: 0.005120051581412554\n",
      "Average test loss: 0.0023323501226388747\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005106905957476961\n",
      "Average test loss: 0.0024476148769673376\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0050843584156698655\n",
      "Average test loss: 0.0023368307689411775\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005061130786728527\n",
      "Average test loss: 0.0023823427216460307\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0050475383042875265\n",
      "Average test loss: 0.0024731981383843554\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005024777712507381\n",
      "Average test loss: 0.002385960628795955\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005019743396590153\n",
      "Average test loss: 0.0024197589637090762\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005006784161345826\n",
      "Average test loss: 0.0023243140135374334\n",
      "Epoch 49/300\n",
      "Average training loss: 0.004980683428131872\n",
      "Average test loss: 0.0023441626113942927\n",
      "Epoch 50/300\n",
      "Average training loss: 0.004978147649516662\n",
      "Average test loss: 0.002415816112835374\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0049530265149143006\n",
      "Average test loss: 0.0023220373772912554\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004950088674409522\n",
      "Average test loss: 0.002365466737912761\n",
      "Epoch 53/300\n",
      "Average training loss: 0.004932174093607399\n",
      "Average test loss: 0.0023417284196863572\n",
      "Epoch 54/300\n",
      "Average training loss: 0.004916463496370448\n",
      "Average test loss: 0.0023196937763649557\n",
      "Epoch 55/300\n",
      "Average training loss: 0.004911034798870484\n",
      "Average test loss: 0.0023743263807975584\n",
      "Epoch 56/300\n",
      "Average training loss: 0.004889978304091427\n",
      "Average test loss: 0.0023645363055790464\n",
      "Epoch 57/300\n",
      "Average training loss: 0.004892467448280917\n",
      "Average test loss: 0.002350232317112386\n",
      "Epoch 58/300\n",
      "Average training loss: 0.004879629269656208\n",
      "Average test loss: 0.0022902493548269073\n",
      "Epoch 59/300\n",
      "Average training loss: 0.004863481140385072\n",
      "Average test loss: 0.0025044029847615295\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0048495606813165875\n",
      "Average test loss: 0.0025182001068153317\n",
      "Epoch 61/300\n",
      "Average training loss: 0.004842819594053758\n",
      "Average test loss: 0.002339192864795526\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004839940920472145\n",
      "Average test loss: 0.002504762339923117\n",
      "Epoch 63/300\n",
      "Average training loss: 0.00481655234305395\n",
      "Average test loss: 0.0023320842899589075\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004805480449563927\n",
      "Average test loss: 0.002365122750815418\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004805922535558542\n",
      "Average test loss: 0.002322765258037382\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004797087204953035\n",
      "Average test loss: 0.0023461983317716252\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0047849037498235705\n",
      "Average test loss: 0.002296771452865667\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004775871068653133\n",
      "Average test loss: 0.002319849127282699\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004779811237007379\n",
      "Average test loss: 0.002450306240055296\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004759045281757911\n",
      "Average test loss: 0.0023337184842675924\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004757536035858922\n",
      "Average test loss: 0.0023115785885602235\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00476522238428394\n",
      "Average test loss: 0.002349104033783078\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0047405944582488806\n",
      "Average test loss: 0.002485770409926772\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004728179127391842\n",
      "Average test loss: 0.002331742895560132\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0047228834331035615\n",
      "Average test loss: 0.002330229331842727\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0047091519033743276\n",
      "Average test loss: 0.0023968485502733126\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004711474530398846\n",
      "Average test loss: 0.002324388890216748\n",
      "Epoch 78/300\n",
      "Average training loss: 0.00471160566724009\n",
      "Average test loss: 0.002295550258623229\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004697680424484942\n",
      "Average test loss: 0.002327284519146714\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004690001355898049\n",
      "Average test loss: 0.0024528349019173118\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004676926842166318\n",
      "Average test loss: 0.0027599493465903734\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0046825892188482815\n",
      "Average test loss: 0.0023183095968431896\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004677089696129163\n",
      "Average test loss: 0.002428688452993002\n",
      "Epoch 84/300\n",
      "Average training loss: 0.004675109565464987\n",
      "Average test loss: 0.002386629189675053\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004663942515436146\n",
      "Average test loss: 0.002543468968942761\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004645673610683945\n",
      "Average test loss: 0.0024580568712618614\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0046485863510105346\n",
      "Average test loss: 0.0023441136767052943\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004644984293729067\n",
      "Average test loss: 0.0022896104423950118\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0046499095401830145\n",
      "Average test loss: 0.0024336700956854554\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004631782869497935\n",
      "Average test loss: 0.0023705095646695956\n",
      "Epoch 91/300\n",
      "Average training loss: 0.004622410715868075\n",
      "Average test loss: 0.0023870089136891893\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004622299335896969\n",
      "Average test loss: 0.002327758151416977\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004623932044332226\n",
      "Average test loss: 0.0023694379447648925\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0046101470883521765\n",
      "Average test loss: 0.0023190856758091185\n",
      "Epoch 95/300\n",
      "Average training loss: 0.004613262213352654\n",
      "Average test loss: 0.0023604146625018784\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004590127564138837\n",
      "Average test loss: 0.0023594405257867444\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004586160086509254\n",
      "Average test loss: 0.002436472359009915\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004590450844830937\n",
      "Average test loss: 0.002639527192339301\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004588272086034219\n",
      "Average test loss: 0.0023337004633827343\n",
      "Epoch 100/300\n",
      "Average training loss: 0.004582856311152379\n",
      "Average test loss: 0.0022732163380003636\n",
      "Epoch 101/300\n",
      "Average training loss: 0.004591109488573339\n",
      "Average test loss: 0.0024411474582221775\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004583882264792919\n",
      "Average test loss: 0.002349729485913283\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004565888956603077\n",
      "Average test loss: 0.0023958191683308948\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004556227263477114\n",
      "Average test loss: 0.0023317417011906702\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004562148280441761\n",
      "Average test loss: 0.0024117568437423973\n",
      "Epoch 106/300\n",
      "Average training loss: 0.00457514344362749\n",
      "Average test loss: 0.002347284795716405\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004548167455113597\n",
      "Average test loss: 0.002310284652850694\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00455368949038287\n",
      "Average test loss: 0.002346197571605444\n",
      "Epoch 109/300\n",
      "Average training loss: 0.004539699650473065\n",
      "Average test loss: 0.002332739607120554\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0045481373485591675\n",
      "Average test loss: 0.0023396506088061465\n",
      "Epoch 111/300\n",
      "Average training loss: 0.004524730247755846\n",
      "Average test loss: 0.0023139931216008135\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004524064902216196\n",
      "Average test loss: 0.0023791252120087545\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004540100800494353\n",
      "Average test loss: 0.0023227694765147236\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004525394815951586\n",
      "Average test loss: 0.002548796696588397\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004508756306436327\n",
      "Average test loss: 0.0023331341601701247\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004518581120090352\n",
      "Average test loss: 0.002383512992825773\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004513555852075418\n",
      "Average test loss: 0.0023867812808603048\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004515648085210058\n",
      "Average test loss: 0.0024436973395446935\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0045200186433891456\n",
      "Average test loss: 0.0024457142777327035\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004496460964903235\n",
      "Average test loss: 0.002451784980379873\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004487510999457703\n",
      "Average test loss: 0.0023772260887134408\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004486325283224384\n",
      "Average test loss: 0.002546537596008016\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004499788084377845\n",
      "Average test loss: 0.002342135135550052\n",
      "Epoch 124/300\n",
      "Average training loss: 0.00448698789295223\n",
      "Average test loss: 0.002295746489531464\n",
      "Epoch 125/300\n",
      "Average training loss: 0.00447092164390617\n",
      "Average test loss: 0.002420658682783445\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004484878351291021\n",
      "Average test loss: 0.0023201609431869456\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004473876694838206\n",
      "Average test loss: 0.002293023382830951\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004476862338268095\n",
      "Average test loss: 0.0024302507052198054\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004478516642418173\n",
      "Average test loss: 0.0023448153181622427\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004481917725048131\n",
      "Average test loss: 0.0025103605818003415\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004454472676747375\n",
      "Average test loss: 0.002304872022838228\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0044542947804762255\n",
      "Average test loss: 0.0023037372693005534\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004465570460591052\n",
      "Average test loss: 0.0023916251884980332\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004448556137995587\n",
      "Average test loss: 0.0023604252005202902\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004452747333794832\n",
      "Average test loss: 0.0025217941041207974\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004450747379412254\n",
      "Average test loss: 0.002408315828690926\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004452127033637629\n",
      "Average test loss: 0.002300114592951205\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004437983224375381\n",
      "Average test loss: 0.00235670298544897\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004439377416546146\n",
      "Average test loss: 0.002413468702799744\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004440834337638484\n",
      "Average test loss: 0.0024025768900497093\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004427393490655554\n",
      "Average test loss: 0.00235653489559061\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004430822528484795\n",
      "Average test loss: 0.002446381069512831\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00441942882620626\n",
      "Average test loss: 0.002464801830136114\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0044165357392695215\n",
      "Average test loss: 0.002290875469023983\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004426535653571288\n",
      "Average test loss: 0.002323031451760067\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004481080891357528\n",
      "Average test loss: 0.002405123503257831\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004411942750629451\n",
      "Average test loss: 0.002394041021044056\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0044024814102384775\n",
      "Average test loss: 0.002456586629152298\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004417887430224154\n",
      "Average test loss: 0.0023894235278583234\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004405125899033414\n",
      "Average test loss: 0.0024153437250190313\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004399012607832749\n",
      "Average test loss: 0.002400419307458732\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004398908572064506\n",
      "Average test loss: 0.002368336662857069\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004390166241468655\n",
      "Average test loss: 0.002529578482111295\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004401028709279166\n",
      "Average test loss: 0.0023783889576378795\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0043933739798764386\n",
      "Average test loss: 0.0023389575756672357\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00439005982875824\n",
      "Average test loss: 0.0023421595143154265\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004392608261770672\n",
      "Average test loss: 0.002341254110758503\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004385715889847941\n",
      "Average test loss: 0.0024014215355532037\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004391033505813943\n",
      "Average test loss: 0.0023303946753342945\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004385029801064067\n",
      "Average test loss: 0.002416191474845012\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004383742466982868\n",
      "Average test loss: 0.0023215115177962517\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004382488829394181\n",
      "Average test loss: 0.0023607585575017662\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00436986373944415\n",
      "Average test loss: 0.4292805257638295\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0044043594035837385\n",
      "Average test loss: 0.0023848119200103814\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004368938423279259\n",
      "Average test loss: 0.002352996848006215\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004358848265475697\n",
      "Average test loss: 0.0023272001697785326\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0043563407191799745\n",
      "Average test loss: 0.0023790945245159996\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004360877052570383\n",
      "Average test loss: 0.0024041512683033945\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00435916267376807\n",
      "Average test loss: 0.002408791437952055\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004384909545795785\n",
      "Average test loss: 0.002407525621768501\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004355135681314601\n",
      "Average test loss: 0.0023393041589814755\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004336290193514692\n",
      "Average test loss: 0.0023750509855647883\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004354907143861056\n",
      "Average test loss: 0.002315003114855952\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00435847549740639\n",
      "Average test loss: 0.002460949992140134\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004366065246777402\n",
      "Average test loss: 0.002428935490962532\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004340370082192951\n",
      "Average test loss: 0.002312302762745983\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004341813002816505\n",
      "Average test loss: 0.002358479061163962\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004337303599963586\n",
      "Average test loss: 0.0024044791933976943\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004340625365161234\n",
      "Average test loss: 0.0025210202199717363\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0043324818333817855\n",
      "Average test loss: 0.002364979213414093\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004342493673165639\n",
      "Average test loss: 0.002447485438858469\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004338241608606444\n",
      "Average test loss: 0.002465449680056837\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004322813409484095\n",
      "Average test loss: 0.0023543310874245232\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0043246754712114735\n",
      "Average test loss: 0.002388685365828375\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0043217834807518455\n",
      "Average test loss: 0.0024455962777137756\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004319064601427979\n",
      "Average test loss: 0.00237175631047123\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004315465688291523\n",
      "Average test loss: 0.0023637441301511395\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0043194513262973895\n",
      "Average test loss: 0.002518312208354473\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004305230496658219\n",
      "Average test loss: 0.0023604007947983013\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004321020111855534\n",
      "Average test loss: 0.002381944496391548\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004313503375069963\n",
      "Average test loss: 0.0024048666949901317\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00431949893799093\n",
      "Average test loss: 0.0024741678009223608\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004317425740261872\n",
      "Average test loss: 0.0023425232662508885\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0043053707358323865\n",
      "Average test loss: 0.0023641069049222603\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004309006953198049\n",
      "Average test loss: 0.0024156024807857144\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00430625728973084\n",
      "Average test loss: 0.002439673294623693\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004297554253497058\n",
      "Average test loss: 0.0024067165054794813\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004297410035919812\n",
      "Average test loss: 0.0024115313533693552\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0042976773960722815\n",
      "Average test loss: 0.4657690450482898\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004372770047850079\n",
      "Average test loss: 0.002518895321318673\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004302479957954751\n",
      "Average test loss: 0.002474255430098209\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0042837461907830506\n",
      "Average test loss: 0.0024322776233570444\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00429493088213106\n",
      "Average test loss: 0.0024471284736775688\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0042929176526765025\n",
      "Average test loss: 0.0024229526614977254\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004292717653016249\n",
      "Average test loss: 0.002444849924287862\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004279437045256297\n",
      "Average test loss: 0.0023449303853429024\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004287694627212154\n",
      "Average test loss: 0.0024366880288968482\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004284322434001499\n",
      "Average test loss: 0.0023850766416225167\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004278126178930203\n",
      "Average test loss: 0.0023479564483794902\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0042777365230851705\n",
      "Average test loss: 0.0024134364233662684\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004272162202124795\n",
      "Average test loss: 0.0023908654341681136\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0042716378341946335\n",
      "Average test loss: 0.0024031942286011246\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0042759551058212915\n",
      "Average test loss: 0.0023386111106309627\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004280858451500535\n",
      "Average test loss: 0.002530077849825223\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00426847180393007\n",
      "Average test loss: 0.0023351542779968845\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0042723896582093504\n",
      "Average test loss: 0.00239617258310318\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004257444633170963\n",
      "Average test loss: 0.0024395147091191677\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004257363405492571\n",
      "Average test loss: 0.0023952316308601036\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004258174538612366\n",
      "Average test loss: 0.0024760758897496593\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004272322993311617\n",
      "Average test loss: 0.002377130458338393\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004261722081651291\n",
      "Average test loss: 0.0024390769137276544\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00425507877042724\n",
      "Average test loss: 0.002576240758204626\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004256462614569399\n",
      "Average test loss: 0.0023810198170443374\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004256219122351872\n",
      "Average test loss: 0.002513617436091105\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004260205404005117\n",
      "Average test loss: 0.0023562722048825688\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004251635304755635\n",
      "Average test loss: 0.0024207522328943015\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004253718316968944\n",
      "Average test loss: 0.002449865735653374\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004258715845230553\n",
      "Average test loss: 0.0024361659751998056\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004246561992913484\n",
      "Average test loss: 0.0025326426447265676\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004250233065750864\n",
      "Average test loss: 0.002394477440044284\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004243189322451751\n",
      "Average test loss: 0.002331954640439815\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004260023153904412\n",
      "Average test loss: 0.002382968532749348\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004241338596161869\n",
      "Average test loss: 0.0023922964826019275\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004248154221723477\n",
      "Average test loss: 0.0023893618565052747\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004241164103150368\n",
      "Average test loss: 0.002356451839622524\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004231545248793231\n",
      "Average test loss: 0.002619388741751512\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004227764878835943\n",
      "Average test loss: 0.0024819146243648398\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004240256791106529\n",
      "Average test loss: 0.0026070362149427334\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00423855658993125\n",
      "Average test loss: 0.0024564511488295265\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004225874269795087\n",
      "Average test loss: 0.0025149038907968336\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0042294470556080346\n",
      "Average test loss: 0.0023213210279742877\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004233251684034864\n",
      "Average test loss: 0.0024510430364559094\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00424509217383133\n",
      "Average test loss: 0.0024515316256632407\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004226409938600328\n",
      "Average test loss: 0.0024152690530237224\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0042288039579159685\n",
      "Average test loss: 0.002450367246237066\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00423086221723093\n",
      "Average test loss: 0.00244673889523579\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004222731920580069\n",
      "Average test loss: 0.0026275638572664725\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004222586895856592\n",
      "Average test loss: 0.0025012665167450905\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004214156247261498\n",
      "Average test loss: 0.0023876850585349734\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0042209529127511716\n",
      "Average test loss: 0.002530543354339898\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0042182838002012835\n",
      "Average test loss: 0.002427422185531921\n",
      "Epoch 252/300\n",
      "Average training loss: 0.00421108957587017\n",
      "Average test loss: 0.0024212324153631924\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00421096215935217\n",
      "Average test loss: 0.0024133881957580645\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004218831955972645\n",
      "Average test loss: 0.0024329766295850277\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004222999718454149\n",
      "Average test loss: 0.002348073883085615\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004209049697551462\n",
      "Average test loss: 0.002426443592438267\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004214920688834455\n",
      "Average test loss: 0.0024039892262468737\n",
      "Epoch 258/300\n",
      "Average training loss: 0.004218300475014581\n",
      "Average test loss: 0.0024248796498609914\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00420285054254863\n",
      "Average test loss: 0.0024205893830706675\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004202512537439664\n",
      "Average test loss: 0.002397221146772305\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004200495877199703\n",
      "Average test loss: 0.002378298867907789\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004208388900384307\n",
      "Average test loss: 0.002422152225031621\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004209051301495896\n",
      "Average test loss: 0.0023825725070718263\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004196484860860639\n",
      "Average test loss: 0.002449092218859328\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0041960437475807135\n",
      "Average test loss: 0.0023942841317297686\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0042106952609287365\n",
      "Average test loss: 0.0024264262546267773\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004188996304447452\n",
      "Average test loss: 0.0024747459960894453\n",
      "Epoch 268/300\n",
      "Average training loss: 0.00418811103163494\n",
      "Average test loss: 0.0024341091089364556\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004185017474409606\n",
      "Average test loss: 0.0023881005911777415\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004197882294861807\n",
      "Average test loss: 0.002417387278440098\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004192216261600454\n",
      "Average test loss: 0.002469522055031525\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0041912870783772736\n",
      "Average test loss: 0.002424586022686627\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004195233781304625\n",
      "Average test loss: 0.002496818671623866\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004186106201675203\n",
      "Average test loss: 0.0024302366046855847\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004199940163228247\n",
      "Average test loss: 0.0023910017613735465\n",
      "Epoch 276/300\n",
      "Average training loss: 0.004173642615477244\n",
      "Average test loss: 0.002454748598237832\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004181525530914465\n",
      "Average test loss: 0.0024519433503349623\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004186737104008595\n",
      "Average test loss: 0.002483791555174523\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004178352950554755\n",
      "Average test loss: 0.0024463566018061507\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004176760079132186\n",
      "Average test loss: 0.002425884396251705\n",
      "Epoch 281/300\n",
      "Average training loss: 0.004185092563016547\n",
      "Average test loss: 0.002377408076491621\n",
      "Epoch 282/300\n",
      "Average training loss: 0.004179958685404724\n",
      "Average test loss: 0.0025803108803100056\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004176175796323353\n",
      "Average test loss: 0.00243919669629799\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004173622080849277\n",
      "Average test loss: 0.002428298303650485\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004172555204066965\n",
      "Average test loss: 0.0024187322221696375\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004179168914341265\n",
      "Average test loss: 0.002446248267673784\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0041757514286372395\n",
      "Average test loss: 0.002373513287657665\n",
      "Epoch 288/300\n",
      "Average training loss: 0.00416243095178571\n",
      "Average test loss: 0.0024044184715797503\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004179260743988885\n",
      "Average test loss: 0.0024522538450029163\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004168714520211021\n",
      "Average test loss: 0.00244888724717829\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004166045668224493\n",
      "Average test loss: 0.0024149638582020997\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0041595293320715425\n",
      "Average test loss: 0.0024110606691489615\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004159350201487541\n",
      "Average test loss: 0.00239538230581416\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004165060351292292\n",
      "Average test loss: 0.0024901285223248933\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004167077866486377\n",
      "Average test loss: 0.002466586924675438\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0041604607622656555\n",
      "Average test loss: 0.0024400230906903744\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004156053140345547\n",
      "Average test loss: 0.0024384993561026125\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004169569862385591\n",
      "Average test loss: 0.002413561624681784\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0041522730882796975\n",
      "Average test loss: 0.002761161613588532\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004154856101092365\n",
      "Average test loss: 0.0023629322870531017\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04211727549135685\n",
      "Average test loss: 0.00485252099028892\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011386817204455535\n",
      "Average test loss: 0.003928206839702196\n",
      "Epoch 3/300\n",
      "Average training loss: 0.009652115379770596\n",
      "Average test loss: 0.004012509892798132\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008762233745720651\n",
      "Average test loss: 0.0035478035331600244\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008197186210089259\n",
      "Average test loss: 0.003051084673239125\n",
      "Epoch 6/300\n",
      "Average training loss: 0.007597979351878166\n",
      "Average test loss: 0.0029539344238324297\n",
      "Epoch 7/300\n",
      "Average training loss: 0.007166242971188492\n",
      "Average test loss: 0.0031077984993656475\n",
      "Epoch 8/300\n",
      "Average training loss: 0.006711256377398968\n",
      "Average test loss: 0.002762962053840359\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006342463516112831\n",
      "Average test loss: 0.002622243953247865\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006026482206251886\n",
      "Average test loss: 0.0025484136785897944\n",
      "Epoch 11/300\n",
      "Average training loss: 0.00577723050614198\n",
      "Average test loss: 0.002363674538830916\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00556097009157141\n",
      "Average test loss: 0.0026018982922865284\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005390175366567241\n",
      "Average test loss: 0.0022268837195717627\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0052308749825590185\n",
      "Average test loss: 0.002212149373566111\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005073316951178842\n",
      "Average test loss: 0.0021812514774501323\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004959478822847208\n",
      "Average test loss: 0.0020642165798279975\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004843175647573339\n",
      "Average test loss: 0.0020442196068664394\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004743862986564636\n",
      "Average test loss: 0.001981275107090672\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004650798879149888\n",
      "Average test loss: 0.0019695702407302126\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004564582735300064\n",
      "Average test loss: 0.0019079939729223648\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004503645110461447\n",
      "Average test loss: 0.0019256150509334273\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004437245335843828\n",
      "Average test loss: 0.0019254238851782349\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004369919638459881\n",
      "Average test loss: 0.0018672868581488728\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0043320668525993825\n",
      "Average test loss: 0.001971803620043728\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004280727464705706\n",
      "Average test loss: 0.0018418466166282693\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004237716391682625\n",
      "Average test loss: 0.0018295046858903435\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004195978808320231\n",
      "Average test loss: 0.0018042418346222904\n",
      "Epoch 28/300\n",
      "Average training loss: 0.004178177863980333\n",
      "Average test loss: 0.0018262821742229991\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0041244113879899185\n",
      "Average test loss: 0.0017730107817591893\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004098277902437581\n",
      "Average test loss: 0.0018514101691544055\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0040804522546629115\n",
      "Average test loss: 0.0017337738381077846\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004041354542805089\n",
      "Average test loss: 0.0017533780504018067\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004014187361010247\n",
      "Average test loss: 0.00174726074664957\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004015078213479783\n",
      "Average test loss: 0.0017766999517463975\n",
      "Epoch 35/300\n",
      "Average training loss: 0.003984384484382139\n",
      "Average test loss: 0.001734818291126026\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003955301590884725\n",
      "Average test loss: 0.001698468623061975\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0039357823830925755\n",
      "Average test loss: 0.0017047355475111141\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003914455449622539\n",
      "Average test loss: 0.0017182714256147543\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003908549479312367\n",
      "Average test loss: 0.0017167402313401302\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003895869358546204\n",
      "Average test loss: 0.0017500979385028283\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003873813656883107\n",
      "Average test loss: 0.0016720006562148532\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0038575040681494608\n",
      "Average test loss: 0.0016979211367045839\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003851322800748878\n",
      "Average test loss: 0.0017431894314164917\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0038278228477057483\n",
      "Average test loss: 0.0016796259510641297\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0038307032816939884\n",
      "Average test loss: 0.0017216580619828568\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003810288545779056\n",
      "Average test loss: 0.001719817408774462\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0037884505291779836\n",
      "Average test loss: 0.0016545166715255214\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003783556758115689\n",
      "Average test loss: 0.0016604279472182194\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0037670039244823987\n",
      "Average test loss: 0.0016880847001448274\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0037751567657623025\n",
      "Average test loss: 0.0016535246812014117\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0037602064795792104\n",
      "Average test loss: 0.0016777565812485086\n",
      "Epoch 52/300\n",
      "Average training loss: 0.003740259762439463\n",
      "Average test loss: 0.0016732233920031124\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0037455039630747504\n",
      "Average test loss: 0.001673086063315471\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003722613946017292\n",
      "Average test loss: 0.001776907516643405\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0037199894497171046\n",
      "Average test loss: 0.0017689810084799925\n",
      "Epoch 56/300\n",
      "Average training loss: 0.003709382880686058\n",
      "Average test loss: 0.0016421047111766205\n",
      "Epoch 57/300\n",
      "Average training loss: 0.003697861571692758\n",
      "Average test loss: 0.0016603037039231922\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003690626383034719\n",
      "Average test loss: 0.001706791636430555\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0036877565468764967\n",
      "Average test loss: 0.0016730844409515461\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003679620204286443\n",
      "Average test loss: 0.0017781015475177102\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0036709089138441614\n",
      "Average test loss: 0.0017393317518548832\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003681074389980899\n",
      "Average test loss: 0.001880237696899308\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0036535816666566664\n",
      "Average test loss: 0.0018016719715669752\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0036549915228452946\n",
      "Average test loss: 0.0017080307631856864\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003640513566012184\n",
      "Average test loss: 0.0016414324283185932\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0036416115725619925\n",
      "Average test loss: 0.0016154384357440802\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0036387507663004927\n",
      "Average test loss: 0.001780889469302363\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0036208789539006023\n",
      "Average test loss: 0.0016596699863051375\n",
      "Epoch 69/300\n",
      "Average training loss: 0.003620983831377493\n",
      "Average test loss: 0.0016796899965653817\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0036136255934834482\n",
      "Average test loss: 0.00164174905905707\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0036046309893329937\n",
      "Average test loss: 0.001662732590507302\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0036007917769667174\n",
      "Average test loss: 0.0016402147400917278\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003602253900633918\n",
      "Average test loss: 0.0016605559295664232\n",
      "Epoch 74/300\n",
      "Average training loss: 0.003594159797247913\n",
      "Average test loss: 0.001746400436386466\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0035914407567017607\n",
      "Average test loss: 0.0016597458405627145\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0035848465543240307\n",
      "Average test loss: 0.001643176104976899\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0035811493342949286\n",
      "Average test loss: 0.0016571449695361984\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0035686312479277452\n",
      "Average test loss: 0.0016973613265694844\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0035645431747867\n",
      "Average test loss: 0.0016754491418186162\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0035598561436765725\n",
      "Average test loss: 0.0017245120083292326\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0035547970885203943\n",
      "Average test loss: 0.0016593644115039045\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0035665347108410463\n",
      "Average test loss: 0.0016109854537579749\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0035448951650824813\n",
      "Average test loss: 0.0016561960509667794\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0035508081747425927\n",
      "Average test loss: 0.0016924684498873022\n",
      "Epoch 85/300\n",
      "Average training loss: 0.003542831402685907\n",
      "Average test loss: 0.0016272407167901596\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003551603759328524\n",
      "Average test loss: 0.0016422651018947363\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0035267635830160643\n",
      "Average test loss: 0.0020636355897618664\n",
      "Epoch 88/300\n",
      "Average training loss: 0.003527532495558262\n",
      "Average test loss: 0.001623131426051259\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0035182013182590405\n",
      "Average test loss: 0.0017471404557840692\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0035179330437547633\n",
      "Average test loss: 0.002036847763384382\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00353547134809196\n",
      "Average test loss: 0.0016620568573060962\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0035095891834547124\n",
      "Average test loss: 0.0016795870586194927\n",
      "Epoch 93/300\n",
      "Average training loss: 0.003505798995701803\n",
      "Average test loss: 0.001740552424142758\n",
      "Epoch 94/300\n",
      "Average training loss: 0.003508439537551668\n",
      "Average test loss: 0.0017043184633884164\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0035118981463213763\n",
      "Average test loss: 0.0017158831600099802\n",
      "Epoch 96/300\n",
      "Average training loss: 0.003494407065005766\n",
      "Average test loss: 0.001643811493491133\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0035026277518934675\n",
      "Average test loss: 0.0016659873807802796\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0034918941301811074\n",
      "Average test loss: 0.0017098023060502278\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0034916341464138693\n",
      "Average test loss: 0.0017386690141219232\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003483079198540913\n",
      "Average test loss: 0.0016673178111927377\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0034870534510248238\n",
      "Average test loss: 0.0017410961277782918\n",
      "Epoch 102/300\n",
      "Average training loss: 0.003478772728807396\n",
      "Average test loss: 0.0016681878898913661\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0034773751873936917\n",
      "Average test loss: 0.001643028182701932\n",
      "Epoch 104/300\n",
      "Average training loss: 0.003469316429562039\n",
      "Average test loss: 0.0016997160881550776\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0034729328703963095\n",
      "Average test loss: 0.0016216375731552641\n",
      "Epoch 106/300\n",
      "Average training loss: 0.003464610455143783\n",
      "Average test loss: 0.0016994070841206445\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003464863690858086\n",
      "Average test loss: 0.0016425462565902206\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0034574266568654114\n",
      "Average test loss: 0.0016516030203137134\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0034525972530245782\n",
      "Average test loss: 0.0016335479263216257\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0034482132277141014\n",
      "Average test loss: 0.003141989528718922\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0034577120898498428\n",
      "Average test loss: 0.0017755734812882212\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0034427878850450118\n",
      "Average test loss: 0.0017208585317970977\n",
      "Epoch 113/300\n",
      "Average training loss: 0.003445005061932736\n",
      "Average test loss: 0.001636033650694622\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0034383704587817193\n",
      "Average test loss: 0.0016680206986558106\n",
      "Epoch 115/300\n",
      "Average training loss: 0.003438483081344101\n",
      "Average test loss: 0.0016637046966287824\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0034415785873101816\n",
      "Average test loss: 0.0017883301282094585\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003445391058714853\n",
      "Average test loss: 0.0017009009508829978\n",
      "Epoch 118/300\n",
      "Average training loss: 0.003432921518675155\n",
      "Average test loss: 0.001676726126215524\n",
      "Epoch 119/300\n",
      "Average training loss: 0.003430238781289922\n",
      "Average test loss: 0.0016672578600959646\n",
      "Epoch 120/300\n",
      "Average training loss: 0.003426982059246964\n",
      "Average test loss: 0.001631973438585798\n",
      "Epoch 121/300\n",
      "Average training loss: 0.003422555298440986\n",
      "Average test loss: 0.0017126249471265409\n",
      "Epoch 122/300\n",
      "Average training loss: 0.003431241064849827\n",
      "Average test loss: 0.005886617666938238\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0034589784807629058\n",
      "Average test loss: 0.001697014953320225\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0034056542199105023\n",
      "Average test loss: 0.0016253161009194123\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0034056233504994047\n",
      "Average test loss: 0.0016851589577272534\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0034086788315325976\n",
      "Average test loss: 0.0019036161698814896\n",
      "Epoch 127/300\n",
      "Average training loss: 0.003403644374675221\n",
      "Average test loss: 0.0016880157325002882\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003400518872878618\n",
      "Average test loss: 0.00173549633871557\n",
      "Epoch 129/300\n",
      "Average training loss: 0.003412768631345696\n",
      "Average test loss: 0.0016994848960182733\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0034005847088992595\n",
      "Average test loss: 0.001692823711782694\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0033940817523333763\n",
      "Average test loss: 0.0016658614211612277\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0033995224403010472\n",
      "Average test loss: 0.0017709572359712587\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0033892493256264264\n",
      "Average test loss: 0.0016384872081172135\n",
      "Epoch 134/300\n",
      "Average training loss: 0.003390158009197977\n",
      "Average test loss: 0.0017274465672671795\n",
      "Epoch 135/300\n",
      "Average training loss: 0.003385024885750479\n",
      "Average test loss: 0.0020055724174405138\n",
      "Epoch 136/300\n",
      "Average training loss: 0.003383088812438978\n",
      "Average test loss: 0.0016704610399384465\n",
      "Epoch 137/300\n",
      "Average training loss: 0.003387248113958372\n",
      "Average test loss: 0.001697506460775104\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0033805422919491925\n",
      "Average test loss: 0.0016681567391173707\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0033771776420374712\n",
      "Average test loss: 0.0018039952110913064\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0033850212076471913\n",
      "Average test loss: 0.0017376905619684193\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0033858161481718223\n",
      "Average test loss: 0.0017296182533933057\n",
      "Epoch 142/300\n",
      "Average training loss: 0.003375347300329142\n",
      "Average test loss: 0.0017144096564087603\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0033687132625944086\n",
      "Average test loss: 0.001644212506711483\n",
      "Epoch 144/300\n",
      "Average training loss: 0.003368513431607021\n",
      "Average test loss: 0.0016398766454723147\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003366337506928378\n",
      "Average test loss: 0.0016672015430198776\n",
      "Epoch 146/300\n",
      "Average training loss: 0.003365907994409402\n",
      "Average test loss: 0.0017014525067061187\n",
      "Epoch 147/300\n",
      "Average training loss: 0.003361124135967758\n",
      "Average test loss: 0.0016940233245905903\n",
      "Epoch 148/300\n",
      "Average training loss: 0.003367792708178361\n",
      "Average test loss: 0.00167090831283066\n",
      "Epoch 149/300\n",
      "Average training loss: 0.003360736425759064\n",
      "Average test loss: 0.0016763183871905008\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0033606299801419177\n",
      "Average test loss: 0.0016877657379955053\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0033487466828276712\n",
      "Average test loss: 0.0016959958699428372\n",
      "Epoch 152/300\n",
      "Average training loss: 0.003359600051616629\n",
      "Average test loss: 0.001659563292749226\n",
      "Epoch 153/300\n",
      "Average training loss: 0.003348490908121069\n",
      "Average test loss: 0.001643918849217395\n",
      "Epoch 154/300\n",
      "Average training loss: 0.003341705718802081\n",
      "Average test loss: 0.0016618152186274528\n",
      "Epoch 155/300\n",
      "Average training loss: 0.003346372108285626\n",
      "Average test loss: 0.0018275339674825469\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0033436699973212348\n",
      "Average test loss: 0.0017091130696030128\n",
      "Epoch 157/300\n",
      "Average training loss: 0.003345418855547905\n",
      "Average test loss: 0.0016916992920968268\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0033518365797483257\n",
      "Average test loss: 0.0017800467927008867\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0033337448216560813\n",
      "Average test loss: 0.001772589876006047\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0033348655617899364\n",
      "Average test loss: 0.0016973795558636387\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0033320164823283753\n",
      "Average test loss: 0.001679535285052326\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0033299886050323644\n",
      "Average test loss: 0.00163562761950824\n",
      "Epoch 163/300\n",
      "Average training loss: 0.003331018351846271\n",
      "Average test loss: 0.0017455033399164676\n",
      "Epoch 164/300\n",
      "Average training loss: 0.003328832000700964\n",
      "Average test loss: 0.0017097147269588377\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0033220106845514644\n",
      "Average test loss: 0.0017261158471099205\n",
      "Epoch 166/300\n",
      "Average training loss: 0.003333213390161594\n",
      "Average test loss: 0.0017299070353102354\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00331979410453803\n",
      "Average test loss: 0.0016945342032445802\n",
      "Epoch 168/300\n",
      "Average training loss: 0.003330299343706833\n",
      "Average test loss: 0.0017201649078892338\n",
      "Epoch 169/300\n",
      "Average training loss: 0.003327218075800273\n",
      "Average test loss: 0.0017345763798803091\n",
      "Epoch 170/300\n",
      "Average training loss: 0.003314902169422971\n",
      "Average test loss: 0.001772511775812341\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0033140907302084896\n",
      "Average test loss: 0.0018723259013560083\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0033138657708962757\n",
      "Average test loss: 0.0016951393950730563\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0033176829051226376\n",
      "Average test loss: 0.001728834315513571\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0033106973723818858\n",
      "Average test loss: 0.0017247244045138358\n",
      "Epoch 175/300\n",
      "Average training loss: 0.003306660772818658\n",
      "Average test loss: 0.001714265761586527\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0033107633433408206\n",
      "Average test loss: 0.0016786870992639\n",
      "Epoch 177/300\n",
      "Average training loss: 0.003312952237410678\n",
      "Average test loss: 0.0016931727841082546\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0033065074334541955\n",
      "Average test loss: 0.0017477406810762154\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0033054861509137683\n",
      "Average test loss: 0.0017392228345076244\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0033067353369875085\n",
      "Average test loss: 0.0016857275251920024\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00330586021931635\n",
      "Average test loss: 0.0017230200027633044\n",
      "Epoch 182/300\n",
      "Average training loss: 0.003297647002049618\n",
      "Average test loss: 0.0017258276330928007\n",
      "Epoch 183/300\n",
      "Average training loss: 0.003294161396308078\n",
      "Average test loss: 0.0016785167815784614\n",
      "Epoch 184/300\n",
      "Average training loss: 0.003293324337651332\n",
      "Average test loss: 0.0017489762468677428\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0032906590478701723\n",
      "Average test loss: 0.0018449006109601922\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0032968979308174717\n",
      "Average test loss: 0.0017093417894923026\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0032949529890384938\n",
      "Average test loss: 0.0016797346875278487\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003285205003908939\n",
      "Average test loss: 0.001909526904579252\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0033011690833502348\n",
      "Average test loss: 0.0016937479265034198\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0032865989771154193\n",
      "Average test loss: 0.0017846728302538396\n",
      "Epoch 191/300\n",
      "Average training loss: 0.003288073932958974\n",
      "Average test loss: 0.0017316285177237458\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0032841396381457647\n",
      "Average test loss: 0.0017322606624414524\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0032830148629016346\n",
      "Average test loss: 0.0016576504485888615\n",
      "Epoch 194/300\n",
      "Average training loss: 0.003282737751594848\n",
      "Average test loss: 0.0017345203285415967\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0032729586720880535\n",
      "Average test loss: 0.0017347744348355466\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0032775889788236885\n",
      "Average test loss: 0.0016840714002028109\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0032747959335231117\n",
      "Average test loss: 0.0017571472979875074\n",
      "Epoch 198/300\n",
      "Average training loss: 0.003269481852857603\n",
      "Average test loss: 0.0017068076866368453\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0032755025685247447\n",
      "Average test loss: 0.014300184950232506\n",
      "Epoch 200/300\n",
      "Average training loss: 0.003733085490349266\n",
      "Average test loss: 0.001806400494236085\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0032744977600458595\n",
      "Average test loss: 0.0017003305849308769\n",
      "Epoch 202/300\n",
      "Average training loss: 0.003253323178531395\n",
      "Average test loss: 0.0016604946971767477\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0032604270204901697\n",
      "Average test loss: 0.001763702038468586\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0032580648265365096\n",
      "Average test loss: 0.0017070470296053422\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0032633247967395517\n",
      "Average test loss: 0.0016877062052695288\n",
      "Epoch 206/300\n",
      "Average training loss: 0.003259129461314943\n",
      "Average test loss: 0.0017268010278542836\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0032735508303675387\n",
      "Average test loss: 0.001713785905804899\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0032568858831913934\n",
      "Average test loss: 0.0017636499434916509\n",
      "Epoch 209/300\n",
      "Average training loss: 0.003272410795506504\n",
      "Average test loss: 0.0016402611395137177\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0032645888601740202\n",
      "Average test loss: 0.0017070860005915164\n",
      "Epoch 211/300\n",
      "Average training loss: 0.003261456266252531\n",
      "Average test loss: 0.0017776185866031381\n",
      "Epoch 212/300\n",
      "Average training loss: 0.003257233378166954\n",
      "Average test loss: 0.001680422314339214\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0032696961450080077\n",
      "Average test loss: 0.0017625374709152512\n",
      "Epoch 214/300\n",
      "Average training loss: 0.003256480562190215\n",
      "Average test loss: 0.001672948673988382\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0033065573970476785\n",
      "Average test loss: 0.0016890795439895656\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0032472195931606824\n",
      "Average test loss: 0.0017283171357380018\n",
      "Epoch 217/300\n",
      "Average training loss: 0.003248630356664459\n",
      "Average test loss: 0.0016901601631608275\n",
      "Epoch 218/300\n",
      "Average training loss: 0.003246877041955789\n",
      "Average test loss: 0.001737696904482113\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0032508996679551073\n",
      "Average test loss: 0.0017598579565270079\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0032522035661256975\n",
      "Average test loss: 0.0017684516067513162\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00324867755236725\n",
      "Average test loss: 0.0017695340601106486\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0032390182554307913\n",
      "Average test loss: 0.001729426281630165\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0032513309336370893\n",
      "Average test loss: 0.0019322824763755003\n",
      "Epoch 224/300\n",
      "Average training loss: 0.003254344226171573\n",
      "Average test loss: 0.001771155407652259\n",
      "Epoch 225/300\n",
      "Average training loss: 0.003247564619820979\n",
      "Average test loss: 0.001737546399442686\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0032434590864512656\n",
      "Average test loss: 0.0018541214489895437\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0032398095762150156\n",
      "Average test loss: 0.00185299448037727\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0032468109054283965\n",
      "Average test loss: 0.0016882588896486494\n",
      "Epoch 229/300\n",
      "Average training loss: 0.00325123610926999\n",
      "Average test loss: 0.0016646740975686245\n",
      "Epoch 230/300\n",
      "Average training loss: 0.003238019414866964\n",
      "Average test loss: 0.0017725653322413564\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0032369105557186737\n",
      "Average test loss: 0.0017416861656432351\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0032389114956474967\n",
      "Average test loss: 0.0017401792940994103\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00323350356788271\n",
      "Average test loss: 0.0017692767654856046\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0032373304191148943\n",
      "Average test loss: 0.001656411345427235\n",
      "Epoch 235/300\n",
      "Average training loss: 0.003233448487189081\n",
      "Average test loss: 0.0017421557120978833\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0032317650721718867\n",
      "Average test loss: 0.001746947758520643\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0032307709782487815\n",
      "Average test loss: 0.0017571264403975673\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0032332761515345837\n",
      "Average test loss: 0.0017011743147547047\n",
      "Epoch 239/300\n",
      "Average training loss: 0.003226311682826943\n",
      "Average test loss: 0.0017356306635257271\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0032291748805178535\n",
      "Average test loss: 0.0016941400752920244\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0032315762146479556\n",
      "Average test loss: 0.0017744876898618208\n",
      "Epoch 242/300\n",
      "Average training loss: 0.003224592572078109\n",
      "Average test loss: 0.004007127825584676\n",
      "Epoch 243/300\n",
      "Average training loss: 0.005055305190177427\n",
      "Average test loss: 0.0016753246278191605\n",
      "Epoch 244/300\n",
      "Average training loss: 0.003456590152449078\n",
      "Average test loss: 0.0017689521718356345\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003277147261839774\n",
      "Average test loss: 0.0017082694119049443\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0032187791522592306\n",
      "Average test loss: 0.001752784340073251\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0031933704875409604\n",
      "Average test loss: 0.0016827978869486187\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0031911219904820123\n",
      "Average test loss: 0.0016979955004321203\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0031987243085685704\n",
      "Average test loss: 0.0017003232270168761\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0032008124865177605\n",
      "Average test loss: 0.0017354137509440382\n",
      "Epoch 251/300\n",
      "Average training loss: 0.003203313507553604\n",
      "Average test loss: 0.0017974950522184371\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0032167312608410914\n",
      "Average test loss: 0.0017243478134688405\n",
      "Epoch 253/300\n",
      "Average training loss: 0.003214442698491944\n",
      "Average test loss: 0.0019425048016839558\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0032175700701773165\n",
      "Average test loss: 0.0017374503527664475\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0032172434164418114\n",
      "Average test loss: 0.0016974978160320057\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0032182268641061254\n",
      "Average test loss: 0.0017330659298329717\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003218355607241392\n",
      "Average test loss: 0.0017095463174498743\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0032191045151816473\n",
      "Average test loss: 0.0017626634910702706\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0032179830318523777\n",
      "Average test loss: 0.001699919495731592\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0032182899771465196\n",
      "Average test loss: 0.0017650657610760795\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003211530962545011\n",
      "Average test loss: 0.0017726765679609445\n",
      "Epoch 262/300\n",
      "Average training loss: 0.003211780661303136\n",
      "Average test loss: 0.0017081586256002387\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0032099431858708463\n",
      "Average test loss: 0.0017172197470855383\n",
      "Epoch 264/300\n",
      "Average training loss: 0.003213690504224764\n",
      "Average test loss: 0.0017141153408835332\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0032034983228271204\n",
      "Average test loss: 0.0017937351300691565\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0032111931319038075\n",
      "Average test loss: 0.0016883725956496265\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0032075502348856795\n",
      "Average test loss: 0.001810962540201015\n",
      "Epoch 268/300\n",
      "Average training loss: 0.003206363639069928\n",
      "Average test loss: 0.0016989843060986863\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0032001373664372495\n",
      "Average test loss: 0.0016793968473147187\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0031993808640787998\n",
      "Average test loss: 0.001706141782200171\n",
      "Epoch 271/300\n",
      "Average training loss: 0.003205415123452743\n",
      "Average test loss: 0.0017221402899465627\n",
      "Epoch 272/300\n",
      "Average training loss: 0.003202126305964258\n",
      "Average test loss: 0.001695331054325733\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0032070914463450512\n",
      "Average test loss: 0.001688229377898905\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0031958844719661606\n",
      "Average test loss: 0.0017765503136648072\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0032025586153484054\n",
      "Average test loss: 0.0017371738941098253\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0031978584768043625\n",
      "Average test loss: 0.0025026276511036687\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0032013957895752456\n",
      "Average test loss: 0.0017741363857769303\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0031974972625159557\n",
      "Average test loss: 0.0017311772307277555\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0032011143320964442\n",
      "Average test loss: 0.001706232721813851\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0031888662502169607\n",
      "Average test loss: 0.0017128272751967112\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003190534291168054\n",
      "Average test loss: 0.0019119513483924998\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0031971076066709225\n",
      "Average test loss: 0.0018277537301182748\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0031918695881548854\n",
      "Average test loss: 0.0017040980882528755\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0031966400649398566\n",
      "Average test loss: 0.00176092022874703\n",
      "Epoch 285/300\n",
      "Average training loss: 0.003190166918767823\n",
      "Average test loss: 0.0016929582954487866\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0031855733713342083\n",
      "Average test loss: 0.0016944737882456846\n",
      "Epoch 287/300\n",
      "Average training loss: 0.003186895483483871\n",
      "Average test loss: 0.0017477754852217105\n",
      "Epoch 288/300\n",
      "Average training loss: 0.003191275566402409\n",
      "Average test loss: 0.0017441887787232797\n",
      "Epoch 289/300\n",
      "Average training loss: 0.003186221985767285\n",
      "Average test loss: 0.0016782661372174818\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003185485133280357\n",
      "Average test loss: 0.0018030651271757152\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0031860516712897352\n",
      "Average test loss: 0.0016654241505182452\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003189219906926155\n",
      "Average test loss: 0.0017343146880674694\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0031877040730582343\n",
      "Average test loss: 0.0017422342687431309\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0031768657302276957\n",
      "Average test loss: 0.0017212712245268955\n",
      "Epoch 295/300\n",
      "Average training loss: 0.003189533405005932\n",
      "Average test loss: 0.0017485952118618622\n",
      "Epoch 296/300\n",
      "Average training loss: 0.003192153884511855\n",
      "Average test loss: 0.0017299648370179865\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003179032979326116\n",
      "Average test loss: 0.0018474889331393772\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003183797216249837\n",
      "Average test loss: 0.001826151463513573\n",
      "Epoch 299/300\n",
      "Average training loss: 0.003177291269517607\n",
      "Average test loss: 0.0018388154794358545\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0031747748183293474\n",
      "Average test loss: 0.0017351607898664144\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.5/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.35\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.74\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.51\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.51\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.00\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.47\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.24\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.75\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.83\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.23\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.31\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.16812392589118746\n",
      "Average test loss: 0.017195122578077846\n",
      "Epoch 2/300\n",
      "Average training loss: 0.033491170177857084\n",
      "Average test loss: 0.014730290088388654\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02730763163831499\n",
      "Average test loss: 0.009274171714981396\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02411398094230228\n",
      "Average test loss: 0.009052936704622375\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022055114567279814\n",
      "Average test loss: 0.009874263973699676\n",
      "Epoch 6/300\n",
      "Average training loss: 0.020514710073669753\n",
      "Average test loss: 0.011750882354047563\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01937884237451686\n",
      "Average test loss: 0.0078731762882736\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018213919926020835\n",
      "Average test loss: 0.007931469324562285\n",
      "Epoch 9/300\n",
      "Average training loss: 0.017509320293863616\n",
      "Average test loss: 0.007549853038870626\n",
      "Epoch 10/300\n",
      "Average training loss: 0.016683536428544258\n",
      "Average test loss: 0.007033855197330316\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01590223021970855\n",
      "Average test loss: 0.011365279646797313\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015271250559224023\n",
      "Average test loss: 0.008528731484793954\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014702502477500174\n",
      "Average test loss: 0.006649571033401622\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014277939514981375\n",
      "Average test loss: 0.0064877007781631415\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013831415640811126\n",
      "Average test loss: 0.006399954176611371\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01350033922990163\n",
      "Average test loss: 0.0065205832123756404\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013206351107193363\n",
      "Average test loss: 0.006669877323011557\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012920288590921297\n",
      "Average test loss: 0.00648451307333178\n",
      "Epoch 19/300\n",
      "Average training loss: 0.012697517443034384\n",
      "Average test loss: 0.005999483716570669\n",
      "Epoch 20/300\n",
      "Average training loss: 0.012521379939383931\n",
      "Average test loss: 0.005923245667583413\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012246774616340796\n",
      "Average test loss: 0.006612297553983\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012137452277872298\n",
      "Average test loss: 0.01618675857368443\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012013243689305253\n",
      "Average test loss: 0.00576476074092918\n",
      "Epoch 24/300\n",
      "Average training loss: 0.011826768449611134\n",
      "Average test loss: 0.005997959670093324\n",
      "Epoch 25/300\n",
      "Average training loss: 0.011749526548716758\n",
      "Average test loss: 0.005560502507206466\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01152166295879417\n",
      "Average test loss: 0.005789088138275676\n",
      "Epoch 27/300\n",
      "Average training loss: 0.011725731387734414\n",
      "Average test loss: 0.005642796124849055\n",
      "Epoch 28/300\n",
      "Average training loss: 0.011309969539443653\n",
      "Average test loss: 0.008332274812377161\n",
      "Epoch 29/300\n",
      "Average training loss: 0.011230146455681987\n",
      "Average test loss: 0.005552124947723415\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011186609650651614\n",
      "Average test loss: 0.012343183134992917\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011025197372668319\n",
      "Average test loss: 0.005560120432327191\n",
      "Epoch 32/300\n",
      "Average training loss: 0.010946962732407781\n",
      "Average test loss: 0.009338456275562445\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010869859793947802\n",
      "Average test loss: 0.005479736359583007\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010786024993492497\n",
      "Average test loss: 0.00554370351218515\n",
      "Epoch 35/300\n",
      "Average training loss: 0.010741231020126078\n",
      "Average test loss: 0.024723485694991217\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011188730476631058\n",
      "Average test loss: 0.005594168444060617\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010560010717146927\n",
      "Average test loss: 0.0060546008911397725\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010499337167375618\n",
      "Average test loss: 0.005525131097270383\n",
      "Epoch 39/300\n",
      "Average training loss: 0.010457517189284166\n",
      "Average test loss: 0.0053602381025751435\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0104621267757482\n",
      "Average test loss: 0.00614319721940491\n",
      "Epoch 41/300\n",
      "Average training loss: 0.010332312216361364\n",
      "Average test loss: 0.01133416818579038\n",
      "Epoch 42/300\n",
      "Average training loss: 0.010237518736057811\n",
      "Average test loss: 0.005322221274591155\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010176782925923665\n",
      "Average test loss: 0.006941730175167322\n",
      "Epoch 44/300\n",
      "Average training loss: 0.010141184540258514\n",
      "Average test loss: 0.005903437512616317\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010071697224759393\n",
      "Average test loss: 0.005260846162835757\n",
      "Epoch 46/300\n",
      "Average training loss: 0.011840884380870395\n",
      "Average test loss: 0.00533162792896231\n",
      "Epoch 47/300\n",
      "Average training loss: 0.010371233133806123\n",
      "Average test loss: 0.005302822320411603\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01010496253023545\n",
      "Average test loss: 0.005372068057457606\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009950587190687657\n",
      "Average test loss: 0.006118837510132128\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009834223617282178\n",
      "Average test loss: 0.005725704161036346\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009866720399922795\n",
      "Average test loss: 0.006304634090512991\n",
      "Epoch 52/300\n",
      "Average training loss: 0.009798013187117047\n",
      "Average test loss: 0.00538927103454868\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009703011165062586\n",
      "Average test loss: 5.518036130534278\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010274151732110316\n",
      "Average test loss: 0.006572859180884229\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009578929737210274\n",
      "Average test loss: 0.011159956177075703\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009584689342313343\n",
      "Average test loss: 0.01727303859591484\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009521199314130677\n",
      "Average test loss: 0.25287838102007903\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009524888271258938\n",
      "Average test loss: 0.3812925053172641\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0094630401076542\n",
      "Average test loss: 0.00590786841015021\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009473139234715038\n",
      "Average test loss: 0.0052555449720886015\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00948702423936791\n",
      "Average test loss: 0.005647863581362698\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009314225755631924\n",
      "Average test loss: 0.007412549128548967\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009284256014559004\n",
      "Average test loss: 0.005528342715154092\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009217487087680235\n",
      "Average test loss: 0.005466372414595551\n",
      "Epoch 65/300\n",
      "Average training loss: 0.00929748144828611\n",
      "Average test loss: 8597.28328104655\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01034453155597051\n",
      "Average test loss: 0.005586168970498774\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009192627574834559\n",
      "Average test loss: 0.0054325879857771925\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009088430523872376\n",
      "Average test loss: 0.0060671583136750595\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009048606478919585\n",
      "Average test loss: 0.005453828831099802\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009542895102666484\n",
      "Average test loss: 0.0054027347550210025\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009051985166552994\n",
      "Average test loss: 0.005323921896103355\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009003613079587618\n",
      "Average test loss: 0.0053083867977062865\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009027202227049403\n",
      "Average test loss: 0.005283966857526037\n",
      "Epoch 74/300\n",
      "Average training loss: 0.010684621468186379\n",
      "Average test loss: 0.005400696659667624\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009575185225241714\n",
      "Average test loss: 0.016972883469528622\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00902528407673041\n",
      "Average test loss: 1.9389158612357245\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008901277653872967\n",
      "Average test loss: 0.0059420558255579735\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008848693658080366\n",
      "Average test loss: 0.0052664542645215985\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00890937873058849\n",
      "Average test loss: 0.005361858632001612\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008790605246606801\n",
      "Average test loss: 0.0054336983660856885\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008787114226569732\n",
      "Average test loss: 0.005476967808273103\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008782385571963256\n",
      "Average test loss: 0.08270529010891914\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00889103288617399\n",
      "Average test loss: 0.005716080156879293\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014144197964833842\n",
      "Average test loss: 8.994470374335018\n",
      "Epoch 85/300\n",
      "Average training loss: 0.011157760436336199\n",
      "Average test loss: 0.0053559301634215645\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010317484359774325\n",
      "Average test loss: 0.008722344628638691\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009852986221512159\n",
      "Average test loss: 0.006702021912568145\n",
      "Epoch 88/300\n",
      "Average training loss: 0.009497829230295287\n",
      "Average test loss: 0.6474308162000444\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009223573561343882\n",
      "Average test loss: 0.0053036856034563646\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009007955793705252\n",
      "Average test loss: 0.006197302499579059\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009139965868658489\n",
      "Average test loss: 0.005383719592251712\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008990457142392794\n",
      "Average test loss: 0.0054687702026632095\n",
      "Epoch 93/300\n",
      "Average training loss: 0.008760539232856697\n",
      "Average test loss: 0.005442839454859495\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008683533378773265\n",
      "Average test loss: 0.005768439109954569\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008686763030787309\n",
      "Average test loss: 0.005775113316045867\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008611755170755917\n",
      "Average test loss: 0.005320414380894767\n",
      "Epoch 97/300\n",
      "Average training loss: 0.00860444843272368\n",
      "Average test loss: 0.00636031462583277\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008611685901052423\n",
      "Average test loss: 0.005605060530619489\n",
      "Epoch 99/300\n",
      "Average training loss: 0.00880658622003264\n",
      "Average test loss: 0.00699578125278155\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06299225417772929\n",
      "Average test loss: 0.00738463334325287\n",
      "Epoch 101/300\n",
      "Average training loss: 0.023510556677977246\n",
      "Average test loss: 0.006608216231481896\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019530208443601926\n",
      "Average test loss: 0.006400091976755195\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017467419566379653\n",
      "Average test loss: 0.0227482159336408\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01615086603992515\n",
      "Average test loss: 0.010769967856092586\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015165828819076221\n",
      "Average test loss: 0.04104012558195326\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014342016244928042\n",
      "Average test loss: 0.0131923042949703\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013704560312959883\n",
      "Average test loss: 0.005741910939415296\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013124095940755474\n",
      "Average test loss: 0.006517221510410309\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012634604902731049\n",
      "Average test loss: 0.0056420539865891135\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012189655877649783\n",
      "Average test loss: 0.005569453418254852\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011793704585068756\n",
      "Average test loss: 0.0062718102369043564\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011563784682088428\n",
      "Average test loss: 0.005879389519078864\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011168135176102321\n",
      "Average test loss: 0.005408711569590701\n",
      "Epoch 114/300\n",
      "Average training loss: 0.18375386000010702\n",
      "Average test loss: 0.47734996984485123\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06081047650509411\n",
      "Average test loss: 0.010014262500736448\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03465024784372912\n",
      "Average test loss: 0.007722907702955935\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02727558422088623\n",
      "Average test loss: 0.009194494760698743\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023521155036158033\n",
      "Average test loss: 0.007080144234001637\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02100368448264069\n",
      "Average test loss: 0.019532560541397997\n",
      "Epoch 120/300\n",
      "Average training loss: 0.019223884731531145\n",
      "Average test loss: 0.00644050783270763\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0178369340515799\n",
      "Average test loss: 0.006618052800082498\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016706568373574152\n",
      "Average test loss: 0.006818574532452557\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015727863354815377\n",
      "Average test loss: 0.03556323210067219\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014870243658622107\n",
      "Average test loss: 0.006137261733826664\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014151820056968266\n",
      "Average test loss: 0.01911150411847565\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01353207093063328\n",
      "Average test loss: 0.006894950478855107\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012968859135276742\n",
      "Average test loss: 0.016026164207607507\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012535253584384919\n",
      "Average test loss: 0.007578376206258933\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012102305087778303\n",
      "Average test loss: 0.010302687312993738\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01173888760846522\n",
      "Average test loss: 0.018755717258486484\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011463648608989185\n",
      "Average test loss: 0.005946149085958799\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011228528722292847\n",
      "Average test loss: 0.007644715122878552\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011004787046462298\n",
      "Average test loss: 0.13480767706533273\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01081831428905328\n",
      "Average test loss: 0.06231752740053667\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01063593047691716\n",
      "Average test loss: 5.982422042225798\n",
      "Epoch 136/300\n",
      "Average training loss: 0.010528679755826791\n",
      "Average test loss: 0.028058880474832324\n",
      "Epoch 137/300\n",
      "Average training loss: 0.010524205378360218\n",
      "Average test loss: 0.005344206255757146\n",
      "Epoch 138/300\n",
      "Average training loss: 0.010264298789203166\n",
      "Average test loss: 0.1679787216950208\n",
      "Epoch 139/300\n",
      "Average training loss: 0.010082824513316154\n",
      "Average test loss: 2.441467402209838\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009971727033456166\n",
      "Average test loss: 1.2755689700378312\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009943522695038053\n",
      "Average test loss: 101.25767959912618\n",
      "Epoch 142/300\n",
      "Average training loss: 0.009771138994230164\n",
      "Average test loss: 0.005358181563930379\n",
      "Epoch 143/300\n",
      "Average training loss: 0.009712058308223883\n",
      "Average test loss: 0.0052293320080886285\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009673924201892482\n",
      "Average test loss: 0.013602240822381444\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009553600517412027\n",
      "Average test loss: 0.005197427021132575\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009507345010009077\n",
      "Average test loss: 0.00878510570194986\n",
      "Epoch 147/300\n",
      "Average training loss: 0.009433018546137545\n",
      "Average test loss: 0.007797281260291736\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009393043149676587\n",
      "Average test loss: 0.4615416558480097\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009503613236877654\n",
      "Average test loss: 3.0887268622848723\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00977919017523527\n",
      "Average test loss: 0.005826716163092189\n",
      "Epoch 151/300\n",
      "Average training loss: 0.010023216769927078\n",
      "Average test loss: 4.382587239280343\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009366863498257267\n",
      "Average test loss: 2670299.314363921\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009838207684457301\n",
      "Average test loss: 0.005830674075004127\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009298844477368725\n",
      "Average test loss: 1488565.6585659722\n",
      "Epoch 155/300\n",
      "Average training loss: 0.00946566783140103\n",
      "Average test loss: 0.4758808720310529\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009218732287486394\n",
      "Average test loss: 0.005310265628827943\n",
      "Epoch 157/300\n",
      "Average training loss: 0.009094318420108822\n",
      "Average test loss: 0.9543666012204356\n",
      "Epoch 158/300\n",
      "Average training loss: 0.009084248666134146\n",
      "Average test loss: 0.023069671163956324\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00903897708406051\n",
      "Average test loss: 0.017298424289044406\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009153070862094562\n",
      "Average test loss: 1272.5823765611797\n",
      "Epoch 161/300\n",
      "Average training loss: 3.1395528160896564\n",
      "Average test loss: 0.25910248152414955\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10057651507192188\n",
      "Average test loss: 0.018568462680611346\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07429966734515296\n",
      "Average test loss: 0.009733393497765063\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06074067175388336\n",
      "Average test loss: 0.0171926107108593\n",
      "Epoch 165/300\n",
      "Average training loss: 0.051091295772128634\n",
      "Average test loss: 0.7528925020098686\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04372548464602894\n",
      "Average test loss: 0.17298848375678064\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03790664100315836\n",
      "Average test loss: 0.010818489230341382\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03351534436477555\n",
      "Average test loss: 19.42155862985717\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03016561172902584\n",
      "Average test loss: 0.04831229912655221\n",
      "Epoch 170/300\n",
      "Average training loss: 0.027500793721940784\n",
      "Average test loss: 0.8303094345165624\n",
      "Epoch 171/300\n",
      "Average training loss: 0.025509349298146036\n",
      "Average test loss: 0.007066862176275916\n",
      "Epoch 172/300\n",
      "Average training loss: 0.023965129262871213\n",
      "Average test loss: 0.8338373227814834\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02272586018509335\n",
      "Average test loss: 6.90242214512825\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021656960919499396\n",
      "Average test loss: 3.761730143653022\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02062043104569117\n",
      "Average test loss: 0.006889065254479646\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019651902213692667\n",
      "Average test loss: 0.0076787216915852495\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018784506797790528\n",
      "Average test loss: 0.007421424907528692\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017960118912988238\n",
      "Average test loss: 0.0075903794633017645\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017127958565950393\n",
      "Average test loss: 0.006486819434911013\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01639277418785625\n",
      "Average test loss: 0.0367405181609922\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015724719658493997\n",
      "Average test loss: 0.006655660833749506\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015109673423899544\n",
      "Average test loss: 0.006166765506896708\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014517688873741361\n",
      "Average test loss: 0.0064027678089009395\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0139713203418586\n",
      "Average test loss: 0.005816632976134618\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013447755192716916\n",
      "Average test loss: 0.005713244152565797\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012994777546160751\n",
      "Average test loss: 0.00991056549217966\n",
      "Epoch 187/300\n",
      "Average training loss: 0.012483174911803669\n",
      "Average test loss: 0.008196240831580427\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013365520632929272\n",
      "Average test loss: 0.007538556300931506\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011793821061650911\n",
      "Average test loss: 0.005425743402292331\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011261438715789053\n",
      "Average test loss: 0.005392118523104323\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01089180068174998\n",
      "Average test loss: 0.0053290814235806465\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010666861658294996\n",
      "Average test loss: 0.005457115426659584\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010402238868176938\n",
      "Average test loss: 0.01026481367316511\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010180728918976254\n",
      "Average test loss: 0.005240087577452262\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009985030809210406\n",
      "Average test loss: 0.005635855082836416\n",
      "Epoch 196/300\n",
      "Average training loss: 0.009777233152753776\n",
      "Average test loss: 0.00612826072714395\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009595294863813454\n",
      "Average test loss: 0.013995085722870297\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009461776258216963\n",
      "Average test loss: 0.005223019688493676\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009280725282099512\n",
      "Average test loss: 2.6408468288866183\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009178171084986793\n",
      "Average test loss: 0.020363946280131738\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009086328588426113\n",
      "Average test loss: 4854.946286387709\n",
      "Epoch 202/300\n",
      "Average training loss: 0.009123497482803132\n",
      "Average test loss: 0.005186130421443118\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0089937546344267\n",
      "Average test loss: 0.6500502247214317\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008951373049782382\n",
      "Average test loss: 0.005186057783249352\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008865497124691804\n",
      "Average test loss: 0.024560038689110015\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00885990389684836\n",
      "Average test loss: 0.005175876445654366\n",
      "Epoch 207/300\n",
      "Average training loss: 0.008808678665094905\n",
      "Average test loss: 0.006420814442137877\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008779758615212309\n",
      "Average test loss: 0.09557185361824101\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008743981516195667\n",
      "Average test loss: 0.2723951034926706\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008752326594458687\n",
      "Average test loss: 1.0906177624530262\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008906358405119844\n",
      "Average test loss: 0.005311197954747412\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00863404154529174\n",
      "Average test loss: 0.27300613679778246\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008583590206586653\n",
      "Average test loss: 0.005816192891034815\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008583437855045001\n",
      "Average test loss: 16.8224763697957\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009321417317208317\n",
      "Average test loss: 4.1971997212991115\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00865168190913068\n",
      "Average test loss: 360751.9615234792\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008598691690299247\n",
      "Average test loss: 0.010351416502147912\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03509306984477573\n",
      "Average test loss: 0.016839298549625608\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015077463042404917\n",
      "Average test loss: 69.29545909226354\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013092457311848799\n",
      "Average test loss: 4.643663950092263\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012091274869938691\n",
      "Average test loss: 0.3324336474206713\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011394069823125998\n",
      "Average test loss: 1703.3537637412185\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010897127920554743\n",
      "Average test loss: 65.38501903328962\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010433149120873874\n",
      "Average test loss: 77.53449998683317\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009999676197767258\n",
      "Average test loss: 9.709707790897124\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00964599440081252\n",
      "Average test loss: 181.94452204109894\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009372203119926982\n",
      "Average test loss: 1627.1985833089332\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00921207275821103\n",
      "Average test loss: 2354902.9254444446\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009004285854597886\n",
      "Average test loss: 0.9468182007306152\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008802984038160907\n",
      "Average test loss: 8231.25826313056\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008763696329047282\n",
      "Average test loss: 0.0053282090771115485\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00861868060214652\n",
      "Average test loss: 12.125729471882185\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00868499077235659\n",
      "Average test loss: 0.007219594535728295\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008547343731754356\n",
      "Average test loss: 1.2746798221353028\n",
      "Epoch 235/300\n",
      "Average training loss: 0.008470028824276395\n",
      "Average test loss: 0.04305223287145297\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008571006821261513\n",
      "Average test loss: 829.3663493172129\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008506654999736282\n",
      "Average test loss: 101.81012294228437\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00845614700184928\n",
      "Average test loss: 0.005928789891302586\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00842847532985939\n",
      "Average test loss: 13.349093729760911\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009769226974083318\n",
      "Average test loss: 395896.76213534584\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009596755614711178\n",
      "Average test loss: 142792.20501131896\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008701150995161798\n",
      "Average test loss: 6.19082444242802\n",
      "Epoch 243/300\n",
      "Average training loss: 0.008433772938946883\n",
      "Average test loss: 38.60126592936946\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008352448541257116\n",
      "Average test loss: 0.012046290683249633\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008304364918834633\n",
      "Average test loss: 0.3884043414990107\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008356638017627928\n",
      "Average test loss: 0.2906848250751694\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008277989885873265\n",
      "Average test loss: 112286632040.064\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008314933088090684\n",
      "Average test loss: 266.9189770625755\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008282860971159406\n",
      "Average test loss: 129103.76725194411\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008238020638624827\n",
      "Average test loss: 205602.38618087806\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008313891886836953\n",
      "Average test loss: 1.3908644754572048\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008231241686476602\n",
      "Average test loss: 3181.1239711600347\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008380619689822197\n",
      "Average test loss: 6051242.820062909\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008355311109787888\n",
      "Average test loss: 53.38551936377916\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008164375194244915\n",
      "Average test loss: 0.005219983836015065\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008216670148074628\n",
      "Average test loss: 50.63181080438693\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008165282325612174\n",
      "Average test loss: 119.7614119342251\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008249649423691961\n",
      "Average test loss: 0.012181574038333363\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008136102508339616\n",
      "Average test loss: 11329.523522240122\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008207941312756803\n",
      "Average test loss: 141.69268990639182\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008057600902186499\n",
      "Average test loss: 1.1906237939095332\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008217856186545558\n",
      "Average test loss: 695.010093041718\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008321629747748374\n",
      "Average test loss: 1.0172211215967932\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007984490058488315\n",
      "Average test loss: 635607.8539501396\n",
      "Epoch 265/300\n",
      "Average training loss: 0.009208667617291211\n",
      "Average test loss: 0.0052884693303041986\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008427867727147209\n",
      "Average test loss: 326.53390690913466\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008068903387006787\n",
      "Average test loss: 8.737906123114957\n",
      "Epoch 268/300\n",
      "Average training loss: 0.00805210042744875\n",
      "Average test loss: 0.03608313069492579\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007951353874471453\n",
      "Average test loss: 0.09833955635295974\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00792623145133257\n",
      "Average test loss: 466.57699155429583\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007924712224139107\n",
      "Average test loss: 86.73754445867075\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007904902635763088\n",
      "Average test loss: 0.0870344077050686\n",
      "Epoch 273/300\n",
      "Average training loss: 0.007848085386885537\n",
      "Average test loss: 0.10868215795026885\n",
      "Epoch 274/300\n",
      "Average training loss: 0.007923372813396983\n",
      "Average test loss: 157634.465556371\n",
      "Epoch 275/300\n",
      "Average training loss: 0.007855755274494489\n",
      "Average test loss: 2.048654200166464\n",
      "Epoch 276/300\n",
      "Average training loss: 0.00787600514292717\n",
      "Average test loss: 746.3569248012006\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007870808627042505\n",
      "Average test loss: 0.6345119274059932\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008172298337022463\n",
      "Average test loss: 90.47982417157624\n",
      "Epoch 279/300\n",
      "Average training loss: 0.007815318956143326\n",
      "Average test loss: 1.7370758022680464\n",
      "Epoch 280/300\n",
      "Average training loss: 0.007830604744040303\n",
      "Average test loss: 0.7744953995065557\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010233920401997037\n",
      "Average test loss: 12637.887234352058\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008285193822450108\n",
      "Average test loss: 0.015666371731294527\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008600731528053681\n",
      "Average test loss: 21.75735725668942\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010062689861489667\n",
      "Average test loss: 60502.89155671075\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008438225586381224\n",
      "Average test loss: 4971.418602489546\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008116227570507261\n",
      "Average test loss: 304033.64573145634\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008256084005865786\n",
      "Average test loss: 161.58977065062317\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007904280739112033\n",
      "Average test loss: 1163.816794161646\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007931456813381779\n",
      "Average test loss: 89.72782460276244\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007880964093738133\n",
      "Average test loss: 64609.226917726955\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007819593084355196\n",
      "Average test loss: 205.7025481155196\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00806033434636063\n",
      "Average test loss: 1283.1151207034952\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0078067629701561395\n",
      "Average test loss: 184061.81723706663\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0077619824690951245\n",
      "Average test loss: 377537.5461873792\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007756728345321285\n",
      "Average test loss: 28.355766913573774\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007851946715679434\n",
      "Average test loss: 6608.900782011882\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007813419342041016\n",
      "Average test loss: 0.012471458789375093\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00768645116686821\n",
      "Average test loss: 216433.21099480442\n",
      "Epoch 299/300\n",
      "Average training loss: 0.007694602700157298\n",
      "Average test loss: 1528.9603299515397\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007668654094139735\n",
      "Average test loss: 57.63783495266984\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14375841173860762\n",
      "Average test loss: 0.022155382575260268\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02353175844417678\n",
      "Average test loss: 0.0071468129290474785\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01823237019446161\n",
      "Average test loss: 0.006505109877636035\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01574179307371378\n",
      "Average test loss: 0.0057486880694826445\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014163723732034365\n",
      "Average test loss: 0.00558708588199483\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013066108808749251\n",
      "Average test loss: 0.0050479734453062215\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012168439495066801\n",
      "Average test loss: 0.004910604440710611\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011467631806102064\n",
      "Average test loss: 0.004734623471481933\n",
      "Epoch 9/300\n",
      "Average training loss: 0.010880236096680164\n",
      "Average test loss: 0.004902331814997726\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010335624412529998\n",
      "Average test loss: 0.005349691588845518\n",
      "Epoch 11/300\n",
      "Average training loss: 0.009882821924156612\n",
      "Average test loss: 0.004362517264982065\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009442105385578341\n",
      "Average test loss: 0.005960112075010936\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009094724124918381\n",
      "Average test loss: 0.004131633814010355\n",
      "Epoch 14/300\n",
      "Average training loss: 0.008760460101895863\n",
      "Average test loss: 0.0043290647342801095\n",
      "Epoch 15/300\n",
      "Average training loss: 0.008447356687237818\n",
      "Average test loss: 0.003973548780712817\n",
      "Epoch 16/300\n",
      "Average training loss: 0.008294723056670693\n",
      "Average test loss: 0.005590349081075854\n",
      "Epoch 17/300\n",
      "Average training loss: 0.007978529518263208\n",
      "Average test loss: 0.003643113411962986\n",
      "Epoch 18/300\n",
      "Average training loss: 0.007757400379826625\n",
      "Average test loss: 0.003566123573523429\n",
      "Epoch 19/300\n",
      "Average training loss: 0.007553860674301784\n",
      "Average test loss: 0.003555118538853195\n",
      "Epoch 20/300\n",
      "Average training loss: 0.007404505826118919\n",
      "Average test loss: 0.00359039383319517\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00726423001786073\n",
      "Average test loss: 0.003438281514164474\n",
      "Epoch 22/300\n",
      "Average training loss: 0.007119060369829336\n",
      "Average test loss: 0.004919355101883412\n",
      "Epoch 23/300\n",
      "Average training loss: 0.006976736955344677\n",
      "Average test loss: 0.003297715918885337\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006858275541414817\n",
      "Average test loss: 0.003418207553525766\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00680797800504499\n",
      "Average test loss: 0.003277936176293426\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006640561496838927\n",
      "Average test loss: 0.003337396307124032\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010749577508204513\n",
      "Average test loss: 0.0034598797911571133\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007142133210682207\n",
      "Average test loss: 0.003359349138620827\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00684810418014725\n",
      "Average test loss: 0.0032556070774379703\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0066940226914982\n",
      "Average test loss: 0.0032131168206946717\n",
      "Epoch 31/300\n",
      "Average training loss: 0.006582791090011596\n",
      "Average test loss: 0.0031938428485559093\n",
      "Epoch 32/300\n",
      "Average training loss: 0.006526997047166029\n",
      "Average test loss: 0.003252351087000635\n",
      "Epoch 33/300\n",
      "Average training loss: 0.006437584796299537\n",
      "Average test loss: 0.0032388131324615744\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006370727574245797\n",
      "Average test loss: 0.0031516831583446926\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0063307695711652436\n",
      "Average test loss: 0.003115773396152589\n",
      "Epoch 36/300\n",
      "Average training loss: 0.006288125868058867\n",
      "Average test loss: 0.003196629323686163\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0062437756587233805\n",
      "Average test loss: 0.0031677101722194087\n",
      "Epoch 38/300\n",
      "Average training loss: 0.006172891786528958\n",
      "Average test loss: 0.0031900081224739553\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0061793266137440996\n",
      "Average test loss: 0.0032352282783637445\n",
      "Epoch 40/300\n",
      "Average training loss: 0.00607967897463176\n",
      "Average test loss: 0.003071647315596541\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0061217176612052655\n",
      "Average test loss: 0.003037417632424169\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006003056236439281\n",
      "Average test loss: 0.0031204565233654445\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005977461400545306\n",
      "Average test loss: 0.003087344289653831\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005938000407897764\n",
      "Average test loss: 0.0030461183355914223\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005917519648869832\n",
      "Average test loss: 0.003374802351411846\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005823800507932901\n",
      "Average test loss: 0.0029995851858208578\n",
      "Epoch 47/300\n",
      "Average training loss: 0.006289627738090025\n",
      "Average test loss: 0.0030160395490626494\n",
      "Epoch 48/300\n",
      "Average training loss: 0.006062655426147911\n",
      "Average test loss: 0.0031432193921258054\n",
      "Epoch 49/300\n",
      "Average training loss: 0.005974105720718701\n",
      "Average test loss: 0.003016860644436545\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005804994446535905\n",
      "Average test loss: 0.0031082131734324826\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005757808813204368\n",
      "Average test loss: 0.002957293268914024\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005721820482363304\n",
      "Average test loss: 0.003011879669295417\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005709021708617608\n",
      "Average test loss: 0.0029656975807415115\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005692700953947173\n",
      "Average test loss: 0.00297466437270244\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005610227884517776\n",
      "Average test loss: 0.0029479072648617957\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005595819934788677\n",
      "Average test loss: 0.0030936695887810654\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005565723269349999\n",
      "Average test loss: 0.0030417179730203416\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005679162398187651\n",
      "Average test loss: 1.169366089688407\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005612137188514074\n",
      "Average test loss: 0.0033702729133268197\n",
      "Epoch 60/300\n",
      "Average training loss: 0.005498850077390671\n",
      "Average test loss: 0.0029680812710689173\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00545442484650347\n",
      "Average test loss: 0.0029496673349705006\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005437040046685272\n",
      "Average test loss: 0.0029827608149498703\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005404425673186779\n",
      "Average test loss: 0.002936073128134012\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005359510598083337\n",
      "Average test loss: 0.0029535337657564218\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005543263530979554\n",
      "Average test loss: 0.0029746874496340754\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0053399208527472285\n",
      "Average test loss: 0.0029825300901300377\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005292490731924772\n",
      "Average test loss: 0.0029637814845061964\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005264960275755988\n",
      "Average test loss: 0.002997660151372353\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006738794558578068\n",
      "Average test loss: 0.0032340535980959735\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005991342909220192\n",
      "Average test loss: 0.002981106537911627\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005594194520471825\n",
      "Average test loss: 0.0029603322864406637\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005430537841386265\n",
      "Average test loss: 0.0029278114202121895\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005313616101526552\n",
      "Average test loss: 0.003031948205911451\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00525276101132234\n",
      "Average test loss: 0.0029524520933628083\n",
      "Epoch 75/300\n",
      "Average training loss: 0.005222401051885552\n",
      "Average test loss: 0.002935573434871104\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0051863432240982854\n",
      "Average test loss: 0.0031576494351029396\n",
      "Epoch 77/300\n",
      "Average training loss: 0.005174884407056703\n",
      "Average test loss: 0.0029482676525496776\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005151662746237384\n",
      "Average test loss: 0.002988594172315465\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007368364430963993\n",
      "Average test loss: 0.0031222312063392665\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005823107754190763\n",
      "Average test loss: 0.0029640294280317094\n",
      "Epoch 81/300\n",
      "Average training loss: 0.005516383967879746\n",
      "Average test loss: 0.0029531240846133893\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00535168596646852\n",
      "Average test loss: 0.0029345559800664583\n",
      "Epoch 83/300\n",
      "Average training loss: 0.005241639513522386\n",
      "Average test loss: 0.003024068157705996\n",
      "Epoch 84/300\n",
      "Average training loss: 0.005331652909103367\n",
      "Average test loss: 0.003209400270341171\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005173829660233524\n",
      "Average test loss: 0.0029820351283997297\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00506891637088524\n",
      "Average test loss: 0.0029677868768986726\n",
      "Epoch 87/300\n",
      "Average training loss: 0.005054693455704385\n",
      "Average test loss: 0.003060928374942806\n",
      "Epoch 88/300\n",
      "Average training loss: 0.005074835978655352\n",
      "Average test loss: 0.0029404186374611327\n",
      "Epoch 89/300\n",
      "Average training loss: 0.005089983164850208\n",
      "Average test loss: 0.003682274718665414\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005029276786372066\n",
      "Average test loss: 0.00296391069309579\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005217584045810832\n",
      "Average test loss: 0.0029761941739254527\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004991507219771544\n",
      "Average test loss: 0.0029976118157307306\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004978268101397488\n",
      "Average test loss: 0.0029630567108591397\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004935519392705626\n",
      "Average test loss: 0.003018355421204534\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005480984109350376\n",
      "Average test loss: 0.0029015239821746945\n",
      "Epoch 96/300\n",
      "Average training loss: 0.005189548731678062\n",
      "Average test loss: 0.0029364937599748375\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005023974360070294\n",
      "Average test loss: 0.0030080429895056617\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0049248552910155724\n",
      "Average test loss: 0.003124206013356646\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004902731319475505\n",
      "Average test loss: 0.0030278104963815874\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0049009481685029135\n",
      "Average test loss: 0.0030203182916674347\n",
      "Epoch 101/300\n",
      "Average training loss: 0.00524096519086096\n",
      "Average test loss: 0.0030218620138863722\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004904969989425606\n",
      "Average test loss: 0.003014919583996137\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004826869989641838\n",
      "Average test loss: 0.003032495774328709\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004816799542556207\n",
      "Average test loss: 0.003269227369998892\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004797958176583052\n",
      "Average test loss: 0.0030285934689972135\n",
      "Epoch 106/300\n",
      "Average training loss: 0.007188183420648178\n",
      "Average test loss: 0.44003192223360144\n",
      "Epoch 107/300\n",
      "Average training loss: 0.00555599908116791\n",
      "Average test loss: 0.0031517864252544113\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005279710335036119\n",
      "Average test loss: 0.018898393099092775\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0051606550709240966\n",
      "Average test loss: 0.002910576043443547\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005092563666817215\n",
      "Average test loss: 0.0030130700413137676\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005047301397969326\n",
      "Average test loss: 0.0029344917461276055\n",
      "Epoch 112/300\n",
      "Average training loss: 0.005022109738240639\n",
      "Average test loss: 0.0029513610787689688\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005005478190879027\n",
      "Average test loss: 0.003191839947882626\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0050288436714973715\n",
      "Average test loss: 0.0029872182700783016\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004919039106617371\n",
      "Average test loss: 0.0030076411279539267\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004897352897458606\n",
      "Average test loss: 0.002975280517919196\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004919170340316163\n",
      "Average test loss: 0.002932034707835151\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004867670965070525\n",
      "Average test loss: 0.0030623254678729507\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004858375982277923\n",
      "Average test loss: 0.002945048668111364\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004839130825259619\n",
      "Average test loss: 0.003148347582668066\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004838738254788849\n",
      "Average test loss: 0.0034564943671640423\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004875871851212448\n",
      "Average test loss: 0.0029601613154841794\n",
      "Epoch 123/300\n",
      "Average training loss: 141.28527894322428\n",
      "Average test loss: 0.2573468326280514\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0937875013285213\n",
      "Average test loss: 0.018080130879249836\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06023286703228951\n",
      "Average test loss: 0.08160897029108471\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04790519024266137\n",
      "Average test loss: 0.008240792979796727\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04016549700498581\n",
      "Average test loss: 0.009192178264260291\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0345712199707826\n",
      "Average test loss: 0.006516794596695238\n",
      "Epoch 129/300\n",
      "Average training loss: 0.030496968375311958\n",
      "Average test loss: 0.020747520860698487\n",
      "Epoch 130/300\n",
      "Average training loss: 0.027511827336417303\n",
      "Average test loss: 0.005374245877481169\n",
      "Epoch 131/300\n",
      "Average training loss: 0.025291594634453456\n",
      "Average test loss: 0.007297086507909827\n",
      "Epoch 132/300\n",
      "Average training loss: 0.023604976798097293\n",
      "Average test loss: 0.025545878193444675\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02219847444858816\n",
      "Average test loss: 0.00552878250181675\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02100006335311466\n",
      "Average test loss: 0.0092801453984446\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019795870050787926\n",
      "Average test loss: 0.013666808789389001\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018611050087544653\n",
      "Average test loss: 0.006506354428827763\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017464064568281173\n",
      "Average test loss: 0.015969343735939927\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01635666120549043\n",
      "Average test loss: 0.010982174784359006\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01537568030340804\n",
      "Average test loss: 0.004843445937252707\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014509184239639176\n",
      "Average test loss: 0.005948015823132462\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013718165288368861\n",
      "Average test loss: 0.003930954699714978\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013019336375925276\n",
      "Average test loss: 0.0038010835134320788\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01237432541201512\n",
      "Average test loss: 0.0038316428812427655\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011759877815014787\n",
      "Average test loss: 0.0037197687617606585\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011204443484544755\n",
      "Average test loss: 0.0037356139247616132\n",
      "Epoch 146/300\n",
      "Average training loss: 0.010674328417413765\n",
      "Average test loss: 0.0036722848527133466\n",
      "Epoch 147/300\n",
      "Average training loss: 0.010103402572787471\n",
      "Average test loss: 0.0035099154675586356\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009532293229053418\n",
      "Average test loss: 0.003912497696363264\n",
      "Epoch 149/300\n",
      "Average training loss: 0.00894855926434199\n",
      "Average test loss: 0.003402288862193624\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008397755300005277\n",
      "Average test loss: 0.0033710594326257707\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007842937976949745\n",
      "Average test loss: 0.0033573065085543527\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007317402648842997\n",
      "Average test loss: 0.0035845895759347413\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006957494167404043\n",
      "Average test loss: 0.0031869587873419127\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006666785063842932\n",
      "Average test loss: 0.003077203221205208\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006452698084215323\n",
      "Average test loss: 0.0041638601683080196\n",
      "Epoch 156/300\n",
      "Average training loss: 0.006242494691577223\n",
      "Average test loss: 0.005110266279843119\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00606987831948532\n",
      "Average test loss: 0.0030952515916691887\n",
      "Epoch 158/300\n",
      "Average training loss: 0.005908851808557908\n",
      "Average test loss: 0.03318902073303859\n",
      "Epoch 159/300\n",
      "Average training loss: 0.005783028899381558\n",
      "Average test loss: 0.0029638295150879357\n",
      "Epoch 160/300\n",
      "Average training loss: 0.005661761200262441\n",
      "Average test loss: 0.0029727298627711006\n",
      "Epoch 161/300\n",
      "Average training loss: 0.005548684361080329\n",
      "Average test loss: 0.0030765097805609304\n",
      "Epoch 162/300\n",
      "Average training loss: 0.005464106633431382\n",
      "Average test loss: 0.0054939310629334714\n",
      "Epoch 163/300\n",
      "Average training loss: 0.005385176348189513\n",
      "Average test loss: 0.007170572067300479\n",
      "Epoch 164/300\n",
      "Average training loss: 0.005309851556602452\n",
      "Average test loss: 0.00319123044423759\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0052301020622253416\n",
      "Average test loss: 0.022864070566164123\n",
      "Epoch 166/300\n",
      "Average training loss: 0.005185501515865326\n",
      "Average test loss: 0.0031270337589085102\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0051074067114128\n",
      "Average test loss: 0.008219548100605607\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0050624745183934765\n",
      "Average test loss: 0.043337976589798925\n",
      "Epoch 169/300\n",
      "Average training loss: 0.005012370424552096\n",
      "Average test loss: 0.0029298306703567506\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004975799347377486\n",
      "Average test loss: 0.0030396901505688827\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004943991555935807\n",
      "Average test loss: 0.15693020352555645\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004895809544250369\n",
      "Average test loss: 0.0030405953029791514\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004880148524211513\n",
      "Average test loss: 1.8164383316967223\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0048476110376003715\n",
      "Average test loss: 0.007047755686152312\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00482460631388757\n",
      "Average test loss: 0.0029965477254655626\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004785017709765169\n",
      "Average test loss: 0.03144287774380711\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0047614771148396864\n",
      "Average test loss: 0.0033720552652246423\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00476735967439082\n",
      "Average test loss: 0.003304973348147339\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004751560480644306\n",
      "Average test loss: 0.0041674348939624095\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004778007162527906\n",
      "Average test loss: 0.002974635218994485\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004736272011987037\n",
      "Average test loss: 0.004832729890114731\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004690879128873348\n",
      "Average test loss: 0.0037346163702507815\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004690256188106205\n",
      "Average test loss: 0.009838983413556384\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004670530620134539\n",
      "Average test loss: 0.01055989009141922\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00465633728230993\n",
      "Average test loss: 0.3126903664999538\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00464238569761316\n",
      "Average test loss: 0.0029613433856931\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004643108896497223\n",
      "Average test loss: 0.006285247194373773\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0046109548670550185\n",
      "Average test loss: 0.057443780378748976\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004604485482391384\n",
      "Average test loss: 0.009313763551414013\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004604816584123505\n",
      "Average test loss: 0.32736993765996564\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004932104496906201\n",
      "Average test loss: 0.015062533987168636\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004607070476230648\n",
      "Average test loss: 0.0030030448734760283\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004542737415681283\n",
      "Average test loss: 0.003704673065079583\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0045292294056465225\n",
      "Average test loss: 0.015087105607820882\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0059440508642130425\n",
      "Average test loss: 0.04186133181138171\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00486829009734922\n",
      "Average test loss: 0.002983136382781797\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0046806417264872125\n",
      "Average test loss: 0.06813416050581468\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0045740392125315135\n",
      "Average test loss: 0.0030309555285299816\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004572442074202829\n",
      "Average test loss: 0.0030129909933441215\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004583593077543709\n",
      "Average test loss: 0.003150069803206457\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0045340240564611224\n",
      "Average test loss: 0.0034701406390716632\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004494140524417162\n",
      "Average test loss: 0.06192300220247772\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004498877843221029\n",
      "Average test loss: 0.20950410039578046\n",
      "Epoch 204/300\n",
      "Average training loss: 0.00449590389844444\n",
      "Average test loss: 0.0029827297306102185\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004501071820242538\n",
      "Average test loss: 0.004158117078244686\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004506794821884897\n",
      "Average test loss: 0.00357144915809234\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004539494944943322\n",
      "Average test loss: 0.0029850128433770605\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004475643489509821\n",
      "Average test loss: 0.003321442283069094\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004566991947591305\n",
      "Average test loss: 0.0058594361305650735\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004405275566710366\n",
      "Average test loss: 0.003914610890050729\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004448615573967496\n",
      "Average test loss: 0.01608401191400157\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004406953594336907\n",
      "Average test loss: 0.002978348838786284\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004408532645967272\n",
      "Average test loss: 0.0059937654849555756\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004410342136604919\n",
      "Average test loss: 0.1310264343586233\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004397468277563652\n",
      "Average test loss: 0.003232078889177905\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004374066168649329\n",
      "Average test loss: 0.0030963223326123425\n",
      "Epoch 217/300\n",
      "Average training loss: 0.00488671360620194\n",
      "Average test loss: 0.0029798694093608196\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0045824274714622235\n",
      "Average test loss: 0.003925263738880555\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004391979014707936\n",
      "Average test loss: 0.003035314660312401\n",
      "Epoch 220/300\n",
      "Average training loss: 0.00433995989875661\n",
      "Average test loss: 0.0031012152868012586\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00433633973573645\n",
      "Average test loss: 0.0030328610019965305\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004340247857073942\n",
      "Average test loss: 0.0030471061219771705\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004346250775373644\n",
      "Average test loss: 0.003185970561992791\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0043642611085540715\n",
      "Average test loss: 0.0029945901804086235\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004568092313491636\n",
      "Average test loss: 0.0032834416582352586\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00438474052813318\n",
      "Average test loss: 0.004969446116851435\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004283547607974874\n",
      "Average test loss: 0.0030205897746814624\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0042785791017942955\n",
      "Average test loss: 0.0031371437035914926\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004350154736803637\n",
      "Average test loss: 3.1938368167016242\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0042963044837945035\n",
      "Average test loss: 0.03291962798891796\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004260290580077304\n",
      "Average test loss: 0.09188153535065552\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00426893702459832\n",
      "Average test loss: 0.0032720926139089795\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00444298389636808\n",
      "Average test loss: 0.003000710300066405\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004265208546072244\n",
      "Average test loss: 0.006787828688820203\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004237435903400183\n",
      "Average test loss: 0.00399202378673686\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004230713331244058\n",
      "Average test loss: 0.0030697797036005393\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004233655442587203\n",
      "Average test loss: 0.0030529399290680887\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00424591021074189\n",
      "Average test loss: 0.0031611661575734616\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004243766715129217\n",
      "Average test loss: 0.0030260730632063414\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004361393849261933\n",
      "Average test loss: 0.0031443187890367375\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004614944679041703\n",
      "Average test loss: 0.012238494103153547\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004260171938273642\n",
      "Average test loss: 0.003010117491500245\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00444094684223334\n",
      "Average test loss: 0.003941011584881279\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004267552505350775\n",
      "Average test loss: 0.0063051691589256125\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0042171317239602405\n",
      "Average test loss: 0.0045428082804299065\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004199694677773449\n",
      "Average test loss: 0.0030793677117261623\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004187397958917751\n",
      "Average test loss: 0.0038131873620053133\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004178032410641511\n",
      "Average test loss: 0.0046017363435692255\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004182712602946493\n",
      "Average test loss: 0.0037558274016612106\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004210846438176102\n",
      "Average test loss: 1147072.1152918837\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004207353068308698\n",
      "Average test loss: 0.0031450340805782213\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004187468553996748\n",
      "Average test loss: 0.030765839946352774\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004276088741090563\n",
      "Average test loss: 0.003027468476858404\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004158181656979852\n",
      "Average test loss: 0.003949243867769838\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0041528947096731925\n",
      "Average test loss: 0.006532241836190224\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004137255747699076\n",
      "Average test loss: 0.0032663237851940923\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004135616063657734\n",
      "Average test loss: 0.010049391022158993\n",
      "Epoch 258/300\n",
      "Average training loss: 0.004130904903428422\n",
      "Average test loss: 0.006937959480409821\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004158533657590548\n",
      "Average test loss: 5.076194087515275\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00427433187680112\n",
      "Average test loss: 0.004049520479722155\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004255161393019888\n",
      "Average test loss: 0.003027530003546013\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0041135405320674185\n",
      "Average test loss: 0.003147279869351122\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0040997703959130575\n",
      "Average test loss: 0.003072262131091621\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00409484148853355\n",
      "Average test loss: 0.003975332853280836\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004126083437353372\n",
      "Average test loss: 0.06556051754206418\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004114561525070005\n",
      "Average test loss: 0.003311509149356021\n",
      "Epoch 267/300\n",
      "Average training loss: 0.00409938914494382\n",
      "Average test loss: 0.04175338878813717\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004277233901951048\n",
      "Average test loss: 0.0031164510483957\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004092446836746401\n",
      "Average test loss: 0.023872287534384265\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004092850983142853\n",
      "Average test loss: 0.01296233709818787\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004143714362962379\n",
      "Average test loss: 0.003039292807587319\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004071373593062162\n",
      "Average test loss: 0.0031466302888260946\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004055098880289329\n",
      "Average test loss: 0.007820124248663584\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004060337567081054\n",
      "Average test loss: 0.023496191209389103\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0042402205616235734\n",
      "Average test loss: 0.1247099012268914\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0041590685484309995\n",
      "Average test loss: 0.16113958075973722\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004065882313996553\n",
      "Average test loss: 0.017018160773648157\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004079910948044724\n",
      "Average test loss: 0.0031347072230031095\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004032070493118631\n",
      "Average test loss: 0.07092418777114814\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005896004831625355\n",
      "Average test loss: 0.0031173551835947566\n",
      "Epoch 281/300\n",
      "Average training loss: 0.005808418791741133\n",
      "Average test loss: 0.0029694176047212546\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00485432675646411\n",
      "Average test loss: 0.0056216944505771\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004362910690406958\n",
      "Average test loss: 0.0041673271370430785\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0041579536828729845\n",
      "Average test loss: 0.0031097604766901995\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004075590787248479\n",
      "Average test loss: 0.005457145411107275\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004035606327570147\n",
      "Average test loss: 0.003548457299462623\n",
      "Epoch 287/300\n",
      "Average training loss: 0.004031661743505134\n",
      "Average test loss: 0.045735386772288215\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004026233915239572\n",
      "Average test loss: 0.0031438715234398842\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004017314483929012\n",
      "Average test loss: 0.003108610250780152\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004062369020448791\n",
      "Average test loss: 0.0030152221930523715\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004016714699980286\n",
      "Average test loss: 0.006024769461817211\n",
      "Epoch 292/300\n",
      "Average training loss: 0.004013887859880924\n",
      "Average test loss: 0.003198128243080444\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004022062694860829\n",
      "Average test loss: 0.003220030149237977\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00412767519781159\n",
      "Average test loss: 0.003097445865265197\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004027056068595913\n",
      "Average test loss: 0.03744285355011622\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004009896393451426\n",
      "Average test loss: 0.0033589043211605814\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003985125832673576\n",
      "Average test loss: 0.003427736940069331\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004009635229905446\n",
      "Average test loss: 0.003091955636938413\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004106909920771917\n",
      "Average test loss: 0.11515795542134179\n",
      "Epoch 300/300\n",
      "Average training loss: 0.003976091127842665\n",
      "Average test loss: 0.00319221311290231\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12304126533865929\n",
      "Average test loss: 0.0065599030926823615\n",
      "Epoch 2/300\n",
      "Average training loss: 0.017759625112016997\n",
      "Average test loss: 0.004985844033045901\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01354586195283466\n",
      "Average training loss: 0.009370817686120669\n",
      "Average test loss: 0.0036628985988597074\n",
      "Epoch 7/300\n",
      "Average training loss: 0.008651954419083065\n",
      "Average test loss: 0.003459338923295339\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008073288497825463\n",
      "Average test loss: 0.0034731122536791697\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007601248930725786\n",
      "Average test loss: 0.003539121515634987\n",
      "Epoch 10/300\n",
      "Average training loss: 0.007211964262856378\n",
      "Average test loss: 0.003262599500724011\n",
      "Epoch 11/300\n",
      "Average training loss: 0.006231986819869942\n",
      "Average test loss: 0.00281149644859963\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006183080041160186\n",
      "Average test loss: 0.0028672745482375226\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005726590443402528\n",
      "Average test loss: 0.0027429693492336408\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00551728636233343\n",
      "Average test loss: 0.002538341579337915\n",
      "Epoch 17/300\n",
      "Average training loss: 0.005354273416308893\n",
      "Average test loss: 0.002497692727173368\n",
      "Epoch 18/300\n",
      "Average training loss: 0.005190378929798802\n",
      "Average test loss: 0.002390180154186156\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005010054142524798\n",
      "Average test loss: 0.0030982460743851133\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00489032285577721\n",
      "Average test loss: 0.0023058648459199403\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004959861433754364\n",
      "Average test loss: 0.002492585515603423\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0046371081322431566\n",
      "Average test loss: 0.002252401794410414\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0046328130240241685\n",
      "Average test loss: 0.0021673625537918673\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004452818547065059\n",
      "Average test loss: 0.0022370167691260576\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004514607638948493\n",
      "Average test loss: 0.0035243033758468097\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004506100803820624\n",
      "Average test loss: 0.0021637857399053042\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004272404765089353\n",
      "Average test loss: 0.002116994761240979\n",
      "Epoch 28/300\n",
      "Average training loss: 0.004200386133044958\n",
      "Average test loss: 0.0020914895952575735\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004189169962580006\n",
      "Average test loss: 0.0021931994271775085\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004110459654488498\n",
      "Average test loss: 0.0020599686230222385\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004041413634808527\n",
      "Average test loss: 0.0020029985095477766\n",
      "Epoch 32/300\n",
      "Average training loss: 0.003994413691469365\n",
      "Average test loss: 0.001997260302098261\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0062239209881259335\n",
      "Average test loss: 0.004534373970909251\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004061747801179687\n",
      "Average test loss: 0.0019962369822379617\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0039918729679452045\n",
      "Average test loss: 0.0020061994230167733\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003956056327455573\n",
      "Average test loss: 0.0024482754508240354\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003920523615553975\n",
      "Average test loss: 0.002004442054364416\n",
      "Epoch 40/300\n",
      "Average training loss: 0.00391817515467604\n",
      "Average test loss: 0.002152976043211917\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0038713897234863705\n",
      "Average test loss: 0.001936093755480316\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0038380206007924346\n",
      "Average test loss: 0.0019578136520253288\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0038876529793358513\n",
      "Average test loss: 0.001934449310724934\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003763838205486536\n",
      "Average test loss: 0.0019994051684108045\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00372543572075665\n",
      "Average test loss: 0.0019340143133368757\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00391576751983828\n",
      "Average test loss: 0.0019370884390340912\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0036844817271663084\n",
      "Average test loss: 0.0019165702559467818\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0036608272112078136\n",
      "Average test loss: 0.0019321297680338224\n",
      "Epoch 49/300\n",
      "Average training loss: 0.003649363728861014\n",
      "Average test loss: 0.0019307293173753553\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0036268690975589887\n",
      "Average test loss: 0.0021779044752733576\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0035902294290976393\n",
      "Average test loss: 0.001951914818647007\n",
      "Epoch 52/300\n",
      "Average training loss: 0.003634107787369026\n",
      "Average test loss: 0.002076927315029833\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0035626205861982374\n",
      "Average test loss: 0.0018880638597119186\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003502149185579684\n",
      "Average test loss: 0.001967568921753102\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003482339229227768\n",
      "Average test loss: 0.001966919969353411\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0034420676475597754\n",
      "Average test loss: 0.0019073101971298456\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0034419096735202603\n",
      "Average test loss: 0.0020799851533439425\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0034187862403276892\n",
      "Average test loss: 0.001887802020439671\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0033741731513291595\n",
      "Average test loss: 0.0018832728998321626\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003486397117583288\n",
      "Average test loss: 0.00190195474266592\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0033499836956875193\n",
      "Average test loss: 0.001895065873550872\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0033267192805392877\n",
      "Average test loss: 0.0026520518478420044\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0033031237936682173\n",
      "Average test loss: 0.0018980568394892745\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003315555861219764\n",
      "Average test loss: 0.0018943984458843867\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0032875231185721028\n",
      "Average test loss: 0.14540786083870463\n",
      "Epoch 66/300\n",
      "Average training loss: 0.003404671696739064\n",
      "Average test loss: 0.0021122025231727296\n",
      "Epoch 67/300\n",
      "Average training loss: 0.003397084878343675\n",
      "Average test loss: 0.0019345718477335242\n",
      "Epoch 68/300\n",
      "Average training loss: 0.003527071286820703\n",
      "Average test loss: 0.006769787336389224\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004504193148679204\n",
      "Average test loss: 0.011178784161185226\n",
      "Epoch 70/300\n",
      "Average training loss: 0.003764614169175426\n",
      "Average test loss: 0.001886520933980743\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0036213982701301575\n",
      "Average test loss: 0.0019100440165234936\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0035453286280648574\n",
      "Average test loss: 0.001946997513063252\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0034652654563801155\n",
      "Average test loss: 0.002263618387074934\n",
      "Epoch 74/300\n",
      "Average training loss: 0.003382251277979877\n",
      "Average test loss: 0.0018819601554423571\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004172359435301688\n",
      "Average test loss: 0.0019068992224832377\n",
      "Epoch 76/300\n",
      "Average training loss: 0.003578551611345675\n",
      "Average test loss: 0.0019305317816841934\n",
      "Epoch 77/300\n",
      "Average training loss: 0.003446880064697729\n",
      "Average test loss: 0.0018540315356933407\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0032879284872776933\n",
      "Average test loss: 0.0018646121421932347\n",
      "Epoch 81/300\n",
      "Average training loss: 0.003270629523942868\n",
      "Average test loss: 0.0019247260879104336\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0032852639976061054\n",
      "Average test loss: 0.0019659332038006847\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003218713000416756\n",
      "Average test loss: 0.0018903644505060381\n",
      "Epoch 84/300\n",
      "Average training loss: 0.003222222390481167\n",
      "Average test loss: 0.0018961623054411676\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0034116881019953223\n",
      "Average test loss: 0.0018563010876791344\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003260414241088761\n",
      "Average test loss: 0.0019105024819986687\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0031550826742831202\n",
      "Average test loss: 0.001896339338997172\n",
      "Epoch 88/300\n",
      "Average training loss: 0.003134651164731218\n",
      "Average test loss: 0.0019232141063031223\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0033878815701852243\n",
      "Average test loss: 0.002430817722239428\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003534448386894332\n",
      "Average test loss: 0.0018951779363883866\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003139918218884203\n",
      "Average test loss: 0.0018874310867653952\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0030980685845845277\n",
      "Average test loss: 0.0018920793674058385\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0030871880960961183\n",
      "Average test loss: 0.002191623613652256\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0031073413137346507\n",
      "Average test loss: 0.0018632115720667772\n",
      "Epoch 95/300\n",
      "Average training loss: 0.003254584935183326\n",
      "Average test loss: 0.0018702270492083495\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00332338059734967\n",
      "Average test loss: 0.0018664667652919889\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0031021373404396904\n",
      "Average test loss: 0.0019112883518553443\n",
      "Epoch 98/300\n",
      "Average training loss: 0.003077267062332895\n",
      "Average test loss: 0.00223823674602641\n",
      "Epoch 99/300\n",
      "Average training loss: 0.00307340922123856\n",
      "Average test loss: 0.0018708159239548775\n",
      "Epoch 100/300\n",
      "Average test loss: 0.0018981798138055536\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0030088067334145307\n",
      "Average test loss: 0.0019643727044264475\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0030229829284879896\n",
      "Average test loss: 0.0018744741433507038\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0030630382976184287\n",
      "Average test loss: 0.0018865491412580013\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002996939453192883\n",
      "Average test loss: 0.001904286368129154\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0030020497911092308\n",
      "Average test loss: 0.0019048725976091292\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0029531091292285257\n",
      "Average test loss: 0.0020443744413140745\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0029522895483920972\n",
      "Average test loss: 0.001986390894278884\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002955551529510154\n",
      "Average test loss: 0.001996952644135389\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002932581233481566\n",
      "Average test loss: 0.0020988003915796676\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00291392665128741\n",
      "Average test loss: 0.0018781140330764983\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0032110533016837307\n",
      "Average test loss: 0.002030563812702894\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002951150738323728\n",
      "Average test loss: 0.00198119290628367\n",
      "Epoch 114/300\n",
      "Average training loss: 0.002888324527276887\n",
      "Average test loss: 0.0018878257524015175\n",
      "Epoch 115/300\n",
      "Average training loss: 0.002880999259650707\n",
      "Average test loss: 0.0030595236270584995\n",
      "Epoch 116/300\n",
      "Average training loss: 0.003113275797002845\n",
      "Average test loss: 0.0018681066911667586\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002882536976494723\n",
      "Average test loss: 0.0019280546453988385\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0028575218502018187\n",
      "Average test loss: 0.0018970503132376406\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0028703029147452777\n",
      "Average test loss: 0.0019758040878093907\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002892999421598183\n",
      "Average test loss: 0.0019095509404109584\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002876648166527351\n",
      "Average test loss: 0.0019566400891376868\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0029527758701394\n",
      "Average test loss: 0.001966318103277849\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002944091222559412\n",
      "Average test loss: 0.0019033914113210308\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0028316851589414807\n",
      "Average test loss: 0.0019196500666439533\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0028060062939508092\n",
      "Average test loss: 0.002003552345559001\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0028182324994769363\n",
      "Average test loss: 0.0019546784119059643\n",
      "Epoch 130/300\n",
      "Average training loss: 0.003261062850140863\n",
      "Average test loss: 0.0018817514895151058\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002843174877886971\n",
      "Average test loss: 0.0018903406827400129\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002810641086039444\n",
      "Average test loss: 0.00264029033937388\n",
      "Epoch 133/300\n",
      "Average training loss: 0.002788402823317382\n",
      "Average test loss: 0.0018785215915284223\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0028945049689047867\n",
      "Average test loss: 0.001949668989620275\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0027655942698733673\n",
      "Average test loss: 0.0021013661202871135\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0029042836351113187\n",
      "Average test loss: 0.0020459494387937917\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0027555093558298215\n",
      "Average test loss: 0.0019285791646689178\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002765184611082077\n",
      "Average test loss: 0.0020339259306589763\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0030265378521548377\n",
      "Average test loss: 0.002402524124003119\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0030328359997106925\n",
      "Average test loss: 0.004314567377480368\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0027807571575459507\n",
      "Average test loss: 0.0019546385975554585\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002729756287402577\n",
      "Average test loss: 0.0019181118255688085\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0027194622995124922\n",
      "Average test loss: 0.009910727109553085\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0027247789547675187\n",
      "Average test loss: 0.0024781052951390547\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0027005641023731893\n",
      "Average test loss: 0.0019578170304497084\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002816718378621671\n",
      "Average test loss: 0.0019411805767772926\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0026977116295860875\n",
      "Average test loss: 0.0020760385066063867\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0027470712761084237\n",
      "Average test loss: 0.001963866348999242\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002681072005381187\n",
      "Average test loss: 0.001928954715737038\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0026787030210511553\n",
      "Average test loss: 0.0020004385548333327\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0028152923250777855\n",
      "Average test loss: 0.0019206943180825975\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0026827484805964762\n",
      "Average test loss: 0.0019332211856833762\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0026619354770001436\n",
      "Average test loss: 0.0020272309365164903\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0026541343258900773\n",
      "Average test loss: 0.0019428609038392702\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002653849651830064\n",
      "Average test loss: 0.0020130981300026177\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0026587607448713647\n",
      "Average test loss: 0.0019865965400305058\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0027821928820469313\n",
      "Average test loss: 0.002015442044991586\n",
      "Epoch 160/300\n",
      "Average training loss: 0.002637123687606719\n",
      "Average test loss: 0.0019280365410571296\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0026325417328625918\n",
      "Average test loss: 0.0024276747753222785\n",
      "Epoch 162/300\n",
      "Average training loss: 0.002622021400887105\n",
      "Average test loss: 0.0020520361916472516\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0027387201216899685\n",
      "Average test loss: 0.0019308948771407208\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0026219325142188203\n",
      "Average test loss: 0.0019800680014822217\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0026321742795407774\n",
      "Average test loss: 0.0020714099653479125\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0030612482165710796\n",
      "Average test loss: 0.0019056123514763182\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0026123666709495915\n",
      "Average test loss: 0.0020026105037993856\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0026025612635744945\n",
      "Average test loss: 0.001975115218096309\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0025957322157919406\n",
      "Average test loss: 0.0019697885607472725\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002594616907959183\n",
      "Average test loss: 0.0020438326178118587\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002611409006226394\n",
      "Average test loss: 0.001924305588627855\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0026908534556213354\n",
      "Average test loss: 0.005736935083650881\n",
      "Epoch 173/300\n",
      "Average training loss: 0.002609875517586867\n",
      "Average test loss: 0.001966611669916246\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002686845364049077\n",
      "Average test loss: 0.0023197062551561327\n",
      "Epoch 175/300\n",
      "Average training loss: 0.002568461595930987\n",
      "Average test loss: 0.002243932146165106\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0027214262444112034\n",
      "Average test loss: 0.002034709922865861\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0025714941035128304\n",
      "Average test loss: 0.0019263687674990959\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0025497296628438763\n",
      "Average test loss: 0.0019810721850436596\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0025618676303161515\n",
      "Average test loss: 0.0020358150839391684\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002659950066978733\n",
      "Average test loss: 0.0020731008272204133\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0025805194090223974\n",
      "Average test loss: 0.0020485283656873636\n",
      "Epoch 182/300\n",
      "Average training loss: 0.002552200042539173\n",
      "Average test loss: 0.0020659796421726544\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0025473434794694187\n",
      "Average test loss: 0.011155746197948853\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0028480775813675588\n",
      "Average test loss: 0.0022628996610227557\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0025377905931737686\n",
      "Average test loss: 0.0020041975523862575\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0025387180416534343\n",
      "Average test loss: 0.00431912943886386\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0025404569655656816\n",
      "Average test loss: 25.512242158677843\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0026513692082630264\n",
      "Average test loss: 0.002003808972167058\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002531484133667416\n",
      "Average test loss: 0.0021110191714639463\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00252457815884716\n",
      "Average test loss: 0.0020512944265889625\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0026394787333491776\n",
      "Average test loss: 0.0024842566131717627\n",
      "Epoch 194/300\n",
      "Average training loss: 0.002511528707626793\n",
      "Average test loss: 0.001957588306110766\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0025171164013445377\n",
      "Average test loss: 0.0020897326741574538\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0025219193088511626\n",
      "Average test loss: 0.002102849863573081\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0025096046440303325\n",
      "Average test loss: 0.001945583407353196\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0025070262770685884\n",
      "Average test loss: 0.0027453182565255296\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0026796127750227847\n",
      "Average test loss: 0.0020173986678322154\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0025435013148105805\n",
      "Average test loss: 0.0019586725673741766\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002485712897860342\n",
      "Average test loss: 0.0020044229965036115\n",
      "Epoch 202/300\n",
      "Average training loss: 0.002516161348877682\n",
      "Average test loss: 0.0020275665250503354\n",
      "Epoch 203/300\n",
      "Average training loss: 0.002487176437758737\n",
      "Average test loss: 0.0020165825045357147\n",
      "Epoch 204/300\n",
      "Average training loss: 0.002495042202580306\n",
      "Average test loss: 0.0020240298337820505\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0025304743746916453\n",
      "Average test loss: 0.001966746880776352\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0030158959763745465\n",
      "Average test loss: 0.0020161678480605286\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0025978337683611446\n",
      "Average test loss: 0.0020327301889451013\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0024727485798713236\n",
      "Average test loss: 0.0019302992649997274\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0024566186807221837\n",
      "Average test loss: 0.001995335529661841\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0024669289032204283\n",
      "Average test loss: 0.0019796755821961495\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0024717484621538054\n",
      "Average test loss: 0.001968772259644336\n",
      "Epoch 212/300\n",
      "Average training loss: 0.002550774398156338\n",
      "Average test loss: 0.001986121634849244\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0024565357714891435\n",
      "Average test loss: 0.0022268633647925325\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002458666953154736\n",
      "Average test loss: 0.002054832369710008\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0024656471104050675\n",
      "Average test loss: 0.0020902987341913913\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0024774991381499504\n",
      "Average test loss: 0.0020343485759157275\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0024606301053944563\n",
      "Average test loss: 0.002063546604373389\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0024672136610994736\n",
      "Average test loss: 0.0019963414429997406\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0024910818429456816\n",
      "Average test loss: 0.00228842474354638\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0025103258276358248\n",
      "Average test loss: 0.0022832299086989628\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0024518829284028874\n",
      "Average test loss: 0.0019672679915610286\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0027467380873858927\n",
      "Average test loss: 0.0019934264797096452\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00254224984968702\n",
      "Average test loss: 0.0020661715383951864\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002446018857674466\n",
      "Average test loss: 0.0020188744254410265\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0024225137051608826\n",
      "Average test loss: 0.0023006955213430856\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002448463242087099\n",
      "Average test loss: 0.002050488110528224\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002501289021430744\n",
      "Average test loss: 0.002076968749364217\n",
      "Epoch 230/300\n",
      "Average training loss: 0.002431034413476785\n",
      "Average test loss: 0.002027782341879275\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0026822504161132707\n",
      "Average test loss: 0.0019699767772108317\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00244824877836638\n",
      "Average test loss: 0.0020175776441271106\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0024258024618029595\n",
      "Average test loss: 0.0019920497459049026\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0027515108794387844\n",
      "Average test loss: 0.0020066613427673777\n",
      "Epoch 235/300\n",
      "Average training loss: 0.002413216547212667\n",
      "Average test loss: 0.002051352283813887\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0027516756548235815\n",
      "Average test loss: 0.002007667274369548\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0024075042079720233\n",
      "Average test loss: 0.002152123947524362\n",
      "Epoch 238/300\n",
      "Average training loss: 0.002398813353644477\n",
      "Average test loss: 0.0021105340143872634\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0024052526677648227\n",
      "Average test loss: 0.0020215412283109296\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0027197134538243213\n",
      "Average test loss: 0.0019676721365087563\n",
      "Epoch 241/300\n",
      "Average training loss: 0.002498912182636559\n",
      "Average test loss: 0.001999185750985311\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0023993964229399958\n",
      "Average test loss: 0.0019993206585446994\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0023945251618408495\n",
      "Average test loss: 0.001980899672127432\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0024322723690420388\n",
      "Average test loss: 0.002113099150359631\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002451326294077767\n",
      "Average test loss: 0.005573708794804083\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0024678006599553755\n",
      "Average test loss: 0.0020647409459989934\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0023959460942488578\n",
      "Average test loss: 0.002370810863872369\n",
      "Epoch 250/300\n",
      "Average training loss: 0.002390377513650391\n",
      "Average test loss: 0.002017253062584334\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0023892892855736946\n",
      "Average test loss: 0.0019910986092355515\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002389898376953271\n",
      "Average test loss: 0.002029099740191466\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00249860013441907\n",
      "Average test loss: 0.002916941630964478\n",
      "Epoch 254/300\n",
      "Average training loss: 0.002437830369091696\n",
      "Average test loss: 0.002083492257528835\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002364897453950511\n",
      "Average test loss: 0.002096097644004557\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0023668380312414634\n",
      "Average test loss: 0.0020210367511543964\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0023817842406117255\n",
      "Average test loss: 0.0032935912571847438\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002389887348852224\n",
      "Average test loss: 0.0019977379274658032\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0023698834520247247\n",
      "Average test loss: 0.0023038659071963692\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0031428567328386837\n",
      "Average test loss: 0.002067190812486741\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0024343169389499557\n",
      "Average test loss: 0.0019779017774595154\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0023605139944702386\n",
      "Average test loss: 0.002121852599601779\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002370246217896541\n",
      "Average test loss: 0.001988825631431407\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0023549024427516594\n",
      "Average test loss: 0.0020239074282969037\n",
      "Epoch 265/300\n",
      "Average training loss: 0.002361628665278355\n",
      "Average test loss: 0.0019816993672607673\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0023626264093650714\n",
      "Average test loss: 0.001971592912243472\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0029638649995128315\n",
      "Average test loss: 0.002046904398335351\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0024470737961431346\n",
      "Average test loss: 0.002044400945512785\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0023618582263588905\n",
      "Average test loss: 0.0020996242147973843\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0023454849742766885\n",
      "Average test loss: 0.0019511103443801403\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0023443918618270094\n",
      "Average test loss: 0.002359985905802912\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002423115366448959\n",
      "Average test loss: 0.002062495017838147\n",
      "Epoch 274/300\n",
      "Average training loss: 0.00257368587433464\n",
      "Average test loss: 0.0020064054962454572\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002349558928774463\n",
      "Average test loss: 0.0021448918009797733\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0023349724693430794\n",
      "Average test loss: 0.0019941724757146506\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0023353921864181755\n",
      "Average test loss: 0.0020398180509606997\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0023553821761161087\n",
      "Average test loss: 0.003509819978641139\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0024264182733992738\n",
      "Average test loss: 0.00311545138919933\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0023395143325130145\n",
      "Average test loss: 0.002006503080949187\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0023214365219076474\n",
      "Average test loss: 0.002029549815588527\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002343781697460347\n",
      "Average test loss: 0.0020743600119733146\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002338491451409128\n",
      "Average test loss: 0.008466087526331345\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002851088054685129\n",
      "Average test loss: 0.002111795200448897\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0025986191280600096\n",
      "Average test loss: 0.002014324050500161\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0023459672197285626\n",
      "Average test loss: 0.002035991831889583\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002311565632828408\n",
      "Average test loss: 0.0020086441369106373\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002697625615220103\n",
      "Average test loss: 0.0020855195969343185\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002365973945500122\n",
      "Average test loss: 0.0019956943591435752\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0023574532634682128\n",
      "Average test loss: 0.00198374283603496\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00232876694885393\n",
      "Average test loss: 0.001998582516486446\n",
      "Epoch 294/300\n",
      "Average training loss: 0.002310861174017191\n",
      "Average test loss: 0.002049489727243781\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0023110584430396557\n",
      "Average test loss: 0.0019894379294580883\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0032743290348185433\n",
      "Average test loss: 0.0019320639808558756\n",
      "Epoch 297/300\n",
      "Average training loss: 0.002379471441730857\n",
      "Average test loss: 96.45559225887722\n",
      "Epoch 298/300\n",
      "Average training loss: 0.002331074972740478\n",
      "Average test loss: 0.0019969219263229103\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0022882970948186186\n",
      "Average test loss: 0.0020808526064372726\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002297687575945424\n",
      "Average test loss: 0.002318067046503226\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13746506213148435\n",
      "Average test loss: 0.023808001515765984\n",
      "Epoch 2/300\n",
      "Average training loss: 0.016824418789810603\n",
      "Average test loss: 0.5128166334927082\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01247590831749969\n",
      "Average test loss: 0.003759746896723906\n",
      "Epoch 4/300\n",
      "Average training loss: 0.007690190432800187\n",
      "Average test loss: 0.0062975387970606485\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007141674789289633\n",
      "Average test loss: 0.0029704592443174785\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006670043446537521\n",
      "Average test loss: 0.0026259711120898525\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006278417155146599\n",
      "Average test loss: 0.0066678263942400616\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005934345330629084\n",
      "Average test loss: 0.0024049127542724214\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00566221296083596\n",
      "Average test loss: 0.0025865962646073764\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005456181244303783\n",
      "Average test loss: 0.002250717302059962\n",
      "Epoch 14/300\n",
      "Average training loss: 0.00515878675546911\n",
      "Average test loss: 0.002238341484632757\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004953810739848349\n",
      "Average test loss: 0.0021427940187147923\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004791822291289767\n",
      "Average test loss: 0.0026213657170948055\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004566241790230075\n",
      "Average test loss: 0.0019999969735120735\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0044895092902912034\n",
      "Average test loss: 0.0044578434990512\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004379814452802141\n",
      "Average test loss: 0.002127533078090184\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004143114699878626\n",
      "Average test loss: 0.0019730334446455043\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00402244431215028\n",
      "Average test loss: 0.0036527881839623056\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004067026746769746\n",
      "Average test loss: 0.001913258270877931\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0038084289485381707\n",
      "Average test loss: 0.0018757631942733295\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004084965475317504\n",
      "Average test loss: 0.0018484554042418797\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0036757831273393497\n",
      "Average test loss: 0.0016986763164814976\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003603772377181384\n",
      "Average test loss: 0.00417206014941136\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0032292342486066952\n",
      "Average test loss: 0.0016296025779512194\n",
      "Epoch 31/300\n",
      "Average training loss: 0.003150007393832008\n",
      "Average test loss: 0.0015070623370508354\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0030643434261696206\n",
      "Average test loss: 0.0015319847704635726\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0029826383077436024\n",
      "Average test loss: 0.0015656598718422983\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002993951819009251\n",
      "Average test loss: 0.0014730578021456798\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002890716168201632\n",
      "Average test loss: 0.0021857051224344307\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0029403113768332535\n",
      "Average test loss: 0.001472997580240998\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0027941833831783796\n",
      "Average test loss: 0.0014481030775027142\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0027344928625971077\n",
      "Average test loss: 0.00137542371193154\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002707352987801035\n",
      "Average test loss: 0.0014000902072423035\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002713023558879892\n",
      "Average test loss: 0.0016660043959402376\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0026705942352612815\n",
      "Average test loss: 0.0013435479313549069\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0026990382195346886\n",
      "Average test loss: 0.0014036695538088679\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002611675108679467\n",
      "Average test loss: 0.0013283575997791357\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002600973045970831\n",
      "Average test loss: 0.001355000842983524\n",
      "Epoch 45/300\n",
      "Average training loss: 0.002530658068549302\n",
      "Average test loss: 0.0013404772044676874\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0025157972255514726\n",
      "Average test loss: 0.0013274895098681252\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0024750113795614904\n",
      "Average test loss: 0.0013040008382457825\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0024551472260306278\n",
      "Average test loss: 0.0015427431884325214\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002448862803944697\n",
      "Average test loss: 0.0015265063137954308\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002487314611673355\n",
      "Average test loss: 0.0014104003972477384\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003008428309112787\n",
      "Average test loss: 0.0013358478256397777\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0025448013388862213\n",
      "Average test loss: 0.0013018556615958612\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002472654589969251\n",
      "Average test loss: 0.001300981923006475\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002620557678449485\n",
      "Average test loss: 0.0013324037320497962\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0024340600300994182\n",
      "Average test loss: 0.0013014443528114093\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0023747999682608577\n",
      "Average test loss: 0.0012896528903187977\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002359486926967899\n",
      "Average test loss: 0.0013209249747710096\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0023510387834782404\n",
      "Average test loss: 0.0012814789888345533\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0022794705948068037\n",
      "Average test loss: 0.0012698067935804525\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0022595391914041504\n",
      "Average test loss: 0.0014641752032977012\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0024053834685020974\n",
      "Average test loss: 0.001265036470360226\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0022198449511908825\n",
      "Average test loss: 0.0012744219710843431\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0022352837159608802\n",
      "Average test loss: 0.0013055244789769251\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002218924800761872\n",
      "Average test loss: 0.0013090315310077534\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0022085368362151914\n",
      "Average test loss: 0.001436096482185854\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0022316270344373253\n",
      "Average test loss: 0.004181057684951358\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0022683706622984674\n",
      "Average test loss: 0.0013885881098297736\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002340085222489304\n",
      "Average test loss: 0.001274976840760145\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002261674772327145\n",
      "Average test loss: 0.0013721508976692954\n",
      "Epoch 74/300\n",
      "Average training loss: 0.003226185103257497\n",
      "Average test loss: 0.0012927307213346164\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002444486480516692\n",
      "Average test loss: 0.001489339533365435\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0023170846468872496\n",
      "Average test loss: 0.001320045524276793\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002248056252900925\n",
      "Average test loss: 0.0021632109018456603\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002201579914842215\n",
      "Average test loss: 0.0012816271706380777\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002173361982115441\n",
      "Average test loss: 0.0012702426219928181\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002146693270239565\n",
      "Average test loss: 0.0013535572805752356\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0021734038839737574\n",
      "Average test loss: 0.0012791867665946483\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002105875042991506\n",
      "Average test loss: 0.0012777204945062597\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0021210947877003086\n",
      "Average test loss: 0.0012879736750490136\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0021573913533033595\n",
      "Average test loss: 0.0015030745233719548\n",
      "Epoch 87/300\n",
      "Average training loss: 0.002076985695502824\n",
      "Average test loss: 0.0013157029155020912\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002980396740138531\n",
      "Average test loss: 0.0013345280612508455\n",
      "Epoch 89/300\n",
      "Average training loss: 0.002258172724602951\n",
      "Average test loss: 0.0014362329273588127\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002091518889595237\n",
      "Average test loss: 0.0012956229916049374\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0020425832741376424\n",
      "Average test loss: 0.001295088945577542\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0020233213289951287\n",
      "Average test loss: 0.0014324368285015226\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0020186241309468945\n",
      "Average test loss: 0.00171348502052327\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002320219653762049\n",
      "Average test loss: 0.0021548490590519377\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0021195810457898512\n",
      "Average test loss: 0.0012965629710815847\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002145257705822587\n",
      "Average test loss: 0.00144390905317333\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0020170116343845926\n",
      "Average test loss: 0.0013847833375136057\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0022445451939064596\n",
      "Average test loss: 0.0012760499976575375\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0020197954691118666\n",
      "Average test loss: 0.5857182713035081\n",
      "Epoch 100/300\n",
      "Average training loss: 0.001984443307750755\n",
      "Average test loss: 0.001310610129808386\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0019737690023870933\n",
      "Average test loss: 0.0012780714015372925\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002017439369008773\n",
      "Average test loss: 0.001304675742259456\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0019737225027961864\n",
      "Average test loss: 0.001652578918211576\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0019539345944714214\n",
      "Average test loss: 0.0014565294372538725\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002180888026435342\n",
      "Average test loss: 0.001295879991311166\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0019745808901886143\n",
      "Average test loss: 0.001303227494781216\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0020945036014955905\n",
      "Average test loss: 0.0014033506211514274\n",
      "Epoch 108/300\n",
      "Average training loss: 0.001899987085774127\n",
      "Average test loss: 0.0013323381623356707\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00190048626354999\n",
      "Average test loss: 0.001281844498589635\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0018907457958492967\n",
      "Average test loss: 0.0014363088208354182\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0019056670785778098\n",
      "Average test loss: 0.0013777207289304998\n",
      "Epoch 114/300\n",
      "Average training loss: 0.002006916720006201\n",
      "Average test loss: 0.0012795119010325935\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0019138200169222222\n",
      "Average test loss: 0.0012993022905559177\n",
      "Epoch 116/300\n",
      "Average training loss: 0.001966218210860259\n",
      "Average test loss: 0.0013815194140705798\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0018518368252035645\n",
      "Average test loss: 0.0012965457341116335\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0018586864445565476\n",
      "Average test loss: 0.0014492347366176547\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002219353675738805\n",
      "Average test loss: 0.0012862779306661752\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0019064206782107552\n",
      "Average test loss: 0.0012909555235463713\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0018517577787861227\n",
      "Average test loss: 0.0013298000047604244\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0021850838280386394\n",
      "Average test loss: 0.001503435630765226\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0023430730549411642\n",
      "Average test loss: 0.001262849892489612\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002048268757864005\n",
      "Average test loss: 0.001728596970335477\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0018942951560020447\n",
      "Average test loss: 0.0013391830092296005\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0018609380241897372\n",
      "Average test loss: 0.001487232955172658\n",
      "Epoch 127/300\n",
      "Average training loss: 0.003068812631174094\n",
      "Average test loss: 0.0012948117292382651\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0020892165364283653\n",
      "Average test loss: 0.0012603586913189954\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0019299592152237893\n",
      "Average test loss: 0.0016990096163418558\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019535647629656725\n",
      "Average test loss: 0.0013010031989672119\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0018495080061256886\n",
      "Average test loss: 0.0012858289027483098\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0018719955019445883\n",
      "Average test loss: 0.001356783959807621\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0019958029097567\n",
      "Average test loss: 0.0013128303094870515\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0017995666516944767\n",
      "Average test loss: 0.004623599488909046\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018192673783956302\n",
      "Average test loss: 0.0013144326150003407\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0018093228356705772\n",
      "Average test loss: 0.0013335992516949773\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0018091081379809314\n",
      "Average training loss: 0.001829650979799529\n",
      "Average test loss: 0.0013110200671686066\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0017756369963495267\n",
      "Average test loss: 0.0013310409562869204\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0017788721821788284\n",
      "Average test loss: 0.001315914599224925\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0017807016496857008\n",
      "Average test loss: 0.0012974357834706703\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0019716865913942455\n",
      "Average test loss: 0.0012698748473905855\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00189250892555962\n",
      "Average test loss: 0.0012829922028920716\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0017572122825723555\n",
      "Average test loss: 0.0018022573410222927\n",
      "Epoch 149/300\n",
      "Average training loss: 0.001994557595294383\n",
      "Average test loss: 0.0013510182907597887\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0019437696701950498\n",
      "Average test loss: 0.0013277408889391356\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0017986528914835718\n",
      "Average test loss: 0.0013081240036214392\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0017426279371397363\n",
      "Average test loss: 0.0013021209246168535\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001767674902660979\n",
      "Average test loss: 0.001922207547351718\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0017350004494397176\n",
      "Average test loss: 0.0013022998017776343\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002523144065712889\n",
      "Average test loss: 63.17943856959873\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0020243320181551907\n",
      "Average test loss: 0.0012728701277325552\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0018255311607693633\n",
      "Average test loss: 0.001303057141136378\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0019079948445368145\n",
      "Average test loss: 0.0013056075142489538\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0017278325652910604\n",
      "Average test loss: 0.0012931138879309098\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0017677963277738956\n",
      "Average test loss: 0.001495964469284647\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0017163863190346293\n",
      "Average test loss: 0.3735452659063869\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0017228863808429903\n",
      "Average test loss: 0.003552409125491977\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0017862327697997292\n",
      "Average test loss: 0.0012953452153338326\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0017269664465760191\n",
      "Average test loss: 0.001352765508616964\n",
      "Epoch 167/300\n",
      "Average training loss: 0.001698529933165345\n",
      "Average test loss: 0.006753930457350281\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0018871799521148205\n",
      "Average test loss: 0.001492060210555792\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00170194179771675\n",
      "Average test loss: 0.0013873119993756216\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0017722387347991267\n",
      "Average test loss: 0.0015394041861097017\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002034637522469792\n",
      "Average test loss: 0.002726343900586168\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0017553041979877484\n",
      "Average test loss: 0.001657270887038774\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0017108651949610148\n",
      "Average test loss: 0.0020353203856696687\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0020000064267466466\n",
      "Average test loss: 0.004507938667717907\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0019288852454887495\n",
      "Average test loss: 0.0014642349344988663\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0017014837437826725\n",
      "Average test loss: 0.0013275381126958462\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0018104010582384136\n",
      "Average test loss: 0.0013508587361623844\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0016749693841362993\n",
      "Average test loss: 0.0013664976534847584\n",
      "Epoch 179/300\n",
      "Average training loss: 0.001667319573255049\n",
      "Average test loss: 0.0013185377146841752\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0016855838074245387\n",
      "Average test loss: 0.001307790937109126\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0016820105536737376\n",
      "Average test loss: 0.0013745203580603832\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0016811136964501607\n",
      "Average test loss: 0.001391331009566784\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0016721391366380785\n",
      "Average test loss: 0.0015120622507399982\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0028402945949799483\n",
      "Average test loss: 0.0013006234292147888\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0021424651152143876\n",
      "Average test loss: 0.001584627557752861\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0019531259750947354\n",
      "Average test loss: 0.0014461766564183764\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0018311193221145206\n",
      "Average test loss: 0.0012988126154264642\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0017395462522076235\n",
      "Average test loss: 0.0013444459186349477\n",
      "Epoch 191/300\n",
      "Average training loss: 0.001687476799926824\n",
      "Average test loss: 0.0013649855165017975\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001671706823942562\n",
      "Average test loss: 0.001925627906082405\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0016818307100070846\n",
      "Average test loss: 0.0013080276284987728\n",
      "Epoch 194/300\n",
      "Average training loss: 0.001659277758974996\n",
      "Average test loss: 0.0013720226385113266\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0016561991799001893\n",
      "Average test loss: 0.0014273408985593253\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0021586390914809374\n",
      "Average test loss: 0.0013811839756866296\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0016899815207968155\n",
      "Average test loss: 0.001356856907097002\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0017320759458881285\n",
      "Average test loss: 0.0014674842250016\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0016407957236386009\n",
      "Average test loss: 0.001403117075148556\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0016284655822027061\n",
      "Average test loss: 0.0013595286454591486\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0020609662597998978\n",
      "Average test loss: 0.0013377368311501211\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0016970072245846193\n",
      "Average test loss: 0.0017861580565157863\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0016468297858825988\n",
      "Average test loss: 0.001500988235283229\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0016350098962171211\n",
      "Average test loss: 0.0013381189313303266\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0018629306989411513\n",
      "Average test loss: 0.0013725525762678847\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0016514310588439305\n",
      "Average test loss: 0.001351334142498672\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0017108058493791355\n",
      "Average test loss: 0.0018120946337779364\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0021239917169635493\n",
      "Average test loss: 0.0013717348949155873\n",
      "Epoch 212/300\n",
      "Average training loss: 0.001673122722034653\n",
      "Average test loss: 0.0013896454765150944\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0016211827537562283\n",
      "Average test loss: 0.0014793280150430898\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002009881594321794\n",
      "Average test loss: 0.0014826830079158148\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0016065619313675495\n",
      "Average test loss: 0.0026376334878926476\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0017476683042219115\n",
      "Average test loss: 0.001374424758936382\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0016086929963073796\n",
      "Average test loss: 0.0013658424499962065\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0016030087533096473\n",
      "Average test loss: 0.001300581337677108\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0018618173664953146\n",
      "Average training loss: 0.0016060608712335429\n",
      "Average test loss: 0.0013911257329293422\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002383053386790885\n",
      "Average test loss: 0.0013024035870201059\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0020703662102007202\n",
      "Average test loss: 0.0013516674793014923\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0017421605806383822\n",
      "Average test loss: 0.7831495685577392\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0016548921002282036\n",
      "Average test loss: 0.0013390645103322136\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001627264833284749\n",
      "Average test loss: 0.0013866923501094182\n",
      "Epoch 229/300\n",
      "Average training loss: 0.001738796079945233\n",
      "Average test loss: 0.0013481021883587042\n",
      "Epoch 230/300\n",
      "Average training loss: 0.001602254905530976\n",
      "Average test loss: 0.001944303856127792\n",
      "Epoch 231/300\n",
      "Average training loss: 0.001604508666218155\n",
      "Average test loss: 0.0014882315248768362\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0016036905783952937\n",
      "Average test loss: 0.0013645338771036929\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0017536219931724999\n",
      "Average test loss: 0.001352354205834369\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0016224327352311876\n",
      "Average test loss: 0.001507888443974985\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0016227619966699017\n",
      "Average test loss: 0.0013417465867371194\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0018671520041922728\n",
      "Average test loss: 0.0013660515057336952\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0016141322687682179\n",
      "Average test loss: 0.0013447959292680026\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0015659355823364523\n",
      "Average test loss: 0.0013291499267021815\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0016481745773926377\n",
      "Average test loss: 0.001361639398150146\n",
      "Epoch 240/300\n",
      "Average training loss: 0.001574601844056613\n",
      "Average test loss: 0.0013799857647261685\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0015776932785908381\n",
      "Average test loss: 0.001766984652986543\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0016016863538469706\n",
      "Average test loss: 0.0014596212272428805\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0017888916085163752\n",
      "Average test loss: 0.00139753825062265\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0015846185258900126\n",
      "Average test loss: 0.0013598537338483665\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0015678917184058162\n",
      "Average test loss: 0.0021652726994620427\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0015834976368480258\n",
      "Average test loss: 0.0013319304978681936\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0015857443491824798\n",
      "Average test loss: 0.0024420321945928863\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0015601064587632816\n",
      "Average test loss: 0.0013493030269940694\n",
      "Epoch 250/300\n",
      "Average training loss: 0.00157090571988374\n",
      "Average test loss: 0.0014759067693311306\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0017746085874322387\n",
      "Average test loss: 0.0013334966944530606\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0015785378544694848\n",
      "Average test loss: 0.001591867218207982\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0015444944732718997\n",
      "Average test loss: 0.0014018095312640071\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0015375473708328273\n",
      "Average test loss: 0.0013725821998280784\n",
      "Epoch 259/300\n",
      "Average training loss: 0.001638169850843648\n",
      "Average test loss: 0.0013644240420932572\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0017468571956786845\n",
      "Average test loss: 0.0014602220920431944\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0015781224202364684\n",
      "Average test loss: 0.0013503403697783748\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0015273921195831563\n",
      "Average test loss: 0.04075072094466951\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0015375893180672494\n",
      "Average test loss: 0.0013887430165583888\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0015306370108284883\n",
      "Average test loss: 0.0015433024293225672\n",
      "Epoch 267/300\n",
      "Average training loss: 0.001537497981865373\n",
      "Average test loss: 0.001424099348175029\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0023866932863990465\n",
      "Average test loss: 0.001430252985511389\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0016844433408437505\n",
      "Average test loss: 0.0013724417181478607\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0015658256046267019\n",
      "Average test loss: 0.00136278956849128\n",
      "Epoch 271/300\n",
      "Average training loss: 0.001620766992887689\n",
      "Average test loss: 0.0015885547678917645\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0015266407679559457\n",
      "Average test loss: 0.002745480389851663\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0016631005364987584\n",
      "Average test loss: 0.00156973124254081\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0015801321998652484\n",
      "Average test loss: 0.0014263518015957541\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0015261132934441168\n",
      "Average test loss: 0.0014268865730199549\n",
      "Epoch 276/300\n",
      "Average training loss: 0.002020895531711479\n",
      "Average test loss: 0.0012987399708686603\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0016502273901262217\n",
      "Average test loss: 0.001349438956628243\n",
      "Epoch 278/300\n",
      "Average training loss: 0.001535255735533105\n",
      "Average test loss: 0.0013754666643217206\n",
      "Epoch 279/300\n",
      "Average training loss: 0.00151595739616702\n",
      "Average test loss: 0.0015596499881810613\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0017637666933652428\n",
      "Average test loss: 0.001346179071503381\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0015359078108643492\n",
      "Average test loss: 0.0013399988127251465\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001520275635127392\n",
      "Average test loss: 0.0013677758201439346\n",
      "Epoch 283/300\n",
      "Average training loss: 0.001522559391024212\n",
      "Average test loss: 0.0013835146173110438\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0018977809691180785\n",
      "Average test loss: 0.0027279349064661396\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0015434098773532445\n",
      "Average test loss: 0.001418048182502389\n",
      "Epoch 288/300\n",
      "Average training loss: 0.001517674383500384\n",
      "Average test loss: 0.00135758583392534\n",
      "Epoch 289/300\n",
      "Average training loss: 0.001765463883160717\n",
      "Average test loss: 0.0014707069880225592\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002122188225078086\n",
      "Average test loss: 0.0015257026741488113\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0016144511875593\n",
      "Average test loss: 0.018155926794641546\n",
      "Epoch 292/300\n",
      "Average training loss: 0.001569147865805361\n",
      "Average test loss: 0.0015888342878056898\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0015025788930555185\n",
      "Average test loss: 0.0013947789472424322\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0018911038835843403\n",
      "Average test loss: 0.0013003162896881501\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002010752029923929\n",
      "Average test loss: 0.0017903752925081385\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0016281400525735485\n",
      "Average test loss: 0.0014471058367441097\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0015527427019551396\n",
      "Average test loss: 0.0015509596041714151\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0017931684680903952\n",
      "Average test loss: 0.0013645103915284078\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0015169779521723588\n",
      "Average test loss: 0.001429031620422999\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0015073720543748803\n",
      "Average test loss: 0.0013918388024386432\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.5/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 10.58\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 15.76\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 20.11\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 17.34\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 22.10\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 23.46\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 20.28\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 23.64\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 22.99\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.17\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.43\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.97\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.54\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.69\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.93\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 11.03\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 15.97\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 18.73\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 21.69\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 22.10\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.72\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.52\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.23\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.60\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 12.65\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 18.81\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 21.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.13\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.14\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.91\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.62\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.69\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.91\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.17\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 13.24\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 19.87\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.11\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.01\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.70\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.25\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.73\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.00\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.46539092163244883\n",
      "Average test loss: 0.015000132814877562\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03900352245900366\n",
      "Average test loss: 0.011331727641324202\n",
      "Epoch 3/300\n",
      "Average training loss: 0.026265661065777143\n",
      "Average test loss: 0.012094489766491783\n",
      "Epoch 5/300\n",
      "Average training loss: 0.023952404230833054\n",
      "Average test loss: 0.009381520212524468\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022145830820004146\n",
      "Average test loss: 0.00897923809207148\n",
      "Epoch 7/300\n",
      "Average training loss: 0.020648784490095246\n",
      "Average test loss: 0.011616419603427252\n",
      "Epoch 8/300\n",
      "Average training loss: 0.019483157528771295\n",
      "Average test loss: 0.009332819092604848\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01852524244454172\n",
      "Average test loss: 0.007996619292845328\n",
      "Epoch 10/300\n",
      "Average training loss: 0.017550900815261734\n",
      "Average test loss: 0.00813274496090081\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01659247510300742\n",
      "Average test loss: 0.007555561126934157\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015779583366380797\n",
      "Average test loss: 0.01017377894785669\n",
      "Epoch 13/300\n",
      "Average training loss: 0.015059392233689626\n",
      "Average test loss: 0.0067956407012210954\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014542634695768357\n",
      "Average test loss: 0.00670674931920237\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013942479754487673\n",
      "Average test loss: 0.007136364996433258\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013669841104083591\n",
      "Average test loss: 0.0065629745059543184\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01334077001363039\n",
      "Average test loss: 0.008592529939280617\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012783923147453202\n",
      "Average test loss: 0.013611657244463761\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01608694884594944\n",
      "Average test loss: 0.007283252272754908\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014059252163602246\n",
      "Average test loss: 0.00631383832792441\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012734830306635962\n",
      "Average test loss: 0.005944969829171896\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012325383917325072\n",
      "Average test loss: 0.07362896777855026\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012118501516679923\n",
      "Average test loss: 0.005808980815112591\n",
      "Epoch 25/300\n",
      "Average training loss: 0.011933869486053785\n",
      "Average test loss: 0.005753193263378408\n",
      "Epoch 26/300\n",
      "Average training loss: 0.011802038637300333\n",
      "Average test loss: 0.006024898836182223\n",
      "Epoch 27/300\n",
      "Average training loss: 0.011628124590549203\n",
      "Average test loss: 0.005789409913536575\n",
      "Epoch 28/300\n",
      "Average training loss: 0.011473002646532324\n",
      "Average test loss: 0.005679479075802697\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0114161278779308\n",
      "Average test loss: 0.005652112911558814\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011191866676840518\n",
      "Average test loss: 0.005645899003578557\n",
      "Epoch 31/300\n",
      "Average training loss: 0.010967379646168814\n",
      "Average test loss: 16.018908344109853\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010945143744349479\n",
      "Average test loss: 0.006580178486804167\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010823711552553707\n",
      "Average test loss: 0.04549166274112132\n",
      "Epoch 35/300\n",
      "Average training loss: 0.010708241060376168\n",
      "Average test loss: 0.039037110971907776\n",
      "Epoch 36/300\n",
      "Average training loss: 0.010655753952761492\n",
      "Average test loss: 0.0053087262324988845\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010584568784468704\n",
      "Average test loss: 0.008903489627771908\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010493556264787913\n",
      "Average test loss: 0.005281191411117713\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012524863979054822\n",
      "Average test loss: 0.005940146655258205\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011095830474462774\n",
      "Average test loss: 0.08630552298741208\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01059281329727835\n",
      "Average test loss: 0.4818290750583013\n",
      "Epoch 42/300\n",
      "Average training loss: 0.010429596259362168\n",
      "Average test loss: 0.09532054521309005\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010295413429538408\n",
      "Average test loss: 0.005527097247540951\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01048281090044313\n",
      "Average test loss: 0.005443202484813002\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010222642682492733\n",
      "Average test loss: 0.006511528731220299\n",
      "Epoch 46/300\n",
      "Average training loss: 0.010162494942545891\n",
      "Average test loss: 0.005224694675869412\n",
      "Epoch 48/300\n",
      "Average training loss: 0.010094434941808382\n",
      "Average test loss: 0.04162947064472569\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009979581356048584\n",
      "Average test loss: 0.006437287692808442\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010577471591946152\n",
      "Average test loss: 0.011351592282454174\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010194066720704237\n",
      "Average test loss: 0.01529036867370208\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009958880480792787\n",
      "Average test loss: 0.007879943089352713\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00990500480019384\n",
      "Average test loss: 0.008000658739772107\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009867206594182385\n",
      "Average test loss: 0.010992930703692966\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009890778644217385\n",
      "Average test loss: 0.03189627962145541\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009765952616102165\n",
      "Average test loss: 0.021655822855730852\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00981160651312934\n",
      "Average test loss: 0.03295131886129578\n",
      "Epoch 59/300\n",
      "Average training loss: 0.00962179771065712\n",
      "Average test loss: 0.008206893069876564\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009616084917551941\n",
      "Average test loss: 0.03481108841713932\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009639750815927982\n",
      "Average test loss: 0.005213975431604518\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009552234972516695\n",
      "Average training loss: 0.009484694917996724\n",
      "Average test loss: 0.6804143182813294\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009451889912287395\n",
      "Average test loss: 0.07180804739519954\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009380047037369676\n",
      "Average test loss: 0.18863219372555615\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009380402166810301\n",
      "Average test loss: 0.22620413741138246\n",
      "Epoch 67/300\n",
      "Average training loss: 0.010153947613305516\n",
      "Average test loss: 0.08999515682665839\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009615809043662415\n",
      "Average test loss: 0.04314503239260779\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009328008556531535\n",
      "Average test loss: 0.019529112074110243\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009306598216295243\n",
      "Average test loss: 0.23514111204114224\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009262516249385145\n",
      "Average test loss: 0.01048884227168229\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00923278284072876\n",
      "Average test loss: 0.6665003284551203\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009518527892728647\n",
      "Average test loss: 0.08294627931051785\n",
      "Epoch 74/300\n",
      "Average training loss: 0.009399720963504579\n",
      "Average test loss: 0.0053882656910767155\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009233873420291477\n",
      "Average test loss: 0.00513727026556929\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009142120987176896\n",
      "Average test loss: 0.08863919034351905\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009042754512694148\n",
      "Average test loss: 0.011801261256138484\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009123408059279125\n",
      "Average test loss: 0.14614181940630078\n",
      "Epoch 80/300\n",
      "Average training loss: 0.009140908123718367\n",
      "Average test loss: 0.5489751791730523\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008986586491266886\n",
      "Average test loss: 0.26644978857371543\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009065578007863628\n",
      "Average test loss: 0.006296445633802149\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008940484122269684\n",
      "Average test loss: 5.142675756189559\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008872768706745571\n",
      "Average test loss: 4.21194394733674\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008894991546041436\n",
      "Average test loss: 0.07922898144440518\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00886518911106719\n",
      "Average test loss: 0.32867753723926013\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0088906053652366\n",
      "Average test loss: 0.04430336296723949\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00875002055035697\n",
      "Average test loss: 2.8025792579087945\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008761297163036134\n",
      "Average test loss: 0.7068480537889732\n",
      "Epoch 90/300\n",
      "Average training loss: 0.008740193152179321\n",
      "Average test loss: 0.02037373965481917\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009834997888240549\n",
      "Average test loss: 42.26757751042975\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012476710246668922\n",
      "Average test loss: 158.67757198673405\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009533608656790522\n",
      "Average test loss: 1.7637629234318932\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008945940050813887\n",
      "Average test loss: 117.98606173924439\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008697272044916948\n",
      "Average test loss: 115.63649402730498\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008601162731647492\n",
      "Average test loss: 3.641506789125502\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008558990724384785\n",
      "Average test loss: 1.0992257778578334\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008532335720956326\n",
      "Average test loss: 1.5908171088960437\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0085346625794967\n",
      "Average test loss: 0.008070060791240797\n",
      "Epoch 101/300\n",
      "Average training loss: 0.008487207221902078\n",
      "Average test loss: 5.179608596319953\n",
      "Epoch 102/300\n",
      "Average training loss: 0.008458724823262957\n",
      "Average test loss: 0.012073264219280747\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008461296283536488\n",
      "Average test loss: 0.010303738657799032\n",
      "Epoch 104/300\n",
      "Average training loss: 0.008411349234481653\n",
      "Average test loss: 0.010180518062578307\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008487897336896923\n",
      "Average test loss: 0.010162150747246213\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008384095990823374\n",
      "Average test loss: 0.015020099090205298\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008373697809047169\n",
      "Average test loss: 0.00788351353092326\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008310093815128008\n",
      "Average test loss: 0.016405192289087506\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008441409449610445\n",
      "Average test loss: 1.4747455817117459\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008353131809996235\n",
      "Average test loss: 2271.7388059353298\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008367656840218438\n",
      "Average test loss: 30681313.535709906\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008323479978574647\n",
      "Average test loss: 0.023110482102880876\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008348420022262467\n",
      "Average test loss: 0.030679097518738773\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0083848376745979\n",
      "Average test loss: 0.014450118521849315\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008351499482989312\n",
      "Average test loss: 1.04445256402923\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008263038516044617\n",
      "Average test loss: 0.355090672744645\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008712015683038367\n",
      "Average test loss: 19.203470712449814\n",
      "Epoch 121/300\n",
      "Average training loss: 0.008329962626513508\n",
      "Average test loss: 0.011283656898058122\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008091483345462216\n",
      "Average test loss: 49.20014990285039\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008105652660959297\n",
      "Average test loss: 9.13185612023539\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008121203849713007\n",
      "Average test loss: 643.9174223768446\n",
      "Epoch 125/300\n",
      "Average training loss: 0.00806196589767933\n",
      "Average test loss: 8.256675218843752\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00815566545807653\n",
      "Average test loss: 0.5787819355887671\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008050525311380625\n",
      "Average test loss: 30.686451811300383\n",
      "Epoch 130/300\n",
      "Average training loss: 0.008670213309841024\n",
      "Average test loss: 0.005860466239766942\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008020396442463001\n",
      "Average test loss: 0.0060646068528294565\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007971704825758935\n",
      "Average test loss: 3.916779000275665\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007959433864388202\n",
      "Average test loss: 0.005296021248731349\n",
      "Epoch 134/300\n",
      "Average training loss: 0.007938471330122815\n",
      "Average test loss: 0.012912108096397585\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007948999224437607\n",
      "Average test loss: 0.18064156137241258\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007933791769047578\n",
      "Average test loss: 0.09049292026522258\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00796298091693057\n",
      "Average test loss: 0.012481509089469909\n",
      "Epoch 138/300\n",
      "Average training loss: 0.007984487134549353\n",
      "Average test loss: 0.005657312885754638\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007903142102890545\n",
      "Average test loss: 0.011797703568306233\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007880517843696807\n",
      "Average test loss: 0.00543965810082025\n",
      "Epoch 141/300\n",
      "Average training loss: 0.007892791884640853\n",
      "Average test loss: 0.03942913417849276\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007871173773374823\n",
      "Average test loss: 0.7239215363156465\n",
      "Epoch 143/300\n",
      "Average training loss: 0.007821389082405303\n",
      "Average test loss: 0.8151006108456188\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007812684920926889\n",
      "Average test loss: 0.01582287549806966\n",
      "Epoch 145/300\n",
      "Average training loss: 0.007843282075391875\n",
      "Average test loss: 0.19868916452262136\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007810790807422664\n",
      "Average test loss: 0.005742271513160732\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007784713736838765\n",
      "Average test loss: 0.010090641682760583\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007802539007531272\n",
      "Average test loss: 0.0053575753817955655\n",
      "Epoch 149/300\n",
      "Average training loss: 0.007735537389086352\n",
      "Average test loss: 0.022159722216013405\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007971969083779389\n",
      "Average test loss: 0.10685228293471866\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007895348668926293\n",
      "Average test loss: 0.08375273630519708\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007693801778058211\n",
      "Average test loss: 439.060896914598\n",
      "Epoch 153/300\n",
      "Average training loss: 0.007719350316872199\n",
      "Average test loss: 0.35873392364382745\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007725065264436934\n",
      "Average test loss: 0.0054153626499076684\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007684825415412585\n",
      "Average test loss: 0.7276639398369524\n",
      "Epoch 156/300\n",
      "Average training loss: 0.007674942852722274\n",
      "Average test loss: 69.19850142799856\n",
      "Epoch 157/300\n",
      "Average training loss: 0.007703740304128991\n",
      "Average test loss: 0.01952381969988346\n",
      "Epoch 158/300\n",
      "Average training loss: 0.007659323725021548\n",
      "Average test loss: 0.005847194440662861\n",
      "Epoch 159/300\n",
      "Average training loss: 0.007614544990989897\n",
      "Average test loss: 18.309121426135302\n",
      "Epoch 160/300\n",
      "Average training loss: 0.007583826621787416\n",
      "Average test loss: 0.005357736643817689\n",
      "Epoch 161/300\n",
      "Average training loss: 0.007635816660192278\n",
      "Average test loss: 0.005700079926600059\n",
      "Epoch 162/300\n",
      "Average training loss: 0.00756444857807623\n",
      "Average test loss: 0.19321399553120136\n",
      "Epoch 163/300\n",
      "Average training loss: 0.007661045074048969\n",
      "Average test loss: 8.2794517785675\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00761352378461096\n",
      "Average test loss: 0.1518939920498265\n",
      "Epoch 165/300\n",
      "Average training loss: 0.007832115998698605\n",
      "Average test loss: 39.00470710388571\n",
      "Epoch 166/300\n",
      "Average training loss: 0.00758286891091201\n",
      "Average test loss: 1913.45133336046\n",
      "Epoch 167/300\n",
      "Average training loss: 0.007591522153880861\n",
      "Average test loss: 3.110406041011214\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0075760634400778345\n",
      "Average test loss: 0.03270981532045537\n",
      "Epoch 169/300\n",
      "Average training loss: 0.007535962514579296\n",
      "Average test loss: 0.20873104609259302\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0075217877477407455\n",
      "Average test loss: 0.013211824860009882\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0075166032972435156\n",
      "Average test loss: 0.005752209114531676\n",
      "Epoch 172/300\n",
      "Average training loss: 0.007593295984798008\n",
      "Average test loss: 0.014903020727965567\n",
      "Epoch 173/300\n",
      "Average training loss: 0.007476345469140344\n",
      "Average test loss: 0.990396527590851\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0074620277749167544\n",
      "Average test loss: 0.007436238058739239\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008937124522609844\n",
      "Average test loss: 0.0654819040497144\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007726006460686525\n",
      "Average test loss: 0.006831253674295213\n",
      "Epoch 177/300\n",
      "Average training loss: 0.007561988730397489\n",
      "Average test loss: 0.6474675419955618\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007663515499482553\n",
      "Average test loss: 1.7009692725307413\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007479847332669629\n",
      "Average test loss: 3.8896587288412783\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007488050825479957\n",
      "Average test loss: 11.336775489982632\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007402746164550384\n",
      "Average test loss: 0.0052586468329860105\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007403959155082703\n",
      "Average test loss: 0.011150863021198247\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007404513728287485\n",
      "Average test loss: 0.17729758763147724\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0074262226981421315\n",
      "Average test loss: 1.3054293182558483\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007420315728833278\n",
      "Average test loss: 0.005737445566803217\n",
      "Epoch 186/300\n",
      "Average training loss: 0.007378407914191485\n",
      "Average test loss: 1.4063584200210042\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0073661330027712715\n",
      "Average test loss: 1.3111951159623763\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007378074900971519\n",
      "Average test loss: 67.00520527263483\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007383529366304477\n",
      "Average test loss: 5.479289354026317\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007376748264663749\n",
      "Average test loss: 0.19625284881227545\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0074361350230044785\n",
      "Average test loss: 0.007562662171406879\n",
      "Epoch 192/300\n",
      "Average training loss: 0.007441903207037184\n",
      "Average test loss: 0.006822142116311524\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007321600924349494\n",
      "Average test loss: 0.16498364529096418\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0073202700689435005\n",
      "Average test loss: 1984.6996860555014\n",
      "Epoch 195/300\n",
      "Average training loss: 0.00731772977569037\n",
      "Average test loss: 0.007372201229963037\n",
      "Epoch 196/300\n",
      "Average training loss: 0.007338118331299888\n",
      "Average test loss: 122.17574878607856\n",
      "Epoch 197/300\n",
      "Average training loss: 0.007325584164096249\n",
      "Average test loss: 1.4341497662531004\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007373822761906518\n",
      "Average test loss: 0.07310738566228085\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0072895027258329925\n",
      "Average test loss: 0.019499386506776014\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007399533664186796\n",
      "Average test loss: 0.9394412541389465\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007287015951342053\n",
      "Average test loss: 3.2506005201621186\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0072586704211102596\n",
      "Average test loss: 21.313443687874823\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008018698177403874\n",
      "Average test loss: 0.013399315719389253\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007308037524835931\n",
      "Average test loss: 0.00758072157866425\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007233204421483808\n",
      "Average test loss: 0.5463518635647164\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0072879354713691604\n",
      "Average test loss: 0.026072052478790284\n",
      "Epoch 207/300\n",
      "Average training loss: 0.007226872270719872\n",
      "Average test loss: 0.007351024465428458\n",
      "Epoch 208/300\n",
      "Average training loss: 0.007228042515201701\n",
      "Average test loss: 5.8082287538001935\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007248332271973292\n",
      "Average test loss: 0.11540189231601027\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007222757977743944\n",
      "Average test loss: 1.2635680829485259\n",
      "Epoch 211/300\n",
      "Average training loss: 0.007250448199195994\n",
      "Average test loss: 0.48870002747906577\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00729161983066135\n",
      "Average test loss: 0.21628414284437894\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007215080299725135\n",
      "Average test loss: 0.035600044783618716\n",
      "Epoch 214/300\n",
      "Average training loss: 0.007237583878139654\n",
      "Average test loss: 0.09714312520126502\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007227266434994009\n",
      "Average test loss: 0.005711830591575967\n",
      "Epoch 216/300\n",
      "Average training loss: 0.007233965246213807\n",
      "Average test loss: 0.010561020333319902\n",
      "Epoch 217/300\n",
      "Average training loss: 0.007184691357529826\n",
      "Average test loss: 0.0456770572318799\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0071591440070834424\n",
      "Average test loss: 0.007975254224406349\n",
      "Epoch 219/300\n",
      "Average training loss: 0.007182446164389451\n",
      "Average test loss: 5.1072951748809885\n",
      "Epoch 220/300\n",
      "Average training loss: 0.007209626968122191\n",
      "Average test loss: 0.0068234367428554425\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007132196643286281\n",
      "Average test loss: 0.011099072691053152\n",
      "Epoch 222/300\n",
      "Average training loss: 0.007121073698831929\n",
      "Average test loss: 0.1289713381462627\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00714590070851975\n",
      "Average test loss: 33.60076480287314\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0071461004403730235\n",
      "Average test loss: 454.13385108947756\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0071232910942700175\n",
      "Average test loss: 0.01245180630352762\n",
      "Epoch 226/300\n",
      "Average training loss: 0.007091073052750693\n",
      "Average test loss: 3.1811241808136304\n",
      "Epoch 227/300\n",
      "Average training loss: 0.007146398364669747\n",
      "Average test loss: 30.68954356217881\n",
      "Epoch 229/300\n",
      "Average training loss: 0.007094340861257579\n",
      "Average test loss: 0.19741427337626616\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007057065440962712\n",
      "Average test loss: 0.006892564416345623\n",
      "Epoch 232/300\n",
      "Average training loss: 0.007065401566939221\n",
      "Average test loss: 0.02890933707770374\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007132144892381297\n",
      "Average test loss: 143.39658198192032\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0070821749803920584\n",
      "Average test loss: 0.00812762186055382\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00706692494907313\n",
      "Average test loss: 0.09371009552809927\n",
      "Epoch 236/300\n",
      "Average training loss: 0.007015876678542959\n",
      "Average test loss: 0.3459833240575261\n",
      "Epoch 238/300\n",
      "Average training loss: 0.007026846050802204\n",
      "Average test loss: 0.08390333957059516\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007035759247839451\n",
      "Average test loss: 0.07835059300892883\n",
      "Epoch 240/300\n",
      "Average training loss: 0.007075705879264408\n",
      "Average test loss: 0.20825970835487048\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007004301225559579\n",
      "Average test loss: 16.108433744283186\n",
      "Epoch 242/300\n",
      "Average training loss: 0.007094746133519543\n",
      "Average test loss: 0.005556520222789712\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006991844745973746\n",
      "Average test loss: 0.9213625333582361\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007028881129291322\n",
      "Average test loss: 0.07559020276160704\n",
      "Epoch 245/300\n",
      "Average training loss: 0.007042168662779861\n",
      "Average test loss: 0.01312487625496255\n",
      "Epoch 246/300\n",
      "Average training loss: 0.006994820052550899\n",
      "Average test loss: 8.677085646284537\n",
      "Epoch 247/300\n",
      "Average training loss: 0.007027600721352631\n",
      "Average test loss: 0.6691218151764737\n",
      "Epoch 248/300\n",
      "Average training loss: 0.006993700723681185\n",
      "Average test loss: 4.286899438052956\n",
      "Epoch 249/300\n",
      "Average training loss: 0.006962008224593269\n",
      "Average test loss: 0.32818602282471127\n",
      "Epoch 252/300\n",
      "Average training loss: 0.006980542288058334\n",
      "Average test loss: 1.8279988238016764\n",
      "Epoch 253/300\n",
      "Average training loss: 0.006959854724506537\n",
      "Average test loss: 0.08841913836532168\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0069833347110284695\n",
      "Average test loss: 0.2613054728557666\n",
      "Epoch 255/300\n",
      "Average training loss: 0.006969954135517279\n",
      "Average test loss: 96.82399028960532\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0069203662491507\n",
      "Average test loss: 2.416632652358876\n",
      "Epoch 257/300\n",
      "Average training loss: 0.006919080496248272\n",
      "Average test loss: 332.5917435455322\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006895684309717682\n",
      "Average test loss: 0.047868860812857746\n",
      "Epoch 259/300\n",
      "Average training loss: 0.006914121213886473\n",
      "Average test loss: 4808.355462031394\n",
      "Epoch 260/300\n",
      "Average training loss: 0.006966835194163852\n",
      "Average test loss: 0.24562348063952394\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00686712222153114\n",
      "Average test loss: 4334.590195616861\n",
      "Epoch 262/300\n",
      "Average training loss: 0.006871339517335097\n",
      "Average test loss: 35.12496937119464\n",
      "Epoch 263/300\n",
      "Average training loss: 0.006868008339570629\n",
      "Average test loss: 9.09963228440202\n",
      "Epoch 264/300\n",
      "Average training loss: 0.006867943724410401\n",
      "Average test loss: 0.2242866473744313\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0068583365736736195\n",
      "Average test loss: 0.015456144721971618\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0068817668813798165\n",
      "Average test loss: 0.5476766774236328\n",
      "Epoch 267/300\n",
      "Average training loss: 0.006955918639070458\n",
      "Average test loss: 21.787425602585078\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006982375823789173\n",
      "Average test loss: 0.07312612739205361\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006924144316464663\n",
      "Average test loss: 1.0093147470135655\n",
      "Epoch 270/300\n",
      "Average training loss: 0.006860910772449441\n",
      "Average test loss: 0.00842833393274082\n",
      "Epoch 271/300\n",
      "Average training loss: 0.00688027432685097\n",
      "Average test loss: 105.09989155529605\n",
      "Epoch 272/300\n",
      "Average training loss: 0.006900658750699626\n",
      "Average test loss: 0.25816608041421407\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0068773811078733866\n",
      "Average test loss: 2.5284234486818313\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006957397747370932\n",
      "Average test loss: 0.008786823807905117\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006802911865628428\n",
      "Average test loss: 0.14104992325603963\n",
      "Epoch 276/300\n",
      "Average training loss: 0.006834748932056957\n",
      "Average test loss: 23.88863374770681\n",
      "Epoch 277/300\n",
      "Average training loss: 0.006813409531282054\n",
      "Average test loss: 3664.1916426323783\n",
      "Epoch 278/300\n",
      "Average training loss: 0.006843109659436676\n",
      "Average test loss: 168.04952307956748\n",
      "Epoch 279/300\n",
      "Average training loss: 0.006794585855470763\n",
      "Average test loss: 77.83419311435024\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0068964570711056395\n",
      "Average test loss: 76.27928463490804\n",
      "Epoch 281/300\n",
      "Average training loss: 0.006879880191551314\n",
      "Average test loss: 4.5086857271558705\n",
      "Epoch 282/300\n",
      "Average training loss: 0.006815324469986889\n",
      "Average test loss: 0.3406284625613027\n",
      "Epoch 283/300\n",
      "Average training loss: 0.006823209337476227\n",
      "Average test loss: 0.015988054792914125\n",
      "Epoch 284/300\n",
      "Average training loss: 0.006817032559050454\n",
      "Average test loss: 3042.298556846503\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00679702537962132\n",
      "Average test loss: 11310.991890888303\n",
      "Epoch 286/300\n",
      "Average training loss: 0.006822551031907399\n",
      "Average test loss: 812.559105429502\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0067793617765936585\n",
      "Average test loss: 0.020382842420703835\n",
      "Epoch 288/300\n",
      "Average training loss: 0.006794177858779828\n",
      "Average test loss: 10.682174831334502\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0067721303763488925\n",
      "Average test loss: 0.015411921005282138\n",
      "Epoch 290/300\n",
      "Average training loss: 0.006763505366113451\n",
      "Average test loss: 0.01391843033913109\n",
      "Epoch 291/300\n",
      "Average training loss: 0.006725362630767955\n",
      "Average test loss: 17.546057416234166\n",
      "Epoch 292/300\n",
      "Average training loss: 0.006755760208186176\n",
      "Average test loss: 6384695.695666667\n",
      "Epoch 293/300\n",
      "Average training loss: 0.006763460443251663\n",
      "Average test loss: 0.8678039135353433\n",
      "Epoch 294/300\n",
      "Average training loss: 0.006782706939097907\n",
      "Average test loss: 0.9734201738747458\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006713630792167451\n",
      "Average test loss: 0.15192506878243553\n",
      "Epoch 296/300\n",
      "Average training loss: 0.006720566830287377\n",
      "Average test loss: 255.29337639184791\n",
      "Epoch 297/300\n",
      "Average training loss: 0.006736375582714876\n",
      "Average test loss: 0.08143449534641371\n",
      "Epoch 298/300\n",
      "Average training loss: 0.006836996758563651\n",
      "Average test loss: 0.1248582183967034\n",
      "Epoch 299/300\n",
      "Average training loss: 0.006787309050559997\n",
      "Average test loss: 1.7614825633532472\n",
      "Epoch 300/300\n",
      "Average training loss: 0.006714055711610449\n",
      "Average test loss: 66.43411073216134\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.400496914330456\n",
      "Average test loss: 15.222230715970198\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0273552606370714\n",
      "Average test loss: 0.008227971507857243\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020221369738380113\n",
      "Average test loss: 0.006446450047194957\n",
      "Epoch 4/300\n",
      "Average training loss: 0.016795842515097723\n",
      "Average test loss: 0.031075797100447947\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014813806632326709\n",
      "Average test loss: 0.005854053460889392\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013507721995313962\n",
      "Average test loss: 0.0054773314504159825\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012648060105741024\n",
      "Average test loss: 0.005505594643453757\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011788681883778837\n",
      "Average test loss: 0.0059703803271469146\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011003266002568934\n",
      "Average test loss: 0.004739967095769114\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010420331364704502\n",
      "Average test loss: 0.004695495653483602\n",
      "Epoch 11/300\n",
      "Average training loss: 0.009972759270005757\n",
      "Average test loss: 0.004556364917092853\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009529363844957617\n",
      "Average test loss: 0.004805910020238824\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009175047832230727\n",
      "Average test loss: 0.0041148556458453335\n",
      "Epoch 14/300\n",
      "Average training loss: 0.008851118552188078\n",
      "Average test loss: 0.004053801562637091\n",
      "Epoch 15/300\n",
      "Average training loss: 0.008515295422739452\n",
      "Average test loss: 0.0039661008508669006\n",
      "Epoch 16/300\n",
      "Average training loss: 0.023709210083716445\n",
      "Average test loss: 0.03896436020649142\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009276896023915874\n",
      "Average test loss: 0.005134953825010194\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00863359545254045\n",
      "Average test loss: 0.003952130845023526\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008258287664916781\n",
      "Average test loss: 0.004381352153917154\n",
      "Epoch 20/300\n",
      "Average training loss: 0.007962738076845804\n",
      "Average test loss: 0.003954848462508784\n",
      "Epoch 21/300\n",
      "Average training loss: 0.007752504354135857\n",
      "Average test loss: 0.003869157482766443\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0075441058700283364\n",
      "Average test loss: 0.0036160147855472235\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007363146701620684\n",
      "Average test loss: 0.004588321791340907\n",
      "Epoch 24/300\n",
      "Average training loss: 0.007205297318183713\n",
      "Average test loss: 0.0034537286123053896\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007071263850977024\n",
      "Average test loss: 0.005414651873211066\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006973624281171296\n",
      "Average test loss: 0.005108893055054876\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006844627664320999\n",
      "Average test loss: 0.004564919175373184\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006734482993682226\n",
      "Average test loss: 0.0033242427005122106\n",
      "Epoch 29/300\n",
      "Average training loss: 0.006638418373134401\n",
      "Average test loss: 0.003380433812323544\n",
      "Epoch 30/300\n",
      "Average training loss: 0.006605709052334229\n",
      "Average test loss: 0.27903701781895424\n",
      "Epoch 31/300\n",
      "Average training loss: 0.006457845957742797\n",
      "Average test loss: 0.06356636581652694\n",
      "Epoch 32/300\n",
      "Average training loss: 0.006835348032828834\n",
      "Average test loss: 0.003239019768519534\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0064016807604995035\n",
      "Average test loss: 0.003672601163594259\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006349296181152264\n",
      "Average test loss: 0.003128754854409231\n",
      "Epoch 35/300\n",
      "Average training loss: 0.00626315503857202\n",
      "Average test loss: 0.0031170859051247437\n",
      "Epoch 36/300\n",
      "Average training loss: 0.006131842412882381\n",
      "Average test loss: 0.0030870521805352637\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01013671496262153\n",
      "Average test loss: 0.05944309793391989\n",
      "Epoch 38/300\n",
      "Average training loss: 0.006787061184230778\n",
      "Average test loss: 0.008159503399704893\n",
      "Epoch 39/300\n",
      "Average training loss: 0.006468895872847901\n",
      "Average test loss: 6.194813788074586\n",
      "Epoch 40/300\n",
      "Average training loss: 0.006568497034410635\n",
      "Average test loss: 0.08361561329414448\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00622492668322391\n",
      "Average test loss: 0.5343747766680188\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006179394435137511\n",
      "Average test loss: 0.01783370164036751\n",
      "Epoch 43/300\n",
      "Average training loss: 0.006132900660236676\n",
      "Average test loss: 0.003735208349509372\n",
      "Epoch 44/300\n",
      "Average training loss: 0.006052382700145245\n",
      "Average test loss: 0.003372817351379328\n",
      "Epoch 45/300\n",
      "Average training loss: 0.006012549361421002\n",
      "Average test loss: 0.2645220151144183\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009466541807270712\n",
      "Average test loss: 0.003380249146165119\n",
      "Epoch 47/300\n",
      "Average training loss: 0.006429772750784954\n",
      "Average test loss: 0.03313952304919561\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0062073083739313814\n",
      "Average test loss: 0.004710798819859823\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0060675723966625\n",
      "Average test loss: 0.007861699946224689\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005991978864288992\n",
      "Average test loss: 0.003218635215320521\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005952871123535765\n",
      "Average test loss: 0.009413022937873999\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0059910539645287725\n",
      "Average test loss: 0.2037606609645817\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005941484986493985\n",
      "Average test loss: 0.0031271055471152067\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005922601909687122\n",
      "Average test loss: 1.6574464112851355\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005856920168631607\n",
      "Average test loss: 0.0030351368938055304\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005751399421857463\n",
      "Average test loss: 0.003014523838336269\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005710765866769684\n",
      "Average test loss: 239.36713655188348\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005862997786452373\n",
      "Average test loss: 0.05126838027106391\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005680459991304411\n",
      "Average test loss: 0.005219240280696087\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008995331522077323\n",
      "Average test loss: 0.05042351089542111\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006409671945290433\n",
      "Average test loss: 0.0031958600771096017\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005946080391398735\n",
      "Average test loss: 0.0031297278909219635\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005806259904470709\n",
      "Average test loss: 0.004272074752176802\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005697455986506409\n",
      "Average test loss: 1.9858392630202903\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005622980264325937\n",
      "Average test loss: 0.545788404032588\n",
      "Epoch 66/300\n",
      "Average training loss: 0.005606094209684266\n",
      "Average test loss: 0.003726864123096069\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00555582717516356\n",
      "Average test loss: 0.013120695172705584\n",
      "Epoch 68/300\n",
      "Average training loss: 0.006236133667950829\n",
      "Average test loss: 0.013338416104929314\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005612806842558914\n",
      "Average test loss: 0.004371657508942816\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005522221277571387\n",
      "Average test loss: 173.5321806034628\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005503873988986016\n",
      "Average test loss: 0.9178786021882875\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005462690438247389\n",
      "Average test loss: 3.76201039934655\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005470694913218418\n",
      "Average test loss: 0.002949925562987725\n",
      "Epoch 74/300\n",
      "Average training loss: 0.005426194644222657\n",
      "Average test loss: 0.02515796484404968\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0053778105605807565\n",
      "Average test loss: 2.3132924871121845\n",
      "Epoch 76/300\n",
      "Average training loss: 0.005340511126650704\n",
      "Average test loss: 0.002912196940018071\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0053166799967487655\n",
      "Average test loss: 0.5312014259232415\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005273318433099323\n",
      "Average test loss: 0.0036967936936351986\n",
      "Epoch 79/300\n",
      "Average training loss: 0.005217199424488677\n",
      "Average test loss: 0.5334214793943489\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005239829692989588\n",
      "Average test loss: 0.5592860867281755\n",
      "Epoch 81/300\n",
      "Average training loss: 0.005234452528258164\n",
      "Average test loss: 1.345862774193287\n",
      "Epoch 82/300\n",
      "Average training loss: 0.005165996658305327\n",
      "Average test loss: 0.01191180902822978\n",
      "Epoch 83/300\n",
      "Average training loss: 0.005148442664494117\n",
      "Average test loss: 0.0030554024742709264\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005095222880235977\n",
      "Average test loss: 0.00811448148224089\n",
      "Epoch 86/300\n",
      "Average training loss: 0.005068940752910243\n",
      "Average test loss: 0.003977256753895845\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00515425698666109\n",
      "Average test loss: 0.030756015914181867\n",
      "Epoch 88/300\n",
      "Average training loss: 0.005015471988668045\n",
      "Average test loss: 0.0030504063247806497\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00497420393054684\n",
      "Average test loss: 26.758350209554038\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004949823508246077\n",
      "Average test loss: 14.010993282432358\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006374744365198744\n",
      "Average test loss: 0.002950244409342607\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005355048478891452\n",
      "Average test loss: 0.004193951564116611\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005073992427852419\n",
      "Average test loss: 0.07256605430795915\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004964491399212016\n",
      "Average test loss: 0.01592096299553911\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004924484640773799\n",
      "Average test loss: 0.026084904516736666\n",
      "Epoch 98/300\n",
      "Average training loss: 0.005406068991869688\n",
      "Average test loss: 0.09940450384219487\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005023951038304302\n",
      "Average test loss: 0.020791628900501465\n",
      "Epoch 100/300\n",
      "Average training loss: 0.004979780508826176\n",
      "Average test loss: 1.4849745730600423\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004872623729002145\n",
      "Average test loss: 55.60112938125597\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005218393759388063\n",
      "Average test loss: 0.07183619045259224\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0048932693265378475\n",
      "Average test loss: 0.020262277669376796\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004831162768105666\n",
      "Average test loss: 0.003696678034340342\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004805638276454475\n",
      "Average test loss: 331.6941376636848\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004775558477060663\n",
      "Average test loss: 569.8724675894637\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004818574751209882\n",
      "Average test loss: 0.6008496664249234\n",
      "Epoch 109/300\n",
      "Average training loss: 0.004839922978232304\n",
      "Average test loss: 0.02179421304145621\n",
      "Epoch 110/300\n",
      "Average training loss: 0.004870026308215327\n",
      "Average test loss: 0.02047941867344909\n",
      "Epoch 111/300\n",
      "Average training loss: 0.004948557009092636\n",
      "Average test loss: 2.275714788071811\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004692844220333629\n",
      "Average test loss: 0.0029340693939270243\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004681740312112702\n",
      "Average test loss: 313.21273272870644\n",
      "Epoch 114/300\n",
      "Average test loss: 12.43625611937046\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004668469671159982\n",
      "Average test loss: 0.34341915831942527\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004663441527220938\n",
      "Average test loss: 0.0072455194503482845\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004815584688757857\n",
      "Average test loss: 0.003332682976913121\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004629554859052102\n",
      "Average test loss: 0.11664277077383466\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004667123149666521\n",
      "Average test loss: 0.002977455920436316\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004577255708682868\n",
      "Average test loss: 0.017502177349395222\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0046561351147376825\n",
      "Average test loss: 0.009231339132620229\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004553351944519414\n",
      "Average test loss: 0.009641568484405677\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004555804845359591\n",
      "Average test loss: 4.1723308002170585\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004564734634425905\n",
      "Average test loss: 0.0031395748280402687\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004509365008109146\n",
      "Average test loss: 0.0044413048405614166\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004616864531404442\n",
      "Average test loss: 0.04535545002255175\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004734187459987071\n",
      "Average test loss: 0.006179367619256178\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004862355159388648\n",
      "Average test loss: 0.1589061845590671\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004563134177691407\n",
      "Average test loss: 65742.12050055862\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004442048712943991\n",
      "Average test loss: 1.7323343714210722\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004414371830928657\n",
      "Average test loss: 0.11077364095672965\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004412641591909859\n",
      "Average test loss: 29.437866987525176\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004404693118813965\n",
      "Average test loss: 0.012543150141835213\n",
      "Epoch 136/300\n",
      "Average training loss: 0.005035417668521404\n",
      "Average test loss: 346.27502550229843\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004463318380630679\n",
      "Average test loss: 0.0051089870919369985\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004389444618175427\n",
      "Average test loss: 0.011884043564399083\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004424093330899874\n",
      "Average test loss: 2.1915702910224595\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004400645184020202\n",
      "Average test loss: 0.010742820145769251\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0043441720882223716\n",
      "Average test loss: 0.0052045282813616926\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00432916689126028\n",
      "Average test loss: 6.536200557538205\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004494570040868388\n",
      "Average test loss: 0.14665084779345328\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004348383412592941\n",
      "Average test loss: 0.003236011803771059\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004343594914095269\n",
      "Average test loss: 1951.4496164822049\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00474149441677663\n",
      "Average test loss: 0.005794725052391489\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004313744046621853\n",
      "Average test loss: 164.42304530963634\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004271753963910871\n",
      "Average test loss: 18060.853167709785\n",
      "Epoch 150/300\n",
      "Average training loss: 0.00427323200708876\n",
      "Average test loss: 35.34839806545112\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004323343840945098\n",
      "Average test loss: 23.25121922217475\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0043042389932605955\n",
      "Average test loss: 196575.1473695861\n",
      "Epoch 153/300\n",
      "Average training loss: 0.00431714035405053\n",
      "Average test loss: 0.8218208983027273\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004264238991878099\n",
      "Average test loss: 7.925458099091219\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004264989386002223\n",
      "Average test loss: 0.03524304831027985\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004259570880896515\n",
      "Average test loss: 5.086428342747605\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004835363600816991\n",
      "Average test loss: 0.003142364937398169\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004230383670164479\n",
      "Average test loss: 0.0033718073030726777\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0042078391483260525\n",
      "Average test loss: 2.8604628341413205\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004199554419144988\n",
      "Average test loss: 0.7886996376961469\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0041798217054456474\n",
      "Average test loss: 0.005232367059422864\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004193583821049995\n",
      "Average test loss: 4.792750334617578\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004347241994407442\n",
      "Average test loss: 0.003194648173948129\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00429807441143526\n",
      "Average test loss: 0.16559393669499292\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004428408505602015\n",
      "Average test loss: 0.011112627350000872\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004195846644333668\n",
      "Average test loss: 906.5820010841157\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004147585827029413\n",
      "Average test loss: 113.74200534842825\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004138378783646557\n",
      "Average test loss: 544.0443281576565\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0041723780242933165\n",
      "Average test loss: 0.01554192745561401\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004147867416756021\n",
      "Average test loss: 0.0054943954965306655\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0041457735447006094\n",
      "Average test loss: 858.3310832739936\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0042852121939261755\n",
      "Average test loss: 0.1044301528616084\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004231062853294942\n",
      "Average test loss: 4.689408223869486\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004117325564018554\n",
      "Average test loss: 0.25222321187953156\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00416467430918581\n",
      "Average test loss: 6.280808038843588\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004115069038131171\n",
      "Average test loss: 0.006255248227881061\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0041122555163585475\n",
      "Average test loss: 1.1892628835087848\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004107699566831192\n",
      "Average test loss: 3.733690986595013\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004107893790635798\n",
      "Average test loss: 1.3078226110554403\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004138120039676627\n",
      "Average test loss: 1.3644421305639876\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004088045351207256\n",
      "Average test loss: 0.04146479506542285\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00409571763417787\n",
      "Average test loss: 1.7859048390818966\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004263290140777826\n",
      "Average test loss: 0.13864023446457255\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004222571513305108\n",
      "Average test loss: 298.57428380196626\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004060028820402092\n",
      "Average test loss: 0.1869180499182807\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004054494161572721\n",
      "Average test loss: 0.0031997179902262156\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004043186951842573\n",
      "Average test loss: 0.007259562104526493\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0040491543283893\n",
      "Average test loss: 0.041499234773839516\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00406428405104412\n",
      "Average test loss: 6.477554588120845\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00403535899023215\n",
      "Average test loss: 0.014244523795735506\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004038143769941396\n",
      "Average test loss: 0.3361024926362766\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004032346493461065\n",
      "Average test loss: 4708.590029253789\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004090844266944461\n",
      "Average test loss: 14.365844133603904\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004119621721406778\n",
      "Average test loss: 2.046386417733298\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004037368297783865\n",
      "Average test loss: 0.003138559229051073\n",
      "Epoch 196/300\n",
      "Average training loss: 0.003997833145989312\n",
      "Average test loss: 0.0037543272448496688\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004002810594729251\n",
      "Average test loss: 10.142239050204763\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00407505267403192\n",
      "Average test loss: 0.02991840584990051\n",
      "Epoch 199/300\n",
      "Average training loss: 0.003981598153503405\n",
      "Average test loss: 3.8764589013647703\n",
      "Epoch 200/300\n",
      "Average training loss: 0.003979724189059602\n",
      "Average test loss: 0.46398481072588926\n",
      "Epoch 201/300\n",
      "Average training loss: 0.003992700878737702\n",
      "Average test loss: 3.1897963101110522\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00418120641551084\n",
      "Average test loss: 194918990.28533334\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0040009519482652346\n",
      "Average test loss: 0.007002689067688253\n",
      "Epoch 204/300\n",
      "Average training loss: 0.003984282399010327\n",
      "Average test loss: 93.15222496541341\n",
      "Epoch 205/300\n",
      "Average training loss: 0.003945178237640195\n",
      "Average test loss: 3.634924995959633\n",
      "Epoch 206/300\n",
      "Average training loss: 0.003964124579810434\n",
      "Average test loss: 65.18777821291238\n",
      "Epoch 207/300\n",
      "Average training loss: 0.003958038164923588\n",
      "Average test loss: 20.6486212428386\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003950851761632496\n",
      "Average test loss: 171776.313809271\n",
      "Epoch 209/300\n",
      "Average training loss: 0.003945517297834158\n",
      "Average test loss: 0.1941012938136442\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0041352015208039015\n",
      "Average test loss: 0.5730656646521141\n",
      "Epoch 211/300\n",
      "Average training loss: 0.003929191486289104\n",
      "Average test loss: 7024.655677411762\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00392743212564124\n",
      "Average test loss: 2.7240856087870067\n",
      "Epoch 213/300\n",
      "Average training loss: 0.003972491369065311\n",
      "Average test loss: 281735.426259145\n",
      "Epoch 214/300\n",
      "Average training loss: 0.003986467302052511\n",
      "Average test loss: 233.10786726493305\n",
      "Epoch 215/300\n",
      "Average training loss: 0.003973136362723178\n",
      "Average test loss: 0.008604620961265432\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0039044063600401084\n",
      "Average test loss: 1.7545366035602572\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0039043464503354494\n",
      "Average test loss: 11.249346837269764\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00391239038709965\n",
      "Average test loss: 7.089234956189162\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004163458134358128\n",
      "Average test loss: 293.7384141354412\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004124819407032596\n",
      "Average test loss: 0.003102127697318792\n",
      "Epoch 221/300\n",
      "Average training loss: 0.003908591787020365\n",
      "Average test loss: 117.1415586829169\n",
      "Epoch 222/300\n",
      "Average training loss: 0.003907338376674387\n",
      "Average test loss: 1.0657993171546194\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00399216299255689\n",
      "Average test loss: 40283.92111134944\n",
      "Epoch 224/300\n",
      "Average training loss: 0.003900767132225964\n",
      "Average test loss: 862042.4678975694\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0038823602211972078\n",
      "Average test loss: 28.176838044598078\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004033081604788701\n",
      "Average test loss: 3183.593121858646\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0038578708945877023\n",
      "Average test loss: 3279.345173613483\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0038536268990072937\n",
      "Average test loss: 4.207683143262441\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0038652712553739546\n",
      "Average test loss: 81338.70285040722\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0038726777107351354\n",
      "Average test loss: 3881.713427121881\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004366379252324502\n",
      "Average test loss: 280.8765446351932\n",
      "Epoch 232/300\n",
      "Average training loss: 0.003845847123819921\n",
      "Average test loss: 3.428467417678071\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0038258977627588643\n",
      "Average test loss: 6299.734468466072\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0038198073613974785\n",
      "Average test loss: 104910.28196210127\n",
      "Epoch 235/300\n",
      "Average training loss: 0.003822559364553955\n",
      "Average test loss: 3446609.516149236\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0038275786091883975\n",
      "Average test loss: 8018180.861114961\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0038422114147494235\n",
      "Average test loss: 100384.29238513495\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0038573217323670784\n",
      "Average test loss: 132.8579385719881\n",
      "Epoch 239/300\n",
      "Average training loss: 0.003829784371579687\n",
      "Average test loss: 8354.660836586874\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004013455391758018\n",
      "Average test loss: 241.34112682434585\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003841043415375882\n",
      "Average test loss: 320.8706030777709\n",
      "Epoch 242/300\n",
      "Average training loss: 0.003828994089530574\n",
      "Average test loss: 6022.986100096613\n",
      "Epoch 243/300\n",
      "Average training loss: 0.003801457714703348\n",
      "Average test loss: 267.531569106429\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0038444192906220754\n",
      "Average test loss: 21.896826919498544\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0038082822571612065\n",
      "Average test loss: 0.21778166738111113\n",
      "Epoch 246/300\n",
      "Average training loss: 0.003809397467308574\n",
      "Average test loss: 366.23487732126915\n",
      "Epoch 247/300\n",
      "Average training loss: 0.00380448366722299\n",
      "Average test loss: 2792877.4371354165\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0039182161610159605\n",
      "Average test loss: 139.37406632884344\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004542858856626683\n",
      "Average test loss: 13.38245932240867\n",
      "Epoch 250/300\n",
      "Average training loss: 0.003985053091413445\n",
      "Average test loss: 14702.085721623198\n",
      "Epoch 251/300\n",
      "Average training loss: 0.003846088902817832\n",
      "Average test loss: 87988.33478859345\n",
      "Epoch 252/300\n",
      "Average training loss: 0.003817687996973594\n",
      "Average test loss: 204.80431662195735\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0038820859293142956\n",
      "Average test loss: 62.808801503143584\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0038024177224271825\n",
      "Average test loss: 3.146268052406609\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0037771550868120458\n",
      "Average test loss: 801.5458542909837\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0037794353746705583\n",
      "Average test loss: 1344.5229586331636\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003902516235286991\n",
      "Average test loss: 27.319243180681433\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003985611676962839\n",
      "Average test loss: 109.87802274270439\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0039660617224872114\n",
      "Average test loss: 54.57578216760854\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0038857371055831514\n",
      "Average test loss: 567.4595893552221\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003840713092850314\n",
      "Average test loss: 0.018417392964371376\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0038100618498606814\n",
      "Average test loss: 0.014217664261245065\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00379112219148212\n",
      "Average test loss: 0.2716322108912799\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0037672964208241965\n",
      "Average test loss: 3.7732248981779235\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004375009479415086\n",
      "Average test loss: 0.229988849690184\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004286732734905349\n",
      "Average test loss: 26053.93195324952\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0040613794579274125\n",
      "Average test loss: 965.8147912801902\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0038780540178219476\n",
      "Average test loss: 4.160668536712726\n",
      "Epoch 269/300\n",
      "Average training loss: 0.003785465567269259\n",
      "Average test loss: 8741.475562986929\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0037670591635008653\n",
      "Average test loss: 276.18475669925743\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0037719836086034775\n",
      "Average test loss: 1254.6831516855618\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0038356354497373103\n",
      "Average test loss: 13445.98034917004\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0037722507150222857\n",
      "Average test loss: 3423170.9620860764\n",
      "Epoch 274/300\n",
      "Average training loss: 0.003766122812198268\n",
      "Average test loss: 118.34228134384017\n",
      "Epoch 275/300\n",
      "Average training loss: 0.003931635640975502\n",
      "Average test loss: 496.94858764409213\n",
      "Epoch 276/300\n",
      "Average training loss: 0.003814953028327889\n",
      "Average test loss: 18.847935336927574\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0037902488687800036\n",
      "Average test loss: 42.407007359024554\n",
      "Epoch 278/300\n",
      "Average training loss: 0.003777839073497388\n",
      "Average test loss: 1067.3772716620867\n",
      "Epoch 279/300\n",
      "Average training loss: 0.003761195081596573\n",
      "Average test loss: 49274.751510228634\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0037699333623879487\n",
      "Average test loss: 4.463123120481769\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003723820566510161\n",
      "Average test loss: 641.9700049416348\n",
      "Epoch 282/300\n",
      "Average training loss: 0.003735542261559102\n",
      "Average test loss: 7418173.398909722\n",
      "Epoch 283/300\n",
      "Average training loss: 0.003827674908356534\n",
      "Average test loss: 1.03168832237646\n",
      "Epoch 284/300\n",
      "Average training loss: 0.003750806712028053\n",
      "Average test loss: 1470.4732367522824\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0037153966993921333\n",
      "Average test loss: 70618.67923562125\n",
      "Epoch 286/300\n",
      "Average training loss: 0.00372462531655199\n",
      "Average test loss: 492967.861097602\n",
      "Epoch 287/300\n",
      "Average training loss: 0.003741523879269759\n",
      "Average test loss: 9290633.018387193\n",
      "Epoch 288/300\n",
      "Average training loss: 0.003731797631002135\n",
      "Average test loss: 504.7308932648152\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0037136746444222\n",
      "Average test loss: 349229.51253686525\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003750728545089563\n",
      "Average test loss: 225.98948545754618\n",
      "Epoch 291/300\n",
      "Average training loss: 0.003779053426244193\n",
      "Average test loss: 8218.944342283712\n",
      "Epoch 292/300\n",
      "Average training loss: 0.003999561514291499\n",
      "Average test loss: 495.2521482712949\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00383013991970155\n",
      "Average test loss: 800.8140100572937\n",
      "Epoch 294/300\n",
      "Average training loss: 0.003755375394390689\n",
      "Average test loss: 3.5977063111081304\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0037299934327602387\n",
      "Average test loss: 7355461.072278798\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0037616395000368355\n",
      "Average test loss: 171404.2441112131\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0036978054913795657\n",
      "Average test loss: 8578.46599574458\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003673027880489826\n",
      "Average test loss: 157505.5111000434\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0036998583815164034\n",
      "Average test loss: 830.7238918576737\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0036668589487671853\n",
      "Average test loss: 17.113407240765582\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.33006756673256554\n",
      "Average test loss: 0.007159776000512971\n",
      "Epoch 2/300\n",
      "Average training loss: 0.023007897953192392\n",
      "Average test loss: 0.13060698950621816\n",
      "Epoch 3/300\n",
      "Average training loss: 0.016211691493789353\n",
      "Average test loss: 0.0049711873767276605\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013537647530436516\n",
      "Average test loss: 0.004730713589116931\n",
      "Epoch 5/300\n",
      "Average training loss: 0.011862695327235592\n",
      "Average test loss: 2.9929240508625905\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01068374440487888\n",
      "Average test loss: 0.004161451330615414\n",
      "Epoch 7/300\n",
      "Average training loss: 0.00987750056054857\n",
      "Average test loss: 0.018908093698736694\n",
      "Epoch 8/300\n",
      "Average training loss: 0.00922254827991128\n",
      "Average test loss: 0.004895010381109185\n",
      "Epoch 9/300\n",
      "Average training loss: 0.00871953591869937\n",
      "Average test loss: 0.08739345231983396\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008185731680028969\n",
      "Average test loss: 0.004514976692489452\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007860967692401674\n",
      "Average test loss: 0.003430878500971529\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007573249430292183\n",
      "Average test loss: 0.00341215805336833\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007184517437799109\n",
      "Average test loss: 0.4479297918130954\n",
      "Epoch 14/300\n",
      "Average training loss: 0.006897211236672269\n",
      "Average test loss: 0.003040823527508312\n",
      "Epoch 15/300\n",
      "Average training loss: 0.006581365047229661\n",
      "Average test loss: 0.08188415800035\n",
      "Epoch 16/300\n",
      "Average training loss: 0.006372974233908786\n",
      "Average test loss: 0.0028152497901270785\n",
      "Epoch 17/300\n",
      "Average training loss: 0.006307209895716773\n",
      "Average test loss: 0.29522071921411486\n",
      "Epoch 18/300\n",
      "Average training loss: 0.005967450273740623\n",
      "Average test loss: 0.0033570709959086444\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005711109765701824\n",
      "Average test loss: 0.003667159704077575\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0057217481570939225\n",
      "Average test loss: 0.0025439172929359807\n",
      "Epoch 21/300\n",
      "Average training loss: 0.005395116003437174\n",
      "Average test loss: 0.0041442079494396844\n",
      "Epoch 22/300\n",
      "Average training loss: 0.005301924058546623\n",
      "Average test loss: 0.004556322831246588\n",
      "Epoch 23/300\n",
      "Average training loss: 0.005112441739274396\n",
      "Average test loss: 0.0023981744029248756\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0049405397278153235\n",
      "Average test loss: 0.013003145991100206\n",
      "Epoch 25/300\n",
      "Average training loss: 0.005195345393071572\n",
      "Average test loss: 0.004285187791619036\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004745940145105124\n",
      "Average test loss: 0.0023426692191925312\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004589272247006496\n",
      "Average test loss: 0.00524890122604039\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0045612342299686534\n",
      "Average test loss: 0.002221629100334313\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004462568556682931\n",
      "Average test loss: 0.0030889020214478176\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004286346319648955\n",
      "Average test loss: 0.002259419822237558\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00446062970161438\n",
      "Average test loss: 0.0029599287098066676\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004165424145551192\n",
      "Average test loss: 0.0020458297653951577\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0040971849124050806\n",
      "Average test loss: 0.002049450488968028\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004028789370631178\n",
      "Average test loss: 0.04532076678963171\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004033669522032142\n",
      "Average test loss: 0.007832758392724726\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003951548257428739\n",
      "Average test loss: 0.048794322438124156\n",
      "Epoch 37/300\n",
      "Average training loss: 0.003868418884774049\n",
      "Average test loss: 0.0030071572615868515\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0038225014166285596\n",
      "Average test loss: 5.872211043805712\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0040446908867193595\n",
      "Average test loss: 0.002075027125991053\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0037346402971694865\n",
      "Average test loss: 4.409064090942343\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003708979223544399\n",
      "Average test loss: 0.2753709506044785\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011471193979183832\n",
      "Average test loss: 0.002936601582914591\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005654279936932855\n",
      "Average test loss: 0.004054041387720241\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004635658343839976\n",
      "Average test loss: 0.033402733449720674\n",
      "Epoch 45/300\n",
      "Average training loss: 0.004303254733482997\n",
      "Average test loss: 0.005619401449337601\n",
      "Epoch 46/300\n",
      "Average training loss: 0.004125986252807908\n",
      "Average test loss: 0.07043638769702779\n",
      "Epoch 47/300\n",
      "Average training loss: 0.004012113609661658\n",
      "Average test loss: 0.02129469557934337\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003920326563012269\n",
      "Average test loss: 0.013438108435728484\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0038690503891557456\n",
      "Average test loss: 0.004349086801418\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0038099490153706735\n",
      "Average test loss: 0.11599366155763467\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0037658863371858994\n",
      "Average test loss: 2.4623750513825153\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0037250803077800406\n",
      "Average test loss: 0.008220593923495875\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00368799196369946\n",
      "Average test loss: 0.8265612297538254\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0036607992257922887\n",
      "Average test loss: 51.4696721104814\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0037604187772505814\n",
      "Average test loss: 0.038641148624320824\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0036053432987795935\n",
      "Average test loss: 0.006745298709720373\n",
      "Epoch 57/300\n",
      "Average training loss: 0.003565823916552795\n",
      "Average test loss: 10.39133504830218\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003546925338813\n",
      "Average test loss: 0.0066623453009459705\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003516448963847425\n",
      "Average test loss: 0.028397146691464717\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003572226951105727\n",
      "Average test loss: 21.69354417695436\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00545548177531196\n",
      "Average test loss: 0.05027421814037694\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003942049886203475\n",
      "Average test loss: 0.03862236752733588\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0037247289040436347\n",
      "Average test loss: 0.13900815661665467\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003623445278654496\n",
      "Average test loss: 0.20119469745374388\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003560335772112012\n",
      "Average test loss: 24.467282734406492\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0035175151798046297\n",
      "Average test loss: 0.012349899935743047\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0034892143634044463\n",
      "Average test loss: 0.689383574951854\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0034482728453973928\n",
      "Average test loss: 0.0018462180140842166\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0034712557622955907\n",
      "Average test loss: 0.011503072094275719\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0034205637346539234\n",
      "Average test loss: 2.9452173347951223\n",
      "Epoch 71/300\n",
      "Average training loss: 0.003417863180860877\n",
      "Average test loss: 1.7134729945047034\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003363929558545351\n",
      "Average test loss: 1.7183234562737246\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0033473018687218427\n",
      "Average test loss: 0.33931440728323325\n",
      "Epoch 74/300\n",
      "Average training loss: 0.003297149972576234\n",
      "Average test loss: 67.25878802977378\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0033175108176138664\n",
      "Average test loss: 4.453038561143602\n",
      "Epoch 76/300\n",
      "Average training loss: 0.003293055733044942\n",
      "Average test loss: 0.004254348408948216\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0034452732279896736\n",
      "Average test loss: 0.0990459922398958\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0032644864422165685\n",
      "Average test loss: 2.1099541295328073\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004651899187101258\n",
      "Average test loss: 0.0019700583500994575\n",
      "Epoch 80/300\n",
      "Average training loss: 0.003776665109106236\n",
      "Average test loss: 0.0021678114116398823\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0035015188817762665\n",
      "Average test loss: 1.2729513073298666\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00336408615940147\n",
      "Average test loss: 0.002568259428669181\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00329013094988962\n",
      "Average test loss: 0.003257498785439465\n",
      "Epoch 84/300\n",
      "Average training loss: 0.003366494102196561\n",
      "Average test loss: 0.003224816010747519\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0032380290828231306\n",
      "Average test loss: 2.4470089395874077\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0031978632282051774\n",
      "Average test loss: 0.052471834626152285\n",
      "Epoch 87/300\n",
      "Average training loss: 0.003247288739308715\n",
      "Average test loss: 0.01234370252252039\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0031630481742322444\n",
      "Average test loss: 2.720264363836911\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003129592924275332\n",
      "Average test loss: 0.31670783608903486\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0031189480562590893\n",
      "Average test loss: 0.02217212726424138\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0032866542598025666\n",
      "Average test loss: 0.08590723175576163\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0030937602139181563\n",
      "Average test loss: 861424.0558055555\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0030736230918102796\n",
      "Average test loss: 0.005326484509847231\n",
      "Epoch 94/300\n",
      "Average training loss: 0.003201579714814822\n",
      "Average test loss: 0.25366518875604704\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0032015818138089445\n",
      "Average test loss: 29.69722705574582\n",
      "Epoch 96/300\n",
      "Average training loss: 0.003047389889963799\n",
      "Average test loss: 0.4404422119938665\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0030128124399731557\n",
      "Average test loss: 0.7295080080297258\n",
      "Epoch 98/300\n",
      "Average training loss: 0.00299282676850756\n",
      "Average test loss: 0.004449884816383322\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005977790257583062\n",
      "Average test loss: 0.002484763789094157\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0037292352070411048\n",
      "Average test loss: 0.039190875398822954\n",
      "Epoch 101/300\n",
      "Average training loss: 0.003477591705198089\n",
      "Average test loss: 0.023098120448076062\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0033261622291886144\n",
      "Average test loss: 0.6995325184344418\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0032018275099496047\n",
      "Average test loss: 212.09811318630642\n",
      "Epoch 104/300\n",
      "Average training loss: 0.003168775711829464\n",
      "Average test loss: 5.64484067142227\n",
      "Epoch 105/300\n",
      "Average training loss: 0.003109495147648785\n",
      "Average test loss: 5.963309344118875\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0030600806346370115\n",
      "Average test loss: 64.29335677494605\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003059143092483282\n",
      "Average test loss: 539.0995317833904\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0031408954699420266\n",
      "Average test loss: 0.0022507309772902065\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0031042179628792735\n",
      "Average test loss: 0.05990909877667824\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003139006065618661\n",
      "Average test loss: 0.2580938422398435\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00301589960936043\n",
      "Average test loss: 7.76675580351303\n",
      "Epoch 112/300\n",
      "Average training loss: 0.002989109759322471\n",
      "Average test loss: 95.21010435380373\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002974625582082404\n",
      "Average test loss: 2.7398211288694294\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0031688912192152606\n",
      "Average test loss: 0.808353785422941\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0029462697147909136\n",
      "Average test loss: 0.8819822091253268\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0029421004801988603\n",
      "Average test loss: 0.004598968533902533\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002965592110943463\n",
      "Average test loss: 0.0031270574498921634\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0029414511456464726\n",
      "Average test loss: 2628.3577640651883\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0029460018920815655\n",
      "Average test loss: 10.131734597429219\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002924548610630963\n",
      "Average test loss: 7.191928704911636\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002921285949854387\n",
      "Average test loss: 0.20592871964164078\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002881622935541802\n",
      "Average test loss: 0.6399891368672251\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0029186687823385\n",
      "Average test loss: 0.7643797983801406\n",
      "Epoch 124/300\n",
      "Average training loss: 0.003047395447269082\n",
      "Average test loss: 0.0555910666162769\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0028499979184319576\n",
      "Average test loss: 0.0019371822761992613\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0028046598264740573\n",
      "Average test loss: 0.06649950990349882\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002995125730211536\n",
      "Average test loss: 19.770342483709463\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0028157679862860175\n",
      "Average test loss: 0.027371736591060958\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0028168111867788764\n",
      "Average test loss: 303.6105282011508\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0027820562165644432\n",
      "Average test loss: 107.54149955576327\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002784115821744005\n",
      "Average test loss: 0.07119323436791698\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0028724196586343976\n",
      "Average test loss: 0.025932334654861028\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0027346223626906674\n",
      "Average test loss: 27.603535656944747\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0027418165592890645\n",
      "Average test loss: 5.003535646117395\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002797447773317496\n",
      "Average test loss: 0.5346581964563164\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0027283367142081263\n",
      "Average test loss: 27203.00909508889\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002737868683412671\n",
      "Average test loss: 0.08219634886831045\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002752240515210562\n",
      "Average test loss: 82836.47136111111\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003222584295603964\n",
      "Average test loss: 0.07778953780316644\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0027728229011926386\n",
      "Average test loss: 3.4843804570659995\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00268650048909088\n",
      "Average test loss: 143.6980458438802\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0026604234553459618\n",
      "Average test loss: 19519.146695389118\n",
      "Epoch 143/300\n",
      "Average training loss: 0.002731963111501601\n",
      "Average test loss: 1548.9448963544733\n",
      "Epoch 144/300\n",
      "Average training loss: 0.002721517206273145\n",
      "Average test loss: 0.584371397290741\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0026524921459042364\n",
      "Average test loss: 222.91696178529247\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002681477315309975\n",
      "Average test loss: 1.2902470701484838\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0026765080772133336\n",
      "Average test loss: 2.4665794757068573\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0026292613686786756\n",
      "Average test loss: 0.004322350067603919\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002759317468851805\n",
      "Average test loss: 0.002007523607669605\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002836246291589406\n",
      "Average test loss: 0.00558402527258214\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0026189430049724047\n",
      "Average test loss: 0.8760429008253333\n",
      "Epoch 152/300\n",
      "Average training loss: 0.002598900452463163\n",
      "Average test loss: 514487.03148871526\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006534094479348924\n",
      "Average test loss: 0.0022008308592355915\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0042713420258628\n",
      "Average test loss: 0.10907142132520675\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0036323546775513223\n",
      "Average test loss: 1.7533342004732952\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0033157168715778323\n",
      "Average test loss: 0.18617467574112945\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0030993018326246078\n",
      "Average test loss: 2.979070248502824\n",
      "Epoch 158/300\n",
      "Average training loss: 0.002950048979371786\n",
      "Average test loss: 1.6395449500059087\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0028411360550671815\n",
      "Average test loss: 57.814049364659525\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0028946465816762712\n",
      "Average test loss: 1.4777182452426187\n",
      "Epoch 161/300\n",
      "Average training loss: 0.002793763119313452\n",
      "Average test loss: 3.0300053032206162\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0027701156652636\n",
      "Average test loss: 3.672586943289886\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002741364658706718\n",
      "Average test loss: 143.04570808951215\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0026930513290895357\n",
      "Average test loss: 1131.5173741231517\n",
      "Epoch 165/300\n",
      "Average training loss: 0.002671385453082621\n",
      "Average test loss: 0.03900678359344602\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0026262168115418817\n",
      "Average test loss: 5.387000456342267\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0026413899610439936\n",
      "Average test loss: 8.760160340435803\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002628290476898352\n",
      "Average test loss: 7.843508555268662\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0026268335030310685\n",
      "Average test loss: 0.12344482873173224\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002678532054647803\n",
      "Average test loss: 12.334184668497079\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002605288353645139\n",
      "Average test loss: 0.18168935486012036\n",
      "Epoch 172/300\n",
      "Average training loss: 0.002576923174990548\n",
      "Average test loss: 1.7220139991772465\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0026984765715897083\n",
      "Average test loss: 29.55645206949363\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0026967220024930105\n",
      "Average test loss: 444.3102501686936\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0025929828652491173\n",
      "Average test loss: 110.87894169113785\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0025572685808357264\n",
      "Average test loss: 241.7175692041988\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0026685808605204026\n",
      "Average test loss: 0.024271719433781175\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0026008386907892095\n",
      "Average test loss: 0.2689066571789897\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0025422009146875806\n",
      "Average test loss: 0.16487259129124382\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0025663451589643957\n",
      "Average test loss: 4.1565680108091065\n",
      "Epoch 181/300\n",
      "Average training loss: 0.002581294629101952\n",
      "Average test loss: 0.1319579857699573\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0027912229556176396\n",
      "Average test loss: 2571.9967787566748\n",
      "Epoch 183/300\n",
      "Average training loss: 0.00261242127397822\n",
      "Average test loss: 3.377904519467925\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002596432208807932\n",
      "Average test loss: 49745.43728933919\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0025708792234460515\n",
      "Average test loss: 377068.1495568323\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0026125138273669613\n",
      "Average test loss: 6.420131489092277\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0025311792809516193\n",
      "Average test loss: 0.012060035766826736\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0025015232685125536\n",
      "Average test loss: 31.130536648220485\n",
      "Epoch 189/300\n",
      "Average training loss: 0.002660916423218118\n",
      "Average test loss: 0.025583421748131514\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00248628855869174\n",
      "Average test loss: 0.09082074496108625\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002536787619193395\n",
      "Average test loss: 0.3060076213040286\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002548166636377573\n",
      "Average test loss: 3290.2233755056477\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0025371895569066207\n",
      "Average test loss: 0.04527889385985004\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0024809422720637585\n",
      "Average test loss: 0.32127117885328416\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0026025755165351764\n",
      "Average test loss: 16.833495762057602\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0024737575062447124\n",
      "Average test loss: 2.8941092734393767\n",
      "Epoch 197/300\n",
      "Average training loss: 0.002811748440687855\n",
      "Average test loss: 7291.596150173611\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0025430853196109333\n",
      "Average test loss: 191.30232662867672\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0024841143337802755\n",
      "Average test loss: 33717.37318374992\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002500645567973455\n",
      "Average test loss: 5.263783791893266\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0024444918462799654\n",
      "Average test loss: 142.18013666195338\n",
      "Epoch 202/300\n",
      "Average training loss: 0.002470910171978176\n",
      "Average test loss: 30.528566269821177\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0024382519485015007\n",
      "Average test loss: 202113.0335596414\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0024557788330647683\n",
      "Average test loss: 0.07742845472113953\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0024646296156570317\n",
      "Average test loss: 7.375946436524184\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0024514647583580677\n",
      "Average test loss: 0.04052174003360172\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002435844838826193\n",
      "Average test loss: 0.0021935465269618563\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0026576893880135483\n",
      "Average test loss: 0.004056954208761453\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002441895884565181\n",
      "Average test loss: 643.5048063142302\n",
      "Epoch 212/300\n",
      "Average training loss: 0.002408448029930393\n",
      "Average test loss: 4.645966420719193\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0026641120391173497\n",
      "Average test loss: 18098.041456108942\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002528490802480115\n",
      "Average test loss: 0.03160654553481274\n",
      "Epoch 215/300\n",
      "Average training loss: 0.002423308626852102\n",
      "Average test loss: 0.1644308409732249\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0024066236103988357\n",
      "Average test loss: 0.11501746569656664\n",
      "Epoch 217/300\n",
      "Average training loss: 0.002550337246722645\n",
      "Average test loss: 64.6723889885015\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0023991262482272254\n",
      "Average test loss: 39.16992354901632\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002425063102816542\n",
      "Average test loss: 5.96203199735408\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002441075916712483\n",
      "Average test loss: 0.10160252683464852\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0024082802834196225\n",
      "Average test loss: 13.191972816784556\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002403325216844678\n",
      "Average test loss: 98.19465288604103\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002550962319597602\n",
      "Average test loss: 376.03970775564926\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002467880649285184\n",
      "Average test loss: 2355.8479668420305\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0024225973048143915\n",
      "Average test loss: 1.0179481349864767\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0024451755835778184\n",
      "Average test loss: 24.678562769027426\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0024530054746816554\n",
      "Average test loss: 226.6108583893213\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0024159595825605923\n",
      "Average test loss: 354.8025871963501\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0024008311772098145\n",
      "Average test loss: 0.32421404962572786\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002456623980154594\n",
      "Average test loss: 86.36628156559087\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0023987773480928605\n",
      "Average test loss: 711.4193597167507\n",
      "Epoch 234/300\n",
      "Average training loss: 0.002399178423401382\n",
      "Average test loss: 2852.0858316760346\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00237715948269599\n",
      "Average test loss: 0.02426435218089157\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0024100545631307695\n",
      "Average test loss: 0.13366364906272954\n",
      "Epoch 237/300\n",
      "Average training loss: 0.002728859550216132\n",
      "Average test loss: 0.24657583527184196\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0023767516509526306\n",
      "Average test loss: 47.30312551949401\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002434036717853612\n",
      "Average test loss: 152.957129183118\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0023636504597961904\n",
      "Average test loss: 8.236736645230714\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0027406487311753963\n",
      "Average test loss: 0.008479170087724924\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0026957303061563936\n",
      "Average test loss: 31.79042102246814\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002388488851280676\n",
      "Average test loss: 4.763448488363065\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0024313550767385297\n",
      "Average test loss: 0.002228019247711119\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0025719421460396715\n",
      "Average test loss: 12.279001594652318\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0023611506621042887\n",
      "Average test loss: 3409.20957532547\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0023534778260315458\n",
      "Average test loss: 2788.044840518708\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002359050113086899\n",
      "Average test loss: 866.198424605887\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0023538649055278964\n",
      "Average test loss: 0.025293737693689762\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0024664595988061694\n",
      "Average test loss: 198554.6297853043\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002368772030911512\n",
      "Average test loss: 4.619424404476666\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002361639480623934\n",
      "Average test loss: 6987.5661382528415\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0023610056264119017\n",
      "Average test loss: 182.47009840007127\n",
      "Epoch 254/300\n",
      "Average training loss: 0.002356482844385836\n",
      "Average test loss: 1246995.239230721\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0026501965704891417\n",
      "Average test loss: 36420709.04256141\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002350362425463067\n",
      "Average test loss: 5225353241.281602\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0023667365182191134\n",
      "Average test loss: 67880.46423522221\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002336972103971574\n",
      "Average test loss: 9.051286439973653\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0023668551536069975\n",
      "Average test loss: 18604.54058577147\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0023622237889923984\n",
      "Average test loss: 1420.8297623296016\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002346517442829079\n",
      "Average test loss: 9.368902918802368\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0023179293206582467\n",
      "Average test loss: 14992.619229629378\n",
      "Epoch 265/300\n",
      "Average training loss: 0.002313471860769722\n",
      "Average test loss: 0.056460608260395624\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0023703078277822996\n",
      "Average test loss: 3.6306666125522717\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0022791303659064903\n",
      "Average test loss: 36.66398836337092\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0022787168284671175\n",
      "Average test loss: 224116.48931597223\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0025170186563498445\n",
      "Average test loss: 0.003952151584956381\n",
      "Epoch 270/300\n",
      "Average training loss: 0.002357982678959767\n",
      "Average test loss: 25.130245550682147\n",
      "Epoch 271/300\n",
      "Average training loss: 0.002425177627760503\n",
      "Average test loss: 4.298535955318974\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0022939275580768784\n",
      "Average test loss: 3726.1951099838216\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002274256848419706\n",
      "Average test loss: 0.04687889996564223\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0022537727538082335\n",
      "Average test loss: 2.5959497122404476\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002359212175425556\n",
      "Average test loss: 47426.44773063543\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0024031827139357727\n",
      "Average test loss: 167.89624893253213\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0022593586784270077\n",
      "Average training loss: 0.0022645761538296937\n",
      "Average test loss: 58.377617932565514\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002267774824880891\n",
      "Average test loss: 0.018368644925920913\n",
      "Epoch 281/300\n",
      "Average training loss: 0.002259026857920819\n",
      "Average test loss: 10.155998368427985\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00228515092904369\n",
      "Average test loss: 1.2404597596774498\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0026541666081175207\n",
      "Average test loss: 1192.345651358763\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002422485929499898\n",
      "Average test loss: 700.8735610752349\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0023587827001594836\n",
      "Average test loss: 35.446155610232715\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0023207455571326945\n",
      "Average test loss: 177426.63181030698\n",
      "Epoch 287/300\n",
      "Average training loss: 0.002320910891517997\n",
      "Average test loss: 462216.4036387463\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002253928746614191\n",
      "Average test loss: 48.77202064623642\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0023046025530331663\n",
      "Average test loss: 17.38669194040882\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0022376270323163933\n",
      "Average test loss: 53902.21743393379\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00224945929636144\n",
      "Average test loss: 45.81400274028443\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0022747598783009583\n",
      "Average test loss: 293401.27375868056\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0023242691230649748\n",
      "Average test loss: 228.01854157893857\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0022271446192430127\n",
      "Average test loss: 2.2196162443980576\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0022211678119169343\n",
      "Average test loss: 4514.506093943534\n",
      "Epoch 298/300\n",
      "Average training loss: 0.002223797688053714\n",
      "Average test loss: 61.696100950213356\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0023620479947162997\n",
      "Average test loss: 0.07889282730646018\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 452.1865669348637\n",
      "Average test loss: 609.6109223137233\n",
      "Epoch 2/300\n",
      "Average training loss: 0.13188997520340814\n",
      "Average test loss: 2.798861076959305\n",
      "Epoch 3/300\n",
      "Average training loss: 0.086290677315659\n",
      "Average test loss: 0.18344625106371112\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06348833396037419\n",
      "Average test loss: 0.008630472085542148\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04969652038481501\n",
      "Average test loss: 5.727652702700761\n",
      "Epoch 6/300\n",
      "Average training loss: 0.040615992493099634\n",
      "Average test loss: 47.30474576688351\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03415040567517281\n",
      "Average test loss: 0.007620235610339377\n",
      "Epoch 8/300\n",
      "Average training loss: 0.029068693273597294\n",
      "Average test loss: 0.013894368216395378\n",
      "Epoch 9/300\n",
      "Average training loss: 0.025174818214442996\n",
      "Average test loss: 0.16438949085937607\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022068383796347513\n",
      "Average test loss: 0.00487112295255065\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01956931285560131\n",
      "Average test loss: 13.135508459326294\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017417060105337036\n",
      "Average test loss: 0.10195454025682475\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014177424218919541\n",
      "Average test loss: 0.22140183925566573\n",
      "Epoch 15/300\n",
      "Average training loss: 0.012832019989689192\n",
      "Average test loss: 0.0037594670874791015\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01174215662231048\n",
      "Average test loss: 0.20095461769402026\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010784579589135116\n",
      "Average test loss: 0.040575152286224894\n",
      "Epoch 18/300\n",
      "Average training loss: 0.009943918802671963\n",
      "Average test loss: 0.2551729074633784\n",
      "Epoch 19/300\n",
      "Average training loss: 0.009222065795626904\n",
      "Average test loss: 0.005091976809004943\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008584446285333898\n",
      "Average test loss: 0.311272809876336\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00802816007203526\n",
      "Average test loss: 0.0038977780718770292\n",
      "Epoch 22/300\n",
      "Average training loss: 0.007518071997910738\n",
      "Average test loss: 0.07255671274579234\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00704709149359001\n",
      "Average test loss: 0.0029615714291317597\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006643871435274681\n",
      "Average test loss: 1.2567597658617629\n",
      "Epoch 25/300\n",
      "Average training loss: 0.006286467740105258\n",
      "Average test loss: 0.0032715894151479005\n",
      "Epoch 26/300\n",
      "Average training loss: 0.005953481004056003\n",
      "Average test loss: 0.08994331481224961\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0056720861763589914\n",
      "Average test loss: 0.0034043668776543605\n",
      "Epoch 28/300\n",
      "Average training loss: 0.005184490737194816\n",
      "Average test loss: 0.009001165812628137\n",
      "Epoch 30/300\n",
      "Average training loss: 0.00499524187338021\n",
      "Average test loss: 0.005641608740306563\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004772036931374007\n",
      "Average test loss: 1.7502201487761404\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004621942297865947\n",
      "Average test loss: 0.006311919605152474\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004459035937156942\n",
      "Average test loss: 0.01245610360138946\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004384020030912426\n",
      "Average test loss: 6.189189887927224\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004164141206691663\n",
      "Average test loss: 0.024703124552137322\n",
      "Epoch 36/300\n",
      "Average training loss: 0.004176779393106699\n",
      "Average test loss: 8.017484959272046\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0039980410798970195\n",
      "Average test loss: 0.024617626652121544\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0039205683983034555\n",
      "Average test loss: 0.0037968188354538545\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003745772176525659\n",
      "Average test loss: 0.07858442440835967\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0038007742578370705\n",
      "Average test loss: 0.3623863677353495\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003596415739092562\n",
      "Average test loss: 0.33480884588509796\n",
      "Epoch 42/300\n",
      "Average training loss: 0.003615338868978951\n",
      "Average test loss: 0.6096490295330683\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0034355890831599635\n",
      "Average test loss: 0.3197738764592343\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003367472903802991\n",
      "Average test loss: 0.09615852019687493\n",
      "Epoch 47/300\n",
      "Average training loss: 0.003273863281433781\n",
      "Average test loss: 18.283422657341593\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003229932788345549\n",
      "Average test loss: 1.6199821935696528\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0032627827781769965\n",
      "Average test loss: 1.2206422344644865\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0033208363230029744\n",
      "Average test loss: 0.0028634277507662773\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0031209941413253544\n",
      "Average test loss: 0.015440817675915442\n",
      "Epoch 52/300\n",
      "Average training loss: 0.003282913519379993\n",
      "Average test loss: 13.539720078477016\n",
      "Epoch 53/300\n",
      "Average training loss: 0.003018990251339144\n",
      "Average test loss: 922.7739132215712\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0029605213560991816\n",
      "Average test loss: 29.076881840048564\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002971324456234773\n",
      "Average test loss: 0.013408575484529138\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0028803580433337224\n",
      "Average test loss: 0.0014448291949099965\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00292382778848211\n",
      "Average test loss: 226.08605305277723\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002742317008992864\n",
      "Average test loss: 3.68021945696945\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002731073335227039\n",
      "Average test loss: 2153255.641352542\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002728946473345988\n",
      "Average test loss: 0.11609545631127224\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0028539663781929347\n",
      "Average test loss: 0.0029104648804706003\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002781492626087533\n",
      "Average test loss: 0.0019241224448713992\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0026600755736645725\n",
      "Average test loss: 0.007022322787592809\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0025502316946577693\n",
      "Average test loss: 0.00682570782241722\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002704959206490053\n",
      "Average test loss: 1.7863651220035843\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002530491459493836\n",
      "Average test loss: 0.002587315226491127\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0025205651910768616\n",
      "Average test loss: 0.06272322210974784\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002520480006519291\n",
      "Average test loss: 0.04770777506298489\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002453763115944134\n",
      "Average test loss: 0.002576831385389798\n",
      "Epoch 71/300\n",
      "Average test loss: 0.34018112913767495\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002443177672103047\n",
      "Average test loss: 0.002630413739424613\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0023728987043723465\n",
      "Average test loss: 0.0013170870769148072\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0023761484515335825\n",
      "Average test loss: 0.0069730716398399735\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002342094124812219\n",
      "Average test loss: 0.0026950934052777788\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002264892736242877\n",
      "Average test loss: 0.35404723019070095\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002277886473470264\n",
      "Average test loss: 0.14380862331473165\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002265491560101509\n",
      "Average test loss: 2.2128584875265758\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0022344813001238637\n",
      "Average test loss: 0.002379743827817341\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0023206761355201405\n",
      "Average test loss: 30.45622844336368\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0022138723543741637\n",
      "Average test loss: 0.01297715157818877\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0022247217571776776\n",
      "Average test loss: 9.47305092204362\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0021591842064840926\n",
      "Average test loss: 0.016201292629871102\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002158376266558965\n",
      "Average test loss: 3.59987495816499\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0022050998833858305\n",
      "Average test loss: 0.03255309957545251\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003767619448713958\n",
      "Average test loss: 0.0014046716156622602\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0024840946193370552\n",
      "Average test loss: 3.2625437404314677\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002157812674736811\n",
      "Average test loss: 0.014593951461629735\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0021134866546425554\n",
      "Average test loss: 0.386613334586223\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0020781914337227743\n",
      "Average test loss: 1.3973653917312623\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0020910636438056826\n",
      "Average test loss: 0.6061689895788829\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002773139346184002\n",
      "Average test loss: 2952.0989008385604\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0027830780760074655\n",
      "Average test loss: 0.04182070423911015\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0022977095500876506\n",
      "Average test loss: 3.8784014518790775\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002162949048914015\n",
      "Average test loss: 0.14678449966054824\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0021244112754033673\n",
      "Average test loss: 28.50006749134759\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002076693584728572\n",
      "Average test loss: 204202.0393665481\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0020540299040989743\n",
      "Average test loss: 0.011421288563818154\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002074555750418868\n",
      "Average test loss: 906.7928680063718\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002066941721778777\n",
      "Average test loss: 0.006814878513312174\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0020291295482052695\n",
      "Average test loss: 0.05531637846637103\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002057677550965713\n",
      "Average test loss: 0.024800554489509928\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0020416534218109317\n",
      "Average test loss: 1593.0928139161435\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002038260077126324\n",
      "Average test loss: 0.018894716555356152\n",
      "Epoch 108/300\n",
      "Average training loss: 0.002041101014138096\n",
      "Average test loss: 0.0026801152759128146\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002011069484675924\n",
      "Average test loss: 0.10478312868376573\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0020018425743199056\n",
      "Average test loss: 0.14011370258385109\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0019430396538227797\n",
      "Average test loss: 1.5394402273102767\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0019947873319809636\n",
      "Average test loss: 1023.4086613273778\n",
      "Epoch 113/300\n",
      "Average training loss: 0.001937762501442598\n",
      "Average test loss: 75.86669593703395\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0019595982010165852\n",
      "Average test loss: 0.003359368736700465\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00192065984217657\n",
      "Average test loss: 13.69952313349603\n",
      "Epoch 116/300\n",
      "Average training loss: 0.001931202753033075\n",
      "Average test loss: 23703.453749543092\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0018842594223096967\n",
      "Average test loss: 60.12155117794623\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0018683694590710931\n",
      "Average test loss: 20390.649513413424\n",
      "Epoch 120/300\n",
      "Average training loss: 0.001870511428453028\n",
      "Average test loss: 0.22602644658875135\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002064213018346992\n",
      "Average test loss: 644.0971024722395\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0018276212685224083\n",
      "Average test loss: 0.006327013748853157\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0018590523587125871\n",
      "Average test loss: 140430.30968535118\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0018438650908776456\n",
      "Average test loss: 5147.601729201345\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0018367411951637931\n",
      "Average test loss: 97.48268672863021\n",
      "Epoch 126/300\n",
      "Average training loss: 0.001821286873271068\n",
      "Average test loss: 104.6095412026979\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0018108453759923576\n",
      "Average test loss: 2.020597118757665\n",
      "Epoch 128/300\n",
      "Average training loss: 0.001886041561452051\n",
      "Average test loss: 0.006295323615790241\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0018012241077505879\n",
      "Average test loss: 95.02821389907433\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0018018291981683836\n",
      "Average test loss: 30.768008983407583\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0017987926664451758\n",
      "Average test loss: 13.37506542845774\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0017816041736967033\n",
      "Average test loss: 25149329.657412976\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002014824317043854\n",
      "Average test loss: 1779.425522682377\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0017480432219389413\n",
      "Average test loss: 158.00430262902773\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0017461772583838965\n",
      "Average test loss: 276.047268704878\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0017472451289908754\n",
      "Average test loss: 0.5112183034134408\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0017940446594730019\n",
      "Average test loss: 541.1290902284197\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0016997593360849553\n",
      "Average test loss: 2181.3307711004823\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0017765323747363356\n",
      "Average test loss: 0.41383968559445605\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0017010069734727342\n",
      "Average test loss: 197.4158993691889\n",
      "Epoch 143/300\n",
      "Average training loss: 0.001720927940474616\n",
      "Average test loss: 103.02857954101555\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0017140164429114925\n",
      "Average test loss: 0.3220739058399987\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0016891008229512308\n",
      "Average test loss: 3249.9430082322524\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0016967561362932126\n",
      "Average test loss: 633.1027830817749\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0017069497942510579\n",
      "Average test loss: 1312.7199046842452\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0016866230215463374\n",
      "Average test loss: 24.921801470953866\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0016884664392305744\n",
      "Average test loss: 4499.473816514305\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0016516275580765473\n",
      "Average test loss: 68.4667907810087\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0017238519341788358\n",
      "Average test loss: 43.84233702895293\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0016659051849403316\n",
      "Average test loss: 16.786134509043922\n",
      "Epoch 155/300\n",
      "Average training loss: 0.001690713391225371\n",
      "Average test loss: 68.36591285275719\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0016400556444293923\n",
      "Average test loss: 13.884003490484837\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0016609616689383985\n",
      "Average test loss: 270.58578828398964\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0016434673309947054\n",
      "Average test loss: 16.84810638654149\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0016786807290174895\n",
      "Average test loss: 129.6993629697789\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0016480266137255562\n",
      "Average test loss: 9505.52145442009\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0017027508188039065\n",
      "Average test loss: 6.216390992727544\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0016166751438544855\n",
      "Average test loss: 3044.688661640712\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0016286231997526355\n",
      "Average test loss: 485.6730874552503\n",
      "Epoch 164/300\n",
      "Average training loss: 0.001610010205147167\n",
      "Average test loss: 16482696.006824506\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001655491870103611\n",
      "Average test loss: 0.056079828694773216\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0016220132004883554\n",
      "Average test loss: 6669.0755777618915\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0016369726130117973\n",
      "Average test loss: 6383347.261700568\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0015995225760464867\n",
      "Average test loss: 1702995.3078842745\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0016215711723392208\n",
      "Average test loss: 381740.9549759767\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0016059789336803887\n",
      "Average test loss: 5480207.953202084\n",
      "Epoch 171/300\n",
      "Average training loss: 0.001611267314809892\n",
      "Average test loss: 0.0032900400617056425\n",
      "Epoch 172/300\n",
      "Average training loss: 0.001575262772747212\n",
      "Average test loss: 122.1836525780569\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0016279759988602665\n",
      "Average test loss: 14536.482105396752\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0015720793441351918\n",
      "Average test loss: 177516108.43414533\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0015832213608341084\n",
      "Average test loss: 11.377588265608065\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0016093860904996594\n",
      "Average test loss: 323.3579393759407\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0015728556843888429\n",
      "Average test loss: 14149.366810997182\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0016092394812860422\n",
      "Average test loss: 22.107855839192144\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0015685370513755415\n",
      "Average test loss: 2768.675485989943\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0015544080413464043\n",
      "Average test loss: 296583.975214175\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001608786896802485\n",
      "Average test loss: 597.8766067501244\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0015499190560852487\n",
      "Average test loss: 221.36905093491492\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0015524409442312188\n",
      "Average test loss: 9.201384002240065\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0015786660936557583\n",
      "Average test loss: 911.476193054675\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0016054165009409189\n",
      "Average test loss: 242.16990026327824\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0015358314230624172\n",
      "Average test loss: 24.387550442333023\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0015284192409987251\n",
      "Average test loss: 741695.8079534736\n",
      "Epoch 188/300\n",
      "Average training loss: 0.00151714259882768\n",
      "Average test loss: 192145.2499550679\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0015335605276955498\n",
      "Average test loss: 5709.731693437619\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0015404020808637141\n",
      "Average test loss: 1376681.732780183\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0015457205073907971\n",
      "Average test loss: 7029.920748248725\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001527702665577332\n",
      "Average test loss: 408.9710519439909\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0015369908114274344\n",
      "Average test loss: 28.18144618287848\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0015225497014406655\n",
      "Average test loss: 3534.4564360550394\n",
      "Epoch 195/300\n",
      "Average training loss: 0.001514130155245463\n",
      "Average test loss: 3555.3015921194283\n",
      "Epoch 196/300\n",
      "Average training loss: 0.001732244378887117\n",
      "Average test loss: 154.6039607922054\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0015138109216673507\n",
      "Average test loss: 59.19281190506535\n",
      "Epoch 198/300\n",
      "Average training loss: 0.001513721381003658\n",
      "Average test loss: 68.5486077320998\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0015125433268646398\n",
      "Average test loss: 4028.298583622488\n",
      "Epoch 200/300\n",
      "Average training loss: 0.001526143137882981\n",
      "Average test loss: 2476.4018696169787\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0015033280118885968\n",
      "Average test loss: 2.5641060721779034\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0015137223965591856\n",
      "Average test loss: 334.6639312453311\n",
      "Epoch 203/300\n",
      "Average training loss: 0.001491678096871409\n",
      "Average test loss: 110.5054503950783\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0015332982962330181\n",
      "Average test loss: 128.20677249806127\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0016393336732354429\n",
      "Average test loss: 24.257168099063552\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0015634562538729773\n",
      "Average test loss: 1295.0972709409095\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0015367868956592348\n",
      "Average test loss: 78.14087321581609\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0014774198123357362\n",
      "Average test loss: 3378.225401972203\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0015346751241013409\n",
      "Average test loss: 2758.3484245312593\n",
      "Epoch 210/300\n",
      "Average training loss: 0.001502837304957211\n",
      "Average test loss: 4029.0202274255894\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0014934752288584907\n",
      "Average test loss: 799.6688995552767\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0015535864165673653\n",
      "Average test loss: 13.2854504477063\n",
      "Epoch 213/300\n",
      "Average training loss: 0.001489378735733529\n",
      "Average test loss: 3497.0422517293296\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0014771594303763574\n",
      "Average test loss: 65.59627775679157\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0015116803134895033\n",
      "Average test loss: 0.3044344006718861\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0015070483232330946\n",
      "Average test loss: 0.13775261305769285\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0014934156641571057\n",
      "Average test loss: 154097.02894418532\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0014851337893762523\n",
      "Average test loss: 133.14567528546922\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0014478677255618903\n",
      "Average test loss: 56661.59749426418\n",
      "Epoch 220/300\n",
      "Average training loss: 0.001478338322084811\n",
      "Average test loss: 48802.50361733483\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0014833329325127933\n",
      "Average test loss: 105378.02576759928\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0014625057558425598\n",
      "Average test loss: 342.9124773225027\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0014522426839297016\n",
      "Average test loss: 1938548.1316769812\n",
      "Epoch 224/300\n",
      "Average training loss: 0.001527164581645694\n",
      "Average test loss: 0.3570133278897653\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0014703140935550133\n",
      "Average test loss: 153.67539142813865\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0014567156061530114\n",
      "Average test loss: 21799.48821980879\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001510091537816657\n",
      "Average test loss: 3.9731670663333496\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0014661065781902936\n",
      "Average test loss: 583.7210532881555\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0015027280715811584\n",
      "Average test loss: 27459.780521429173\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0014842314078576035\n",
      "Average test loss: 14.91661760358409\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0014824737774001228\n",
      "Average test loss: 35.11667495210272\n",
      "Epoch 232/300\n",
      "Average training loss: 0.001424804542420639\n",
      "Average test loss: 194.48679166773096\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0014531298162829545\n",
      "Average test loss: 225.85633772774455\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0014407896169771751\n",
      "Average test loss: 16.56094570774088\n",
      "Epoch 235/300\n",
      "Average training loss: 0.001505922151936425\n",
      "Average test loss: 2854.961458157031\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0014538902452008592\n",
      "Average test loss: 1021.785732882217\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0014094044256748425\n",
      "Average test loss: 2957.2552889095386\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0014995898791692324\n",
      "Average test loss: 1442.5992792256445\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0014480496602546837\n",
      "Average test loss: 8765.245740361208\n",
      "Epoch 240/300\n",
      "Average training loss: 0.001406238904222846\n",
      "Average test loss: 5.261154721936418\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0014277978189703492\n",
      "Average test loss: 3.747832818710659\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0014295597655905618\n",
      "Average test loss: 10.52558087385777\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0014501135289255115\n",
      "Average test loss: 361.3253121687025\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0014255016340563695\n",
      "Average test loss: 19.60723261434916\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0014406708714862664\n",
      "Average test loss: 185.29776907176137\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0014416254421489106\n",
      "Average test loss: 4237823.5305422675\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0014302351015309492\n",
      "Average test loss: 1.595523809592964\n",
      "Epoch 248/300\n",
      "Average training loss: 0.00145104691448311\n",
      "Average test loss: 321.86354335066426\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0014134207201293772\n",
      "Average test loss: 453.22794350158756\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0013964675415514243\n",
      "Average test loss: 33.18833267765213\n",
      "Epoch 251/300\n",
      "Average training loss: 0.001440229724885689\n",
      "Average test loss: 1722.5579069792802\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0014265731710733638\n",
      "Average test loss: 0.0022762202552209297\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0014108384407849775\n",
      "Average test loss: 16439.011885434138\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0014161064362981253\n",
      "Average test loss: 0.08063296698437383\n",
      "Epoch 255/300\n",
      "Average training loss: 0.001439774710788495\n",
      "Average test loss: 0.07839080826193094\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0014000226590368482\n",
      "Average test loss: 12.864327062775278\n",
      "Epoch 257/300\n",
      "Average training loss: 0.001415257069799635\n",
      "Average test loss: 7.943033827079253\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0014214749230692784\n",
      "Average test loss: 1886.9340865091685\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0014001702684909106\n",
      "Average test loss: 9.25619661805344\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0013872070433571934\n",
      "Average test loss: 0.7484948383304808\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0014152580238878728\n",
      "Average test loss: 2.9401757202220873\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0014040769800129863\n",
      "Average test loss: 469.00248654487854\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0013842305781112778\n",
      "Average test loss: 733205.1761941233\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0013795701253952251\n",
      "Average test loss: 591.9693197789317\n",
      "Epoch 265/300\n",
      "Average training loss: 0.001412907515755958\n",
      "Average test loss: 4098.571801043509\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0013929610521429116\n",
      "Average test loss: 12.00410229749067\n",
      "Epoch 267/300\n",
      "Average training loss: 0.001442582236809863\n",
      "Average test loss: 34.16585716438087\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0013881109363089005\n",
      "Average test loss: 271.89539775996406\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0013940295816088717\n",
      "Average test loss: 16009.076335754395\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0013892065823699036\n",
      "Average test loss: 58353.53315551758\n",
      "Epoch 271/300\n",
      "Average training loss: 0.001404603257878787\n",
      "Average test loss: 168.22451668233467\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0013744022408500314\n",
      "Average test loss: 73856.15141374589\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0013996107758333285\n",
      "Average test loss: 921031.8409635416\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0013848856464028358\n",
      "Average test loss: 41.22686912843047\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0013968895017686818\n",
      "Average test loss: 35732401.53072194\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0015191136153621806\n",
      "Average test loss: 18.539886849621517\n",
      "Epoch 277/300\n",
      "Average training loss: 0.001448109397975107\n",
      "Average test loss: 0.002901741066533658\n",
      "Epoch 278/300\n",
      "Average training loss: 0.001397537339478731\n",
      "Average test loss: 6.708250537418657\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0013967372812330723\n",
      "Average test loss: 5.588262839788881\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0013740450681394172\n",
      "Average test loss: 1.7032945104953316\n",
      "Epoch 281/300\n",
      "Average training loss: 0.001403212975917591\n",
      "Average test loss: 291.5814170013451\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0013755079116672278\n",
      "Average test loss: 3983.7635134120524\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0014292890089046624\n",
      "Average test loss: 521.8903953432962\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0014289591726329592\n",
      "Average test loss: 0.007677396382412149\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0014140875830004613\n",
      "Average test loss: 1.4134821502529085\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0013873810820902387\n",
      "Average test loss: 6.311599461893654\n",
      "Epoch 287/300\n",
      "Average training loss: 0.001409149642723302\n",
      "Average test loss: 18.72501883299152\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0013711782581069404\n",
      "Average test loss: 14.917789241483538\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0013590502113931709\n",
      "Average test loss: 0.4119409154061642\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0014061111466338236\n",
      "Average test loss: 0.02168380439012415\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0013988637565117744\n",
      "Average test loss: 0.8118204296802481\n",
      "Epoch 292/300\n",
      "Average training loss: 0.001356018722988665\n",
      "Average test loss: 13.414964810968273\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0013573388843279746\n",
      "Average test loss: 23.998670152562255\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0013473832894944483\n",
      "Average test loss: 119.08171422461348\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0013768369633083543\n",
      "Average test loss: 3413.91376060267\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0013809798391949798\n",
      "Average test loss: 59.96465731974691\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0013504730123612616\n",
      "Average test loss: 325.8082396855495\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0013821566140072214\n",
      "Average test loss: 773.1286239610132\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0013420814195026953\n",
      "Average test loss: 9.136622589135438\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0013604895021352504\n",
      "Average test loss: 0.775397027112647\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.5/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 4.31\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 3.66\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 1.25\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -1.57\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 0.00\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -0.93\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 0.33\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 3.95\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 10.15\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 12.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 17.06\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 8.16\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 17.46\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 20.26\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 21.94\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 22.88\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 13.36\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 21.82\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 23.36\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 24.55\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 19.26\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 24.10\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 22.37\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 24.74\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 25.66\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.20\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.60\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 7.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 7.77\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 5.76\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 5.85\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 8.83\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 10.01\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 10.57\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 12.51\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 16.38\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 10.78\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 19.28\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 21.72\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 22.05\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 13.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 23.23\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 24.05\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 16.18\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 23.59\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 22.56\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 25.97\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 21.22\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 25.31\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 25.05\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.42\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 0.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 1.42\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -2.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -3.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -3.07\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -1.48\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 0.90\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 5.51\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 11.56\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 17.61\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 16.11\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 22.09\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 22.09\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.38\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.14\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 14.46\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 21.41\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 23.89\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.00\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 21.76\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.21\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.81\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.89\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 5.88\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 1.87\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 1.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 3.00\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 5.95\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 10.31\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 12.84\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 17.17\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 19.86\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 17.36\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 23.45\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 22.60\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 14.30\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 18.39\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 23.52\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.33\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 25.90\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.72\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.56\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.15\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.56\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.74\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
