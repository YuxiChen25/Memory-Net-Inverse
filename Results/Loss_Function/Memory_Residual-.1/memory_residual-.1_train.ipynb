{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.1)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.034284711153970825\n",
      "Average test loss: 0.011314661231305864\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01264155189279053\n",
      "Average test loss: 0.010587815262377263\n",
      "Epoch 3/300\n",
      "Average training loss: 0.011298432192868657\n",
      "Average test loss: 0.009881245691743161\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01062683865096834\n",
      "Average test loss: 0.008677920708225833\n",
      "Epoch 5/300\n",
      "Average training loss: 0.010071788743966156\n",
      "Average test loss: 0.008914146616227097\n",
      "Epoch 6/300\n",
      "Average training loss: 0.009596516640649902\n",
      "Average test loss: 0.009041640475806263\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009181352467172676\n",
      "Average test loss: 0.0077993762161996626\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008886910840041107\n",
      "Average test loss: 0.007865392111655739\n",
      "Epoch 9/300\n",
      "Average training loss: 0.008644668077429135\n",
      "Average test loss: 0.007698972972730795\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008475303853551547\n",
      "Average test loss: 0.007613205102582773\n",
      "Epoch 11/300\n",
      "Average training loss: 0.008279255541662376\n",
      "Average test loss: 0.007274105584869782\n",
      "Epoch 12/300\n",
      "Average training loss: 0.008078804667211241\n",
      "Average test loss: 0.00719214702066448\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007926536093983385\n",
      "Average test loss: 0.007082949470728636\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0077943517602980135\n",
      "Average test loss: 0.007131533290363021\n",
      "Epoch 15/300\n",
      "Average training loss: 0.00765633192906777\n",
      "Average test loss: 0.007430406647423903\n",
      "Epoch 16/300\n",
      "Average training loss: 0.007539300109777185\n",
      "Average test loss: 0.0076088025495409965\n",
      "Epoch 17/300\n",
      "Average training loss: 0.007454331987847885\n",
      "Average test loss: 0.0069004375835259755\n",
      "Epoch 18/300\n",
      "Average training loss: 0.007334308596948782\n",
      "Average test loss: 0.006830336379508178\n",
      "Epoch 19/300\n",
      "Average training loss: 0.007219685338023636\n",
      "Average test loss: 0.006802564276589288\n",
      "Epoch 20/300\n",
      "Average training loss: 0.007149814889662796\n",
      "Average test loss: 0.006681443238837851\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0070836290775073895\n",
      "Average test loss: 0.006570154271192021\n",
      "Epoch 22/300\n",
      "Average training loss: 0.006985703878104686\n",
      "Average test loss: 0.006772293270048168\n",
      "Epoch 23/300\n",
      "Average training loss: 0.006930819210906824\n",
      "Average test loss: 0.006489669669005606\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00682694834139612\n",
      "Average test loss: 0.006561528717064195\n",
      "Epoch 25/300\n",
      "Average training loss: 0.006773582936575016\n",
      "Average test loss: 0.006506125446408987\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006718937879635228\n",
      "Average test loss: 0.006460649618258079\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006655567025972737\n",
      "Average test loss: 0.006413785818964243\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006576243803319004\n",
      "Average test loss: 0.006446306007189883\n",
      "Epoch 29/300\n",
      "Average training loss: 0.006532696687512928\n",
      "Average test loss: 0.0064679485861625936\n",
      "Epoch 30/300\n",
      "Average training loss: 0.006509640100929472\n",
      "Average test loss: 0.006892552647739649\n",
      "Epoch 31/300\n",
      "Average training loss: 0.006426642727520731\n",
      "Average test loss: 0.006592975143343211\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0063921496491465305\n",
      "Average test loss: 0.006393780891266134\n",
      "Epoch 33/300\n",
      "Average training loss: 0.006339500919398334\n",
      "Average test loss: 0.0064251558234294254\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0062972452330092585\n",
      "Average test loss: 0.006570718931655089\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0062578317063550154\n",
      "Average test loss: 0.006371684170431561\n",
      "Epoch 36/300\n",
      "Average training loss: 0.006225137030084928\n",
      "Average test loss: 0.006730029457559188\n",
      "Epoch 37/300\n",
      "Average training loss: 0.006173169885244635\n",
      "Average test loss: 0.006432995869881577\n",
      "Epoch 38/300\n",
      "Average training loss: 0.006147823388791746\n",
      "Average test loss: 0.00640577154358228\n",
      "Epoch 39/300\n",
      "Average training loss: 0.006115153378496567\n",
      "Average test loss: 0.006420029051601887\n",
      "Epoch 40/300\n",
      "Average training loss: 0.006062540365176068\n",
      "Average test loss: 0.006448529795226123\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006042720826549663\n",
      "Average test loss: 0.006272867979274855\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0060022688880562785\n",
      "Average test loss: 0.006337093590862221\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005998387203034428\n",
      "Average test loss: 0.006328948523849249\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005943207807424996\n",
      "Average test loss: 0.0064671695538693\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005915391721659237\n",
      "Average test loss: 0.009535809820724859\n",
      "Epoch 46/300\n",
      "Average training loss: 0.005900382509662045\n",
      "Average test loss: 0.006373396204577552\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005884579720389512\n",
      "Average test loss: 0.00648532376686732\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005856671563039224\n",
      "Average test loss: 0.006702595064209567\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0058050875941084495\n",
      "Average test loss: 0.006381446015503671\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005797069136881166\n",
      "Average test loss: 0.006576858946846591\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005776750388658709\n",
      "Average test loss: 0.006281764269702964\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005751067277044058\n",
      "Average test loss: 0.00636268554151886\n",
      "Epoch 53/300\n",
      "Average training loss: 0.006926008815152778\n",
      "Average test loss: 0.006397595095551676\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005886869656956858\n",
      "Average test loss: 0.006286467427594794\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0057206746799250445\n",
      "Average test loss: 0.006318646649519603\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0056864129201405576\n",
      "Average test loss: 0.0065161048596103985\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005672003082517121\n",
      "Average test loss: 0.006342829038699468\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005652338590265976\n",
      "Average test loss: 0.006596209485290779\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005637954665140973\n",
      "Average test loss: 0.006469382068763177\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0056398792419996525\n",
      "Average test loss: 0.0063869617610341975\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005623931798256106\n",
      "Average test loss: 0.006336678853879373\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005617112175871929\n",
      "Average test loss: 0.006383346445858478\n",
      "Epoch 63/300\n",
      "Average training loss: 0.00559813775618871\n",
      "Average test loss: 0.006343427598062489\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005577411850707399\n",
      "Average test loss: 0.006298100638720724\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005571056830386321\n",
      "Average test loss: 0.010296062587036027\n",
      "Epoch 66/300\n",
      "Average training loss: 0.005568929477284352\n",
      "Average test loss: 0.006358454254766306\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005519041374739673\n",
      "Average test loss: 0.006315191817780336\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005495941201431884\n",
      "Average test loss: 0.006401040169927809\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005511568607969417\n",
      "Average test loss: 0.00630339696051346\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005482023492869403\n",
      "Average test loss: 0.006319814080993334\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005460483154488934\n",
      "Average test loss: 0.006407115973946121\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005437353862242566\n",
      "Average test loss: 0.006511147387325764\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005446128246270948\n",
      "Average test loss: 0.006379810171822707\n",
      "Epoch 74/300\n",
      "Average training loss: 0.005413700405094359\n",
      "Average test loss: 0.006325437590893772\n",
      "Epoch 75/300\n",
      "Average training loss: 0.005405166282008092\n",
      "Average test loss: 0.0063095597670310075\n",
      "Epoch 76/300\n",
      "Average training loss: 0.005400086967067586\n",
      "Average test loss: 0.006270033475425508\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0054018512777984145\n",
      "Average test loss: 0.006326225329190493\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005367216503040658\n",
      "Average test loss: 0.006363158286031749\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00549846113100648\n",
      "Average test loss: 0.006373870412094726\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0053230930316365424\n",
      "Average test loss: 0.006473549734387133\n",
      "Epoch 81/300\n",
      "Average training loss: 0.005322433346261581\n",
      "Average test loss: 0.006420786565376653\n",
      "Epoch 82/300\n",
      "Average training loss: 0.005318463595584035\n",
      "Average test loss: 0.006658023917012744\n",
      "Epoch 83/300\n",
      "Average training loss: 0.005317563405881325\n",
      "Average test loss: 0.006390344600296683\n",
      "Epoch 84/300\n",
      "Average training loss: 0.005365294547130664\n",
      "Average test loss: 0.006328962117433548\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005288125396602683\n",
      "Average test loss: 0.006308323166436619\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0052769485099448095\n",
      "Average test loss: 0.006313040606677532\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0052623265936142865\n",
      "Average test loss: 0.00640283291000459\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00525995960417721\n",
      "Average test loss: 0.006696033981939157\n",
      "Epoch 89/300\n",
      "Average training loss: 0.005262429335465034\n",
      "Average test loss: 0.006564334974106815\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005236692499783304\n",
      "Average test loss: 0.006341284121076266\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005232806907966733\n",
      "Average test loss: 0.006508575493676795\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0052177017695373955\n",
      "Average test loss: 0.00634840979344315\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005207343374896381\n",
      "Average test loss: 0.006422670145001677\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005201085601829821\n",
      "Average test loss: 0.006341180238458846\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0051910585458907815\n",
      "Average test loss: 0.006469012180964152\n",
      "Epoch 96/300\n",
      "Average training loss: 0.005181862271080415\n",
      "Average test loss: 0.006317418705672026\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005178786642021603\n",
      "Average test loss: 0.006373444558845626\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0051689475542969175\n",
      "Average test loss: 0.006429308309737179\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0051685053416424325\n",
      "Average test loss: 0.006416779095100032\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005165384124343594\n",
      "Average test loss: 0.00643869938991136\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005142388759594825\n",
      "Average test loss: 0.006352637437482675\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005132380259533723\n",
      "Average test loss: 0.006385803019007047\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005129392323601577\n",
      "Average test loss: 0.006410261181907522\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005118635386642483\n",
      "Average test loss: 0.006293725039396021\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005109392472025421\n",
      "Average test loss: 0.006301633721424474\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005097543576939238\n",
      "Average test loss: 0.006599230512976647\n",
      "Epoch 107/300\n",
      "Average training loss: 0.005093731858250168\n",
      "Average test loss: 0.006499640168415175\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00509162389445636\n",
      "Average test loss: 0.006475833586106697\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005082782099230422\n",
      "Average test loss: 0.006371324142648114\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005071337528112862\n",
      "Average test loss: 0.006344598479155037\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005067049594802989\n",
      "Average test loss: 0.006381367955356836\n",
      "Epoch 112/300\n",
      "Average training loss: 0.005068625110305018\n",
      "Average test loss: 0.006685659482661221\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005088273047573037\n",
      "Average test loss: 0.006425340495175786\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005024737172035707\n",
      "Average test loss: 0.006740535702970293\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005028955293612348\n",
      "Average test loss: 0.006402210673110352\n",
      "Epoch 116/300\n",
      "Average training loss: 0.005036346634022064\n",
      "Average test loss: 0.006469707244386276\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005029960139965018\n",
      "Average test loss: 0.0065366750810709265\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00501385762459702\n",
      "Average test loss: 0.0064937105394072\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005004271951400571\n",
      "Average test loss: 0.00646909086075094\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005001454728758997\n",
      "Average test loss: 0.006428797490480873\n",
      "Epoch 121/300\n",
      "Average training loss: 0.005002802268291513\n",
      "Average test loss: 0.006373242436183824\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00499843990844157\n",
      "Average test loss: 0.006479689458178149\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004990724582225084\n",
      "Average test loss: 0.006382522637231483\n",
      "Epoch 124/300\n",
      "Average training loss: 0.004972959289120303\n",
      "Average test loss: 0.0064101074685653054\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004978705119341612\n",
      "Average test loss: 0.006406040063748757\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004962014057156112\n",
      "Average test loss: 0.006436027297129234\n",
      "Epoch 127/300\n",
      "Average training loss: 0.005024145895822181\n",
      "Average test loss: 0.006485666952199406\n",
      "Epoch 128/300\n",
      "Average training loss: 0.004947256747633219\n",
      "Average test loss: 0.006456596697370211\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004936192636688551\n",
      "Average test loss: 0.006412150527454085\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004956177767780092\n",
      "Average test loss: 0.006331105971088012\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004952057314415773\n",
      "Average test loss: 0.006407167060093747\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004920048681398233\n",
      "Average test loss: 0.006360973323384921\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004918299841798014\n",
      "Average test loss: 0.006477890607797437\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004939899569584264\n",
      "Average test loss: 0.0063341656691498225\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004920197819670041\n",
      "Average test loss: 0.006501310402320491\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0049044124430252444\n",
      "Average test loss: 0.0067051959629688\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00491308737711774\n",
      "Average test loss: 0.006549768922229608\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004914551655451457\n",
      "Average test loss: 0.0063831757410532895\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004896072545399269\n",
      "Average test loss: 0.006462381731718779\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004900437907626232\n",
      "Average test loss: 0.006527634563545386\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00489293836346931\n",
      "Average test loss: 0.006751857428914971\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004899095052646266\n",
      "Average test loss: 0.006465419789983167\n",
      "Epoch 143/300\n",
      "Average training loss: 0.004881017736676666\n",
      "Average test loss: 0.006439411267224285\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004879433151748445\n",
      "Average test loss: 0.0065915162964827485\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004900533075133959\n",
      "Average test loss: 0.006646959088742733\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0049598145741555424\n",
      "Average test loss: 0.00661233341983623\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004839621129135291\n",
      "Average test loss: 0.0063033192770348655\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004849296342995432\n",
      "Average test loss: 0.006490199620111121\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004945971669422256\n",
      "Average test loss: 0.00651378030785256\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004835597254749802\n",
      "Average test loss: 0.0062998708130584825\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004837527245283127\n",
      "Average test loss: 0.006540637751006418\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004828476244376765\n",
      "Average test loss: 0.006446502286940813\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004815526091390186\n",
      "Average test loss: 0.006764773192918963\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004825280232148038\n",
      "Average test loss: 0.006492885702600082\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004826473984039492\n",
      "Average test loss: 0.006427123398416572\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004811926242791944\n",
      "Average test loss: 0.006602838607711925\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004840735199136866\n",
      "Average test loss: 0.006474535388665067\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004816084124147892\n",
      "Average test loss: 0.006455338968998856\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004798669235573875\n",
      "Average test loss: 0.006486343852761719\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004891251621146997\n",
      "Average test loss: 0.006574574627396133\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004805794708430767\n",
      "Average test loss: 0.0071374968368974\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0047885366053216985\n",
      "Average test loss: 0.00646876180337535\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004783672452386882\n",
      "Average test loss: 0.006445422013600668\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004793752920089497\n",
      "Average test loss: 0.00665758526035481\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004788824506931835\n",
      "Average test loss: 0.006338895361042685\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004757232062932518\n",
      "Average test loss: 0.006619256135904127\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004780365352829297\n",
      "Average test loss: 0.006540658728943931\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004774126530107525\n",
      "Average test loss: 0.006563914399180147\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004769672635528776\n",
      "Average test loss: 0.00654930836872922\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0047649634501172435\n",
      "Average test loss: 0.006471471048477623\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004763703210072385\n",
      "Average test loss: 0.006480166217519177\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004768320079065032\n",
      "Average test loss: 0.0065851588735563885\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004774684341417418\n",
      "Average test loss: 0.006613842823853095\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004752954540774226\n",
      "Average test loss: 0.00658430377787186\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004744775018758244\n",
      "Average test loss: 0.006535358433094289\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004742057938956552\n",
      "Average test loss: 0.007287315998640326\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004724626712501049\n",
      "Average test loss: 0.0066859652615255775\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004725243261290921\n",
      "Average test loss: 0.00659933658875525\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0047298174690869116\n",
      "Average test loss: 0.006380020211968157\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004734797998848889\n",
      "Average test loss: 0.006570825340433253\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00472772003379133\n",
      "Average test loss: 0.006529646676447657\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004714005525327391\n",
      "Average test loss: 0.0067100980105913345\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004761550065957838\n",
      "Average test loss: 0.0064243853568202915\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0047074569716221756\n",
      "Average test loss: 0.006689561890231239\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00470629378573762\n",
      "Average test loss: 0.006641006820731693\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0047024371686081095\n",
      "Average test loss: 0.006535264513972733\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004691338684823778\n",
      "Average test loss: 0.0065199022649063\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004704519607540634\n",
      "Average test loss: 0.0064512785793178614\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004695592770145999\n",
      "Average test loss: 0.0066188929614921415\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004703687361131112\n",
      "Average test loss: 0.006473221271816227\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004687908317893744\n",
      "Average test loss: 0.006456259401308165\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004680429345617691\n",
      "Average test loss: 0.006559418066922161\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004681178526331981\n",
      "Average test loss: 0.006565721927417649\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004674041325847308\n",
      "Average test loss: 0.006613671761833959\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004665756655650007\n",
      "Average test loss: 0.006474510613414976\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004667945166842805\n",
      "Average test loss: 0.006589732013228867\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004669715045640866\n",
      "Average test loss: 0.006539353474560711\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0046772782906062075\n",
      "Average test loss: 0.0066118218973279\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004661037026801043\n",
      "Average test loss: 0.006624303544561068\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0046665967694587175\n",
      "Average test loss: 0.0065415721999274356\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00466577239582936\n",
      "Average test loss: 0.0065009879879653456\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004655388756344716\n",
      "Average test loss: 0.006417712257968055\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0046476164998279675\n",
      "Average test loss: 0.006559390960054265\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004642930524216758\n",
      "Average test loss: 0.006727894221742948\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004646561953963505\n",
      "Average test loss: 0.006547651323179404\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004776009168475866\n",
      "Average test loss: 0.006440808771385087\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004627240550186899\n",
      "Average test loss: 0.006516536568601926\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004628648562149869\n",
      "Average test loss: 0.006471268277201387\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0046288196411397725\n",
      "Average test loss: 0.006479450156291326\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004626519976390733\n",
      "Average test loss: 0.006611936465733581\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004621773910191324\n",
      "Average test loss: 0.006649854350835085\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004621407384466794\n",
      "Average test loss: 0.006562103408078353\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00462867017587026\n",
      "Average test loss: 0.006712552461359236\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004638009082112047\n",
      "Average test loss: 0.006499642133298848\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004623452939300074\n",
      "Average test loss: 0.006691408667713404\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004623229623668724\n",
      "Average test loss: 0.00690421555398239\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004603523650103145\n",
      "Average test loss: 0.00657279459759593\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004600585995035039\n",
      "Average test loss: 0.00656339305639267\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00461196997265021\n",
      "Average test loss: 0.006504242348588175\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004606093329480952\n",
      "Average test loss: 0.006636430240753624\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00459511612314317\n",
      "Average test loss: 0.006541248107535972\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00459356086783939\n",
      "Average test loss: 0.00677300501594113\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004594485653150413\n",
      "Average test loss: 0.006576454977608389\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006442933276295662\n",
      "Average test loss: 0.0062694407838086285\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0058385182470083235\n",
      "Average test loss: 0.006314478708224164\n",
      "Epoch 226/300\n",
      "Average training loss: 0.005089738318903579\n",
      "Average test loss: 0.006394650233288606\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0047076507454944985\n",
      "Average test loss: 0.006538036053793298\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004562479556020763\n",
      "Average test loss: 0.006687043818748659\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004515901890065935\n",
      "Average test loss: 0.006521803446941906\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004517782659166389\n",
      "Average test loss: 0.0064542874184747535\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004537757558334205\n",
      "Average test loss: 0.006566947755300336\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004561279541916317\n",
      "Average test loss: 0.006599517673254013\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0045660959916810195\n",
      "Average test loss: 0.006549163707428508\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004564474209729168\n",
      "Average test loss: 0.006526824676742157\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00458723539320959\n",
      "Average test loss: 0.0064929975498881605\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004564253160109123\n",
      "Average test loss: 0.006548412882619434\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0045898476077450645\n",
      "Average test loss: 0.0065575688601368\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004578648939728737\n",
      "Average test loss: 0.006512845971518092\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004568791671345631\n",
      "Average test loss: 0.006568506587296724\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0045720309072898495\n",
      "Average test loss: 0.006548119558228387\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004561899126610822\n",
      "Average test loss: 0.006598191396229797\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004562185508302516\n",
      "Average test loss: 0.006538074422627688\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004554267137000958\n",
      "Average test loss: 0.006864836375746462\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004563821921332015\n",
      "Average test loss: 0.006522794803811444\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004545730439325174\n",
      "Average test loss: 0.0067266111444267964\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004559085464518931\n",
      "Average test loss: 0.006517240949389007\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0045522925626072614\n",
      "Average test loss: 0.006622333835396501\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004551747321668598\n",
      "Average test loss: 0.006579876394321521\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004543409751314256\n",
      "Average test loss: 0.0065402677895294295\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004532817804151111\n",
      "Average test loss: 0.006684502450542317\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004556691448100739\n",
      "Average test loss: 0.0066169313728395435\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004610548206501537\n",
      "Average test loss: 0.0066477623536354965\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004534123505983088\n",
      "Average test loss: 0.006615560199651453\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00452354353459345\n",
      "Average test loss: 0.006516386214229795\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004531248317410548\n",
      "Average test loss: 0.006607877167148723\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004529113894535436\n",
      "Average test loss: 0.006456080065833198\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0045287385512557295\n",
      "Average test loss: 0.007231163945049047\n",
      "Epoch 258/300\n",
      "Average training loss: 0.004527006051399642\n",
      "Average test loss: 0.0067357804460657965\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004506953220193585\n",
      "Average test loss: 0.006692203597476085\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004529101825836632\n",
      "Average test loss: 0.006618037939071655\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004514916095054812\n",
      "Average test loss: 0.007241234168410301\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004511063551737203\n",
      "Average test loss: 0.0065941709321406155\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0045124914286037284\n",
      "Average test loss: 0.006576622681485282\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004513938740723663\n",
      "Average test loss: 0.006687658385684093\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004520299049715201\n",
      "Average test loss: 0.006592977869427866\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004509773382296165\n",
      "Average test loss: 0.006685029324144125\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004505369483596749\n",
      "Average test loss: 0.006724766359561019\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004513244828002321\n",
      "Average test loss: 0.006620505253473918\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004510708796481291\n",
      "Average test loss: 0.006498414213458697\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004488286305632856\n",
      "Average test loss: 0.006574705570108361\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004496828975776831\n",
      "Average test loss: 0.8938640130360921\n",
      "Epoch 272/300\n",
      "Average training loss: 0.009972585149937206\n",
      "Average test loss: 0.006577377836530407\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00680465351148612\n",
      "Average test loss: 0.006453187793493271\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006244332021723191\n",
      "Average test loss: 0.006227224383917119\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0056812611108438835\n",
      "Average test loss: 0.006448109680579768\n",
      "Epoch 276/300\n",
      "Average training loss: 0.005149304984344376\n",
      "Average test loss: 0.006511088480552037\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004776542606867022\n",
      "Average test loss: 0.006825836575279633\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004576216836356455\n",
      "Average test loss: 0.006474164550089174\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004488622520325912\n",
      "Average test loss: 0.006743518453505304\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004449981708493498\n",
      "Average test loss: 0.006472034423508578\n",
      "Epoch 281/300\n",
      "Average training loss: 0.004440042805754476\n",
      "Average test loss: 0.0065514183545278175\n",
      "Epoch 282/300\n",
      "Average training loss: 0.004560664953043064\n",
      "Average test loss: 0.006583651090661685\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004451329642285904\n",
      "Average test loss: 0.006624777490894\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0044537269377874006\n",
      "Average test loss: 0.006521684889164236\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004472227125532097\n",
      "Average test loss: 0.006546419961584939\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004484677644653452\n",
      "Average test loss: 0.006522522615889708\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0044903136843608485\n",
      "Average test loss: 0.006625464371095101\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004487753174164229\n",
      "Average test loss: 0.006792510759913259\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004482667305196325\n",
      "Average test loss: 0.0065896104383799765\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004470339909609821\n",
      "Average test loss: 0.02757808645400736\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004569475030733479\n",
      "Average test loss: 0.006684462901618745\n",
      "Epoch 292/300\n",
      "Average training loss: 0.004476178300877412\n",
      "Average test loss: 0.006600697153558334\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004468034571657578\n",
      "Average test loss: 0.006833893927849001\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0044647744426296815\n",
      "Average test loss: 0.00654566587963038\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004483180372044444\n",
      "Average test loss: 0.006523526356865962\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004467152249068022\n",
      "Average test loss: 0.006766576833195157\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004464226134535339\n",
      "Average test loss: 0.006734835659050279\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004468554817554023\n",
      "Average test loss: 0.0065436181293593515\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004467336024261183\n",
      "Average test loss: 0.006603592483947675\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004464016822477182\n",
      "Average test loss: 0.006656117874301142\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02870218947943714\n",
      "Average test loss: 0.008857949544158247\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008877533577382565\n",
      "Average test loss: 0.008428116598063045\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007821306835446093\n",
      "Average test loss: 0.006631213512685564\n",
      "Epoch 4/300\n",
      "Average training loss: 0.007172407913953066\n",
      "Average test loss: 0.006311149864147107\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006779798141370217\n",
      "Average test loss: 0.005552323903888464\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006459880172378487\n",
      "Average test loss: 0.005550529034187396\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006110363495846589\n",
      "Average test loss: 0.0070660764550169305\n",
      "Epoch 8/300\n",
      "Average training loss: 0.00585963179419438\n",
      "Average test loss: 0.005112590818355481\n",
      "Epoch 9/300\n",
      "Average training loss: 0.00570061042946246\n",
      "Average test loss: 0.004988325028369824\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005461700555764967\n",
      "Average test loss: 0.004805706951767207\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005295613719771305\n",
      "Average test loss: 0.004645149005370008\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005137138656857941\n",
      "Average test loss: 0.004717879579299026\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0049941131091780135\n",
      "Average test loss: 0.004493634563146366\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004895439174854093\n",
      "Average test loss: 0.004415752607087294\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004781822576704953\n",
      "Average test loss: 0.004336723589234882\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00469707204008268\n",
      "Average test loss: 0.0043483343045744634\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00460963272717264\n",
      "Average test loss: 0.004305540544084377\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004522890977975395\n",
      "Average test loss: 0.004178429389579429\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004461100474413898\n",
      "Average test loss: 0.0049839519984606236\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004378816293345558\n",
      "Average test loss: 0.004089608888659212\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004316929718479514\n",
      "Average test loss: 0.004074063354068332\n",
      "Epoch 22/300\n",
      "Average training loss: 0.00427546847735842\n",
      "Average test loss: 0.004011346996658378\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00421890380150742\n",
      "Average test loss: 0.004021392250433564\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004149259067244\n",
      "Average test loss: 0.0039247338751124015\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004094657474921809\n",
      "Average test loss: 0.004050554028815693\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00406709936592314\n",
      "Average test loss: 0.0038612462433262004\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004015639318981104\n",
      "Average test loss: 0.0039053380630082552\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003976707344667779\n",
      "Average test loss: 0.00399639802157051\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0039371273031251295\n",
      "Average test loss: 0.0038831144478172065\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0039073296562872\n",
      "Average test loss: 0.003821667136210534\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0038635271206084224\n",
      "Average test loss: 0.0037567726767932376\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0038471917539007134\n",
      "Average test loss: 0.0037771521591477924\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0038059549391683605\n",
      "Average test loss: 0.0037911014101571507\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0037805244336939522\n",
      "Average test loss: 0.004057544395327568\n",
      "Epoch 35/300\n",
      "Average training loss: 0.003772029708036118\n",
      "Average test loss: 0.003767759070214298\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003739381793058581\n",
      "Average test loss: 0.004017513513358103\n",
      "Epoch 37/300\n",
      "Average training loss: 0.003715675663616922\n",
      "Average test loss: 0.003739188123287426\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003682574565211932\n",
      "Average test loss: 0.003808781471413871\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0036682940675980516\n",
      "Average test loss: 0.0037594133836941586\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0036483691220896113\n",
      "Average test loss: 0.0040230564282586175\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0036332524718923702\n",
      "Average test loss: 0.0037442541101740468\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0036103621801982324\n",
      "Average test loss: 0.003684455770999193\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0035899833587722647\n",
      "Average test loss: 0.0037095156659682593\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003580076208959023\n",
      "Average test loss: 0.003719503647544318\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00355864343761156\n",
      "Average test loss: 0.00376181346964505\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003551780033028788\n",
      "Average test loss: 0.003672452270156807\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0035564010391632714\n",
      "Average test loss: 0.003674144874430365\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003519143746130996\n",
      "Average test loss: 0.0038601006844805347\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0035083163703481354\n",
      "Average test loss: 0.003691647059801552\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0035009422014570897\n",
      "Average test loss: 0.0037192894677735037\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0035266163013875484\n",
      "Average test loss: 0.003677287077738179\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0035131134810960957\n",
      "Average test loss: 0.003650936386237542\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0034503726137595043\n",
      "Average test loss: 0.0036943479308651555\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003435607190968262\n",
      "Average test loss: 0.003765948311322265\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0034251986911727323\n",
      "Average test loss: 0.0037455821209069757\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0034359268684768015\n",
      "Average test loss: 0.00418816661917501\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0034239565703190035\n",
      "Average test loss: 0.003782904508627123\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003395218274038699\n",
      "Average test loss: 0.0037073002567307818\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003386314818221662\n",
      "Average test loss: 0.003716341498825285\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003385383455496695\n",
      "Average test loss: 0.003829842825730642\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0033694701043681967\n",
      "Average test loss: 0.003624028681880898\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0033688938969539273\n",
      "Average test loss: 0.003668062922027376\n",
      "Epoch 63/300\n",
      "Average training loss: 0.003661868564163645\n",
      "Average test loss: 0.003722002547027336\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0033411653822080956\n",
      "Average test loss: 0.003760640827524993\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0033067484067545996\n",
      "Average test loss: 0.003765760981374317\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0033330412335279913\n",
      "Average test loss: 0.0037581897547675504\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00332614001362688\n",
      "Average test loss: 0.0037122627960311043\n",
      "Epoch 68/300\n",
      "Average training loss: 0.003323117859247658\n",
      "Average test loss: 0.0036661330945789816\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0033084418955776425\n",
      "Average test loss: 0.004187024624397358\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0032963361189597183\n",
      "Average test loss: 0.0037590547564129036\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0032963765905135206\n",
      "Average test loss: 0.003641205844365888\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003294877281412482\n",
      "Average test loss: 0.0036418492131763034\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003276963667323192\n",
      "Average test loss: 0.003956469716297256\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0032736388345559436\n",
      "Average test loss: 0.0037316219707330066\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0032673186872982317\n",
      "Average test loss: 0.003664330909028649\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00329924711274604\n",
      "Average test loss: 0.0036493313355992238\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0032440012300180066\n",
      "Average test loss: 0.003707144943790303\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0032450267546292808\n",
      "Average test loss: 0.003651739002308912\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0032495610252436663\n",
      "Average test loss: 0.0037374274370570977\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0032448617892546785\n",
      "Average test loss: 0.0037968929786649014\n",
      "Epoch 81/300\n",
      "Average training loss: 0.003248745321813557\n",
      "Average test loss: 0.0038001413005921574\n",
      "Epoch 82/300\n",
      "Average training loss: 0.003214571028119988\n",
      "Average test loss: 0.0036939049292769696\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003220433656540182\n",
      "Average test loss: 0.003691503430199292\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0032080964069399567\n",
      "Average test loss: 0.0036823169990546174\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0032027118456446463\n",
      "Average test loss: 0.0037798759879337416\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003205076230275962\n",
      "Average test loss: 0.003759367294609547\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0031970790088590648\n",
      "Average test loss: 0.003708433207952314\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0031891012088292175\n",
      "Average test loss: 0.0037688500040935144\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003188395028313001\n",
      "Average test loss: 0.0037336615746219955\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0031804780883507595\n",
      "Average test loss: 0.003803847466491991\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0031842832397669554\n",
      "Average test loss: 0.003678498985659745\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0031656374376681116\n",
      "Average test loss: 0.0036873646560642453\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0031780320583946176\n",
      "Average test loss: 0.0037642321002980075\n",
      "Epoch 94/300\n",
      "Average training loss: 0.003179797331078185\n",
      "Average test loss: 0.004111986881328954\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0031579350464873844\n",
      "Average test loss: 0.0037216495556963813\n",
      "Epoch 96/300\n",
      "Average training loss: 0.003157969975223144\n",
      "Average test loss: 0.0036945032293183937\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0031385580137785938\n",
      "Average test loss: 0.0036390500730938383\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0031477221093244023\n",
      "Average test loss: 0.0036408226299617027\n",
      "Epoch 99/300\n",
      "Average training loss: 0.003139301005130013\n",
      "Average test loss: 0.0036732847413255107\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0031291946284472943\n",
      "Average test loss: 0.0036955253817141055\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0031318480883621505\n",
      "Average test loss: 0.003803110092671381\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0031258398942235442\n",
      "Average test loss: 0.003775611027247376\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0031245004412614637\n",
      "Average test loss: 0.003707251524345742\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0031189205708603064\n",
      "Average test loss: 0.0036927020059277613\n",
      "Epoch 105/300\n",
      "Average training loss: 0.003111715067178011\n",
      "Average test loss: 0.0037396050503270494\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0031006190106272697\n",
      "Average test loss: 0.003666130387948619\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003095374830481079\n",
      "Average test loss: 0.003720274575468567\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0030912778327862422\n",
      "Average test loss: 0.0036712670458687677\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00310509882391327\n",
      "Average test loss: 0.003725327152758837\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003089449862225188\n",
      "Average test loss: 0.003666902095079422\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0030934367732455333\n",
      "Average test loss: 0.0037230175628016394\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0030792625300172303\n",
      "Average test loss: 0.0038480836744937633\n",
      "Epoch 113/300\n",
      "Average training loss: 0.003090391636929578\n",
      "Average test loss: 0.003755377551747693\n",
      "Epoch 114/300\n",
      "Average training loss: 0.003067435123854213\n",
      "Average test loss: 0.00372928618805276\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0030868515190151003\n",
      "Average test loss: 0.0037210867839554945\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0030743366724087133\n",
      "Average test loss: 0.0037026222644166813\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003069443275531133\n",
      "Average test loss: 0.0038218919444415306\n",
      "Epoch 118/300\n",
      "Average training loss: 0.003054064588000377\n",
      "Average test loss: 0.0037368586481445368\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0030520084345092378\n",
      "Average test loss: 0.003782067282125354\n",
      "Epoch 120/300\n",
      "Average training loss: 0.003072793669584725\n",
      "Average test loss: 0.0037204905566242006\n",
      "Epoch 121/300\n",
      "Average training loss: 0.003057954291916556\n",
      "Average test loss: 0.0038482708463238344\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0030498756644212538\n",
      "Average test loss: 0.0038795792601174777\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0030395295891082947\n",
      "Average test loss: 0.003814867607007424\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0030311340430958403\n",
      "Average test loss: 0.003690559604929553\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0030367614146735934\n",
      "Average test loss: 0.003663891332017051\n",
      "Epoch 126/300\n",
      "Average training loss: 0.003044783665281203\n",
      "Average test loss: 0.0037075691711571483\n",
      "Epoch 127/300\n",
      "Average training loss: 0.003039621502988868\n",
      "Average test loss: 0.003863961131622394\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003032943893224001\n",
      "Average test loss: 0.003724091242791878\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00301744140084419\n",
      "Average test loss: 0.003722514210268855\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0030287912525236607\n",
      "Average test loss: 0.0037289226224852933\n",
      "Epoch 131/300\n",
      "Average training loss: 0.003012560546191202\n",
      "Average test loss: 0.0037579397153523235\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0030136544923815463\n",
      "Average test loss: 0.0038464311038454374\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0030163153008454374\n",
      "Average test loss: 0.00375549634007944\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0030130642919490736\n",
      "Average test loss: 0.003764372914822565\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0030058157081819243\n",
      "Average test loss: 0.00370552762846152\n",
      "Epoch 136/300\n",
      "Average training loss: 0.003003508312627673\n",
      "Average test loss: 0.003967515097310146\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0030011690753615566\n",
      "Average test loss: 0.003711230424957143\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0029899927408744893\n",
      "Average test loss: 0.0037134609027869173\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0030036518985612523\n",
      "Average test loss: 0.0038978906981647016\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0029874611395514675\n",
      "Average test loss: 0.004387928009861045\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0029813137486990955\n",
      "Average test loss: 0.0038110613019929993\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002989445629219214\n",
      "Average test loss: 5.368434075249566\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0038600764905826913\n",
      "Average test loss: 0.003654017524379823\n",
      "Epoch 144/300\n",
      "Average training loss: 0.003000355858562721\n",
      "Average test loss: 0.0036674690002368557\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002939074483389656\n",
      "Average test loss: 0.0037853044824053845\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0029401529925978847\n",
      "Average test loss: 0.003678614432613055\n",
      "Epoch 147/300\n",
      "Average training loss: 0.002947058082661695\n",
      "Average test loss: 0.003727417219637169\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0029604048838631973\n",
      "Average test loss: 0.0037180728088650437\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0029643834196031093\n",
      "Average test loss: 0.0037710906219565205\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002960003570343057\n",
      "Average test loss: 0.0038290140471524663\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0029784043091866707\n",
      "Average test loss: 0.0037142685529672438\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0029590804572734567\n",
      "Average test loss: 0.003681006266425053\n",
      "Epoch 153/300\n",
      "Average training loss: 0.002964771755867534\n",
      "Average test loss: 0.0037991383170915974\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0029535100869834425\n",
      "Average test loss: 0.00385987606479062\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0029616603089703455\n",
      "Average test loss: 0.003737684120734533\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002956660638252894\n",
      "Average test loss: 0.0039045522262652716\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0029646987627363866\n",
      "Average test loss: 0.0037169303666386337\n",
      "Epoch 158/300\n",
      "Average training loss: 0.002945201038900349\n",
      "Average test loss: 0.0037139874667757086\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0029526602251248226\n",
      "Average test loss: 0.003700435085636046\n",
      "Epoch 160/300\n",
      "Average training loss: 0.002949162717908621\n",
      "Average test loss: 0.0037668713873459233\n",
      "Epoch 161/300\n",
      "Average training loss: 0.002964028971683648\n",
      "Average test loss: 0.00378004096262157\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0029298442993313076\n",
      "Average test loss: 0.0036781150814559726\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002935469147645765\n",
      "Average test loss: 0.003702084076901277\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002936870744037959\n",
      "Average test loss: 0.0037742426076696977\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0029366761973748604\n",
      "Average test loss: 0.0038591248244047166\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0029356702713088855\n",
      "Average test loss: 0.003975642684433196\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0029329613604479367\n",
      "Average test loss: 0.0037825018744915722\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0029183561605297856\n",
      "Average test loss: 0.0037601521605004867\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002936414403013057\n",
      "Average test loss: 0.0038710127061025966\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0029245456084609034\n",
      "Average test loss: 0.0037226809333595966\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002922175544831488\n",
      "Average test loss: 0.0037171105383377936\n",
      "Epoch 172/300\n",
      "Average training loss: 0.002921260770410299\n",
      "Average test loss: 0.0038798236230181324\n",
      "Epoch 173/300\n",
      "Average training loss: 0.002912768996424145\n",
      "Average test loss: 0.0038034259014659458\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0029194101308368976\n",
      "Average test loss: 0.0038090570755302906\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0029184657219383453\n",
      "Average test loss: 0.0037941141174071364\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002905219035430087\n",
      "Average test loss: 0.0037939342961957057\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0029034062458409204\n",
      "Average test loss: 0.003792309754424625\n",
      "Epoch 178/300\n",
      "Average training loss: 0.002910787107009027\n",
      "Average test loss: 0.0038242039310021534\n",
      "Epoch 179/300\n",
      "Average training loss: 0.002918608185938663\n",
      "Average test loss: 0.003824884719318814\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002903508569838272\n",
      "Average test loss: 0.003756910792241494\n",
      "Epoch 181/300\n",
      "Average training loss: 0.002898162082251575\n",
      "Average test loss: 0.003929499080197679\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0028930893809431127\n",
      "Average test loss: 0.0037164831726501386\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0029092039465904236\n",
      "Average test loss: 0.003818029513375627\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002891388957388699\n",
      "Average test loss: 0.0038738500508997175\n",
      "Epoch 185/300\n",
      "Average training loss: 0.002899602122604847\n",
      "Average test loss: 0.003791627276274893\n",
      "Epoch 186/300\n",
      "Average training loss: 0.002889339170936081\n",
      "Average test loss: 0.003944239268907242\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0028868971976141137\n",
      "Average test loss: 0.0038041316245992977\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0028849880983018213\n",
      "Average test loss: 0.0040267383903265\n",
      "Epoch 189/300\n",
      "Average training loss: 0.002878717319212026\n",
      "Average test loss: 0.0037723931138300233\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0028771286788913937\n",
      "Average test loss: 0.0037331575796835954\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0028783470226658716\n",
      "Average test loss: 0.0038152128813995255\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0028787914698736533\n",
      "Average test loss: 0.0037305666622188356\n",
      "Epoch 193/300\n",
      "Average training loss: 0.002889458244252536\n",
      "Average test loss: 0.0038451631317536037\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0028845149123420317\n",
      "Average test loss: 0.00387965903017256\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0028766403036812943\n",
      "Average test loss: 0.0038632548703915544\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0028731957754741114\n",
      "Average test loss: 0.004204967057953278\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0028673090702957577\n",
      "Average test loss: 0.003822325237509277\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002877235711655683\n",
      "Average test loss: 0.0037593905019263425\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0028694435695393217\n",
      "Average test loss: 0.0038278994214617544\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002863821263114611\n",
      "Average test loss: 0.003798199586984184\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0028677058981524573\n",
      "Average test loss: 0.003765866569346852\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0028652545168168014\n",
      "Average test loss: 0.003750833351785938\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0028699125870027477\n",
      "Average test loss: 0.0038560769222676754\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0028632675821168557\n",
      "Average test loss: 0.0038513145351575483\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0028588883444252942\n",
      "Average test loss: 0.0037434080081681412\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0028679143465641472\n",
      "Average test loss: 0.0040414478718820545\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0028530069726208844\n",
      "Average test loss: 0.003747917069743077\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002851735843345523\n",
      "Average test loss: 0.0048995356282426255\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0028511041802250676\n",
      "Average test loss: 0.0038145939144823286\n",
      "Epoch 210/300\n",
      "Average training loss: 0.002849312405826317\n",
      "Average test loss: 0.0037555065484096606\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0028487495742738245\n",
      "Average test loss: 0.003818100023186869\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0028615002950860393\n",
      "Average test loss: 0.0037967014807379906\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0028414187197470003\n",
      "Average test loss: 0.0038021806766175563\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002851352362997002\n",
      "Average test loss: 0.0037775570766793356\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0028365745294011303\n",
      "Average test loss: 0.003958416281268\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0028441327990343175\n",
      "Average test loss: 0.0039040320735010837\n",
      "Epoch 217/300\n",
      "Average training loss: 0.002834759403847986\n",
      "Average test loss: 0.0038582643260144526\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0028435832859120434\n",
      "Average test loss: 0.0038746231312139167\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0028405323812945023\n",
      "Average test loss: 0.0038161008596006366\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002832173033720917\n",
      "Average test loss: 0.0037284951895061465\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0028378092965318097\n",
      "Average test loss: 0.00377164372553428\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002837877378695541\n",
      "Average test loss: 0.004112647071480751\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0028309792863825956\n",
      "Average test loss: 0.003908640688699153\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0028440168015658857\n",
      "Average test loss: 0.0038478463689486184\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0028282431738658083\n",
      "Average test loss: 0.0037906620771520666\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0028373128751085865\n",
      "Average test loss: 0.004183273660226001\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0028193728747881123\n",
      "Average test loss: 0.003919909081525273\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002828765930193994\n",
      "Average test loss: 0.0040995213944051\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002823675786041551\n",
      "Average test loss: 0.0037802916382335956\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0028161918218764996\n",
      "Average test loss: 0.003758792250520653\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0028184650486542118\n",
      "Average test loss: 0.003906072414997551\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002819035856674115\n",
      "Average test loss: 0.0038227249613652626\n",
      "Epoch 233/300\n",
      "Average training loss: 0.002809174534554283\n",
      "Average test loss: 0.003933874153428608\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0028136143589185345\n",
      "Average test loss: 0.003788758739001221\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0028141215331852435\n",
      "Average test loss: 0.003792441427293751\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002805898191407323\n",
      "Average test loss: 0.003942145253014233\n",
      "Epoch 237/300\n",
      "Average training loss: 0.002823290571777357\n",
      "Average test loss: 0.003782594892092877\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0028016622896409695\n",
      "Average test loss: 0.004033816790829102\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0028002251719848976\n",
      "Average test loss: 0.0037496743036641014\n",
      "Epoch 240/300\n",
      "Average training loss: 0.002807940716958708\n",
      "Average test loss: 0.0038315801065829064\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0028026418017430437\n",
      "Average test loss: 0.0038571947833730114\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002830177396328913\n",
      "Average test loss: 0.0038989862509899668\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00279776825176345\n",
      "Average test loss: 0.003743805442419317\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0027976859942492513\n",
      "Average test loss: 0.004123971476116114\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0028018815720246898\n",
      "Average test loss: 0.00391036606559323\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0028071136907156973\n",
      "Average test loss: 0.0038101984510819117\n",
      "Epoch 247/300\n",
      "Average training loss: 0.002796954188288914\n",
      "Average test loss: 0.003943531180421511\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0028000632871149314\n",
      "Average test loss: 0.0039024581876065997\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0027934414028293556\n",
      "Average test loss: 0.004349978572792477\n",
      "Epoch 250/300\n",
      "Average training loss: 0.002796432033388151\n",
      "Average test loss: 0.0038206868138578203\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0027828730570359362\n",
      "Average test loss: 0.0038425763375643227\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0027942060627457166\n",
      "Average test loss: 0.003815610982063744\n",
      "Epoch 253/300\n",
      "Average training loss: 0.002788193942565057\n",
      "Average test loss: 0.003800486509170797\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0027881232572512494\n",
      "Average test loss: 0.003867774135950539\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0027872195748819247\n",
      "Average test loss: 0.003952092041571935\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002785302870389488\n",
      "Average test loss: 0.004355272003139059\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0028105133929186396\n",
      "Average test loss: 0.003800594205657641\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0027850817752381168\n",
      "Average test loss: 0.003790361068935858\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0027824384262785315\n",
      "Average test loss: 0.0038592129776047336\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0027774470860345494\n",
      "Average test loss: 0.003814404927815\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002782260563224554\n",
      "Average test loss: 0.003902714617964294\n",
      "Epoch 262/300\n",
      "Average training loss: 0.002772373587307003\n",
      "Average test loss: 0.0041309850923717025\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0027812209936479725\n",
      "Average test loss: 0.0038334845213426486\n",
      "Epoch 264/300\n",
      "Average training loss: 0.002773209347493119\n",
      "Average test loss: 0.0037993102624184556\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0027819181088772084\n",
      "Average test loss: 0.0038946700185123416\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002772784020130833\n",
      "Average test loss: 0.0038233435485098095\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0027678282558918\n",
      "Average test loss: 0.0038974045403301714\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0027828476008855633\n",
      "Average test loss: 0.0038134113053480785\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002764739986923006\n",
      "Average test loss: 0.003822719002349509\n",
      "Epoch 270/300\n",
      "Average training loss: 0.002769490496772859\n",
      "Average test loss: 0.003968020060410102\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0027804462341591714\n",
      "Average test loss: 0.003949718149378896\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0027712751208908027\n",
      "Average test loss: 0.0038068100729336343\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0027640018351376056\n",
      "Average test loss: 0.003933101443780793\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0027634265447656313\n",
      "Average test loss: 0.003827735813955466\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0027630615226096576\n",
      "Average test loss: 0.0037801058199256657\n",
      "Epoch 276/300\n",
      "Average training loss: 0.002764336010855105\n",
      "Average test loss: 0.0038865790106356146\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0027641117498278618\n",
      "Average test loss: 0.0038543802462518216\n",
      "Epoch 278/300\n",
      "Average training loss: 0.002765335019263956\n",
      "Average test loss: 0.003989285513344739\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002759034688894947\n",
      "Average test loss: 0.0037954831038498216\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0027732864374087917\n",
      "Average test loss: 0.003925533120209972\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0027560743163857196\n",
      "Average test loss: 0.0038827130724158553\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002773318187644084\n",
      "Average test loss: 0.003976222559395764\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002759729344190823\n",
      "Average test loss: 0.005145648288644023\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0027552005325754483\n",
      "Average test loss: 0.0038865629235903423\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0027506567190090816\n",
      "Average test loss: 0.0039441194964779745\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002750697533082631\n",
      "Average test loss: 0.003803891337994072\n",
      "Epoch 287/300\n",
      "Average training loss: 0.002752112261330088\n",
      "Average test loss: 0.0039047725204792287\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002749543682154682\n",
      "Average test loss: 0.0038757680627620884\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0027493570500777826\n",
      "Average test loss: 0.004046058102200429\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002756861459877756\n",
      "Average test loss: 0.004212543511763215\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002749942378451427\n",
      "Average test loss: 0.0038201629494627317\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00274776181144019\n",
      "Average test loss: 0.0038866143642614284\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0027485554739832877\n",
      "Average test loss: 0.003863239786277215\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0027446754311935767\n",
      "Average test loss: 0.0038839309236241714\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0027387671797639793\n",
      "Average test loss: 0.0038973195892241267\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00275039087028967\n",
      "Average test loss: 0.003909105833412872\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0027401262240277398\n",
      "Average test loss: 0.003965955001612505\n",
      "Epoch 298/300\n",
      "Average training loss: 0.002736092820142706\n",
      "Average test loss: 0.003913449604064226\n",
      "Epoch 299/300\n",
      "Average training loss: 0.002742757828699218\n",
      "Average test loss: 0.0038118413407355546\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002762752705357141\n",
      "Average test loss: 0.00423551214631233\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.025130835642417273\n",
      "Average test loss: 0.006239066468344794\n",
      "Epoch 2/300\n",
      "Average training loss: 0.007022624568392833\n",
      "Average test loss: 0.005970913226819701\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0060092123361925286\n",
      "Average test loss: 0.004875258100115591\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0054440459787017765\n",
      "Average test loss: 0.004447208270637526\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005056146652748187\n",
      "Average test loss: 0.004577218293315834\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004749408021983173\n",
      "Average test loss: 0.003938715401209063\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004520390876051453\n",
      "Average test loss: 0.003944670892424054\n",
      "Epoch 8/300\n",
      "Average training loss: 0.004312729399858249\n",
      "Average test loss: 0.003709800321401821\n",
      "Epoch 9/300\n",
      "Average training loss: 0.004120273547040092\n",
      "Average test loss: 0.0037223837634341585\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003966122674859233\n",
      "Average test loss: 0.0035448967632320193\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0038209823614193334\n",
      "Average test loss: 0.003732314725716909\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0036691975868824458\n",
      "Average test loss: 0.0034605262180169424\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0035553106094400088\n",
      "Average test loss: 0.003122625475956334\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0034489656107293235\n",
      "Average test loss: 0.003102910060228573\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0033637599856075314\n",
      "Average test loss: 0.003573170527608858\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0032953285955720476\n",
      "Average test loss: 0.003104478806257248\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0032234531918333637\n",
      "Average test loss: 0.0029341607093811037\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003158915520128277\n",
      "Average test loss: 0.0030212060696341923\n",
      "Epoch 19/300\n",
      "Average training loss: 0.003088059379822678\n",
      "Average test loss: 0.002881392857680718\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0030416210388971698\n",
      "Average test loss: 0.0028410315418408976\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002990253823912806\n",
      "Average test loss: 0.002770016260445118\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0029434969559725787\n",
      "Average test loss: 0.00297702021482918\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0029076965080781117\n",
      "Average test loss: 0.0026837240532040594\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0028518289774656295\n",
      "Average test loss: 0.0027115457560867072\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002828083268056313\n",
      "Average test loss: 0.0026594989188015463\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002796249926297201\n",
      "Average test loss: 0.002641104979854491\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0027559059905923073\n",
      "Average test loss: 0.002593869344641765\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0027274550520297552\n",
      "Average test loss: 0.002674312540433473\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0027066810853365393\n",
      "Average test loss: 0.002739928906576501\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002679365143386854\n",
      "Average test loss: 0.002637715661070413\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0026533542428579596\n",
      "Average test loss: 0.0026040792734258705\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0026455236084552275\n",
      "Average test loss: 0.0025810112228824033\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0026027875708209145\n",
      "Average test loss: 0.0025770506169646976\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002602997373168667\n",
      "Average test loss: 0.0025401981288774144\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002592763778857059\n",
      "Average test loss: 0.0024984287884500292\n",
      "Epoch 36/300\n",
      "Average training loss: 0.00259041152190831\n",
      "Average test loss: 0.0025716295035348997\n",
      "Epoch 37/300\n",
      "Average training loss: 0.002546733563558923\n",
      "Average test loss: 0.002514265982227193\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0025322121996432545\n",
      "Average test loss: 0.0024747126417027578\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002508027426070637\n",
      "Average test loss: 0.0024813609388139512\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0024994700451691945\n",
      "Average test loss: 0.0024953757979803617\n",
      "Epoch 41/300\n",
      "Average training loss: 0.002488255457745658\n",
      "Average test loss: 0.002437299713285433\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0024834385172774393\n",
      "Average test loss: 0.0025725983482682044\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0024780373132477205\n",
      "Average test loss: 0.0024789760371463168\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0024677186583479246\n",
      "Average test loss: 0.002539632581174374\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0024518071522729263\n",
      "Average test loss: 0.0026102498994312355\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002431476548728016\n",
      "Average test loss: 0.002511110159051087\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0024208340829031333\n",
      "Average test loss: 0.0025254410029285483\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002407205261910955\n",
      "Average test loss: 0.002453352587090598\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0024112637787022523\n",
      "Average test loss: 0.002505877810219924\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002398707891917891\n",
      "Average test loss: 0.0024866322292428876\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0023897960933132304\n",
      "Average test loss: 0.002434226483727495\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002394869646264447\n",
      "Average test loss: 0.0024950354550447727\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00239742254672779\n",
      "Average test loss: 0.0024659623075276613\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0023634984445654685\n",
      "Average test loss: 0.0024248974155634644\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002357650713581178\n",
      "Average test loss: 0.0027997596394270657\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0023483133983487883\n",
      "Average test loss: 0.002486306998775237\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00233767030450205\n",
      "Average test loss: 0.0025387355408941705\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002335848265844915\n",
      "Average test loss: 0.0024218747614779407\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0023235097067016695\n",
      "Average test loss: 0.002560976029270225\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002326323060836229\n",
      "Average test loss: 0.002797154528606269\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002315408899345332\n",
      "Average test loss: 0.0024348693792190816\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002316454005220698\n",
      "Average test loss: 0.002553540426409907\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0023472029047293796\n",
      "Average test loss: 0.002444296801152329\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0023139584153476687\n",
      "Average test loss: 0.0024935519678725136\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0022982587927124565\n",
      "Average test loss: 0.0024858760612292423\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0022802571081038977\n",
      "Average test loss: 0.002492676611782776\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0022819902589544653\n",
      "Average test loss: 0.0024328002294318543\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0022710154617412224\n",
      "Average test loss: 0.0024753964942776496\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002269944420705239\n",
      "Average test loss: 0.002459470281584395\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0022631377714375654\n",
      "Average test loss: 0.002461632903251383\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0022652393871297437\n",
      "Average test loss: 0.0024733028604338567\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0022757937976469597\n",
      "Average test loss: 0.0024650139972153635\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002250338056642148\n",
      "Average test loss: 0.0025261378108213345\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002238556916721993\n",
      "Average test loss: 0.0024536759505669275\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0022332226615399122\n",
      "Average test loss: 0.002464305512400137\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0022292569960571\n",
      "Average test loss: 0.002469590142990152\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0022373538522256747\n",
      "Average test loss: 0.0024504861486040882\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0022321710433397027\n",
      "Average test loss: 0.00254199096477694\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0022278014218641653\n",
      "Average test loss: 0.0024746194289376336\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0022513045652045142\n",
      "Average test loss: 0.002473466308063103\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0022122332487876217\n",
      "Average test loss: 0.0026487727945463525\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0022159260913936626\n",
      "Average test loss: 0.002424869683260719\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0022111405583305492\n",
      "Average test loss: 0.002546771022491157\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0022021700725373293\n",
      "Average test loss: 0.002490291026731332\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0021975336797121496\n",
      "Average test loss: 0.0025039236036439737\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002194529993459582\n",
      "Average test loss: 0.0026223820998436875\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0021905449089697666\n",
      "Average test loss: 0.002519807826106747\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0021949003609932132\n",
      "Average test loss: 0.00243280345822374\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0022041284123228656\n",
      "Average test loss: 0.002517710095478429\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0021783281920684707\n",
      "Average test loss: 0.002601714020801915\n",
      "Epoch 91/300\n",
      "Average training loss: 0.002179344783847531\n",
      "Average test loss: 0.0024988385666575696\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002169393305459784\n",
      "Average test loss: 0.002481928659603\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0021707447956626613\n",
      "Average test loss: 0.002512163797600402\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002174637455286251\n",
      "Average test loss: 0.002430841045971546\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002163427739714583\n",
      "Average test loss: 0.002469869926571846\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0021636972404602503\n",
      "Average test loss: 0.0024921260186367565\n",
      "Epoch 97/300\n",
      "Average training loss: 0.00216187356143362\n",
      "Average test loss: 0.002498356342315674\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0021620328040379616\n",
      "Average test loss: 0.0024589156705058283\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002144411920880278\n",
      "Average test loss: 0.0024591266005817387\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0021451453069845834\n",
      "Average test loss: 0.002406251990546783\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0021480083950898713\n",
      "Average test loss: 0.002548522217716608\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002152395506285959\n",
      "Average test loss: 0.0024348412588652635\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0021347199312100806\n",
      "Average test loss: 0.002495809662983649\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0021358340341183875\n",
      "Average test loss: 0.002453629268747237\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0021349729334728586\n",
      "Average test loss: 0.0024816512800753115\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0021336559443217184\n",
      "Average test loss: 0.0025004850895040567\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0021242753300401897\n",
      "Average test loss: 0.0024808513408319816\n",
      "Epoch 108/300\n",
      "Average training loss: 0.002128617077031069\n",
      "Average test loss: 0.0025076542020671896\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002118318538284964\n",
      "Average test loss: 0.0025330378990620376\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002118861217465666\n",
      "Average test loss: 0.002459271753206849\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0021131144606818756\n",
      "Average test loss: 0.0024686225139432484\n",
      "Epoch 112/300\n",
      "Average training loss: 0.002118879163430797\n",
      "Average test loss: 0.002531444931609763\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0021087664537545706\n",
      "Average test loss: 0.002477050343942311\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0021087951811237468\n",
      "Average test loss: 0.0030242193496475616\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0021020743099765646\n",
      "Average test loss: 0.0025249613729409048\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0021087889253265327\n",
      "Average test loss: 0.0025419984153575367\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0020972226745345525\n",
      "Average test loss: 0.00251525520792024\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0021021086518756217\n",
      "Average test loss: 0.0025229111180330318\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002103773898238109\n",
      "Average test loss: 0.0025861323482046523\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0020908442571138343\n",
      "Average test loss: 0.002549046229157183\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002086882536092566\n",
      "Average test loss: 0.0025131545127886866\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0020904016646866998\n",
      "Average test loss: 0.0026760003766458894\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0020961446268483994\n",
      "Average test loss: 0.002446317729436689\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0020888368910592465\n",
      "Average test loss: 0.0024185709913985597\n",
      "Epoch 125/300\n",
      "Average training loss: 0.002074729320489698\n",
      "Average test loss: 0.002658826786196894\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0020832441985193227\n",
      "Average test loss: 0.0024714234561348954\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002075330970601903\n",
      "Average test loss: 0.002450041222696503\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0020745347007695173\n",
      "Average test loss: 0.002580430436258515\n",
      "Epoch 129/300\n",
      "Average training loss: 0.002079851840933164\n",
      "Average test loss: 0.002505950037596954\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002077480631880462\n",
      "Average test loss: 0.0026801381167024375\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002069752143902911\n",
      "Average test loss: 0.002458101266581151\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0020639817590514817\n",
      "Average test loss: 0.0024803913577149312\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0020657999428610008\n",
      "Average test loss: 0.0026651466488838197\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0020618843290333947\n",
      "Average test loss: 0.002485546355239219\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002063419419961671\n",
      "Average test loss: 0.002647866601538327\n",
      "Epoch 136/300\n",
      "Average training loss: 0.002056275846850541\n",
      "Average test loss: 0.0025623835952331623\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002064480139460001\n",
      "Average test loss: 0.002462425097823143\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0020574316481749215\n",
      "Average test loss: 0.002500535688052575\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0020534009232910143\n",
      "Average test loss: 0.002563376293207208\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0020496598827756114\n",
      "Average test loss: 0.0025414277540726795\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002044544392161899\n",
      "Average test loss: 0.0024930511549529103\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002048454335373309\n",
      "Average test loss: 0.0025749163878046803\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00205683367823561\n",
      "Average test loss: 0.0025824516235540313\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0020464360904362466\n",
      "Average test loss: 0.0024953008972936207\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0020390030115635858\n",
      "Average test loss: 0.0027698796569473215\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0020450291287981803\n",
      "Average test loss: 0.0025983411729749704\n",
      "Epoch 147/300\n",
      "Average training loss: 0.002042868948231141\n",
      "Average test loss: 0.0025148480406237974\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0020376786291599275\n",
      "Average test loss: 0.0025930221387081677\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002039967448036704\n",
      "Average test loss: 0.0025483136574427287\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002037431886005733\n",
      "Average test loss: 0.002491213794797659\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002027101688604388\n",
      "Average test loss: 0.002597715483047068\n",
      "Epoch 152/300\n",
      "Average training loss: 0.002030958024164041\n",
      "Average test loss: 0.0025439756562312442\n",
      "Epoch 153/300\n",
      "Average training loss: 0.002029889963567257\n",
      "Average test loss: 0.002901574306190014\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002025399982308348\n",
      "Average test loss: 0.0024643576573580503\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002027872471138835\n",
      "Average test loss: 0.002461432309821248\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0020279974971587457\n",
      "Average test loss: 0.0025160722971583407\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0020244807882441413\n",
      "Average test loss: 0.0024648334278414646\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0020164113213411634\n",
      "Average test loss: 0.0025736315117941963\n",
      "Epoch 159/300\n",
      "Average training loss: 0.002016315277044972\n",
      "Average test loss: 0.0024986114982101654\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0020157318302533693\n",
      "Average test loss: 0.002545641341764066\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0020172466717453467\n",
      "Average test loss: 0.002484899366481437\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0020154452525700133\n",
      "Average test loss: 0.002510425373911858\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0020079898186441923\n",
      "Average test loss: 0.005321311315521598\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0020114377218609053\n",
      "Average test loss: 0.0025390584363291662\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0020105465407379798\n",
      "Average test loss: 0.0025156250016556848\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0020068618706944917\n",
      "Average test loss: 0.002494667853228748\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0020065662908471294\n",
      "Average test loss: 0.0025378139009699227\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002017651673509843\n",
      "Average test loss: 0.002607696186337206\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002001032879990008\n",
      "Average test loss: 0.0025677992093066375\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002005853473001884\n",
      "Average test loss: 0.0025803411851326626\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002002079826262262\n",
      "Average test loss: 0.0025081656069184345\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0019921029872364466\n",
      "Average test loss: 0.0025093362894323137\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001996769097323219\n",
      "Average test loss: 0.0024603780981981094\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0019953265943460996\n",
      "Average test loss: 0.0025707960875911845\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0019933884041383864\n",
      "Average test loss: 0.002553145113090674\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0019931732695549725\n",
      "Average test loss: 0.0024883049031098683\n",
      "Epoch 177/300\n",
      "Average training loss: 0.001988195411964423\n",
      "Average test loss: 0.002518379139610463\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0019873654736826817\n",
      "Average test loss: 0.0025397039482163057\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0019856370377043884\n",
      "Average test loss: 0.0025840801691843405\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0019957894275171887\n",
      "Average test loss: 0.002516833958422972\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001986111040537556\n",
      "Average test loss: 0.0025455640628933907\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0019839431130223802\n",
      "Average test loss: 0.002573212016787794\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001976408938566844\n",
      "Average test loss: 0.0024774875175207853\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001982285391332375\n",
      "Average test loss: 0.0025759897467990717\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0019796102200117377\n",
      "Average test loss: 0.0026161291711032392\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0019750430165893502\n",
      "Average test loss: 0.002588682776523961\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0019777126022511057\n",
      "Average test loss: 0.0024860354092799955\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0019845694539447627\n",
      "Average test loss: 0.0025188061857803\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0019745169138121934\n",
      "Average test loss: 0.002535974083054397\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0019728965821365517\n",
      "Average test loss: 0.002471263564295239\n",
      "Epoch 191/300\n",
      "Average training loss: 0.001971401382651594\n",
      "Average test loss: 0.002532232311243812\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0019788799972997773\n",
      "Average test loss: 0.002494623789874216\n",
      "Epoch 193/300\n",
      "Average training loss: 0.001971109584387806\n",
      "Average test loss: 0.0025054204085220894\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0019698978849790163\n",
      "Average test loss: 0.0025311888634330696\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0019765712197663057\n",
      "Average test loss: 0.0025743670902318425\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0019621675019669865\n",
      "Average test loss: 0.0026537214015713998\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0019588650862375897\n",
      "Average test loss: 0.0025758216712209914\n",
      "Epoch 198/300\n",
      "Average training loss: 0.001968115019198093\n",
      "Average test loss: 0.002541817159081499\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0019611166675264635\n",
      "Average test loss: 0.531558074288898\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002285587394506567\n",
      "Average test loss: 0.0025291000914035573\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0019457008871767256\n",
      "Average test loss: 0.0026366211279398866\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0019350568937758605\n",
      "Average test loss: 0.0026094553899019958\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0019431489430781867\n",
      "Average test loss: 0.002525829299663504\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001950579275170134\n",
      "Average test loss: 0.0025446144700464277\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0019549442237863936\n",
      "Average test loss: 0.002557321783879565\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0019497669585463073\n",
      "Average test loss: 0.0024819762555675375\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0019501501665347152\n",
      "Average test loss: 0.008084470217840539\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0019507470195078188\n",
      "Average test loss: 0.0025805998970237043\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0019543728976406985\n",
      "Average test loss: 0.0024879315023620925\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0019553277184151943\n",
      "Average test loss: 0.002520486248036226\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0019522747228232524\n",
      "Average test loss: 0.0026411420485625663\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00194321052202334\n",
      "Average test loss: 0.0025067452817327447\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0019510515342570014\n",
      "Average test loss: 0.0024980810321867464\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0019522392672176162\n",
      "Average test loss: 0.002775761864251561\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0019483627134727107\n",
      "Average test loss: 0.0024810845324148736\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0019476122496028742\n",
      "Average test loss: 0.002581531966932946\n",
      "Epoch 217/300\n",
      "Average training loss: 0.001940870803159972\n",
      "Average test loss: 0.0025317819611065918\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0019405140748454464\n",
      "Average test loss: 0.0026659238330192037\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0019454622817639676\n",
      "Average test loss: 0.0026241440897186597\n",
      "Epoch 220/300\n",
      "Average training loss: 0.001950402379863792\n",
      "Average test loss: 0.002551538496174746\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0019416838294516008\n",
      "Average test loss: 0.0028340177829894756\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0019410810024581022\n",
      "Average test loss: 0.0026943731292461354\n",
      "Epoch 223/300\n",
      "Average training loss: 0.001942527993168268\n",
      "Average test loss: 0.002515181747575601\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0019302936879297098\n",
      "Average test loss: 0.0025836053856958946\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0019384553045448329\n",
      "Average test loss: 0.0025357383141915006\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0019413360849850707\n",
      "Average test loss: 0.002581732740211818\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001934822968724701\n",
      "Average test loss: 0.0025182612234105665\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0019324989849701525\n",
      "Average test loss: 0.002572667343128059\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0019288828432973888\n",
      "Average test loss: 0.0026062982990923854\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0019400277671714624\n",
      "Average test loss: 0.002858795061086615\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0019385887631732557\n",
      "Average test loss: 0.0025276598928289282\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0019300496416787306\n",
      "Average test loss: 0.002507973981400331\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0019321250105276703\n",
      "Average test loss: 0.0026069510068951383\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0019246696621800462\n",
      "Average test loss: 0.0025146489973283475\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0019319258528865046\n",
      "Average test loss: 0.0025709724403503867\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0019325913859324323\n",
      "Average test loss: 0.002569010051795178\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0019210623996332287\n",
      "Average test loss: 0.0026169342822912668\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0019289384200755094\n",
      "Average test loss: 0.0026986383832991122\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0019228575441779363\n",
      "Average test loss: 0.0025708322187678683\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0019157448654166527\n",
      "Average test loss: 0.0026812367135037977\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0019211115777078602\n",
      "Average test loss: 0.002478788206767705\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0019284640319852365\n",
      "Average test loss: 0.0026063422844227815\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0019235571775999335\n",
      "Average test loss: 0.0025873862246258393\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0019160272604268457\n",
      "Average test loss: 0.0025883098619265688\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0019222258325252268\n",
      "Average test loss: 0.0025599615024402738\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0019193483893242148\n",
      "Average test loss: 0.002576743870559666\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0019179577021342185\n",
      "Average test loss: 0.0030326745299001537\n",
      "Epoch 248/300\n",
      "Average training loss: 0.001917755362474256\n",
      "Average test loss: 0.0026398850633866256\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0019133504701571331\n",
      "Average test loss: 0.0025414152650369537\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0019133154853350587\n",
      "Average test loss: 0.0025963494333749015\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0019106688126921654\n",
      "Average test loss: 0.0025546527831918665\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0019084876851282186\n",
      "Average test loss: 0.002572203653968043\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0019113244113201895\n",
      "Average test loss: 0.0025468821188228\n",
      "Epoch 254/300\n",
      "Average training loss: 0.001922357042837474\n",
      "Average test loss: 0.0025436466267953315\n",
      "Epoch 255/300\n",
      "Average training loss: 0.001909757118154731\n",
      "Average test loss: 0.0025445768334385424\n",
      "Epoch 256/300\n",
      "Average training loss: 0.001907529132026765\n",
      "Average test loss: 0.0026036008983436557\n",
      "Epoch 257/300\n",
      "Average training loss: 0.001913256730677353\n",
      "Average test loss: 0.0025329676162865427\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0019118520430185728\n",
      "Average test loss: 0.002613180636635257\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0019045722283836867\n",
      "Average test loss: 0.002597668701161941\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0019037665114220645\n",
      "Average test loss: 0.002545301422683729\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0019053622303116652\n",
      "Average test loss: 0.002644516308274534\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0019043719956858291\n",
      "Average test loss: 0.0025799434795561764\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0019106366644716926\n",
      "Average test loss: 0.002543369448122879\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0018997104658434788\n",
      "Average test loss: 0.0025273715425282715\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0019009084552526474\n",
      "Average test loss: 0.0025626524525384107\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0019095019821284546\n",
      "Average test loss: 0.0026665117676473327\n",
      "Epoch 267/300\n",
      "Average training loss: 0.001903955721606811\n",
      "Average test loss: 0.0026681831552543573\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0018990858368989495\n",
      "Average test loss: 0.002607580231709613\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0018994315650521054\n",
      "Average test loss: 0.0025508118822342818\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0019008980340634782\n",
      "Average test loss: 0.00258188637232201\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0018932995837595728\n",
      "Average test loss: 0.0026766478133698306\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0018963748700916768\n",
      "Average test loss: 0.002534259633678529\n",
      "Epoch 273/300\n",
      "Average training loss: 0.001898116537887189\n",
      "Average test loss: 0.0026714163979308474\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0018969131801277398\n",
      "Average test loss: 0.0026174894908650055\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001898278644411928\n",
      "Average test loss: 0.0026327898053245414\n",
      "Epoch 276/300\n",
      "Average training loss: 0.001889426718879905\n",
      "Average test loss: 0.002596652622955541\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0018954772646021512\n",
      "Average test loss: 0.002636807552021411\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0018937155991378758\n",
      "Average test loss: 0.002676592312960161\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0019002662117903431\n",
      "Average test loss: 0.0026306728553026917\n",
      "Epoch 280/300\n",
      "Average training loss: 0.00188518839288089\n",
      "Average test loss: 0.002565628830136524\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0018915264427454936\n",
      "Average test loss: 0.0025503403950068687\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0018958063654394613\n",
      "Average test loss: 0.0028725267777012452\n",
      "Epoch 283/300\n",
      "Average training loss: 0.001895450545888808\n",
      "Average test loss: 0.0026052837340782087\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0018855843457082908\n",
      "Average test loss: 0.002574897615859906\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0018849434274145298\n",
      "Average test loss: 0.002573289269167516\n",
      "Epoch 286/300\n",
      "Average training loss: 0.001895415461415218\n",
      "Average test loss: 0.00261224324711495\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0018847187293900385\n",
      "Average test loss: 0.002544987427484658\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0018883653166186479\n",
      "Average test loss: 0.0025862909551295967\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0018863403390472135\n",
      "Average test loss: 0.002637340629680289\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0018863575937640336\n",
      "Average test loss: 0.0026052722881237667\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0018817649614065886\n",
      "Average test loss: 0.0025625353563163015\n",
      "Epoch 292/300\n",
      "Average training loss: 0.001875201446418133\n",
      "Average test loss: 0.002584341949162384\n",
      "Epoch 293/300\n",
      "Average training loss: 0.001881235351268616\n",
      "Average test loss: 0.002589586698346668\n",
      "Epoch 294/300\n",
      "Average training loss: 0.001887381620808608\n",
      "Average test loss: 0.002594651714588205\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0018814401850104333\n",
      "Average test loss: 0.002602487181623777\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0018772475201015671\n",
      "Average test loss: 0.002588218952839573\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0018756694313552644\n",
      "Average test loss: 0.002620820187860065\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0018846027708301941\n",
      "Average test loss: 0.00258580120280385\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0018762852787557575\n",
      "Average test loss: 0.0027990940453277696\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0018780636290709177\n",
      "Average test loss: 0.0025310439581258427\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02172201941576269\n",
      "Average test loss: 0.005556772918336921\n",
      "Epoch 2/300\n",
      "Average training loss: 0.005542433321889904\n",
      "Average test loss: 0.00433535828979479\n",
      "Epoch 3/300\n",
      "Average training loss: 0.004688406229847008\n",
      "Average test loss: 0.0039785997457802294\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00422371899543537\n",
      "Average test loss: 0.0038373954844557576\n",
      "Epoch 5/300\n",
      "Average training loss: 0.003897818954454528\n",
      "Average test loss: 0.003247063919280966\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0036415391667849488\n",
      "Average test loss: 0.003119302049279213\n",
      "Epoch 7/300\n",
      "Average training loss: 0.003470726978240742\n",
      "Average test loss: 0.0030704939843465883\n",
      "Epoch 8/300\n",
      "Average training loss: 0.003251140608555741\n",
      "Average test loss: 0.0028379086676157184\n",
      "Epoch 9/300\n",
      "Average training loss: 0.003094080951064825\n",
      "Average test loss: 0.0028404181119468476\n",
      "Epoch 10/300\n",
      "Average training loss: 0.002967672461643815\n",
      "Average test loss: 0.002663474174009429\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0028459477054162156\n",
      "Average test loss: 0.0024474711784472068\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0027547060896952946\n",
      "Average test loss: 0.002681617565660013\n",
      "Epoch 13/300\n",
      "Average training loss: 0.002648813412007358\n",
      "Average test loss: 0.0023055958352569076\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0025714746647410923\n",
      "Average test loss: 0.002343388501347767\n",
      "Epoch 15/300\n",
      "Average training loss: 0.002478717306214902\n",
      "Average test loss: 0.002202632101666596\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002428167627296514\n",
      "Average test loss: 0.0021904686097469596\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0023636825726264053\n",
      "Average test loss: 0.0021270492303495604\n",
      "Epoch 18/300\n",
      "Average training loss: 0.002301809823140502\n",
      "Average test loss: 0.002102576631638739\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0022642108557952775\n",
      "Average test loss: 0.0020929212379786704\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0022140239582707486\n",
      "Average test loss: 0.002027698160149157\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0021797933202857772\n",
      "Average test loss: 0.0020607795539415545\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0021395068418027627\n",
      "Average test loss: 0.002039483122113678\n",
      "Epoch 23/300\n",
      "Average training loss: 0.002100379579183128\n",
      "Average test loss: 0.001918945632978446\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002073542323998279\n",
      "Average test loss: 0.002025564143434167\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002055916027993792\n",
      "Average test loss: 0.001897949955633117\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0020165239514576064\n",
      "Average test loss: 0.0019175318634758393\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0019993512818796766\n",
      "Average test loss: 0.0019009313620626926\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0019882424596904054\n",
      "Average test loss: 0.0019760299546437133\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0019610676400156485\n",
      "Average test loss: 0.0018443722890483009\n",
      "Epoch 30/300\n",
      "Average training loss: 0.001947572408657935\n",
      "Average test loss: 0.0018880037032067776\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0019324319097730849\n",
      "Average test loss: 0.0017971905067356096\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0019109001252800226\n",
      "Average test loss: 0.0018158271533126632\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0018881413752420081\n",
      "Average test loss: 0.0018189359831934173\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0018893093953116073\n",
      "Average test loss: 0.00185040584889551\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0018640387331446012\n",
      "Average test loss: 0.0018533812070058453\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0018600842382551896\n",
      "Average test loss: 0.0017669969958563647\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0018524417572965224\n",
      "Average test loss: 0.0018053688221714563\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0018330481823326814\n",
      "Average test loss: 0.0017699977844539616\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0018224485464808013\n",
      "Average test loss: 0.0017785269814646907\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0018120110762409039\n",
      "Average test loss: 0.0017663678021894562\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0018089783654237786\n",
      "Average test loss: 0.0017377940594322152\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0018041385344954\n",
      "Average test loss: 0.0017807852284361918\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0018018073911468188\n",
      "Average test loss: 0.001746932195396059\n",
      "Epoch 44/300\n",
      "Average training loss: 0.001788423150467376\n",
      "Average test loss: 0.001728888606859578\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0017757650008425117\n",
      "Average test loss: 0.0017885402881850799\n",
      "Epoch 46/300\n",
      "Average training loss: 0.001769921125119759\n",
      "Average test loss: 0.0017803323614514535\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0017661319319158793\n",
      "Average test loss: 0.0017328033710622952\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0017978313980210158\n",
      "Average test loss: 0.0017555340069035689\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0017490100287314919\n",
      "Average test loss: 0.0017427996769547463\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0017334565464407206\n",
      "Average test loss: 0.0017230022168821758\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0017353110807016492\n",
      "Average test loss: 0.0017426542051964336\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0017239864563776386\n",
      "Average test loss: 0.001770721128417386\n",
      "Epoch 53/300\n",
      "Average training loss: 0.001733666995850702\n",
      "Average test loss: 0.0017334930756025843\n",
      "Epoch 54/300\n",
      "Average training loss: 0.001733038116660383\n",
      "Average test loss: 0.001782703718687925\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0017187097287840313\n",
      "Average test loss: 0.0018591172277099557\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0017142878059918682\n",
      "Average test loss: 0.0017328547034412623\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0017080058644836148\n",
      "Average test loss: 0.0017142451398281587\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0017007819170960121\n",
      "Average test loss: 0.0017453217148366903\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0016976528383998407\n",
      "Average test loss: 0.0017309585531345671\n",
      "Epoch 60/300\n",
      "Average training loss: 0.001693024801504281\n",
      "Average test loss: 0.0018766038223273225\n",
      "Epoch 61/300\n",
      "Average training loss: 0.001689980394517382\n",
      "Average test loss: 0.0017359662358131674\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0016775464245842562\n",
      "Average test loss: 0.001932342829182744\n",
      "Epoch 63/300\n",
      "Average training loss: 0.001680866996343765\n",
      "Average test loss: 0.0020186965491415724\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0016802587561930218\n",
      "Average test loss: 0.001793542315562566\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0016824511862877343\n",
      "Average test loss: 0.0017042580586340693\n",
      "Epoch 66/300\n",
      "Average training loss: 0.001674006363687416\n",
      "Average test loss: 0.0017037153471675184\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0016824600481324726\n",
      "Average test loss: 0.0018057548643814192\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0016563277200071348\n",
      "Average test loss: 0.001731552636457814\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0016538870509506928\n",
      "Average test loss: 0.0017507037272055943\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0016488667668567763\n",
      "Average test loss: 0.0017251435449967781\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0016468388157793218\n",
      "Average test loss: 0.001721476973241402\n",
      "Epoch 72/300\n",
      "Average training loss: 0.001640839367794494\n",
      "Average test loss: 0.001747248008226355\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0016542882109060885\n",
      "Average test loss: 0.0017126915085957282\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0016362064876076247\n",
      "Average test loss: 0.0017762633965661128\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0016340132607147097\n",
      "Average test loss: 0.002150670228732957\n",
      "Epoch 76/300\n",
      "Average training loss: 0.001633028683045672\n",
      "Average test loss: 0.0017086449306872155\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0016324953101575375\n",
      "Average test loss: 0.0017596949074003432\n",
      "Epoch 78/300\n",
      "Average training loss: 0.001616746111255553\n",
      "Average test loss: 0.001768164806274904\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0016186253876528806\n",
      "Average test loss: 0.0017425645815415515\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0016208987622004416\n",
      "Average test loss: 0.0018115380571948158\n",
      "Epoch 81/300\n",
      "Average training loss: 0.001626037188598679\n",
      "Average test loss: 0.0017580733169905013\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0016138778974612554\n",
      "Average test loss: 0.0017020078591174549\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0016034874760856232\n",
      "Average test loss: 0.0017306629843595956\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0016078068025203215\n",
      "Average test loss: 0.0018113329809986882\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0016098282225430012\n",
      "Average test loss: 0.0017217338759866025\n",
      "Epoch 86/300\n",
      "Average training loss: 0.001615458451418413\n",
      "Average test loss: 0.0017191167028827799\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0015995866684760485\n",
      "Average test loss: 0.0018171469712009033\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0015904659731313586\n",
      "Average test loss: 0.0016945013461841477\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0015879505296341246\n",
      "Average test loss: 0.0017793471043308577\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0015894711387033265\n",
      "Average test loss: 0.0018399970183769862\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0016118119841234551\n",
      "Average test loss: 0.0017584339388542706\n",
      "Epoch 92/300\n",
      "Average training loss: 0.001585490950383246\n",
      "Average test loss: 0.001759062137765189\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0015842177117657331\n",
      "Average test loss: 0.0016999289465861188\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0015820617986222107\n",
      "Average test loss: 0.0017480889746091432\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0015799516359758046\n",
      "Average test loss: 0.0017899495874428087\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0015740877346446117\n",
      "Average test loss: 0.0017917596246633265\n",
      "Epoch 97/300\n",
      "Average training loss: 0.001577660670814415\n",
      "Average test loss: 0.001783401605569654\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0015795953340000577\n",
      "Average test loss: 0.0017834568620762891\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0015760968607436451\n",
      "Average test loss: 0.0018465333260181877\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0015659780287597743\n",
      "Average test loss: 0.0017472847838782603\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0015771717315332756\n",
      "Average test loss: 0.001726603909706076\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0015579346894390054\n",
      "Average test loss: 0.001753059303181039\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0015614451014747221\n",
      "Average test loss: 0.0017416936569950646\n",
      "Epoch 104/300\n",
      "Average training loss: 0.001579467022481064\n",
      "Average test loss: 0.001782108736741874\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0015523169758833116\n",
      "Average test loss: 0.0017137471391922897\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0015576300714164972\n",
      "Average test loss: 0.001742297324196746\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0015614229977751772\n",
      "Average test loss: 0.0016919469500167503\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0015448833881980843\n",
      "Average test loss: 0.0017202319293800328\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0015424568807292315\n",
      "Average test loss: 0.0017012076117098331\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0015423862115583485\n",
      "Average test loss: 0.0028268527012939256\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0015460845028153723\n",
      "Average test loss: 0.001821597167911629\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0015410767246244682\n",
      "Average test loss: 0.001842423299430973\n",
      "Epoch 113/300\n",
      "Average training loss: 0.001543158101849258\n",
      "Average test loss: 0.0017340944409370421\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0015428532606197728\n",
      "Average test loss: 0.0017573925899341703\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0015411114844804008\n",
      "Average test loss: 0.0017641824550098844\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0015472117457538844\n",
      "Average test loss: 0.001801179754651255\n",
      "Epoch 117/300\n",
      "Average training loss: 0.001538993680021829\n",
      "Average test loss: 0.0017997269377940232\n",
      "Epoch 118/300\n",
      "Average training loss: 0.001532387279284497\n",
      "Average test loss: 0.0017666048709717062\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0015284488759934902\n",
      "Average test loss: 0.0017874721315585904\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0015346206717400087\n",
      "Average test loss: 0.001719786694376833\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0015259172363827627\n",
      "Average test loss: 0.0017994343259682259\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0015238987801389561\n",
      "Average test loss: 0.00745861192420125\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0015278532932408982\n",
      "Average test loss: 0.0018212417852547433\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0015233922203381856\n",
      "Average test loss: 0.0017097181526737081\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0015168598780615462\n",
      "Average test loss: 0.001762348282461365\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0015158935298936235\n",
      "Average test loss: 0.0017476455653086306\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0015187056435065137\n",
      "Average test loss: 0.0017777450951851077\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0015165734038584762\n",
      "Average test loss: 0.0018092031070134707\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0015096073474528062\n",
      "Average test loss: 0.0018080700731111898\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0015132757589841883\n",
      "Average test loss: 0.0017894570390797324\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0015067744156759646\n",
      "Average test loss: 0.0018027736896442043\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0015112795934288038\n",
      "Average test loss: 0.0018518126225098967\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0015041254515656168\n",
      "Average test loss: 0.001738362649248706\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0015098783199985822\n",
      "Average test loss: 0.0018112136385300094\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0015073344754055141\n",
      "Average test loss: 0.0029885701625090508\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0015009321157510081\n",
      "Average test loss: 0.0018213611061995228\n",
      "Epoch 137/300\n",
      "Average training loss: 0.001504112460753984\n",
      "Average test loss: 0.0017854412940020363\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0015098049703778493\n",
      "Average test loss: 0.0018529466243667734\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0014967659789447982\n",
      "Average test loss: 0.0018009102694276306\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0015005896053173476\n",
      "Average test loss: 0.001974383887938327\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001495425073016021\n",
      "Average test loss: 0.0017608452236486806\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0015071831721191606\n",
      "Average test loss: 0.0017652788624788324\n",
      "Epoch 143/300\n",
      "Average training loss: 0.001501260417410069\n",
      "Average test loss: 0.0017302561290562153\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0014919793596491217\n",
      "Average test loss: 0.0017680563728014627\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0014889760442698995\n",
      "Average test loss: 0.0017568007114653786\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0014860946289781067\n",
      "Average test loss: 0.0018039469956937764\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0014889767377947768\n",
      "Average test loss: 0.0017478672015584177\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0014874183064740565\n",
      "Average test loss: 0.0017380900968694026\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0014886601434813606\n",
      "Average test loss: 0.0017337706703692674\n",
      "Epoch 150/300\n",
      "Average training loss: 0.001486259562480781\n",
      "Average test loss: 0.0017734795688754983\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0014898264577819242\n",
      "Average test loss: 0.0017746547739952803\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0014872671408164833\n",
      "Average test loss: 0.001732384088035259\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0014837772158078021\n",
      "Average test loss: 0.0017545830389070842\n",
      "Epoch 154/300\n",
      "Average training loss: 0.001475234694364998\n",
      "Average test loss: 0.0017536763813760545\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0014835403809944788\n",
      "Average test loss: 0.002037988753264977\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0014787897618694437\n",
      "Average test loss: 0.0017933845992924439\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0014736285056504937\n",
      "Average test loss: 0.0018134863819513057\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001479737335919506\n",
      "Average test loss: 0.0018521164755026498\n",
      "Epoch 159/300\n",
      "Average training loss: 0.001470215373362104\n",
      "Average test loss: 0.0018474364770162436\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0014723712941631674\n",
      "Average test loss: 0.0018000491058660878\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0014753247330793076\n",
      "Average test loss: 0.0017779663894325495\n",
      "Epoch 162/300\n",
      "Average training loss: 0.001467165334150195\n",
      "Average test loss: 0.0017487068475327558\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001470168371581369\n",
      "Average test loss: 0.0017764466079986758\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0014697931201921568\n",
      "Average test loss: 0.0017791420011263755\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0014630673467699025\n",
      "Average test loss: 0.0017974712215363979\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0014681976427220636\n",
      "Average test loss: 0.0018274279567930434\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0014633615697837539\n",
      "Average test loss: 0.0017472018553978867\n",
      "Epoch 168/300\n",
      "Average training loss: 0.00146908164376186\n",
      "Average test loss: 0.0018016761657264498\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0014659155833845337\n",
      "Average test loss: 0.0018043887418591313\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0014570928381548987\n",
      "Average test loss: 0.0018071577190938923\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0014603058063528604\n",
      "Average test loss: 0.0017501051937126452\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0014561844369293086\n",
      "Average test loss: 0.0017822935050353407\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001459907619489564\n",
      "Average test loss: 0.0018250984268056022\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0014535624729469418\n",
      "Average test loss: 0.0017941091902967956\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0014513300547583236\n",
      "Average test loss: 0.00181272295396775\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0014574284976762203\n",
      "Average test loss: 0.001748811630739106\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0014631390300475888\n",
      "Average test loss: 0.0017818828214787774\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0014532362507242295\n",
      "Average test loss: 0.0018580085744874346\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0014510273992394407\n",
      "Average test loss: 0.0018010927823682626\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0014571190003512635\n",
      "Average test loss: 0.0017473288123599356\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0014529420075317225\n",
      "Average test loss: 0.0017985780901379055\n",
      "Epoch 182/300\n",
      "Average training loss: 0.001444563392860194\n",
      "Average test loss: 0.0017894607670605183\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0014450080094652042\n",
      "Average test loss: 0.0017727997439603012\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001445428919667999\n",
      "Average test loss: 0.0018318267280442847\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0014443825192542539\n",
      "Average test loss: 0.0018186135915004545\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00145105774110804\n",
      "Average test loss: 0.001799168399017718\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0014458654147262375\n",
      "Average test loss: 0.0017816838192650014\n",
      "Epoch 188/300\n",
      "Average training loss: 0.00144327973232915\n",
      "Average test loss: 0.0017666254951618611\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0014430206706747414\n",
      "Average test loss: 0.0019401606221993765\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0014409990599378944\n",
      "Average test loss: 0.0018717197314318684\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0014449945533027253\n",
      "Average test loss: 0.0018158659233401219\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0014382964783451622\n",
      "Average test loss: 0.0018358041637887557\n",
      "Epoch 193/300\n",
      "Average training loss: 0.001452954925596714\n",
      "Average test loss: 0.0017508854423132208\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0014485859444571866\n",
      "Average test loss: 0.0018259439998202854\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0014307481221233806\n",
      "Average test loss: 0.0017915449643300639\n",
      "Epoch 196/300\n",
      "Average training loss: 0.001434004529689749\n",
      "Average test loss: 0.0017885218802839518\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0014382415638408727\n",
      "Average test loss: 0.0018035133602097631\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0014293078936429488\n",
      "Average test loss: 0.0017891705049615768\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0014374800618324015\n",
      "Average test loss: 0.004905098622457848\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0041279838542557425\n",
      "Average test loss: 0.0021901984236513575\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0022182994426952467\n",
      "Average test loss: 0.0018843074560993248\n",
      "Epoch 202/300\n",
      "Average training loss: 0.001982780467201438\n",
      "Average test loss: 0.0017982829112766518\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0018604862756199307\n",
      "Average test loss: 0.0017919472955788176\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0017733039091237717\n",
      "Average test loss: 0.0017641026905427376\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0017044696271833447\n",
      "Average test loss: 0.0017470199956248205\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0016425430566693346\n",
      "Average test loss: 0.001744472821139627\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0015842128534697825\n",
      "Average test loss: 0.001774441741199957\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0015275240936833952\n",
      "Average test loss: 0.0017959340333731637\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0014881544771293798\n",
      "Average test loss: 0.0017553128291749292\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0014558737439413865\n",
      "Average test loss: 0.0017980060059991148\n",
      "Epoch 211/300\n",
      "Average training loss: 0.001436750702559948\n",
      "Average test loss: 0.0019041916810803943\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0014274074944357078\n",
      "Average test loss: 0.0017892318165136708\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0014311580056738523\n",
      "Average test loss: 0.00189471358133273\n",
      "Epoch 214/300\n",
      "Average training loss: 0.001438000711508923\n",
      "Average test loss: 0.0017557558135853873\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0014357939729880956\n",
      "Average test loss: 0.0017975145116862323\n",
      "Epoch 216/300\n",
      "Average training loss: 0.001428405194129381\n",
      "Average test loss: 0.0018011438111878103\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0014369118499259154\n",
      "Average test loss: 0.0017671888406491942\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0014394611089179913\n",
      "Average test loss: 0.0017783980232973895\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0014277859975894292\n",
      "Average test loss: 0.001828400374079744\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0014220153190609482\n",
      "Average test loss: 0.0018617191923161348\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0014238735998256338\n",
      "Average test loss: 0.0018295050900843407\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0014252704985750217\n",
      "Average test loss: 0.0018270463040098548\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0014275443876783052\n",
      "Average test loss: 0.0018751388154923916\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0014264448213701447\n",
      "Average test loss: 0.0018881310929233829\n",
      "Epoch 225/300\n",
      "Average training loss: 0.001426312261261046\n",
      "Average test loss: 0.0018223881630433931\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0014187324058471454\n",
      "Average test loss: 0.0019096199898049236\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001418798220757809\n",
      "Average test loss: 0.0018721338948234916\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0014239782751020458\n",
      "Average test loss: 0.001782418759332763\n",
      "Epoch 229/300\n",
      "Average training loss: 0.001422693706324531\n",
      "Average test loss: 0.0017546462277985282\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0014180330390421052\n",
      "Average test loss: 0.001909286146517843\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0014186202627089288\n",
      "Average test loss: 0.0018468865590790908\n",
      "Epoch 232/300\n",
      "Average training loss: 0.001418812669451452\n",
      "Average test loss: 0.0018475784301343892\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0014137517718805206\n",
      "Average test loss: 0.0018553243732700746\n",
      "Epoch 234/300\n",
      "Average training loss: 0.001411973282177415\n",
      "Average test loss: 0.001785859876519276\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0014199581494968797\n",
      "Average test loss: 0.001879886281159189\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001411182275455859\n",
      "Average test loss: 0.0018193695445855459\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0014122947885965307\n",
      "Average test loss: 0.0022071664855918952\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0014163573588141137\n",
      "Average test loss: 0.0017988574157158533\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0014076855562420354\n",
      "Average test loss: 0.0018103560634578268\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0014155498744092053\n",
      "Average test loss: 0.0017905861167237162\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0014098431593738497\n",
      "Average test loss: 0.0018795511714803676\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0014086851961910725\n",
      "Average test loss: 0.006089648077057468\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0014235880563242568\n",
      "Average test loss: 0.0018375539613059825\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0014053737856447696\n",
      "Average test loss: 0.0018436327162716123\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0014043108746409417\n",
      "Average test loss: 0.0018226422377758556\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0014124463437539007\n",
      "Average test loss: 0.0018149050839969681\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0014016983026845587\n",
      "Average test loss: 0.0017921862623964746\n",
      "Epoch 248/300\n",
      "Average training loss: 0.00140520371351805\n",
      "Average test loss: 0.0017822024897775717\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0014161058603268531\n",
      "Average test loss: 0.0017977606740055812\n",
      "Epoch 250/300\n",
      "Average training loss: 0.001402769091953006\n",
      "Average test loss: 0.002094282541113595\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0013984700526214308\n",
      "Average test loss: 0.0018195490185171366\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0014030640894133184\n",
      "Average test loss: 0.0018081719587660498\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0014014349970966579\n",
      "Average test loss: 0.0018901397602425682\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0014042516946792603\n",
      "Average test loss: 0.0018239433934084243\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0014022023981023167\n",
      "Average test loss: 0.0017732356859164107\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0014020390905853774\n",
      "Average test loss: 0.0018284839490014645\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0014000615794211627\n",
      "Average test loss: 0.0018215715609904792\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0013983805704758398\n",
      "Average test loss: 0.0018429599346386062\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0014028261932027008\n",
      "Average test loss: 0.0017877899861584107\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0013975912410145004\n",
      "Average test loss: 0.001859463385409779\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0013976648556482461\n",
      "Average test loss: 0.0018500676883591546\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0013978635309677984\n",
      "Average test loss: 0.001781883782086273\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0013951927892242868\n",
      "Average test loss: 0.001762178344445096\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0014009103311432732\n",
      "Average test loss: 0.001829293207368917\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0013933952738427453\n",
      "Average test loss: 0.001858109218068421\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0013906220550545388\n",
      "Average test loss: 0.001805603317502472\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0013964052206008799\n",
      "Average test loss: 0.0019010323620297842\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0013916343393632108\n",
      "Average test loss: 0.001823649891031285\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0013935408427690466\n",
      "Average test loss: 0.0017780229701764053\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0013916312397147218\n",
      "Average test loss: 0.0017982298143001067\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0013902874496868915\n",
      "Average test loss: 0.0017942908401083616\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0013938492466178205\n",
      "Average test loss: 0.0017925194942702849\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0013929493770831161\n",
      "Average test loss: 0.0017795723336231377\n",
      "Epoch 274/300\n",
      "Average training loss: 0.001386300498412715\n",
      "Average test loss: 0.0019029191479914718\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001389409613692098\n",
      "Average test loss: 0.001853651498651339\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0013880304625878731\n",
      "Average test loss: 0.002354352345897092\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0013887927620154289\n",
      "Average test loss: 0.001871316868604885\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0013874165475782421\n",
      "Average test loss: 0.0018613500401067236\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0013863764869877033\n",
      "Average test loss: 0.0018001949021385776\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0013869963177583285\n",
      "Average test loss: 0.0018189066055541237\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0013852521024851335\n",
      "Average test loss: 0.001850682218869527\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0013843066692352296\n",
      "Average test loss: 0.0018981532375845645\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0013883152091875672\n",
      "Average test loss: 0.001814617089720236\n",
      "Epoch 284/300\n",
      "Average training loss: 0.00138483253111028\n",
      "Average test loss: 0.0018198077801304558\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0013809965024184849\n",
      "Average test loss: 0.0018148847447915209\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0013886009632713265\n",
      "Average test loss: 0.0017827448638983898\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0013793857504303256\n",
      "Average test loss: 0.0018584907229782807\n",
      "Epoch 288/300\n",
      "Average training loss: 0.001378539167965452\n",
      "Average test loss: 0.001812544826625122\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0013821889581158758\n",
      "Average test loss: 0.001792444638804429\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0013811899382207128\n",
      "Average test loss: 0.0018593437301201953\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0013839842443250947\n",
      "Average test loss: 0.0017802786545621024\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0013791508658064736\n",
      "Average test loss: 0.0018665507986313768\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0013806946984388762\n",
      "Average test loss: 0.0018742632247093652\n",
      "Epoch 294/300\n",
      "Average training loss: 0.001375811895252102\n",
      "Average test loss: 0.0018082970720198419\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0013774817599397566\n",
      "Average test loss: 0.0018034428416026963\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0013847572368880113\n",
      "Average test loss: 0.0018245091514868868\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0013725163809334238\n",
      "Average test loss: 0.0021861061956733466\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0013834756873548032\n",
      "Average test loss: 0.0018522077380783027\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0013743995909786058\n",
      "Average test loss: 0.001937226130006214\n",
      "Epoch 300/300\n",
      "Average training loss: 0.001374252390737335\n",
      "Average test loss: 0.0018202110199878614\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.1/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 13.21\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 19.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 21.52\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 15.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.50\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.65\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.23\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 17.73\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.05\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 18.94\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.89\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.08921172893047333\n",
      "Average test loss: 0.05556458235449261\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015404069806138674\n",
      "Average test loss: 0.015731954554716748\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013579661950469018\n",
      "Average test loss: 0.01043929080499543\n",
      "Epoch 4/300\n",
      "Average training loss: 0.012342524657646814\n",
      "Average test loss: 0.01021995196160343\n",
      "Epoch 5/300\n",
      "Average training loss: 0.011452551224165492\n",
      "Average test loss: 0.018809722706675528\n",
      "Epoch 6/300\n",
      "Average training loss: 0.010612828117277887\n",
      "Average test loss: 0.03060338205595811\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009983162984665898\n",
      "Average test loss: 0.00839532863183154\n",
      "Epoch 8/300\n",
      "Average training loss: 0.009515324172874292\n",
      "Average test loss: 0.007871354819171958\n",
      "Epoch 9/300\n",
      "Average training loss: 0.00912248805248075\n",
      "Average test loss: 0.007813014616154962\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008697217533157931\n",
      "Average test loss: 0.00726207470231586\n",
      "Epoch 11/300\n",
      "Average training loss: 0.00828579681697819\n",
      "Average test loss: 0.027709326308634544\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00800088806077838\n",
      "Average test loss: 0.02153435931685898\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007844395180543263\n",
      "Average test loss: 0.05693574971291754\n",
      "Epoch 14/300\n",
      "Average training loss: 0.00765281946460406\n",
      "Average test loss: 0.008755088314000103\n",
      "Epoch 15/300\n",
      "Average training loss: 0.007497735802498129\n",
      "Average test loss: 0.006671940162777901\n",
      "Epoch 16/300\n",
      "Average training loss: 0.007333103218840229\n",
      "Average test loss: 0.24965221520264944\n",
      "Epoch 17/300\n",
      "Average training loss: 0.007211026007102595\n",
      "Average test loss: 1.3725123891135056\n",
      "Epoch 18/300\n",
      "Average training loss: 0.007099183173643218\n",
      "Average test loss: 0.06925031663642989\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0069767574850055905\n",
      "Average test loss: 0.03134034670723809\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006852569675279988\n",
      "Average test loss: 0.006248967307309309\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010112806451817353\n",
      "Average test loss: 0.007854296069178316\n",
      "Epoch 22/300\n",
      "Average training loss: 0.007840134580102232\n",
      "Average test loss: 0.006748346855656969\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007112516513715188\n",
      "Average test loss: 0.006384484185940689\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006892247684713867\n",
      "Average test loss: 0.006636221098403136\n",
      "Epoch 25/300\n",
      "Average training loss: 0.006811461910605431\n",
      "Average test loss: 0.00600660136838754\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006691213112324477\n",
      "Average test loss: 0.010859729008542167\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006581988448484077\n",
      "Average test loss: 0.005951411191787985\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006512667821927203\n",
      "Average test loss: 0.012719776978095372\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0064434991242984935\n",
      "Average test loss: 0.005848447622524368\n",
      "Epoch 30/300\n",
      "Average training loss: 0.006497050518790881\n",
      "Average test loss: 0.008628895262877146\n",
      "Epoch 31/300\n",
      "Average training loss: 0.006471320252451632\n",
      "Average test loss: 0.010481983840465546\n",
      "Epoch 32/300\n",
      "Average training loss: 0.006254814424448543\n",
      "Average test loss: 0.0603136161632008\n",
      "Epoch 33/300\n",
      "Average training loss: 0.006212241510964102\n",
      "Average test loss: 0.0056450213582979305\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006136033753554026\n",
      "Average test loss: 0.005757286989854442\n",
      "Epoch 35/300\n",
      "Average training loss: 0.006111620760626263\n",
      "Average test loss: 0.015015615918156174\n",
      "Epoch 36/300\n",
      "Average training loss: 0.006220711840937535\n",
      "Average test loss: 0.02763123742077086\n",
      "Epoch 37/300\n",
      "Average training loss: 0.006266027988659011\n",
      "Average test loss: 0.005810959647099177\n",
      "Epoch 38/300\n",
      "Average training loss: 0.006056988815466563\n",
      "Average test loss: 0.005585094555798504\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005958373316460186\n",
      "Average test loss: 0.005438810016132063\n",
      "Epoch 40/300\n",
      "Average training loss: 0.005881593628062142\n",
      "Average test loss: 0.006257555033597682\n",
      "Epoch 41/300\n",
      "Average training loss: 0.005854217485007312\n",
      "Average test loss: 1.7079164697229863\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005837707724215256\n",
      "Average test loss: 0.005432212873879406\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005851047104431523\n",
      "Average test loss: 0.05899137221566505\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005785635129652089\n",
      "Average test loss: 0.125844849380768\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0057481691520661115\n",
      "Average test loss: 0.08663703282343017\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0057122995410528446\n",
      "Average test loss: 0.2195437509533432\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005736998303896851\n",
      "Average test loss: 0.007832617017957899\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005683425049814913\n",
      "Average test loss: 0.023865722331735822\n",
      "Epoch 49/300\n",
      "Average training loss: 0.005669349517673254\n",
      "Average test loss: 0.25250418111847506\n",
      "Epoch 50/300\n",
      "Average training loss: 0.005619122113618586\n",
      "Average test loss: 0.24078223064728083\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0055979448366496294\n",
      "Average test loss: 0.5118129482037491\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005592146218650871\n",
      "Average test loss: 0.12681286743676498\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005547995211763515\n",
      "Average test loss: 0.9644765334626039\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00552289768639538\n",
      "Average test loss: 0.2868074338485797\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005552757079816527\n",
      "Average test loss: 10.747939494265864\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005499401829308933\n",
      "Average test loss: 0.16751749607920646\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005455662928521633\n",
      "Average test loss: 0.15896968294390373\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005436495053271453\n",
      "Average test loss: 0.04195809589119421\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005415949676185847\n",
      "Average test loss: 45.95683705488179\n",
      "Epoch 60/300\n",
      "Average training loss: 0.005376804809603427\n",
      "Average test loss: 0.04040116059448984\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005408843961440855\n",
      "Average test loss: 0.005279324048923122\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0053522241858558525\n",
      "Average test loss: 0.12004718290269376\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005330079584485955\n",
      "Average test loss: 2.8277145113216506\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005324385339601173\n",
      "Average test loss: 0.05036263079568744\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005295373911658923\n",
      "Average test loss: 0.015919513449072838\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00526867729301254\n",
      "Average test loss: 173.7411834668335\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005228449350843827\n",
      "Average test loss: 0.0053170593819684454\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005233408531794945\n",
      "Average test loss: 56.28269603205389\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005213641374475426\n",
      "Average test loss: 0.375654526318113\n",
      "Epoch 70/300\n",
      "Average training loss: 0.00517679792518417\n",
      "Average test loss: 0.27997142317104673\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005177769279521373\n",
      "Average test loss: 29.77707809759842\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005143246991352902\n",
      "Average test loss: 870.2558259109622\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00512350254588657\n",
      "Average test loss: 0.04623324687365028\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020088268457187548\n",
      "Average test loss: 0.13606036711070274\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01807947166264057\n",
      "Average test loss: 0.08686776284625133\n",
      "Epoch 76/300\n",
      "Average training loss: 0.010881806138488982\n",
      "Average test loss: 0.033082916537920636\n",
      "Epoch 77/300\n",
      "Average training loss: 0.00925022110508548\n",
      "Average test loss: 0.007643794855309857\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008400327555835247\n",
      "Average test loss: 0.007274251243306531\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007870311647239659\n",
      "Average test loss: 0.006713776773048772\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007485010232362482\n",
      "Average test loss: 0.006621174003514979\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007235422606269518\n",
      "Average test loss: 0.0067083171630899115\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006994688961240981\n",
      "Average test loss: 0.006786085674746169\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006764059861914979\n",
      "Average test loss: 0.32911639915241137\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006561347382350101\n",
      "Average test loss: 0.17177221577283408\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006410244744684961\n",
      "Average test loss: 0.005811566408516632\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006283894386970334\n",
      "Average test loss: 0.020585790716939504\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00616797745683127\n",
      "Average test loss: 0.006032745725578732\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006073416016581986\n",
      "Average test loss: 3.074938993324836\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00597487916962968\n",
      "Average test loss: 0.006095635304434432\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0059076279944015875\n",
      "Average test loss: 0.006686240326613187\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005841435672508346\n",
      "Average test loss: 0.18233550825880634\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005772695354289478\n",
      "Average test loss: 0.005467371836304665\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0057161665796819664\n",
      "Average test loss: 1.1438021622387071\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005664948946071995\n",
      "Average test loss: 0.006094935435387823\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005865064280728499\n",
      "Average test loss: 562.2438457668093\n",
      "Epoch 96/300\n",
      "Average training loss: 0.005471806236853202\n",
      "Average test loss: 4.369495494465033\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005408475867576069\n",
      "Average test loss: 0.8649151770803664\n",
      "Epoch 98/300\n",
      "Average training loss: 0.005369891106668446\n",
      "Average test loss: 152.08694296672277\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0052722746883001595\n",
      "Average test loss: 15.933078367259768\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005510136718965239\n",
      "Average test loss: 0.19378676958050992\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005227851937628454\n",
      "Average test loss: 0.6556766103423304\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005181598591307799\n",
      "Average test loss: 1.0410549121159647\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005213636863148875\n",
      "Average test loss: 263.9447848632004\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005184860608230034\n",
      "Average test loss: 17.33134240790208\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005284255877551105\n",
      "Average test loss: 6.082270194465088\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005186780397676759\n",
      "Average test loss: 17.2991545302603\n",
      "Epoch 107/300\n",
      "Average training loss: 0.005098952735049857\n",
      "Average test loss: 9.636945409360859\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005052410509851244\n",
      "Average test loss: 0.018716838735673164\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005043380147674017\n",
      "Average test loss: 1.9975785628325409\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005007375852101379\n",
      "Average test loss: 636.8508969336019\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00500093174022105\n",
      "Average test loss: 0.012130422913779816\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004978487256914377\n",
      "Average test loss: 0.6519587230777575\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004979771658778191\n",
      "Average test loss: 12.973425295225448\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004950208149643408\n",
      "Average test loss: 0.19494446428430576\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00493167252279818\n",
      "Average test loss: 4.60073902260926\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004901902886935406\n",
      "Average test loss: 0.005452751606702805\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004880150398032533\n",
      "Average test loss: 0.010666330645895667\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004898223283390204\n",
      "Average test loss: 10.410848421060377\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004847322147753503\n",
      "Average test loss: 0.7502902438205977\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004873253730022245\n",
      "Average test loss: 0.08332738895755674\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004832430911146932\n",
      "Average test loss: 11.894068470269442\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004821325704869297\n",
      "Average test loss: 1758.730856416571\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004825797395573722\n",
      "Average test loss: 101077.51318967014\n",
      "Epoch 124/300\n",
      "Average training loss: 0.004840222090275751\n",
      "Average test loss: 9.222244527634647\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004782084967320164\n",
      "Average test loss: 0.1933881841715839\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004809669286840492\n",
      "Average test loss: 35.806219954979504\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004746878938956393\n",
      "Average test loss: 9758.96048046875\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0047177457000232405\n",
      "Average test loss: 432.88594167294934\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004738348078810507\n",
      "Average test loss: 306.44352985223384\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0049612915571779015\n",
      "Average test loss: 0.0537115263649159\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004683046319418484\n",
      "Average test loss: 991.5925441285736\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004664802405569288\n",
      "Average test loss: 126.04718197599219\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0046968185651219555\n",
      "Average test loss: 4.771337523056401\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004659660173787011\n",
      "Average test loss: 0.6483919886855616\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004641613364012705\n",
      "Average test loss: 56.17673823817902\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0046295427073621085\n",
      "Average test loss: 0.006175984627670712\n",
      "Epoch 137/300\n",
      "Average training loss: 0.004613572693119446\n",
      "Average test loss: 0.22072952229446835\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0046148276242117085\n",
      "Average test loss: 0.007682793171869384\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004667759650697311\n",
      "Average test loss: 27864.93723462518\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004615892221116358\n",
      "Average test loss: 24261.86592092726\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004599549272822009\n",
      "Average test loss: 62.155484016718134\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0045745307685186466\n",
      "Average test loss: 0.07565087790828612\n",
      "Epoch 143/300\n",
      "Average training loss: 0.004562622425870763\n",
      "Average test loss: 184.88504482420618\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004541547706557645\n",
      "Average test loss: 5.513293287829806\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004530757946686613\n",
      "Average test loss: 0.005714658142791854\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004565944185687436\n",
      "Average test loss: 0.016671147969861827\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0045176996087862385\n",
      "Average test loss: 18.02563159841506\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004507956468396717\n",
      "Average test loss: 52.29248642244521\n",
      "Epoch 149/300\n",
      "Average training loss: 0.00454897298456894\n",
      "Average test loss: 8.76312840570675\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004530437997438842\n",
      "Average test loss: 0.0055993235382354924\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004506576309601466\n",
      "Average test loss: 418.46883112395267\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004474391650408506\n",
      "Average test loss: 1602.2328519038044\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0044569450165662505\n",
      "Average test loss: 0.5399357880900304\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0044587065229813256\n",
      "Average test loss: 101456.08609424869\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004439527690824535\n",
      "Average test loss: 13.503932918391294\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004464541356596682\n",
      "Average test loss: 0.00572786951996386\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004497782924936878\n",
      "Average test loss: 115.76622983103329\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004418963791595565\n",
      "Average test loss: 0.008385627546658118\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004442379582880272\n",
      "Average test loss: 0.005613046971460183\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0044006119958228535\n",
      "Average test loss: 564.4637829261422\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004380111682953106\n",
      "Average test loss: 92.188008382683\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0043757597050733035\n",
      "Average test loss: 885.3652978965292\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004412804616822137\n",
      "Average test loss: 0.00564100697884957\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004380934582402309\n",
      "Average test loss: 149.82073137384114\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004405042618927028\n",
      "Average test loss: 898610.7574631076\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004445207789747252\n",
      "Average test loss: 148.8492821536304\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0043825646481580206\n",
      "Average test loss: 5.501432594446673\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004371918935328722\n",
      "Average test loss: 66125.80919649274\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004378182662237022\n",
      "Average test loss: 0.0078845038248433\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004336664673768812\n",
      "Average test loss: 6534.815300614532\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004351718289570676\n",
      "Average test loss: 0.09797813315689564\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004350778130607472\n",
      "Average test loss: 2450.545240758227\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004319064019040929\n",
      "Average test loss: 1813.5050428428187\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004353860732581881\n",
      "Average test loss: 2.861218099806044\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0043035310657901895\n",
      "Average test loss: 0.10436987255513668\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004287208946628703\n",
      "Average test loss: 5.925528647633063\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004276987234337462\n",
      "Average test loss: 3.1239101553695896\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004286127256850402\n",
      "Average test loss: 3.8680429386115738\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004377571872745951\n",
      "Average test loss: 389.90475979804\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0042910448797047135\n",
      "Average test loss: 6569.689796031573\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004265233740624454\n",
      "Average test loss: 186.0689556681199\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0042407067976892\n",
      "Average test loss: 171.5705807514704\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004249760656928022\n",
      "Average test loss: 12212.620948630809\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004248444037925866\n",
      "Average test loss: 46.632144880568404\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004247538956503073\n",
      "Average test loss: 2314.4222930583896\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004238325500653849\n",
      "Average test loss: 4516.763559400956\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004248139065586859\n",
      "Average test loss: 1.1014428808026844\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004263275738805533\n",
      "Average test loss: 2975.7613309002686\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004260156642231676\n",
      "Average test loss: 0.928076219117062\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004269251792174246\n",
      "Average test loss: 4.803831156189036\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004241488846846752\n",
      "Average test loss: 0.005890238907395138\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004206549322232604\n",
      "Average test loss: 0.9795894417729643\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004201168655935261\n",
      "Average test loss: 5.148218275869886\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004206690732389689\n",
      "Average test loss: 0.006549854814385374\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0041822175996171105\n",
      "Average test loss: 39568.098366560276\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0041801734351449545\n",
      "Average test loss: 173.86520440771844\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004183309846454196\n",
      "Average test loss: 1956.9361406246787\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00417622555916508\n",
      "Average test loss: 5.1966399177548785\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004152277439418766\n",
      "Average test loss: 46883.17484586311\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004198803508033355\n",
      "Average test loss: 0.2798322056473957\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004263311420049932\n",
      "Average test loss: 0.4114886789437797\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004172398694273498\n",
      "Average test loss: 16541.001470269097\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004248801327207022\n",
      "Average test loss: 267.1742075659368\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004195992434811261\n",
      "Average test loss: 2.1396424378711316\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004207966222945187\n",
      "Average test loss: 15.5672858644757\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004186919723533922\n",
      "Average test loss: 0.10481823211080499\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0041680828117662\n",
      "Average test loss: 8.393825960992938\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004180699662201934\n",
      "Average test loss: 320.61490624218845\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004212897308377756\n",
      "Average test loss: 6354.027456874506\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004137268271918098\n",
      "Average test loss: 11.744791469348801\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004146807596915298\n",
      "Average test loss: 0.2165105813327763\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004118491432319085\n",
      "Average test loss: 0.22440768839170536\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004118072049278352\n",
      "Average test loss: 13158.607841978053\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004105271561692159\n",
      "Average test loss: 4.453802377303855\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0041219274943901435\n",
      "Average test loss: 0.5832295230676731\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004103204454398817\n",
      "Average test loss: 2651.2911246571657\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004200635864088933\n",
      "Average test loss: 12.135872124699254\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0042385141973694165\n",
      "Average test loss: 51.15440825940669\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004165831690861119\n",
      "Average test loss: 577515.8870981558\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004128828980649511\n",
      "Average test loss: 1799.6220582345907\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004091206222772598\n",
      "Average test loss: 138.91793540035852\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004085481830562155\n",
      "Average test loss: 529169.8637837302\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004139509698169099\n",
      "Average test loss: 0.7373446805377801\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004132992640551594\n",
      "Average test loss: 793.0336765622749\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004198126051988866\n",
      "Average test loss: 17.185408907635345\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0040701333048443\n",
      "Average test loss: 249.03028840160204\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004056978022472726\n",
      "Average test loss: 760595.8801556535\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004050454552388854\n",
      "Average test loss: 3168408.7187725697\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004057573608226246\n",
      "Average test loss: 707.4949410199208\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004040049782643716\n",
      "Average test loss: 3.6587182363698054\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004026258333275716\n",
      "Average test loss: 0.1498444835063484\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004054598420651423\n",
      "Average test loss: 0.01125470227189362\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004087078778280152\n",
      "Average test loss: 92392.05868242265\n",
      "Epoch 234/300\n",
      "Average training loss: 0.00415761967541443\n",
      "Average test loss: 1019.7377068247492\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004051135628380709\n",
      "Average test loss: 58.861709685221314\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0040157474722299314\n",
      "Average test loss: 35.043063077281744\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004005969619171487\n",
      "Average test loss: 2017.9086016008187\n",
      "Epoch 238/300\n",
      "Average training loss: 0.003992746860823697\n",
      "Average test loss: 0.15209609991146458\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004009354444013701\n",
      "Average test loss: 10.581727916111548\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0039911969237857396\n",
      "Average test loss: 18.939879014730455\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003977733318590456\n",
      "Average test loss: 44209.65036132905\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0039631354436278346\n",
      "Average test loss: 132781.5646402961\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0039927695615010125\n",
      "Average test loss: 585.1622954547977\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0039673370698259935\n",
      "Average test loss: 76.9022336022738\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003978668760508299\n",
      "Average test loss: 79.43547053497947\n",
      "Epoch 246/300\n",
      "Average training loss: 0.003961413017991517\n",
      "Average test loss: 1534.580642036953\n",
      "Epoch 247/300\n",
      "Average training loss: 0.003954701395705342\n",
      "Average test loss: 872521.9808689236\n",
      "Epoch 248/300\n",
      "Average training loss: 0.003950919303008252\n",
      "Average test loss: 4.711969768800669\n",
      "Epoch 249/300\n",
      "Average training loss: 0.003987507802744706\n",
      "Average test loss: 98150.39092761319\n",
      "Epoch 250/300\n",
      "Average training loss: 0.003958941381424666\n",
      "Average test loss: 59104429.14876678\n",
      "Epoch 251/300\n",
      "Average training loss: 0.003960388978322347\n",
      "Average test loss: 61847.121853304176\n",
      "Epoch 252/300\n",
      "Average training loss: 0.003951770366066032\n",
      "Average test loss: 257.1590895997369\n",
      "Epoch 253/300\n",
      "Average training loss: 0.003936019611441427\n",
      "Average test loss: 666615.5861011955\n",
      "Epoch 254/300\n",
      "Average training loss: 0.003969228584319353\n",
      "Average test loss: 615.9641313748724\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0039265177854233314\n",
      "Average test loss: 0.005654685636775361\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003918763698389133\n",
      "Average test loss: 517124.12577584945\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003952617385942075\n",
      "Average test loss: 800.2848619172362\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003912259709089995\n",
      "Average test loss: 623.3016995773427\n",
      "Epoch 259/300\n",
      "Average training loss: 0.003912702498750554\n",
      "Average test loss: 9006728.045577053\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00391522573472725\n",
      "Average test loss: 1410407.3270769257\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003914220662994517\n",
      "Average test loss: 27706.82457458618\n",
      "Epoch 262/300\n",
      "Average training loss: 0.003912684583415588\n",
      "Average test loss: 70400.59718623494\n",
      "Epoch 263/300\n",
      "Average training loss: 0.003917732834815979\n",
      "Average test loss: 386.56568360055735\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0039104440121187104\n",
      "Average test loss: 65448888.122724585\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004831524376653963\n",
      "Average test loss: 235307.06705215204\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004218163908769687\n",
      "Average test loss: 44833.06227292309\n",
      "Epoch 267/300\n",
      "Average training loss: 0.00393065582960844\n",
      "Average test loss: 46824.50555552883\n",
      "Epoch 268/300\n",
      "Average training loss: 0.003889407209638092\n",
      "Average test loss: 3.5010893659997318\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0038967439900669787\n",
      "Average test loss: 896.206250946631\n",
      "Epoch 270/300\n",
      "Average training loss: 0.003916017453289694\n",
      "Average test loss: 193.40800796120408\n",
      "Epoch 271/300\n",
      "Average training loss: 0.003919354028999805\n",
      "Average test loss: 1200.5983205694217\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0038939506953789126\n",
      "Average test loss: 35.518305829964994\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0039775301516056065\n",
      "Average test loss: 21.368464266440935\n",
      "Epoch 274/300\n",
      "Average training loss: 0.00394088693935838\n",
      "Average test loss: 184387.17661568456\n",
      "Epoch 275/300\n",
      "Average training loss: 0.003867239076230261\n",
      "Average test loss: 22412.181073280753\n",
      "Epoch 276/300\n",
      "Average training loss: 0.003876211419908537\n",
      "Average test loss: 79796801.2081927\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004658847098963128\n",
      "Average test loss: 0.835091892660492\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004140491645369265\n",
      "Average test loss: 6.266372777485185\n",
      "Epoch 279/300\n",
      "Average training loss: 0.003980249860220485\n",
      "Average test loss: 0.011635887967629564\n",
      "Epoch 280/300\n",
      "Average training loss: 0.003932446101680398\n",
      "Average test loss: 247.1871977225037\n",
      "Epoch 281/300\n",
      "Average training loss: 0.003919330658184158\n",
      "Average test loss: 121.564495576962\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0038857341872321235\n",
      "Average test loss: 377.171424743856\n",
      "Epoch 283/300\n",
      "Average training loss: 0.003903458146171437\n",
      "Average test loss: 15348.748934433801\n",
      "Epoch 284/300\n",
      "Average training loss: 0.003906574873253703\n",
      "Average test loss: 1348838.3381544682\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0038900294831643503\n",
      "Average test loss: 234.4054420486937\n",
      "Epoch 286/300\n",
      "Average training loss: 0.003873529582801792\n",
      "Average test loss: 7729.048735167185\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0038538650994499523\n",
      "Average test loss: 3.127061537054264\n",
      "Epoch 288/300\n",
      "Average training loss: 0.003852685110643506\n",
      "Average test loss: 54560.64408359516\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004175086878240109\n",
      "Average test loss: 790.7529467653699\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004083180397748947\n",
      "Average test loss: 700.9988403148192\n",
      "Epoch 291/300\n",
      "Average training loss: 0.003987376501162847\n",
      "Average test loss: 14394.343580944595\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00392333252955642\n",
      "Average test loss: 620.4484258280683\n",
      "Epoch 293/300\n",
      "Average training loss: 0.003942371163931158\n",
      "Average test loss: 29882.80429872144\n",
      "Epoch 294/300\n",
      "Average training loss: 0.003926240022190743\n",
      "Average test loss: 426.6100771773706\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0039050560548073714\n",
      "Average test loss: 232.65544004084046\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0038909347672015427\n",
      "Average test loss: 801444.4552441837\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003921748726939162\n",
      "Average test loss: 11367.242150896514\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003967537218497859\n",
      "Average test loss: 1487.3550503337185\n",
      "Epoch 299/300\n",
      "Average training loss: 0.003917765129150616\n",
      "Average test loss: 266415.8084698373\n",
      "Epoch 300/300\n",
      "Average training loss: 0.00392414434792267\n",
      "Average test loss: 3507871.4568055556\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.07395944648815526\n",
      "Average test loss: 0.07954253351357247\n",
      "Epoch 2/300\n",
      "Average training loss: 0.010181958661311203\n",
      "Average test loss: 0.01287396743065781\n",
      "Epoch 3/300\n",
      "Average training loss: 0.008593931540846825\n",
      "Average test loss: 0.007198443337033193\n",
      "Epoch 4/300\n",
      "Average training loss: 0.007769528645194239\n",
      "Average test loss: 0.006634230497810575\n",
      "Epoch 5/300\n",
      "Average training loss: 0.007118511531088088\n",
      "Average test loss: 0.03377302789936463\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006565227842993206\n",
      "Average test loss: 0.0054874177500605584\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0061477592471573085\n",
      "Average test loss: 0.0053697050164143246\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005817375302728679\n",
      "Average test loss: 0.004998469216956032\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0055929399848812155\n",
      "Average test loss: 0.023024982700745263\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005360259254773458\n",
      "Average test loss: 0.1903254811366399\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005164995197620657\n",
      "Average test loss: 0.004745391985194551\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004967316108859247\n",
      "Average test loss: 0.009057213869359758\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004796415912608306\n",
      "Average test loss: 0.01327051559338967\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004624313381397062\n",
      "Average test loss: 0.024157228744692272\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0044528008873264\n",
      "Average test loss: 0.00548937658758627\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004341675231854121\n",
      "Average test loss: 9.34650453848309\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00422163782061802\n",
      "Average test loss: 0.006289807396630446\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004114054775072469\n",
      "Average test loss: 0.003701035196168555\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004039668562304643\n",
      "Average test loss: 0.003623700647097495\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003964398429832525\n",
      "Average test loss: 0.003547102986317542\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0038956105609734854\n",
      "Average test loss: 0.005343776101867358\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003849393047185408\n",
      "Average test loss: 0.005281704510251681\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00381213073970543\n",
      "Average test loss: 0.004213985214630763\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0037026589732203218\n",
      "Average test loss: 0.0033552646359635723\n",
      "Epoch 25/300\n",
      "Average training loss: 0.003669400493717856\n",
      "Average test loss: 0.10752338454706802\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003594142548429469\n",
      "Average test loss: 0.003439745008945465\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0035527043139768973\n",
      "Average test loss: 0.020217015667301084\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0034958010309686263\n",
      "Average test loss: 0.009323438661379947\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0034797656639582583\n",
      "Average test loss: 0.06806941011134121\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0034373932120700676\n",
      "Average test loss: 1.2883943756777378\n",
      "Epoch 31/300\n",
      "Average training loss: 0.003396407586004999\n",
      "Average test loss: 0.2330527782847898\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0033710688402255377\n",
      "Average test loss: 0.7070487489915557\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0035486007084449133\n",
      "Average test loss: 0.0031965515828794905\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0033072856962680815\n",
      "Average test loss: 0.6119925410714414\n",
      "Epoch 35/300\n",
      "Average training loss: 0.00330581067999204\n",
      "Average test loss: 0.024204407748248842\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003296727605577972\n",
      "Average test loss: 0.029316977444622253\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0032617316829661527\n",
      "Average test loss: 0.1442476483558615\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005722057393234637\n",
      "Average test loss: 0.02533607581143992\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0036151345022436645\n",
      "Average test loss: 0.003437217306552662\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003529129261150956\n",
      "Average test loss: 0.003608946847418944\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003412467752686805\n",
      "Average test loss: 0.041770438969963124\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0033366765464759537\n",
      "Average test loss: 0.24963323640781973\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0033200426443169514\n",
      "Average test loss: 0.4990962688819402\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003293508733312289\n",
      "Average test loss: 14.942556005701423\n",
      "Epoch 45/300\n",
      "Average training loss: 0.003292013848821322\n",
      "Average test loss: 0.007473486265581515\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003401945534472664\n",
      "Average test loss: 2.8195765931970542\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0032460912914118833\n",
      "Average test loss: 0.00420986065434085\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0036030517862074905\n",
      "Average test loss: 9.604165874634353\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0032678331076684925\n",
      "Average test loss: 0.014932986919457714\n",
      "Epoch 50/300\n",
      "Average training loss: 0.003211694388339917\n",
      "Average test loss: 1.1293743343812723\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003181662602453596\n",
      "Average test loss: 3.329535672656364\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0031675830541385546\n",
      "Average test loss: 18.5580316548364\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0031409361052016417\n",
      "Average test loss: 0.45476936478416125\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003160279954473178\n",
      "Average test loss: 4.062612740431395\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003180517901562982\n",
      "Average test loss: 0.007324173119333055\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0031301406255612773\n",
      "Average test loss: 2.62101037391627\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0030733541869040993\n",
      "Average test loss: 81755.58008269186\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0030564716164436607\n",
      "Average test loss: 33.51007941362593\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003044898794963956\n",
      "Average test loss: 1.4560060801063146\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0030335199596981206\n",
      "Average test loss: 0.24334239722539983\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0030096866140762965\n",
      "Average test loss: 8.41458678075423\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003006984009924862\n",
      "Average test loss: 0.4442354644578364\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002980812857341435\n",
      "Average test loss: 367.622099034066\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0029625615860439007\n",
      "Average test loss: 1273.4154382483305\n",
      "Epoch 65/300\n",
      "Average training loss: 0.004788485936613547\n",
      "Average test loss: 0.010230903618451621\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0034924467446075546\n",
      "Average test loss: 250.4349200161414\n",
      "Epoch 67/300\n",
      "Average training loss: 0.003220373808923695\n",
      "Average test loss: 8.118080877912542\n",
      "Epoch 68/300\n",
      "Average training loss: 0.003095362486731675\n",
      "Average test loss: 0.1553550928177105\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0030237677074554893\n",
      "Average test loss: 926.5140168910788\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0029823963830454482\n",
      "Average test loss: 8.403761398836142\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0029583715290452044\n",
      "Average test loss: 3.298784493408683\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002942041590395901\n",
      "Average test loss: 0.1619731787627356\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0029307112546844614\n",
      "Average test loss: 0.7731349063511523\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0029146144036203622\n",
      "Average test loss: 0.9844931144389427\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0028955617615332207\n",
      "Average test loss: 505.2204322164315\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002886556001173125\n",
      "Average test loss: 0.003300355842543973\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0029113187760942513\n",
      "Average test loss: 0.0035014425387812987\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002865174344016446\n",
      "Average test loss: 16.912883636429907\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0028548345116691455\n",
      "Average test loss: 29283.488462622747\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002841566762783461\n",
      "Average test loss: 3.323190514792792\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0028248002318044503\n",
      "Average test loss: 0.005749879601515002\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0028326216464241347\n",
      "Average test loss: 0.26178736260740293\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002812390520547827\n",
      "Average test loss: 0.41669957653412387\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002790975955211454\n",
      "Average test loss: 272306.1463637153\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002775867681329449\n",
      "Average test loss: 234.12631389723552\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0027834935556683275\n",
      "Average test loss: 23.5254475807084\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0027865194969086183\n",
      "Average test loss: 1.480329117100272\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0027566003089563714\n",
      "Average test loss: 18.17920575817757\n",
      "Epoch 89/300\n",
      "Average training loss: 0.006036881474571096\n",
      "Average test loss: 0.01500394893768761\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005505561436629958\n",
      "Average test loss: 0.003934545891152488\n",
      "Epoch 91/300\n",
      "Average training loss: 0.004013683680444956\n",
      "Average test loss: 0.0035720707985262075\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0036573436541689768\n",
      "Average test loss: 0.003917422047919697\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0034502994995564223\n",
      "Average test loss: 0.05785306845605374\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0032816206564505895\n",
      "Average test loss: 0.0033395601709683737\n",
      "Epoch 95/300\n",
      "Average training loss: 0.003148609390689267\n",
      "Average test loss: 0.0042471522823390034\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0030368975879003605\n",
      "Average test loss: 41.78997628083991\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0031263741277572183\n",
      "Average test loss: 17.79584861565257\n",
      "Epoch 98/300\n",
      "Average training loss: 0.002981368300608463\n",
      "Average test loss: 179.1006152026049\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0029025961825003225\n",
      "Average test loss: 5.6620719518297244\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0028493520458125407\n",
      "Average test loss: 185.1970427144708\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0028656132265718445\n",
      "Average test loss: 65.77825003348126\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002823875301103625\n",
      "Average test loss: 549.204856672439\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002951316518088182\n",
      "Average test loss: 26.14187383126902\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002807364473533299\n",
      "Average test loss: 18333.23795405025\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002770684008590049\n",
      "Average test loss: 1717.9413303536776\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0028017246536910533\n",
      "Average test loss: 2618.827129564278\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002763595248055127\n",
      "Average test loss: 166.22962644455333\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0027140838280320166\n",
      "Average test loss: 98199.24959870275\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002687492391922408\n",
      "Average test loss: 178.43837127025756\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0026820628117356035\n",
      "Average test loss: 1812.759985681826\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002676596476799912\n",
      "Average test loss: 1.2597013281687266\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0026654889302121267\n",
      "Average test loss: 877.7903980671426\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0026702160050885546\n",
      "Average test loss: 1225.483870422787\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0026584085958699386\n",
      "Average test loss: 344.0424252682908\n",
      "Epoch 115/300\n",
      "Average training loss: 0.002641276666894555\n",
      "Average test loss: 31404.716659977137\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0026243589545289674\n",
      "Average test loss: 9.956282720674658\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003578255782317784\n",
      "Average test loss: 15960.256853491985\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0032282949882662957\n",
      "Average test loss: 27.6532929692852\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0028526171915647055\n",
      "Average test loss: 0.011094782591693931\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0027172352692319286\n",
      "Average test loss: 41126.69350782342\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0026632048055115673\n",
      "Average test loss: 166.90176964696911\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002614660845655534\n",
      "Average test loss: 78.8468521067734\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0026113961652542155\n",
      "Average test loss: 31787.627981987847\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002587388643994927\n",
      "Average test loss: 0.30613011341955926\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0025807556379586457\n",
      "Average test loss: 4.018644861765206\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0025813237723583977\n",
      "Average test loss: 0.6497792435636123\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002571245074686077\n",
      "Average test loss: 35946.62124865621\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0026186957731842994\n",
      "Average test loss: 0.006088089432153437\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0025704754712060096\n",
      "Average test loss: 2807.836450188973\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002564002130387558\n",
      "Average test loss: 4.581010053468248\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0025624950817889638\n",
      "Average test loss: 397.3103491339816\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002645793133725723\n",
      "Average test loss: 50800.8655659764\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0028056319651918278\n",
      "Average test loss: 5651.724264382976\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002647779502388504\n",
      "Average test loss: 740.5587852858802\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0026120533060489427\n",
      "Average test loss: 13.687384310627149\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0026067986492481496\n",
      "Average test loss: 9160229.691578887\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00263108121479551\n",
      "Average test loss: 754.5440342793005\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0027089260282615823\n",
      "Average test loss: 98870.2145947952\n",
      "Epoch 139/300\n",
      "Average training loss: 0.002581519895336694\n",
      "Average test loss: 38899.170676534755\n",
      "Epoch 140/300\n",
      "Average training loss: 0.002581296137120161\n",
      "Average test loss: 14238.791503211047\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0025601933699929053\n",
      "Average test loss: 2044.0929932812849\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002587177250120375\n",
      "Average test loss: 0.038512033852438135\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0025569537786973846\n",
      "Average test loss: 5517.298701566584\n",
      "Epoch 144/300\n",
      "Average training loss: 0.002550940392124984\n",
      "Average test loss: 495.31149997623845\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002537415107505189\n",
      "Average test loss: 306.5180839256014\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002545249804026551\n",
      "Average test loss: 0.4165338059645146\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0025501269089678925\n",
      "Average test loss: 131.40966646936576\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002534600479528308\n",
      "Average test loss: 262.0588767079678\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0025193955918980967\n",
      "Average test loss: 254.47297288695867\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0024755288482540185\n",
      "Average test loss: 61.011596342619924\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0024716928311520154\n",
      "Average test loss: 291.4202984529655\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0024481892490552533\n",
      "Average test loss: 3.381604368268823\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0024411714459872907\n",
      "Average test loss: 640.3996641103228\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0024420203141247232\n",
      "Average test loss: 5835453.92941767\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0024783572043395705\n",
      "Average test loss: 0.005062196827183167\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0024402452448589935\n",
      "Average test loss: 527.7684364761114\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0024206440607085823\n",
      "Average test loss: 225.79462461844253\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0024149142613427506\n",
      "Average test loss: 7747.162264121137\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0024138214664740695\n",
      "Average test loss: 147.47110127523416\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0024033493428594534\n",
      "Average test loss: 25104.18441443256\n",
      "Epoch 161/300\n",
      "Average training loss: 0.002395901102365719\n",
      "Average test loss: 178.32249809243945\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0024365213045643434\n",
      "Average test loss: 0.2126582660401861\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002390462377005153\n",
      "Average test loss: 3150.2677749573127\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0024057548405188654\n",
      "Average test loss: 0.01341203699612783\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0024075925608889924\n",
      "Average test loss: 0.8564548108374906\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002434628112345106\n",
      "Average test loss: 226.63763516092217\n",
      "Epoch 167/300\n",
      "Average training loss: 0.002598478291494151\n",
      "Average test loss: 3.346516620144248\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002540864598212971\n",
      "Average test loss: 0.0032296006878217063\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002492142833976282\n",
      "Average test loss: 63.77490757272186\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0024763990194640226\n",
      "Average test loss: 0.037375922971715526\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002560160355642438\n",
      "Average test loss: 542.5069869723618\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0024452809426519606\n",
      "Average test loss: 45.413944794612625\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0024231178011331293\n",
      "Average test loss: 486.4992464021047\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0023975102081894876\n",
      "Average test loss: 0.4353800390969134\n",
      "Epoch 175/300\n",
      "Average training loss: 0.002393442653533485\n",
      "Average test loss: 1463.1038069661458\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002372044839585821\n",
      "Average test loss: 0.022947563532532918\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0023688266488413015\n",
      "Average test loss: 22232.449179275405\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0024053270862334306\n",
      "Average test loss: 75.55189737332695\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0024796038332084813\n",
      "Average test loss: 429058.20512847224\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002568623888823721\n",
      "Average test loss: 41.25233193761607\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0024213722362700437\n",
      "Average test loss: 0.010029992476519612\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0024086816246724793\n",
      "Average test loss: 31.15387837891943\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0023949344938413966\n",
      "Average test loss: 151.49090350706877\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0024085655036485856\n",
      "Average test loss: 4339.172481221354\n",
      "Epoch 185/300\n",
      "Average training loss: 0.002704736595352491\n",
      "Average test loss: 0.44750745017681687\n",
      "Epoch 186/300\n",
      "Average training loss: 0.002481221860171192\n",
      "Average test loss: 6848085.369416666\n",
      "Epoch 187/300\n",
      "Average training loss: 0.002462741641120778\n",
      "Average test loss: 2.9162174520003092\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0023962414043231144\n",
      "Average test loss: 124.78503669926856\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0023761263272414606\n",
      "Average test loss: 36513.363291988695\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0023814279461900393\n",
      "Average test loss: 728.0012543455354\n",
      "Epoch 191/300\n",
      "Average training loss: 0.00235990120160083\n",
      "Average test loss: 3307.6645030496657\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0023866847296141916\n",
      "Average test loss: 433658.264136811\n",
      "Epoch 193/300\n",
      "Average training loss: 0.002445219387817714\n",
      "Average test loss: 9.558156690725747\n",
      "Epoch 194/300\n",
      "Average training loss: 0.002392496251190702\n",
      "Average test loss: 0.01652524478899108\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0023816570411953663\n",
      "Average test loss: 52567.33767381144\n",
      "Epoch 196/300\n",
      "Average training loss: 0.002368287768835823\n",
      "Average test loss: 193.78547983387526\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0023773137653867404\n",
      "Average test loss: 54982.06974359008\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002357832354389959\n",
      "Average test loss: 27.229930635516325\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0023517379733837313\n",
      "Average test loss: 10.030446013909247\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0026166361783527665\n",
      "Average test loss: 728996.4101186438\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0023716628652893836\n",
      "Average test loss: 266451.1824683173\n",
      "Epoch 202/300\n",
      "Average training loss: 0.002356767929262585\n",
      "Average test loss: 49.62759963505012\n",
      "Epoch 203/300\n",
      "Average training loss: 0.002447506924677226\n",
      "Average test loss: 15.024308410939243\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0024793592370632622\n",
      "Average test loss: 87.66508612891742\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0023734435064511165\n",
      "Average test loss: 40147.936554559594\n",
      "Epoch 206/300\n",
      "Average training loss: 0.002333552875245611\n",
      "Average test loss: 3521.7360852028\n",
      "Epoch 207/300\n",
      "Average training loss: 0.002328618438914418\n",
      "Average test loss: 29972824.873948786\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002356996023820506\n",
      "Average test loss: 65298.113019443794\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0023435888290405274\n",
      "Average test loss: 3757324879.4168887\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0023373872881962193\n",
      "Average test loss: 69087.08363964559\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002342872913305958\n",
      "Average test loss: 908.9753049805032\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0023357542670435377\n",
      "Average test loss: 9577.497719267825\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0023069481609596146\n",
      "Average test loss: 1042.0657201075896\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002324457577740153\n",
      "Average test loss: 81741.21840549824\n",
      "Epoch 215/300\n",
      "Average training loss: 0.002298053176038795\n",
      "Average test loss: 35294823.57234898\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002356508316886094\n",
      "Average test loss: 21.30239468031708\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0023107282399303384\n",
      "Average test loss: 11557.336595848572\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0022759416554537083\n",
      "Average test loss: 72.36219134085212\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0023632674189284445\n",
      "Average test loss: 6.539907034444313\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002326831274251971\n",
      "Average test loss: 47476.96823422629\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0023022976141009064\n",
      "Average test loss: 424016.82879166666\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002279064782584707\n",
      "Average test loss: 0.3483615754637867\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0022818867098540066\n",
      "Average test loss: 698845.2594846599\n",
      "Epoch 224/300\n",
      "Average training loss: 0.002288641863192121\n",
      "Average test loss: 0.0037241126315461265\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002272610242995951\n",
      "Average test loss: 1166.3462512400233\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002275714663995637\n",
      "Average test loss: 1128.2067727446697\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002407292867286338\n",
      "Average test loss: 26.69492382810265\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002361275981490811\n",
      "Average test loss: 3078716.1564039765\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002283370345003075\n",
      "Average test loss: 18808.859515184467\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0023511339891701937\n",
      "Average test loss: 38.33438487477394\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0024001204603248173\n",
      "Average test loss: 6.673981504334344\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0023155340184975\n",
      "Average test loss: 30302.794703814376\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0023221502490341664\n",
      "Average test loss: 199.6023714212494\n",
      "Epoch 234/300\n",
      "Average training loss: 0.002309885202596585\n",
      "Average test loss: 1.8716850316379634\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0022754457705757686\n",
      "Average test loss: 0.04212894728283088\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0022615435949216284\n",
      "Average test loss: 2764.86958603144\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0022370828728501997\n",
      "Average test loss: 18.531787674993485\n",
      "Epoch 238/300\n",
      "Average training loss: 0.002249865902691252\n",
      "Average test loss: 6644.890994413073\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0022364315506484774\n",
      "Average test loss: 430.4657040083094\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0022383275454243025\n",
      "Average test loss: 21546.654193779046\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0022641175660408205\n",
      "Average test loss: 141.80498300524448\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00232051089219749\n",
      "Average test loss: 287.9271612752568\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0022210958403431707\n",
      "Average test loss: 13.889476570496129\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002204777909235822\n",
      "Average test loss: 83.43128768594853\n",
      "Epoch 245/300\n",
      "Average training loss: 0.002209236693672008\n",
      "Average test loss: 107059.96311082604\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0022080404789497454\n",
      "Average test loss: 2897.7014098013624\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0022350778399656216\n",
      "Average test loss: 28.06645503114495\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0022103761558731395\n",
      "Average test loss: 195.02459216509635\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0021945444068147075\n",
      "Average test loss: 157.64732459947942\n",
      "Epoch 250/300\n",
      "Average training loss: 0.002190349551124705\n",
      "Average test loss: 89892.51188389758\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002198570058681071\n",
      "Average test loss: 1015.7902594426291\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0023536839208876093\n",
      "Average test loss: 6059274.375449329\n",
      "Epoch 253/300\n",
      "Average training loss: 0.002186016415970193\n",
      "Average test loss: 0.006658858552575112\n",
      "Epoch 254/300\n",
      "Average training loss: 0.002184681221842766\n",
      "Average test loss: 35942.48738023972\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0021718003108269638\n",
      "Average test loss: 27618.670547674392\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0021780011837060254\n",
      "Average test loss: 1.3002112185532848\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002170981495744652\n",
      "Average test loss: 11155.462415890988\n",
      "Epoch 258/300\n",
      "Average training loss: 0.002198644415785869\n",
      "Average test loss: 7769.038020624384\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00217650393044783\n",
      "Average test loss: 31.903574114569153\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002169308975131975\n",
      "Average test loss: 2023.9911405219345\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002169908019610577\n",
      "Average test loss: 188325.9899697246\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0021827874891459944\n",
      "Average test loss: 18.408879942939098\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0021583293422849643\n",
      "Average test loss: 2.969029492130917\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0021598509611665375\n",
      "Average test loss: 141.2640897237551\n",
      "Epoch 265/300\n",
      "Average training loss: 0.002161565436050296\n",
      "Average test loss: 333.3369240400369\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0021526773071123494\n",
      "Average test loss: 53.50900049074988\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0021701962964402305\n",
      "Average test loss: 2222379.100141071\n",
      "Epoch 268/300\n",
      "Average training loss: 0.002188529865609275\n",
      "Average test loss: 177.09357656842388\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002146835977832476\n",
      "Average test loss: 18.695703891598516\n",
      "Epoch 270/300\n",
      "Average training loss: 0.002144245734024379\n",
      "Average test loss: 1307.0658697989104\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0021947947748833232\n",
      "Average test loss: 32596.55599977181\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0021603587344288825\n",
      "Average test loss: 10709.845388425108\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0021515414031843343\n",
      "Average test loss: 5338.208303556088\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002141708894011875\n",
      "Average test loss: 21.256897852896195\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002165795678065883\n",
      "Average test loss: 532657.7445554233\n",
      "Epoch 276/300\n",
      "Average training loss: 0.002144495969845189\n",
      "Average test loss: 42379246.34933333\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0021447941199358965\n",
      "Average test loss: 108009.54855016587\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0021415131346633038\n",
      "Average test loss: 6591.040081970553\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0021318515182162327\n",
      "Average test loss: 6408.702851022942\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002126856423448771\n",
      "Average test loss: 1.7891821535329024\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0021180558391950197\n",
      "Average test loss: 0.06381370886208283\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002119922743489345\n",
      "Average test loss: 148732.97609869097\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002132165403208799\n",
      "Average test loss: 768.3496433923509\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002114687851526671\n",
      "Average test loss: 362.65509274783193\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0021160170678049327\n",
      "Average test loss: 3159545.3586876155\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0021256561728401316\n",
      "Average test loss: 224.27271378551754\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0021302774457467926\n",
      "Average test loss: 3395.5887687385116\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0021081734899845387\n",
      "Average test loss: 0.003770488743773765\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002101888008415699\n",
      "Average test loss: 8.583321240719098\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003290722706251674\n",
      "Average test loss: 173152.69290984343\n",
      "Epoch 291/300\n",
      "Average training loss: 0.003323802042959465\n",
      "Average test loss: 64587.53273437459\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002534968020601405\n",
      "Average test loss: 641.714443014972\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002320993354751004\n",
      "Average test loss: 0.0034151450614962314\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0022242101234280403\n",
      "Average test loss: 4743.55515897256\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0021566230691969393\n",
      "Average test loss: 9.670757134006669\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0021281324815418984\n",
      "Average test loss: 2247.887293525617\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0021926338338396616\n",
      "Average test loss: 757.870674910658\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0021865315463186967\n",
      "Average test loss: 129.15982857534456\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0021856412647498977\n",
      "Average test loss: 475910.7459438598\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0021551547876248757\n",
      "Average test loss: 927831.2229258665\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06491732887675365\n",
      "Average test loss: 0.010114208689994282\n",
      "Epoch 2/300\n",
      "Average training loss: 0.007783543301125367\n",
      "Average test loss: 0.006213613558146688\n",
      "Epoch 3/300\n",
      "Average training loss: 0.006373325272152821\n",
      "Average test loss: 0.005007702436298132\n",
      "Epoch 4/300\n",
      "Average training loss: 0.005648988832202223\n",
      "Average test loss: 0.004547253479560216\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005175932314246893\n",
      "Average test loss: 0.00721229164633486\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004792687466575039\n",
      "Average test loss: 0.0039089073540849815\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004487100598298841\n",
      "Average test loss: 0.005479575135641628\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0042171221189200876\n",
      "Average test loss: 0.024033309674097433\n",
      "Epoch 9/300\n",
      "Average training loss: 0.003993206329229805\n",
      "Average test loss: 0.6707511624942223\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0037993293890936507\n",
      "Average test loss: 0.04788632313162088\n",
      "Epoch 11/300\n",
      "Average training loss: 0.003627034753147099\n",
      "Average test loss: 0.0035234591151691147\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0034507088708794777\n",
      "Average test loss: 0.11841618171334267\n",
      "Epoch 13/300\n",
      "Average training loss: 0.003318844317769011\n",
      "Average test loss: 0.011334400720894337\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0031966994471020168\n",
      "Average test loss: 0.16466679404489695\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003084860772929258\n",
      "Average test loss: 0.022635321139461463\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002981213778671291\n",
      "Average test loss: 0.22780004837115606\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0028951249391668374\n",
      "Average test loss: 0.015206482732461559\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0028294727603594464\n",
      "Average test loss: 2.0905176148932014\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0027340838480740784\n",
      "Average test loss: 0.13191445184085104\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0026556278706217806\n",
      "Average test loss: 0.003050367840876182\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002618952885477079\n",
      "Average test loss: 0.6663069221875113\n",
      "Epoch 22/300\n",
      "Average training loss: 0.002592226376136144\n",
      "Average test loss: 40.479647372600105\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00250065595873942\n",
      "Average test loss: 0.36820803945449493\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0024519257517531512\n",
      "Average test loss: 0.005859770148992538\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0024074198857156766\n",
      "Average test loss: 11.426215217726925\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0033067951740490065\n",
      "Average test loss: 0.026713064559424918\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0025476121614790626\n",
      "Average test loss: 3621.7030180630154\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002446481763695677\n",
      "Average test loss: 7.874754449508464\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0023988789591110416\n",
      "Average test loss: 0.29658159366457\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002367738376888964\n",
      "Average test loss: 3.8459821007251738\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00234347293080969\n",
      "Average test loss: 4.482567236117191\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0023175851568165754\n",
      "Average test loss: 83.10336969222212\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0022967132271991837\n",
      "Average test loss: 0.014139888026234176\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0023013084854723678\n",
      "Average test loss: 0.033832268650333085\n",
      "Epoch 35/300\n",
      "Average training loss: 0.005394847023404307\n",
      "Average test loss: 47.19313868644668\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003052100493055251\n",
      "Average test loss: 3859.060854636482\n",
      "Epoch 37/300\n",
      "Average training loss: 0.002672212423963679\n",
      "Average test loss: 9.557447673453225\n",
      "Epoch 38/300\n",
      "Average training loss: 0.002529665035299129\n",
      "Average test loss: 52.50428613103098\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0024635337580823234\n",
      "Average test loss: 2977.0470823687174\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0023822938212090068\n",
      "Average test loss: 124.97930041308204\n",
      "Epoch 41/300\n",
      "Average training loss: 0.002349895625685652\n",
      "Average test loss: 0.20317021674041946\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002313924280512664\n",
      "Average test loss: 0.01718065914387504\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002304146059167882\n",
      "Average test loss: 0.024164325137105255\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002288303743323518\n",
      "Average test loss: 56.60459011705882\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0023000244594489536\n",
      "Average test loss: 0.4061390135296517\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0022448898333435256\n",
      "Average test loss: 0.18587363647462593\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002202069307780928\n",
      "Average test loss: 3.717418034892736\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0022574510253551934\n",
      "Average test loss: 8.829256885022753\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0021866076208858025\n",
      "Average test loss: 0.002135832670972579\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0023115084678348567\n",
      "Average test loss: 172.53414433293665\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003455763342479865\n",
      "Average test loss: 2.0006442796902526\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0028711466118693353\n",
      "Average test loss: 0.013685091618655457\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0024561352429704536\n",
      "Average test loss: 0.19217229778878392\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0023515173630375003\n",
      "Average test loss: 0.07836102700957821\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002298494567266769\n",
      "Average test loss: 0.5872804358535343\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002249212203029957\n",
      "Average test loss: 0.027708051346242428\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0022210482999475466\n",
      "Average test loss: 0.2138751658755872\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00219525515443335\n",
      "Average test loss: 4.070373066826827\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002175190596220394\n",
      "Average test loss: 154.20701917381584\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002171067958904637\n",
      "Average test loss: 1.3442984120027266\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0021622831969418457\n",
      "Average test loss: 143.8012191211722\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002141393668949604\n",
      "Average test loss: 3.9880441428708533\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0021447414886206387\n",
      "Average test loss: 3.7473150150891805\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0021133620520639752\n",
      "Average test loss: 1.523090573505809\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002096503633177943\n",
      "Average test loss: 0.07324299704117908\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002207711602975097\n",
      "Average test loss: 0.003627657437697053\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00209205845826202\n",
      "Average test loss: 0.00428475327996744\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002079020955082443\n",
      "Average test loss: 2.546548986034675\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002377765315170917\n",
      "Average test loss: 0.8122614224321313\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002063620045160254\n",
      "Average test loss: 16.423726273810697\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0020426324180637797\n",
      "Average test loss: 180.79834909626365\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0020417027248897488\n",
      "Average test loss: 13.887222444122864\n",
      "Epoch 73/300\n",
      "Average training loss: 0.002045960094469289\n",
      "Average test loss: 0.4217885545703272\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00203783260534207\n",
      "Average test loss: 37.896315528722276\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0020045907087624074\n",
      "Average test loss: 5.227675266450478\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002008057859312329\n",
      "Average test loss: 634087.5724290065\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002013915004312164\n",
      "Average test loss: 2273.709233707571\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002042446690508061\n",
      "Average test loss: 0.0887040230664942\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0019747568589324753\n",
      "Average test loss: 73.71563549140717\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0020150516440884934\n",
      "Average test loss: 63.97937410443442\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00196471148967329\n",
      "Average test loss: 64.18233688790185\n",
      "Epoch 82/300\n",
      "Average training loss: 0.001945826891809702\n",
      "Average test loss: 1016.5128717705517\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0019214480979782012\n",
      "Average test loss: 19.588607787794537\n",
      "Epoch 84/300\n",
      "Average training loss: 0.001933348724515074\n",
      "Average test loss: 0.2197095992302315\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002533520150722729\n",
      "Average test loss: 0.5163968266439106\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002184301700236069\n",
      "Average test loss: 0.0730071459867888\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0021143974570764436\n",
      "Average test loss: 28.93931791561842\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0021193803139030933\n",
      "Average test loss: 1.3902178704874175\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0020220221588388085\n",
      "Average test loss: 105.97580334250546\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002011929015835954\n",
      "Average test loss: 9858.706604817304\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0019702064255252482\n",
      "Average test loss: 936.1564124848437\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0019411012815932433\n",
      "Average test loss: 6.074925956713553\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002017785454272396\n",
      "Average test loss: 355.8430633863244\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002210570356498162\n",
      "Average test loss: 1233.2767106957597\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0019429095910034246\n",
      "Average test loss: 0.05863785766520434\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0019334824773379498\n",
      "Average test loss: 6.228500136210687\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0019290276885860495\n",
      "Average test loss: 0.34409647920810515\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0019316035997536448\n",
      "Average test loss: 15626.328428328945\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0019454163837557038\n",
      "Average test loss: 0.6713998341812856\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0019082719090705116\n",
      "Average test loss: 2287.435683851731\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0018716988288280036\n",
      "Average test loss: 0.10910326706121365\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0018832387971795268\n",
      "Average test loss: 1628.846250408094\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0018914944221162134\n",
      "Average test loss: 1.3536304686038962\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0018678284909369217\n",
      "Average test loss: 179.82709960054538\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0018720813458785416\n",
      "Average test loss: 2053.314111972956\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002248368035484519\n",
      "Average test loss: 0.01984101930446923\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0019862545064340035\n",
      "Average test loss: 670.1981019756322\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0019102448234334587\n",
      "Average test loss: 22.666348854247065\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0018800460220211083\n",
      "Average test loss: 16996.99112434562\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0018570827837102114\n",
      "Average test loss: 883.365759880422\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0018780261601010959\n",
      "Average test loss: 14.375247420163618\n",
      "Epoch 112/300\n",
      "Average training loss: 0.00189497854332957\n",
      "Average test loss: 1.1407830003069506\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0018914112277949849\n",
      "Average test loss: 150.11754484400817\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0018778627333748672\n",
      "Average test loss: 1.2588184260142345\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0018552246938149134\n",
      "Average test loss: 217.77649102931554\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0018206577757373453\n",
      "Average test loss: 13.430185075312233\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0018323775941712988\n",
      "Average test loss: 25333.116804008467\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00930976360405071\n",
      "Average test loss: 0.014244545557432705\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0024520140749712785\n",
      "Average test loss: 11.322075235676435\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0022799731907952162\n",
      "Average test loss: 0.022264903403404687\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0021821094039413663\n",
      "Average test loss: 40.86517991735124\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0021268328374458682\n",
      "Average test loss: 0.30794933513624384\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002076713283649749\n",
      "Average test loss: 0.33232554599363356\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002037082035508421\n",
      "Average test loss: 0.013336234098093378\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0020077810840060314\n",
      "Average test loss: 34.38779940793498\n",
      "Epoch 126/300\n",
      "Average training loss: 0.001975594212094115\n",
      "Average test loss: 0.08565579798486497\n",
      "Epoch 127/300\n",
      "Average training loss: 0.001956883286022478\n",
      "Average test loss: 4.173604444751516\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0019334605860834321\n",
      "Average test loss: 28.02970089477549\n",
      "Epoch 129/300\n",
      "Average training loss: 0.001915027762659722\n",
      "Average test loss: 106.96009929585374\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0018967694707421794\n",
      "Average test loss: 0.004149955882380406\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0018751502343349986\n",
      "Average test loss: 2.870463027967347\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0018602143321186303\n",
      "Average test loss: 217.81080123722884\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0018471860643476247\n",
      "Average test loss: 2767.768018009656\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0018448282461613417\n",
      "Average test loss: 0.005353246653245555\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0018192854672670364\n",
      "Average test loss: 8.656881461612052\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0018081836813232965\n",
      "Average test loss: 1.1610948354676365\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0018060741982319289\n",
      "Average test loss: 770.7871809631247\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0017896448844112456\n",
      "Average test loss: 254.72858909598656\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0018036342748544282\n",
      "Average test loss: 17.102830078994234\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0017895974330604076\n",
      "Average test loss: 13.184643522698845\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002243480745289061\n",
      "Average test loss: 0.37382281800193917\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0021785739868258438\n",
      "Average test loss: 27.13480711814906\n",
      "Epoch 143/300\n",
      "Average training loss: 0.001891506545763049\n",
      "Average test loss: 23.212216952501073\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001825904643163085\n",
      "Average test loss: 201691.32439457494\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0017898684256813593\n",
      "Average test loss: 101.16035748529185\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0017747477938731512\n",
      "Average test loss: 2502.3413027881625\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0017580753198514383\n",
      "Average test loss: 3188.359631432106\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0017522291814287504\n",
      "Average test loss: 0.2297777805717455\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0017527679906537135\n",
      "Average test loss: 7857.733422726835\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018031476740207938\n",
      "Average test loss: 5097.908120225058\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0017772736103377409\n",
      "Average test loss: 26274.305727541087\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0017522592431762152\n",
      "Average test loss: 16.591024014702896\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0017534953288527\n",
      "Average test loss: 934.3255004978652\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0018145367521792651\n",
      "Average test loss: 134743.17655111838\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0017475102059543134\n",
      "Average test loss: 397.80718654439886\n",
      "Epoch 156/300\n",
      "Average training loss: 0.001708216751822167\n",
      "Average test loss: 2.935566203345648\n",
      "Epoch 157/300\n",
      "Average training loss: 0.001753964768515693\n",
      "Average test loss: 13356.381634298645\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0017229085442506604\n",
      "Average test loss: 33076.107916868896\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0017113148593861196\n",
      "Average test loss: 4446.010275412627\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0021954659150085517\n",
      "Average test loss: 0.20329924793406906\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018458400089293718\n",
      "Average test loss: 22.58359557682607\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0017575408476922247\n",
      "Average test loss: 0.5917480783066195\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0017196820148577294\n",
      "Average test loss: 0.13796307436966648\n",
      "Epoch 164/300\n",
      "Average training loss: 0.001707916911950128\n",
      "Average test loss: 1.836562179463812\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0017111353438554538\n",
      "Average test loss: 60.69605956234038\n",
      "Epoch 166/300\n",
      "Average training loss: 0.001733031251674725\n",
      "Average test loss: 0.2662491228668433\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0017106934557151463\n",
      "Average test loss: 27.75233840313264\n",
      "Epoch 168/300\n",
      "Average training loss: 0.001694315620077153\n",
      "Average test loss: 129.72506665645633\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0016797715732827783\n",
      "Average test loss: 44.741378106602056\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0016866412360428107\n",
      "Average test loss: 206.87028946598278\n",
      "Epoch 171/300\n",
      "Average training loss: 0.001683818055316806\n",
      "Average test loss: 7635.482180722966\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0016644588155258033\n",
      "Average test loss: 695677.4073454861\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0017533045624279313\n",
      "Average test loss: 4.985285211462735\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0017548048841870493\n",
      "Average test loss: 419.60754953162535\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0017153161850033535\n",
      "Average test loss: 22528.619368652344\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0017689099797151154\n",
      "Average test loss: 2.984504387765812\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0016895057394479712\n",
      "Average test loss: 6.924560458225095\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0016842991114697523\n",
      "Average test loss: 298.0451254624501\n",
      "Epoch 179/300\n",
      "Average training loss: 0.001673827457655635\n",
      "Average test loss: 3006.57021801704\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0016976119444395105\n",
      "Average test loss: 263.98796811850866\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0017773965895175935\n",
      "Average test loss: 240.81195175670686\n",
      "Epoch 182/300\n",
      "Average training loss: 0.001722849945227305\n",
      "Average test loss: 17624.20000482696\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001680143132391903\n",
      "Average test loss: 65.51217028187081\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0016627165094638865\n",
      "Average test loss: 42.39587367681807\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0016549819686139623\n",
      "Average test loss: 7397.788843024297\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0016510320592464673\n",
      "Average test loss: 20704.01004708404\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0016463047293946147\n",
      "Average test loss: 0.10069041643095099\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0016421036423287458\n",
      "Average test loss: 7459.58479279963\n",
      "Epoch 189/300\n",
      "Average training loss: 0.001659315591160622\n",
      "Average test loss: 4024.2821810535365\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0016267326641827822\n",
      "Average test loss: 79.4456448901275\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0016275453337778647\n",
      "Average test loss: 120793.70235652651\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001640351226553321\n",
      "Average test loss: 5088.688933982732\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0016194975102941196\n",
      "Average test loss: 1.8341938174085484\n",
      "Epoch 194/300\n",
      "Average training loss: 0.001609381537987954\n",
      "Average test loss: 4559.597801694925\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0016210774005287222\n",
      "Average test loss: 9112.20537645913\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0016095570351721512\n",
      "Average test loss: 5064.338533512425\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0016005275751360588\n",
      "Average test loss: 1143.5503429530777\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0015852832097767128\n",
      "Average test loss: 0.3106292887047554\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0015889501893478963\n",
      "Average test loss: 1000.2105755136754\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00158247402889861\n",
      "Average test loss: 11523.037715836947\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0015874610687088635\n",
      "Average test loss: 4380843.1737968335\n",
      "Epoch 202/300\n",
      "Average training loss: 0.001593929557233221\n",
      "Average test loss: 1642.9077182578958\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0015811422866665655\n",
      "Average test loss: 9620.415348670807\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0017171407379209995\n",
      "Average test loss: 5.472267084999424\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0015920779696769185\n",
      "Average test loss: 121.47249739631204\n",
      "Epoch 206/300\n",
      "Average training loss: 0.001567997143810822\n",
      "Average test loss: 332.40664864621283\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0015667395591218438\n",
      "Average test loss: 37.43578531383185\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0015596149717974994\n",
      "Average test loss: 62353.051084627485\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0015695592399893536\n",
      "Average test loss: 555.8213502139615\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0017678321312285133\n",
      "Average test loss: 4709.720078937849\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0015849334739355576\n",
      "Average test loss: 2689.47001378495\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0015516299030019177\n",
      "Average test loss: 64.06766338639996\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0015525306468415591\n",
      "Average test loss: 21376.44932911182\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0015472592886330353\n",
      "Average test loss: 19657.780713436576\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0015331116481166747\n",
      "Average test loss: 202.40629281444393\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0015336290785214967\n",
      "Average test loss: 53.55506053195728\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0015370753864861197\n",
      "Average test loss: 176199.54798274057\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0015345794579221142\n",
      "Average test loss: 188391.89155911122\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0015298499162826273\n",
      "Average test loss: 418875.2625558446\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0015385929082209866\n",
      "Average test loss: 1146009.2259097223\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0015685430442293484\n",
      "Average test loss: 83.49637943745302\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0019021612062222427\n",
      "Average test loss: 307.3154800577544\n",
      "Epoch 223/300\n",
      "Average training loss: 0.001595906660064227\n",
      "Average test loss: 2082.4616822767125\n",
      "Epoch 224/300\n",
      "Average training loss: 0.001648072401372095\n",
      "Average test loss: 0.09768574665714469\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0016034014133943452\n",
      "Average test loss: 71.47624707589888\n",
      "Epoch 226/300\n",
      "Average training loss: 0.001518713064595229\n",
      "Average test loss: 28.536145849787733\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0015098877176642417\n",
      "Average test loss: 19.80589109847736\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0015183644766608873\n",
      "Average test loss: 3989.340831258138\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0015223356487436427\n",
      "Average test loss: 3.8256183055599946\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0015023527410295274\n",
      "Average test loss: 0.03296893128235307\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0015146825525702702\n",
      "Average test loss: 0.057885991908816825\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00149765435161276\n",
      "Average test loss: 248.42992997256005\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0014995125757737292\n",
      "Average test loss: 33931.36519295247\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0015036337658142051\n",
      "Average test loss: 0.05085694307895998\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0014840459729441337\n",
      "Average test loss: 0.5071998582111878\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0014844275986154874\n",
      "Average test loss: 46676.599916965955\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001479322102231284\n",
      "Average test loss: 8270.238751935047\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0014802219931864076\n",
      "Average test loss: 2.2305892896167934\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0014710797412941852\n",
      "Average test loss: 2036.4066158455648\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0014737049731322462\n",
      "Average test loss: 92053.54148492744\n",
      "Epoch 241/300\n",
      "Average training loss: 0.001466371689353966\n",
      "Average test loss: 366.02103020009696\n",
      "Epoch 242/300\n",
      "Average training loss: 0.001461562417758008\n",
      "Average test loss: 2541.9278634033203\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0014592088754806253\n",
      "Average test loss: 40059.71330008528\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0016347766733831829\n",
      "Average test loss: 170.8400649785656\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0014640563151074781\n",
      "Average test loss: 12478.104208419385\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0014480285082633297\n",
      "Average test loss: 8896.177537833775\n",
      "Epoch 247/300\n",
      "Average training loss: 0.001446687884732253\n",
      "Average test loss: 11438.93380755637\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0014513494005012843\n",
      "Average test loss: 5926.677825858161\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0014554027570411563\n",
      "Average test loss: 158.9843125208006\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0014416812198857465\n",
      "Average test loss: 6.048581722757055\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0014490671345136232\n",
      "Average test loss: 0.002372068071530925\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0014944697820271054\n",
      "Average test loss: 132113.78836177246\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0014509160378947854\n",
      "Average test loss: 4270.877592117248\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0014351197574287652\n",
      "Average test loss: 52.16130123482044\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0014363944851275947\n",
      "Average test loss: 17905.151166072363\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0014514160292843977\n",
      "Average test loss: 13113.974885167738\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0014342841896125011\n",
      "Average test loss: 27365.810593950442\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0014707270895855293\n",
      "Average test loss: 642.8901028161064\n",
      "Epoch 259/300\n",
      "Average training loss: 0.001438187305091156\n",
      "Average test loss: 182.05727842881282\n",
      "Epoch 260/300\n",
      "Average training loss: 0.001462271144406663\n",
      "Average test loss: 144.76652297152953\n",
      "Epoch 261/300\n",
      "Average training loss: 0.001445386051821212\n",
      "Average test loss: 3.5068340261185336\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0014300691720305217\n",
      "Average test loss: 8.96772178648909\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0014455610185654627\n",
      "Average test loss: 17.34900516445024\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0014321437426325348\n",
      "Average test loss: 268.54515648382903\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0014465029573895866\n",
      "Average test loss: 112.87395788549135\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0014330620387982991\n",
      "Average test loss: 9930.34905614862\n",
      "Epoch 267/300\n",
      "Average training loss: 0.001412590294248528\n",
      "Average test loss: 12.638176572817688\n",
      "Epoch 268/300\n",
      "Average training loss: 0.001422165165655315\n",
      "Average test loss: 1500.0218853481317\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0014298571160373588\n",
      "Average test loss: 6581.75463205694\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0014221362344299753\n",
      "Average test loss: 16883.66136767237\n",
      "Epoch 271/300\n",
      "Average training loss: 0.001644444630895224\n",
      "Average test loss: 17.039031327246793\n",
      "Epoch 272/300\n",
      "Average training loss: 0.001476772208749834\n",
      "Average test loss: 1426.552347056152\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0014168290050907268\n",
      "Average test loss: 1061.1457460120685\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0014138110945415166\n",
      "Average test loss: 1451.417619872041\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0014003194252856903\n",
      "Average test loss: 1171801.6280460497\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0014195764885387486\n",
      "Average test loss: 35.632939100353774\n",
      "Epoch 277/300\n",
      "Average training loss: 0.001527528042284151\n",
      "Average test loss: 71.07314847457896\n",
      "Epoch 278/300\n",
      "Average training loss: 0.001407138234211339\n",
      "Average test loss: 95.57526842812118\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0014041196072681083\n",
      "Average test loss: 127.16787636641041\n",
      "Epoch 280/300\n",
      "Average training loss: 0.001406879318981535\n",
      "Average test loss: 43.63209301020288\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0013995653844128052\n",
      "Average test loss: 1609417.6030538194\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0013938135450912846\n",
      "Average test loss: 6813.12857981936\n",
      "Epoch 283/300\n",
      "Average training loss: 0.001395987844405075\n",
      "Average test loss: 2.719157472113975\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0014182820521915953\n",
      "Average test loss: 18.707759277713176\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0013922310372193655\n",
      "Average test loss: 48.40118648192928\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0013873754345501462\n",
      "Average test loss: 10.492153872080147\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0013992884766517413\n",
      "Average test loss: 31313.329700633196\n",
      "Epoch 288/300\n",
      "Average training loss: 0.001394635862360398\n",
      "Average test loss: 41049.31956500377\n",
      "Epoch 289/300\n",
      "Average training loss: 0.001387893141247332\n",
      "Average test loss: 70.11430920296411\n",
      "Epoch 290/300\n",
      "Average training loss: 0.001408685152936313\n",
      "Average test loss: 1.5340793871991336\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0013828327310168081\n",
      "Average test loss: 91.22381933046991\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0013831530069518421\n",
      "Average test loss: 1467.1094470061234\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0013776987898680899\n",
      "Average test loss: 1.992051339858522\n",
      "Epoch 294/300\n",
      "Average training loss: 0.001390728569175634\n",
      "Average test loss: 2987.400764625113\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0013846689372633894\n",
      "Average test loss: 23115.591027686325\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0013936293475950757\n",
      "Average test loss: 10845.263778353814\n",
      "Epoch 297/300\n",
      "Average training loss: 0.001374463350719048\n",
      "Average test loss: 28454.105821292844\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0013713601280210747\n",
      "Average test loss: 65.30983456916404\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0013660011070056094\n",
      "Average test loss: 44.720065061868375\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0013646865512968766\n",
      "Average test loss: 171555.08372068006\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.07660655366629363\n",
      "Average test loss: 0.0477265623708566\n",
      "Epoch 2/300\n",
      "Average training loss: 0.006687586984700627\n",
      "Average test loss: 5.289244533210993\n",
      "Epoch 3/300\n",
      "Average training loss: 0.005372400122798151\n",
      "Average test loss: 0.00495936528303557\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00474829516477055\n",
      "Average test loss: 0.0037982729422963326\n",
      "Epoch 5/300\n",
      "Average training loss: 0.004319248772329754\n",
      "Average test loss: 0.004297233609275686\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004000826562444369\n",
      "Average test loss: 0.003450192700450619\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0037456933237198327\n",
      "Average test loss: 0.09331406529247761\n",
      "Epoch 8/300\n",
      "Average training loss: 0.003514508857081334\n",
      "Average test loss: 0.3284244676441368\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0033139284362809526\n",
      "Average test loss: 0.015369183811876509\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003151436867606309\n",
      "Average test loss: 0.336451086057557\n",
      "Epoch 11/300\n",
      "Average training loss: 0.002990183791766564\n",
      "Average test loss: 0.0025547944953044255\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0028611209587090544\n",
      "Average test loss: 0.06863221598747704\n",
      "Epoch 13/300\n",
      "Average training loss: 0.002735568606812093\n",
      "Average test loss: 0.008147600965574384\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002653427117297219\n",
      "Average test loss: 0.007337709012958739\n",
      "Epoch 15/300\n",
      "Average training loss: 0.002536574259193407\n",
      "Average test loss: 0.09357832924607727\n",
      "Epoch 16/300\n",
      "Average training loss: 0.002460875495129989\n",
      "Average test loss: 1.0349098827325636\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0023863250877087317\n",
      "Average test loss: 0.0031863032538029883\n",
      "Epoch 18/300\n",
      "Average training loss: 0.002301951880256335\n",
      "Average test loss: 610.0338721148612\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0022479722406715153\n",
      "Average test loss: 2158.295027235667\n",
      "Epoch 20/300\n",
      "Average training loss: 0.002200111616195904\n",
      "Average test loss: 39.78465542905529\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0021174428396754795\n",
      "Average test loss: 0.471252794056096\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0021036825318717293\n",
      "Average test loss: 3.083731680132242\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0020299231788764396\n",
      "Average test loss: 6.910954240476143\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0019836364689593514\n",
      "Average test loss: 6.2819044342132075\n",
      "Epoch 25/300\n",
      "Average training loss: 0.001953520219048692\n",
      "Average test loss: 0.002689774091872904\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0018939236827815573\n",
      "Average test loss: 1.572680288184848\n",
      "Epoch 27/300\n",
      "Average training loss: 0.001870788992796507\n",
      "Average test loss: 8.485878791561143\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0018018723771803908\n",
      "Average test loss: 0.8764443073901865\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0017801751979730196\n",
      "Average test loss: 53.48739029238953\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0017415840450881255\n",
      "Average test loss: 0.5551746383706728\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0017128978693443867\n",
      "Average test loss: 0.09015506895631552\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0017901477887191707\n",
      "Average test loss: 0.25625391636312833\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0016724274863178532\n",
      "Average test loss: 0.011853784613414771\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0016752845670820937\n",
      "Average test loss: 0.5817643428200648\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0017450979380971856\n",
      "Average test loss: 0.09735485234111547\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0016682536789319581\n",
      "Average test loss: 0.0020668322352899444\n",
      "Epoch 37/300\n",
      "Average training loss: 0.001586599209294137\n",
      "Average test loss: 0.28134267215016817\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0015866956275163424\n",
      "Average test loss: 4.088094965317183\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0015580147051562866\n",
      "Average test loss: 1.5225944700435632\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0015289247048397858\n",
      "Average test loss: 2.8151677705136438\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0015168077972096702\n",
      "Average test loss: 0.05037861843862467\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0015570575391563276\n",
      "Average test loss: 14.146298832527052\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0014809518198793134\n",
      "Average test loss: 2.5887260381819472\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0014824972218937344\n",
      "Average test loss: 0.01292878104829126\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0014880257539658082\n",
      "Average test loss: 0.04177735834889528\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0015336363324895501\n",
      "Average test loss: 1.601025325578948\n",
      "Epoch 47/300\n",
      "Average training loss: 0.001429912591456539\n",
      "Average test loss: 20.63614731283817\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0014632007111277845\n",
      "Average test loss: 0.0014857129740735724\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0014155497091511886\n",
      "Average test loss: 0.6655440505357563\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0014135443395934999\n",
      "Average test loss: 20.769398725439277\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0014156021033931109\n",
      "Average test loss: 0.002494638700866037\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0013851119003569087\n",
      "Average test loss: 4.7458219565488395\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0013690542282743586\n",
      "Average test loss: 12.279790105075058\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0013629080483482945\n",
      "Average test loss: 83.02462749170594\n",
      "Epoch 55/300\n",
      "Average training loss: 0.001426287951051361\n",
      "Average test loss: 0.5074515133872628\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0013508998143176238\n",
      "Average test loss: 0.00510960169426269\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0013612760060156385\n",
      "Average test loss: 0.14677094930865697\n",
      "Epoch 58/300\n",
      "Average training loss: 0.001338693746106906\n",
      "Average test loss: 9.508212162786474\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0020760028440919187\n",
      "Average test loss: 0.05811061083070106\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0018061776836920116\n",
      "Average test loss: 423.8680245090185\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0015165783596328562\n",
      "Average test loss: 61058.520107659235\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0014409122648131516\n",
      "Average test loss: 1036401.8758333334\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0014841447717820605\n",
      "Average test loss: 20.206991594441234\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0013819829349716504\n",
      "Average test loss: 0.5640818562164075\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0013588911793712113\n",
      "Average test loss: 68.85016199149025\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00133595705891235\n",
      "Average test loss: 0.48801293061073453\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0013373976413988404\n",
      "Average test loss: 524.1898221614783\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0013307188803123103\n",
      "Average test loss: 0.6472522977323582\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0013219778795416155\n",
      "Average test loss: 590.5106576724673\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0013509941227320167\n",
      "Average test loss: 21350.573799045138\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0013058913913038042\n",
      "Average test loss: 2534.2286389207443\n",
      "Epoch 72/300\n",
      "Average training loss: 0.001294428553018305\n",
      "Average test loss: 89.61990979483971\n",
      "Epoch 73/300\n",
      "Average training loss: 0.001284178058616817\n",
      "Average test loss: 0.0037196995777388415\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0012965280791330668\n",
      "Average test loss: 6.32275945199281\n",
      "Epoch 75/300\n",
      "Average training loss: 0.001284379106428888\n",
      "Average test loss: 16.675112940037003\n",
      "Epoch 76/300\n",
      "Average training loss: 0.001269259556920992\n",
      "Average test loss: 661.9072107868557\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0012691346385205785\n",
      "Average test loss: 3096.513705931262\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0012841282527790302\n",
      "Average test loss: 5.203665986620304\n",
      "Epoch 79/300\n",
      "Average training loss: 0.001362438406381342\n",
      "Average test loss: 0.03176535110589531\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0012982359137386084\n",
      "Average test loss: 2412.6200849032634\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0012580284513533115\n",
      "Average test loss: 17.503318166419035\n",
      "Epoch 82/300\n",
      "Average training loss: 0.001240907517572244\n",
      "Average test loss: 0.008286523713730275\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0013072837282282611\n",
      "Average test loss: 27.045930015356177\n",
      "Epoch 84/300\n",
      "Average training loss: 0.001235941170093914\n",
      "Average test loss: 84.86174137402192\n",
      "Epoch 85/300\n",
      "Average training loss: 0.001216271733865142\n",
      "Average test loss: 208.86010177280795\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0013079703081813124\n",
      "Average test loss: 6.6612777974050905\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0012214970223398671\n",
      "Average test loss: 0.03492007324058149\n",
      "Epoch 88/300\n",
      "Average training loss: 0.001230604931143009\n",
      "Average test loss: 0.33652881487872865\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0012476168716222876\n",
      "Average test loss: 0.12051172886333532\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0011979391333750552\n",
      "Average test loss: 0.0036783932739247877\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0012082302184878951\n",
      "Average test loss: 11192.730106099685\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0011943800097538365\n",
      "Average test loss: 1.8042974622574015\n",
      "Epoch 93/300\n",
      "Average training loss: 0.001443866063426766\n",
      "Average test loss: 0.7302970512103704\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0012177904695789847\n",
      "Average test loss: 102.05473323938499\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0012112193427359064\n",
      "Average test loss: 0.008586304256019906\n",
      "Epoch 96/300\n",
      "Average training loss: 0.001193564809186177\n",
      "Average test loss: 0.001477225236905118\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0011909394862337245\n",
      "Average test loss: 262.38369838646656\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0012240176916950278\n",
      "Average test loss: 4.1941119557635655\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0011912834408382574\n",
      "Average test loss: 82.2127165846538\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00118951083223025\n",
      "Average test loss: 154.04621167888098\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0012136071846923894\n",
      "Average test loss: 0.0013364504694731699\n",
      "Epoch 102/300\n",
      "Average training loss: 0.001206060738199287\n",
      "Average test loss: 0.0281121368282992\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0027380775871376196\n",
      "Average test loss: 0.0018332740780380038\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0017653254403008356\n",
      "Average test loss: 0.006129232962926228\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0016543897837400436\n",
      "Average test loss: 0.004701919550076127\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0016107283107315502\n",
      "Average test loss: 0.0017084166577292813\n",
      "Epoch 107/300\n",
      "Average training loss: 0.001549891607939369\n",
      "Average test loss: 0.0018188181983100043\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0014967691158461903\n",
      "Average test loss: 0.004954141915672355\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0014513409635983408\n",
      "Average test loss: 0.024641082844386496\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0014573458129954007\n",
      "Average test loss: 0.004502482796294821\n",
      "Epoch 111/300\n",
      "Average training loss: 0.001455661397634281\n",
      "Average test loss: 0.03292028468185001\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0014488734256269203\n",
      "Average test loss: 0.0016005649427986808\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0013914852613169286\n",
      "Average test loss: 0.763120247864061\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0013587893931609062\n",
      "Average test loss: 0.7297085159685877\n",
      "Epoch 115/300\n",
      "Average training loss: 0.001331834883739551\n",
      "Average test loss: 0.005081145980705817\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0012974922194456061\n",
      "Average test loss: 0.4013050442010992\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0012710629170760513\n",
      "Average test loss: 1.2036117218070561\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0012466392516572442\n",
      "Average test loss: 4.773366071371982\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0012315924962361653\n",
      "Average test loss: 19.05877805358461\n",
      "Epoch 120/300\n",
      "Average training loss: 0.001216878921000494\n",
      "Average test loss: 7.851246358876013\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0012099887947551906\n",
      "Average test loss: 20.789022060261004\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0012102961268069016\n",
      "Average test loss: 0.7520951090686852\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0012024470318315757\n",
      "Average test loss: 296.4021096359537\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0011868563529828357\n",
      "Average test loss: 0.6422022890033614\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0011901539928900699\n",
      "Average test loss: 0.6235217353399429\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0011912165010968844\n",
      "Average test loss: 1248.5829981083857\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0011916427678532072\n",
      "Average test loss: 0.00833161198337459\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0012313016211200091\n",
      "Average test loss: 1.8504270466346708\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0011829927043161459\n",
      "Average test loss: 0.003429821021233996\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0011800565446416537\n",
      "Average test loss: 27.13781258760755\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0012131785663465659\n",
      "Average test loss: 0.12450404101403223\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0011662461291481224\n",
      "Average test loss: 0.002186698476680451\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0011945629275093477\n",
      "Average test loss: 0.02036788725656354\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0011364472944082485\n",
      "Average test loss: 5.908985482422014\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0011423499401555294\n",
      "Average test loss: 105.89897150014424\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0011352279101394945\n",
      "Average test loss: 121.64130287526066\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0011450246270332072\n",
      "Average test loss: 1430.7361954937141\n",
      "Epoch 138/300\n",
      "Average training loss: 0.001162416953438272\n",
      "Average test loss: 39.68376908628674\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0011152740290595426\n",
      "Average test loss: 47.86457263220526\n",
      "Epoch 140/300\n",
      "Average training loss: 0.001116186317677299\n",
      "Average test loss: 26.23603550143447\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0011671773364974392\n",
      "Average test loss: 0.09875066388874418\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0011728148128216465\n",
      "Average test loss: 792.7376726439325\n",
      "Epoch 143/300\n",
      "Average training loss: 0.001131482247573634\n",
      "Average test loss: 641.0653797919233\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001125483817834821\n",
      "Average test loss: 75.61549601298239\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0011204301385312444\n",
      "Average test loss: 25.414206405479614\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0011220124184878335\n",
      "Average test loss: 1803.155966913537\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0011616586560590399\n",
      "Average test loss: 49.04188316682312\n",
      "Epoch 148/300\n",
      "Average training loss: 0.001099391263185276\n",
      "Average test loss: 2.074312164249106\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0010926062873978582\n",
      "Average test loss: 144.35158025635613\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0011025864875668452\n",
      "Average test loss: 6.562151701932928\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0011075055218922596\n",
      "Average test loss: 0.017772681020200253\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0011028588974537948\n",
      "Average test loss: 12.733143061983503\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001090435574348602\n",
      "Average test loss: 302.34871540496385\n",
      "Epoch 154/300\n",
      "Average training loss: 0.001300093142212265\n",
      "Average test loss: 9401.937237361919\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0011295975796464417\n",
      "Average test loss: 1.092706693056764\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0010956989590906435\n",
      "Average test loss: 983.6763316661409\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0010930897295475005\n",
      "Average test loss: 6.686060713146296\n",
      "Epoch 158/300\n",
      "Average training loss: 0.001070504288189113\n",
      "Average test loss: 0.8940221064210766\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0010829192691275643\n",
      "Average test loss: 0.00479019662603322\n",
      "Epoch 160/300\n",
      "Average training loss: 0.001088065209488074\n",
      "Average test loss: 8.059443297877493\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0010792861576709482\n",
      "Average test loss: 120.67413503688036\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0011021513221785425\n",
      "Average test loss: 0.002141475540689296\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0010585432876832784\n",
      "Average test loss: 68.53733370772076\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0010769019787096315\n",
      "Average test loss: 34.06037746738508\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0010566384048304625\n",
      "Average test loss: 6.73120286673266\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0011002328832530313\n",
      "Average test loss: 0.20345764096815966\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0010859359151476787\n",
      "Average test loss: 181.36324148329473\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0010737887372055815\n",
      "Average test loss: 229.12137012171993\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0010439685425307189\n",
      "Average test loss: 238.5819831911553\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0010503905402082536\n",
      "Average test loss: 74129.1733440534\n",
      "Epoch 171/300\n",
      "Average training loss: 0.001047517508113136\n",
      "Average test loss: 0.0014141233327488105\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0011048963938115372\n",
      "Average test loss: 0.8259226852051086\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0010479485130765371\n",
      "Average test loss: 97.49742740536854\n",
      "Epoch 174/300\n",
      "Average training loss: 0.001074781107612782\n",
      "Average test loss: 436.22678854027555\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0010895475116040971\n",
      "Average test loss: 40.96685796774882\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0010281684023535087\n",
      "Average test loss: 140.48290843643662\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0010208783346849183\n",
      "Average test loss: 1904.5740286387152\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0010265026119951573\n",
      "Average test loss: 0.29097663504050836\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0010556913119637303\n",
      "Average test loss: 975.6556546924023\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0010274729818209177\n",
      "Average test loss: 0.0837435157323877\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001023104171993004\n",
      "Average test loss: 472.4559783565882\n",
      "Epoch 182/300\n",
      "Average training loss: 0.001043897953350097\n",
      "Average test loss: 52.23663278573565\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0010081654650987023\n",
      "Average test loss: 129430.11082579453\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0010129627018339105\n",
      "Average test loss: 350747.257823628\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0010825632321130897\n",
      "Average test loss: 0.008512417327198717\n",
      "Epoch 186/300\n",
      "Average training loss: 0.001058924897418668\n",
      "Average test loss: 0.0061604843116882775\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0010350678237155079\n",
      "Average test loss: 46.70344046952906\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0010207410840214126\n",
      "Average test loss: 0.5693103271553086\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0010234730816446244\n",
      "Average test loss: 164.92333726150946\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001014111896407687\n",
      "Average test loss: 13.530557827893645\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0010292191638921699\n",
      "Average test loss: 431.24366848993935\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0010290667270827624\n",
      "Average test loss: 20.716324695558804\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0010117475914561914\n",
      "Average test loss: 2.0255513909879244\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0010322131919157174\n",
      "Average test loss: 2.2343998583289277\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0010568158715549443\n",
      "Average test loss: 0.9018407681373258\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0010493750118960937\n",
      "Average test loss: 0.022932711200478177\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0010182041473583215\n",
      "Average test loss: 96.43339392149613\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0009954425672379632\n",
      "Average test loss: 35.65922302677886\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0009897768938293059\n",
      "Average test loss: 4.435864516166763\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0010461797157509459\n",
      "Average test loss: 3.4253168936613947\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0010233294767741528\n",
      "Average test loss: 4.4227559844718005\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0010008563581957585\n",
      "Average test loss: 11.90184751545255\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0010030881997404827\n",
      "Average test loss: 0.018175645729733837\n",
      "Epoch 204/300\n",
      "Average training loss: 0.000995773697582384\n",
      "Average test loss: 6.531944937486667\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0009875911733446022\n",
      "Average test loss: 641.7717984111464\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0009862433354784217\n",
      "Average test loss: 0.017270143651196525\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0009921750360582437\n",
      "Average test loss: 58.0437193517153\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0010164939165115355\n",
      "Average test loss: 453516.82207638887\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0009988043955527245\n",
      "Average test loss: 59.945321346584706\n",
      "Epoch 210/300\n",
      "Average training loss: 0.001023894098897775\n",
      "Average test loss: 951.4410349889522\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0010391451629499595\n",
      "Average test loss: 1142.7817730470615\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0009769780970592466\n",
      "Average test loss: 0.3901764458228524\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0009747146118121843\n",
      "Average test loss: 27.652879675976106\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0010053064314027628\n",
      "Average test loss: 10.203718918267844\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0009913042111115323\n",
      "Average test loss: 0.09061601051108704\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0009733302513033979\n",
      "Average test loss: 3.1162014144059893\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0009822340757689543\n",
      "Average test loss: 317.7702345744239\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0009875413414815234\n",
      "Average test loss: 93.37803683634128\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0009785592165879077\n",
      "Average test loss: 0.11533775113098738\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0009639341202047136\n",
      "Average test loss: 77.01947528698916\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0009724695707360903\n",
      "Average test loss: 4564.234903580547\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0009599555981241994\n",
      "Average test loss: 953.2826263484661\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0009748771585420602\n",
      "Average test loss: 4557.605423774448\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0009556884511063497\n",
      "Average test loss: 9.972147486146746\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0009614232716461023\n",
      "Average test loss: 713.4621799513532\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0009481060521987577\n",
      "Average test loss: 634.9184454354743\n",
      "Epoch 227/300\n",
      "Average training loss: 0.000984458576939586\n",
      "Average test loss: 10703.400568047395\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0009651675266213715\n",
      "Average test loss: 381.4678532817612\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0009436574744371076\n",
      "Average test loss: 0.8501723474491801\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0009975683830368023\n",
      "Average test loss: 0.5962532887860305\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0009547382852890425\n",
      "Average test loss: 239.33234816642602\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0009497290871416529\n",
      "Average test loss: 10979.907703256542\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0009481605945879386\n",
      "Average test loss: 12148.868208591037\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0009393532601599064\n",
      "Average test loss: 25.890764969223696\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0009519879079113404\n",
      "Average test loss: 0.03278884383699753\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0009533502033704685\n",
      "Average test loss: 93954.7838974745\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0009445038837794629\n",
      "Average test loss: 333.7086090574848\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0009622160392916864\n",
      "Average test loss: 1.5269752230809794\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0009482755504238109\n",
      "Average test loss: 0.17154028863811657\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0009494119743920034\n",
      "Average test loss: 2.0097536412394708\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0009533563166235884\n",
      "Average test loss: 60.66671363213452\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0009359235645323578\n",
      "Average test loss: 0.47175018834612437\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0009446214913494057\n",
      "Average test loss: 43.02323363503964\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0009512535632174048\n",
      "Average test loss: 15.410806443914254\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0009361437658468883\n",
      "Average test loss: 14.983223643291328\n",
      "Epoch 246/300\n",
      "Average training loss: 0.000934562316371335\n",
      "Average test loss: 918.9225593261334\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0009466931876829929\n",
      "Average test loss: 2540.2945237842523\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0009626683557095627\n",
      "Average test loss: 5232.772411771673\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0009166148725586633\n",
      "Average test loss: 16474.553122938367\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0009237233138022323\n",
      "Average test loss: 19.044596963469147\n",
      "Epoch 251/300\n",
      "Average training loss: 0.000933564027512653\n",
      "Average test loss: 48180.94432681056\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0009405534297952222\n",
      "Average test loss: 35682.68103984218\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0009212481713232895\n",
      "Average test loss: 8.8727874488843\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01816628812920923\n",
      "Average test loss: 132.97694724201887\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004143917807895276\n",
      "Average test loss: 2.061822499218914\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002219200474520524\n",
      "Average test loss: 32.59538216768743\n",
      "Epoch 257/300\n",
      "Average training loss: 0.001978720348017911\n",
      "Average test loss: 0.015945101119991804\n",
      "Epoch 258/300\n",
      "Average training loss: 0.00183342319737292\n",
      "Average test loss: 0.045337770724358656\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0017244831793424157\n",
      "Average test loss: 33.28641859391911\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0016381519485471976\n",
      "Average test loss: 0.25083728083843987\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0015705035952851176\n",
      "Average test loss: 1.421607502463791\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0015081309217752682\n",
      "Average test loss: 1.040558819675611\n",
      "Epoch 263/300\n",
      "Average training loss: 0.001447617329036196\n",
      "Average test loss: 0.0015196553718091713\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0014012732586512963\n",
      "Average test loss: 1.8143484175035522\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0013464800817374553\n",
      "Average test loss: 4.060218451514013\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00129631808353588\n",
      "Average test loss: 0.12827927392058902\n",
      "Epoch 267/300\n",
      "Average training loss: 0.001262983028104322\n",
      "Average test loss: 2.930320041693385\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0012060968699968524\n",
      "Average test loss: 43.064846445140326\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0011774294685779347\n",
      "Average test loss: 20345.715562431014\n",
      "Epoch 270/300\n",
      "Average training loss: 0.001137502398290154\n",
      "Average test loss: 1.0067213465749907\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0011137794786546793\n",
      "Average test loss: 1.4527162494344843\n",
      "Epoch 272/300\n",
      "Average training loss: 0.001071354483978616\n",
      "Average test loss: 1.7228341786063586\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0010561397029086948\n",
      "Average test loss: 6946.926165997009\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0010381338767603868\n",
      "Average test loss: 10583.343773663528\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0010312596453974645\n",
      "Average test loss: 155.8590821222928\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0010042972802184523\n",
      "Average test loss: 80597.68208970974\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0010161564400316112\n",
      "Average test loss: 2.279500462207736\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0009966157923142116\n",
      "Average test loss: 30.287300693516308\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0009939256019165946\n",
      "Average test loss: 149497.68820023452\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0009808239279211395\n",
      "Average test loss: 58.98903690394573\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0009812373081222177\n",
      "Average test loss: 468.98481447540524\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001029311038967636\n",
      "Average test loss: 0.25275633214786647\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0010129783717501494\n",
      "Average test loss: 20.80282836800938\n",
      "Epoch 284/300\n",
      "Average training loss: 0.000975840950332996\n",
      "Average test loss: 0.04048625535724892\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0009828197576312555\n",
      "Average test loss: 0.9065626681554648\n",
      "Epoch 286/300\n",
      "Average training loss: 0.000984587846013407\n",
      "Average test loss: 0.0814154752323197\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0009781357666684522\n",
      "Average test loss: 3247.599543649807\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0009759667569047047\n",
      "Average test loss: 0.001416283884913557\n",
      "Epoch 289/300\n",
      "Average training loss: 0.000950507862545136\n",
      "Average test loss: 0.5815487586750339\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0009671426305754318\n",
      "Average test loss: 35.262462306396\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0009678295499955615\n",
      "Average test loss: 153.2741699963453\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0009576315127520097\n",
      "Average test loss: 10264.396859780365\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0009530326807871461\n",
      "Average test loss: 32.21094108456828\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0013182150868491995\n",
      "Average test loss: 462.7946350274417\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0010473634728437496\n",
      "Average test loss: 20478.189957019018\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0010035933026423056\n",
      "Average test loss: 72.41014950133943\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0009625795962185495\n",
      "Average test loss: 63265.60195486111\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0009647491220384836\n",
      "Average test loss: 3269.2539510207903\n",
      "Epoch 299/300\n",
      "Average training loss: 0.000955337394753264\n",
      "Average test loss: 269.2028185702732\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0009557098244420356\n",
      "Average test loss: 890.8308279748704\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.1/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 5.34\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 11.50\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 16.09\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 14.88\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 20.58\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 21.32\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 10.49\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 20.80\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 22.74\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.69\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 15.03\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.03\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 23.97\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.32\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.73\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 5.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 10.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 14.49\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 16.70\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 20.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 16.34\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.50\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.68\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 12.97\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 23.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.38\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 21.58\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.15\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 0.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 3.00\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 7.38\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 9.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 15.30\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 18.36\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 14.27\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 22.35\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 23.56\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.67\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 20.89\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.93\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.06\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 3.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 6.43\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 13.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 20.13\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.54\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 18.05\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.02\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.67\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.34\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.51\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.26050928941534623\n",
      "Average test loss: 0.018052405796945094\n",
      "Epoch 2/300\n",
      "Average training loss: 0.021455550061331857\n",
      "Average test loss: 0.014757054846319888\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01680204262998369\n",
      "Average test loss: 0.012095896686116854\n",
      "Epoch 4/300\n",
      "Average training loss: 0.014942899543378088\n",
      "Average test loss: 0.018189512178301812\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01397633307509952\n",
      "Average test loss: 0.011349175732168886\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013019312486880355\n",
      "Average test loss: 0.010014628834194607\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01198061959611045\n",
      "Average test loss: 0.034797214090824126\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011420264722572432\n",
      "Average test loss: 0.020099861840407053\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011000640263987912\n",
      "Average test loss: 0.01058987069543865\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011042490445905261\n",
      "Average test loss: 0.00911638157442212\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010610655623177687\n",
      "Average test loss: 0.00840700459190541\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010286560287078222\n",
      "Average test loss: 0.009018831382195155\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009912925648192565\n",
      "Average test loss: 0.09201453391545349\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009645623973674244\n",
      "Average test loss: 0.00843642058637407\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009352700422207514\n",
      "Average test loss: 0.028622822223438156\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009223452150821685\n",
      "Average test loss: 0.007570161716805564\n",
      "Epoch 17/300\n",
      "Average training loss: 0.008859813397129377\n",
      "Average test loss: 0.04479024674163924\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03521406205743551\n",
      "Average test loss: 0.012913171555433008\n",
      "Epoch 19/300\n",
      "Average training loss: 0.011444451236890422\n",
      "Average test loss: 0.00863067651208904\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010097329333424568\n",
      "Average test loss: 0.008080968394047684\n",
      "Epoch 21/300\n",
      "Average training loss: 0.009439811537663143\n",
      "Average test loss: 0.00787086961666743\n",
      "Epoch 22/300\n",
      "Average training loss: 0.00915567651639382\n",
      "Average test loss: 0.026964236470560232\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008793845524390538\n",
      "Average test loss: 0.3070676624245114\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00908349220123556\n",
      "Average test loss: 0.007017688580685192\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00857090720285972\n",
      "Average test loss: 0.15707929992675781\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008573468817190991\n",
      "Average test loss: 0.11084142669621441\n",
      "Epoch 27/300\n",
      "Average training loss: 0.008340450872977575\n",
      "Average test loss: 0.007293025468786557\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0082549275731047\n",
      "Average test loss: 0.042534417704989516\n",
      "Epoch 29/300\n",
      "Average training loss: 0.007938895513614019\n",
      "Average test loss: 0.011091220555620061\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013015100205110179\n",
      "Average test loss: 0.009502932937608824\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00942456608513991\n",
      "Average test loss: 0.021186422768980265\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008635852495829264\n",
      "Average test loss: 0.007019661967953046\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008385431761542956\n",
      "Average test loss: 0.006942806935972637\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008142548729148175\n",
      "Average test loss: 0.19806280109948582\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008132203998665015\n",
      "Average test loss: 0.007411720217516025\n",
      "Epoch 36/300\n",
      "Average training loss: 0.00797445401466555\n",
      "Average test loss: 0.1487559478936924\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009771666718026002\n",
      "Average test loss: 0.006508583723670906\n",
      "Epoch 38/300\n",
      "Average training loss: 0.008501394487503502\n",
      "Average test loss: 0.010959638064106306\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008405281056960423\n",
      "Average test loss: 0.0068677656162116265\n",
      "Epoch 40/300\n",
      "Average training loss: 0.007841250574423208\n",
      "Average test loss: 0.0066900411645571395\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00890104894588391\n",
      "Average test loss: 1.1317367734909058\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008076021569470565\n",
      "Average test loss: 0.009174807710366117\n",
      "Epoch 43/300\n",
      "Average training loss: 0.00781675198343065\n",
      "Average test loss: 0.022009241084257763\n",
      "Epoch 44/300\n",
      "Average training loss: 0.00799506758650144\n",
      "Average test loss: 0.10559587523423963\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007628606331845125\n",
      "Average test loss: 0.01339179574780994\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007338046053631438\n",
      "Average test loss: 0.00814687660502063\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00795164241310623\n",
      "Average test loss: 289.9409476662224\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008767701572428148\n",
      "Average test loss: 0.019506480833722487\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007642290094660388\n",
      "Average test loss: 1.5397053923135002\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007256937490983142\n",
      "Average test loss: 8.62758533690994\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007666054953303602\n",
      "Average test loss: 0.0065260971838401426\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007240333740495973\n",
      "Average test loss: 5.1507327164577115\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007358242205033699\n",
      "Average test loss: 98.60125834308398\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0072604531794786455\n",
      "Average test loss: 0.11590841015014383\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009588882570465406\n",
      "Average test loss: 37.71576352973614\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007427878120707141\n",
      "Average test loss: 55.27758450592061\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007141159963276651\n",
      "Average test loss: 1.3890241976256172\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007080165366331736\n",
      "Average test loss: 927.3549052870563\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0070972689998646575\n",
      "Average test loss: 8.471768783762224\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0069898572812477746\n",
      "Average test loss: 0.006599240568776925\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0071796625343461835\n",
      "Average test loss: 4077.5493746321367\n",
      "Epoch 62/300\n",
      "Average training loss: 0.007468701396551397\n",
      "Average test loss: 12.944587301413632\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007349261842668056\n",
      "Average test loss: 29.600971444624992\n",
      "Epoch 64/300\n",
      "Average training loss: 0.008195950811521874\n",
      "Average test loss: 5.84223305356337\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0073972942564222545\n",
      "Average test loss: 153.61193353531428\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00711685413784451\n",
      "Average test loss: 506.1199555774294\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007067741281456418\n",
      "Average test loss: 170.58452314594552\n",
      "Epoch 68/300\n",
      "Average training loss: 0.010347556461890539\n",
      "Average test loss: 0.20859654748729534\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007799569384919273\n",
      "Average test loss: 0.7440846742577851\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007374778006639745\n",
      "Average test loss: 4.13826639592482\n",
      "Epoch 71/300\n",
      "Average training loss: 0.00719293462485075\n",
      "Average test loss: 2.373089072617392\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00704567402932379\n",
      "Average test loss: 39.56626623902056\n",
      "Epoch 73/300\n",
      "Average training loss: 0.006895552059428559\n",
      "Average test loss: 3.9911835126198\n",
      "Epoch 74/300\n",
      "Average training loss: 0.006816818484415611\n",
      "Average test loss: 0.4820340479558541\n",
      "Epoch 75/300\n",
      "Average training loss: 0.006926517849167188\n",
      "Average test loss: 0.8040734805535111\n",
      "Epoch 76/300\n",
      "Average training loss: 0.006924288308454885\n",
      "Average test loss: 3.4512140312724644\n",
      "Epoch 77/300\n",
      "Average training loss: 0.006723458402272728\n",
      "Average test loss: 9221113.73305173\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006606656762046946\n",
      "Average test loss: 9.168866869753847\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006488888633333975\n",
      "Average test loss: 75.47790911384259\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006370630212128163\n",
      "Average test loss: 1062.7393631817065\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006343606620613071\n",
      "Average test loss: 55.45868356981874\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006883512118210395\n",
      "Average test loss: 4912.388443576939\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006504796533121003\n",
      "Average test loss: 948.3218288906481\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006191275374756919\n",
      "Average test loss: 7868.378810123983\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006106506048805184\n",
      "Average test loss: 36422.90933953008\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006026320591155026\n",
      "Average test loss: 98.56089662071234\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0059819950664208995\n",
      "Average test loss: 3.420581151774774\n",
      "Epoch 88/300\n",
      "Average training loss: 0.005922251498119699\n",
      "Average test loss: 621162.6662873467\n",
      "Epoch 89/300\n",
      "Average training loss: 0.005887472418447335\n",
      "Average test loss: 25653.20003659016\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006225887373917632\n",
      "Average test loss: 0.1837676139333182\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005963090954969327\n",
      "Average test loss: 1.2812042508241204\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005910866919904947\n",
      "Average test loss: 3.46940377439455\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005884633122632901\n",
      "Average test loss: 361.3572119105839\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005771959580067132\n",
      "Average test loss: 232.32518973416256\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005875328215459982\n",
      "Average test loss: 583666.65090699\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0058017378581894765\n",
      "Average test loss: 6497.228533398759\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005716591873102718\n",
      "Average test loss: 0.006030013014458948\n",
      "Epoch 98/300\n",
      "Average training loss: 0.005954137853864167\n",
      "Average test loss: 0.005579509711513916\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005780002817925479\n",
      "Average test loss: 509.45253694429994\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00566617623095711\n",
      "Average test loss: 44228.65854377178\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005636427578412824\n",
      "Average test loss: 60634.60413099966\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005637798043588797\n",
      "Average test loss: 191365343592891.84\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005652159506248103\n",
      "Average test loss: 5291664.071347155\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005568318790031804\n",
      "Average test loss: 1087.709623843847\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0055126282924579245\n",
      "Average test loss: 186.08693613039702\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005487869662957059\n",
      "Average test loss: 4119981.2701631743\n",
      "Epoch 107/300\n",
      "Average training loss: 0.005514453368054496\n",
      "Average test loss: 206.9218860344804\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005425169904198912\n",
      "Average test loss: 377.89758559037125\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005406267447604074\n",
      "Average test loss: 7114.098101388193\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005377272318634722\n",
      "Average test loss: 490285.88559885276\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005357437409874466\n",
      "Average test loss: 21.991420340188675\n",
      "Epoch 112/300\n",
      "Average training loss: 0.005508958457451728\n",
      "Average test loss: 53557.9832144152\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005518234304255909\n",
      "Average test loss: 2235.7012537041123\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005313589719434579\n",
      "Average test loss: 0.8714375995364454\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005276208383755552\n",
      "Average test loss: 648193.9541522946\n",
      "Epoch 116/300\n",
      "Average training loss: 0.005250080106986893\n",
      "Average test loss: 2499.236067806668\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005233271534658141\n",
      "Average test loss: 686041.2541612418\n",
      "Epoch 118/300\n",
      "Average training loss: 0.005220722045335505\n",
      "Average test loss: 7.439932721265488\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005195679476277696\n",
      "Average test loss: 435.7451947080675\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005184567607939243\n",
      "Average test loss: 0.04421339164177577\n",
      "Epoch 121/300\n",
      "Average training loss: 0.005165479979581303\n",
      "Average test loss: 0.09032326859649685\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00511974618376957\n",
      "Average test loss: 2292.4413755689097\n",
      "Epoch 123/300\n",
      "Average training loss: 0.005113702084869146\n",
      "Average test loss: 81423.8304923671\n",
      "Epoch 124/300\n",
      "Average training loss: 0.005092586547550228\n",
      "Average test loss: 30655.04422048611\n",
      "Epoch 125/300\n",
      "Average training loss: 0.005075235387517346\n",
      "Average test loss: 117372.66379659233\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0050455762710836195\n",
      "Average test loss: 15810.76718832357\n",
      "Epoch 127/300\n",
      "Average training loss: 0.005022651118950712\n",
      "Average test loss: 60306.642510508456\n",
      "Epoch 128/300\n",
      "Average training loss: 0.005007693385498392\n",
      "Average test loss: 215.950252917851\n",
      "Epoch 129/300\n",
      "Average training loss: 0.005093498322284884\n",
      "Average test loss: 820.0397620630016\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004960660652774903\n",
      "Average test loss: 104.59996755766372\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004942938283085823\n",
      "Average test loss: 31433.654054379687\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004934103813022375\n",
      "Average test loss: 205329.16410105128\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004901916948457559\n",
      "Average test loss: 2.6246279597000943\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004899621189054516\n",
      "Average test loss: 30032.967210569383\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004879656881093979\n",
      "Average test loss: 837243182.504\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004864383719861507\n",
      "Average test loss: 4485.507826439063\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0048363652347276605\n",
      "Average test loss: 269.5789027917865\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0048292056504223085\n",
      "Average test loss: 10049.002471154074\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004815743370602529\n",
      "Average test loss: 12940794.057195429\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004793439110741019\n",
      "Average test loss: 234.62020225454413\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004795338891860511\n",
      "Average test loss: 10516.962781584389\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004775784506772955\n",
      "Average test loss: 3865.906659677874\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0047603226241966085\n",
      "Average test loss: 5104426.795172304\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004745178548412191\n",
      "Average test loss: 24593.04745048941\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004714816874100102\n",
      "Average test loss: 13297.883246416715\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0047113335594120955\n",
      "Average test loss: 3.128730227338564\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00468613951984379\n",
      "Average test loss: 243.5416088122593\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004718347688929902\n",
      "Average test loss: 29.686732525021664\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004677845879975292\n",
      "Average test loss: 26.98121020219268\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0046596926239629586\n",
      "Average test loss: 97379.3740738861\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0046583433697621026\n",
      "Average test loss: 14423.666816550534\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00463532206374738\n",
      "Average test loss: 1282033409.0258245\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004630040854215622\n",
      "Average test loss: 6285.153200961545\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004609650678932667\n",
      "Average test loss: 17540.329243251363\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004588034240321981\n",
      "Average test loss: 6582966.313163409\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004599212882005506\n",
      "Average test loss: 2847.1276636417274\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004572003682454427\n",
      "Average test loss: 836570.7409480858\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0045615212929745515\n",
      "Average test loss: 0.0055204514393375976\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0045418189490834876\n",
      "Average test loss: 2716303.5035555554\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004546579601036178\n",
      "Average test loss: 59.09888063405154\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004530212995078829\n",
      "Average test loss: 918.4115698218246\n",
      "Epoch 162/300\n",
      "Average training loss: 0.00450847354841729\n",
      "Average test loss: 16909.725371724897\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004499227509730392\n",
      "Average test loss: 698676.8504514135\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004493656996637583\n",
      "Average test loss: 760273.4715295488\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004476141463965177\n",
      "Average test loss: 1783232.9354769662\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004601756798310412\n",
      "Average test loss: 1153749.6180405456\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004691919520911243\n",
      "Average test loss: 19648100.240800485\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004594365973025561\n",
      "Average test loss: 39279568.332444444\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0045942987182901966\n",
      "Average test loss: 154136275.0724391\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0046113088954654\n",
      "Average test loss: 29.24781897180714\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004543319235245387\n",
      "Average test loss: 1701678.2608333332\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00450732550645868\n",
      "Average test loss: 6599.091021553874\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004433490455150604\n",
      "Average test loss: 1928.4939478549743\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004453464758892854\n",
      "Average test loss: 61417063713.265816\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00452384969426526\n",
      "Average test loss: 4976.884735013392\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004433842170155711\n",
      "Average test loss: 0.017667266263729997\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004399851616885927\n",
      "Average test loss: 26321.916920984495\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004375142055460148\n",
      "Average test loss: 3749.16848173966\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0043743021691011056\n",
      "Average test loss: 141971.29222585142\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004369253917286793\n",
      "Average test loss: 124070.91402622726\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004354255226751169\n",
      "Average test loss: 3473594.8590032505\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00439669338054955\n",
      "Average test loss: 4402.223237959395\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004394903954532412\n",
      "Average test loss: 173.09571879419437\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004353578838416272\n",
      "Average test loss: 220145.58044281683\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00432578570706149\n",
      "Average test loss: 271.0570699436743\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004333146179715792\n",
      "Average test loss: 164298.51589086608\n",
      "Epoch 187/300\n",
      "Average training loss: 0.00433830963820219\n",
      "Average test loss: 246334.04284441884\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004545218075315158\n",
      "Average test loss: 320384.35507535574\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004567514795189103\n",
      "Average test loss: 47317163.0114476\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004322368779530128\n",
      "Average test loss: 3790.027745351477\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0042981023542169065\n",
      "Average test loss: 27.91930898317736\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004344023146977028\n",
      "Average test loss: 5.916359451306363\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004257336491511928\n",
      "Average test loss: 4664.537045075782\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004259379238925046\n",
      "Average test loss: 14568621.602273216\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0042502480679088165\n",
      "Average test loss: 180350.45681286932\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004245581102867921\n",
      "Average test loss: 21968928.512\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004235846722291575\n",
      "Average test loss: 435522072.6730618\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004274850542760558\n",
      "Average test loss: 2443.004161753383\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004267101891752746\n",
      "Average test loss: 2025231.9002106416\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004236115963094764\n",
      "Average test loss: 8061.225583400594\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004226420463373264\n",
      "Average test loss: 8.050288539436128\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004214392283103532\n",
      "Average test loss: 4875726.524797175\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004208076908563574\n",
      "Average test loss: 11945.15080152406\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004194258135639959\n",
      "Average test loss: 37848.111357572656\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004184604734389318\n",
      "Average test loss: 121210518.2414879\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004184590147187313\n",
      "Average test loss: 140373.5827217344\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00417901830168234\n",
      "Average test loss: 4397707.979605163\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004162721751464738\n",
      "Average test loss: 7909.597211892556\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004175921449644698\n",
      "Average test loss: 125534.95796301807\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0041554766038639675\n",
      "Average test loss: 5133646.937855067\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0041462177203761205\n",
      "Average test loss: 265089.10789204895\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004150088799496492\n",
      "Average test loss: 2943.0041900971723\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004135288551449776\n",
      "Average test loss: 7941.462936663841\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004134745428131686\n",
      "Average test loss: 1700519.9022531451\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004118998404592276\n",
      "Average test loss: 3514.3477774842027\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004117230462117327\n",
      "Average test loss: 380343.0457598967\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004115699167052905\n",
      "Average test loss: 45079.491757641605\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00410909901600745\n",
      "Average test loss: 346616.9529607568\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0041142778934703935\n",
      "Average test loss: 659557.8507273754\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004096689055570298\n",
      "Average test loss: 3047809.4608749384\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004106603704806831\n",
      "Average test loss: 2979.7490985211457\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004077826348029905\n",
      "Average test loss: 243279033.51222223\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004096818593641122\n",
      "Average test loss: 38489075.175882034\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004081887690143453\n",
      "Average test loss: 299429.7472222222\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0040834005541271635\n",
      "Average test loss: 233069618.99648654\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004071255986475282\n",
      "Average test loss: 27080108.145888887\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004073215427084101\n",
      "Average test loss: 9004.601588739793\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004066401867195964\n",
      "Average test loss: 109815.53254576775\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004104044987923569\n",
      "Average test loss: 2968.7495040495032\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004094118186169201\n",
      "Average test loss: 12013105.998162892\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004060664865291781\n",
      "Average test loss: 3220.2595017624426\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004030105454640256\n",
      "Average test loss: 2098.271210598893\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004018985270212094\n",
      "Average test loss: 6006571.481312623\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004036749337282446\n",
      "Average test loss: 1841.6876663249757\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004021722300805979\n",
      "Average test loss: 35994.813835521236\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004017202153387997\n",
      "Average test loss: 27548.715316593614\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004019399265862173\n",
      "Average test loss: 2136211.554516474\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0040146702538347906\n",
      "Average test loss: 1213.6912925596962\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004009233129728172\n",
      "Average test loss: 379063.9194880242\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004000792150075237\n",
      "Average test loss: 5797750.595367392\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003996104514019357\n",
      "Average test loss: 6016382.254272368\n",
      "Epoch 242/300\n",
      "Average training loss: 0.003994724125497871\n",
      "Average test loss: 45.42893541502518\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0039817292942768995\n",
      "Average test loss: 347760.5731114199\n",
      "Epoch 244/300\n",
      "Average training loss: 0.003984648699561755\n",
      "Average test loss: 49.17103312680415\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003978595895485745\n",
      "Average test loss: 5397.765750144301\n",
      "Epoch 246/300\n",
      "Average training loss: 0.003972171687003639\n",
      "Average test loss: 10835.459107357105\n",
      "Epoch 247/300\n",
      "Average training loss: 0.003978839179293977\n",
      "Average test loss: 17330.574937997477\n",
      "Epoch 248/300\n",
      "Average training loss: 0.003971336038783193\n",
      "Average test loss: 301597.4658337094\n",
      "Epoch 249/300\n",
      "Average training loss: 0.003960371307200856\n",
      "Average test loss: 20818.711306859353\n",
      "Epoch 250/300\n",
      "Average training loss: 0.003948599916365411\n",
      "Average test loss: 0.011506621189415455\n",
      "Epoch 251/300\n",
      "Average training loss: 0.003948971760976645\n",
      "Average test loss: 19.572038372887505\n",
      "Epoch 252/300\n",
      "Average training loss: 0.003945071049448517\n",
      "Average test loss: 834.8569595620716\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0039458257110996375\n",
      "Average test loss: 866.172966102812\n",
      "Epoch 254/300\n",
      "Average training loss: 0.003941911095132431\n",
      "Average test loss: 42487.6975794067\n",
      "Epoch 255/300\n",
      "Average training loss: 0.003934730793452925\n",
      "Average test loss: 917434.4104574702\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003932508148252964\n",
      "Average test loss: 1111.8858493874272\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0039389731689459745\n",
      "Average test loss: 27393504.417333335\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0039357287771999835\n",
      "Average test loss: 1648.8386700721321\n",
      "Epoch 259/300\n",
      "Average training loss: 0.003912678273187743\n",
      "Average test loss: 499566948.06755555\n",
      "Epoch 260/300\n",
      "Average training loss: 0.003910543265855974\n",
      "Average test loss: 21215.631648102655\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003898998584598303\n",
      "Average test loss: 11311165.861581845\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0039045509981612365\n",
      "Average test loss: 2288.4977139906537\n",
      "Epoch 263/300\n",
      "Average training loss: 0.003915345822771391\n",
      "Average test loss: 206692.19957859055\n",
      "Epoch 264/300\n",
      "Average training loss: 0.003904076832243138\n",
      "Average test loss: 94671.81245400966\n",
      "Epoch 265/300\n",
      "Average training loss: 0.003894734381801552\n",
      "Average test loss: 46549.02672989992\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0038834531576269204\n",
      "Average test loss: 4045.370220491257\n",
      "Epoch 267/300\n",
      "Average training loss: 0.003889464944187138\n",
      "Average test loss: 1413.6627145907332\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0038790579959750177\n",
      "Average test loss: 154.77391234294117\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0038747900716132586\n",
      "Average test loss: 514675.2595048084\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0038589501664456393\n",
      "Average test loss: 41.60672189552606\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0038688721586432723\n",
      "Average test loss: 336945.9407020027\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0038730554237133927\n",
      "Average test loss: 7841.772201211112\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0038621401405996747\n",
      "Average test loss: 2497118.590986074\n",
      "Epoch 274/300\n",
      "Average training loss: 0.003860112285448445\n",
      "Average test loss: 12150.149352071574\n",
      "Epoch 275/300\n",
      "Average training loss: 0.003846686931533946\n",
      "Average test loss: 34525.337433123175\n",
      "Epoch 276/300\n",
      "Average training loss: 0.003843773296930724\n",
      "Average test loss: 86674016953.23088\n",
      "Epoch 277/300\n",
      "Average training loss: 0.003877059326817592\n",
      "Average test loss: 20235.33843240696\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0038469642208268243\n",
      "Average test loss: 10129118.969764851\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0038452400606539512\n",
      "Average test loss: 93911.47888070512\n",
      "Epoch 280/300\n",
      "Average training loss: 0.003831237353177534\n",
      "Average test loss: 2322313.7725\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0038229817143744893\n",
      "Average test loss: 91764796.77591425\n",
      "Epoch 282/300\n",
      "Average training loss: 0.003828887776782115\n",
      "Average test loss: 83183.63458613388\n",
      "Epoch 283/300\n",
      "Average training loss: 0.003826570931615101\n",
      "Average test loss: 31861.663076467772\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0038230835418734286\n",
      "Average test loss: 66295542.44776404\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0038117697342402407\n",
      "Average test loss: 17105.429029670675\n",
      "Epoch 286/300\n",
      "Average training loss: 0.003827069664994876\n",
      "Average test loss: 3127242.388289181\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0038116710488167076\n",
      "Average test loss: 215.60888146801085\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0038006858148922525\n",
      "Average test loss: 467394.7841875175\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00380834502975146\n",
      "Average test loss: 18505.459710925792\n",
      "Epoch 290/300\n",
      "Average training loss: 0.003798579782868425\n",
      "Average test loss: 5.900660436561538\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0037880365769896243\n",
      "Average test loss: 5655.844141876194\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0037927980741692914\n",
      "Average test loss: 17348624.605333332\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00379110245903333\n",
      "Average test loss: 74680.33932579537\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0038004906941205264\n",
      "Average test loss: 10358.027031510022\n",
      "Epoch 295/300\n",
      "Average training loss: 0.00379808075643248\n",
      "Average test loss: 2562.5447763666016\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0037739506494253875\n",
      "Average test loss: 1784.9579234238697\n",
      "Epoch 297/300\n",
      "Average training loss: 0.003783678965436088\n",
      "Average test loss: 63.204850053874154\n",
      "Epoch 298/300\n",
      "Average training loss: 0.003778872112847037\n",
      "Average test loss: 1676270.7891458182\n",
      "Epoch 299/300\n",
      "Average training loss: 0.003774348345067766\n",
      "Average test loss: 1504144.2792411614\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0037912492617550825\n",
      "Average test loss: 235157.50385994682\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2158387002473076\n",
      "Average test loss: 0.5810818273093965\n",
      "Epoch 2/300\n",
      "Average training loss: 0.012416946401198704\n",
      "Average test loss: 0.011682041755153073\n",
      "Epoch 3/300\n",
      "Average training loss: 0.010006160856948958\n",
      "Average test loss: 0.007484150445295705\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008868384117881456\n",
      "Average test loss: 0.7099015213284228\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008157573600196175\n",
      "Average test loss: 0.007297719233979781\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0075332099575963285\n",
      "Average test loss: 0.00994452619428436\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006947620187368658\n",
      "Average test loss: 0.017878543925782045\n",
      "Epoch 8/300\n",
      "Average training loss: 0.006531223150177135\n",
      "Average test loss: 3.339205535763668\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006196905050840642\n",
      "Average test loss: 0.0074496308378875255\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005905174169689417\n",
      "Average test loss: 0.30790213779360054\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005653211248831617\n",
      "Average test loss: 0.04322462822496891\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005373845990747214\n",
      "Average test loss: 0.6441747084057166\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0051539118948082125\n",
      "Average test loss: 0.2987181388205952\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004949208437568612\n",
      "Average test loss: 0.008951110772374603\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004773462540573544\n",
      "Average test loss: 0.03725693291342921\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004607012964785099\n",
      "Average test loss: 90.15284287362297\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004442527472351988\n",
      "Average test loss: 1065.3753564423455\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004321067241330942\n",
      "Average test loss: 9057.315235229498\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004182857727838887\n",
      "Average test loss: 70.82945356160236\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00409580035880208\n",
      "Average test loss: 2.9599171892619793\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004009430619577567\n",
      "Average test loss: 64.51583021816776\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0038981592293000885\n",
      "Average test loss: 0.5502460175432886\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003818082207813859\n",
      "Average test loss: 27.271777716288963\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00374275356096526\n",
      "Average test loss: 95.74169766568538\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0036962464244829282\n",
      "Average test loss: 8128.3779753600775\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0036442256619532904\n",
      "Average test loss: 46.66503120574521\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0035703201494697067\n",
      "Average test loss: 23.95772797366894\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003535504241577453\n",
      "Average test loss: 27.049031087322575\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0035369426167259615\n",
      "Average test loss: 12.503182377469209\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0034912738868345815\n",
      "Average test loss: 122992.41210931104\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0034257119809173874\n",
      "Average test loss: 14.608055891102387\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0057798737169553836\n",
      "Average test loss: 92.05322261509961\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004290442225419812\n",
      "Average test loss: 6494.386319858403\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0038611216168436737\n",
      "Average test loss: 435.99350075434626\n",
      "Epoch 35/300\n",
      "Average training loss: 0.00366195360198617\n",
      "Average test loss: 0.1671554704664482\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003554382359195087\n",
      "Average test loss: 2.181805211034086\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0035141717539065413\n",
      "Average test loss: 765.1549693256703\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003441068669367168\n",
      "Average test loss: 26.02944559706996\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0034058277780810993\n",
      "Average test loss: 395.7083575231731\n",
      "Epoch 40/300\n",
      "Average training loss: 0.003386463860463765\n",
      "Average test loss: 21818.889098594227\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006356238024102317\n",
      "Average test loss: 20.16039391932388\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0041526944428268405\n",
      "Average test loss: 0.04213097112708621\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003853477871252431\n",
      "Average test loss: 3343.2345922888426\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003636574916127655\n",
      "Average test loss: 2315.9562055984156\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00354159109832512\n",
      "Average test loss: 5037.008100337378\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0034933973950230413\n",
      "Average test loss: 397.007479351451\n",
      "Epoch 47/300\n",
      "Average training loss: 0.003454709015786648\n",
      "Average test loss: 62298.20668595236\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0034269429915067223\n",
      "Average test loss: 44568.0808755006\n",
      "Epoch 49/300\n",
      "Average training loss: 0.003382952785119414\n",
      "Average test loss: 566.3857530626607\n",
      "Epoch 50/300\n",
      "Average training loss: 0.003362473508550061\n",
      "Average test loss: 114378.34023535754\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003348778856918216\n",
      "Average test loss: 46134.90800153587\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0033158635292202233\n",
      "Average test loss: 1421.4937624686625\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0033678610807077753\n",
      "Average test loss: 0.0556653897907171\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0034057687477519116\n",
      "Average test loss: 12.649749379424586\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0033460158675702083\n",
      "Average test loss: 4.968382924363845\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0032346806650360425\n",
      "Average test loss: 9272.293425550215\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0032617058358672592\n",
      "Average test loss: 3.750086456200315\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0032141900846941606\n",
      "Average test loss: 5.889403329480439\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003172420512470934\n",
      "Average test loss: 3194.8195305038225\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003162601583947738\n",
      "Average test loss: 393.85188463181754\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005601250133373671\n",
      "Average test loss: 0.05485628149906794\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004208407728622357\n",
      "Average test loss: 2.129764890757079\n",
      "Epoch 63/300\n",
      "Average training loss: 0.003640610882598493\n",
      "Average test loss: 2.07909543422444\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0034218648303714063\n",
      "Average test loss: 0.5781840557377371\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0033173457268211577\n",
      "Average test loss: 0.3710681221663124\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0032748538692378335\n",
      "Average test loss: 576.4395233629015\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0032180026682714622\n",
      "Average test loss: 0.009892949509123962\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0032174510012070337\n",
      "Average test loss: 3.27919112618102\n",
      "Epoch 69/300\n",
      "Average training loss: 0.003206931810826063\n",
      "Average test loss: 0.9806883412914144\n",
      "Epoch 70/300\n",
      "Average training loss: 0.003166547424884306\n",
      "Average test loss: 0.07382600451374634\n",
      "Epoch 71/300\n",
      "Average training loss: 0.003166168066776461\n",
      "Average test loss: 15334.521373434209\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0031245545571049055\n",
      "Average test loss: 123.16433838532865\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003107214727956388\n",
      "Average test loss: 0.15669585588946938\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0031303849425166844\n",
      "Average test loss: 135866.42542989497\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0030852181338187723\n",
      "Average test loss: 10.57120628233047\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0030833632602459853\n",
      "Average test loss: 811.1705138471673\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0030536322030756207\n",
      "Average test loss: 253.69638748203053\n",
      "Epoch 78/300\n",
      "Average training loss: 0.003025227127596736\n",
      "Average test loss: 8864.15673566986\n",
      "Epoch 79/300\n",
      "Average training loss: 0.003038046156780587\n",
      "Average test loss: 56718.97919093253\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0029866628856915567\n",
      "Average test loss: 502794.10440897284\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002965277824136946\n",
      "Average test loss: 127653118.73121712\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0045084909829828474\n",
      "Average test loss: 1.804182575042463\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0032271085067962606\n",
      "Average test loss: 0.053782793319059746\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0030986011270433666\n",
      "Average test loss: 0.013602755763361023\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0030237562931660147\n",
      "Average test loss: 3.651333626014077\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0029955856694529456\n",
      "Average test loss: 0.005165129042644468\n",
      "Epoch 87/300\n",
      "Average training loss: 0.002959075256354279\n",
      "Average test loss: 8.704221037045121\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0029465378034445973\n",
      "Average test loss: 0.07679736679254306\n",
      "Epoch 89/300\n",
      "Average training loss: 0.002931197703712516\n",
      "Average test loss: 2133.4729259982637\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003030481996635596\n",
      "Average test loss: 4.431767466419894\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0030115425048602954\n",
      "Average test loss: 0.004519765239208937\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002964675797977381\n",
      "Average test loss: 256.0717362837493\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002989238214989503\n",
      "Average test loss: 1767695528.7916667\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0029220021420882807\n",
      "Average test loss: 7.400931541969379\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0029125715435172122\n",
      "Average test loss: 1625.0504793391808\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002881055845361617\n",
      "Average test loss: 3428.849741079314\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0028691576648917465\n",
      "Average test loss: 3431.845025520419\n",
      "Epoch 98/300\n",
      "Average training loss: 0.002857195893095599\n",
      "Average test loss: 890.9885019901419\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002847802220740252\n",
      "Average test loss: 89.43009763391068\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0028376674053983554\n",
      "Average test loss: 326.4820129815406\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002846530820346541\n",
      "Average test loss: 45642.966135649425\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002804206242577897\n",
      "Average test loss: 1297503.8159248275\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002941242577508092\n",
      "Average test loss: 0.05363546063337061\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002854963131248951\n",
      "Average test loss: 1.1325482800489084\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0028115764800459146\n",
      "Average test loss: 1.8485153070984202\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0027588894754234285\n",
      "Average test loss: 13.034079451015426\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002749658923389183\n",
      "Average test loss: 1656.1454830058267\n",
      "Epoch 108/300\n",
      "Average training loss: 0.003023698438786798\n",
      "Average test loss: 27.090091909357245\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002764229725839363\n",
      "Average test loss: 146.32705311229535\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002724906875234511\n",
      "Average test loss: 1517.91204236518\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0027508987615712816\n",
      "Average test loss: 88.18254308751308\n",
      "Epoch 112/300\n",
      "Average training loss: 0.00270887636165652\n",
      "Average test loss: 888.7624402833812\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002694627622349395\n",
      "Average test loss: 1334.3984795966023\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0027610971921434007\n",
      "Average test loss: 326.5645318051713\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0026891537507375083\n",
      "Average test loss: 839.4527885101265\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0026682665741278064\n",
      "Average test loss: 111.70728392794025\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002654662555704514\n",
      "Average test loss: 994.0093659530293\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0032292203979773656\n",
      "Average test loss: 622.7850140751261\n",
      "Epoch 119/300\n",
      "Average training loss: 0.003043362858187821\n",
      "Average test loss: 93.01440479788019\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0029759644321683382\n",
      "Average test loss: 0.227356474523743\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0030208718824303814\n",
      "Average test loss: 0.013752013224073583\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00294410785784324\n",
      "Average test loss: 0.011829091764986515\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0028888845222277775\n",
      "Average test loss: 15.334944592658845\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0029001859569301206\n",
      "Average test loss: 8.54148049626727\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0028446867521852255\n",
      "Average test loss: 33.47811685626209\n",
      "Epoch 126/300\n",
      "Average training loss: 0.002828320260056191\n",
      "Average test loss: 38410.3366524685\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0027996286851250463\n",
      "Average test loss: 0.5465731195419405\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0028723628115322856\n",
      "Average test loss: 4.054644029357367\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0027578650339403084\n",
      "Average test loss: 160904.56045788538\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002744686825097435\n",
      "Average test loss: 0.003061211082877384\n",
      "Epoch 131/300\n",
      "Average training loss: 0.002699668218692144\n",
      "Average test loss: 184.52483596386762\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0027236157928903896\n",
      "Average test loss: 97.82215613795569\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0026524510513991116\n",
      "Average test loss: 8622.852161269642\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002640331636907326\n",
      "Average test loss: 116.98739748528186\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002646970793915292\n",
      "Average test loss: 5.0105826799087225\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0026170548299948374\n",
      "Average test loss: 981.3161153576633\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002650277282628748\n",
      "Average test loss: 97.65443734413634\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0027342354336546528\n",
      "Average test loss: 1687.7593229223407\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0026059893988486793\n",
      "Average test loss: 15971.359135540542\n",
      "Epoch 140/300\n",
      "Average training loss: 0.002712475567228264\n",
      "Average test loss: 4525333.173889072\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002678655362791485\n",
      "Average test loss: 13154.773692572162\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0026359282286009854\n",
      "Average test loss: 78.96557297936755\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0025891565940239364\n",
      "Average test loss: 222888.66946240116\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0026071881624973483\n",
      "Average test loss: 172243.20959266828\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002586761774909165\n",
      "Average test loss: 156521.95203179843\n",
      "Epoch 146/300\n",
      "Average training loss: 0.003009368835637967\n",
      "Average test loss: 1.8926149618911245\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0028991651290820705\n",
      "Average test loss: 0.019766068785968753\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002748670968123608\n",
      "Average test loss: 4.8489776104059485\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0026875532002498706\n",
      "Average test loss: 113.75402250938448\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002659727416311701\n",
      "Average test loss: 123.73207609636502\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002656378062441945\n",
      "Average test loss: 18.092680418077443\n",
      "Epoch 152/300\n",
      "Average training loss: 0.002625034606601629\n",
      "Average test loss: 710689.7340833333\n",
      "Epoch 153/300\n",
      "Average training loss: 0.003103463199403551\n",
      "Average test loss: 21050.075486189162\n",
      "Epoch 154/300\n",
      "Average training loss: 0.003074631351977587\n",
      "Average test loss: 2.00185652073866\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002747348721449574\n",
      "Average test loss: 0.1483410825725231\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0028589060310688285\n",
      "Average test loss: 59.94449042761988\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002709882605406973\n",
      "Average test loss: 1290.525433948156\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0026341026141825648\n",
      "Average test loss: 41.55528300658034\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00262002192520433\n",
      "Average test loss: 32.91025367825147\n",
      "Epoch 160/300\n",
      "Average training loss: 0.002779245654328002\n",
      "Average test loss: 392.1035865342882\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0026140220617461535\n",
      "Average test loss: 28321.343881618923\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0026083733826461767\n",
      "Average test loss: 97568.40008094501\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0025596213864369525\n",
      "Average test loss: 234.25162859153085\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0025512909282826715\n",
      "Average test loss: 23799.316489467106\n",
      "Epoch 165/300\n",
      "Average training loss: 0.002571430481142468\n",
      "Average test loss: 1644.2610839225094\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0025388689123921923\n",
      "Average test loss: 108.2936807472207\n",
      "Epoch 167/300\n",
      "Average training loss: 0.002524776961447464\n",
      "Average test loss: 17352.404588516507\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0025469164831770793\n",
      "Average test loss: 7.247616984292865\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002540236334212952\n",
      "Average test loss: 81.38016880846148\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002534410020129548\n",
      "Average test loss: 7.380081309841739\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002613406353112724\n",
      "Average test loss: 14435.09543339081\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0026137058929436735\n",
      "Average test loss: 5.351556682981551\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0025875449357554317\n",
      "Average test loss: 0.006859071364212367\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0025364775345143344\n",
      "Average test loss: 101342.99909682033\n",
      "Epoch 175/300\n",
      "Average training loss: 0.002506277399344577\n",
      "Average test loss: 2974.089813902847\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002502009811914629\n",
      "Average test loss: 968.1214131910635\n",
      "Epoch 177/300\n",
      "Average training loss: 0.002547617042437196\n",
      "Average test loss: 0.7441403287818862\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0025172479716647003\n",
      "Average test loss: 28738.78147471757\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0024834420184294383\n",
      "Average test loss: 160.84053272819375\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002506387910288241\n",
      "Average test loss: 206.1449167352917\n",
      "Epoch 181/300\n",
      "Average training loss: 0.002619922031338016\n",
      "Average test loss: 337.48695133115416\n",
      "Epoch 182/300\n",
      "Average training loss: 0.002635322719501952\n",
      "Average test loss: 375.0613250430756\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0025110383580128352\n",
      "Average test loss: 6134.725260718034\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002456790271939503\n",
      "Average test loss: 1404.7307747724644\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00250832114306589\n",
      "Average test loss: 21142.40376856699\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0025447625801381137\n",
      "Average test loss: 148674.5577624011\n",
      "Epoch 187/300\n",
      "Average training loss: 0.002532751444209781\n",
      "Average test loss: 0.015234292133814758\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002471165462086598\n",
      "Average test loss: 1472.5232999674479\n",
      "Epoch 189/300\n",
      "Average training loss: 0.002460703114254607\n",
      "Average test loss: 96.9251129620398\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0025030530542135237\n",
      "Average test loss: 605966.0408171657\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002501020127700435\n",
      "Average test loss: 1774.701369575267\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002504185693648954\n",
      "Average test loss: 3161.4408488682284\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0026130031107200516\n",
      "Average test loss: 6969.861531932349\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00247505891467962\n",
      "Average test loss: 46.66193344453536\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002402742145996955\n",
      "Average test loss: 224.71938087613984\n",
      "Epoch 196/300\n",
      "Average training loss: 0.002393996046649085\n",
      "Average test loss: 20385.69727595251\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0023934003611405692\n",
      "Average test loss: 411445.68242524087\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002387979075519575\n",
      "Average test loss: 2979.8957168839765\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002376787774471773\n",
      "Average test loss: 31719.164735596532\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002375345990475681\n",
      "Average test loss: 89963.32402915273\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002421521528106597\n",
      "Average test loss: 3279935603.397118\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0024632121267625026\n",
      "Average test loss: 1041474.6054012097\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0023838466322049496\n",
      "Average test loss: 1199552.5917027544\n",
      "Epoch 204/300\n",
      "Average training loss: 0.002743152880627248\n",
      "Average test loss: 0.01923633103734917\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0025188530582106777\n",
      "Average test loss: 0.003097795602762037\n",
      "Epoch 206/300\n",
      "Average training loss: 0.002376179334293637\n",
      "Average test loss: 17.00699158594633\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0023555306624621153\n",
      "Average test loss: 10.665867891537854\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0024230933137651947\n",
      "Average test loss: 3377.878264539318\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0024218170824978087\n",
      "Average test loss: 1.0742379918005318\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0024080047257658507\n",
      "Average test loss: 9146.794477431882\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0023773294888022873\n",
      "Average test loss: 452.5293836119862\n",
      "Epoch 212/300\n",
      "Average training loss: 0.002373981113235156\n",
      "Average test loss: 159.52076159532493\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0023967445269227027\n",
      "Average test loss: 13433.974295872373\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00236746854107413\n",
      "Average test loss: 8244043.903555555\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0023621730159761175\n",
      "Average test loss: 327972.4183498264\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0023710765925546488\n",
      "Average test loss: 6.6405591013075576\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0023058208332707485\n",
      "Average test loss: 8979.807366026955\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0022930179416305488\n",
      "Average test loss: 290.0384915131322\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0022908357717096807\n",
      "Average test loss: 38726.60185588653\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002292853333987296\n",
      "Average test loss: 0.742195273966425\n",
      "Epoch 221/300\n",
      "Average training loss: 0.002277735351274411\n",
      "Average test loss: 52121.97333161276\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002272515842380623\n",
      "Average test loss: 4641.114934171761\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0022761625210858056\n",
      "Average test loss: 13024.547980375128\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0022723633135772413\n",
      "Average test loss: 83617325.2151111\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002276036920853787\n",
      "Average test loss: 38.30497269467182\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0023136631432506775\n",
      "Average test loss: 77.64316019413414\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002251069964013166\n",
      "Average test loss: 5.2930170720372764\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002252053854780065\n",
      "Average test loss: 6.665235999100117\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0022450954026232164\n",
      "Average test loss: 142418.44624726957\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0022497512191120123\n",
      "Average test loss: 3748.832027502724\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0022396746328514484\n",
      "Average test loss: 319.5646029764745\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0022402607121815286\n",
      "Average test loss: 11.229072581800736\n",
      "Epoch 233/300\n",
      "Average training loss: 0.002241778201320105\n",
      "Average test loss: 32231.103064999428\n",
      "Epoch 234/300\n",
      "Average training loss: 0.002230982592329383\n",
      "Average test loss: 65075.435066636004\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0022272271460129153\n",
      "Average test loss: 86095.84551059546\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0022507916684779855\n",
      "Average test loss: 81384.82292286704\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0022194068893376323\n",
      "Average test loss: 22669.18461901392\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0022204912302808627\n",
      "Average test loss: 9.504799073669853\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0022174139969671765\n",
      "Average test loss: 13992.50465689652\n",
      "Epoch 240/300\n",
      "Average training loss: 0.002214447585451934\n",
      "Average test loss: 4225.667601686455\n",
      "Epoch 241/300\n",
      "Average training loss: 0.002227903619615568\n",
      "Average test loss: 98011.71882011337\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0022111983129547703\n",
      "Average test loss: 412.3623856330994\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00220984914029638\n",
      "Average test loss: 595.863993199822\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002198554889402456\n",
      "Average test loss: 8533.481351663615\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0021974061059041156\n",
      "Average test loss: 1.7265646111878257\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0021979132158060867\n",
      "Average test loss: 30.587685534357195\n",
      "Epoch 247/300\n",
      "Average training loss: 0.002190122741378016\n",
      "Average test loss: 141900.01965456814\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002201687827706337\n",
      "Average test loss: 43668.44989508527\n",
      "Epoch 249/300\n",
      "Average training loss: 0.002207247119396925\n",
      "Average test loss: 11.115460595331259\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0021807636179857785\n",
      "Average test loss: 198.7077420158506\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002178297768657406\n",
      "Average test loss: 31618.818037970846\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0021798974720554217\n",
      "Average test loss: 40401.322905422996\n",
      "Epoch 253/300\n",
      "Average training loss: 0.002171967663698726\n",
      "Average test loss: 18746.508729371046\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0021803604091207188\n",
      "Average test loss: 12322.849442863102\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002168972471108039\n",
      "Average test loss: 2472032.795081059\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002188079859026604\n",
      "Average test loss: 3577.786821445111\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002162554230644471\n",
      "Average test loss: 34865.527442749706\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0021637446634057496\n",
      "Average test loss: 14.986861319899559\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0021632139939400884\n",
      "Average test loss: 86299.95734897193\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002161295462710162\n",
      "Average test loss: 48889.449283181006\n",
      "Epoch 261/300\n",
      "Average training loss: 0.002156045831946863\n",
      "Average test loss: 94398.79144341848\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00215290264868074\n",
      "Average test loss: 15030522.886011962\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0021579156031625137\n",
      "Average test loss: 301259.61148706224\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0021576680260606937\n",
      "Average test loss: 266663.5619888299\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0021690508234832023\n",
      "Average test loss: 71496.30959541711\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002142697078279323\n",
      "Average test loss: 541706.6746693404\n",
      "Epoch 267/300\n",
      "Average training loss: 0.002147269803409775\n",
      "Average test loss: 380556.0778085491\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0021409757228361237\n",
      "Average test loss: 50841614.129332565\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002156732972918285\n",
      "Average test loss: 2430288.336223797\n",
      "Epoch 270/300\n",
      "Average training loss: 0.002141968600038025\n",
      "Average test loss: 780381.8674740355\n",
      "Epoch 271/300\n",
      "Average training loss: 0.002135928618411223\n",
      "Average test loss: 33368.23527421638\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0021329534109681843\n",
      "Average test loss: 24998.203504953868\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002149319830764499\n",
      "Average test loss: 209784.84234214612\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002130204229305188\n",
      "Average test loss: 140.54683183996855\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0021296328546272386\n",
      "Average test loss: 2590.2819779187103\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0021284683079769214\n",
      "Average test loss: 6166.075335263141\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0021204409836274056\n",
      "Average test loss: 5.089850036812325\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0021177856961472168\n",
      "Average test loss: 9243.217693411547\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0021208853603651126\n",
      "Average test loss: 490144.813692742\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0021210657708967727\n",
      "Average test loss: 3421.3106391912243\n",
      "Epoch 281/300\n",
      "Average training loss: 0.002118598211660153\n",
      "Average test loss: 46.053806559871674\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002113139503945907\n",
      "Average test loss: 572294.2295121527\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002111602032971051\n",
      "Average test loss: 48262.8988095387\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0021044745242430104\n",
      "Average test loss: 299754.39054944675\n",
      "Epoch 285/300\n",
      "Average training loss: 0.002105553385284212\n",
      "Average test loss: 104749.50014093342\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002107535480418139\n",
      "Average test loss: 3265371.6503939745\n",
      "Epoch 287/300\n",
      "Average training loss: 0.00210061535363396\n",
      "Average test loss: 19597286.753981672\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002134172826591465\n",
      "Average test loss: 3432.6086141730198\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0021056415440721646\n",
      "Average test loss: 5455.522262722439\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0020970816794368954\n",
      "Average test loss: 90.05070288761954\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002104191109745039\n",
      "Average test loss: 2530.54501736063\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0020987376630720164\n",
      "Average test loss: 1071.8552550496581\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0020977474078536034\n",
      "Average test loss: 113077.39332547697\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0020976572971170148\n",
      "Average test loss: 1.3570123829413205\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002086530587532454\n",
      "Average test loss: 50.537815716211995\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00208226159784115\n",
      "Average test loss: 3843.91536015725\n",
      "Epoch 297/300\n",
      "Average training loss: 0.002092157433844275\n",
      "Average test loss: 61039.23312184505\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0020885063242167233\n",
      "Average test loss: 2862515.178347222\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0020826554459830123\n",
      "Average test loss: 3515.5420299787284\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002081850252631638\n",
      "Average test loss: 25.73404312454371\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.17825562999480302\n",
      "Average test loss: 0.008496639620926645\n",
      "Epoch 2/300\n",
      "Average training loss: 0.009146308071083492\n",
      "Average test loss: 0.9011208132372962\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007286986128323608\n",
      "Average test loss: 0.008191997009432978\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006379114087257121\n",
      "Average test loss: 0.005554245683881971\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0058245986551046375\n",
      "Average test loss: 2.189832582935691\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005410067248261637\n",
      "Average test loss: 0.004553058220280542\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005036893405020237\n",
      "Average test loss: 0.11879273652823436\n",
      "Epoch 8/300\n",
      "Average training loss: 0.004720418926328421\n",
      "Average test loss: 0.005390669984122117\n",
      "Epoch 9/300\n",
      "Average training loss: 0.004470212453769313\n",
      "Average test loss: 0.20316027250554827\n",
      "Epoch 10/300\n",
      "Average training loss: 0.004249986932095554\n",
      "Average test loss: 0.004839627385346426\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004048742823095785\n",
      "Average test loss: 0.003631497963021199\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0038811077976392376\n",
      "Average test loss: 0.01019814397705098\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0037077021116597784\n",
      "Average test loss: 2.0917774675207004\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0035603631370597416\n",
      "Average test loss: 0.0031006820350885393\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003408376371074054\n",
      "Average test loss: 0.7635916501482328\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0032999554235074256\n",
      "Average test loss: 0.0029048615565730467\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0032035050189329517\n",
      "Average test loss: 29.1156155175368\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0030854137539863585\n",
      "Average test loss: 0.3171038900038434\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0029838061309936975\n",
      "Average test loss: 0.7371794539913535\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0028910596244451072\n",
      "Average test loss: 0.0026153474445972176\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0028332999987113806\n",
      "Average test loss: 11.298881341209842\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0027499282701561848\n",
      "Average test loss: 2.0099793341668946\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00269851283589378\n",
      "Average test loss: 0.00479798663728353\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002610192597740226\n",
      "Average test loss: 92.57023838976522\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002549342765990231\n",
      "Average test loss: 22.355551081086197\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002514366942768296\n",
      "Average test loss: 2.454844487843414\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0024337638184014293\n",
      "Average test loss: 61.19931636319444\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0024692082955605453\n",
      "Average test loss: 1.921252162148969\n",
      "Epoch 29/300\n",
      "Average training loss: 0.002361985194703771\n",
      "Average test loss: 1.7693867098697358\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0023178554066560337\n",
      "Average test loss: 68.15072582746545\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0023185150904787913\n",
      "Average test loss: 5500.139107575046\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002248246431661149\n",
      "Average test loss: 0.2990097000350555\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0022296808138489725\n",
      "Average test loss: 0.11811571043895351\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0021994762760069635\n",
      "Average test loss: 2.7467483204288614\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002190079251407749\n",
      "Average test loss: 610.883050126899\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0030635963870833316\n",
      "Average test loss: 0.005161965516292387\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0023058979597149623\n",
      "Average test loss: 0.625017976678287\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0022043897175333567\n",
      "Average test loss: 0.002262194173514015\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002175955960320102\n",
      "Average test loss: 0.5021652092660467\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002152063002395961\n",
      "Average test loss: 12959.071915262908\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003271575030767255\n",
      "Average test loss: 13.634796913043493\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002335126095658375\n",
      "Average test loss: 11.351394495863467\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002228474255754716\n",
      "Average test loss: 0.07656559892164336\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0021999368994600242\n",
      "Average test loss: 1560.08109260899\n",
      "Epoch 45/300\n",
      "Average training loss: 0.002154567393888202\n",
      "Average test loss: 6.542488390539463\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0021350910873669717\n",
      "Average test loss: 639.4519684505512\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00213712863644792\n",
      "Average test loss: 0.06850073882854647\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0021531150180639494\n",
      "Average test loss: 0.025154834396309323\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002184499573169483\n",
      "Average test loss: 1.2406076194178313\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0021521091926842927\n",
      "Average test loss: 0.1661307180582856\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002095410431838698\n",
      "Average test loss: 60.14381954850505\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002073684691141049\n",
      "Average test loss: 23.23544586103575\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0020215792460367083\n",
      "Average test loss: 0.05296098794167241\n",
      "Epoch 54/300\n",
      "Average training loss: 0.002019411673148473\n",
      "Average test loss: 1688.9357539106045\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0025672642599998247\n",
      "Average test loss: 0.032823952614847156\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0021789838328129716\n",
      "Average test loss: 0.13899657108303573\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002084375893076261\n",
      "Average test loss: 490.56836597979816\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002040049937967625\n",
      "Average test loss: 0.542122259085377\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0020224738306262426\n",
      "Average test loss: 0.0916557907451772\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002081343668099079\n",
      "Average test loss: 14.870834769248756\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0020540291995017066\n",
      "Average test loss: 0.6142006343524489\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002023939746949408\n",
      "Average test loss: 0.1872514614297284\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002013664009877377\n",
      "Average test loss: 15620.865189704531\n",
      "Epoch 64/300\n",
      "Average training loss: 0.001997188390698284\n",
      "Average test loss: 26739.05584695118\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0019698425517934892\n",
      "Average test loss: 69730.2650316692\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0019658860254825818\n",
      "Average test loss: 261.5242956543007\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00195252388343215\n",
      "Average test loss: 1732.4246149175026\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0026442169559498627\n",
      "Average test loss: 19.61488201344924\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002243059468973014\n",
      "Average test loss: 3.462955549551381\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0021148236001738243\n",
      "Average test loss: 397.68134626447016\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0021275754825522504\n",
      "Average test loss: 0.6773629567184382\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0020337411854416134\n",
      "Average test loss: 0.32697479636760224\n",
      "Epoch 73/300\n",
      "Average training loss: 0.001967479707983633\n",
      "Average test loss: 22.967377254999345\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0019725945581578544\n",
      "Average test loss: 23.113509554153516\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0019511560438614753\n",
      "Average test loss: 6.851745626178467\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0019763709244628746\n",
      "Average test loss: 118.17260381587843\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0019456860075394312\n",
      "Average test loss: 0.12148920068206887\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0021726502131463755\n",
      "Average test loss: 50.94824907677786\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0020334447004521886\n",
      "Average test loss: 2.820562645614354\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002011395006026659\n",
      "Average test loss: 0.039288060111304125\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0019289474211012323\n",
      "Average test loss: 20273.741554301767\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0019083621818572283\n",
      "Average test loss: 16.154554390514786\n",
      "Epoch 83/300\n",
      "Average training loss: 0.001948168250421683\n",
      "Average test loss: 39.084283697605755\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0018908603969547484\n",
      "Average test loss: 0.23750780839059088\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0018462555600951116\n",
      "Average test loss: 20533.150330020526\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0018376924061319895\n",
      "Average test loss: 14841.525685400005\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0019364805299167832\n",
      "Average test loss: 103.44369839109066\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0019040632415562868\n",
      "Average test loss: 0.6544636624757615\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0018773618046608236\n",
      "Average test loss: 1.7785292890372997\n",
      "Epoch 90/300\n",
      "Average training loss: 0.001982560796145764\n",
      "Average test loss: 72073.75794117122\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0018393791122362018\n",
      "Average test loss: 484501.0754295301\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0019531998997554183\n",
      "Average test loss: 12.78018524603318\n",
      "Epoch 93/300\n",
      "Average training loss: 0.001966611673641536\n",
      "Average test loss: 2.7870081899381347\n",
      "Epoch 94/300\n",
      "Average training loss: 0.001928920925905307\n",
      "Average test loss: 6472.954877158949\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0018883125325664877\n",
      "Average test loss: 2028.5698962549086\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0019335880608608326\n",
      "Average test loss: 1.2736556967792825\n",
      "Epoch 97/300\n",
      "Average training loss: 0.001983737064111564\n",
      "Average test loss: 1.5141099661258566\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0018730207168393665\n",
      "Average test loss: 22.18988405751468\n",
      "Epoch 99/300\n",
      "Average training loss: 0.001834428691615661\n",
      "Average test loss: 10379.596816453442\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0018175570473281874\n",
      "Average test loss: 1325.4549732439818\n",
      "Epoch 101/300\n",
      "Average training loss: 0.001776701981201768\n",
      "Average test loss: 1042.0567780852878\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0018926657644203967\n",
      "Average test loss: 458.1470700118335\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0019174754198433624\n",
      "Average test loss: 0.0028754350882437495\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0018400294890420304\n",
      "Average test loss: 80.94287882371205\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0018201176188886166\n",
      "Average test loss: 45.890712420074266\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0018250956522921722\n",
      "Average test loss: 1564.2153467368814\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0018315760184907251\n",
      "Average test loss: 168.45210032541698\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0017674548420020275\n",
      "Average test loss: 123.5394260138948\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0017607586204798685\n",
      "Average test loss: 2485.7273805024274\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0018002208505446713\n",
      "Average test loss: 0.6974243836756796\n",
      "Epoch 111/300\n",
      "Average training loss: 0.001801153100716571\n",
      "Average test loss: 1968.2515623141237\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0017375208915521702\n",
      "Average test loss: 165680.25396474247\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0017153879498235053\n",
      "Average test loss: 145327.14100593686\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0017082795456258787\n",
      "Average test loss: 19.14226741339763\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0017991575847069422\n",
      "Average test loss: 0.13705616437974905\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0017456636542661323\n",
      "Average test loss: 115.42724885546167\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0017374084059976868\n",
      "Average test loss: 6.74102098224312\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0017054638452827931\n",
      "Average test loss: 7641.204078301638\n",
      "Epoch 119/300\n",
      "Average training loss: 0.001805335212395423\n",
      "Average test loss: 982.1910599559992\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0017575411786221796\n",
      "Average test loss: 45866.86637860767\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0017383745461702347\n",
      "Average test loss: 44.25714398956299\n",
      "Epoch 122/300\n",
      "Average training loss: 0.001761464982190066\n",
      "Average test loss: 10.625193371155195\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0017034517475921246\n",
      "Average test loss: 986.411416273329\n",
      "Epoch 124/300\n",
      "Average training loss: 0.001769200627795524\n",
      "Average test loss: 4270.748173391486\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0017549507380980585\n",
      "Average test loss: 1953.6928394507302\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0016662149620759819\n",
      "Average test loss: 441.22300972666983\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0016638147490512993\n",
      "Average test loss: 197.81923704622002\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0016943871707965931\n",
      "Average test loss: 4490.999595412192\n",
      "Epoch 129/300\n",
      "Average training loss: 0.028263318658495944\n",
      "Average test loss: 0.0040681175670276085\n",
      "Epoch 130/300\n",
      "Average training loss: 0.003209695085262259\n",
      "Average test loss: 0.0057345308338602385\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0029266099516923228\n",
      "Average test loss: 0.0029017411118580235\n",
      "Epoch 132/300\n",
      "Average training loss: 0.00274540686669449\n",
      "Average test loss: 0.0214815740454942\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0026237308635479874\n",
      "Average test loss: 0.00863625375305613\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0025254511999794177\n",
      "Average test loss: 0.00837214717310336\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0024416464069444274\n",
      "Average test loss: 0.03895847454004817\n",
      "Epoch 136/300\n",
      "Average training loss: 0.002379025478950805\n",
      "Average test loss: 0.03227896385557122\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002321656790044573\n",
      "Average test loss: 0.5819664434682992\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002270723094335861\n",
      "Average test loss: 0.11373694180324674\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0022214953513402078\n",
      "Average test loss: 0.030920503401094014\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0021817453525339566\n",
      "Average test loss: 0.002492983978655603\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002132772592931158\n",
      "Average test loss: 0.06291808373377555\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00210243346200635\n",
      "Average test loss: 0.933810005824599\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0020720066415766875\n",
      "Average test loss: 0.002432547031332635\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0020404090194238557\n",
      "Average test loss: 0.0030999633057249916\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002000149182768332\n",
      "Average test loss: 0.0022168100937787027\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0019796914838047493\n",
      "Average test loss: 2.041423937540915\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0019734829963288375\n",
      "Average test loss: 2.239753104130427\n",
      "Epoch 148/300\n",
      "Average training loss: 0.001971593927281598\n",
      "Average test loss: 0.3329692054713766\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0019163180113666587\n",
      "Average test loss: 0.08900459949444565\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0019330028752899831\n",
      "Average test loss: 0.6004789143196411\n",
      "Epoch 151/300\n",
      "Average training loss: 0.001905912390910089\n",
      "Average test loss: 0.10174317708404527\n",
      "Epoch 152/300\n",
      "Average training loss: 0.001877164772608214\n",
      "Average test loss: 0.004263490054032041\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001968915722436375\n",
      "Average test loss: 263.03426089389376\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0018691431619226933\n",
      "Average test loss: 23.43724712390287\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0017962189345724054\n",
      "Average test loss: 587.8067384621551\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0017719750651675793\n",
      "Average test loss: 10.502896157140947\n",
      "Epoch 157/300\n",
      "Average training loss: 0.001762145231374436\n",
      "Average test loss: 4.429632453990169\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0017260448944030537\n",
      "Average test loss: 6719.629762105048\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0017240832038223743\n",
      "Average test loss: 0.3420669231696261\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0017209749831300642\n",
      "Average test loss: 0.004168972241588765\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0017212717967728774\n",
      "Average test loss: 1242.7106335817443\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0016867835161586602\n",
      "Average test loss: 2.211332386221737\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0016893716734937495\n",
      "Average test loss: 0.11925105051561777\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0016735761885841687\n",
      "Average test loss: 476.0698862336195\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00163280102579544\n",
      "Average test loss: 527.5097491833998\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0016524083211811053\n",
      "Average test loss: 11.29994747528558\n",
      "Epoch 167/300\n",
      "Average training loss: 0.001627781768432922\n",
      "Average test loss: 1826.915790124596\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0016265009097341035\n",
      "Average test loss: 0.4758468098644581\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0016180556651411784\n",
      "Average test loss: 6.8905393557780314\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0016013981209446987\n",
      "Average test loss: 0.009193121305977305\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0016530232863086792\n",
      "Average test loss: 2.2065419680178167\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0016023197141993377\n",
      "Average test loss: 38.81757308359439\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0016219948059361842\n",
      "Average test loss: 16.99821140387406\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0016514843597801195\n",
      "Average test loss: 6791.925041507257\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0016313753673392866\n",
      "Average test loss: 119.59633560299211\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0015978296572963396\n",
      "Average test loss: 628.6619012210261\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0015829957101701035\n",
      "Average test loss: 29.40869528703971\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0015795269523643785\n",
      "Average test loss: 350.30856841146823\n",
      "Epoch 179/300\n",
      "Average training loss: 0.001573626062936253\n",
      "Average test loss: 30.532449876890414\n",
      "Epoch 180/300\n",
      "Average training loss: 0.001562408580755194\n",
      "Average test loss: 4.086914642127024\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0015621110738979445\n",
      "Average test loss: 1.783046271032033\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0015515198516142038\n",
      "Average test loss: 0.36504540828222204\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001584362089426981\n",
      "Average test loss: 126.15699866793801\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001584546842922767\n",
      "Average test loss: 608.2940181289281\n",
      "Epoch 185/300\n",
      "Average training loss: 0.001555861107384165\n",
      "Average test loss: 3.199653168040017\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0015503437519073487\n",
      "Average test loss: 655.6941092136047\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0015557237532403735\n",
      "Average test loss: 60.031997761160966\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0015774810657733017\n",
      "Average test loss: 123.39834733026889\n",
      "Epoch 189/300\n",
      "Average training loss: 0.001602844530923499\n",
      "Average test loss: 469.20319420277656\n",
      "Epoch 190/300\n",
      "Average training loss: 0.001568346700941523\n",
      "Average test loss: 403.3116646668058\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0016097136440997323\n",
      "Average test loss: 0.16026691426729991\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0015723082152091794\n",
      "Average test loss: 0.018795402149566345\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0015651723115394512\n",
      "Average test loss: 997.043061714545\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0015755059451071753\n",
      "Average test loss: 0.14027898054249172\n",
      "Epoch 195/300\n",
      "Average training loss: 0.001641937849836217\n",
      "Average test loss: 0.16673339796314637\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0016018908461555839\n",
      "Average test loss: 1.6886160676600412\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0015533691914752126\n",
      "Average test loss: 2.9918049951932497\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0015391446794900628\n",
      "Average test loss: 290.4952264574224\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0016042614489172897\n",
      "Average test loss: 309.1006160482969\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0015696329396838943\n",
      "Average test loss: 941.0913238242301\n",
      "Epoch 201/300\n",
      "Average training loss: 0.001593666881426341\n",
      "Average test loss: 19.931896874249396\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0015524312347794573\n",
      "Average test loss: 82.16319417139722\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0015745973617562817\n",
      "Average test loss: 600.6682801719382\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001553185585472319\n",
      "Average test loss: 905.9570828252608\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0015919870496210124\n",
      "Average test loss: 1.274345800380119\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0015569745911699203\n",
      "Average test loss: 14.584029993766919\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0015464468750481803\n",
      "Average test loss: 899.085269094593\n",
      "Epoch 208/300\n",
      "Average training loss: 0.001520294612687495\n",
      "Average test loss: 7.236743272864571\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0015147829529725842\n",
      "Average test loss: 10283.926827459378\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0015057383836764428\n",
      "Average test loss: 1457.4333471204532\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0014976243034212126\n",
      "Average test loss: 113.53222502742656\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0014875212189637953\n",
      "Average test loss: 64.54931089255162\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0014763610147767597\n",
      "Average test loss: 40732.47029861111\n",
      "Epoch 214/300\n",
      "Average training loss: 0.001519705731421709\n",
      "Average test loss: 0.18950081837239366\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00187705323814104\n",
      "Average test loss: 7.019344134016169\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0016305264967183273\n",
      "Average test loss: 192.52179080547475\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0016487178515849842\n",
      "Average test loss: 0.010679389118320412\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0015908992599902882\n",
      "Average test loss: 3.454677048748566\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0016115319616057807\n",
      "Average test loss: 0.00886361017367906\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0016061387999604145\n",
      "Average test loss: 0.5511395029601538\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0015613269802803795\n",
      "Average test loss: 8.5895811188608\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0015320364898070693\n",
      "Average test loss: 2052.2800674395485\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0015247366058950623\n",
      "Average test loss: 2971.6098071717843\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0015338371288445261\n",
      "Average test loss: 84.63649041529413\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0015614004138236244\n",
      "Average test loss: 37.3880224477924\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0015202005783923798\n",
      "Average test loss: 452.820554003367\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001505844971165061\n",
      "Average test loss: 851.24412785229\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0015468790328337088\n",
      "Average test loss: 107.22524436892404\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0015069194337767032\n",
      "Average test loss: 730.1131043388629\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0015364324884075257\n",
      "Average test loss: 1.0703735185828473\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0015271896437431375\n",
      "Average test loss: 19.56382709265666\n",
      "Epoch 232/300\n",
      "Average training loss: 1.0054643922803095\n",
      "Average test loss: 2.443079464831286\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0058084747058649855\n",
      "Average test loss: 0.009866940738012394\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004787393253503574\n",
      "Average test loss: 0.008259895895918211\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004286103493637509\n",
      "Average test loss: 0.3217559359094335\n",
      "Epoch 236/300\n",
      "Average training loss: 0.003963632709864113\n",
      "Average test loss: 0.34891255100319785\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00370113580653237\n",
      "Average test loss: 0.005725790974580579\n",
      "Epoch 238/300\n",
      "Average training loss: 0.003499056656120552\n",
      "Average test loss: 0.0057699195477697585\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0033260012908528248\n",
      "Average test loss: 0.0026007333809716835\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0031596175570868783\n",
      "Average test loss: 0.676518293508225\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0030331687186327247\n",
      "Average test loss: 1.73267486782372\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002917285456218653\n",
      "Average test loss: 0.013593833525975546\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002821768855262134\n",
      "Average test loss: 0.09278310941884087\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0027278264864451355\n",
      "Average test loss: 0.002440711434620122\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0026528592944766085\n",
      "Average test loss: 0.048167763057061365\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002579725693290432\n",
      "Average test loss: 0.002872315203770995\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0025107354211310546\n",
      "Average test loss: 0.019348389569256042\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0024429248991525835\n",
      "Average test loss: 0.040380694071969224\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0023858114493389925\n",
      "Average test loss: 2.5380153587833045\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0023290810922367705\n",
      "Average test loss: 0.736500213706659\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0022772033205255868\n",
      "Average test loss: 0.34387067887849276\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002223034596691529\n",
      "Average test loss: 0.42389179815351963\n",
      "Epoch 253/300\n",
      "Average training loss: 0.002184549641278055\n",
      "Average test loss: 1.4174808580908511\n",
      "Epoch 254/300\n",
      "Average training loss: 0.002131463620087339\n",
      "Average test loss: 0.04881380607187748\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00208101396366126\n",
      "Average test loss: 0.23697463474671046\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0020556544833299187\n",
      "Average test loss: 0.028036245324338478\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002009787096434997\n",
      "Average test loss: 273.92974548085533\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0019764683631559215\n",
      "Average test loss: 2.2603197605800296\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0019366180062707927\n",
      "Average test loss: 4.138673860532542\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0018836449444707896\n",
      "Average test loss: 0.08116392003082566\n",
      "Epoch 261/300\n",
      "Average training loss: 0.001841444009811514\n",
      "Average test loss: 20.036409594641793\n",
      "Epoch 262/300\n",
      "Average training loss: 0.001788107465331753\n",
      "Average test loss: 1342.7122186672307\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0017087429846740431\n",
      "Average test loss: 111.9323129334872\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0016593732486168543\n",
      "Average test loss: 1.8026798654629124\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0025584204141050576\n",
      "Average test loss: 0.002369374129507277\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002307558410283592\n",
      "Average test loss: 0.0020364045080625347\n",
      "Epoch 267/300\n",
      "Average training loss: 0.002013979399887224\n",
      "Average test loss: 0.003256086900209387\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0018763284664601088\n",
      "Average test loss: 4.578234221935272\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0017798828469175432\n",
      "Average test loss: 0.004654129778241946\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0017161911207561691\n",
      "Average test loss: 0.014006058448718654\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0016573136158080566\n",
      "Average test loss: 0.04838335100313028\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0016228783087184032\n",
      "Average test loss: 1.5114845894757245\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0015851607701430719\n",
      "Average test loss: 0.0024823346928589873\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0015586264540130893\n",
      "Average test loss: 0.029942083701491356\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0015390326303119462\n",
      "Average test loss: 0.011396557731760872\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0015304882027622727\n",
      "Average test loss: 2.246172713816166\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0015193028706643316\n",
      "Average test loss: 0.02202481891659813\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0015042809379390545\n",
      "Average test loss: 0.0020312988344166013\n",
      "Epoch 279/300\n",
      "Average training loss: 0.001495465403319233\n",
      "Average test loss: 3.884774586495012\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0014930570540535781\n",
      "Average test loss: 0.0503896930458852\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0014750876679188675\n",
      "Average test loss: 16.813268418112564\n",
      "Epoch 282/300\n",
      "Average training loss: 0.001524552367731101\n",
      "Average test loss: 6.135331333602675\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0015308574280805058\n",
      "Average test loss: 153.61300370713323\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0015001210044655535\n",
      "Average test loss: 22776.045949388317\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0014865931140051949\n",
      "Average test loss: 0.6990478304773569\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0014903126498684287\n",
      "Average test loss: 36.78710273517999\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0015156396247653497\n",
      "Average test loss: 37.92776228896984\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0014725267955412466\n",
      "Average test loss: 250.6991671825637\n",
      "Epoch 289/300\n",
      "Average training loss: 0.001468632399311496\n",
      "Average test loss: 433.80161691946785\n",
      "Epoch 290/300\n",
      "Average training loss: 0.001461113340014385\n",
      "Average test loss: 10.387141199223697\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0014658496066307029\n",
      "Average test loss: 17673.070045970002\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0014672933429376118\n",
      "Average test loss: 781.8577510183463\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0014695240419564976\n",
      "Average test loss: 1377.7437558013496\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0014775502937328484\n",
      "Average test loss: 0.8711504597700097\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0014613575756342876\n",
      "Average test loss: 14.099521492649904\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0014573260980347792\n",
      "Average test loss: 315.0724617389907\n",
      "Epoch 297/300\n",
      "Average training loss: 0.001432082859488825\n",
      "Average test loss: 2768.0067924724085\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0014223582645257315\n",
      "Average test loss: 74638.15808299996\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 354.12093291149534\n",
      "Average test loss: 644.8963056780034\n",
      "Epoch 2/300\n",
      "Average training loss: 0.045510100765360724\n",
      "Average test loss: 2.4870899835791853\n",
      "Epoch 3/300\n",
      "Average training loss: 0.027208038344979285\n",
      "Average test loss: 0.2649017027980751\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01990272011773454\n",
      "Average test loss: 0.010319842917223772\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015808599475357267\n",
      "Average test loss: 12.50546436683668\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013250720656580395\n",
      "Average test loss: 136.88052640797034\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011487537862526046\n",
      "Average test loss: 0.017756239636904664\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01005993655655119\n",
      "Average test loss: 0.11647325855452154\n",
      "Epoch 9/300\n",
      "Average training loss: 0.008939098021222485\n",
      "Average test loss: 0.29735392999980187\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008008921806183126\n",
      "Average test loss: 0.005945414197113779\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007257949578265349\n",
      "Average test loss: 52.31256865580877\n",
      "Epoch 12/300\n",
      "Average training loss: 0.006636542210148441\n",
      "Average test loss: 0.13778521339098612\n",
      "Epoch 13/300\n",
      "Average training loss: 0.006158508182399803\n",
      "Average test loss: 36.30202146507469\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005750508079098331\n",
      "Average test loss: 0.18791549015748832\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0053594143241643905\n",
      "Average test loss: 0.004181329847623904\n",
      "Epoch 16/300\n",
      "Average training loss: 0.005056776243365473\n",
      "Average test loss: 0.21060266743269232\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004776851792302397\n",
      "Average test loss: 0.031673860053221384\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004505822174251079\n",
      "Average test loss: 0.2199624919841687\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004254628700514634\n",
      "Average test loss: 0.004530434012413025\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004025551181286573\n",
      "Average test loss: 0.1353982743144863\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003835919505606095\n",
      "Average test loss: 0.004822164755314589\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003643124971538782\n",
      "Average test loss: 0.1038402847258581\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003482245667113198\n",
      "Average test loss: 0.004252973235108786\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0033300809771236446\n",
      "Average test loss: 0.571423840334846\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0031681419163942337\n",
      "Average test loss: 0.004461068799926175\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0030212065289831826\n",
      "Average test loss: 0.1293540872161587\n",
      "Epoch 27/300\n",
      "Average training loss: 0.002886661884892318\n",
      "Average test loss: 0.004341451111870507\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002763011769702037\n",
      "Average test loss: 0.002999543052684102\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0026614777587561146\n",
      "Average test loss: 0.010743052223490344\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0025839524727521673\n",
      "Average test loss: 0.003758800446573231\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0024711222702430355\n",
      "Average test loss: 3.421252106320527\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002393238180834386\n",
      "Average test loss: 0.046598335532678496\n",
      "Epoch 33/300\n",
      "Average training loss: 0.00232519559820907\n",
      "Average test loss: 0.026860518146202796\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0022573567831681834\n",
      "Average test loss: 5.484608570410456\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0021909684238748416\n",
      "Average test loss: 0.10413028410904937\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002435050078150299\n",
      "Average test loss: 68.6260885322301\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0021356764275373683\n",
      "Average test loss: 0.005846379273053672\n",
      "Epoch 38/300\n",
      "Average training loss: 0.002053075356201993\n",
      "Average test loss: 0.017485829145337143\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002053272596249978\n",
      "Average test loss: 0.29479994764013423\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0019808028709764284\n",
      "Average test loss: 15.765398851598302\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0019233377506542536\n",
      "Average test loss: 30.143149085751837\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0018987006907247835\n",
      "Average test loss: 714.4883853759766\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0019074803160296545\n",
      "Average test loss: 135.90610270964478\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0018318313632367386\n",
      "Average test loss: 4.069506564631851\n",
      "Epoch 45/300\n",
      "Average training loss: 0.001831941008153889\n",
      "Average test loss: 30.798982959877286\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00180255189889835\n",
      "Average test loss: 0.38083385670536923\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0017288650351886948\n",
      "Average test loss: 60.5241145165693\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0017179126440443927\n",
      "Average test loss: 23.628707734995313\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0017069629022023745\n",
      "Average test loss: 1801.8417458366446\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0016614329462043113\n",
      "Average test loss: 4.054490610946384\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002198980516133209\n",
      "Average test loss: 0.8151334721806148\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0018537557215636802\n",
      "Average test loss: 39.5956915597688\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0016785251184677085\n",
      "Average test loss: 135.5281719839308\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0016361973440895477\n",
      "Average test loss: 39.975887891260285\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0016040229419660237\n",
      "Average test loss: 1.1731134808354804\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0015794876096252766\n",
      "Average test loss: 0.12252728794763486\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0016248172578505343\n",
      "Average test loss: 1796.2593414789073\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0015745704407907194\n",
      "Average test loss: 308.21088560056813\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0015390798852054609\n",
      "Average test loss: 35.96438488902141\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0015510802199650141\n",
      "Average test loss: 1929.8952911039592\n",
      "Epoch 61/300\n",
      "Average training loss: 4.249078883437233\n",
      "Average test loss: 0.2981690572732025\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004721139793387718\n",
      "Average test loss: 0.09347996343837844\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0039472579670449095\n",
      "Average test loss: 0.042191392825709446\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003522611638324128\n",
      "Average test loss: 0.06165062471334305\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003235278379172087\n",
      "Average test loss: 0.2657450423263427\n",
      "Epoch 66/300\n",
      "Average training loss: 0.003010297486972478\n",
      "Average test loss: 0.0034942238326701853\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002835854980887638\n",
      "Average test loss: 0.002227515059005883\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0026905121331413586\n",
      "Average test loss: 0.00822988240669171\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0025750724607043795\n",
      "Average test loss: 0.0031524250391456814\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0024764819474269945\n",
      "Average test loss: 0.0020853735997031133\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0023777139141327804\n",
      "Average test loss: 0.00729467596196466\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0022852577155248986\n",
      "Average test loss: 0.0037638784535229206\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0021953562838542794\n",
      "Average test loss: 0.003861105592403975\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002140097856004205\n",
      "Average test loss: 0.1778744253358907\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0020675043699642025\n",
      "Average test loss: 0.23053615776118305\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00200860519738247\n",
      "Average test loss: 3.1232337125672234\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0019616125729969808\n",
      "Average test loss: 0.10394561595676674\n",
      "Epoch 78/300\n",
      "Average training loss: 0.001921999327114059\n",
      "Average test loss: 0.006667394996310274\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0018665106077161101\n",
      "Average test loss: 1.0820446002458532\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0017813575852455364\n",
      "Average test loss: 0.004434632937527365\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0017473036487483315\n",
      "Average test loss: 0.43515652944892647\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0017341434887299936\n",
      "Average test loss: 1.4936487531682683\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0016930983478410377\n",
      "Average test loss: 0.12026867027860134\n",
      "Epoch 85/300\n",
      "Average training loss: 0.001665177137280504\n",
      "Average test loss: 0.0027722170994513564\n",
      "Epoch 86/300\n",
      "Average training loss: 0.001624091021820075\n",
      "Average test loss: 0.031486018777617975\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0015900793307357364\n",
      "Average test loss: 0.2926797602445715\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0015884945771346489\n",
      "Average test loss: 0.19486563075851235\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0015628532864567306\n",
      "Average test loss: 1.1665724524234733\n",
      "Epoch 90/300\n",
      "Average training loss: 0.001565133744540314\n",
      "Average test loss: 0.05554098777472973\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0015433491348392434\n",
      "Average test loss: 13.426725962203824\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0015178121626377105\n",
      "Average test loss: 46.75649422101842\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0015159991391830974\n",
      "Average test loss: 0.12437593522817932\n",
      "Epoch 94/300\n",
      "Average training loss: 0.001489176370927857\n",
      "Average test loss: 19.377611764767103\n",
      "Epoch 95/300\n",
      "Average training loss: 0.001530400839013358\n",
      "Average test loss: 12.002204697544377\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0015067757004871964\n",
      "Average test loss: 381.8391679585245\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0014717956279507941\n",
      "Average test loss: 7.102596313684351\n",
      "Epoch 98/300\n",
      "Average training loss: 0.001461141990445968\n",
      "Average test loss: 456.5319723567967\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0014925829126603072\n",
      "Average test loss: 3.38036307826721\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0014930202748833432\n",
      "Average test loss: 0.006564740869734022\n",
      "Epoch 101/300\n",
      "Average training loss: 0.00144845230359998\n",
      "Average test loss: 593.8742847536297\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0014290196891460153\n",
      "Average test loss: 3.908000725582242\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0014252195865329768\n",
      "Average test loss: 4.550227198008862\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0014208870634643568\n",
      "Average test loss: 0.7195220425764306\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0014351139442167348\n",
      "Average test loss: 0.5568791607916355\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0014677092477472292\n",
      "Average test loss: 0.1696008177747329\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0014005832339947422\n",
      "Average test loss: 0.4489034547143512\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0013663879110891787\n",
      "Average test loss: 0.9560851810789771\n",
      "Epoch 109/300\n",
      "Average training loss: 0.001351328728772286\n",
      "Average test loss: 1.3171321767422681\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0013276143664908077\n",
      "Average test loss: 0.0025682477704766725\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0014587777245582806\n",
      "Average test loss: 2.232085505850199\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0014081072615873482\n",
      "Average test loss: 3.189014485033436\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0013977084934918417\n",
      "Average test loss: 0.29558697302577397\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0013246287141616146\n",
      "Average test loss: 0.12430730322375894\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0013086271406565276\n",
      "Average test loss: 3574.3726304131555\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0013107998900943332\n",
      "Average test loss: 323.50795863060074\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0013125884879587426\n",
      "Average test loss: 485677.7386825087\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0012652834942564369\n",
      "Average test loss: 559.8951719518325\n",
      "Epoch 119/300\n",
      "Average training loss: 0.001249673656343172\n",
      "Average test loss: 1.453514638102303\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0012444999197291004\n",
      "Average test loss: 0.0018369122667031155\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0012257643553117911\n",
      "Average test loss: 27.777995756478877\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0028726398941232925\n",
      "Average test loss: 19.737991206788355\n",
      "Epoch 123/300\n",
      "Average training loss: 0.001625163057819009\n",
      "Average test loss: 950.9689654821789\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0014297939822491672\n",
      "Average test loss: 33.7389398733096\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0013657527216192748\n",
      "Average test loss: 1.2722648059858217\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0013212866273501682\n",
      "Average test loss: 1592972.7344444445\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0013900482196153865\n",
      "Average test loss: 0.45440387006259214\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0013811795018199417\n",
      "Average test loss: 29.21693760670918\n",
      "Epoch 129/300\n",
      "Average training loss: 0.001316144210493399\n",
      "Average test loss: 0.01818269590071092\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0012986682269515264\n",
      "Average test loss: 13.332838699334612\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0012467569269550343\n",
      "Average test loss: 22523.025277585963\n",
      "Epoch 132/300\n",
      "Average training loss: 0.001234153918352806\n",
      "Average test loss: 6.933771495870004\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0012267901358298128\n",
      "Average test loss: 235197.38246875\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0012232316186030706\n",
      "Average test loss: 311183.0713142361\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0013229302862245176\n",
      "Average test loss: 0.0339530155778759\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0013794070803042915\n",
      "Average test loss: 8.599419872603482\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0013356121851959162\n",
      "Average test loss: 0.005440138062772651\n",
      "Epoch 138/300\n",
      "Average training loss: 0.001325000460777018\n",
      "Average test loss: 51.84451947339376\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0012912656547915604\n",
      "Average test loss: 5.118235589281553\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0012954991624380152\n",
      "Average test loss: 10.041966581387342\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001258612567351924\n",
      "Average test loss: 246.43024014551193\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0012386302125329772\n",
      "Average test loss: 2330.720180229287\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0012180191536123553\n",
      "Average test loss: 50.25060118603567\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0011780007752693362\n",
      "Average test loss: 3.5350729600615387\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0012051134465469254\n",
      "Average test loss: 0.0013723984346725047\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0011619032737798989\n",
      "Average test loss: 4.822735130058809\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0011521710549067291\n",
      "Average test loss: 0.5285608128006053\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0011573029028562208\n",
      "Average test loss: 348.37473266894244\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0011862202944854896\n",
      "Average test loss: 17.38857912764936\n",
      "Epoch 150/300\n",
      "Average training loss: 2.86418854595995\n",
      "Average test loss: 1406.9687519739998\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0052877615897191895\n",
      "Average test loss: 0.14467413752443262\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0027743429433968333\n",
      "Average test loss: 116.79908349831899\n",
      "Epoch 153/300\n",
      "Average training loss: 0.002429351606302791\n",
      "Average test loss: 3.167265283425649\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0022536468523451023\n",
      "Average test loss: 0.0030773716583434078\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002108096983697679\n",
      "Average test loss: 42.93756245146133\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0020062738383809725\n",
      "Average test loss: 2.8413017283038546\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0019270482844569617\n",
      "Average test loss: 0.03749451759747333\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0018591148250011935\n",
      "Average test loss: 0.22359000119991188\n",
      "Epoch 159/300\n",
      "Average training loss: 0.001787974240982698\n",
      "Average test loss: 0.12018626945465803\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0017403337312862277\n",
      "Average test loss: 0.0857224418806533\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0016835727164935734\n",
      "Average test loss: 0.48922728493561346\n",
      "Epoch 162/300\n",
      "Average training loss: 0.001641713007974128\n",
      "Average test loss: 0.5344413395143734\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0015981707571902208\n",
      "Average test loss: 0.052354599008647106\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0015621553242413534\n",
      "Average test loss: 0.3333409125006033\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0015229135813812414\n",
      "Average test loss: 0.3846078673414886\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0014944734832064972\n",
      "Average test loss: 0.3902343229494161\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0014724378844516144\n",
      "Average test loss: 2.037364809504503\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0014425439551058743\n",
      "Average test loss: 0.9394626486326257\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0014209631666955022\n",
      "Average test loss: 0.09719893369026895\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0013946362939766712\n",
      "Average test loss: 0.001526228052770926\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0013684446092488037\n",
      "Average test loss: 0.6551023712721136\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0013547523334208461\n",
      "Average test loss: 5.005433399290674\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0013314081661196218\n",
      "Average test loss: 24.56913708621015\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0013117083475614588\n",
      "Average test loss: 0.8649245159166554\n",
      "Epoch 175/300\n",
      "Average training loss: 0.001295026844781306\n",
      "Average test loss: 2.7725367975747215\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0012752098309704\n",
      "Average test loss: 0.7941121461163793\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0012721022743110856\n",
      "Average test loss: 0.0035895434290998513\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0012451015268969867\n",
      "Average test loss: 37.26414263661464\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0016036642109975218\n",
      "Average test loss: 1324.6765957301895\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0013385144951235917\n",
      "Average test loss: 4.29454330011095\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0012600517556485202\n",
      "Average test loss: 2.3248960455800924\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0012282216067218946\n",
      "Average test loss: 0.10272681454672582\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0012113401438626978\n",
      "Average test loss: 90.64893454224658\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0012180860792286694\n",
      "Average test loss: 821.5083862857851\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0012008595452643932\n",
      "Average test loss: 358.5914125242974\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0011623963999768925\n",
      "Average test loss: 1.839962706019481\n",
      "Epoch 189/300\n",
      "Average training loss: 0.001155224665792452\n",
      "Average test loss: 370.00407872769836\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0012810799224923055\n",
      "Average test loss: 7.397982932225697\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0011571745594135588\n",
      "Average test loss: 499.1426811056021\n",
      "Epoch 192/300\n",
      "Average training loss: 0.001130478353653517\n",
      "Average test loss: 63.64196006341196\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0011252623772662546\n",
      "Average test loss: 1.893691155865685\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0011191967903222476\n",
      "Average test loss: 0.09107587187882099\n",
      "Epoch 195/300\n",
      "Average training loss: 0.001113070735707879\n",
      "Average test loss: 3.9084754832346094\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0013753448045916027\n",
      "Average test loss: 53866.90009375\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0011498415807986424\n",
      "Average test loss: 0.08389502643255724\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0011162800160754059\n",
      "Average test loss: 21.256133947549596\n",
      "Epoch 199/300\n",
      "Average training loss: 0.00110512999093367\n",
      "Average test loss: 19219301.0576278\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0011168334010160633\n",
      "Average test loss: 24.597615253888897\n",
      "Epoch 201/300\n",
      "Average training loss: 0.001087231167126447\n",
      "Average test loss: 0.17666625082523874\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0011007913113054303\n",
      "Average test loss: 805.7895911766299\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0010769657001623678\n",
      "Average test loss: 119.21816553469209\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0010750339199374946\n",
      "Average test loss: 419633.7590756659\n",
      "Epoch 205/300\n",
      "Average training loss: 0.001075102933527281\n",
      "Average test loss: 25.98197100614208\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004714398283045739\n",
      "Average test loss: 26139.475654997434\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0016517966878083017\n",
      "Average test loss: 617.7201670861464\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0014650932289659976\n",
      "Average test loss: 4858.477667586649\n",
      "Epoch 209/300\n",
      "Average training loss: 0.001364761066933473\n",
      "Average test loss: 1700.5201242550181\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0013003808142255163\n",
      "Average test loss: 567.4920337145068\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0012432250179764297\n",
      "Average test loss: 32264.183992165083\n",
      "Epoch 212/300\n",
      "Average training loss: 0.001197721832121412\n",
      "Average test loss: 213.80823224757114\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0011653262139298022\n",
      "Average test loss: 25288.490625868057\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0011451850897218618\n",
      "Average test loss: 440.95136045605494\n",
      "Epoch 215/300\n",
      "Average training loss: 0.001150943943609794\n",
      "Average test loss: 38.155943318413776\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0011175249924158883\n",
      "Average test loss: 4748.300907904731\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0011070559651901325\n",
      "Average test loss: 476563.5463993056\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0010960663517212703\n",
      "Average test loss: 867.1445678443121\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0010738115260481007\n",
      "Average test loss: 11354.10279493173\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0010878239948716429\n",
      "Average test loss: 0.12349660087304397\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0011213342315103444\n",
      "Average test loss: 9.338404625497706\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0011061726415322888\n",
      "Average test loss: 1568.1212330322824\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0010699570988201433\n",
      "Average test loss: 77.45938414904268\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0010883480300092035\n",
      "Average test loss: 22.879077524130544\n",
      "Epoch 225/300\n",
      "Average training loss: 0.001114954486541036\n",
      "Average test loss: 0.00402593163980378\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0011049046728035642\n",
      "Average test loss: 0.1736052350112651\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001129659100746115\n",
      "Average test loss: 0.3605468518642916\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0011739686109746496\n",
      "Average test loss: 0.15660505746480904\n",
      "Epoch 229/300\n",
      "Average training loss: 0.001178133150252203\n",
      "Average test loss: 0.44298050710662373\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0011207985426299274\n",
      "Average test loss: 1.247419513332761\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0011483145081955526\n",
      "Average test loss: 0.004956918538444572\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0011905264294602804\n",
      "Average test loss: 0.03033821321113242\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0011553661306906078\n",
      "Average test loss: 0.17344241304054028\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0011087286472320557\n",
      "Average test loss: 89.45985565119774\n",
      "Epoch 235/300\n",
      "Average training loss: 0.001098036775055031\n",
      "Average test loss: 1515.1148081262418\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0010901443056451777\n",
      "Average test loss: 802.3571667976076\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0010827959201609096\n",
      "Average test loss: 13.692083706827628\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0010689577775903874\n",
      "Average test loss: 236092.1289310198\n",
      "Epoch 239/300\n",
      "Average training loss: 0.001087152171259125\n",
      "Average test loss: 17095.476767578126\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0011138057209447855\n",
      "Average test loss: 0.05398857715787987\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0011128839143655367\n",
      "Average test loss: 59.55261866576721\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0010867837885808614\n",
      "Average test loss: 17.43102796272064\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0010648090588963694\n",
      "Average test loss: 3.697833834614191\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0010740297343064514\n",
      "Average test loss: 59.61028853386392\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0010693750045158797\n",
      "Average test loss: 373.4467417107787\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0013007666148348814\n",
      "Average test loss: 3.9857381840294432\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0012060600933101442\n",
      "Average test loss: 6777.75565109592\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0010911189405144088\n",
      "Average test loss: 0.1226426633663165\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0010648986673396496\n",
      "Average test loss: 6.8694414796954435\n",
      "Epoch 250/300\n",
      "Average training loss: 0.001043651740377148\n",
      "Average test loss: 341.62480269409343\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0010424317162897852\n",
      "Average test loss: 0.13386990684188074\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0010779513696001636\n",
      "Average test loss: 78.9375794715157\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0010732814451783067\n",
      "Average test loss: 9.927695033304083\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0010857605647502675\n",
      "Average test loss: 5.042792799041917\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0010458168124573097\n",
      "Average test loss: 68.02576259946161\n",
      "Epoch 256/300\n",
      "Average training loss: 0.001062618990894407\n",
      "Average test loss: 8.35460717924767\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0010210646412645778\n",
      "Average test loss: 243.31911480371159\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0010433446882396108\n",
      "Average test loss: 14.419487912287314\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0010558934332544191\n",
      "Average test loss: 0.042087703244139754\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0010337395190985665\n",
      "Average test loss: 36.07021943940181\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0010155020449827943\n",
      "Average test loss: 110.43947431255629\n",
      "Epoch 262/300\n",
      "Average training loss: 0.001046699079219252\n",
      "Average test loss: 2546.2027326022735\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0010628126373307572\n",
      "Average test loss: 0.6949308199626911\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0010188907673582435\n",
      "Average test loss: 666.7503677262602\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0010008488237443897\n",
      "Average test loss: 10.678136240980692\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0010083172242674563\n",
      "Average test loss: 1692.5743389108534\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0009721937927727898\n",
      "Average test loss: 563979.5493012153\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0009936688834180434\n",
      "Average test loss: 52685.20329925701\n",
      "Epoch 271/300\n",
      "Average training loss: 0.001058639821027302\n",
      "Average test loss: 5714.766163286284\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0009828154624750218\n",
      "Average test loss: 46515.1309593383\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0009662605229144295\n",
      "Average test loss: 1203.928122748481\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0009660083953705098\n",
      "Average test loss: 17143.298656463776\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001011826395471063\n",
      "Average test loss: 4339.786462625472\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0009819814527096847\n",
      "Average test loss: 142.5683453790234\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0010072046200641327\n",
      "Average test loss: 443.3071273931902\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0009592402815922267\n",
      "Average test loss: 171.27614520090944\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0009648075182404783\n",
      "Average test loss: 3.194560415903727\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0009783783541060984\n",
      "Average test loss: 48.04093586728898\n",
      "Epoch 281/300\n",
      "Average training loss: 0.000964858373782287\n",
      "Average test loss: 84.84922648085892\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0009535750941787329\n",
      "Average test loss: 247.9924846464234\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0009526398720012771\n",
      "Average test loss: 6139.237834664423\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0012154360279026959\n",
      "Average test loss: 31.973818321358205\n",
      "Epoch 285/300\n",
      "Average training loss: 0.001032234709451182\n",
      "Average test loss: 14027.0614419008\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0009727035317466491\n",
      "Average test loss: 172.34687130439323\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0009623227664269507\n",
      "Average test loss: 36150.08699681908\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0009541676771930522\n",
      "Average test loss: 30882.6807253389\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0009723039537461268\n",
      "Average test loss: 6.981715181212355\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0009492487009200785\n",
      "Average test loss: 30.160744977049944\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0009684970068952276\n",
      "Average test loss: 808.7864910439663\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0009545205755987101\n",
      "Average test loss: 5100.049818586238\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0009373428393672738\n",
      "Average test loss: 968.3480602452453\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0009426957360572285\n",
      "Average test loss: 2937.1615372505457\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0009450503301082386\n",
      "Average test loss: 6881.858684265137\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0009418904612151285\n",
      "Average test loss: 67.92079757178554\n",
      "Epoch 297/300\n",
      "Average training loss: 0.000956465586554259\n",
      "Average test loss: 23.05294531794327\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0010233049753846394\n",
      "Average test loss: 0.003161827931387557\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0009190311304086612\n",
      "Average test loss: 0.0022514842815904153\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0009057869638523293\n",
      "Average test loss: 6.990878735670199\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.1/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: -3.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -2.31\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -4.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -8.59\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -7.45\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -7.89\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -7.92\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -6.40\n",
      "Average PSNR for Projection Layer 8 across 2500 images: -8.36\n",
      "Average PSNR for Projection Layer 9 across 2500 images: -7.15\n",
      "Average PSNR for Projection Layer 10 across 2500 images: -6.25\n",
      "Average PSNR for Projection Layer 11 across 2500 images: -5.08\n",
      "Average PSNR for Projection Layer 12 across 2500 images: -4.35\n",
      "Average PSNR for Projection Layer 13 across 2500 images: -2.05\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 1.03\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 3.36\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 8.62\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 7.94\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 19.43\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 20.90\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 9.00\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 19.63\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 22.43\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 24.71\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 11.96\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 21.10\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 22.34\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 25.35\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.13\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.81\n",
      "Average PSNR for Projection Layer 0 across 2500 images: -6.01\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -9.70\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -11.28\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -11.54\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -10.80\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -12.21\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -14.05\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -11.60\n",
      "Average PSNR for Projection Layer 8 across 2500 images: -9.39\n",
      "Average PSNR for Projection Layer 9 across 2500 images: -10.23\n",
      "Average PSNR for Projection Layer 10 across 2500 images: -9.39\n",
      "Average PSNR for Projection Layer 11 across 2500 images: -7.71\n",
      "Average PSNR for Projection Layer 12 across 2500 images: -1.98\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 0.06\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 8.32\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 10.75\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 13.26\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 11.25\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 18.47\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 22.03\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 23.24\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 8.86\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 16.17\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 20.71\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 19.38\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 24.29\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 23.72\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 0 across 2500 images: -9.77\n",
      "Average PSNR for Projection Layer 1 across 2500 images: -15.01\n",
      "Average PSNR for Projection Layer 2 across 2500 images: -16.11\n",
      "Average PSNR for Projection Layer 3 across 2500 images: -17.26\n",
      "Average PSNR for Projection Layer 4 across 2500 images: -16.84\n",
      "Average PSNR for Projection Layer 5 across 2500 images: -16.27\n",
      "Average PSNR for Projection Layer 6 across 2500 images: -15.80\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -14.90\n",
      "Average PSNR for Projection Layer 8 across 2500 images: -13.91\n",
      "Average PSNR for Projection Layer 9 across 2500 images: -13.87\n",
      "Average PSNR for Projection Layer 10 across 2500 images: -11.90\n",
      "Average PSNR for Projection Layer 11 across 2500 images: -10.28\n",
      "Average PSNR for Projection Layer 12 across 2500 images: -9.19\n",
      "Average PSNR for Projection Layer 13 across 2500 images: -4.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 3.52\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 7.85\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 12.56\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 15.60\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 20.50\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 24.20\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.26\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 14.64\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 19.29\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 23.30\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 24.98\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 24.74\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.15\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 1.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 1.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 3.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 2.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 2.22\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 1.56\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 0.14\n",
      "Average PSNR for Projection Layer 7 across 2500 images: -1.73\n",
      "Average PSNR for Projection Layer 8 across 2500 images: -0.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 2.71\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 7.95\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 12.18\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 14.23\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 17.18\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 11.45\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 16.04\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 20.73\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 23.91\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 21.97\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 15.90\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 20.24\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 25.51\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.97\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.03\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73e93d9-0bd1-4458-8d85-78d2de63f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def zip_folder(source_folder, output_zip_file):\n",
    "    # Ensure the output does not contain an extension (shutil will append .zip)\n",
    "    output_zip_file = output_zip_file.rstrip('.zip')\n",
    "\n",
    "    # Zip the folder\n",
    "    shutil.make_archive(output_zip_file, 'zip', source_folder)\n",
    "\n",
    "# Define the folder to zip and the location of the output file\n",
    "source_folder = 'Memory_Residual-.1'  # Replace with the path to your folder\n",
    "output_zip_file = 'Memory_Residual-.1.zip'  # Replace with the new zip location without .zip\n",
    "\n",
    "# Call the function to zip the folder\n",
    "zip_folder(source_folder, output_zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a298339-0471-487c-9447-ecfe3f276977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
