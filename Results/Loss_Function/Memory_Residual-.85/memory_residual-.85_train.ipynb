{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Unequal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 0.85)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11984441893630557\n",
      "Average test loss: 0.01092091878172424\n",
      "Epoch 2/300\n",
      "Average training loss: 0.046517899327807956\n",
      "Average test loss: 0.009389024864054388\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0413745494534572\n",
      "Average test loss: 0.010342218408154117\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03873923212620947\n",
      "Average test loss: 0.00874684216371841\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03691767263412476\n",
      "Average test loss: 0.00819423331734207\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03513824044499132\n",
      "Average test loss: 0.008620480890903209\n",
      "Epoch 7/300\n",
      "Average training loss: 0.034022664626439415\n",
      "Average test loss: 0.008317220031387277\n",
      "Epoch 8/300\n",
      "Average training loss: 0.033053444835874767\n",
      "Average test loss: 0.00793242485821247\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03238094708323479\n",
      "Average test loss: 0.0079666754487488\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03175298091106945\n",
      "Average test loss: 0.007521012492477894\n",
      "Epoch 11/300\n",
      "Average training loss: 0.031220013333691492\n",
      "Average test loss: 0.007231251495165957\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03062187276615037\n",
      "Average test loss: 0.007131553906533453\n",
      "Epoch 13/300\n",
      "Average training loss: 0.030039166574676833\n",
      "Average test loss: 0.007399666901677847\n",
      "Epoch 14/300\n",
      "Average training loss: 0.029653268160091506\n",
      "Average test loss: 0.006934622877587874\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02924809675746494\n",
      "Average test loss: 0.0070307201552722185\n",
      "Epoch 16/300\n",
      "Average training loss: 0.028932359001702733\n",
      "Average test loss: 0.006953595861378644\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02867747735314899\n",
      "Average test loss: 0.0069112981835173235\n",
      "Epoch 18/300\n",
      "Average training loss: 0.028248935081892544\n",
      "Average test loss: 0.0066381403108437854\n",
      "Epoch 19/300\n",
      "Average training loss: 0.027944637005527816\n",
      "Average test loss: 0.0068042854414218\n",
      "Epoch 20/300\n",
      "Average training loss: 0.027690431505441666\n",
      "Average test loss: 0.006722610743095477\n",
      "Epoch 21/300\n",
      "Average training loss: 0.027428676759203276\n",
      "Average test loss: 0.006643189071781105\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02723137100206481\n",
      "Average test loss: 0.006487729842464129\n",
      "Epoch 23/300\n",
      "Average training loss: 0.027053035297327572\n",
      "Average test loss: 0.0066574790585372185\n",
      "Epoch 24/300\n",
      "Average training loss: 0.026819239914417266\n",
      "Average test loss: 0.0064021906335320735\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026568494651052686\n",
      "Average test loss: 0.006496173363178969\n",
      "Epoch 26/300\n",
      "Average training loss: 0.026504109218716623\n",
      "Average test loss: 0.006395043163663811\n",
      "Epoch 27/300\n",
      "Average training loss: 0.026275232025318677\n",
      "Average test loss: 0.006338325672679477\n",
      "Epoch 28/300\n",
      "Average training loss: 0.026070166819625432\n",
      "Average test loss: 0.00638165565331777\n",
      "Epoch 29/300\n",
      "Average training loss: 0.025955955603056483\n",
      "Average test loss: 0.006276575069460604\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02595075078970856\n",
      "Average test loss: 0.007138199543787373\n",
      "Epoch 31/300\n",
      "Average training loss: 0.025684471319119134\n",
      "Average test loss: 0.006355728930069341\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02553950835433271\n",
      "Average test loss: 0.006222090488092767\n",
      "Epoch 33/300\n",
      "Average training loss: 0.025438925544420878\n",
      "Average test loss: 0.006160559041632547\n",
      "Epoch 34/300\n",
      "Average training loss: 0.025354178961780337\n",
      "Average test loss: 0.006238874750418796\n",
      "Epoch 35/300\n",
      "Average training loss: 0.025203076847725443\n",
      "Average test loss: 0.006305453095171187\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02511912326514721\n",
      "Average test loss: 0.007400694465471639\n",
      "Epoch 37/300\n",
      "Average training loss: 0.025042237336436908\n",
      "Average test loss: 0.006130280330363247\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024950617457429568\n",
      "Average test loss: 0.006040846678117911\n",
      "Epoch 39/300\n",
      "Average training loss: 0.024771140404045582\n",
      "Average test loss: 0.0061477612488799625\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024707924246788023\n",
      "Average test loss: 0.00622605179498593\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0246994327571657\n",
      "Average test loss: 0.005995879117399454\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02455762275391155\n",
      "Average test loss: 0.006125934465891785\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02455322373410066\n",
      "Average test loss: 0.006351323679503467\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02439012300471465\n",
      "Average test loss: 0.006330024557809035\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024334572735759948\n",
      "Average test loss: 0.006027420830395486\n",
      "Epoch 46/300\n",
      "Average training loss: 0.024264056220650674\n",
      "Average test loss: 0.0060161096648209626\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02428187852435642\n",
      "Average test loss: 0.0060628678749005\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02421758053368992\n",
      "Average test loss: 0.006445820652362373\n",
      "Epoch 49/300\n",
      "Average training loss: 0.024026586554116672\n",
      "Average test loss: 0.006002328288224008\n",
      "Epoch 50/300\n",
      "Average training loss: 0.024027139451768664\n",
      "Average test loss: 0.006165287750876612\n",
      "Epoch 51/300\n",
      "Average training loss: 0.023983450149496398\n",
      "Average test loss: 0.006807980487330092\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02389114429884487\n",
      "Average test loss: 0.00596431987794737\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02387134821381834\n",
      "Average test loss: 0.006025818751090103\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023765727289848857\n",
      "Average test loss: 0.005993067526568969\n",
      "Epoch 55/300\n",
      "Average training loss: 0.023748047658138805\n",
      "Average test loss: 0.005932385757565498\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02370600965950224\n",
      "Average test loss: 0.006303775616818004\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023610932826995848\n",
      "Average test loss: 0.005953103324605359\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02359470444586542\n",
      "Average test loss: 0.006572911169379949\n",
      "Epoch 59/300\n",
      "Average training loss: 0.023525938954618242\n",
      "Average test loss: 0.005908748901138703\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02349212306489547\n",
      "Average test loss: 0.005904329001903534\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02344447668062316\n",
      "Average test loss: 0.006252089929663473\n",
      "Epoch 62/300\n",
      "Average training loss: 0.023403513186507754\n",
      "Average test loss: 0.007092418116413885\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02336948279208607\n",
      "Average test loss: 0.006210973769012425\n",
      "Epoch 64/300\n",
      "Average training loss: 0.023335874219735462\n",
      "Average test loss: 0.006133723904689153\n",
      "Epoch 65/300\n",
      "Average training loss: 0.023401759314868185\n",
      "Average test loss: 25.997201839870876\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06069886386063364\n",
      "Average test loss: 0.00715891737697853\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0309694427549839\n",
      "Average test loss: 0.00662183280951447\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02845767284764184\n",
      "Average test loss: 0.006393679800546831\n",
      "Epoch 69/300\n",
      "Average training loss: 0.026997301500704552\n",
      "Average test loss: 0.006093269206997421\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02588311146199703\n",
      "Average test loss: 0.006083912631289826\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02508532317976157\n",
      "Average test loss: 0.005989143870770932\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024459793600771162\n",
      "Average test loss: 0.00608485309407115\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024021022746960323\n",
      "Average test loss: 0.0060073752494321925\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0236604741844866\n",
      "Average test loss: 0.005962799720466137\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023485801064305836\n",
      "Average test loss: 0.006273581946475639\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02338590965833929\n",
      "Average test loss: 0.005879015940344996\n",
      "Epoch 77/300\n",
      "Average training loss: 0.023286813060442606\n",
      "Average test loss: 0.005953875293748246\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02328250041935179\n",
      "Average test loss: 0.006089760476309393\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02318839202158981\n",
      "Average test loss: 0.006611523182441791\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02318633045670059\n",
      "Average test loss: 0.006095105588436127\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023215545030103788\n",
      "Average test loss: 0.006053111245648729\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023130131618844137\n",
      "Average test loss: 0.0063179265922970245\n",
      "Epoch 83/300\n",
      "Average training loss: 0.023076568747560183\n",
      "Average test loss: 0.006303718477901486\n",
      "Epoch 84/300\n",
      "Average training loss: 0.023080579221248627\n",
      "Average test loss: 0.005969852226061953\n",
      "Epoch 85/300\n",
      "Average training loss: 0.023056253761053084\n",
      "Average test loss: 0.005901934112939569\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0229642017616166\n",
      "Average test loss: 0.006420788157731294\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02294648400776916\n",
      "Average test loss: 0.005881261148386532\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022876927552951707\n",
      "Average test loss: 0.005885490299926864\n",
      "Epoch 89/300\n",
      "Average training loss: 0.022907445208893883\n",
      "Average test loss: 0.006021775192270676\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022900559799538717\n",
      "Average test loss: 0.005908834752937158\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022837929932607543\n",
      "Average test loss: 0.00630051616123981\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022736358574695057\n",
      "Average test loss: 0.005896091825846168\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022744019099407726\n",
      "Average test loss: 0.005885160547163751\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022743613911999597\n",
      "Average test loss: 0.0058971764995819995\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022648782211873265\n",
      "Average test loss: 0.005993725959625509\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022641320985224513\n",
      "Average test loss: 0.005950051509671741\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02263495186633534\n",
      "Average test loss: 0.005878267021642791\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022567341478334533\n",
      "Average test loss: 0.00585625821352005\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02257674973540836\n",
      "Average test loss: 0.005933231097128656\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022529179097049767\n",
      "Average test loss: 0.005997072434673707\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022521842295924823\n",
      "Average test loss: 0.006088776946895653\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022480097573664454\n",
      "Average test loss: 0.005926591831776831\n",
      "Epoch 103/300\n",
      "Average training loss: 0.022431546870205137\n",
      "Average test loss: 0.0059176960409515434\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022411378526025347\n",
      "Average test loss: 0.005838080797758367\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02237865394188298\n",
      "Average test loss: 0.0059887137115001675\n",
      "Epoch 106/300\n",
      "Average training loss: 0.022350982730587325\n",
      "Average test loss: 0.005983384978853994\n",
      "Epoch 107/300\n",
      "Average training loss: 0.022329738933179112\n",
      "Average test loss: 0.00586830245786243\n",
      "Epoch 108/300\n",
      "Average training loss: 0.022313009349836245\n",
      "Average test loss: 0.0062569096146358385\n",
      "Epoch 109/300\n",
      "Average training loss: 0.022323542926046585\n",
      "Average test loss: 0.006083869544996155\n",
      "Epoch 110/300\n",
      "Average training loss: 0.022276564677556357\n",
      "Average test loss: 0.005850964765167899\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022248034253716468\n",
      "Average test loss: 0.005920010141614411\n",
      "Epoch 112/300\n",
      "Average training loss: 0.022188483087552918\n",
      "Average test loss: 0.00606678270879719\n",
      "Epoch 113/300\n",
      "Average training loss: 0.022189411386847498\n",
      "Average test loss: 0.006036246401154332\n",
      "Epoch 114/300\n",
      "Average training loss: 0.022150262616574764\n",
      "Average test loss: 0.006697348855021927\n",
      "Epoch 115/300\n",
      "Average training loss: 0.022189755944742096\n",
      "Average test loss: 0.005878766443580389\n",
      "Epoch 116/300\n",
      "Average training loss: 0.022113799719346895\n",
      "Average test loss: 0.005865720369335678\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02209353171288967\n",
      "Average test loss: 0.005894032770767808\n",
      "Epoch 118/300\n",
      "Average training loss: 0.022118507745365303\n",
      "Average test loss: 0.0059206770256989534\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02203945013549593\n",
      "Average test loss: 0.005978392955329683\n",
      "Epoch 120/300\n",
      "Average training loss: 0.022051992547180918\n",
      "Average test loss: 0.005900575549238258\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02199912580433819\n",
      "Average test loss: 0.006000410426821974\n",
      "Epoch 122/300\n",
      "Average training loss: 0.022016539100143645\n",
      "Average test loss: 0.005945547502901819\n",
      "Epoch 123/300\n",
      "Average training loss: 0.021979234006669787\n",
      "Average test loss: 0.006065673064440489\n",
      "Epoch 124/300\n",
      "Average training loss: 0.021957356297307544\n",
      "Average test loss: 0.006309046261012554\n",
      "Epoch 125/300\n",
      "Average training loss: 0.021963189504212804\n",
      "Average test loss: 0.006046569967021545\n",
      "Epoch 126/300\n",
      "Average training loss: 0.021902433888779747\n",
      "Average test loss: 0.005904911517683003\n",
      "Epoch 127/300\n",
      "Average training loss: 0.021899089290036096\n",
      "Average test loss: 0.006031717913432254\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02189671274026235\n",
      "Average test loss: 0.005907494668331411\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02185974332027965\n",
      "Average test loss: 0.005945507252795829\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021861508472098244\n",
      "Average test loss: 0.005837067513830132\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021851354531115957\n",
      "Average test loss: 0.006001343233717813\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021758405062887405\n",
      "Average test loss: 0.005951602110018332\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021776150978273817\n",
      "Average test loss: 0.00616928617325094\n",
      "Epoch 134/300\n",
      "Average training loss: 0.021770345744159487\n",
      "Average test loss: 0.006022288888692856\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021745173081755637\n",
      "Average test loss: 0.0059394000081552396\n",
      "Epoch 136/300\n",
      "Average training loss: 0.021739071115851404\n",
      "Average test loss: 0.005975391466170549\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021710976898670197\n",
      "Average test loss: 0.0059887806034336485\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02170438009334935\n",
      "Average test loss: 0.005916051318248113\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021701668621765244\n",
      "Average test loss: 0.005928073575513231\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021661118543810316\n",
      "Average test loss: 0.00594777523767617\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021683151758379406\n",
      "Average test loss: 0.0063652261801891855\n",
      "Epoch 142/300\n",
      "Average training loss: 0.021680191453960208\n",
      "Average test loss: 0.006319846079995235\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021616623610258102\n",
      "Average test loss: 0.005893798404269748\n",
      "Epoch 144/300\n",
      "Average training loss: 0.021617494136095046\n",
      "Average test loss: 0.0060416907850239015\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021615823225842584\n",
      "Average test loss: 0.006051552372260226\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021541580660475623\n",
      "Average test loss: 0.006042673560066355\n",
      "Epoch 147/300\n",
      "Average training loss: 0.021583777613110013\n",
      "Average test loss: 0.005893574224991931\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021562036871910096\n",
      "Average test loss: 0.005919569791191154\n",
      "Epoch 149/300\n",
      "Average training loss: 0.021526679653260442\n",
      "Average test loss: 0.005882499330159691\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02151058449347814\n",
      "Average test loss: 0.00588611646865805\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02149631222916974\n",
      "Average test loss: 0.006312813824663559\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02149742627474997\n",
      "Average test loss: 0.009278818757169776\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021496389117505815\n",
      "Average test loss: 0.006037684249795145\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02146835267378224\n",
      "Average test loss: 0.0059910638593137265\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02143004258473714\n",
      "Average test loss: 0.005913033386899366\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021422200241022638\n",
      "Average test loss: 0.0059286009300914076\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021433100667264726\n",
      "Average test loss: 0.005814528529221813\n",
      "Epoch 158/300\n",
      "Average training loss: 0.021392494113908873\n",
      "Average test loss: 0.005986439776917298\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021354366108775138\n",
      "Average test loss: 0.005893493332382705\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02136727338698175\n",
      "Average test loss: 0.005988761714763111\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02134890605012576\n",
      "Average test loss: 0.005959477027671205\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021304510947730806\n",
      "Average test loss: 0.005942322307990657\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02135370241271125\n",
      "Average test loss: 0.0063756679859426285\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021321087954772843\n",
      "Average test loss: 0.006027624101688465\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02133306619359387\n",
      "Average test loss: 0.005891660974257522\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021265523857540554\n",
      "Average test loss: 0.0060306874447398715\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021297625737885635\n",
      "Average test loss: 0.006013521225088173\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02124887182480759\n",
      "Average test loss: 0.005979037557625108\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02122813311715921\n",
      "Average test loss: 0.006339286627040969\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021245986154509917\n",
      "Average test loss: 0.00604326847071449\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02122104090121057\n",
      "Average test loss: 0.00609696762305167\n",
      "Epoch 172/300\n",
      "Average training loss: 0.021244274598028925\n",
      "Average test loss: 0.005937057399294443\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021234393695990245\n",
      "Average test loss: 0.005988310058497721\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021187939001454246\n",
      "Average test loss: 0.006007220953289006\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02119410146276156\n",
      "Average test loss: 0.006008735745317406\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021164107395542992\n",
      "Average test loss: 0.006729336948030525\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02114342775113053\n",
      "Average test loss: 0.006169342219001717\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02114417854944865\n",
      "Average test loss: 0.006514677262761527\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021146385547187594\n",
      "Average test loss: 0.005925899640553527\n",
      "Epoch 180/300\n",
      "Average training loss: 0.021144099114669693\n",
      "Average test loss: 0.006039346904390388\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02113766506479846\n",
      "Average test loss: 0.006118303317576647\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02108327127330833\n",
      "Average test loss: 0.006004146008234885\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02105615379744106\n",
      "Average test loss: 0.005936660082803832\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021117227978176542\n",
      "Average test loss: 0.006731570555104149\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021084073669380612\n",
      "Average test loss: 0.006040759162770378\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021047133381168048\n",
      "Average test loss: 0.005950291078123782\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021033301752474573\n",
      "Average test loss: 0.006107008763071563\n",
      "Epoch 188/300\n",
      "Average training loss: 0.021033451507488886\n",
      "Average test loss: 0.006753554155015283\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021038336464100416\n",
      "Average test loss: 0.005975586602671279\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020999779803885354\n",
      "Average test loss: 0.005925138166381253\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021008842887149916\n",
      "Average test loss: 0.005919343262910843\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02100362167921331\n",
      "Average test loss: 0.0059947280573348204\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02096841563946671\n",
      "Average test loss: 0.006057955207096206\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021018917550643286\n",
      "Average test loss: 0.006010345748315255\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02096253895262877\n",
      "Average test loss: 0.006051515503269103\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020939714653624427\n",
      "Average test loss: 0.00673832832235429\n",
      "Epoch 197/300\n",
      "Average training loss: 0.020964452068010966\n",
      "Average test loss: 0.0061178123267988365\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020942850567400455\n",
      "Average test loss: 0.006137310907244682\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020979287068876955\n",
      "Average test loss: 0.006109930646088388\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020939036005073124\n",
      "Average test loss: 0.006075698355833689\n",
      "Epoch 201/300\n",
      "Average training loss: 0.020931172892451288\n",
      "Average test loss: 0.0059790788251492715\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020866887948579257\n",
      "Average test loss: 0.006328901128222545\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020893825489613747\n",
      "Average test loss: 0.006055613230913878\n",
      "Epoch 204/300\n",
      "Average training loss: 0.020901569465796153\n",
      "Average test loss: 0.006233587122211854\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02086758665409353\n",
      "Average test loss: 0.0059824671836362945\n",
      "Epoch 206/300\n",
      "Average training loss: 0.020841480516725117\n",
      "Average test loss: 0.005932248315049542\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0208392803221941\n",
      "Average test loss: 0.00606378717886077\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020857716262340547\n",
      "Average test loss: 0.0060876765511929986\n",
      "Epoch 209/300\n",
      "Average training loss: 0.020833029089702502\n",
      "Average test loss: 0.005975109465420246\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020838215743501982\n",
      "Average test loss: 0.006141335301101208\n",
      "Epoch 211/300\n",
      "Average training loss: 0.020778659264246623\n",
      "Average test loss: 0.006142893715451161\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020805388846331173\n",
      "Average test loss: 0.005992003390358554\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0207953410587377\n",
      "Average test loss: 0.006179516891224516\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020795608811908298\n",
      "Average test loss: 0.006012505123184787\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020793217670586374\n",
      "Average test loss: 0.006099720894462532\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020791201412677766\n",
      "Average test loss: 0.006352273622734679\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020750884215037027\n",
      "Average test loss: 0.006047126036137342\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020769753495852153\n",
      "Average test loss: 0.006177313153114584\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020786199940575494\n",
      "Average test loss: 0.005995043486770656\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02073751097751988\n",
      "Average test loss: 0.006191242792126205\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02071459578971068\n",
      "Average test loss: 0.006204564803176456\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02071497639848126\n",
      "Average test loss: 0.006228777259381281\n",
      "Epoch 223/300\n",
      "Average training loss: 0.020739409948388736\n",
      "Average test loss: 0.006091440990153286\n",
      "Epoch 224/300\n",
      "Average training loss: 0.020714237780206732\n",
      "Average test loss: 0.0062825975943770675\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020671978823012777\n",
      "Average test loss: 0.006026803643753131\n",
      "Epoch 226/300\n",
      "Average training loss: 0.020726329568359587\n",
      "Average test loss: 0.0059471060492926175\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0207277013146215\n",
      "Average test loss: 0.006045813057985571\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02064916947318448\n",
      "Average test loss: 0.0060947654785381425\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02064608905216058\n",
      "Average test loss: 0.006007153575619061\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02063921612666713\n",
      "Average test loss: 0.006001169050733248\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02064375515282154\n",
      "Average test loss: 0.006101719917936457\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020668563233481513\n",
      "Average test loss: 0.006137200765725639\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020685485081540212\n",
      "Average test loss: 0.006520802588098579\n",
      "Epoch 234/300\n",
      "Average training loss: 0.020626580682065753\n",
      "Average test loss: 0.006098276017854611\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02065393151508437\n",
      "Average test loss: 0.005953870162781742\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0206246919416719\n",
      "Average test loss: 0.0059590159977475805\n",
      "Epoch 237/300\n",
      "Average training loss: 0.020588585340314443\n",
      "Average test loss: 0.006045241542160511\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02062900310754776\n",
      "Average test loss: 0.00635156056119336\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02063087889386548\n",
      "Average test loss: 0.006363167455212937\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02057118775281641\n",
      "Average test loss: 0.006271414749324322\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02057780371275213\n",
      "Average test loss: 0.006203076223118438\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020546979058119984\n",
      "Average test loss: 0.006109390527837807\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020560036460558574\n",
      "Average test loss: 0.006155217006802559\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02056076265871525\n",
      "Average test loss: 0.006127226501289341\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020537416484620836\n",
      "Average test loss: 0.0060545999341540865\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020506647882362207\n",
      "Average test loss: 0.005988699124091201\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02055705042514536\n",
      "Average test loss: 0.006149789885514312\n",
      "Epoch 248/300\n",
      "Average training loss: 0.020516232043504716\n",
      "Average test loss: 0.006298322163522244\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020533686861395836\n",
      "Average test loss: 0.006110636967751715\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020510423203309376\n",
      "Average test loss: 0.006102099187672138\n",
      "Epoch 251/300\n",
      "Average training loss: 0.020509691460265055\n",
      "Average test loss: 0.006148323616426852\n",
      "Epoch 252/300\n",
      "Average training loss: 0.020570736621816955\n",
      "Average test loss: 0.006051998540759086\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02050084742738141\n",
      "Average test loss: 0.005994824104838901\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02045130393902461\n",
      "Average test loss: 0.006018069161309136\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02047961181567775\n",
      "Average test loss: 0.006032643306172556\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02044837258093887\n",
      "Average test loss: 0.006067936230036947\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020512787900037237\n",
      "Average test loss: 0.006397939766032828\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020498658501439625\n",
      "Average test loss: 0.006330317853639523\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020446004379954603\n",
      "Average test loss: 0.006237811711513334\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02046117688384321\n",
      "Average test loss: 0.006038173796402083\n",
      "Epoch 261/300\n",
      "Average training loss: 0.020438743121094175\n",
      "Average test loss: 0.006008059753725926\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020453350277410614\n",
      "Average test loss: 0.006076193063623375\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020420258581638335\n",
      "Average test loss: 0.007131593976169825\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02044614224632581\n",
      "Average test loss: 0.006124149924765031\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020439873665571213\n",
      "Average test loss: 0.006100000617404779\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020431045312020515\n",
      "Average test loss: 0.006055641062557697\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020423651417096456\n",
      "Average test loss: 0.006010579324844811\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020415514152910975\n",
      "Average test loss: 0.006306422625564866\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02041258698867427\n",
      "Average test loss: 0.006083802502188418\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020347471212347347\n",
      "Average test loss: 0.0061408542779584725\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0204068503462606\n",
      "Average test loss: 0.005997112108601464\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020380590250922575\n",
      "Average test loss: 0.006212194031518367\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02042049799280034\n",
      "Average test loss: 0.006153344268600146\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02034213594926728\n",
      "Average test loss: 0.006042433225446277\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02034776503096024\n",
      "Average test loss: 0.006828746183051003\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02035142669743962\n",
      "Average test loss: 0.00615140333895882\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02035187975400024\n",
      "Average test loss: 0.006466985217399067\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02031825688150194\n",
      "Average test loss: 0.00615179135153691\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020354474487404028\n",
      "Average test loss: 0.006322667091257042\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020338845493064986\n",
      "Average test loss: 0.006169328356368674\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020304484281275008\n",
      "Average test loss: 0.006138578888442782\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020297702372074126\n",
      "Average test loss: 0.006129688742260138\n",
      "Epoch 283/300\n",
      "Average training loss: 0.020301940712663863\n",
      "Average test loss: 0.0061427993679212195\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020338387371765243\n",
      "Average test loss: 0.0060426817792985175\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020266381493873067\n",
      "Average test loss: 0.006172990377578471\n",
      "Epoch 286/300\n",
      "Average training loss: 0.020321802157494757\n",
      "Average test loss: 0.006142095911006133\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02032028697265519\n",
      "Average test loss: 0.007256451144814492\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020309499813450708\n",
      "Average test loss: 0.006771305636813244\n",
      "Epoch 289/300\n",
      "Average training loss: 0.020260762380229103\n",
      "Average test loss: 0.006212463649196757\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020274885632925562\n",
      "Average test loss: 0.0060754219045241675\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020274815807739894\n",
      "Average test loss: 0.006043134525418281\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02028554154601362\n",
      "Average test loss: 0.006119118049533831\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02029812926716275\n",
      "Average test loss: 0.006158186308211751\n",
      "Epoch 294/300\n",
      "Average training loss: 0.020265198876460392\n",
      "Average test loss: 0.006183025128311581\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020299812535444894\n",
      "Average test loss: 0.0061915210766924755\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02025601630906264\n",
      "Average test loss: 0.006368814162496063\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020223829006155332\n",
      "Average test loss: 0.006239094624502791\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02021590696606371\n",
      "Average test loss: 0.0062418663452068965\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020235216157303917\n",
      "Average test loss: 0.006157624358932177\n",
      "Epoch 300/300\n",
      "Average training loss: 0.020240861690706678\n",
      "Average test loss: 0.006155431530955765\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10270736705263456\n",
      "Average test loss: 0.007682531675530804\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0362769481241703\n",
      "Average test loss: 0.010525376233789656\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03215962686141332\n",
      "Average test loss: 0.006416111072732342\n",
      "Epoch 4/300\n",
      "Average training loss: 0.029574576483832465\n",
      "Average test loss: 0.005696333540396558\n",
      "Epoch 5/300\n",
      "Average training loss: 0.027639576852321626\n",
      "Average test loss: 0.005336745718701018\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02625269774761465\n",
      "Average test loss: 0.006798927747127083\n",
      "Epoch 7/300\n",
      "Average training loss: 0.024971564206812118\n",
      "Average test loss: 0.005505586624145508\n",
      "Epoch 8/300\n",
      "Average training loss: 0.023847232459319963\n",
      "Average test loss: 0.0051873197841147585\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02321071282029152\n",
      "Average test loss: 0.004764191299262974\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022363554192913904\n",
      "Average test loss: 0.004783393628481362\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021743102095193333\n",
      "Average test loss: 0.004610041433324417\n",
      "Epoch 12/300\n",
      "Average training loss: 0.021234157141711976\n",
      "Average test loss: 0.00459177669386069\n",
      "Epoch 13/300\n",
      "Average training loss: 0.020746388872464498\n",
      "Average test loss: 0.004296831908532315\n",
      "Epoch 14/300\n",
      "Average training loss: 0.020436201706528663\n",
      "Average test loss: 0.004303618423226807\n",
      "Epoch 15/300\n",
      "Average training loss: 0.019993984409504468\n",
      "Average test loss: 0.004583249624611603\n",
      "Epoch 16/300\n",
      "Average training loss: 0.019744154796004297\n",
      "Average test loss: 0.004215690683780445\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01943161809278859\n",
      "Average test loss: 0.004338353058116304\n",
      "Epoch 18/300\n",
      "Average training loss: 0.019147966850962903\n",
      "Average test loss: 0.004033546236447162\n",
      "Epoch 19/300\n",
      "Average training loss: 0.018945742677483293\n",
      "Average test loss: 0.004477922891991006\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018669773346847957\n",
      "Average test loss: 0.0039307808629754516\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018543881388174165\n",
      "Average test loss: 0.003892406665616565\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018342808792988458\n",
      "Average test loss: 0.0038101190216839314\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018108805702792274\n",
      "Average test loss: 0.0037966717103910114\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01799552475247118\n",
      "Average test loss: 0.0037805898789730337\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01785308065182633\n",
      "Average test loss: 0.0037852572744919194\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01768486519323455\n",
      "Average test loss: 0.003797733582763208\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017555790331628587\n",
      "Average test loss: 0.003679740651200215\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01749154670867655\n",
      "Average test loss: 0.004186530781702863\n",
      "Epoch 29/300\n",
      "Average training loss: 0.017319126574529543\n",
      "Average test loss: 0.0038660787995904683\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01725566788431671\n",
      "Average test loss: 0.0036803474836051464\n",
      "Epoch 31/300\n",
      "Average training loss: 0.017164042433102926\n",
      "Average test loss: 0.003611655804846022\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017064550399780273\n",
      "Average test loss: 0.003600741318944428\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01695165478769276\n",
      "Average test loss: 0.0036244303197082548\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016903003734019068\n",
      "Average test loss: 0.0036846506759110422\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016829401139583852\n",
      "Average test loss: 0.003564725243176023\n",
      "Epoch 36/300\n",
      "Average training loss: 0.016753870252105924\n",
      "Average test loss: 0.003942355337656207\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0166574657542838\n",
      "Average test loss: 0.003524865916205777\n",
      "Epoch 38/300\n",
      "Average training loss: 0.016602517579992614\n",
      "Average test loss: 0.0037251063879165385\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016534009271197848\n",
      "Average test loss: 0.0035242452787028417\n",
      "Epoch 40/300\n",
      "Average training loss: 0.016515337318181993\n",
      "Average test loss: 0.0036242795412739118\n",
      "Epoch 41/300\n",
      "Average training loss: 0.016472714533408483\n",
      "Average test loss: 0.0035137803107500075\n",
      "Epoch 42/300\n",
      "Average training loss: 0.016427679904633098\n",
      "Average test loss: 0.003460582189882795\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0163432470758756\n",
      "Average test loss: 0.003541511268872354\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0163113357639975\n",
      "Average test loss: 0.003740903002520402\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01627072455982367\n",
      "Average test loss: 0.0035690529284377894\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016198745709326532\n",
      "Average test loss: 0.003466533196469148\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01619224676158693\n",
      "Average test loss: 0.0035119086042460466\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016137040627499422\n",
      "Average test loss: 0.0036073591361443203\n",
      "Epoch 49/300\n",
      "Average training loss: 0.016113164530032212\n",
      "Average test loss: 0.003428811482257313\n",
      "Epoch 50/300\n",
      "Average training loss: 0.016058693979349403\n",
      "Average test loss: 0.003495687130631672\n",
      "Epoch 51/300\n",
      "Average training loss: 0.016025442883372307\n",
      "Average test loss: 0.0034435028351015515\n",
      "Epoch 52/300\n",
      "Average training loss: 0.016012124980489414\n",
      "Average test loss: 0.0034551110768483743\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01596778954234388\n",
      "Average test loss: 0.0034430196839902137\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015926978909307055\n",
      "Average test loss: 0.003463495487968127\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015903374671936036\n",
      "Average test loss: 0.0035997826276967923\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015878343454665607\n",
      "Average test loss: 0.004470219920078913\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015895261477265093\n",
      "Average test loss: 0.003672138647072845\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015798875548773343\n",
      "Average test loss: 0.003471459477312035\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01579093674570322\n",
      "Average test loss: 0.0033898834865540265\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015741436080800163\n",
      "Average test loss: 0.0034359038083089724\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01572443547844887\n",
      "Average test loss: 0.0034182806002597014\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01576827283203602\n",
      "Average test loss: 0.00340619051663412\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015675608449512057\n",
      "Average test loss: 0.0036544339058713782\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015685906550950476\n",
      "Average test loss: 0.0034465926974597905\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015603309720754624\n",
      "Average test loss: 0.003513231640888585\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015627941076126365\n",
      "Average test loss: 0.0034077368943641584\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015583562819494141\n",
      "Average test loss: 0.003398041845609744\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015578926152653164\n",
      "Average test loss: 0.003416626966661877\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015556167685323292\n",
      "Average test loss: 0.003677489611837599\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015506102043721411\n",
      "Average test loss: 0.0035893453864587676\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015501712994443046\n",
      "Average test loss: 0.0034656301058000986\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015533775864375963\n",
      "Average test loss: 0.0033663394757443006\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015443078137106365\n",
      "Average test loss: 0.003719931325564782\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015417470940285258\n",
      "Average test loss: 0.0034268611615730655\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015421092761887445\n",
      "Average test loss: 0.0034028167755653462\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015428141015271346\n",
      "Average test loss: 0.003438813947762052\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015401007429593138\n",
      "Average test loss: 0.003372908317897883\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015357431516879134\n",
      "Average test loss: 0.0033665453725390962\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015387408257772526\n",
      "Average test loss: 0.0034082549259894426\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015357038230531745\n",
      "Average test loss: 0.0034801703438990645\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015308751047485405\n",
      "Average test loss: 0.003391186032237278\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015292372071080737\n",
      "Average test loss: 0.0033931199945509433\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015318131891389688\n",
      "Average test loss: 0.0033437679409980773\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015243639544480377\n",
      "Average test loss: 0.0033796522445562814\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015234293404552672\n",
      "Average test loss: 0.003467919879489475\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01523946218027009\n",
      "Average test loss: 0.0033746660300013093\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015222179372277525\n",
      "Average test loss: 0.003401248050439689\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015213655373288525\n",
      "Average test loss: 0.003462851520213816\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015187583496173223\n",
      "Average test loss: 0.003402313187925352\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015159548697372278\n",
      "Average test loss: 0.003763750895857811\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01516993810236454\n",
      "Average test loss: 0.003350968034317096\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015157124307420519\n",
      "Average test loss: 0.0033355184383690355\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015135358192854458\n",
      "Average test loss: 0.0033808141901261277\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01513493572589424\n",
      "Average test loss: 0.0038531252979818317\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015085378723012077\n",
      "Average test loss: 0.0034307605951196616\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015102232591973411\n",
      "Average test loss: 0.003360600121629735\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01506649099704292\n",
      "Average test loss: 0.003394165841448638\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015072170149948862\n",
      "Average test loss: 0.00336275089201\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015075720571809345\n",
      "Average test loss: 0.0033468352229230934\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015024744131498867\n",
      "Average test loss: 0.003373841196919481\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015040777787566186\n",
      "Average test loss: 0.0034683301884474026\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015051971005896727\n",
      "Average test loss: 0.0033678594055689044\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014986359641783768\n",
      "Average test loss: 0.003458608243200514\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01499313599533505\n",
      "Average test loss: 0.003419977476199468\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014990377829306656\n",
      "Average test loss: 0.0034330580180717838\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01498387871682644\n",
      "Average test loss: 0.00336606263058881\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01495492173731327\n",
      "Average test loss: 0.003498618385858006\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014957882926695877\n",
      "Average test loss: 0.003360449097222752\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014945490598678588\n",
      "Average test loss: 0.003390759344937073\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01491165974156724\n",
      "Average test loss: 0.003435341453593638\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014917005944583152\n",
      "Average test loss: 0.0034378699933489162\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014885756878389252\n",
      "Average test loss: 0.003464236383015911\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014922689518994755\n",
      "Average test loss: 0.003400395321763224\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014874764840635988\n",
      "Average test loss: 0.0035614688257790275\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014880916164153152\n",
      "Average test loss: 0.0033765061597029367\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014874454435375002\n",
      "Average test loss: 0.003376759835622377\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014857848469581868\n",
      "Average test loss: 0.003421707505567206\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014827929760018984\n",
      "Average test loss: 0.003457889010715816\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014821905151009559\n",
      "Average test loss: 0.0033914128076285126\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014845698569383886\n",
      "Average test loss: 0.003376524235846268\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014813139810330339\n",
      "Average test loss: 0.0034048610733201107\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014797604684200551\n",
      "Average test loss: 0.0036478864749272663\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014806364425354534\n",
      "Average test loss: 0.0035273357215854857\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014778350669476721\n",
      "Average test loss: 0.0033498288370254967\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01476370295468304\n",
      "Average test loss: 0.0034259408557166656\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014762573890388013\n",
      "Average test loss: 0.0035395537022915153\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01477352619667848\n",
      "Average test loss: 0.003606835526517696\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014736590635445383\n",
      "Average test loss: 0.0034762220955971215\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014725338142779138\n",
      "Average test loss: 0.00335939409956336\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014733908848630057\n",
      "Average test loss: 0.003466916917512814\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0147052855350905\n",
      "Average test loss: 0.003413827973107497\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014718718772961034\n",
      "Average test loss: 0.0034696816653013228\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014733657916386922\n",
      "Average test loss: 0.0034181691077020434\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014696393778754605\n",
      "Average test loss: 0.0033827802752041153\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014689496319327089\n",
      "Average test loss: 0.0036064995411369534\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014663571131726106\n",
      "Average test loss: 0.003661335045885709\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014668558879858918\n",
      "Average test loss: 0.0034223225423031384\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014656947704652945\n",
      "Average test loss: 0.003414180997759104\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01466164282626576\n",
      "Average test loss: 0.0035858081788238553\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014648340879215134\n",
      "Average test loss: 0.00396632723717226\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014623193076087369\n",
      "Average test loss: 0.003387506990176108\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014645843168099722\n",
      "Average test loss: 0.2084043980439504\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014743275933795505\n",
      "Average test loss: 0.0033608170578049287\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014577996613250838\n",
      "Average test loss: 0.0033541038350926506\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014588770039379596\n",
      "Average test loss: 0.0034312620667947664\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014600818440318108\n",
      "Average test loss: 0.0033505621860838598\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014581728348301516\n",
      "Average test loss: 0.0033898675584544736\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014579868049257332\n",
      "Average test loss: 0.003394366543326113\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014570284287962648\n",
      "Average test loss: 0.0033880932428356674\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01455659034765429\n",
      "Average test loss: 0.0037482982261313334\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014578193614052402\n",
      "Average test loss: 0.0034350423686620264\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014534078281786707\n",
      "Average test loss: 0.0033748214395923747\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014568458428813352\n",
      "Average test loss: 0.0034566430043843055\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014540605684949292\n",
      "Average test loss: 0.0034612462143931125\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014518497375150522\n",
      "Average test loss: 0.0034164145328104494\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014515925957096948\n",
      "Average test loss: 0.003527638514836629\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01455638183405002\n",
      "Average test loss: 0.003522440229025152\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01448337729440795\n",
      "Average test loss: 0.0036099626670281094\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014532123922473855\n",
      "Average test loss: 0.003408908963927792\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01451764874574211\n",
      "Average test loss: 0.003503220235928893\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014479305249949297\n",
      "Average test loss: 0.0034769265262617004\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014461164456274773\n",
      "Average test loss: 0.0033698137955119212\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014489152810639805\n",
      "Average test loss: 0.0034000530919680995\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014477028279668755\n",
      "Average test loss: 0.003503620494570997\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014456000782549381\n",
      "Average test loss: 0.0034701592756642237\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014467963197165066\n",
      "Average test loss: 0.003439586435755094\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014470715511176322\n",
      "Average test loss: 0.003536920390609238\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014432563847137822\n",
      "Average test loss: 0.0034318478796631097\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01444636023375723\n",
      "Average test loss: 0.003406094581923551\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014450186209546195\n",
      "Average test loss: 0.0033785502024822763\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014405880761643251\n",
      "Average test loss: 0.0033905752833104795\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014421175715823967\n",
      "Average test loss: 0.003514381969761517\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014420342332786985\n",
      "Average test loss: 0.0034941664437452952\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014420683927006192\n",
      "Average test loss: 0.0036760347680085234\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014408818643954066\n",
      "Average test loss: 0.0034980699030889404\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014374371079934969\n",
      "Average test loss: 0.0034867616105410787\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01439393949508667\n",
      "Average test loss: 0.003421540258659257\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01439275704489814\n",
      "Average test loss: 0.0034898923033227524\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014366739014784495\n",
      "Average test loss: 0.0034315466144018703\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014369636716941992\n",
      "Average test loss: 0.0034252046642618047\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014394770306017664\n",
      "Average test loss: 0.003432683883027898\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014335736605856153\n",
      "Average test loss: 0.003519883001016246\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014345354235834546\n",
      "Average test loss: 0.0034653132839335336\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014378287990060117\n",
      "Average test loss: 0.003449452231120732\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014331631523867449\n",
      "Average test loss: 0.0035078697732339303\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01436512836896711\n",
      "Average test loss: 0.003549665881320834\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014326528165075514\n",
      "Average test loss: 0.003619130967391862\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014326906267967489\n",
      "Average test loss: 0.00379175757120053\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01430649806227949\n",
      "Average test loss: 0.0034303875824229584\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014324412354164654\n",
      "Average test loss: 0.0034038567236728138\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014314331325391928\n",
      "Average test loss: 0.0035221684724092484\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014290449400742849\n",
      "Average test loss: 0.003477524337876174\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01428838657339414\n",
      "Average test loss: 0.0035249234230981933\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014320242306424511\n",
      "Average test loss: 0.003447197896738847\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014296283135811489\n",
      "Average test loss: 0.0035270672171480127\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014289662960502837\n",
      "Average test loss: 0.0034882613213525876\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014289116248488427\n",
      "Average test loss: 0.003542887881398201\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01429357562545273\n",
      "Average test loss: 0.003818544511993726\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014288271476825078\n",
      "Average test loss: 0.0034653582363906835\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014260229056080183\n",
      "Average test loss: 0.0034041216265824105\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014257090393867758\n",
      "Average test loss: 0.003434860059991479\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014247823229266538\n",
      "Average test loss: 0.00348934358648128\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01424076888213555\n",
      "Average test loss: 0.0035022315393305487\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014258194473054674\n",
      "Average test loss: 0.0036188812454541522\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014222978388269742\n",
      "Average test loss: 0.003425800897181034\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014272132424016794\n",
      "Average test loss: 0.00348337968169815\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014206995359725423\n",
      "Average test loss: 0.0035711798158784705\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014216495713426007\n",
      "Average test loss: 0.008601195797324181\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014205457255244256\n",
      "Average test loss: 0.003430803391461571\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01421058702468872\n",
      "Average test loss: 0.003468993275736769\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014209953205453025\n",
      "Average test loss: 0.003502706424643596\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01420340303166045\n",
      "Average test loss: 0.0036867534352673426\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014218434863620334\n",
      "Average test loss: 0.0034172475044098164\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014201820606986682\n",
      "Average test loss: 0.003407579140530692\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01419213880598545\n",
      "Average test loss: 0.0036325251242766777\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014215956967324018\n",
      "Average test loss: 0.0035248754614343246\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01418252146989107\n",
      "Average test loss: 0.004044753530062735\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014170665204111072\n",
      "Average test loss: 0.0034788435062186587\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014177132224871053\n",
      "Average test loss: 0.0034319859879712263\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014159795232945019\n",
      "Average test loss: 0.0033814607475780778\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014156409590608543\n",
      "Average test loss: 0.0034027833802004657\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014174031000998285\n",
      "Average test loss: 0.0037318160918851694\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014142737913462851\n",
      "Average test loss: 0.0035104720832573044\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014150436784658167\n",
      "Average test loss: 0.0035001266691833733\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01416034947666857\n",
      "Average test loss: 0.0034210237444688876\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014151493094033666\n",
      "Average test loss: 0.0034952537661625278\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014138437637852297\n",
      "Average test loss: 0.003496347334028946\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014133742342392603\n",
      "Average test loss: 0.003660183896000187\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01414233123179939\n",
      "Average test loss: 0.003424174998369482\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014125007325576412\n",
      "Average test loss: 0.0034544956996623013\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014148243241012097\n",
      "Average test loss: 0.0036479993948919905\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01412877505272627\n",
      "Average test loss: 0.0034806420683032935\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014114703254567252\n",
      "Average test loss: 0.0035666423415144284\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014102466639545229\n",
      "Average test loss: 0.0034261255636811256\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014112766116029686\n",
      "Average test loss: 0.0034653484229412346\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014092370505134264\n",
      "Average test loss: 0.003666587710380554\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014115004680222936\n",
      "Average test loss: 0.0036686838111943667\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014078714366588328\n",
      "Average test loss: 0.0035813775652398664\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014080982729792595\n",
      "Average test loss: 0.003522446869975991\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014109226829475827\n",
      "Average test loss: 0.003498316324626406\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014085789532297187\n",
      "Average test loss: 0.0035717508759763508\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014123355756203333\n",
      "Average test loss: 0.003543190440369977\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014072106018662453\n",
      "Average test loss: 0.003411683856199185\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014064601872530249\n",
      "Average test loss: 0.0035044312706838054\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014077394865453244\n",
      "Average test loss: 0.003506994629175299\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014087362272044024\n",
      "Average test loss: 0.003442670732529627\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014072862117654747\n",
      "Average test loss: 0.0035433944674829643\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014058995590441757\n",
      "Average test loss: 0.0034436318501830103\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014038132391870023\n",
      "Average test loss: 0.0035391853749752043\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014068758024109734\n",
      "Average test loss: 0.0034757161382585763\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014026637997064325\n",
      "Average test loss: 0.003676575965558489\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014030592638585302\n",
      "Average test loss: 0.0035934811033722426\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014007500495347712\n",
      "Average test loss: 0.0034456535352187026\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014020894010033872\n",
      "Average test loss: 0.003498192057427433\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014039535519149568\n",
      "Average test loss: 0.0035808588224980568\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014030593197378847\n",
      "Average test loss: 0.0036634452210532295\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01402745150278012\n",
      "Average test loss: 0.00346386142861512\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014010870722432931\n",
      "Average test loss: 0.00344180231106778\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014014709242102172\n",
      "Average test loss: 0.0036003386547995937\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013997777193784713\n",
      "Average test loss: 0.003461041099495358\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014012050728417106\n",
      "Average test loss: 0.0038079943023622037\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014007090522183312\n",
      "Average test loss: 0.0036230571642518043\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013996805955138471\n",
      "Average test loss: 0.003903294713132911\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013987157444987032\n",
      "Average test loss: 0.0034876844787763224\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014000704761180613\n",
      "Average test loss: 0.003472085426458054\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013995553945501645\n",
      "Average test loss: 0.0035510299967394933\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013980259382062488\n",
      "Average test loss: 0.0035855220307906468\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014004693094227048\n",
      "Average test loss: 0.003494273486236731\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01396164295160108\n",
      "Average test loss: 0.0036050899337149328\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013949951925211482\n",
      "Average test loss: 0.0035702145867463615\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0139886156792442\n",
      "Average test loss: 0.0036121745966374874\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01398909509761466\n",
      "Average test loss: 0.0034853996951133013\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013971582745512327\n",
      "Average test loss: 0.004592845741659403\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013949825969835123\n",
      "Average test loss: 0.0034651276752766637\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013956517564753692\n",
      "Average test loss: 0.003569542171847489\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013950157336062856\n",
      "Average test loss: 0.0035653155617829824\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013962719935509894\n",
      "Average test loss: 0.003520901106091009\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01394365992065933\n",
      "Average test loss: 0.003531140706812342\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013945894637869464\n",
      "Average test loss: 0.0035590886464342475\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013945879278083642\n",
      "Average test loss: 0.003597946788494786\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013949256237182352\n",
      "Average test loss: 0.003482232953318291\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013941913874612914\n",
      "Average test loss: 0.003652504410180781\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013940875082380242\n",
      "Average test loss: 0.003946476021160682\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013952008027169439\n",
      "Average test loss: 0.0035753120926933155\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013916090740925736\n",
      "Average test loss: 0.00363178403613468\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013918133286966217\n",
      "Average test loss: 0.003442702635501822\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013919407685597738\n",
      "Average test loss: 0.004217995538065831\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013923882738583618\n",
      "Average test loss: 0.003902529238619738\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01394141457726558\n",
      "Average test loss: 0.003611405954385797\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013909377885361512\n",
      "Average test loss: 0.003551210231251187\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013917136064006222\n",
      "Average test loss: 0.003498779646638367\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013941079232427808\n",
      "Average test loss: 0.0035053081278585726\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013912418730556965\n",
      "Average test loss: 0.0035088555311991107\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01393090392731958\n",
      "Average test loss: 0.0035462300903681254\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013876160377429591\n",
      "Average test loss: 0.0038035695370700626\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013887586282359228\n",
      "Average test loss: 0.003540620610738794\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01388667600436343\n",
      "Average test loss: 0.0035995299373235965\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013888054033948315\n",
      "Average test loss: 0.003981727978214621\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013881679538223479\n",
      "Average test loss: 0.0036626654488758907\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013923779242568546\n",
      "Average test loss: 0.003472016140404675\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.09214389922221501\n",
      "Average test loss: 0.0061769114343656435\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0299769248465697\n",
      "Average test loss: 0.004985236283598675\n",
      "Epoch 3/300\n",
      "Average training loss: 0.026124097420109644\n",
      "Average test loss: 0.004373571782062451\n",
      "Epoch 4/300\n",
      "Average training loss: 0.024052532972560988\n",
      "Average test loss: 0.004218809008391367\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022430087450477813\n",
      "Average test loss: 0.004221027444220251\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02114987120860153\n",
      "Average test loss: 0.0037573114331397746\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019917245670325226\n",
      "Average test loss: 0.003751980843229426\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01896090086301168\n",
      "Average test loss: 0.003624018270522356\n",
      "Epoch 9/300\n",
      "Average training loss: 0.018194355309009553\n",
      "Average test loss: 0.0037577575619022053\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01748624955283271\n",
      "Average test loss: 0.0033819463526209197\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01695006262924936\n",
      "Average test loss: 0.0035724371406767102\n",
      "Epoch 12/300\n",
      "Average training loss: 0.016473376408219338\n",
      "Average test loss: 0.003287968257235156\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016074114073481825\n",
      "Average test loss: 0.0031762610967788433\n",
      "Epoch 14/300\n",
      "Average training loss: 0.015716855017675294\n",
      "Average test loss: 0.0029341934161881606\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0153524008517464\n",
      "Average test loss: 0.005520215851979123\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015140316929254266\n",
      "Average test loss: 0.002972809167371856\n",
      "Epoch 17/300\n",
      "Average training loss: 0.014842975310981273\n",
      "Average test loss: 0.0027875699864493477\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0146000695352753\n",
      "Average test loss: 0.0028480720535541573\n",
      "Epoch 19/300\n",
      "Average training loss: 0.014415905040171411\n",
      "Average test loss: 0.002693771795266204\n",
      "Epoch 20/300\n",
      "Average training loss: 0.014249013106856081\n",
      "Average test loss: 0.0026823726865566437\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014072519797417853\n",
      "Average test loss: 0.002597694331366155\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013930022660228941\n",
      "Average test loss: 0.0027170260186410613\n",
      "Epoch 23/300\n",
      "Average training loss: 0.013844613306224347\n",
      "Average test loss: 0.0025881703295227553\n",
      "Epoch 24/300\n",
      "Average training loss: 0.013666076497899162\n",
      "Average test loss: 0.002548265172996455\n",
      "Epoch 25/300\n",
      "Average training loss: 0.013555776378346814\n",
      "Average test loss: 0.0025742643140256403\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01345747543954187\n",
      "Average test loss: 0.0025065102991130616\n",
      "Epoch 27/300\n",
      "Average training loss: 0.013388675674796104\n",
      "Average test loss: 0.00247753606364131\n",
      "Epoch 28/300\n",
      "Average training loss: 0.013305747908022669\n",
      "Average test loss: 0.002485601900973254\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013211449370616012\n",
      "Average test loss: 0.0025564435501065517\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013115780820449193\n",
      "Average test loss: 0.0025724247963064247\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013083899089859592\n",
      "Average test loss: 0.0024093821971780723\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012993461665179994\n",
      "Average test loss: 0.002495545027156671\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012914090580410428\n",
      "Average test loss: 0.002428791155728201\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012860087509784434\n",
      "Average test loss: 0.0024057029514676993\n",
      "Epoch 35/300\n",
      "Average training loss: 0.012804731681115098\n",
      "Average test loss: 0.0024141732959283724\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012742030768758721\n",
      "Average test loss: 0.0024725616334213153\n",
      "Epoch 37/300\n",
      "Average training loss: 0.012712992631726795\n",
      "Average test loss: 0.0024308776373250618\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01265807469520304\n",
      "Average test loss: 0.0023464749260909026\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012592341138256921\n",
      "Average test loss: 0.0023640713019089567\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012567664160496659\n",
      "Average test loss: 0.002392324743999375\n",
      "Epoch 41/300\n",
      "Average training loss: 0.012538828315006361\n",
      "Average test loss: 0.0023294686884101894\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012501999292108748\n",
      "Average test loss: 0.002488397033057279\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012454504540397062\n",
      "Average test loss: 0.002329048230002324\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01242633382562134\n",
      "Average test loss: 0.0023578156704703967\n",
      "Epoch 45/300\n",
      "Average training loss: 0.012397829887974594\n",
      "Average test loss: 0.002441453430387709\n",
      "Epoch 46/300\n",
      "Average training loss: 0.012347590843836466\n",
      "Average test loss: 0.002346258172765374\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012317565671271747\n",
      "Average test loss: 0.0023537572376016113\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01230338522957431\n",
      "Average test loss: 0.0023238217189080184\n",
      "Epoch 49/300\n",
      "Average training loss: 0.012273173468808333\n",
      "Average test loss: 0.0023112272520860034\n",
      "Epoch 50/300\n",
      "Average training loss: 0.012252170538736715\n",
      "Average test loss: 0.0023345387055435114\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012206546413401763\n",
      "Average test loss: 0.0023004242406330176\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012205960702564982\n",
      "Average test loss: 0.0023305007339351706\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012162886944909891\n",
      "Average test loss: 0.0023053375917176403\n",
      "Epoch 54/300\n",
      "Average training loss: 0.012116843769947688\n",
      "Average test loss: 0.002347031812287039\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012131310297383202\n",
      "Average test loss: 0.002304487333115604\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01207958815495173\n",
      "Average test loss: 0.002291493589989841\n",
      "Epoch 57/300\n",
      "Average training loss: 0.012078192060192425\n",
      "Average test loss: 0.0024474767823186184\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01204359215994676\n",
      "Average test loss: 0.0022626257431175973\n",
      "Epoch 59/300\n",
      "Average training loss: 0.012007217790517543\n",
      "Average test loss: 0.0022958912870122325\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011991766682929462\n",
      "Average test loss: 0.002373173106358283\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011981594285617273\n",
      "Average test loss: 0.0023092612319936356\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011955558098852634\n",
      "Average test loss: 0.0023474759053852823\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011945044345325893\n",
      "Average test loss: 0.0022781253002790943\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01192439049979051\n",
      "Average test loss: 0.0023548803565402827\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011902972804175483\n",
      "Average test loss: 0.0022851438615471126\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011890309465428194\n",
      "Average test loss: 0.0023169940896332264\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01188865765515301\n",
      "Average test loss: 0.0023411901514563296\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011846941850251622\n",
      "Average test loss: 0.0022748551263163485\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011858260242475403\n",
      "Average test loss: 0.002479490188674794\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01182349818944931\n",
      "Average test loss: 0.002286460515525606\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01180905499557654\n",
      "Average test loss: 0.0022421130943629476\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01179739577902688\n",
      "Average test loss: 0.0023468939051445987\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01176038110918469\n",
      "Average test loss: 0.002372436434444454\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011779219959345128\n",
      "Average test loss: 0.0022712686732411385\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011747306049697929\n",
      "Average test loss: 0.002280164152176844\n",
      "Epoch 76/300\n",
      "Average training loss: 0.011711140241887835\n",
      "Average test loss: 0.002300583470198843\n",
      "Epoch 77/300\n",
      "Average training loss: 0.011731365686489477\n",
      "Average test loss: 0.002327290691021416\n",
      "Epoch 78/300\n",
      "Average training loss: 0.011695208333432674\n",
      "Average test loss: 0.00232359772009982\n",
      "Epoch 79/300\n",
      "Average training loss: 0.011688631311058998\n",
      "Average test loss: 0.002325504951385988\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011688708083497153\n",
      "Average test loss: 0.00240694595914748\n",
      "Epoch 81/300\n",
      "Average training loss: 0.011656677511003282\n",
      "Average test loss: 0.0028648361545056105\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01166767417308357\n",
      "Average test loss: 0.002269387188160585\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01163402909040451\n",
      "Average test loss: 0.0024181326440432006\n",
      "Epoch 84/300\n",
      "Average training loss: 0.011619905055397086\n",
      "Average test loss: 0.0022948733164618413\n",
      "Epoch 85/300\n",
      "Average training loss: 0.011616132740345265\n",
      "Average test loss: 0.002359035541820857\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011599797319206927\n",
      "Average test loss: 0.002504097398577465\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011591130964457989\n",
      "Average test loss: 0.0022751809884276656\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01157774052520593\n",
      "Average test loss: 0.002278978285069267\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011585290889773104\n",
      "Average test loss: 0.002290289462854465\n",
      "Epoch 90/300\n",
      "Average training loss: 0.011548220093879435\n",
      "Average test loss: 0.0024491901089333827\n",
      "Epoch 91/300\n",
      "Average training loss: 0.011527374842928515\n",
      "Average test loss: 0.0023695089707358014\n",
      "Epoch 92/300\n",
      "Average training loss: 0.011535706794096363\n",
      "Average test loss: 0.002286607223459416\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011517257837371694\n",
      "Average test loss: 0.0023181195485716065\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011515628337860108\n",
      "Average test loss: 0.0022685330115879577\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011507099745174248\n",
      "Average test loss: 0.0023886271117048133\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011472149281038178\n",
      "Average test loss: 0.0023016874939203263\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011468571920361784\n",
      "Average test loss: 0.0022864398064298764\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011476953405472968\n",
      "Average test loss: 0.0023251323757900134\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011460126683115959\n",
      "Average test loss: 0.002362864313957592\n",
      "Epoch 100/300\n",
      "Average training loss: 0.011445901146365537\n",
      "Average test loss: 0.0022213870061354507\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01144915067156156\n",
      "Average test loss: 0.0024282565995429953\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011444994344479507\n",
      "Average test loss: 0.0023325054637259907\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011406132787466048\n",
      "Average test loss: 0.0024343890158666504\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011404604851371711\n",
      "Average test loss: 0.002301015495840046\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011416260971791214\n",
      "Average test loss: 0.002267286296313008\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011418093356821273\n",
      "Average test loss: 0.0023725043375872903\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011384303842981657\n",
      "Average test loss: 0.002317686702642176\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011387880823678441\n",
      "Average test loss: 0.002299039831591977\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01136096554994583\n",
      "Average test loss: 0.00248481480549607\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0113822543538279\n",
      "Average test loss: 0.0022441897245330943\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011327979705399937\n",
      "Average test loss: 0.002250454512528247\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011349456706808673\n",
      "Average test loss: 0.0026545289477540386\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011349101309974989\n",
      "Average test loss: 0.0022680904265079235\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011324775050911638\n",
      "Average test loss: 0.004200187130934663\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011328863469262918\n",
      "Average test loss: 0.0023226704889287553\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011294639811747604\n",
      "Average test loss: 0.002278624366141028\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011298445759548081\n",
      "Average test loss: 0.002344479281351798\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01129812238779333\n",
      "Average test loss: 0.0024564786857614916\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011312651054726706\n",
      "Average test loss: 0.0023502029503385227\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011278311974472469\n",
      "Average test loss: 0.0023690842089967596\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011274872419734796\n",
      "Average test loss: 0.0023732505669403405\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01125798609190517\n",
      "Average test loss: 0.002591303725519942\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011281582306656572\n",
      "Average test loss: 0.0022641479874340197\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01125082421882285\n",
      "Average test loss: 0.002262531499999265\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011233476045231025\n",
      "Average test loss: 0.0023342659869748685\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011240120814906227\n",
      "Average test loss: 0.002279446327644918\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01122786601798402\n",
      "Average test loss: 0.002269134579019414\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011235504648751683\n",
      "Average test loss: 0.0023834977004056177\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011232249067889319\n",
      "Average test loss: 0.002306904191358222\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0112175565643443\n",
      "Average test loss: 0.0032829144895076753\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011191240344610479\n",
      "Average test loss: 0.0022700981092121865\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011197854516406853\n",
      "Average test loss: 0.002232217128905985\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011190921317372057\n",
      "Average test loss: 0.002654265093513661\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011196073326799604\n",
      "Average test loss: 0.0022564620739883847\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011182652302914195\n",
      "Average test loss: 0.0024110487134506304\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011179304988847838\n",
      "Average test loss: 0.002335876478710108\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011173931166529656\n",
      "Average test loss: 0.0022223353758454322\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011150495550698704\n",
      "Average test loss: 0.0023244309733725255\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011150209801064598\n",
      "Average test loss: 0.00234884445514116\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011167229734361172\n",
      "Average test loss: 0.002334256431294812\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011131029056178198\n",
      "Average test loss: 0.002289368158206344\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011134121292995082\n",
      "Average test loss: 0.002319128022011783\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01111817357275221\n",
      "Average test loss: 0.0023096427594621978\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011115366464687719\n",
      "Average test loss: 0.0022495844612518945\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01112082751095295\n",
      "Average test loss: 0.002366345752651493\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011127104873458544\n",
      "Average test loss: 0.002383422860254844\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011116163387066788\n",
      "Average test loss: 0.0022932178276694483\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011099630523059104\n",
      "Average test loss: 0.0023475979790091514\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011112249892618921\n",
      "Average test loss: 0.002430415908495585\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011081568024224706\n",
      "Average test loss: 0.0023151477747079398\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011081301046742334\n",
      "Average test loss: 0.00233836943252633\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01107872476014826\n",
      "Average test loss: 0.002301369080837402\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011080041122933228\n",
      "Average test loss: 0.0031458778791129587\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011070507220923901\n",
      "Average test loss: 0.0023040855057123635\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011064425767295891\n",
      "Average test loss: 0.0022785833792554008\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011054742088748348\n",
      "Average test loss: 0.0022803081466505927\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011047350905007787\n",
      "Average test loss: 0.0022696075145569113\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011037017062306404\n",
      "Average test loss: 0.0023223267685001095\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011056380132006275\n",
      "Average test loss: 0.002299286807162894\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011040668073627684\n",
      "Average test loss: 0.002360017035984331\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011071410400172075\n",
      "Average test loss: 0.0022369866565697723\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011027183680070772\n",
      "Average test loss: 0.0023137689509118596\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011013200528091854\n",
      "Average test loss: 1.1735025636884902\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011134888285564052\n",
      "Average test loss: 0.0023137385458168055\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01102512734134992\n",
      "Average test loss: 0.0023000685923422375\n",
      "Epoch 166/300\n",
      "Average training loss: 0.010992719330721431\n",
      "Average test loss: 0.002251743848539061\n",
      "Epoch 167/300\n",
      "Average training loss: 0.010980033004449474\n",
      "Average test loss: 0.002355088207663761\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011001968595716689\n",
      "Average test loss: 0.0023169172348247633\n",
      "Epoch 169/300\n",
      "Average training loss: 0.010988880222870244\n",
      "Average test loss: 0.0023898968247489795\n",
      "Epoch 170/300\n",
      "Average training loss: 0.010981324742237727\n",
      "Average test loss: 0.0023174199971059957\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01098519759459628\n",
      "Average test loss: 0.002281749476575189\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01097331356174416\n",
      "Average test loss: 0.0022918751713716322\n",
      "Epoch 173/300\n",
      "Average training loss: 0.010981317370302147\n",
      "Average test loss: 0.0022605316763122875\n",
      "Epoch 174/300\n",
      "Average training loss: 0.010981957068873777\n",
      "Average test loss: 0.0023571008689080675\n",
      "Epoch 175/300\n",
      "Average training loss: 0.010968285710447364\n",
      "Average test loss: 0.0023540318664163353\n",
      "Epoch 176/300\n",
      "Average training loss: 0.010953579304946793\n",
      "Average test loss: 0.002264366757952505\n",
      "Epoch 177/300\n",
      "Average training loss: 0.010958255213167933\n",
      "Average test loss: 0.002365476558916271\n",
      "Epoch 178/300\n",
      "Average training loss: 0.010951706763770845\n",
      "Average test loss: 0.002377337662710084\n",
      "Epoch 179/300\n",
      "Average training loss: 0.010953699193894863\n",
      "Average test loss: 0.002381646943796012\n",
      "Epoch 180/300\n",
      "Average training loss: 0.010949253886938095\n",
      "Average test loss: 0.0023446160137860312\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010939726993441581\n",
      "Average test loss: 0.0024771034547852147\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010940221206181579\n",
      "Average test loss: 0.002339439733988709\n",
      "Epoch 183/300\n",
      "Average training loss: 0.010917028860913383\n",
      "Average test loss: 0.0022824736190959812\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010926149053706063\n",
      "Average test loss: 0.0023163429241006574\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010924506839778688\n",
      "Average test loss: 0.0024200572125199766\n",
      "Epoch 186/300\n",
      "Average training loss: 0.010913981953428851\n",
      "Average test loss: 0.0024141666744318274\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010906277920636866\n",
      "Average test loss: 0.0022763751733841167\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010916273825698429\n",
      "Average test loss: 0.0023303266217311225\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010890735757847627\n",
      "Average test loss: 0.002343616219340927\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010899850571321116\n",
      "Average test loss: 0.002306028873970111\n",
      "Epoch 191/300\n",
      "Average training loss: 0.010897457150949372\n",
      "Average test loss: 0.0025078310329053138\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010903291775948471\n",
      "Average test loss: 0.0024110696892150575\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010893773198955589\n",
      "Average test loss: 0.0022914753278924357\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010879117945002185\n",
      "Average test loss: 0.002283521839314037\n",
      "Epoch 195/300\n",
      "Average training loss: 0.010898523488806353\n",
      "Average test loss: 0.0026049941337356964\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01088356193817324\n",
      "Average test loss: 0.0023593932803099354\n",
      "Epoch 197/300\n",
      "Average training loss: 0.010867924060258601\n",
      "Average test loss: 0.0023586271098918385\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010864987939596176\n",
      "Average test loss: 0.002329857317109903\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01086625213176012\n",
      "Average test loss: 0.015349930128289594\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01086702502767245\n",
      "Average test loss: 0.002340645655679206\n",
      "Epoch 201/300\n",
      "Average training loss: 0.010873352789216572\n",
      "Average test loss: 0.0024380064450411332\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0108391648862097\n",
      "Average test loss: 0.0023404102954599592\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010860215000808238\n",
      "Average test loss: 0.0022887051382826434\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010862782268060579\n",
      "Average test loss: 0.002471973821106884\n",
      "Epoch 205/300\n",
      "Average training loss: 0.010854382881687746\n",
      "Average test loss: 0.0023603922369786435\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0108298376666175\n",
      "Average test loss: 0.0023192239579641156\n",
      "Epoch 207/300\n",
      "Average training loss: 0.010847490877740913\n",
      "Average test loss: 0.0026954637192603613\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01083074492464463\n",
      "Average test loss: 0.0023211616824070614\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01082278778983487\n",
      "Average test loss: 0.002316050593223837\n",
      "Epoch 210/300\n",
      "Average training loss: 0.010820752935277091\n",
      "Average test loss: 0.0023673621722393565\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01081004319878088\n",
      "Average test loss: 0.0024196251833604444\n",
      "Epoch 212/300\n",
      "Average training loss: 0.010831102341413499\n",
      "Average test loss: 0.0022676334151377282\n",
      "Epoch 213/300\n",
      "Average training loss: 0.010818891779416136\n",
      "Average test loss: 0.0022982792113390234\n",
      "Epoch 214/300\n",
      "Average training loss: 0.010808851057042677\n",
      "Average test loss: 0.0024200553045504625\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010808237830797832\n",
      "Average test loss: 0.00226444518711004\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010806619329584969\n",
      "Average test loss: 0.0023023041836503478\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010821907565825515\n",
      "Average test loss: 0.0023821163155759374\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010791389700439242\n",
      "Average test loss: 0.0023914508838206528\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01078341258979506\n",
      "Average test loss: 0.0024292695514029927\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010807265596257316\n",
      "Average test loss: 0.002365831691771746\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010790877438253826\n",
      "Average test loss: 0.0024334788726021847\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010794295387963454\n",
      "Average test loss: 0.0024466780088841914\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01078341086457173\n",
      "Average test loss: 0.0023141752510435053\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010780019308957788\n",
      "Average test loss: 0.002405138014919228\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010779107273452812\n",
      "Average test loss: 0.0023318598518768945\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010782617765168348\n",
      "Average test loss: 0.002465842908869187\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010770769044756889\n",
      "Average test loss: 0.0022718098994551432\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01077180086572965\n",
      "Average test loss: 0.0023433568380359145\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010780805505398247\n",
      "Average test loss: 0.002460963364276621\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010772169835037655\n",
      "Average test loss: 0.002330982574986087\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010751335576176644\n",
      "Average test loss: 0.002280831346184843\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010757797941565514\n",
      "Average test loss: 0.002291047588404682\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01075980645045638\n",
      "Average test loss: 0.0023844803047055997\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01075118018190066\n",
      "Average test loss: 0.0023111069343156286\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010750713758998448\n",
      "Average test loss: 0.0023330381638887857\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010734848528272575\n",
      "Average test loss: 0.0023596050137033066\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010732413415279654\n",
      "Average test loss: 0.0023627713218124374\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010734270923667483\n",
      "Average test loss: 0.002470716405349473\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010742101454072528\n",
      "Average test loss: 0.0024603097945865657\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010720809925347567\n",
      "Average test loss: 0.0025492165717813705\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010735259674489498\n",
      "Average test loss: 0.0022836933970037435\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010729030329320165\n",
      "Average test loss: 0.002408458461984992\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010739606713255247\n",
      "Average test loss: 0.0023332282296485373\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010720387530823548\n",
      "Average test loss: 0.002350840736284024\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0107187120831675\n",
      "Average test loss: 0.002357508711516857\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010717218265765244\n",
      "Average test loss: 0.002326415102721916\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0107070996388793\n",
      "Average test loss: 0.0024949053975029125\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010710754381285773\n",
      "Average test loss: 0.0024458790577741134\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010698317354751958\n",
      "Average test loss: 0.0023746484651540715\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010718461910883586\n",
      "Average test loss: 0.0024162615354483327\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010705482419994142\n",
      "Average test loss: 0.0023618992898199295\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010691117182374001\n",
      "Average test loss: 0.002338882060514556\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010692514778839217\n",
      "Average test loss: 0.0023619789708819654\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010706693027582433\n",
      "Average test loss: 0.0023142637122008534\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010696111691494783\n",
      "Average test loss: 0.002299730219050414\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010681131302482553\n",
      "Average test loss: 0.0023448831751528712\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01068842204991314\n",
      "Average test loss: 0.002343746329761214\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010692569902373685\n",
      "Average test loss: 0.0025856076802851426\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010688387660516633\n",
      "Average test loss: 0.0023465558937233355\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010684364652468099\n",
      "Average test loss: 0.002304994276414315\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010680720475812753\n",
      "Average test loss: 0.0023288227038251027\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010669047308464845\n",
      "Average test loss: 0.002372083811606798\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010678213758601082\n",
      "Average test loss: 0.002336736627543966\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01066152719573842\n",
      "Average test loss: 0.0023729612743481994\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010672685258918338\n",
      "Average test loss: 0.0023579640108057193\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010659190965195497\n",
      "Average test loss: 0.002604284568586283\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010659334392183356\n",
      "Average test loss: 0.002415432151200043\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010656019795272085\n",
      "Average test loss: 0.0024022737371010914\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010664705167214076\n",
      "Average test loss: 0.002332595006459289\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010649209034939607\n",
      "Average test loss: 0.002385610863669879\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010663365079296959\n",
      "Average test loss: 0.002403625640604231\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01063652876433399\n",
      "Average test loss: 0.002437603012037774\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010644641479684247\n",
      "Average test loss: 0.002453880609944463\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010642338728739155\n",
      "Average test loss: 0.0024181126985285016\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01064314014547401\n",
      "Average test loss: 0.0023717958925084937\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010627675986952252\n",
      "Average test loss: 0.0024522957464473117\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01062695121847921\n",
      "Average test loss: 0.002386851464294725\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010656134727100532\n",
      "Average test loss: 0.002426734597111742\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010636464128891627\n",
      "Average test loss: 0.0024008914838648506\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010618304785754946\n",
      "Average test loss: 0.002338220508976115\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010632996308306853\n",
      "Average test loss: 0.0023343482924004398\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010641189712617133\n",
      "Average test loss: 0.0024489696419073476\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010613716965748204\n",
      "Average test loss: 0.0024245124144686592\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010617873024609354\n",
      "Average test loss: 0.0024094736199412083\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010603948288493686\n",
      "Average test loss: 0.0023939479127940205\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010613517079916266\n",
      "Average test loss: 0.002404686625012093\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0106249492921763\n",
      "Average test loss: 0.002348090765894287\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01060677574078242\n",
      "Average test loss: 0.002347767065796587\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010634915298057927\n",
      "Average test loss: 0.002359605776766936\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010614628136985832\n",
      "Average test loss: 0.002393435510082377\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010592406001355912\n",
      "Average test loss: 0.0023383261745588647\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010588687405818038\n",
      "Average test loss: 0.0023850140629543197\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010583579152822495\n",
      "Average test loss: 0.0023661372980309857\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01058956869939963\n",
      "Average test loss: 0.0023720675973842543\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010608277845713828\n",
      "Average test loss: 0.0023877002002878323\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01058236574464374\n",
      "Average test loss: 0.0023674474120553997\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010600533758600553\n",
      "Average test loss: 0.0024440497008876667\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010585368207759327\n",
      "Average test loss: 0.0023844745970434613\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010578845798141426\n",
      "Average test loss: 0.002522594821742839\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01057477136535777\n",
      "Average test loss: 0.002320033383141789\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.08218745798865953\n",
      "Average test loss: 0.0049521830855972235\n",
      "Epoch 2/300\n",
      "Average training loss: 0.025310763898822995\n",
      "Average test loss: 0.0040274928423265615\n",
      "Epoch 3/300\n",
      "Average training loss: 0.021867562082078722\n",
      "Average test loss: 0.004330252237617969\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019983255273765987\n",
      "Average test loss: 0.0034989437458829748\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018748802319169044\n",
      "Average test loss: 0.0030065383437193104\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017372246793574755\n",
      "Average test loss: 0.0030257026178555357\n",
      "Epoch 7/300\n",
      "Average training loss: 0.016451811446911758\n",
      "Average test loss: 0.0029582658755696483\n",
      "Epoch 8/300\n",
      "Average training loss: 0.015487065888113445\n",
      "Average test loss: 0.002831541645237141\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014781401082873345\n",
      "Average test loss: 0.0026509441048320798\n",
      "Epoch 10/300\n",
      "Average training loss: 0.014197894865439998\n",
      "Average test loss: 0.0025512535298864046\n",
      "Epoch 11/300\n",
      "Average training loss: 0.013754161637690332\n",
      "Average test loss: 0.002396706801942653\n",
      "Epoch 12/300\n",
      "Average training loss: 0.013359621374971337\n",
      "Average test loss: 0.002468325790017843\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013024951441420449\n",
      "Average test loss: 0.0022298777941614388\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012761160081459416\n",
      "Average test loss: 0.002432485504903727\n",
      "Epoch 15/300\n",
      "Average training loss: 0.012461481280624867\n",
      "Average test loss: 0.0023349801815218396\n",
      "Epoch 16/300\n",
      "Average training loss: 0.012238610973788632\n",
      "Average test loss: 0.002091986690854861\n",
      "Epoch 17/300\n",
      "Average training loss: 0.012051275482608212\n",
      "Average test loss: 0.00207152915890846\n",
      "Epoch 18/300\n",
      "Average training loss: 0.011823202670448356\n",
      "Average test loss: 0.0019839544689489734\n",
      "Epoch 19/300\n",
      "Average training loss: 0.011696841076016425\n",
      "Average test loss: 0.0020217863594492276\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01154716091354688\n",
      "Average test loss: 0.0019073495306074619\n",
      "Epoch 21/300\n",
      "Average training loss: 0.011396320648491382\n",
      "Average test loss: 0.0019587456828190223\n",
      "Epoch 22/300\n",
      "Average training loss: 0.011299447408980793\n",
      "Average test loss: 0.0019197035175230767\n",
      "Epoch 23/300\n",
      "Average training loss: 0.011169101745718056\n",
      "Average test loss: 0.001873890747833583\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01108385559419791\n",
      "Average test loss: 0.0019984875606993834\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010991680333183871\n",
      "Average test loss: 0.0018615737187986574\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010902009026871787\n",
      "Average test loss: 0.001844808647202121\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010803178331918187\n",
      "Average test loss: 0.001796997481957078\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010760297128723728\n",
      "Average test loss: 0.0018121589846495124\n",
      "Epoch 29/300\n",
      "Average training loss: 0.010668099196420776\n",
      "Average test loss: 0.0017627220421822536\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010621478996343083\n",
      "Average test loss: 0.0017807165340830882\n",
      "Epoch 31/300\n",
      "Average training loss: 0.010559223745432165\n",
      "Average test loss: 0.0017163727448011438\n",
      "Epoch 32/300\n",
      "Average training loss: 0.010488219178385204\n",
      "Average test loss: 0.0017481725573953654\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010453156849576367\n",
      "Average test loss: 0.0017448944434937503\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010440222611029943\n",
      "Average test loss: 0.0017434114266393914\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01037605864637428\n",
      "Average test loss: 0.0017293456234037875\n",
      "Epoch 36/300\n",
      "Average training loss: 0.010315122074551052\n",
      "Average test loss: 0.0016985967446946435\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010282640207972791\n",
      "Average test loss: 0.0017024158061378532\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010231880618466272\n",
      "Average test loss: 0.001755583485484951\n",
      "Epoch 39/300\n",
      "Average training loss: 0.010224397309952312\n",
      "Average test loss: 0.001746667705476284\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01019842408431901\n",
      "Average test loss: 0.001698342618222038\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01014576534844107\n",
      "Average test loss: 0.0016650110165485077\n",
      "Epoch 42/300\n",
      "Average training loss: 0.010110557427008946\n",
      "Average test loss: 0.0016783447281147044\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01010897069507175\n",
      "Average test loss: 0.001715492767488791\n",
      "Epoch 44/300\n",
      "Average training loss: 0.010058139069212808\n",
      "Average test loss: 0.0016581430360674857\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010057115520868036\n",
      "Average test loss: 0.002005893662571907\n",
      "Epoch 46/300\n",
      "Average training loss: 0.010016177490767506\n",
      "Average test loss: 0.0017341050364904932\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00996998053126865\n",
      "Average test loss: 0.0016540150284870632\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009948696494102478\n",
      "Average test loss: 0.0016524463518419199\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009930169267786873\n",
      "Average test loss: 0.0016847559428877302\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009920367865098848\n",
      "Average test loss: 0.001644800193193886\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009906010725431971\n",
      "Average test loss: 0.001662224689912465\n",
      "Epoch 52/300\n",
      "Average training loss: 0.009855497821337647\n",
      "Average test loss: 0.0016706213139825398\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009865007204314073\n",
      "Average test loss: 0.001636348602258497\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00982516336109903\n",
      "Average test loss: 0.0017079013946155706\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00982029850822356\n",
      "Average test loss: 0.0017417321408995325\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009788663780523671\n",
      "Average test loss: 0.001655220395161046\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009778674317730798\n",
      "Average test loss: 0.0016349374705718622\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00973930175933573\n",
      "Average test loss: 0.00164264882562889\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009754239875409338\n",
      "Average test loss: 0.001659983580828541\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009720864574321442\n",
      "Average test loss: 0.001718287294730544\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009709437588850657\n",
      "Average test loss: 0.0016623053759750393\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009704390871855947\n",
      "Average test loss: 0.0017109553217887878\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009669991518060366\n",
      "Average test loss: 0.0016847316138446332\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009662780200441678\n",
      "Average test loss: 0.001760440183389518\n",
      "Epoch 65/300\n",
      "Average training loss: 0.00963811584148142\n",
      "Average test loss: 0.0016273739023341073\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009646075298388799\n",
      "Average test loss: 0.0016106512220576406\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009617820993893677\n",
      "Average test loss: 0.002014764319691393\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009602782538367643\n",
      "Average test loss: 0.0016927496180352238\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009594637821945879\n",
      "Average test loss: 0.0016358395487897925\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009570283821059598\n",
      "Average test loss: 0.001675726769078109\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009563712309632037\n",
      "Average test loss: 0.0016463684387918976\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009547126466201412\n",
      "Average test loss: 0.0016380560125948654\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009551921910709804\n",
      "Average test loss: 0.0016294772263823283\n",
      "Epoch 74/300\n",
      "Average training loss: 0.009551498451994525\n",
      "Average test loss: 0.001708335121990078\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009510247088968754\n",
      "Average test loss: 0.001669936356238193\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009514110151264403\n",
      "Average test loss: 0.0017044178810384537\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009502408644391431\n",
      "Average test loss: 0.0016333574010059238\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009483201365504\n",
      "Average test loss: 0.0017110732315729063\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00946836191498571\n",
      "Average test loss: 0.0016361383597056072\n",
      "Epoch 80/300\n",
      "Average training loss: 0.009453995785779424\n",
      "Average test loss: 0.0017355910632759332\n",
      "Epoch 81/300\n",
      "Average training loss: 0.009447410029669603\n",
      "Average test loss: 0.001624838866500391\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00944342543101973\n",
      "Average test loss: 0.0016006766779141294\n",
      "Epoch 83/300\n",
      "Average training loss: 0.009420363071891997\n",
      "Average test loss: 0.0016326681735614936\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009435757015314367\n",
      "Average test loss: 0.0016770407812048992\n",
      "Epoch 85/300\n",
      "Average training loss: 0.009428780438171493\n",
      "Average test loss: 0.0016241404440047013\n",
      "Epoch 86/300\n",
      "Average training loss: 0.009386616018911203\n",
      "Average test loss: 0.0016169384233653545\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009388191415617864\n",
      "Average test loss: 0.0018152880428565874\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00938027025014162\n",
      "Average test loss: 0.0016121691999336084\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009373501204782062\n",
      "Average test loss: 0.0016702637076377869\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009363495159480307\n",
      "Average test loss: 0.004114109979735481\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009378090178387033\n",
      "Average test loss: 0.0016474220446414418\n",
      "Epoch 92/300\n",
      "Average training loss: 0.00933877667867475\n",
      "Average test loss: 0.0016545823326127396\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00933881744576825\n",
      "Average test loss: 0.0016630909745064047\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009318082485761908\n",
      "Average test loss: 0.0016294612584428654\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0093287677048809\n",
      "Average test loss: 0.0016785736144002941\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009311800996462505\n",
      "Average test loss: 0.001716694863099191\n",
      "Epoch 97/300\n",
      "Average training loss: 0.009313070875075128\n",
      "Average test loss: 0.001619711394318276\n",
      "Epoch 98/300\n",
      "Average training loss: 0.009304105329430766\n",
      "Average test loss: 0.001649842901672754\n",
      "Epoch 99/300\n",
      "Average training loss: 0.009291835378855466\n",
      "Average test loss: 0.00180197139756961\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00927026596913735\n",
      "Average test loss: 0.0016309898362184565\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009269597760505146\n",
      "Average test loss: 0.0015995232474265827\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009280561851958434\n",
      "Average test loss: 0.0016247986637883717\n",
      "Epoch 103/300\n",
      "Average training loss: 0.009254754763924414\n",
      "Average test loss: 0.0016020988751616742\n",
      "Epoch 104/300\n",
      "Average training loss: 0.009250231042918233\n",
      "Average test loss: 0.0017235450041997763\n",
      "Epoch 105/300\n",
      "Average training loss: 0.009253023635182116\n",
      "Average test loss: 0.0015943665732112196\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009237566743459966\n",
      "Average test loss: 0.0016406818697642948\n",
      "Epoch 107/300\n",
      "Average training loss: 0.00922737708191077\n",
      "Average test loss: 0.0015974171087145806\n",
      "Epoch 108/300\n",
      "Average training loss: 0.009224821334083875\n",
      "Average test loss: 0.0016343712655620443\n",
      "Epoch 109/300\n",
      "Average training loss: 0.009208171351088418\n",
      "Average test loss: 0.001589891288222538\n",
      "Epoch 110/300\n",
      "Average training loss: 0.009196699822114573\n",
      "Average test loss: 0.002155015566179322\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00921240768291884\n",
      "Average test loss: 0.0017968707436488735\n",
      "Epoch 112/300\n",
      "Average training loss: 0.009204003557562828\n",
      "Average test loss: 0.0017013743924390939\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009196413202418222\n",
      "Average test loss: 0.0016175618985046943\n",
      "Epoch 114/300\n",
      "Average training loss: 0.009180456279466549\n",
      "Average test loss: 0.001646632230012781\n",
      "Epoch 115/300\n",
      "Average training loss: 0.009172635364863608\n",
      "Average test loss: 0.001629239887206091\n",
      "Epoch 116/300\n",
      "Average training loss: 0.009176812945968576\n",
      "Average test loss: 0.0018049029409885407\n",
      "Epoch 117/300\n",
      "Average training loss: 0.009158806946128606\n",
      "Average test loss: 0.0016974250886382328\n",
      "Epoch 118/300\n",
      "Average training loss: 0.009164244413375855\n",
      "Average test loss: 0.0016249305640036861\n",
      "Epoch 119/300\n",
      "Average training loss: 0.009144413520892462\n",
      "Average test loss: 0.001627084536684884\n",
      "Epoch 120/300\n",
      "Average training loss: 0.009147493498192893\n",
      "Average test loss: 0.0015932418807513185\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009145516701042652\n",
      "Average test loss: 0.001673069525199632\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009137533343500561\n",
      "Average test loss: 0.0017488931667887502\n",
      "Epoch 123/300\n",
      "Average training loss: 0.00913177105370495\n",
      "Average test loss: 0.0016507683767833644\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009123670433130528\n",
      "Average test loss: 0.0015852159721155962\n",
      "Epoch 125/300\n",
      "Average training loss: 0.009118427304757966\n",
      "Average test loss: 0.0016193695853774746\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009107684994737308\n",
      "Average test loss: 0.0016133775181240506\n",
      "Epoch 127/300\n",
      "Average training loss: 0.009101618874404166\n",
      "Average test loss: 0.0016383011812965075\n",
      "Epoch 128/300\n",
      "Average training loss: 0.009091090433299542\n",
      "Average test loss: 0.0016937676686276165\n",
      "Epoch 129/300\n",
      "Average training loss: 0.009101740048163467\n",
      "Average test loss: 0.0017602996442260014\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009102898708648152\n",
      "Average test loss: 0.0016297895372327832\n",
      "Epoch 131/300\n",
      "Average training loss: 0.009063417811774546\n",
      "Average test loss: 0.001626834728030695\n",
      "Epoch 132/300\n",
      "Average training loss: 0.00907546597181095\n",
      "Average test loss: 0.0016719390988970797\n",
      "Epoch 133/300\n",
      "Average training loss: 0.009067274667322635\n",
      "Average test loss: 0.0015922987043029732\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009064579769141145\n",
      "Average test loss: 0.0016774386437609793\n",
      "Epoch 135/300\n",
      "Average training loss: 0.009066232358002\n",
      "Average test loss: 0.0018330792864370677\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009039991824577253\n",
      "Average test loss: 0.0016367568517517712\n",
      "Epoch 137/300\n",
      "Average training loss: 0.009055798672553565\n",
      "Average test loss: 0.0016424196352147393\n",
      "Epoch 138/300\n",
      "Average training loss: 0.009045721867018276\n",
      "Average test loss: 0.0016365699271361034\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00904531364970737\n",
      "Average test loss: 0.0018390193573302692\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009046853594895866\n",
      "Average test loss: 0.0016936738765281109\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009037420621348751\n",
      "Average test loss: 0.001668824394337005\n",
      "Epoch 142/300\n",
      "Average training loss: 0.009039198718137211\n",
      "Average test loss: 0.0016244266596105363\n",
      "Epoch 143/300\n",
      "Average training loss: 0.009010467372834683\n",
      "Average test loss: 0.0016323229256603453\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009019015704592069\n",
      "Average test loss: 0.0016288478924996322\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009008949287235737\n",
      "Average test loss: 0.0016113032282640536\n",
      "Epoch 146/300\n",
      "Average training loss: 0.00901097577313582\n",
      "Average test loss: 0.0016727684872845808\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00899751750462585\n",
      "Average test loss: 0.0016219052276056675\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009001835656662783\n",
      "Average test loss: 0.0016302704902158842\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008986093738012844\n",
      "Average test loss: 0.0016053444803174998\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008988809175789356\n",
      "Average test loss: 0.0016794103367461098\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008976554401218891\n",
      "Average test loss: 0.0016460246331989766\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008985285650524828\n",
      "Average test loss: 0.0016144262944451637\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008967064120703274\n",
      "Average test loss: 0.0016208311370056537\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008965724040236739\n",
      "Average test loss: 0.0016146397469565272\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008971691600150532\n",
      "Average test loss: 0.0017986203034718832\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008962240507205328\n",
      "Average test loss: 0.0016521372383253442\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00895425812734498\n",
      "Average test loss: 0.0016633056346327065\n",
      "Epoch 158/300\n",
      "Average training loss: 0.008947688887516658\n",
      "Average test loss: 0.0017241580747067928\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0089518083292577\n",
      "Average test loss: 0.0016711615205018057\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008925389648311669\n",
      "Average test loss: 0.0016591316281507412\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008952278977466954\n",
      "Average test loss: 0.0016174678187817334\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008925301685101457\n",
      "Average test loss: 0.0016110370977678232\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008928709778520796\n",
      "Average test loss: 0.001671916574343211\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008933365986578994\n",
      "Average test loss: 0.0016644599544298317\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00891061318375998\n",
      "Average test loss: 0.0016764792416037785\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008921798879487648\n",
      "Average test loss: 0.0017289438352195753\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008922557992239793\n",
      "Average test loss: 0.0016253235529487332\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008928981493330664\n",
      "Average test loss: 0.0016686132949673468\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008920125952197445\n",
      "Average test loss: 0.0016447362286142178\n",
      "Epoch 170/300\n",
      "Average training loss: 0.008895474571734667\n",
      "Average test loss: 0.0017093272967677977\n",
      "Epoch 171/300\n",
      "Average training loss: 0.00889933751606279\n",
      "Average test loss: 0.0017088953472880854\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008903253030859763\n",
      "Average test loss: 0.0016357885481168826\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00889396101070775\n",
      "Average test loss: 0.0016639765383054813\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00888573362140192\n",
      "Average test loss: 0.0016558449177278413\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008877548296418455\n",
      "Average test loss: 0.0017220632270392444\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008892647246105804\n",
      "Average test loss: 0.001630847628849248\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008882680291930834\n",
      "Average test loss: 0.001676207502476043\n",
      "Epoch 178/300\n",
      "Average training loss: 0.008872028950187894\n",
      "Average test loss: 0.0017827442092821003\n",
      "Epoch 179/300\n",
      "Average training loss: 0.008878581768108738\n",
      "Average test loss: 0.0016712520490917895\n",
      "Epoch 180/300\n",
      "Average training loss: 0.008870060451328755\n",
      "Average test loss: 0.0016120528524948491\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008862339684532749\n",
      "Average test loss: 0.0016714155579813652\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00885811655720075\n",
      "Average test loss: 0.0016602612773163451\n",
      "Epoch 183/300\n",
      "Average training loss: 0.00884905737472905\n",
      "Average test loss: 0.0016382400132715703\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008858516823914316\n",
      "Average test loss: 0.0017122893719416525\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00883967016885678\n",
      "Average test loss: 0.0016907191649079322\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008850642561084695\n",
      "Average test loss: 0.0016575267486688163\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008846540109978781\n",
      "Average test loss: 0.0016336469409159488\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008832393179337183\n",
      "Average test loss: 0.0018589729706032408\n",
      "Epoch 189/300\n",
      "Average training loss: 0.008841839213338164\n",
      "Average test loss: 0.0016419870557470454\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008821058301462067\n",
      "Average test loss: 0.0017211130634985036\n",
      "Epoch 191/300\n",
      "Average training loss: 0.00883522232166595\n",
      "Average test loss: 0.0016717660944494937\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008825298843698369\n",
      "Average test loss: 0.001830460901061694\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008826021826929516\n",
      "Average test loss: 0.0016135818424324194\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008818831423090564\n",
      "Average test loss: 0.0016794166573219829\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008816392639444934\n",
      "Average test loss: 0.0016342356820694274\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00881036281088988\n",
      "Average test loss: 0.0016869151374428637\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008807553697791364\n",
      "Average test loss: 0.0016552978106256987\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008805122785270214\n",
      "Average test loss: 0.0016968265685977208\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008794703376789888\n",
      "Average test loss: 0.0018863551185155907\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008803448056181272\n",
      "Average test loss: 0.00175866164934511\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008800578648845354\n",
      "Average test loss: 0.0016792122176848352\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008789110109210014\n",
      "Average test loss: 0.0016341251836468776\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008795978022118409\n",
      "Average test loss: 0.0017991883035542236\n",
      "Epoch 204/300\n",
      "Average training loss: 0.00879658720890681\n",
      "Average test loss: 0.0016574261975992057\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008782261200663116\n",
      "Average test loss: 0.0016447780286479327\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008787122494230668\n",
      "Average test loss: 0.0016866289231305322\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00880169804725382\n",
      "Average test loss: 0.0017151550145612822\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008765380939675702\n",
      "Average test loss: 0.0016723938145571284\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008781209669179387\n",
      "Average test loss: 0.0016590516672780116\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00876697332246436\n",
      "Average test loss: 0.001663243645388219\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008760750569403172\n",
      "Average test loss: 0.00172435917177548\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008764275882807043\n",
      "Average test loss: 0.0016594286694501838\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008786797058251168\n",
      "Average test loss: 0.0016640465727800295\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008759042857421769\n",
      "Average test loss: 0.0016365138608962298\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008766539018187258\n",
      "Average test loss: 0.0016553790523774095\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008751728518969482\n",
      "Average test loss: 0.0016513520895193022\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008751889392733574\n",
      "Average test loss: 0.0016254727583792474\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008742109941111671\n",
      "Average test loss: 0.001644355387116472\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008738474646376239\n",
      "Average test loss: 0.0017061743632786803\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008749188991056549\n",
      "Average test loss: 0.0016876845491222208\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008746392323739\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-.85/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-.85/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-.85/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
