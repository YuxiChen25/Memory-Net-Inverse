{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_64x64_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05655643938150671\n",
      "Average test loss: 0.00712753005946676\n",
      "Epoch 2/300\n",
      "Average training loss: 0.023064448446035384\n",
      "Average test loss: 0.0043023791377329165\n",
      "Epoch 3/300\n",
      "Average training loss: 0.021623671904206276\n",
      "Average test loss: 0.00416704971405367\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02111254774199592\n",
      "Average test loss: 0.004105091603265869\n",
      "Epoch 5/300\n",
      "Average training loss: 0.020835365363293223\n",
      "Average test loss: 0.004063301986911231\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02062974299987157\n",
      "Average test loss: 0.00403718104130692\n",
      "Epoch 7/300\n",
      "Average training loss: 0.020465954151418473\n",
      "Average test loss: 0.004006370697998338\n",
      "Epoch 8/300\n",
      "Average training loss: 0.020330383674965966\n",
      "Average test loss: 0.003981315208805932\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02021778308517403\n",
      "Average test loss: 0.003985573992754022\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02012985150350465\n",
      "Average test loss: 0.003955222529255681\n",
      "Epoch 11/300\n",
      "Average training loss: 0.020029004843698607\n",
      "Average test loss: 0.003929491459909413\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019950218722224234\n",
      "Average test loss: 0.003928787140796582\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01987715259525511\n",
      "Average test loss: 0.0038992722959568103\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019798590216371748\n",
      "Average test loss: 0.003888786897684137\n",
      "Epoch 15/300\n",
      "Average training loss: 0.019735672844780817\n",
      "Average test loss: 0.0038696727028323546\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01965957001513905\n",
      "Average test loss: 0.003858870264556673\n",
      "Epoch 17/300\n",
      "Average training loss: 0.019613084779845344\n",
      "Average test loss: 0.00385219833213422\n",
      "Epoch 18/300\n",
      "Average training loss: 0.019569328918225236\n",
      "Average test loss: 0.003854386524607738\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01950806495381726\n",
      "Average test loss: 0.0038351738734377755\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01946159394996034\n",
      "Average test loss: 0.0038252243790775535\n",
      "Epoch 21/300\n",
      "Average training loss: 0.019412789235512415\n",
      "Average test loss: 0.0038401294781102075\n",
      "Epoch 22/300\n",
      "Average training loss: 0.019380823214848835\n",
      "Average test loss: 0.0038162730197525687\n",
      "Epoch 23/300\n",
      "Average training loss: 0.019324830462535223\n",
      "Average test loss: 0.0037908088341355325\n",
      "Epoch 24/300\n",
      "Average training loss: 0.019287940783633127\n",
      "Average test loss: 0.0037948701087799335\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01926568722890483\n",
      "Average test loss: 0.003786490894647108\n",
      "Epoch 26/300\n",
      "Average training loss: 0.019223570047153367\n",
      "Average test loss: 0.0037841625480602185\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019184879988431932\n",
      "Average test loss: 0.003776288999244571\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01914795507490635\n",
      "Average test loss: 0.0037699656014641126\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019120785358879303\n",
      "Average test loss: 0.0037771070564372672\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01909979651371638\n",
      "Average test loss: 0.0037809634161078266\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0190764475663503\n",
      "Average test loss: 0.0037643915912550356\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01903911793563101\n",
      "Average test loss: 0.003756733331001467\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019025645739502377\n",
      "Average test loss: 0.0037394589421649773\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018994313399824832\n",
      "Average test loss: 0.003757878815755248\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018984437801771694\n",
      "Average test loss: 0.0037411597267621092\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01895801124638981\n",
      "Average test loss: 0.0037456515977780025\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01892996359533734\n",
      "Average test loss: 0.003734004254349404\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018916464345322716\n",
      "Average test loss: 0.00372920146294766\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018899818451868163\n",
      "Average test loss: 0.003760740540921688\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018882024498449432\n",
      "Average test loss: 0.0037368733535210292\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018862234056823784\n",
      "Average test loss: 0.0037317884870701366\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018840525638726024\n",
      "Average test loss: 0.003722819881306754\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018832921078635587\n",
      "Average test loss: 0.0037163349390029908\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018815215879844294\n",
      "Average test loss: 0.003735626314249304\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018798918860654035\n",
      "Average test loss: 0.003714577159533898\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01878420510391394\n",
      "Average test loss: 0.003717579183065229\n",
      "Epoch 47/300\n",
      "Average training loss: 0.018767494135432772\n",
      "Average test loss: 0.0037258054723756182\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018754597059554522\n",
      "Average test loss: 0.0037286672306557495\n",
      "Epoch 49/300\n",
      "Average training loss: 0.018739839752515156\n",
      "Average test loss: 0.0037156319353315566\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018725918965207207\n",
      "Average test loss: 0.0037320941591428387\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018709869705968434\n",
      "Average test loss: 0.0037247812596874104\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01870230048398177\n",
      "Average test loss: 0.003730466270819306\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018677618875271744\n",
      "Average test loss: 0.003723145136402713\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01867604025039408\n",
      "Average test loss: 0.003737494179358085\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018654789225922692\n",
      "Average test loss: 0.0037013164481355084\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01864479484160741\n",
      "Average test loss: 0.0037084160428494215\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018626494866278435\n",
      "Average test loss: 0.003703329511400726\n",
      "Epoch 58/300\n",
      "Average training loss: 0.018617085452708935\n",
      "Average test loss: 0.0037148890096901192\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018611527499225403\n",
      "Average test loss: 0.0036942689340147706\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018591208327975537\n",
      "Average test loss: 0.003720878982709514\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018586499389674927\n",
      "Average test loss: 0.0037088725150873263\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018571124406324493\n",
      "Average test loss: 0.003726541846990585\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018559638015925885\n",
      "Average test loss: 0.0037141086597823436\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018550914896859064\n",
      "Average test loss: 0.003695418478714095\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018539528573552767\n",
      "Average test loss: 0.0037238322554363143\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018531281371911366\n",
      "Average test loss: 0.00370965538918972\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018510063900715776\n",
      "Average test loss: 0.0037091922151545683\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01849425274133682\n",
      "Average test loss: 0.0037029258623305296\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018494550195005205\n",
      "Average test loss: 0.0036923084650188685\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0184794768575165\n",
      "Average test loss: 0.003718905792882045\n",
      "Epoch 71/300\n",
      "Average training loss: 0.018470817589097552\n",
      "Average test loss: 0.0037086753162244956\n",
      "Epoch 72/300\n",
      "Average training loss: 0.018450415015220642\n",
      "Average test loss: 0.003717731706798077\n",
      "Epoch 73/300\n",
      "Average training loss: 0.018453346759080887\n",
      "Average test loss: 0.0037095050935943923\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01843642762137784\n",
      "Average test loss: 0.003706289823063546\n",
      "Epoch 75/300\n",
      "Average training loss: 0.018413008098800976\n",
      "Average test loss: 0.0036961440245310466\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018412462153368527\n",
      "Average test loss: 0.0036990933797011773\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018403529815375803\n",
      "Average test loss: 0.0037035336386826304\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018396919440892008\n",
      "Average test loss: 0.0036981261569178766\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018384313848283557\n",
      "Average test loss: 0.0037444884425236117\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018359770552979576\n",
      "Average test loss: 0.0037361818018058937\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018362213042047288\n",
      "Average test loss: 0.003709414610846175\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018353428350554573\n",
      "Average test loss: 0.0037418815737797154\n",
      "Epoch 83/300\n",
      "Average training loss: 0.018341205585334037\n",
      "Average test loss: 0.003704494441342023\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018322675979799693\n",
      "Average test loss: 0.003704641026755174\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018319562246402105\n",
      "Average test loss: 0.0036961394949919647\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018305124431848525\n",
      "Average test loss: 0.0037488329774803585\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018294429793953897\n",
      "Average test loss: 0.003706786755679382\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018292525966962177\n",
      "Average test loss: 0.0037197904750290844\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01826889147526688\n",
      "Average test loss: 0.0037274113872812855\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018261731204059387\n",
      "Average test loss: 0.003721163044580155\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018255143682161967\n",
      "Average test loss: 0.003734850465837452\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018247677276531857\n",
      "Average test loss: 0.0037096874345507887\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018222773915363682\n",
      "Average test loss: 0.0037654667504959634\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018229408307207956\n",
      "Average test loss: 0.003706851186437739\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01821090751224094\n",
      "Average test loss: 0.003748143959997429\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01819904162320826\n",
      "Average test loss: 0.0037444247427499957\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018189043405983183\n",
      "Average test loss: 0.0037051010378119018\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01817512243654993\n",
      "Average test loss: 0.003707389334630635\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018166360550456576\n",
      "Average test loss: 0.003733475195036994\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018162534784939555\n",
      "Average test loss: 0.0037431058374543984\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018153730269935397\n",
      "Average test loss: 0.0037067488150464164\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018145062900251814\n",
      "Average test loss: 0.0037225837868948776\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01812136874596278\n",
      "Average test loss: 0.003729956502508786\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018120910841557716\n",
      "Average test loss: 0.003812931137366427\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01809921352068583\n",
      "Average test loss: 0.0037082949282808437\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018094665169715883\n",
      "Average test loss: 0.003791505462179581\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018078700525893106\n",
      "Average test loss: 0.003732625045296219\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018080099683668877\n",
      "Average test loss: 0.0037502103719032473\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018065378472208977\n",
      "Average test loss: 0.003820451884013083\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018055706005957392\n",
      "Average test loss: 0.0037302504115634496\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018053356369336446\n",
      "Average test loss: 0.0037391121776567567\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01804392783343792\n",
      "Average test loss: 0.0037415066591153542\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018041204257143867\n",
      "Average test loss: 0.0037508215504801936\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01801697292923927\n",
      "Average test loss: 0.0037740718612654343\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018005725678470398\n",
      "Average test loss: 0.003730849117040634\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018012450945046214\n",
      "Average test loss: 0.0037504787132557897\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017988451739152273\n",
      "Average test loss: 0.003755919607356191\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017975841456817257\n",
      "Average test loss: 0.0037670284958763256\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017969172452059055\n",
      "Average test loss: 0.0037782870990534624\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01795874620642927\n",
      "Average test loss: 0.003737501374963257\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017946672978500524\n",
      "Average test loss: 0.0037562165496249993\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017935959522922835\n",
      "Average test loss: 0.003720874519397815\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017940585282113817\n",
      "Average test loss: 0.003734007756329245\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01793540866341856\n",
      "Average test loss: 0.0037592164393928317\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017921715603934393\n",
      "Average test loss: 0.003817458962607715\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017908016287618213\n",
      "Average test loss: 0.0037320054738471907\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017902843766742282\n",
      "Average test loss: 0.0037654158292959134\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017889855886499086\n",
      "Average test loss: 0.0037632438470092083\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01787835025952922\n",
      "Average test loss: 0.003768701699872812\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017874606902400652\n",
      "Average test loss: 0.00373341978341341\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01786280306842592\n",
      "Average test loss: 0.0037535201787120766\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017850890499022273\n",
      "Average test loss: 0.003731665053508348\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017832816327611606\n",
      "Average test loss: 0.0037390113903416526\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01782990928987662\n",
      "Average test loss: 0.003753832217512859\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01783412785662545\n",
      "Average test loss: 0.0037534466394119792\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01781590065277285\n",
      "Average test loss: 0.00379521482417153\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01781172259648641\n",
      "Average test loss: 0.003841967281575004\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017796572494837973\n",
      "Average test loss: 0.003777496464136574\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01779631978107823\n",
      "Average test loss: 0.0037928484115335678\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017784368897477785\n",
      "Average test loss: 0.0037810976246578825\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01778153280665477\n",
      "Average test loss: 0.003876363783246941\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017770843538145224\n",
      "Average test loss: 0.003784355607194205\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01776173870265484\n",
      "Average test loss: 0.003785219344206982\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017755194667312835\n",
      "Average test loss: 0.003800019605913096\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01773604651540518\n",
      "Average test loss: 0.003776667529717088\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0177325871653027\n",
      "Average test loss: 0.003773137558251619\n",
      "Epoch 147/300\n",
      "Average training loss: 0.017723813035421903\n",
      "Average test loss: 0.0037372834833545817\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017714210180772676\n",
      "Average test loss: 0.0038895020257267688\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017706920836534766\n",
      "Average test loss: 0.003760101930755708\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01770193918214904\n",
      "Average test loss: 0.0037627908866852524\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017689279050462775\n",
      "Average test loss: 0.0038250155699335866\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01768773876958423\n",
      "Average test loss: 0.0037938846055832172\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017685874292420016\n",
      "Average test loss: 0.0038590797868867715\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017671185966995028\n",
      "Average test loss: 0.003810045125790768\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01765729032456875\n",
      "Average test loss: 0.0038140886761248113\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01766528976460298\n",
      "Average test loss: 0.0037786473478708003\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0176497290664249\n",
      "Average test loss: 0.0037576945749007995\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01765005459719234\n",
      "Average test loss: 0.0037811591182318\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01762952520781093\n",
      "Average test loss: 0.003790944331429071\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0176222987903489\n",
      "Average test loss: 0.003795952637783355\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017616709641284412\n",
      "Average test loss: 0.003844680224234859\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017611771170463825\n",
      "Average test loss: 0.003760568725152148\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01760447685254945\n",
      "Average test loss: 0.0037815053696847625\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01761129161218802\n",
      "Average test loss: 0.0038027706829210123\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017592549183302456\n",
      "Average test loss: 0.0037659539410637485\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017581246067252424\n",
      "Average test loss: 0.0038236553851101135\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017574550330638886\n",
      "Average test loss: 0.003851738180137343\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01757563044461939\n",
      "Average test loss: 0.0038335447315540577\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01757021853990025\n",
      "Average test loss: 0.0037913764795909325\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0175532249427504\n",
      "Average test loss: 0.0038166729677468537\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017558775645163324\n",
      "Average test loss: 0.003778989898454812\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01754084142876996\n",
      "Average test loss: 0.0038020258096771106\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017536229657630124\n",
      "Average test loss: 0.0038305082774410643\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01753040395759874\n",
      "Average test loss: 0.0038455683932536178\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01752945455908775\n",
      "Average test loss: 0.0038406798880961206\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017509927342335384\n",
      "Average test loss: 0.0039360711475213365\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017505713881717788\n",
      "Average test loss: 0.0038784812883370452\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017502932543555897\n",
      "Average test loss: 0.0038266017984391913\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017491178853644266\n",
      "Average test loss: 0.0038056795485317705\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017497895547085338\n",
      "Average test loss: 0.003877463421680861\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01749511203252607\n",
      "Average test loss: 0.003862622271395392\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01747965156369739\n",
      "Average test loss: 0.003823733798124724\n",
      "Epoch 183/300\n",
      "Average training loss: 0.017467604748076862\n",
      "Average test loss: 0.0038013096971230374\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017466848173075252\n",
      "Average test loss: 0.0038938489759133923\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017474074352946546\n",
      "Average test loss: 0.003812826042994857\n",
      "Epoch 186/300\n",
      "Average training loss: 0.017452936958935527\n",
      "Average test loss: 0.00391332141103016\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01744506719046169\n",
      "Average test loss: 0.0038436231495191653\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017434533011582164\n",
      "Average test loss: 0.0037977598218454254\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01743290930489699\n",
      "Average test loss: 0.003839695172177421\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017430459186434745\n",
      "Average test loss: 0.0038788283071998094\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01741965730405516\n",
      "Average test loss: 0.0038334202729165554\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01741343167424202\n",
      "Average test loss: 0.003918961698603299\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017409930133157305\n",
      "Average test loss: 0.003890405405726698\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01740529298947917\n",
      "Average test loss: 0.0038579342967520157\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01739527768558926\n",
      "Average test loss: 0.0038629122148785325\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017397242842449083\n",
      "Average test loss: 0.0038771882868475385\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01738737836976846\n",
      "Average test loss: 0.0038204586123012836\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017383854589528507\n",
      "Average test loss: 0.003886934645060036\n",
      "Epoch 199/300\n",
      "Average training loss: 0.017381032266550595\n",
      "Average test loss: 0.0038982344983766477\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017385175550977388\n",
      "Average test loss: 0.0038653648948917788\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017356398133767974\n",
      "Average test loss: 0.003847473458904359\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017363216136892637\n",
      "Average test loss: 0.0038635475602414875\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01736908761329121\n",
      "Average test loss: 0.003858851168718603\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01734443479610814\n",
      "Average test loss: 0.0038238156986319357\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01734092506600751\n",
      "Average test loss: 0.003818450221998824\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017342891716294818\n",
      "Average test loss: 0.003854087119922042\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017329539731144906\n",
      "Average test loss: 0.0038243814268045955\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017334233012464312\n",
      "Average test loss: 0.003837754763662815\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017319432798359128\n",
      "Average test loss: 0.003854805199015472\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017319858690102895\n",
      "Average test loss: 0.003908927033758826\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017312890518042776\n",
      "Average test loss: 0.00388338538321356\n",
      "Epoch 212/300\n",
      "Average training loss: 0.017309086961050827\n",
      "Average test loss: 0.003952336885448959\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017298127240190902\n",
      "Average test loss: 0.003943760481145647\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017298127971589566\n",
      "Average test loss: 0.0038327998591379986\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017285832178261545\n",
      "Average test loss: 0.003909553679741091\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017291342549026013\n",
      "Average test loss: 0.00392748601631158\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01727909442782402\n",
      "Average test loss: 0.0038687902744859457\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01727297330233786\n",
      "Average test loss: 0.00382987838714487\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01727636873225371\n",
      "Average test loss: 0.0038361426829877825\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017272485251228015\n",
      "Average test loss: 0.003913999678980973\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017260717300905122\n",
      "Average test loss: 0.003830423157662153\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017257350094616413\n",
      "Average test loss: 0.003934437323568595\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017251767206523154\n",
      "Average test loss: 0.003920857959530419\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017252929531037808\n",
      "Average test loss: 0.00394650835192038\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017247773755755688\n",
      "Average test loss: 0.003896730253059003\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01723925775786241\n",
      "Average test loss: 0.003867956063399712\n",
      "Epoch 227/300\n",
      "Average training loss: 0.017233190174731943\n",
      "Average test loss: 0.0038709194821616015\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017229599310292137\n",
      "Average test loss: 0.003976362863141629\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017229117266833783\n",
      "Average test loss: 0.0038654014066689542\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017218322838346165\n",
      "Average test loss: 0.003856300213684638\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017228208151128556\n",
      "Average test loss: 0.0039090827624830935\n",
      "Epoch 232/300\n",
      "Average training loss: 0.017213432204392223\n",
      "Average test loss: 0.003863194021499819\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017215030080742308\n",
      "Average test loss: 0.00393988742493093\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017204308967623445\n",
      "Average test loss: 0.003904336138938864\n",
      "Epoch 235/300\n",
      "Average training loss: 0.017199968171616394\n",
      "Average test loss: 0.0037983024823996757\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01719465538362662\n",
      "Average test loss: 0.003825223699212074\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017193523726529544\n",
      "Average test loss: 0.003846977748597662\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01718909606503116\n",
      "Average test loss: 0.00397453361708257\n",
      "Epoch 239/300\n",
      "Average training loss: 0.017191090332137213\n",
      "Average test loss: 0.0038903976128333143\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017182918151219684\n",
      "Average test loss: 0.003906504187939895\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0171722333042158\n",
      "Average test loss: 0.003842991760621468\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01716913206047482\n",
      "Average test loss: 0.003832557822060254\n",
      "Epoch 243/300\n",
      "Average training loss: 0.017169588756230144\n",
      "Average test loss: 0.0038934835063086618\n",
      "Epoch 244/300\n",
      "Average training loss: 0.017163783433536688\n",
      "Average test loss: 0.0038487912519938414\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01716048044628567\n",
      "Average test loss: 0.0038528374570111434\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01715060448232624\n",
      "Average test loss: 0.003917711747603284\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017163846193088427\n",
      "Average test loss: 0.003859677552762959\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017140074441830317\n",
      "Average test loss: 0.003921054858300421\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017148549471464422\n",
      "Average test loss: 0.0038931569471541377\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017153141137626435\n",
      "Average test loss: 0.003903268362291985\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01713119114645653\n",
      "Average test loss: 0.003914701838253273\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0171352206700378\n",
      "Average test loss: 0.003969331831153896\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017124955783287685\n",
      "Average test loss: 0.0038887347845981517\n",
      "Epoch 254/300\n",
      "Average training loss: 0.017126843851473596\n",
      "Average test loss: 0.0038061822354793547\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017122030028038554\n",
      "Average test loss: 0.003893464317338334\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017113447303573292\n",
      "Average test loss: 0.0039414844715760814\n",
      "Epoch 257/300\n",
      "Average training loss: 0.017108151371280352\n",
      "Average test loss: 0.003937249319420921\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01711260642608007\n",
      "Average test loss: 0.003953307659054796\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017102643688519797\n",
      "Average test loss: 0.003955604513072305\n",
      "Epoch 260/300\n",
      "Average training loss: 0.017101010208328566\n",
      "Average test loss: 0.0038703508174253833\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017100395518872474\n",
      "Average test loss: 0.0038321478611065283\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017102640059259204\n",
      "Average test loss: 0.003925311710271571\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01708538878460725\n",
      "Average test loss: 0.0039777893846233685\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017093976153267753\n",
      "Average test loss: 0.003919502360125383\n",
      "Epoch 265/300\n",
      "Average training loss: 0.017081904678708976\n",
      "Average test loss: 0.003935225955314106\n",
      "Epoch 266/300\n",
      "Average training loss: 0.017076460440953574\n",
      "Average test loss: 0.003850150094471044\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017083700646956763\n",
      "Average test loss: 0.0038745252608011166\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017078474563029078\n",
      "Average test loss: 0.003935382024695476\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017068062366710768\n",
      "Average test loss: 0.003883248551852173\n",
      "Epoch 270/300\n",
      "Average training loss: 0.017068342313998276\n",
      "Average test loss: 0.003977353487370743\n",
      "Epoch 271/300\n",
      "Average training loss: 0.017071354844503932\n",
      "Average test loss: 0.004025594275444746\n",
      "Epoch 272/300\n",
      "Average training loss: 0.017069935546980965\n",
      "Average test loss: 0.003994955367305213\n",
      "Epoch 273/300\n",
      "Average training loss: 0.017052314543061785\n",
      "Average test loss: 0.003948298041605287\n",
      "Epoch 274/300\n",
      "Average training loss: 0.017049013240469826\n",
      "Average test loss: 0.0038566752653568983\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017056389171216223\n",
      "Average test loss: 0.0039185255505144595\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01702753969695833\n",
      "Average test loss: 0.0038420954189366764\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017051522293024594\n",
      "Average test loss: 0.003879977156304651\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017038998929990664\n",
      "Average test loss: 0.0039020532057103185\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017034041460189553\n",
      "Average test loss: 0.004016320486863454\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01703849536511633\n",
      "Average test loss: 0.003852868144503898\n",
      "Epoch 281/300\n",
      "Average training loss: 0.017033424988389015\n",
      "Average test loss: 0.003925755541150769\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017018821015126175\n",
      "Average test loss: 0.003909463880376683\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017017797857522964\n",
      "Average test loss: 0.003866424695899089\n",
      "Epoch 284/300\n",
      "Average training loss: 0.017030429636438686\n",
      "Average test loss: 0.003916778229590919\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017014608202709093\n",
      "Average test loss: 0.003861560450659858\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017007636449403233\n",
      "Average test loss: 0.004011542335152626\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01700958449062374\n",
      "Average test loss: 0.004059849824756384\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017010068302353222\n",
      "Average test loss: 0.003985629601404071\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017011105878485575\n",
      "Average test loss: 0.0038759629873351916\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017003884535696772\n",
      "Average test loss: 0.003921839507710603\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0169961087687148\n",
      "Average test loss: 0.0038851498779323367\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0169964874751038\n",
      "Average test loss: 0.0039043901295711596\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01699255443364382\n",
      "Average test loss: 0.003905747678337826\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016986086575521364\n",
      "Average test loss: 0.0038979730701280964\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016992819589873157\n",
      "Average test loss: 0.003869225555409988\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01698455531315671\n",
      "Average test loss: 0.003924981681009134\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0169799636999766\n",
      "Average test loss: 0.0038973911909593478\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016967855841749244\n",
      "Average test loss: 0.003835905856349402\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016971058244506516\n",
      "Average test loss: 0.003886983898985717\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016978788586126434\n",
      "Average test loss: 0.003935115834698081\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05266691924466027\n",
      "Average test loss: 0.003875327844586637\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0193380942079756\n",
      "Average test loss: 0.003767308416051997\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01835335837304592\n",
      "Average test loss: 0.0035255086411618525\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01785075385702981\n",
      "Average test loss: 0.0034119927574776943\n",
      "Epoch 5/300\n",
      "Average training loss: 0.017469662631551426\n",
      "Average test loss: 0.0033453626436077888\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017156886191831696\n",
      "Average test loss: 0.0032702205400500033\n",
      "Epoch 7/300\n",
      "Average training loss: 0.016870164153476558\n",
      "Average test loss: 0.003255468015041616\n",
      "Epoch 8/300\n",
      "Average training loss: 0.016603638508253627\n",
      "Average test loss: 0.003178754227856795\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01637881694899665\n",
      "Average test loss: 0.003141139835326208\n",
      "Epoch 10/300\n",
      "Average training loss: 0.016166093417339853\n",
      "Average test loss: 0.0031120505522315703\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01598037665751245\n",
      "Average test loss: 0.003046086359769106\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015819971415731642\n",
      "Average test loss: 0.003038893865628375\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01567173397458262\n",
      "Average test loss: 0.002997197765856981\n",
      "Epoch 14/300\n",
      "Average training loss: 0.015543679237365722\n",
      "Average test loss: 0.0029662582096126344\n",
      "Epoch 15/300\n",
      "Average training loss: 0.015421327428685294\n",
      "Average test loss: 0.002929008278581831\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015319295653038554\n",
      "Average test loss: 0.0029149593093122047\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01520386921779977\n",
      "Average test loss: 0.0028933151699602603\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015119250663452679\n",
      "Average test loss: 0.002899460461611549\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015020785390502877\n",
      "Average test loss: 0.0028897320373604696\n",
      "Epoch 20/300\n",
      "Average training loss: 0.014949456781976752\n",
      "Average test loss: 0.0028327255139334335\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014862166985869408\n",
      "Average test loss: 0.002837434301773707\n",
      "Epoch 22/300\n",
      "Average training loss: 0.014800564016732905\n",
      "Average test loss: 0.0027879543089204366\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014732367783784867\n",
      "Average test loss: 0.0027878539074833193\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014682508764167627\n",
      "Average test loss: 0.0027976156340705025\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014622759974665112\n",
      "Average test loss: 0.0027888091396954325\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014563423438204659\n",
      "Average test loss: 0.0027542029145277208\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01450889878306124\n",
      "Average test loss: 0.0027792691462155844\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01447106344666746\n",
      "Average test loss: 0.002798356998504864\n",
      "Epoch 29/300\n",
      "Average training loss: 0.014428575824532244\n",
      "Average test loss: 0.002740696953402625\n",
      "Epoch 30/300\n",
      "Average training loss: 0.014396279632217355\n",
      "Average test loss: 0.0027416621568716235\n",
      "Epoch 31/300\n",
      "Average training loss: 0.014340222003559272\n",
      "Average test loss: 0.0027297211300788653\n",
      "Epoch 32/300\n",
      "Average training loss: 0.014315705640448465\n",
      "Average test loss: 0.002709234830819898\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01428124383588632\n",
      "Average test loss: 0.002727960385589136\n",
      "Epoch 34/300\n",
      "Average training loss: 0.014244704946875571\n",
      "Average test loss: 0.002738630150548286\n",
      "Epoch 35/300\n",
      "Average training loss: 0.014217091702752644\n",
      "Average test loss: 0.0026989713861710497\n",
      "Epoch 36/300\n",
      "Average training loss: 0.014186084171964063\n",
      "Average test loss: 0.002745725326447023\n",
      "Epoch 37/300\n",
      "Average training loss: 0.014149087319771448\n",
      "Average test loss: 0.002680202013502518\n",
      "Epoch 38/300\n",
      "Average training loss: 0.014120732232100433\n",
      "Average test loss: 0.00268913782077531\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01410451619575421\n",
      "Average test loss: 0.0026808544527739285\n",
      "Epoch 40/300\n",
      "Average training loss: 0.014075321218205823\n",
      "Average test loss: 0.00271691006835964\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014062328526543247\n",
      "Average test loss: 0.002670396863379412\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014031501467857095\n",
      "Average test loss: 0.002670661571746071\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014011240598228242\n",
      "Average test loss: 0.002670622622180316\n",
      "Epoch 44/300\n",
      "Average training loss: 0.013977981312407387\n",
      "Average test loss: 0.0026649744941128627\n",
      "Epoch 45/300\n",
      "Average training loss: 0.013969777097304662\n",
      "Average test loss: 0.0026928577826668817\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013936631896429592\n",
      "Average test loss: 0.002642749378043744\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01392869164629115\n",
      "Average test loss: 0.0026504712445247504\n",
      "Epoch 48/300\n",
      "Average training loss: 0.013909270158244504\n",
      "Average test loss: 0.0026591591431448856\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01388657327575816\n",
      "Average test loss: 0.0026584140035427277\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013873235448367065\n",
      "Average test loss: 0.002663291237213545\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013844477528499233\n",
      "Average test loss: 0.002658040977600548\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013830253668957286\n",
      "Average test loss: 0.0026359136054913204\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013812216545144716\n",
      "Average test loss: 0.0026603533691830104\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01381605340540409\n",
      "Average test loss: 0.0026427388325747517\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013774121411972576\n",
      "Average test loss: 0.002676016177878612\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013772260953154829\n",
      "Average test loss: 0.0027126780808385876\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01375102188024256\n",
      "Average test loss: 0.002672607008781698\n",
      "Epoch 58/300\n",
      "Average training loss: 0.013720948839353191\n",
      "Average test loss: 0.0026428242077430088\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013711523786187172\n",
      "Average test loss: 0.002642584772470097\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013703293329311741\n",
      "Average test loss: 0.00266006751689646\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013697308499779966\n",
      "Average test loss: 0.0026226804405450822\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013677026698158847\n",
      "Average test loss: 0.0026551929304583204\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01364244464205371\n",
      "Average test loss: 0.002643189844364921\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013649524961080815\n",
      "Average test loss: 0.002641916632031401\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013615957996911473\n",
      "Average test loss: 0.0026230732430186534\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013609481485353576\n",
      "Average test loss: 0.0026427538856450055\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013590251991318332\n",
      "Average test loss: 0.0026612662898583546\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01357215047462119\n",
      "Average test loss: 0.0026340013547903962\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013579178676837021\n",
      "Average test loss: 0.002638098289155298\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013566135213606887\n",
      "Average test loss: 0.002645142029143042\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01354056513061126\n",
      "Average test loss: 0.002633620638607277\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01352511787497335\n",
      "Average test loss: 0.002661521797792779\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013531726988653342\n",
      "Average test loss: 0.0026353313188172047\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01349166237976816\n",
      "Average test loss: 0.0026390231837415032\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013476916208035416\n",
      "Average test loss: 0.0026264585020641484\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013477305859327316\n",
      "Average test loss: 0.002624272417484058\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013466100411282645\n",
      "Average test loss: 0.002624206271229519\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013451991694668928\n",
      "Average test loss: 0.002632823557489448\n",
      "Epoch 79/300\n",
      "Average training loss: 0.013442843592001333\n",
      "Average test loss: 0.002671104163552324\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013437841928253572\n",
      "Average test loss: 0.0026363351601693366\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013429447847108046\n",
      "Average test loss: 0.0026423034767309823\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013406643859214253\n",
      "Average test loss: 0.00268112251535058\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013386390017138587\n",
      "Average test loss: 0.0026201326715656454\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013381280611786577\n",
      "Average test loss: 0.002640503054898646\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013356023245387608\n",
      "Average test loss: 0.0026545344305535156\n",
      "Epoch 86/300\n",
      "Average training loss: 0.013366990729338593\n",
      "Average test loss: 0.00262888140976429\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01334606128020419\n",
      "Average test loss: 0.0026595353052640954\n",
      "Epoch 88/300\n",
      "Average training loss: 0.013330384766062102\n",
      "Average test loss: 0.0026624898811181386\n",
      "Epoch 89/300\n",
      "Average training loss: 0.013322782064477602\n",
      "Average test loss: 0.0026962443116224474\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013309440103669962\n",
      "Average test loss: 0.0026432023271918295\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013308852173388005\n",
      "Average test loss: 0.002665040922144221\n",
      "Epoch 92/300\n",
      "Average training loss: 0.013276333537366656\n",
      "Average test loss: 0.0026336746865676507\n",
      "Epoch 93/300\n",
      "Average training loss: 0.013279096303714646\n",
      "Average test loss: 0.0027098760733173955\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013263230298128393\n",
      "Average test loss: 0.0026844403803762463\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013271868604752752\n",
      "Average test loss: 0.0026887352870156368\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013246513137386905\n",
      "Average test loss: 0.002680455500777397\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01323808042705059\n",
      "Average test loss: 0.002656834439271026\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01323695845156908\n",
      "Average test loss: 0.0026402425923281246\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013227786080704795\n",
      "Average test loss: 0.0026843852260046534\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013215692371129989\n",
      "Average test loss: 0.0026837530814939076\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013192283670935365\n",
      "Average test loss: 0.0026347060973445574\n",
      "Epoch 102/300\n",
      "Average training loss: 0.013226580592493216\n",
      "Average test loss: 0.0026625549899828103\n",
      "Epoch 103/300\n",
      "Average training loss: 0.013187398172914982\n",
      "Average test loss: 0.002623336342473825\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01316672652380334\n",
      "Average test loss: 0.00262090133358207\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013147612601518631\n",
      "Average test loss: 0.0026475878972560167\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013161165029638343\n",
      "Average test loss: 0.00265080748208695\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013134305812418461\n",
      "Average test loss: 0.002678386849247747\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013154509441720115\n",
      "Average test loss: 0.002671535037871864\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013120934375458294\n",
      "Average test loss: 0.002642850294088324\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013122802553077538\n",
      "Average test loss: 0.0026503846185902753\n",
      "Epoch 111/300\n",
      "Average training loss: 0.013092643492751651\n",
      "Average test loss: 0.002678439361560676\n",
      "Epoch 112/300\n",
      "Average training loss: 0.013098416840864553\n",
      "Average test loss: 0.0026479481882933113\n",
      "Epoch 113/300\n",
      "Average training loss: 0.013096287701692846\n",
      "Average test loss: 0.0027553465047644244\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013072247637642755\n",
      "Average test loss: 0.0026544347550306055\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013070548589858745\n",
      "Average test loss: 0.002676759121318658\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013055066647628942\n",
      "Average test loss: 0.0026724893450736998\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013058806760443582\n",
      "Average test loss: 0.0026893886524356073\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013038603410124779\n",
      "Average test loss: 0.002693829144868586\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013028786505262057\n",
      "Average test loss: 0.002770754495014747\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013020929603113069\n",
      "Average test loss: 0.0026849921910713117\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013023663453757764\n",
      "Average test loss: 0.0027214110591966246\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013005779393845134\n",
      "Average test loss: 0.002730035754955477\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013004410316132836\n",
      "Average test loss: 0.00274245630370246\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012996398440665668\n",
      "Average test loss: 0.0026470577880326244\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012981620844039652\n",
      "Average test loss: 0.002658113391035133\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012965402502152655\n",
      "Average test loss: 0.002665247697590126\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01295915501895878\n",
      "Average test loss: 0.0027175905505816144\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012953025847673416\n",
      "Average test loss: 0.002719213050686651\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01294210498697228\n",
      "Average test loss: 0.0026961229058603446\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012956714015040133\n",
      "Average test loss: 0.0026936590385933717\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012942285078267257\n",
      "Average test loss: 0.002684108297444052\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012932068264732758\n",
      "Average test loss: 0.0027151239404661787\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012920231356388992\n",
      "Average test loss: 0.0026981908285783397\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012905103902022043\n",
      "Average test loss: 0.002727929297524194\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012900749874611695\n",
      "Average test loss: 0.0026892603799286816\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012910864813460245\n",
      "Average test loss: 0.002688139545627766\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012932772989074389\n",
      "Average test loss: 0.002648467584616608\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012874794282019139\n",
      "Average test loss: 0.0027305994605024654\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012901312819785541\n",
      "Average test loss: 0.0028139953155898387\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012863504221041997\n",
      "Average test loss: 0.11312149165736304\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012870110953847567\n",
      "Average test loss: 0.0027190834240367013\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012841630045738486\n",
      "Average test loss: 0.0027094888834075796\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012844260364770889\n",
      "Average test loss: 0.002713509993834628\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01284461071093877\n",
      "Average test loss: 0.002663276905193925\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01284455749971999\n",
      "Average test loss: 0.0027211282381580937\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012818243886033694\n",
      "Average test loss: 0.0026832585814926357\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012822464392417007\n",
      "Average test loss: 0.0026840696879145173\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01282538054469559\n",
      "Average test loss: 0.0026925336650262277\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012800120672418012\n",
      "Average test loss: 0.00271574585698545\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012822287053283718\n",
      "Average test loss: 0.002795786444718639\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012812389406065145\n",
      "Average test loss: 0.002697352097887132\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012778699022200373\n",
      "Average test loss: 0.0028522493567110763\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012787914086547163\n",
      "Average test loss: 0.0026998533372663788\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012772670601805052\n",
      "Average test loss: 0.0027637939202702706\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012774038393464353\n",
      "Average test loss: 0.0027323205725600323\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012763634067442683\n",
      "Average test loss: 0.0028190027847886084\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012762893502910932\n",
      "Average test loss: 0.002774573925468657\n",
      "Epoch 158/300\n",
      "Average training loss: 0.012745555106964377\n",
      "Average test loss: 0.002717072248665823\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012741205845442083\n",
      "Average test loss: 0.0026813304125227863\n",
      "Epoch 160/300\n",
      "Average training loss: 0.012738112000127633\n",
      "Average test loss: 0.0026684265360236168\n",
      "Epoch 161/300\n",
      "Average training loss: 0.012742340591218737\n",
      "Average test loss: 0.0027700236895018154\n",
      "Epoch 162/300\n",
      "Average training loss: 0.012729220118787554\n",
      "Average test loss: 0.002716553347185254\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012731144125262897\n",
      "Average test loss: 0.002703747655575474\n",
      "Epoch 164/300\n",
      "Average training loss: 0.012709015587965647\n",
      "Average test loss: 0.002675997777531544\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01272449734972583\n",
      "Average test loss: 0.0027807273606045377\n",
      "Epoch 166/300\n",
      "Average training loss: 0.012691521441770924\n",
      "Average test loss: 0.0027271413291907972\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01269488285895851\n",
      "Average test loss: 0.002770178919244144\n",
      "Epoch 168/300\n",
      "Average training loss: 0.012699298664099641\n",
      "Average test loss: 0.002785505464507474\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012687951660818524\n",
      "Average test loss: 0.0027407201290544536\n",
      "Epoch 170/300\n",
      "Average training loss: 0.012681663635704252\n",
      "Average test loss: 0.0026901229843497277\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012669232915672991\n",
      "Average test loss: 0.002679835382124616\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012696992210215993\n",
      "Average test loss: 0.0027892103536675374\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012673295956518916\n",
      "Average test loss: 0.002755281088459823\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012675919181770749\n",
      "Average test loss: 0.002719225312686629\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0126425867991315\n",
      "Average test loss: 0.002716644043309821\n",
      "Epoch 176/300\n",
      "Average training loss: 0.012652653753757476\n",
      "Average test loss: 0.0027380539420992136\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012651839530302418\n",
      "Average test loss: 0.002695853292528126\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012638674590322706\n",
      "Average test loss: 0.0027493404366282954\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012633475235766835\n",
      "Average test loss: 0.0027053736994663872\n",
      "Epoch 180/300\n",
      "Average training loss: 0.012619455892178747\n",
      "Average test loss: 0.0027351308157667516\n",
      "Epoch 181/300\n",
      "Average training loss: 0.012620148002273507\n",
      "Average test loss: 0.0027276283920639093\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01261042281240225\n",
      "Average test loss: 0.0027448195659865934\n",
      "Epoch 183/300\n",
      "Average training loss: 0.012613495378858513\n",
      "Average test loss: 0.002769919920505749\n",
      "Epoch 184/300\n",
      "Average training loss: 0.012609502459979719\n",
      "Average test loss: 0.0027508012391626833\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01260277797944016\n",
      "Average test loss: 0.0027324418808437057\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012625809335460265\n",
      "Average test loss: 0.0028192192115303544\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01258324929823478\n",
      "Average test loss: 0.002749367679365807\n",
      "Epoch 188/300\n",
      "Average training loss: 0.012597667137781779\n",
      "Average test loss: 0.0028444747080405555\n",
      "Epoch 189/300\n",
      "Average training loss: 0.012592720400955942\n",
      "Average test loss: 0.0027527000359776946\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01256861709141069\n",
      "Average test loss: 0.002743137273937464\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012592798031038709\n",
      "Average test loss: 0.002729722181128131\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01256286817871862\n",
      "Average test loss: 0.00271393308788538\n",
      "Epoch 193/300\n",
      "Average training loss: 0.012568068683147431\n",
      "Average test loss: 0.002816456541419029\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012560046814382076\n",
      "Average test loss: 0.0027003102128704388\n",
      "Epoch 195/300\n",
      "Average training loss: 0.012578158559070693\n",
      "Average test loss: 0.0027413444409353865\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012546061543954743\n",
      "Average test loss: 0.0027039269569019476\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012554248885975944\n",
      "Average test loss: 0.002770374650342597\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012545048255059454\n",
      "Average test loss: 0.0028111279801362092\n",
      "Epoch 199/300\n",
      "Average training loss: 0.012532754007312987\n",
      "Average test loss: 0.0027774646892729732\n",
      "Epoch 200/300\n",
      "Average training loss: 0.012545342957807912\n",
      "Average test loss: 0.00277345928704987\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012524547110001246\n",
      "Average test loss: 0.002754771990701556\n",
      "Epoch 202/300\n",
      "Average training loss: 0.012518033326499992\n",
      "Average test loss: 0.0027573086956722867\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012548323353131613\n",
      "Average test loss: 0.002755649366726478\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01249620794173744\n",
      "Average test loss: 0.0027501694874631035\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012532077606353494\n",
      "Average test loss: 0.002724896258985003\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0125469824920098\n",
      "Average test loss: 0.002778777222459515\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012505270048148102\n",
      "Average test loss: 0.002717030832130048\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012488456554710864\n",
      "Average test loss: 0.002797261374692122\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012490764099276729\n",
      "Average test loss: 0.0027565305704871815\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01250395468622446\n",
      "Average test loss: 0.002778737619312273\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01248624109228452\n",
      "Average test loss: 0.002690219594165683\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012485552581648032\n",
      "Average test loss: 0.0027026192226136726\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012477746348414157\n",
      "Average test loss: 0.0027537037630875907\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012475512214832835\n",
      "Average test loss: 0.0027731229029595854\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012480145613352457\n",
      "Average test loss: 0.002822835641602675\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012496999250931872\n",
      "Average test loss: 0.002827901511349612\n",
      "Epoch 217/300\n",
      "Average training loss: 0.012477413703997929\n",
      "Average test loss: 0.0027964351987466215\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012491661706732379\n",
      "Average test loss: 0.0028161590730564462\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012457359686493873\n",
      "Average test loss: 0.002803206215095189\n",
      "Epoch 220/300\n",
      "Average training loss: 0.012446962780422635\n",
      "Average test loss: 0.002700405677780509\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012453512698411941\n",
      "Average test loss: 0.0027071480332977243\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012437396847539478\n",
      "Average test loss: 0.0028719095072398584\n",
      "Epoch 223/300\n",
      "Average training loss: 0.012455682545072502\n",
      "Average test loss: 0.002773811905541354\n",
      "Epoch 224/300\n",
      "Average training loss: 0.012471829045977857\n",
      "Average test loss: 0.002727362451247043\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012447110477421018\n",
      "Average test loss: 0.0027035612410141363\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012429314576917224\n",
      "Average test loss: 0.0028047479142745335\n",
      "Epoch 227/300\n",
      "Average training loss: 0.012423930868506431\n",
      "Average test loss: 0.0027461017726196184\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012438782569434908\n",
      "Average test loss: 0.0027418445175927545\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012427763238549232\n",
      "Average test loss: 0.00272915267613199\n",
      "Epoch 230/300\n",
      "Average training loss: 0.012415419482522541\n",
      "Average test loss: 0.0027324634220244155\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012414545873800914\n",
      "Average test loss: 0.0028183313235640526\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01241638577481111\n",
      "Average test loss: 0.0027503844909369944\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01241321206962069\n",
      "Average test loss: 0.0028042410721795426\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012409277295900715\n",
      "Average test loss: 0.0027940402606295217\n",
      "Epoch 235/300\n",
      "Average training loss: 0.012395114329126147\n",
      "Average test loss: 0.002763431157502863\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012400074279970592\n",
      "Average test loss: 0.002829313990970453\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012408888056874276\n",
      "Average test loss: 0.0027590312551086147\n",
      "Epoch 238/300\n",
      "Average training loss: 0.012397437748809656\n",
      "Average test loss: 0.002781055265830623\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012394540732105573\n",
      "Average test loss: 0.002765085033658478\n",
      "Epoch 240/300\n",
      "Average training loss: 0.012380864526662561\n",
      "Average test loss: 0.0028285393493456975\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012381219752132893\n",
      "Average test loss: 0.0027850029712749853\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012409192013243834\n",
      "Average test loss: 0.0027589809977345997\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012382740984360378\n",
      "Average test loss: 0.002726008038553927\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012382126049862968\n",
      "Average test loss: 0.002745029137780269\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012369517438941532\n",
      "Average test loss: 0.0028120740556882486\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012370773240923881\n",
      "Average test loss: 0.0027285706135961743\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012357352117697397\n",
      "Average test loss: 0.002765927830297086\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012368922657850716\n",
      "Average test loss: 0.0028020118510143624\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012359552936835421\n",
      "Average test loss: 0.002859862992953923\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012368687306841214\n",
      "Average test loss: 0.0028067221043424472\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012353804022901588\n",
      "Average test loss: 0.0027216542802958026\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012336931097838615\n",
      "Average test loss: 0.00282451178609497\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012347669583227899\n",
      "Average test loss: 0.0027652870137244464\n",
      "Epoch 254/300\n",
      "Average training loss: 0.012353150623540084\n",
      "Average test loss: 0.002731891938795646\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012337091623908943\n",
      "Average test loss: 0.002790504905084769\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012339121365712748\n",
      "Average test loss: 0.0029565928996437125\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012328084276782142\n",
      "Average test loss: 0.0027434782385826113\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01233915180216233\n",
      "Average test loss: 0.0027881227715147866\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012330953911360767\n",
      "Average test loss: 0.0028119949704657\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01234233887328042\n",
      "Average test loss: 0.0027811051468468375\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01232603663040532\n",
      "Average test loss: 0.002886718698259857\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012316899518171946\n",
      "Average test loss: 0.002817008712432451\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012330756458971236\n",
      "Average test loss: 0.002737690984788868\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01230526951700449\n",
      "Average test loss: 0.002824988634636005\n",
      "Epoch 265/300\n",
      "Average training loss: 0.012308698281645774\n",
      "Average test loss: 0.0028472858077536025\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01231210776584016\n",
      "Average test loss: 0.0027896886124379105\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012312979968885581\n",
      "Average test loss: 0.002777266764599416\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012308835843371021\n",
      "Average test loss: 0.0027499115820974114\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012297221524847878\n",
      "Average test loss: 0.0028166767500547897\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01229676900886827\n",
      "Average test loss: 0.0028261129351125824\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012307065312233237\n",
      "Average test loss: 0.0028008149628423983\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012305410877697997\n",
      "Average test loss: 0.0028266867211916382\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012297528449032041\n",
      "Average test loss: 0.0030379363062481087\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012297425870266226\n",
      "Average test loss: 0.0027260907721809217\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01229110817031728\n",
      "Average test loss: 0.0027745196496446927\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012295338254008028\n",
      "Average test loss: 0.002834411706154545\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012277879291110568\n",
      "Average test loss: 0.002826177653753095\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012287004698481825\n",
      "Average test loss: 0.0027969657300661007\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012290368561943371\n",
      "Average test loss: 0.0027311088218250207\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012276158715287845\n",
      "Average test loss: 0.0027908540248043007\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0122731178369787\n",
      "Average test loss: 0.0028527915806819994\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012271197345521715\n",
      "Average test loss: 0.002770679224903385\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01225855061908563\n",
      "Average test loss: 0.0028010334848529764\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012295953655408488\n",
      "Average test loss: 0.0027870921567082407\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012258653052151203\n",
      "Average test loss: 0.002807960935971803\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012257692494326167\n",
      "Average test loss: 0.002825967427964012\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012252040171788799\n",
      "Average test loss: 0.0027545601951165333\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012256553371747334\n",
      "Average test loss: 0.0027500437702983617\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01224815343403154\n",
      "Average test loss: 0.0028519267928269173\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012262500057617823\n",
      "Average test loss: 0.0028054578927242096\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012251509436302715\n",
      "Average test loss: 0.0027874568408976\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012245811209910445\n",
      "Average test loss: 0.0028648911292354267\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012233745168480609\n",
      "Average test loss: 0.002848816808934013\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012265128089321984\n",
      "Average test loss: 0.0028080983095698886\n",
      "Epoch 295/300\n",
      "Average training loss: 0.012230015771256552\n",
      "Average test loss: 0.0028052973441986574\n",
      "Epoch 296/300\n",
      "Average training loss: 0.012249748221702045\n",
      "Average test loss: 0.0027839794407288235\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01223666077769465\n",
      "Average test loss: 0.002832248594818844\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012234519488281673\n",
      "Average test loss: 0.0028801530943148666\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012236620116564963\n",
      "Average test loss: 0.0027782253107676903\n",
      "Epoch 300/300\n",
      "Average training loss: 0.012227249330116643\n",
      "Average test loss: 0.0027986535746604203\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04754606573283672\n",
      "Average test loss: 0.0034124369443290763\n",
      "Epoch 2/300\n",
      "Average training loss: 0.016825403271449938\n",
      "Average test loss: 0.0031867087940788933\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01584066517319944\n",
      "Average test loss: 0.0030079025913857752\n",
      "Epoch 4/300\n",
      "Average training loss: 0.015236146250532732\n",
      "Average test loss: 0.0028576292420427005\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014727139014336798\n",
      "Average test loss: 0.002822075857884354\n",
      "Epoch 6/300\n",
      "Average training loss: 0.014316957794957692\n",
      "Average test loss: 0.0026860207511732975\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01396737745569812\n",
      "Average test loss: 0.0026231797309592365\n",
      "Epoch 8/300\n",
      "Average training loss: 0.013673253434399764\n",
      "Average test loss: 0.002559105527276794\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01339817090663645\n",
      "Average test loss: 0.002512136079164015\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01316645220418771\n",
      "Average test loss: 0.002505143501692348\n",
      "Epoch 11/300\n",
      "Average training loss: 0.012930350258118577\n",
      "Average test loss: 0.0024625891999651988\n",
      "Epoch 12/300\n",
      "Average training loss: 0.012737649223870702\n",
      "Average test loss: 0.002443119952351683\n",
      "Epoch 13/300\n",
      "Average training loss: 0.012565074381728967\n",
      "Average test loss: 0.0022947857609639565\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012401313176585568\n",
      "Average test loss: 0.002270875783326725\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01227067648867766\n",
      "Average test loss: 0.002360110765736964\n",
      "Epoch 16/300\n",
      "Average training loss: 0.012149762470689085\n",
      "Average test loss: 0.0022480135777344307\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01203226276818249\n",
      "Average test loss: 0.002252097106228272\n",
      "Epoch 18/300\n",
      "Average training loss: 0.011934344687809547\n",
      "Average test loss: 0.002187473493421243\n",
      "Epoch 19/300\n",
      "Average training loss: 0.011836595795220799\n",
      "Average test loss: 0.002146565899045931\n",
      "Epoch 20/300\n",
      "Average training loss: 0.011754794538021088\n",
      "Average test loss: 0.002140849723584122\n",
      "Epoch 21/300\n",
      "Average training loss: 0.011669670084284411\n",
      "Average test loss: 0.0021283528450876475\n",
      "Epoch 22/300\n",
      "Average training loss: 0.011602341602245967\n",
      "Average test loss: 0.0021234362255781888\n",
      "Epoch 23/300\n",
      "Average training loss: 0.011548762554509772\n",
      "Average test loss: 0.002107932331247462\n",
      "Epoch 24/300\n",
      "Average training loss: 0.011477247809370358\n",
      "Average test loss: 0.002121658673406475\n",
      "Epoch 25/300\n",
      "Average training loss: 0.011433579306635591\n",
      "Average test loss: 0.0020834785495988197\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01137104179792934\n",
      "Average test loss: 0.0020754220607793993\n",
      "Epoch 27/300\n",
      "Average training loss: 0.011333725246290366\n",
      "Average test loss: 0.002061159851650397\n",
      "Epoch 28/300\n",
      "Average training loss: 0.011272681161761285\n",
      "Average test loss: 0.002035833368802236\n",
      "Epoch 29/300\n",
      "Average training loss: 0.011244252005385027\n",
      "Average test loss: 0.0020693239081237052\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011192828020287884\n",
      "Average test loss: 0.0020189004919181266\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011152790296408865\n",
      "Average test loss: 0.0020142777937774857\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01114591783285141\n",
      "Average test loss: 0.002024155751336366\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01108646999630663\n",
      "Average test loss: 0.0020262770805921817\n",
      "Epoch 34/300\n",
      "Average training loss: 0.011053393399549855\n",
      "Average test loss: 0.0020144794289436605\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011032648368842072\n",
      "Average test loss: 0.002002506688858072\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011024315435025427\n",
      "Average test loss: 0.00201022840042909\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01096999499367343\n",
      "Average test loss: 0.0019949196902000242\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010949788430498706\n",
      "Average test loss: 0.002005108823792802\n",
      "Epoch 39/300\n",
      "Average training loss: 0.010921554767423207\n",
      "Average test loss: 0.002010262837012609\n",
      "Epoch 40/300\n",
      "Average training loss: 0.010901279120809503\n",
      "Average test loss: 0.001977204652089212\n",
      "Epoch 41/300\n",
      "Average training loss: 0.010872350203494231\n",
      "Average test loss: 0.0019751477914137974\n",
      "Epoch 42/300\n",
      "Average training loss: 0.010862990696397092\n",
      "Average test loss: 0.001988810209971335\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010831335152188936\n",
      "Average test loss: 0.0019692967577526967\n",
      "Epoch 44/300\n",
      "Average training loss: 0.010817516667147477\n",
      "Average test loss: 0.001979502685368061\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010795526260096167\n",
      "Average test loss: 0.0020035305676153964\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01077978420837058\n",
      "Average test loss: 0.001970005493818058\n",
      "Epoch 47/300\n",
      "Average training loss: 0.010759257901873853\n",
      "Average test loss: 0.0019926959871210985\n",
      "Epoch 48/300\n",
      "Average training loss: 0.010735493363605604\n",
      "Average test loss: 0.0019765064989527067\n",
      "Epoch 49/300\n",
      "Average training loss: 0.010735755231645373\n",
      "Average test loss: 0.0019678210401907563\n",
      "Epoch 50/300\n",
      "Average training loss: 0.010714035953084627\n",
      "Average test loss: 0.0020580909642287427\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010689685043361452\n",
      "Average test loss: 0.0019553269531784787\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010668853887253337\n",
      "Average test loss: 0.001959147069810165\n",
      "Epoch 53/300\n",
      "Average training loss: 0.010658107404907545\n",
      "Average test loss: 0.0019653814737167624\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010635708053906758\n",
      "Average test loss: 0.001967090359164609\n",
      "Epoch 55/300\n",
      "Average training loss: 0.010625022106700473\n",
      "Average test loss: 0.001959281412884593\n",
      "Epoch 56/300\n",
      "Average training loss: 0.010608574440495835\n",
      "Average test loss: 0.001981371055564119\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01059721967164013\n",
      "Average test loss: 0.0019544827807694673\n",
      "Epoch 58/300\n",
      "Average training loss: 0.010583577214015854\n",
      "Average test loss: 0.0019393554320558907\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01056295388440291\n",
      "Average test loss: 0.0019615642107609246\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010561771793497933\n",
      "Average test loss: 0.001955307862100502\n",
      "Epoch 61/300\n",
      "Average training loss: 0.010546009364227454\n",
      "Average test loss: 0.00196666972235673\n",
      "Epoch 62/300\n",
      "Average training loss: 0.010540480327275064\n",
      "Average test loss: 0.0019978713053796027\n",
      "Epoch 63/300\n",
      "Average training loss: 0.010502132436467542\n",
      "Average test loss: 0.001969894638595482\n",
      "Epoch 64/300\n",
      "Average training loss: 0.010489711728360917\n",
      "Average test loss: 0.00196661808134781\n",
      "Epoch 65/300\n",
      "Average training loss: 0.010491374081207646\n",
      "Average test loss: 0.00194425804561211\n",
      "Epoch 66/300\n",
      "Average training loss: 0.010473114473952187\n",
      "Average test loss: 0.001957212687159578\n",
      "Epoch 67/300\n",
      "Average training loss: 0.010471325888401932\n",
      "Average test loss: 0.0019522189500017298\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01047600543167856\n",
      "Average test loss: 0.00194286058905224\n",
      "Epoch 69/300\n",
      "Average training loss: 0.010454833202064038\n",
      "Average test loss: 0.0019399360136853323\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01041409597876999\n",
      "Average test loss: 0.0019517612252384425\n",
      "Epoch 71/300\n",
      "Average training loss: 0.010428860683408049\n",
      "Average test loss: 0.0019346423006306093\n",
      "Epoch 72/300\n",
      "Average training loss: 0.010404573630955484\n",
      "Average test loss: 0.001967408551524083\n",
      "Epoch 73/300\n",
      "Average training loss: 0.010383636612031195\n",
      "Average test loss: 0.0019665994012935295\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0103885236804684\n",
      "Average test loss: 0.0019425527242322763\n",
      "Epoch 75/300\n",
      "Average training loss: 0.010373371582064363\n",
      "Average test loss: 0.001972115194942388\n",
      "Epoch 76/300\n",
      "Average training loss: 0.010350119012097517\n",
      "Average test loss: 0.001956511222033037\n",
      "Epoch 77/300\n",
      "Average training loss: 0.010353871447758543\n",
      "Average test loss: 0.001963424883989824\n",
      "Epoch 78/300\n",
      "Average training loss: 0.010346010191573036\n",
      "Average test loss: 0.001980248170180453\n",
      "Epoch 79/300\n",
      "Average training loss: 0.010343719207578235\n",
      "Average test loss: 0.0019614490933923256\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010313480844928159\n",
      "Average test loss: 0.0019949928661808372\n",
      "Epoch 81/300\n",
      "Average training loss: 0.010315884429133601\n",
      "Average test loss: 0.0019719961480134064\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010294157581196891\n",
      "Average test loss: 0.0019421244986976187\n",
      "Epoch 83/300\n",
      "Average training loss: 0.010297329402632184\n",
      "Average test loss: 0.001974241547907392\n",
      "Epoch 84/300\n",
      "Average training loss: 0.010277076224072111\n",
      "Average test loss: 0.0019532106469074884\n",
      "Epoch 85/300\n",
      "Average training loss: 0.010284575157281425\n",
      "Average test loss: 0.001948427007223169\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010253509751624531\n",
      "Average test loss: 0.001965642662304971\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01024742268357012\n",
      "Average test loss: 0.0019532667864114047\n",
      "Epoch 88/300\n",
      "Average training loss: 0.010239913060433335\n",
      "Average test loss: 0.0019444155337082015\n",
      "Epoch 89/300\n",
      "Average training loss: 0.010248100197149647\n",
      "Average test loss: 0.0019483164755834473\n",
      "Epoch 90/300\n",
      "Average training loss: 0.010251396105521254\n",
      "Average test loss: 0.0019793767692107295\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0102241315982408\n",
      "Average test loss: 0.001979154098778963\n",
      "Epoch 92/300\n",
      "Average training loss: 0.010194436419341299\n",
      "Average test loss: 0.0019398962466253174\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010191378008160326\n",
      "Average test loss: 0.001958744880381144\n",
      "Epoch 94/300\n",
      "Average training loss: 0.010193054947174257\n",
      "Average test loss: 0.0019365003526003823\n",
      "Epoch 95/300\n",
      "Average training loss: 0.010188504214915965\n",
      "Average test loss: 0.0019903834625664683\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010201143592596055\n",
      "Average test loss: 0.0019328302676892943\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010155250960754024\n",
      "Average test loss: 0.001945812963363197\n",
      "Epoch 98/300\n",
      "Average training loss: 0.010149514858921368\n",
      "Average test loss: 0.001960378255902065\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010144984359542529\n",
      "Average test loss: 0.0019533296376466752\n",
      "Epoch 100/300\n",
      "Average training loss: 0.010153722298641999\n",
      "Average test loss: 0.0019455180406156513\n",
      "Epoch 101/300\n",
      "Average training loss: 0.010154767852690485\n",
      "Average test loss: 0.001953468888894551\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010114171198672719\n",
      "Average test loss: 0.001948072981606755\n",
      "Epoch 103/300\n",
      "Average training loss: 0.010134289387199614\n",
      "Average test loss: 0.001953570760579573\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01011130342218611\n",
      "Average test loss: 0.0019406113789106408\n",
      "Epoch 105/300\n",
      "Average training loss: 0.010097751207649708\n",
      "Average test loss: 0.0019976877957168554\n",
      "Epoch 106/300\n",
      "Average training loss: 0.010097977211078009\n",
      "Average test loss: 0.001957527899493774\n",
      "Epoch 107/300\n",
      "Average training loss: 0.010084693209992515\n",
      "Average test loss: 0.0019427694622427224\n",
      "Epoch 108/300\n",
      "Average training loss: 0.010085415815727578\n",
      "Average test loss: 0.0019722356241610316\n",
      "Epoch 109/300\n",
      "Average training loss: 0.010081306089129712\n",
      "Average test loss: 0.001996086623105738\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010059001365469562\n",
      "Average test loss: 0.0019581405085821947\n",
      "Epoch 111/300\n",
      "Average training loss: 0.010051692369911406\n",
      "Average test loss: 0.0019596245961470736\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0100466778849562\n",
      "Average test loss: 0.001970745394213332\n",
      "Epoch 113/300\n",
      "Average training loss: 0.010059750918712881\n",
      "Average test loss: 0.0020252809163389934\n",
      "Epoch 114/300\n",
      "Average training loss: 0.010028824783861637\n",
      "Average test loss: 0.0020759045030507776\n",
      "Epoch 115/300\n",
      "Average training loss: 0.010030663214623928\n",
      "Average test loss: 0.0019493868471019797\n",
      "Epoch 116/300\n",
      "Average training loss: 0.010030249855584568\n",
      "Average test loss: 0.0019955339446249935\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01000551870920592\n",
      "Average test loss: 0.0019644897372151416\n",
      "Epoch 118/300\n",
      "Average training loss: 0.010008216930760278\n",
      "Average test loss: 0.0019847667617723347\n",
      "Epoch 119/300\n",
      "Average training loss: 0.010020222813718848\n",
      "Average test loss: 0.0020491019543260338\n",
      "Epoch 120/300\n",
      "Average training loss: 0.00998602318018675\n",
      "Average test loss: 0.00199267191439867\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009994246137638886\n",
      "Average test loss: 0.0019905004012915825\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009976092182099819\n",
      "Average test loss: 0.002375546086476081\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009979731971191036\n",
      "Average test loss: 0.0019749706625524495\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009966393243521452\n",
      "Average test loss: 0.0019434443568396899\n",
      "Epoch 125/300\n",
      "Average training loss: 0.009971420687933764\n",
      "Average test loss: 0.0019457363792074224\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009950937391983139\n",
      "Average test loss: 0.00195318047526396\n",
      "Epoch 127/300\n",
      "Average training loss: 0.009959047165181902\n",
      "Average test loss: 0.001973511815898948\n",
      "Epoch 128/300\n",
      "Average training loss: 0.009941608291533259\n",
      "Average test loss: 0.0020424911585739916\n",
      "Epoch 129/300\n",
      "Average training loss: 0.009966483138501644\n",
      "Average test loss: 0.002071659373740355\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009948382939315504\n",
      "Average test loss: 0.0020372870239532656\n",
      "Epoch 131/300\n",
      "Average training loss: 0.009935456068151527\n",
      "Average test loss: 0.0020212440801163517\n",
      "Epoch 132/300\n",
      "Average training loss: 0.009909539505839348\n",
      "Average test loss: 0.0019811207650022376\n",
      "Epoch 133/300\n",
      "Average training loss: 0.009906263651119337\n",
      "Average test loss: 0.0020032019321289327\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009905050875412094\n",
      "Average test loss: 0.00196116175254186\n",
      "Epoch 135/300\n",
      "Average training loss: 0.009913701311167743\n",
      "Average test loss: 0.0020083917101017305\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009903216807378663\n",
      "Average test loss: 0.0020964015782293345\n",
      "Epoch 137/300\n",
      "Average training loss: 0.009898947277002865\n",
      "Average test loss: 0.0019597356950657235\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00989040001316203\n",
      "Average test loss: 0.0019658177260102497\n",
      "Epoch 139/300\n",
      "Average training loss: 0.009874906301084492\n",
      "Average test loss: 0.00201792552591198\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009881430049323374\n",
      "Average test loss: 0.001983747684914205\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009869889174898465\n",
      "Average test loss: 0.0019599970324585833\n",
      "Epoch 142/300\n",
      "Average training loss: 0.009874395835730765\n",
      "Average test loss: 0.0019612451517540546\n",
      "Epoch 143/300\n",
      "Average training loss: 0.009854653131630685\n",
      "Average test loss: 0.001980006494662828\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009883914685911602\n",
      "Average test loss: 0.001995569876084725\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009846515422066052\n",
      "Average test loss: 0.0019760504549162254\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009852076436082522\n",
      "Average test loss: 0.001988003417125179\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00984952402363221\n",
      "Average test loss: 0.0019678803947236804\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00982858353604873\n",
      "Average test loss: 0.0019974435747911533\n",
      "Epoch 149/300\n",
      "Average training loss: 0.00985569789715939\n",
      "Average test loss: 0.0019699960329259436\n",
      "Epoch 150/300\n",
      "Average training loss: 0.009818986817366548\n",
      "Average test loss: 0.0019727698436213864\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009811003804206847\n",
      "Average test loss: 0.0019939807337812252\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009822886464496453\n",
      "Average test loss: 0.0019708171810747846\n",
      "Epoch 153/300\n",
      "Average training loss: 0.00980939052750667\n",
      "Average test loss: 0.0020664355872819823\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009816062993473477\n",
      "Average test loss: 0.0020071308756661084\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009801518050332863\n",
      "Average test loss: 0.001986637395910091\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009810768363376458\n",
      "Average test loss: 0.0019783999368341433\n",
      "Epoch 157/300\n",
      "Average training loss: 0.009784509007301595\n",
      "Average test loss: 0.0020052981403552825\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00979517910712295\n",
      "Average test loss: 0.0019959763372316957\n",
      "Epoch 159/300\n",
      "Average training loss: 0.009778141587144799\n",
      "Average test loss: 0.0019759120733166733\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009777800968123807\n",
      "Average test loss: 0.0019990152118520605\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009775936836997668\n",
      "Average test loss: 0.002076175198165907\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009778152378896872\n",
      "Average test loss: 0.0019687206525769497\n",
      "Epoch 163/300\n",
      "Average training loss: 0.009758468275268873\n",
      "Average test loss: 0.0020052981661218736\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00976021723035309\n",
      "Average test loss: 0.0019966453982310163\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009760364221615924\n",
      "Average test loss: 0.0019747762857005\n",
      "Epoch 166/300\n",
      "Average training loss: 0.009746044328643216\n",
      "Average test loss: 0.0019608865790069104\n",
      "Epoch 167/300\n",
      "Average training loss: 0.009772880713144938\n",
      "Average test loss: 0.0019794802468063102\n",
      "Epoch 168/300\n",
      "Average training loss: 0.009726882801287704\n",
      "Average test loss: 0.0020481253632654746\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0097331511783931\n",
      "Average test loss: 0.0019877770483079883\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00973261099598474\n",
      "Average test loss: 0.002148439245401985\n",
      "Epoch 171/300\n",
      "Average training loss: 0.009735921051767137\n",
      "Average test loss: 0.001975873187494775\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0097265163279242\n",
      "Average test loss: 0.00199257743337916\n",
      "Epoch 173/300\n",
      "Average training loss: 0.009725002125733429\n",
      "Average test loss: 0.0020047429661369984\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00972437526202864\n",
      "Average test loss: 0.001998766172573798\n",
      "Epoch 175/300\n",
      "Average training loss: 0.009721575313972101\n",
      "Average test loss: 0.002029348166245553\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009700885776016447\n",
      "Average test loss: 0.0019933615772881443\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009710019018087122\n",
      "Average test loss: 0.0019767650018135707\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009695490789082314\n",
      "Average test loss: 0.001976256927889254\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009693126152786944\n",
      "Average test loss: 0.0020167735510816178\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009703542053699494\n",
      "Average test loss: 0.002006474296251933\n",
      "Epoch 181/300\n",
      "Average training loss: 0.009695675547752115\n",
      "Average test loss: 0.002012845123393668\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0096899475413892\n",
      "Average test loss: 0.0020066371036486495\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0096733722479807\n",
      "Average test loss: 0.0019661334483987757\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00968344758823514\n",
      "Average test loss: 0.001996070428027047\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009678932568265332\n",
      "Average test loss: 0.002003031690811945\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009678838580018945\n",
      "Average test loss: 0.00198478693742719\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009666244618594647\n",
      "Average test loss: 0.0020073592396866943\n",
      "Epoch 188/300\n",
      "Average training loss: 0.00965890967928701\n",
      "Average test loss: 0.0020135058886888955\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009651326701458957\n",
      "Average test loss: 0.002048513367979063\n",
      "Epoch 190/300\n",
      "Average training loss: 0.009663873559070958\n",
      "Average test loss: 0.002030147774145007\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009667042165994644\n",
      "Average test loss: 0.002008486192052563\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00965798655897379\n",
      "Average test loss: 0.0020013623273827962\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009639237788816294\n",
      "Average test loss: 0.0019810696289771132\n",
      "Epoch 194/300\n",
      "Average training loss: 0.009642220742172665\n",
      "Average test loss: 0.0019766802671882843\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009644175584945415\n",
      "Average test loss: 0.0019777227707414163\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00964166627658738\n",
      "Average test loss: 0.002031499341233737\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009632454399847322\n",
      "Average test loss: 0.00197787851964434\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009622762171758547\n",
      "Average test loss: 0.002046074039199286\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009636508553392356\n",
      "Average test loss: 0.002040929978920354\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009619456371913354\n",
      "Average test loss: 0.0020158507008519438\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009616920515067047\n",
      "Average test loss: 0.0020184734250522324\n",
      "Epoch 202/300\n",
      "Average training loss: 0.009613028284576204\n",
      "Average test loss: 0.0019962659267087775\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009612469511727492\n",
      "Average test loss: 0.0020041195071405834\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009612523448136118\n",
      "Average test loss: 0.0020559919112258488\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009607623546487755\n",
      "Average test loss: 0.00198513318453398\n",
      "Epoch 206/300\n",
      "Average training loss: 0.009601827235685454\n",
      "Average test loss: 0.0020022889595582255\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009595542175074419\n",
      "Average test loss: 0.002582747023759617\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009616072873274486\n",
      "Average test loss: 0.0019885751801646417\n",
      "Epoch 209/300\n",
      "Average training loss: 0.00958816815333234\n",
      "Average test loss: 0.001971128520762755\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009589013243714969\n",
      "Average test loss: 0.0019936782674243054\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009591263031793966\n",
      "Average test loss: 0.0020398789749791224\n",
      "Epoch 212/300\n",
      "Average training loss: 0.009580712149126662\n",
      "Average test loss: 0.0019889426512850654\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00959582560426659\n",
      "Average test loss: 0.002044419324439433\n",
      "Epoch 214/300\n",
      "Average training loss: 0.009578179978662067\n",
      "Average test loss: 0.002067197931723462\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009581665117707517\n",
      "Average test loss: 0.0019859618049942783\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00956228092395597\n",
      "Average test loss: 0.0020104992549038595\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009577414330508974\n",
      "Average test loss: 0.0020112708278207316\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009563055224716663\n",
      "Average test loss: 0.0020107025219541458\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00956065684515569\n",
      "Average test loss: 0.0020608264849417738\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009576041328410308\n",
      "Average test loss: 0.00198631024091608\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009557963311672212\n",
      "Average test loss: 0.002051252992409799\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009562567525025871\n",
      "Average test loss: 0.002086614743495981\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009559751759800646\n",
      "Average test loss: 0.002018625637309419\n",
      "Epoch 224/300\n",
      "Average training loss: 0.009564335647142595\n",
      "Average test loss: 0.002137848660142885\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009551480744861894\n",
      "Average test loss: 0.0020572128960241872\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009550321090552542\n",
      "Average test loss: 0.0020233201036850613\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00955342924180958\n",
      "Average test loss: 0.0020128516603468193\n",
      "Epoch 228/300\n",
      "Average training loss: 0.009533162972165478\n",
      "Average test loss: 0.002072883784564005\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009543602803515064\n",
      "Average test loss: 0.0020684466746946177\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009535516848994625\n",
      "Average test loss: 0.002005077754250831\n",
      "Epoch 231/300\n",
      "Average training loss: 0.009530863782597912\n",
      "Average test loss: 0.0020458037871867417\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00953151361975405\n",
      "Average test loss: 0.0019852737502919302\n",
      "Epoch 233/300\n",
      "Average training loss: 0.009534957458575567\n",
      "Average test loss: 0.00207093521207571\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009523631271388796\n",
      "Average test loss: 0.0020267853024933074\n",
      "Epoch 235/300\n",
      "Average training loss: 0.009514742096265157\n",
      "Average test loss: 0.0020490869981133275\n",
      "Epoch 236/300\n",
      "Average training loss: 0.00953008875830306\n",
      "Average test loss: 0.00204598930829929\n",
      "Epoch 237/300\n",
      "Average training loss: 0.009526452535970344\n",
      "Average test loss: 0.002093452383660608\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009524441140393415\n",
      "Average test loss: 0.0020339572516580424\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00951308702180783\n",
      "Average test loss: 0.0020352836528586015\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009499767235169808\n",
      "Average test loss: 0.0020379915836577613\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009517194515301121\n",
      "Average test loss: 0.0019989039128025373\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009514215010735724\n",
      "Average test loss: 0.002003646300795178\n",
      "Epoch 243/300\n",
      "Average training loss: 0.009511915757424301\n",
      "Average test loss: 0.0020261961933639313\n",
      "Epoch 244/300\n",
      "Average training loss: 0.009501985594630242\n",
      "Average test loss: 0.0020436530224978923\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009504163283440802\n",
      "Average test loss: 0.002008798701999088\n",
      "Epoch 246/300\n",
      "Average training loss: 0.009493073751529057\n",
      "Average test loss: 0.0020437523245604504\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009497551004919742\n",
      "Average test loss: 0.0020601179340026445\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009485715864019261\n",
      "Average test loss: 0.0020421622312731215\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009492949672457245\n",
      "Average test loss: 0.0020037630792293284\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009480277345412307\n",
      "Average test loss: 0.00208602978889313\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009491239973240428\n",
      "Average test loss: 0.0020626802597608833\n",
      "Epoch 252/300\n",
      "Average training loss: 0.009479991082515982\n",
      "Average test loss: 0.002020367775319351\n",
      "Epoch 253/300\n",
      "Average training loss: 0.009484738122257922\n",
      "Average test loss: 0.0020326140594358246\n",
      "Epoch 254/300\n",
      "Average training loss: 0.009485890370690161\n",
      "Average test loss: 0.002033953576038281\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00948196467757225\n",
      "Average test loss: 0.00203515665512532\n",
      "Epoch 256/300\n",
      "Average training loss: 0.009479683960477511\n",
      "Average test loss: 0.002021087392957674\n",
      "Epoch 257/300\n",
      "Average training loss: 0.009465265335308181\n",
      "Average test loss: 0.002027494736222757\n",
      "Epoch 258/300\n",
      "Average training loss: 0.009468667550219429\n",
      "Average test loss: 0.0020148842873879604\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009460378722184234\n",
      "Average test loss: 0.0019938212980826694\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009489693726930353\n",
      "Average test loss: 0.0020300430415405167\n",
      "Epoch 261/300\n",
      "Average training loss: 0.009462708464927144\n",
      "Average test loss: 0.0020107138947480256\n",
      "Epoch 262/300\n",
      "Average training loss: 0.009447649916013082\n",
      "Average test loss: 0.002060734417806897\n",
      "Epoch 263/300\n",
      "Average training loss: 0.009471748298241033\n",
      "Average test loss: 0.0020178386867046355\n",
      "Epoch 264/300\n",
      "Average training loss: 0.009452930375933647\n",
      "Average test loss: 0.002027621073855294\n",
      "Epoch 265/300\n",
      "Average training loss: 0.009455206875171927\n",
      "Average test loss: 0.00200915523359759\n",
      "Epoch 266/300\n",
      "Average training loss: 0.009449661758210924\n",
      "Average test loss: 0.002079934002003736\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0094474275579883\n",
      "Average test loss: 0.0020619444059621957\n",
      "Epoch 268/300\n",
      "Average training loss: 0.009449502329859468\n",
      "Average test loss: 0.0020187329676830106\n",
      "Epoch 269/300\n",
      "Average training loss: 0.009452695194217894\n",
      "Average test loss: 0.002028382255178359\n",
      "Epoch 270/300\n",
      "Average training loss: 0.009441105495724413\n",
      "Average test loss: 0.0020209587434720662\n",
      "Epoch 271/300\n",
      "Average training loss: 0.009447373622821437\n",
      "Average test loss: 0.0020494601111859085\n",
      "Epoch 272/300\n",
      "Average training loss: 0.009437789189318816\n",
      "Average test loss: 0.0020649288271864254\n",
      "Epoch 273/300\n",
      "Average training loss: 0.009434715742038356\n",
      "Average test loss: 0.002035628462417258\n",
      "Epoch 274/300\n",
      "Average training loss: 0.009429575506183837\n",
      "Average test loss: 0.002084835431021121\n",
      "Epoch 275/300\n",
      "Average training loss: 0.009431965938872761\n",
      "Average test loss: 0.0020058866313969097\n",
      "Epoch 276/300\n",
      "Average training loss: 0.009443849214249187\n",
      "Average test loss: 0.0020542427263119155\n",
      "Epoch 277/300\n",
      "Average training loss: 0.009428301502433088\n",
      "Average test loss: 0.002031910947420531\n",
      "Epoch 278/300\n",
      "Average training loss: 0.009429746497836377\n",
      "Average test loss: 0.002085479043217169\n",
      "Epoch 279/300\n",
      "Average training loss: 0.009420651702417267\n",
      "Average test loss: 0.002040766815551453\n",
      "Epoch 280/300\n",
      "Average training loss: 0.009432852170533605\n",
      "Average test loss: 0.0020516300487021604\n",
      "Epoch 281/300\n",
      "Average training loss: 0.009426846904059252\n",
      "Average test loss: 0.002089461058171259\n",
      "Epoch 282/300\n",
      "Average training loss: 0.009417544532981183\n",
      "Average test loss: 0.0020693729985505342\n",
      "Epoch 283/300\n",
      "Average training loss: 0.009414414050678412\n",
      "Average test loss: 0.0020839013010263445\n",
      "Epoch 284/300\n",
      "Average training loss: 0.009413246021502548\n",
      "Average test loss: 0.0020238499756281576\n",
      "Epoch 285/300\n",
      "Average training loss: 0.009404715386943685\n",
      "Average test loss: 0.0020035684749277103\n",
      "Epoch 286/300\n",
      "Average training loss: 0.009412369906074471\n",
      "Average test loss: 0.0020199621501896118\n",
      "Epoch 287/300\n",
      "Average training loss: 0.009409660023947556\n",
      "Average test loss: 0.002035498377246161\n",
      "Epoch 288/300\n",
      "Average training loss: 0.009410992415414916\n",
      "Average test loss: 0.0020225532270140116\n",
      "Epoch 289/300\n",
      "Average training loss: 0.009407378363940452\n",
      "Average test loss: 0.002091502351375918\n",
      "Epoch 290/300\n",
      "Average training loss: 0.009411138324273958\n",
      "Average test loss: 0.0020336437622706094\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00941055984215604\n",
      "Average test loss: 0.002078347662049863\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00939256072541078\n",
      "Average test loss: 0.002053950389019317\n",
      "Epoch 293/300\n",
      "Average training loss: 0.009384387705889013\n",
      "Average test loss: 0.002063933056054844\n",
      "Epoch 294/300\n",
      "Average training loss: 0.009390233248472215\n",
      "Average test loss: 0.0020881937144117224\n",
      "Epoch 295/300\n",
      "Average training loss: 0.009389107243468364\n",
      "Average test loss: 0.002071335781676074\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00939860649075773\n",
      "Average test loss: 0.002070670484047797\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0093970659615265\n",
      "Average test loss: 0.002099972939532664\n",
      "Epoch 298/300\n",
      "Average training loss: 0.009387416887614462\n",
      "Average test loss: 0.001997456786636677\n",
      "Epoch 299/300\n",
      "Average training loss: 0.009389924007985327\n",
      "Average test loss: 0.0025220434011684525\n",
      "Epoch 300/300\n",
      "Average training loss: 0.009391016409215\n",
      "Average test loss: 0.0020394985989357034\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.042896915002001657\n",
      "Average test loss: 0.0030322939267175067\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01398255393240187\n",
      "Average test loss: 0.002581364673872789\n",
      "Epoch 3/300\n",
      "Average training loss: 0.012925384515689479\n",
      "Average test loss: 0.002475937431367735\n",
      "Epoch 4/300\n",
      "Average training loss: 0.012263013332254357\n",
      "Average test loss: 0.0022668990628379915\n",
      "Epoch 5/300\n",
      "Average training loss: 0.011762466750211186\n",
      "Average test loss: 0.002132880364337729\n",
      "Epoch 6/300\n",
      "Average training loss: 0.011329879938314358\n",
      "Average test loss: 0.002051473056897521\n",
      "Epoch 7/300\n",
      "Average training loss: 0.010976782999104923\n",
      "Average test loss: 0.0019788127125551303\n",
      "Epoch 8/300\n",
      "Average training loss: 0.010652664622498883\n",
      "Average test loss: 0.001936514104406039\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01039343589047591\n",
      "Average test loss: 0.0018630812287123667\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010150677657789655\n",
      "Average test loss: 0.0018050494573803413\n",
      "Epoch 11/300\n",
      "Average training loss: 0.009964937712583277\n",
      "Average test loss: 0.0017613166325415174\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009790962565276357\n",
      "Average test loss: 0.0017826635158724256\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009643057992060979\n",
      "Average test loss: 0.0016968086489165823\n",
      "Epoch 14/300\n",
      "Average training loss: 0.009513546283874246\n",
      "Average test loss: 0.001723300306747357\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009385588611165683\n",
      "Average test loss: 0.001657871471510993\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009277084607217047\n",
      "Average test loss: 0.0016221840964216324\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009176774577134185\n",
      "Average test loss: 0.0016186980407478081\n",
      "Epoch 18/300\n",
      "Average training loss: 0.009084889043950372\n",
      "Average test loss: 0.0015943879370267193\n",
      "Epoch 19/300\n",
      "Average training loss: 0.009008005614082019\n",
      "Average test loss: 0.001576445078322043\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008942321520298719\n",
      "Average test loss: 0.001558820336850153\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00887366469990876\n",
      "Average test loss: 0.001545338597562578\n",
      "Epoch 22/300\n",
      "Average training loss: 0.008802679708020554\n",
      "Average test loss: 0.0015767599700225723\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008747003965079785\n",
      "Average test loss: 0.0015140032108045287\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008702760093741947\n",
      "Average test loss: 0.0015290545016113256\n",
      "Epoch 25/300\n",
      "Average training loss: 0.008643801732609669\n",
      "Average test loss: 0.0015175241017714143\n",
      "Epoch 26/300\n",
      "Average training loss: 0.008599490772518847\n",
      "Average test loss: 0.001502695720642805\n",
      "Epoch 27/300\n",
      "Average training loss: 0.008557682596974902\n",
      "Average test loss: 0.0015237424179083772\n",
      "Epoch 28/300\n",
      "Average training loss: 0.008523073190616237\n",
      "Average test loss: 0.0015070535784794225\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00848779066445099\n",
      "Average test loss: 0.0014638633193034265\n",
      "Epoch 30/300\n",
      "Average training loss: 0.008454948672403892\n",
      "Average test loss: 0.0014708411318974362\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00842031626527508\n",
      "Average test loss: 0.0014521629257748525\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008383815542277363\n",
      "Average test loss: 0.001460415665163762\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008363311195539104\n",
      "Average test loss: 0.0014622031801069775\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008337145961407158\n",
      "Average test loss: 0.0014724470169490411\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008308692579468091\n",
      "Average test loss: 0.0014764850501798922\n",
      "Epoch 36/300\n",
      "Average training loss: 0.008281941663473845\n",
      "Average test loss: 0.0014334876586993535\n",
      "Epoch 37/300\n",
      "Average training loss: 0.008265196575472752\n",
      "Average test loss: 0.0014376521796608964\n",
      "Epoch 38/300\n",
      "Average training loss: 0.008240333014478286\n",
      "Average test loss: 0.0014232884714793829\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00821652541971869\n",
      "Average test loss: 0.0014346759719774127\n",
      "Epoch 40/300\n",
      "Average training loss: 0.008195949596663315\n",
      "Average test loss: 0.0014321595982958873\n",
      "Epoch 41/300\n",
      "Average training loss: 0.008175704503224955\n",
      "Average test loss: 0.0014125653217650122\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008161596488207579\n",
      "Average test loss: 0.0014148616071583495\n",
      "Epoch 43/300\n",
      "Average training loss: 0.00815264325381981\n",
      "Average test loss: 0.0014273070333939459\n",
      "Epoch 44/300\n",
      "Average training loss: 0.008120073773794704\n",
      "Average test loss: 0.001402793182577524\n",
      "Epoch 45/300\n",
      "Average training loss: 0.008118827063590288\n",
      "Average test loss: 0.0014210261380713848\n",
      "Epoch 46/300\n",
      "Average training loss: 0.008097647051016489\n",
      "Average test loss: 0.0014148793311582672\n",
      "Epoch 47/300\n",
      "Average training loss: 0.008073069755401877\n",
      "Average test loss: 0.0014035827016147474\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008055420502192445\n",
      "Average test loss: 0.0013960852633333868\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008052355048557122\n",
      "Average test loss: 0.0013999624721085032\n",
      "Epoch 50/300\n",
      "Average training loss: 0.008034621857934528\n",
      "Average test loss: 0.001421186990932458\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00802434221530954\n",
      "Average test loss: 0.0013972878960064716\n",
      "Epoch 52/300\n",
      "Average training loss: 0.008006372868186899\n",
      "Average test loss: 0.001404131154012349\n",
      "Epoch 53/300\n",
      "Average training loss: 0.008001595174272855\n",
      "Average test loss: 0.0013809079219483666\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00798390568047762\n",
      "Average test loss: 0.0015561410411157543\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007972142243343923\n",
      "Average test loss: 0.0014268574016168714\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007951188457508881\n",
      "Average test loss: 0.0014059520134081444\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007947543227424225\n",
      "Average test loss: 0.001378597935092532\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007928163687802024\n",
      "Average test loss: 0.0013941042211113705\n",
      "Epoch 59/300\n",
      "Average training loss: 0.00792377045378089\n",
      "Average test loss: 0.001398627744263245\n",
      "Epoch 60/300\n",
      "Average training loss: 0.00791953516461783\n",
      "Average test loss: 0.0014308623425248596\n",
      "Epoch 61/300\n",
      "Average training loss: 0.007892514144380887\n",
      "Average test loss: 0.0014724787627864214\n",
      "Epoch 62/300\n",
      "Average training loss: 0.007885683696303103\n",
      "Average test loss: 0.00139303642221623\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007875214592450195\n",
      "Average test loss: 0.0014378371111945145\n",
      "Epoch 64/300\n",
      "Average training loss: 0.007863159616788228\n",
      "Average test loss: 0.0014137614856784542\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007848421590609682\n",
      "Average test loss: 0.0013798784415961969\n",
      "Epoch 66/300\n",
      "Average training loss: 0.007842992436554697\n",
      "Average test loss: 0.0013881514597063264\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007839511668930451\n",
      "Average test loss: 0.0014071346103317207\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007822504150784677\n",
      "Average test loss: 0.0013794614379811618\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007818163466536337\n",
      "Average test loss: 0.0013922691613228785\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007811697437117497\n",
      "Average test loss: 0.001373611638839874\n",
      "Epoch 71/300\n",
      "Average training loss: 0.007798558649917443\n",
      "Average test loss: 0.001375136324721906\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00778899846971035\n",
      "Average test loss: 0.0014117267582462066\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007773634308742152\n",
      "Average test loss: 0.0013777896277606487\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0077676001758211185\n",
      "Average test loss: 0.0014185884955028693\n",
      "Epoch 75/300\n",
      "Average training loss: 0.00776363357115123\n",
      "Average test loss: 0.0013894030605960223\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0077572904978361395\n",
      "Average test loss: 0.001371588899133106\n",
      "Epoch 77/300\n",
      "Average training loss: 0.007742428240676721\n",
      "Average test loss: 0.0013863747080581055\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0077397559020254345\n",
      "Average test loss: 0.0013846289264038206\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007724447133226527\n",
      "Average test loss: 0.0013859698709307445\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007725899728222026\n",
      "Average test loss: 0.0014150010351505545\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007710790682170126\n",
      "Average test loss: 0.0013977394372017848\n",
      "Epoch 82/300\n",
      "Average training loss: 0.007703451276653343\n",
      "Average test loss: 0.0013673409638512465\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007702926741706\n",
      "Average test loss: 0.0013881026682340436\n",
      "Epoch 84/300\n",
      "Average training loss: 0.007688269223190016\n",
      "Average test loss: 0.0013863906673052245\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00767964823005928\n",
      "Average test loss: 0.0013657579545138612\n",
      "Epoch 86/300\n",
      "Average training loss: 0.007687589304314719\n",
      "Average test loss: 0.0013682054142053757\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007661309980890817\n",
      "Average test loss: 0.0014961596633204155\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0076660078801214695\n",
      "Average test loss: 0.0013677819418824381\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007653130683220095\n",
      "Average test loss: 0.0014057496659871606\n",
      "Epoch 90/300\n",
      "Average training loss: 0.007639149305307203\n",
      "Average test loss: 0.001379719915592836\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00763530102537738\n",
      "Average test loss: 0.001418774341750476\n",
      "Epoch 92/300\n",
      "Average training loss: 0.00763934675604105\n",
      "Average test loss: 0.001379668241677185\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007626345024754603\n",
      "Average test loss: 0.0013632334429356787\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007621696869118346\n",
      "Average test loss: 0.0013741256679511732\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007608815845929914\n",
      "Average test loss: 0.0013969825603481797\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00761240006900496\n",
      "Average test loss: 0.0013833951758634713\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007600593624015649\n",
      "Average test loss: 0.0013804429065850045\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0075925718131992555\n",
      "Average test loss: 0.0013883808091696766\n",
      "Epoch 99/300\n",
      "Average training loss: 0.007584500585165289\n",
      "Average test loss: 0.001404449323295719\n",
      "Epoch 100/300\n",
      "Average training loss: 0.007575682282033894\n",
      "Average test loss: 0.001383494867115385\n",
      "Epoch 101/300\n",
      "Average training loss: 0.007569221925818258\n",
      "Average test loss: 0.0013933990762775971\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007567824280924267\n",
      "Average test loss: 0.001385724185862475\n",
      "Epoch 103/300\n",
      "Average training loss: 0.007567157400978936\n",
      "Average test loss: 0.0013757707308977843\n",
      "Epoch 104/300\n",
      "Average training loss: 0.007559077377948496\n",
      "Average test loss: 0.001371355987360908\n",
      "Epoch 105/300\n",
      "Average training loss: 0.007558533396157954\n",
      "Average test loss: 0.0013665626987607942\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0075439686584803795\n",
      "Average test loss: 0.0013941631076029605\n",
      "Epoch 107/300\n",
      "Average training loss: 0.007546024550580316\n",
      "Average test loss: 0.0013732240101736453\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0075381305668916964\n",
      "Average test loss: 0.0014056620799625912\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0075373055069810815\n",
      "Average test loss: 0.0013715169026205936\n",
      "Epoch 110/300\n",
      "Average training loss: 0.007525729128056102\n",
      "Average test loss: 0.0017408745764340792\n",
      "Epoch 111/300\n",
      "Average training loss: 0.007523226799236404\n",
      "Average test loss: 0.0015778538338426086\n",
      "Epoch 112/300\n",
      "Average training loss: 0.007512283550368415\n",
      "Average test loss: 0.0014810767984535132\n",
      "Epoch 113/300\n",
      "Average training loss: 0.007503688569284147\n",
      "Average test loss: 0.001372997042309079\n",
      "Epoch 114/300\n",
      "Average training loss: 0.007504079661849472\n",
      "Average test loss: 0.0013762574639792243\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00750063538716899\n",
      "Average test loss: 0.0013874508272856475\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0074916084077623155\n",
      "Average test loss: 0.0013992209164425732\n",
      "Epoch 117/300\n",
      "Average training loss: 0.007487092284278738\n",
      "Average test loss: 0.001390198102945255\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007489504124555323\n",
      "Average test loss: 0.0013858492439612746\n",
      "Epoch 119/300\n",
      "Average training loss: 0.007476731111605962\n",
      "Average test loss: 0.0013844255801911155\n",
      "Epoch 120/300\n",
      "Average training loss: 0.007476514209061861\n",
      "Average test loss: 0.0013703792541184358\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007471200388752752\n",
      "Average test loss: 0.00137409537528745\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0074627835630542705\n",
      "Average test loss: 0.0014922686584500802\n",
      "Epoch 123/300\n",
      "Average training loss: 0.00747394321527746\n",
      "Average test loss: 0.001393210726376209\n",
      "Epoch 124/300\n",
      "Average training loss: 0.007457827115224467\n",
      "Average test loss: 0.0013879300637377632\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0074506251915461485\n",
      "Average test loss: 0.0013834651923841901\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0074451919909980565\n",
      "Average test loss: 0.001369913194535507\n",
      "Epoch 127/300\n",
      "Average training loss: 0.007439850699984365\n",
      "Average test loss: 0.0013739178250026373\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007440138822214471\n",
      "Average test loss: 0.001385910134555565\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007431204435726007\n",
      "Average test loss: 0.0013779127014180025\n",
      "Epoch 130/300\n",
      "Average training loss: 0.007430048202060991\n",
      "Average test loss: 0.0013878032274337278\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007420090076410108\n",
      "Average test loss: 0.0013877634366767273\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0074245301100114985\n",
      "Average test loss: 0.0013898120364174247\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007422720268368721\n",
      "Average test loss: 0.0013693373766210345\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0074102468271222376\n",
      "Average test loss: 0.0014039937641678584\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007405317718370093\n",
      "Average test loss: 0.0014175803560970558\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007400102093815804\n",
      "Average test loss: 0.0013822812049960097\n",
      "Epoch 137/300\n",
      "Average training loss: 0.007396881182574563\n",
      "Average test loss: 0.0014082547382762035\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0073941545109781955\n",
      "Average test loss: 0.001394953152578738\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007389910628398259\n",
      "Average test loss: 0.001420322769269761\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007389444713791211\n",
      "Average test loss: 0.0014255547900684177\n",
      "Epoch 141/300\n",
      "Average training loss: 0.007380741479910083\n",
      "Average test loss: 0.0014097187731208073\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007383019954793983\n",
      "Average test loss: 0.0013902502482136092\n",
      "Epoch 143/300\n",
      "Average training loss: 0.007375635235673851\n",
      "Average test loss: 0.001373999345012837\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007368102954493629\n",
      "Average test loss: 0.0013968276877163186\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0073697127372854285\n",
      "Average test loss: 0.001394430721903013\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007360581714659929\n",
      "Average test loss: 0.0014175167950905032\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007361115568213993\n",
      "Average test loss: 0.0013834020997294122\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00734971747878525\n",
      "Average test loss: 0.001383889112725026\n",
      "Epoch 149/300\n",
      "Average training loss: 0.00735847663672434\n",
      "Average test loss: 0.0014075779556814167\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007350633026411136\n",
      "Average test loss: 0.0013961703843540616\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007336797597921557\n",
      "Average test loss: 0.001398351125833061\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007339377365178532\n",
      "Average test loss: 0.001385027831627263\n",
      "Epoch 153/300\n",
      "Average training loss: 0.00733120895218518\n",
      "Average test loss: 0.0013938923085936243\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007340496729231543\n",
      "Average test loss: 0.001391340374843114\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007331111669126484\n",
      "Average test loss: 0.0015730247464444903\n",
      "Epoch 156/300\n",
      "Average training loss: 0.007331010698858235\n",
      "Average test loss: 0.0013938789660524992\n",
      "Epoch 157/300\n",
      "Average training loss: 0.007323655466238658\n",
      "Average test loss: 0.001400944883728193\n",
      "Epoch 158/300\n",
      "Average training loss: 0.007317472285280625\n",
      "Average test loss: 0.0015431396729416317\n",
      "Epoch 159/300\n",
      "Average training loss: 0.007310944163137012\n",
      "Average test loss: 0.0014368772547071178\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00731524310219619\n",
      "Average test loss: 0.0014026593841198417\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0073148069936368205\n",
      "Average test loss: 0.0014060440639861757\n",
      "Epoch 162/300\n",
      "Average training loss: 0.007308566208100982\n",
      "Average test loss: 0.0013906589098688629\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0073064657433165444\n",
      "Average test loss: 0.0014064061872454154\n",
      "Epoch 164/300\n",
      "Average training loss: 0.007299603020151456\n",
      "Average test loss: 0.0013868219947131971\n",
      "Epoch 165/300\n",
      "Average training loss: 0.007290581403507127\n",
      "Average test loss: 0.0014083743335472213\n",
      "Epoch 166/300\n",
      "Average training loss: 0.007295986187954744\n",
      "Average test loss: 0.0013880785466689203\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00729225756890244\n",
      "Average test loss: 0.001387570790429082\n",
      "Epoch 168/300\n",
      "Average training loss: 0.007298600944379965\n",
      "Average test loss: 0.0013898473354056477\n",
      "Epoch 169/300\n",
      "Average training loss: 0.007280858320908414\n",
      "Average test loss: 0.0014222285833934115\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007277527131968074\n",
      "Average test loss: 0.0014126878168640865\n",
      "Epoch 171/300\n",
      "Average training loss: 0.007277823839336633\n",
      "Average test loss: 0.0013955284479695062\n",
      "Epoch 172/300\n",
      "Average training loss: 0.007278899754501051\n",
      "Average test loss: 0.0014262229828163981\n",
      "Epoch 173/300\n",
      "Average training loss: 0.007271307774715953\n",
      "Average test loss: 0.001415702925581071\n",
      "Epoch 174/300\n",
      "Average training loss: 0.007272636089060041\n",
      "Average test loss: 0.001414418608157171\n",
      "Epoch 175/300\n",
      "Average training loss: 0.007265083197918203\n",
      "Average test loss: 0.0014291803671254051\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007267327441109551\n",
      "Average test loss: 0.0013817513381234473\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0072967555253869954\n",
      "Average test loss: 0.0013880104026239778\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007260197549644444\n",
      "Average test loss: 0.0014257772001955245\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007261217436028851\n",
      "Average test loss: 0.001440732427769237\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007252335491279761\n",
      "Average test loss: 0.001413955950902568\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007255695864144299\n",
      "Average test loss: 0.0013978125258452363\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007240244574844837\n",
      "Average test loss: 0.001391239567556315\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007240819290694263\n",
      "Average test loss: 0.0013883552331891325\n",
      "Epoch 184/300\n",
      "Average training loss: 0.007244674398253361\n",
      "Average test loss: 0.0014002173111463587\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007242519429160489\n",
      "Average test loss: 0.0015074801240116359\n",
      "Epoch 186/300\n",
      "Average training loss: 0.007238172868473662\n",
      "Average test loss: 0.0014434049822804002\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007241892465286785\n",
      "Average test loss: 0.0014031811817031767\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007239348612311814\n",
      "Average test loss: 0.0014242531540803612\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007236438432915343\n",
      "Average test loss: 0.0013921052132629685\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007220938295953804\n",
      "Average test loss: 0.0014132412779662344\n",
      "Epoch 191/300\n",
      "Average training loss: 0.007225881370819277\n",
      "Average test loss: 0.0014206596651218004\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00721046630334523\n",
      "Average test loss: 0.001428000600180692\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007217298755215274\n",
      "Average test loss: 0.0014022016094790565\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0072146512261695335\n",
      "Average test loss: 0.0014201484138353004\n",
      "Epoch 195/300\n",
      "Average training loss: 0.007213196941134002\n",
      "Average test loss: 0.0014270046898681257\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00720449133672648\n",
      "Average test loss: 0.001422221113824182\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0072156555176609094\n",
      "Average test loss: 0.0014107248076341218\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007202872425731685\n",
      "Average test loss: 0.0014172098909815153\n",
      "Epoch 199/300\n",
      "Average training loss: 0.007206215664744377\n",
      "Average test loss: 0.0016563738121961554\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007209669404766626\n",
      "Average test loss: 0.0014199321964859135\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007193713870727354\n",
      "Average test loss: 0.0014092057133093475\n",
      "Epoch 202/300\n",
      "Average training loss: 0.007189177241176367\n",
      "Average test loss: 0.0014136026649632387\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007197099026292562\n",
      "Average test loss: 0.0014027175550452536\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007190294449528059\n",
      "Average test loss: 0.0014177792886685993\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007187104829069641\n",
      "Average test loss: 0.0014054257326448958\n",
      "Epoch 206/300\n",
      "Average training loss: 0.007189495108607742\n",
      "Average test loss: 0.001409007453566624\n",
      "Epoch 207/300\n",
      "Average training loss: 0.007192334123369721\n",
      "Average test loss: 0.0014285231536875168\n",
      "Epoch 208/300\n",
      "Average training loss: 0.007183721451709668\n",
      "Average test loss: 0.0014144995887246396\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0071809851167102656\n",
      "Average test loss: 0.0013961329380464222\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007178185941858424\n",
      "Average test loss: 0.0014123159360347522\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0071860008463263516\n",
      "Average test loss: 0.0014261488032837708\n",
      "Epoch 212/300\n",
      "Average training loss: 0.007169293318357733\n",
      "Average test loss: 0.0014326083555610642\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007179238535049889\n",
      "Average test loss: 0.0014269666418743631\n",
      "Epoch 214/300\n",
      "Average training loss: 0.007167464503811466\n",
      "Average test loss: 0.0014101413691726824\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007185095160371727\n",
      "Average test loss: 0.0014299940474124418\n",
      "Epoch 216/300\n",
      "Average training loss: 0.007159486619962586\n",
      "Average test loss: 0.0014294224637043146\n",
      "Epoch 217/300\n",
      "Average training loss: 0.007158050233705176\n",
      "Average test loss: 0.0014445966160338787\n",
      "Epoch 218/300\n",
      "Average training loss: 0.007155558740099271\n",
      "Average test loss: 0.0014039008591531052\n",
      "Epoch 219/300\n",
      "Average training loss: 0.007155976570728752\n",
      "Average test loss: 0.001426254873474439\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0071579402022891574\n",
      "Average test loss: 0.0014680338468816545\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007161632337090042\n",
      "Average test loss: 0.001414335816167295\n",
      "Epoch 222/300\n",
      "Average training loss: 0.007151386167026228\n",
      "Average test loss: 0.001418042418340014\n",
      "Epoch 223/300\n",
      "Average training loss: 0.007151028211745951\n",
      "Average test loss: 0.001683617087494996\n",
      "Epoch 224/300\n",
      "Average training loss: 0.007152093870358335\n",
      "Average test loss: 0.0014410658715706733\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0071470447642107806\n",
      "Average test loss: 0.0014145958713359303\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0071435038207305806\n",
      "Average test loss: 0.0014476577625092532\n",
      "Epoch 227/300\n",
      "Average training loss: 0.007143242566949791\n",
      "Average test loss: 0.0014782430252267254\n",
      "Epoch 228/300\n",
      "Average training loss: 0.007141159512516525\n",
      "Average test loss: 0.0014498447194281551\n",
      "Epoch 229/300\n",
      "Average training loss: 0.007141917600813839\n",
      "Average test loss: 0.0013979987127499447\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00714037330283059\n",
      "Average test loss: 0.0014538961852538503\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007135225259181526\n",
      "Average test loss: 0.0014194525172933935\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00712469522944755\n",
      "Average test loss: 0.0014291969246955382\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007125558269106679\n",
      "Average test loss: 0.0014118935390272074\n",
      "Epoch 234/300\n",
      "Average training loss: 0.007134682275354862\n",
      "Average test loss: 0.0014164322908553813\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007123387525478999\n",
      "Average test loss: 0.0014555190990989406\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0071286539236704504\n",
      "Average test loss: 0.0014112931369907327\n",
      "Epoch 237/300\n",
      "Average training loss: 0.007122535999036498\n",
      "Average test loss: 0.001464592392038968\n",
      "Epoch 238/300\n",
      "Average training loss: 0.007126140914029545\n",
      "Average test loss: 0.0014208888844069508\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00711824357468221\n",
      "Average test loss: 0.0014137941440567374\n",
      "Epoch 240/300\n",
      "Average training loss: 0.007117101427167654\n",
      "Average test loss: 0.001409167645757811\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007111778413256009\n",
      "Average test loss: 0.0014306815916465389\n",
      "Epoch 242/300\n",
      "Average training loss: 0.007110338776475853\n",
      "Average test loss: 0.001437675378492309\n",
      "Epoch 243/300\n",
      "Average training loss: 0.007112894001934263\n",
      "Average test loss: 0.0014099114109865493\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007113885606742567\n",
      "Average test loss: 0.0014258486806518502\n",
      "Epoch 245/300\n",
      "Average training loss: 0.007108821508785089\n",
      "Average test loss: 0.0014337889057480626\n",
      "Epoch 246/300\n",
      "Average training loss: 0.007112266018572781\n",
      "Average test loss: 0.001450074566528201\n",
      "Epoch 247/300\n",
      "Average training loss: 0.007113515845603413\n",
      "Average test loss: 0.0014183797475157513\n",
      "Epoch 248/300\n",
      "Average training loss: 0.007101351037207577\n",
      "Average test loss: 0.0014229690671587983\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0071028845277097494\n",
      "Average test loss: 0.001411519597025795\n",
      "Epoch 250/300\n",
      "Average training loss: 0.007104911992119418\n",
      "Average test loss: 0.001532080544779698\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007100023703028758\n",
      "Average test loss: 0.0014163746799652774\n",
      "Epoch 252/300\n",
      "Average training loss: 0.007098534735540549\n",
      "Average test loss: 0.0014373087867990963\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007087510215739409\n",
      "Average test loss: 0.0014521586892919409\n",
      "Epoch 254/300\n",
      "Average training loss: 0.007086943161570364\n",
      "Average test loss: 0.001434642895228333\n",
      "Epoch 255/300\n",
      "Average training loss: 0.007093343736810817\n",
      "Average test loss: 0.0014307968506796491\n",
      "Epoch 256/300\n",
      "Average training loss: 0.007085736116601361\n",
      "Average test loss: 0.0014319334724504087\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007084947641111083\n",
      "Average test loss: 0.001445664949197736\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007084120074080096\n",
      "Average test loss: 0.0014344624059481753\n",
      "Epoch 259/300\n",
      "Average training loss: 0.007087420831537909\n",
      "Average test loss: 0.0014276227712010344\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007081164690769381\n",
      "Average test loss: 0.001423110753401286\n",
      "Epoch 261/300\n",
      "Average training loss: 0.007084083751257923\n",
      "Average test loss: 0.0014379513793521456\n",
      "Epoch 262/300\n",
      "Average training loss: 0.007073262188997533\n",
      "Average test loss: 0.0014200584511272608\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00708007258673509\n",
      "Average test loss: 0.0014262315088676082\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007074473556131124\n",
      "Average test loss: 0.001426363883436554\n",
      "Epoch 265/300\n",
      "Average training loss: 0.007078745199160443\n",
      "Average test loss: 0.0014321492235693668\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00707642461400893\n",
      "Average test loss: 0.001421275504761272\n",
      "Epoch 267/300\n",
      "Average training loss: 0.007070316282825338\n",
      "Average test loss: 0.0014503101464878353\n",
      "Epoch 268/300\n",
      "Average training loss: 0.007063448507752684\n",
      "Average test loss: 0.0014692257491664754\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007074660624066989\n",
      "Average test loss: 0.0014236828511994745\n",
      "Epoch 270/300\n",
      "Average training loss: 0.007071659188717603\n",
      "Average test loss: 0.0014093523894747099\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007061505515128374\n",
      "Average test loss: 0.0014279063032526108\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007067019081364075\n",
      "Average test loss: 0.0014511777630282774\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00706999625141422\n",
      "Average test loss: 0.001410767642625918\n",
      "Epoch 274/300\n",
      "Average training loss: 0.00705660771454374\n",
      "Average test loss: 0.0014430453343730835\n",
      "Epoch 275/300\n",
      "Average training loss: 0.007057179697271851\n",
      "Average test loss: 0.0014016894041043188\n",
      "Epoch 276/300\n",
      "Average training loss: 0.007057985636923048\n",
      "Average test loss: 0.001474300570682519\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007054877443859974\n",
      "Average test loss: 0.0014339681058708165\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0070561812482774254\n",
      "Average test loss: 0.0014148899612741338\n",
      "Epoch 279/300\n",
      "Average training loss: 0.007046652796781725\n",
      "Average test loss: 0.0014240396297019388\n",
      "Epoch 280/300\n",
      "Average training loss: 0.007047705515805218\n",
      "Average test loss: 0.00142090627623515\n",
      "Epoch 281/300\n",
      "Average training loss: 0.007048118083841271\n",
      "Average test loss: 0.0014725511891560423\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0070511692832741476\n",
      "Average test loss: 0.0014970313078827328\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00705370684630341\n",
      "Average test loss: 0.0014578321653728684\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0070454875806139575\n",
      "Average test loss: 0.0014531920035369694\n",
      "Epoch 285/300\n",
      "Average training loss: 0.007045208587828609\n",
      "Average test loss: 0.00142483502243542\n",
      "Epoch 286/300\n",
      "Average training loss: 0.007041162131975094\n",
      "Average test loss: 0.0014304152270779014\n",
      "Epoch 287/300\n",
      "Average training loss: 0.007043426979747083\n",
      "Average test loss: 0.0014468288871770103\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007029077306389808\n",
      "Average test loss: 0.001443668736765782\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007043894091828002\n",
      "Average test loss: 0.0014848008413488665\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007038287985449036\n",
      "Average test loss: 0.0014659250618682968\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007037405194921626\n",
      "Average test loss: 0.0014186746253528528\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0070354323486487074\n",
      "Average test loss: 0.0014584900141797132\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0070312794579399955\n",
      "Average test loss: 0.0014595705570860042\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007033078812476661\n",
      "Average test loss: 0.0014375586224099001\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007034777724494537\n",
      "Average test loss: 0.0014194119906880789\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007033226738373439\n",
      "Average test loss: 0.0014551276997145678\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007022382028814819\n",
      "Average test loss: 0.001496208115066919\n",
      "Epoch 298/300\n",
      "Average training loss: 0.007023431254757775\n",
      "Average test loss: 0.0014553563580330874\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00703230661029617\n",
      "Average test loss: 0.0015333958393376735\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007022809729393986\n",
      "Average test loss: 0.0014221723902867072\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_64_Depth5/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.46\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.62\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 32.50\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 33.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.51\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.8947471406194899\n",
      "Average test loss: 0.005145649433549907\n",
      "Epoch 2/300\n",
      "Average training loss: 0.18164061706595952\n",
      "Average test loss: 0.004501519038238459\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12133590792947345\n",
      "Average test loss: 0.0043025948032736775\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0992928966416253\n",
      "Average test loss: 0.004211789356958535\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08696916284163793\n",
      "Average test loss: 0.00415490544297629\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07946715786059698\n",
      "Average test loss: 0.004118487681779597\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0745587374832895\n",
      "Average test loss: 0.004085693310118384\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07116308510303497\n",
      "Average test loss: 0.00404561313158936\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06867491979731453\n",
      "Average test loss: 0.004014316964894533\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06679546452230878\n",
      "Average test loss: 0.0039817216195580036\n",
      "Epoch 11/300\n",
      "Average training loss: 0.065135813275973\n",
      "Average test loss: 0.003968925020553999\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06380778377917078\n",
      "Average test loss: 0.003941506283150779\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06277141003145112\n",
      "Average test loss: 0.003930970750749111\n",
      "Epoch 14/300\n",
      "Average training loss: 0.062082936144537394\n",
      "Average test loss: 0.0039051621589395735\n",
      "Epoch 15/300\n",
      "Average training loss: 0.061490655250019496\n",
      "Average test loss: 0.0038902450770967535\n",
      "Epoch 16/300\n",
      "Average training loss: 0.061018040696779886\n",
      "Average test loss: 0.0038970802722291813\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06062474371989568\n",
      "Average test loss: 0.0038851872115499442\n",
      "Epoch 18/300\n",
      "Average training loss: 0.060295537230041295\n",
      "Average test loss: 0.003958333452542623\n",
      "Epoch 19/300\n",
      "Average training loss: 0.059977038188113105\n",
      "Average test loss: 0.0038560208982477587\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0597066866060098\n",
      "Average test loss: 0.0038341753478679393\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05943922090530396\n",
      "Average test loss: 0.0038156333135233983\n",
      "Epoch 22/300\n",
      "Average training loss: 0.059189357062180835\n",
      "Average test loss: 0.0038139858668049175\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0589634574883514\n",
      "Average test loss: 0.0038158461857173177\n",
      "Epoch 24/300\n",
      "Average training loss: 0.058748873247040645\n",
      "Average test loss: 0.003799687574307124\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05853739461965031\n",
      "Average test loss: 0.003778365195625358\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05833806573682361\n",
      "Average test loss: 0.003765947982668877\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05816511234972212\n",
      "Average test loss: 0.0037641443721950054\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05801699427763621\n",
      "Average test loss: 0.0037653640595575175\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05783564415574074\n",
      "Average test loss: 0.00375009977672663\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05768454659316275\n",
      "Average test loss: 0.0037686664457950328\n",
      "Epoch 31/300\n",
      "Average training loss: 0.057545190595918234\n",
      "Average test loss: 0.0037321644922097526\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05741043806738324\n",
      "Average test loss: 0.0037573695985807314\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05723376198940807\n",
      "Average test loss: 0.00372382138007217\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05710516056418419\n",
      "Average test loss: 0.0037125103059742186\n",
      "Epoch 35/300\n",
      "Average training loss: 0.056993524177206885\n",
      "Average test loss: 0.0037461976644893487\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05681909829378128\n",
      "Average test loss: 0.0037176820143229434\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05671297312113974\n",
      "Average test loss: 0.0037183290548208686\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05662789763013522\n",
      "Average test loss: 0.003704864531962408\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0565161694586277\n",
      "Average test loss: 0.0036882799375388357\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05639819555150138\n",
      "Average test loss: 0.0036949867842098077\n",
      "Epoch 41/300\n",
      "Average training loss: 0.056265216847260796\n",
      "Average test loss: 0.003690686917139424\n",
      "Epoch 42/300\n",
      "Average training loss: 0.056200338001052536\n",
      "Average test loss: 0.003684257066912121\n",
      "Epoch 43/300\n",
      "Average training loss: 0.056085154467158845\n",
      "Average test loss: 0.003691118243460854\n",
      "Epoch 44/300\n",
      "Average training loss: 0.055994321674108505\n",
      "Average test loss: 0.0036807967316773204\n",
      "Epoch 45/300\n",
      "Average training loss: 0.055899043367968665\n",
      "Average test loss: 0.003699716532809867\n",
      "Epoch 46/300\n",
      "Average training loss: 0.055814052681128185\n",
      "Average test loss: 0.0036665186155587434\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05573631347219149\n",
      "Average test loss: 0.003691039516694016\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05568432283070352\n",
      "Average test loss: 0.0036827895583377945\n",
      "Epoch 49/300\n",
      "Average training loss: 0.055562987479898664\n",
      "Average test loss: 0.0036691199731495646\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05550299140148693\n",
      "Average test loss: 0.003689049870396654\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05543051777945624\n",
      "Average test loss: 0.003657051860872242\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05536998125248485\n",
      "Average test loss: 0.003665739289381438\n",
      "Epoch 53/300\n",
      "Average training loss: 0.055275916394260195\n",
      "Average test loss: 0.0037051466612352266\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05528052617443933\n",
      "Average test loss: 0.003656553295751413\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05514796087476942\n",
      "Average test loss: 0.0036537802304244704\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05509154827396075\n",
      "Average test loss: 0.0036610426778594654\n",
      "Epoch 57/300\n",
      "Average training loss: 0.055001781033145056\n",
      "Average test loss: 0.0036611849984361064\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05496652705139584\n",
      "Average test loss: 0.003653118666675356\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05486505352126227\n",
      "Average test loss: 0.0036743700568460757\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05483007061812613\n",
      "Average test loss: 0.0036902158440401157\n",
      "Epoch 61/300\n",
      "Average training loss: 0.054796635654237535\n",
      "Average test loss: 0.003649898605835107\n",
      "Epoch 62/300\n",
      "Average training loss: 0.054694257944822314\n",
      "Average test loss: 0.003657674865797162\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0546487527721458\n",
      "Average test loss: 0.0036653323724038073\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05457607873280843\n",
      "Average test loss: 0.0036486771168808144\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05451539961828126\n",
      "Average test loss: 0.0036642712015244696\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05444893492923843\n",
      "Average test loss: 0.003687863210630086\n",
      "Epoch 67/300\n",
      "Average training loss: 0.054375375284088985\n",
      "Average test loss: 0.0037163949815763367\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05434270960920387\n",
      "Average test loss: 0.0036553075599173703\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05427942498524984\n",
      "Average test loss: 0.003662825670093298\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05418175544010268\n",
      "Average test loss: 0.0036617824503531056\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05416049379441473\n",
      "Average test loss: 0.003658242555542125\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05410625759098265\n",
      "Average test loss: 0.00366564436422454\n",
      "Epoch 73/300\n",
      "Average training loss: 0.054009091741508905\n",
      "Average test loss: 0.0036572384635607403\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05394889058338271\n",
      "Average test loss: 0.0036829664872752295\n",
      "Epoch 75/300\n",
      "Average training loss: 0.053858999070194036\n",
      "Average test loss: 0.0036600373726752068\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05382371974984805\n",
      "Average test loss: 0.0036572045754227375\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05373963899744882\n",
      "Average test loss: 0.003679469005929099\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05371209103862445\n",
      "Average test loss: 0.0036603839544372425\n",
      "Epoch 79/300\n",
      "Average training loss: 0.053653302086724176\n",
      "Average test loss: 0.003665781339009603\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05355229182044665\n",
      "Average test loss: 0.0036972493011918333\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05348490036527316\n",
      "Average test loss: 0.0036932187792327667\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05344976244370143\n",
      "Average test loss: 0.00369143917846183\n",
      "Epoch 83/300\n",
      "Average training loss: 0.053372152401341334\n",
      "Average test loss: 0.003680600255727768\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05333354317479663\n",
      "Average test loss: 0.003673870181457864\n",
      "Epoch 85/300\n",
      "Average training loss: 0.053232212308380336\n",
      "Average test loss: 0.0036753412168473005\n",
      "Epoch 86/300\n",
      "Average training loss: 0.053209538105461335\n",
      "Average test loss: 0.0037085791441301506\n",
      "Epoch 87/300\n",
      "Average training loss: 0.053126094516780645\n",
      "Average test loss: 0.0036840314269065855\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05303420522477892\n",
      "Average test loss: 0.0037144307117495273\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05303213351468245\n",
      "Average test loss: 0.003727005826930205\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05292396526866489\n",
      "Average test loss: 0.0036759921490318245\n",
      "Epoch 91/300\n",
      "Average training loss: 0.052902959641483095\n",
      "Average test loss: 0.0036912199190507334\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05283237586915493\n",
      "Average test loss: 0.0037064056218498284\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05272296522392167\n",
      "Average test loss: 0.0036823999111851055\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05266314484013451\n",
      "Average test loss: 0.0036934739570650787\n",
      "Epoch 95/300\n",
      "Average training loss: 0.052644236548079384\n",
      "Average test loss: 0.0037118330132216216\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05255022470487489\n",
      "Average test loss: 0.00368152200202975\n",
      "Epoch 97/300\n",
      "Average training loss: 0.052519389198886025\n",
      "Average test loss: 0.0037151534060637158\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05239683100912306\n",
      "Average test loss: 0.0036806658978263537\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05238998552825716\n",
      "Average test loss: 0.0037308324252565703\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05235380421082179\n",
      "Average test loss: 0.0037447004802525045\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05223105729950799\n",
      "Average test loss: 0.003773547286995583\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05226802532871564\n",
      "Average test loss: 0.0037698532862381804\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05211977145075798\n",
      "Average test loss: 0.0037263454066382515\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05211153638031748\n",
      "Average test loss: 0.003753561596489615\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05203282223145167\n",
      "Average test loss: 0.0037519289987782635\n",
      "Epoch 106/300\n",
      "Average training loss: 0.052026624689499534\n",
      "Average test loss: 0.0037162704223559963\n",
      "Epoch 107/300\n",
      "Average training loss: 0.051907689061429764\n",
      "Average test loss: 0.003709037859406736\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05190205505490303\n",
      "Average test loss: 0.003924451133236289\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05183300064504147\n",
      "Average test loss: 0.0037973900551183356\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05176744426290194\n",
      "Average test loss: 0.0037514947085744805\n",
      "Epoch 111/300\n",
      "Average training loss: 0.051685954784353574\n",
      "Average test loss: 0.003810699711036351\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05166692986090978\n",
      "Average test loss: 0.0037216823564635384\n",
      "Epoch 113/300\n",
      "Average training loss: 0.051564956396818164\n",
      "Average test loss: 0.003816303844874104\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05160447432928615\n",
      "Average test loss: 0.0038015902677757873\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05144880935880873\n",
      "Average test loss: 0.003913726000529197\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05144177007675171\n",
      "Average test loss: 0.003766189160860247\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05137540185451508\n",
      "Average test loss: 0.0037341235489067102\n",
      "Epoch 118/300\n",
      "Average training loss: 0.051323676652378504\n",
      "Average test loss: 0.003832635135907266\n",
      "Epoch 119/300\n",
      "Average training loss: 0.051272559695773656\n",
      "Average test loss: 0.003737740989981426\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05126177220543226\n",
      "Average test loss: 0.0037567591650618445\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05118790819578701\n",
      "Average test loss: 0.003806350103786422\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05118742394778464\n",
      "Average test loss: 0.003799758935968081\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05112074398663309\n",
      "Average test loss: 0.0037852310643841825\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05104091282354461\n",
      "Average test loss: 0.0037904078492687807\n",
      "Epoch 125/300\n",
      "Average training loss: 0.050989647060632706\n",
      "Average test loss: 0.0037878825424446\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05095327328973346\n",
      "Average test loss: 0.003815579086956051\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05089426087339719\n",
      "Average test loss: 0.0038233798150387075\n",
      "Epoch 128/300\n",
      "Average training loss: 0.050877897777491146\n",
      "Average test loss: 0.003908152522519231\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05083456614613533\n",
      "Average test loss: 0.0037333361212578083\n",
      "Epoch 130/300\n",
      "Average training loss: 0.050759965287314524\n",
      "Average test loss: 0.003757020921756824\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05076035899917285\n",
      "Average test loss: 0.0037730104389290015\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05071948257088661\n",
      "Average test loss: 0.0038269633219266933\n",
      "Epoch 133/300\n",
      "Average training loss: 0.050660449077685674\n",
      "Average test loss: 0.00375074206209845\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05062076890799734\n",
      "Average test loss: 0.0037704884463714227\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05052631648712688\n",
      "Average test loss: 0.0038127823792811896\n",
      "Epoch 136/300\n",
      "Average training loss: 0.050480913990073734\n",
      "Average test loss: 0.003818144300331672\n",
      "Epoch 137/300\n",
      "Average training loss: 0.050460807191001045\n",
      "Average test loss: 0.0037736592116869158\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05046500981516308\n",
      "Average test loss: 0.0037898790418273874\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05038057353099187\n",
      "Average test loss: 0.00379696563941737\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05039468354317877\n",
      "Average test loss: 0.003759290387440059\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05033050216237704\n",
      "Average test loss: 0.0037531119961705475\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05024230515956879\n",
      "Average test loss: 0.0038480127815985017\n",
      "Epoch 143/300\n",
      "Average training loss: 0.050232188847329884\n",
      "Average test loss: 0.0037782338122940725\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05019480601284239\n",
      "Average test loss: 0.0038013763069692586\n",
      "Epoch 145/300\n",
      "Average training loss: 0.050146862733695244\n",
      "Average test loss: 0.0038812102789266244\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05013942977786064\n",
      "Average test loss: 0.00375612704642117\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05008639800548553\n",
      "Average test loss: 0.003790580659276909\n",
      "Epoch 148/300\n",
      "Average training loss: 0.050034732189443376\n",
      "Average test loss: 0.003782276266978847\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05000249261657397\n",
      "Average test loss: 0.0038170263475428025\n",
      "Epoch 150/300\n",
      "Average training loss: 0.050020105169879064\n",
      "Average test loss: 0.003810744980764058\n",
      "Epoch 151/300\n",
      "Average training loss: 0.049915897021691004\n",
      "Average test loss: 0.0038516180531846154\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04990063701404466\n",
      "Average test loss: 0.0038581809091071287\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04985711142089632\n",
      "Average test loss: 0.0038320451641662254\n",
      "Epoch 154/300\n",
      "Average training loss: 0.049836435003413095\n",
      "Average test loss: 0.0038547600677443874\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04977102834979693\n",
      "Average test loss: 0.0037865985576063395\n",
      "Epoch 156/300\n",
      "Average training loss: 0.049762125250365996\n",
      "Average test loss: 0.0038380841449317006\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04973074768980344\n",
      "Average test loss: 0.003959818290132615\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04967416258321868\n",
      "Average test loss: 0.003997605406161811\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04964271253181828\n",
      "Average test loss: 0.0038080034827192625\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04963195655081007\n",
      "Average test loss: 0.0038535004560318257\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04959193197555012\n",
      "Average test loss: 0.00506322435496582\n",
      "Epoch 162/300\n",
      "Average training loss: 0.049553788198365104\n",
      "Average test loss: 0.0038510997626516556\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04949332219362259\n",
      "Average test loss: 0.0038262377907004623\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04959123041563564\n",
      "Average test loss: 0.003831877431521813\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04944765227701929\n",
      "Average test loss: 0.004232265726973613\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04938558387756348\n",
      "Average test loss: 0.0038064267966482373\n",
      "Epoch 167/300\n",
      "Average training loss: 0.049448247753911545\n",
      "Average test loss: 0.0038382177953090934\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04940476817554898\n",
      "Average test loss: 0.0038682458684262304\n",
      "Epoch 169/300\n",
      "Average training loss: 0.049350967158873875\n",
      "Average test loss: 0.003792427345075541\n",
      "Epoch 170/300\n",
      "Average training loss: 0.049260074069102606\n",
      "Average test loss: 0.0038383379661374624\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04928321617510584\n",
      "Average test loss: 0.003811512396981319\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04926167930828201\n",
      "Average test loss: 0.003811490160516567\n",
      "Epoch 173/300\n",
      "Average training loss: 0.049263057519992194\n",
      "Average test loss: 0.0037776676923450495\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04919199766384231\n",
      "Average test loss: 0.003825591940225826\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04915913096732563\n",
      "Average test loss: 0.0038157012280490665\n",
      "Epoch 176/300\n",
      "Average training loss: 0.049118397139840654\n",
      "Average test loss: 0.0038575032506552006\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0491154531273577\n",
      "Average test loss: 0.0038481403396775325\n",
      "Epoch 178/300\n",
      "Average training loss: 0.049082815183533565\n",
      "Average test loss: 0.003872390934576591\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04905102758275138\n",
      "Average test loss: 0.0038133623864915634\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04908725201090177\n",
      "Average test loss: 0.003917653000603119\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04901850785149468\n",
      "Average test loss: 0.0038188661636991634\n",
      "Epoch 182/300\n",
      "Average training loss: 0.048967892517646154\n",
      "Average test loss: 0.003835481471485562\n",
      "Epoch 183/300\n",
      "Average training loss: 0.048903660992781324\n",
      "Average test loss: 0.003829666496978866\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04889804481963317\n",
      "Average test loss: 0.003919054023093648\n",
      "Epoch 185/300\n",
      "Average training loss: 0.048873360249731275\n",
      "Average test loss: 0.0038273577696333327\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04888171938723988\n",
      "Average test loss: 0.0038414451082547504\n",
      "Epoch 187/300\n",
      "Average training loss: 0.048861436552471584\n",
      "Average test loss: 0.004009388720409738\n",
      "Epoch 188/300\n",
      "Average training loss: 0.048817735595835576\n",
      "Average test loss: 0.0038406949082596432\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04883813989162445\n",
      "Average test loss: 0.0038438318971958426\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04881887591547436\n",
      "Average test loss: 0.0038729080342584186\n",
      "Epoch 191/300\n",
      "Average training loss: 0.048752992934650845\n",
      "Average test loss: 0.00383874276590844\n",
      "Epoch 192/300\n",
      "Average training loss: 0.048755836864312486\n",
      "Average test loss: 0.0039268023864262635\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04870407926705148\n",
      "Average test loss: 0.0039712009777625405\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0486740976439582\n",
      "Average test loss: 0.003915036997654372\n",
      "Epoch 195/300\n",
      "Average training loss: 0.048596690439515644\n",
      "Average test loss: 0.003830045045663913\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0486250535580847\n",
      "Average test loss: 0.003934895878864659\n",
      "Epoch 197/300\n",
      "Average training loss: 0.048634390546215905\n",
      "Average test loss: 0.003913889521112044\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04855623777045144\n",
      "Average test loss: 0.0039083116315305234\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04850076545940505\n",
      "Average test loss: 0.0038828246750765376\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04855675930447049\n",
      "Average test loss: 0.0038116766119168865\n",
      "Epoch 201/300\n",
      "Average training loss: 0.048558326883448494\n",
      "Average test loss: 0.0039353978087504704\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04846766344706217\n",
      "Average test loss: 0.0038612727779481146\n",
      "Epoch 203/300\n",
      "Average training loss: 0.048459173154499796\n",
      "Average test loss: 0.0038943538680258722\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04844104708234469\n",
      "Average test loss: 0.003863979479504956\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04837420893046591\n",
      "Average test loss: 0.003907363730586237\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04839176071683566\n",
      "Average test loss: 0.0038886478833026357\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04835135934750239\n",
      "Average test loss: 0.0038860122580081226\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04833946062790023\n",
      "Average test loss: 0.0038195652990705435\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04830437034699652\n",
      "Average test loss: 0.0038992036022245883\n",
      "Epoch 210/300\n",
      "Average training loss: 0.048329324818319745\n",
      "Average test loss: 0.003864386561430163\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04829082298941082\n",
      "Average test loss: 0.003855930678339468\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04824247563547558\n",
      "Average test loss: 0.0039046429068677955\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04822965722282727\n",
      "Average test loss: 0.003995484285470512\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04820205711987283\n",
      "Average test loss: 0.0038689711389856205\n",
      "Epoch 215/300\n",
      "Average training loss: 0.048157999065187244\n",
      "Average test loss: 0.003930949922651052\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04820538941356871\n",
      "Average test loss: 0.0038538952682995132\n",
      "Epoch 217/300\n",
      "Average training loss: 0.048175979018211364\n",
      "Average test loss: 0.0038675064425915478\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04812274813320901\n",
      "Average test loss: 0.0038897924441844223\n",
      "Epoch 219/300\n",
      "Average training loss: 0.048142973519033856\n",
      "Average test loss: 0.0038898135941061707\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04807840105891228\n",
      "Average test loss: 0.003805488714327415\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04813318879405658\n",
      "Average test loss: 0.0038946068050960698\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04812347568074862\n",
      "Average test loss: 0.003921911633676953\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04813627175821198\n",
      "Average test loss: 0.003852623705855674\n",
      "Epoch 224/300\n",
      "Average training loss: 0.047999644769562616\n",
      "Average test loss: 0.003833651851034827\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04801284225781759\n",
      "Average test loss: 0.0038873260869748062\n",
      "Epoch 226/300\n",
      "Average training loss: 0.047953328271706896\n",
      "Average test loss: 0.0039918202594336535\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04794383197360569\n",
      "Average test loss: 0.003904386083285014\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04792060533497069\n",
      "Average test loss: 0.0039263650857739975\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04796043436394797\n",
      "Average test loss: 0.003953604299782051\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04791373420423931\n",
      "Average test loss: 0.003809506062625183\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04786728380786048\n",
      "Average test loss: 0.003939587105272545\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04792963719367981\n",
      "Average test loss: 0.00394061794401043\n",
      "Epoch 233/300\n",
      "Average training loss: 0.047863115136822065\n",
      "Average test loss: 0.003968566366781791\n",
      "Epoch 234/300\n",
      "Average training loss: 0.047811380714178085\n",
      "Average test loss: 0.003917053726398283\n",
      "Epoch 235/300\n",
      "Average training loss: 0.047814478798045054\n",
      "Average test loss: 0.0038754362045890756\n",
      "Epoch 236/300\n",
      "Average training loss: 0.047809500124719406\n",
      "Average test loss: 0.0039580686005453265\n",
      "Epoch 237/300\n",
      "Average training loss: 0.047755304909414716\n",
      "Average test loss: 0.0038861730924497047\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04774750755892859\n",
      "Average test loss: 0.003870784592297342\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04778142318129539\n",
      "Average test loss: 0.003929345326705112\n",
      "Epoch 240/300\n",
      "Average training loss: 0.047684472620487216\n",
      "Average test loss: 0.003931698017029299\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04772404811117384\n",
      "Average test loss: 0.004012531898087926\n",
      "Epoch 242/300\n",
      "Average training loss: 0.047719592437148094\n",
      "Average test loss: 0.003914076316687796\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04766263315412733\n",
      "Average test loss: 0.004005554328569107\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04765722645984755\n",
      "Average test loss: 0.00387119295510153\n",
      "Epoch 245/300\n",
      "Average training loss: 0.047674406068192586\n",
      "Average test loss: 0.003885059588485294\n",
      "Epoch 246/300\n",
      "Average training loss: 0.047674196716811924\n",
      "Average test loss: 0.003861565602736341\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04758700581722789\n",
      "Average test loss: 0.004142476625740528\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04757896920376354\n",
      "Average test loss: 0.00395308700348768\n",
      "Epoch 249/300\n",
      "Average training loss: 0.047611857606305014\n",
      "Average test loss: 0.003834760534680552\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04756110506256422\n",
      "Average test loss: 0.003907383005031281\n",
      "Epoch 251/300\n",
      "Average training loss: 0.047509889082776174\n",
      "Average test loss: 0.003913597289058897\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04753175405661265\n",
      "Average test loss: 0.003971717024428977\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04755863523814413\n",
      "Average test loss: 0.0039097698332948815\n",
      "Epoch 254/300\n",
      "Average training loss: 0.047517306605974835\n",
      "Average test loss: 0.004001378626459174\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04747293558716774\n",
      "Average test loss: 0.003949845964295997\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04746701021326913\n",
      "Average test loss: 0.00384216899300615\n",
      "Epoch 257/300\n",
      "Average training loss: 0.047439559655057056\n",
      "Average test loss: 0.003973417679882712\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04740860849287775\n",
      "Average test loss: 0.003967652760446072\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0474090872572528\n",
      "Average test loss: 0.003910435937138067\n",
      "Epoch 260/300\n",
      "Average training loss: 0.047382944570647345\n",
      "Average test loss: 0.003958131144651108\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04737758235798942\n",
      "Average test loss: 0.00393995684840613\n",
      "Epoch 262/300\n",
      "Average training loss: 0.047370972343617014\n",
      "Average test loss: 0.0039915901244514515\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04736954184704357\n",
      "Average test loss: 0.0040112136256777575\n",
      "Epoch 264/300\n",
      "Average training loss: 0.047316461410787373\n",
      "Average test loss: 0.003909196241448323\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04736341598298815\n",
      "Average test loss: 0.003901893892842862\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04728670745425754\n",
      "Average test loss: 0.0038555112584597534\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04733581462502479\n",
      "Average test loss: 0.003970464603561494\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04729273306992319\n",
      "Average test loss: 0.004026498921008574\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04724035205112563\n",
      "Average test loss: 0.004028352228717671\n",
      "Epoch 270/300\n",
      "Average training loss: 0.047226398785909014\n",
      "Average test loss: 0.004035256332821317\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04725837054517534\n",
      "Average test loss: 0.0038369598624606927\n",
      "Epoch 272/300\n",
      "Average training loss: 0.047224632736709383\n",
      "Average test loss: 0.003892425354777111\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04722064289781782\n",
      "Average test loss: 0.003941175350505445\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04722029147876634\n",
      "Average test loss: 0.003986842764334546\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04719212746620178\n",
      "Average test loss: 0.003933862858762344\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04717231916719013\n",
      "Average test loss: 0.004019504698407319\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04716698914104038\n",
      "Average test loss: 0.003917685562951697\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04712184373206563\n",
      "Average test loss: 0.003955731544229719\n",
      "Epoch 279/300\n",
      "Average training loss: 0.047157113240824805\n",
      "Average test loss: 0.00394673340746926\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04709782913327217\n",
      "Average test loss: 0.003915917036434014\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04705117244521777\n",
      "Average test loss: 0.0039187898757970996\n",
      "Epoch 282/300\n",
      "Average training loss: 0.047116937312814924\n",
      "Average test loss: 0.003863545476148526\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04711589687400394\n",
      "Average test loss: 0.003997495796531439\n",
      "Epoch 284/300\n",
      "Average training loss: 0.047065691058834395\n",
      "Average test loss: 0.004157760781132513\n",
      "Epoch 285/300\n",
      "Average training loss: 0.047043371165792144\n",
      "Average test loss: 0.0039202139356897935\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04703116436799367\n",
      "Average test loss: 0.003947838916132847\n",
      "Epoch 287/300\n",
      "Average training loss: 0.047013748112652035\n",
      "Average test loss: 0.003987916102011999\n",
      "Epoch 288/300\n",
      "Average training loss: 0.046962090952528845\n",
      "Average test loss: 0.0039835962475174005\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04701536714037259\n",
      "Average test loss: 0.003958618559771114\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04696691385242674\n",
      "Average test loss: 0.00401390971036421\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04696894798676173\n",
      "Average test loss: 0.003900566922914651\n",
      "Epoch 292/300\n",
      "Average training loss: 0.046959842132197484\n",
      "Average test loss: 0.0039132731238173115\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04693620185388459\n",
      "Average test loss: 0.003937261006484429\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04690093958708975\n",
      "Average test loss: 0.0039891186137166286\n",
      "Epoch 295/300\n",
      "Average training loss: 0.046904794971148175\n",
      "Average test loss: 0.00393645445505778\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0469239050646623\n",
      "Average test loss: 0.003971848226876722\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04693235668871138\n",
      "Average test loss: 0.004052732904959056\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04683716267678473\n",
      "Average test loss: 0.003930417257878516\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04688315911425484\n",
      "Average test loss: 0.003907291711204582\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04683748271067937\n",
      "Average test loss: 0.003888735885421435\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.821088280333413\n",
      "Average test loss: 0.004544565055519342\n",
      "Epoch 2/300\n",
      "Average training loss: 0.20126138366593255\n",
      "Average test loss: 0.004127030832899941\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1317957185043229\n",
      "Average test loss: 0.0038590599255015454\n",
      "Epoch 4/300\n",
      "Average training loss: 0.10312322010596593\n",
      "Average test loss: 0.003716273396793339\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0878097653388977\n",
      "Average test loss: 0.0035186871443357733\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07825277990102768\n",
      "Average test loss: 0.003423745021637943\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07127148044109345\n",
      "Average test loss: 0.003356782254245546\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06626848161882824\n",
      "Average test loss: 0.0032928874175995586\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06248635247349739\n",
      "Average test loss: 0.0032648960436797805\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05946152365869946\n",
      "Average test loss: 0.003212809512598647\n",
      "Epoch 11/300\n",
      "Average training loss: 0.056986718376477556\n",
      "Average test loss: 0.003142713763233688\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05469973759187592\n",
      "Average test loss: 0.003420698634866211\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05304521527224117\n",
      "Average test loss: 0.003054009040403697\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05181081287397279\n",
      "Average test loss: 0.0030713122824413907\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05083015818728341\n",
      "Average test loss: 0.0029729194208565684\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04999321750468678\n",
      "Average test loss: 0.0030098092969920902\n",
      "Epoch 17/300\n",
      "Average training loss: 0.049221929523679944\n",
      "Average test loss: 0.0028751956120961243\n",
      "Epoch 18/300\n",
      "Average training loss: 0.048596985277202395\n",
      "Average test loss: 0.0028406912547846636\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04796090874738163\n",
      "Average test loss: 0.002826698593588339\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04739738519986471\n",
      "Average test loss: 0.0028049679957330227\n",
      "Epoch 21/300\n",
      "Average training loss: 0.046874107182025906\n",
      "Average test loss: 0.0027851618524226875\n",
      "Epoch 22/300\n",
      "Average training loss: 0.046357972241110275\n",
      "Average test loss: 0.0028308517920474213\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04592408612370491\n",
      "Average test loss: 0.0027439617628438606\n",
      "Epoch 24/300\n",
      "Average training loss: 0.045469061454137163\n",
      "Average test loss: 0.002744273143510024\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04507090832127465\n",
      "Average test loss: 0.002726383387214608\n",
      "Epoch 26/300\n",
      "Average training loss: 0.044643420833680364\n",
      "Average test loss: 0.002715991445299652\n",
      "Epoch 27/300\n",
      "Average training loss: 0.044311169776651595\n",
      "Average test loss: 0.0026716446175964344\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04393616846203804\n",
      "Average test loss: 0.002672434416703052\n",
      "Epoch 29/300\n",
      "Average training loss: 0.043651990410354405\n",
      "Average test loss: 0.0026904859878122806\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04332164987921715\n",
      "Average test loss: 0.0026442512383477554\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04300412784020106\n",
      "Average test loss: 0.002627962400515874\n",
      "Epoch 32/300\n",
      "Average training loss: 0.042781690663761564\n",
      "Average test loss: 0.002615198957009448\n",
      "Epoch 33/300\n",
      "Average training loss: 0.042422539207670425\n",
      "Average test loss: 0.002613701326151689\n",
      "Epoch 34/300\n",
      "Average training loss: 0.042231402787897325\n",
      "Average test loss: 0.0026059720158163042\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04192945889963044\n",
      "Average test loss: 0.0026159524356739387\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04170133814877934\n",
      "Average test loss: 0.002620379861858156\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04149953215320905\n",
      "Average test loss: 0.0025868523036026293\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04128431981967555\n",
      "Average test loss: 0.002604262997706731\n",
      "Epoch 39/300\n",
      "Average training loss: 0.041103672963049676\n",
      "Average test loss: 0.002643028506077826\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04125497865014606\n",
      "Average test loss: 0.0025611514792674116\n",
      "Epoch 41/300\n",
      "Average training loss: 0.040824294676383335\n",
      "Average test loss: 0.002547888756626182\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0405962688393063\n",
      "Average test loss: 0.002566710699142681\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04048847605784734\n",
      "Average test loss: 0.00255906873755157\n",
      "Epoch 44/300\n",
      "Average training loss: 0.040339549496769905\n",
      "Average test loss: 0.0025524609538002145\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04015191508332888\n",
      "Average test loss: 0.002548397411695785\n",
      "Epoch 46/300\n",
      "Average training loss: 0.040013240188360215\n",
      "Average test loss: 0.0025364944990724325\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03991251574787829\n",
      "Average test loss: 0.0025444109268072577\n",
      "Epoch 48/300\n",
      "Average training loss: 0.039717100554042394\n",
      "Average test loss: 0.0025361151569005514\n",
      "Epoch 49/300\n",
      "Average training loss: 0.039632624275154536\n",
      "Average test loss: 0.002535926827746961\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03951028360426426\n",
      "Average test loss: 0.0025561367600328393\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0394115125603146\n",
      "Average test loss: 0.002558899260333015\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03929426564441787\n",
      "Average test loss: 0.0025448828665539623\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03917729465497865\n",
      "Average test loss: 0.002515184319474631\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03906800241933928\n",
      "Average test loss: 0.0025218230372087824\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03894730931686031\n",
      "Average test loss: 0.0025098327489362824\n",
      "Epoch 56/300\n",
      "Average training loss: 0.038851245289047556\n",
      "Average test loss: 0.002519358437301384\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03872440391116672\n",
      "Average test loss: 0.0026102206634564533\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03876298780242602\n",
      "Average test loss: 0.002564180458792382\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03859759947657585\n",
      "Average test loss: 0.00253200321437584\n",
      "Epoch 60/300\n",
      "Average training loss: 0.038477744340896604\n",
      "Average test loss: 0.002525622764395343\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03838118659125434\n",
      "Average test loss: 0.002515099679844247\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03829102749956979\n",
      "Average test loss: 0.0025231834962550137\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03818121422992812\n",
      "Average test loss: 0.0025323984447038835\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03806600151459376\n",
      "Average test loss: 0.00252341693908804\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03806526063548194\n",
      "Average test loss: 0.0025357945553130573\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0379655032257239\n",
      "Average test loss: 0.002522393668484357\n",
      "Epoch 67/300\n",
      "Average training loss: 0.037804577305912974\n",
      "Average test loss: 0.0025080632039656244\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03772279345492522\n",
      "Average test loss: 0.002528137956849403\n",
      "Epoch 69/300\n",
      "Average training loss: 0.037676667498217686\n",
      "Average test loss: 0.0025185115952044727\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03759709488021003\n",
      "Average test loss: 0.002584781364020374\n",
      "Epoch 71/300\n",
      "Average training loss: 0.037459708069761594\n",
      "Average test loss: 0.002565080819444524\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03737795861396525\n",
      "Average test loss: 0.0025502572485970126\n",
      "Epoch 73/300\n",
      "Average training loss: 0.037300964534282686\n",
      "Average test loss: 0.0025722651698937018\n",
      "Epoch 74/300\n",
      "Average training loss: 0.037330803195635476\n",
      "Average test loss: 0.002548463001019425\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03711846597658263\n",
      "Average test loss: 0.0025253333521799907\n",
      "Epoch 76/300\n",
      "Average training loss: 0.037077419209811426\n",
      "Average test loss: 0.0025681082513183354\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03696266559263071\n",
      "Average test loss: 0.002612885753520661\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0369091839061843\n",
      "Average test loss: 0.0025316688430806\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03681138102544679\n",
      "Average test loss: 0.002550110722995467\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03681529956062635\n",
      "Average test loss: 0.0025993961308979326\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03669377829465601\n",
      "Average test loss: 0.002605860523879528\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03671047968003485\n",
      "Average test loss: 0.0025480748338417873\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03657248115208414\n",
      "Average test loss: 0.0025868681400186486\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0364820243385103\n",
      "Average test loss: 0.0025422179500261943\n",
      "Epoch 85/300\n",
      "Average training loss: 0.036393530309200284\n",
      "Average test loss: 0.0026235864363196824\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03635993642773893\n",
      "Average test loss: 0.002554764778146313\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03626754399140676\n",
      "Average test loss: 0.002568943690508604\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0361664546314213\n",
      "Average test loss: 0.002554453381544186\n",
      "Epoch 89/300\n",
      "Average training loss: 0.036151796887318295\n",
      "Average test loss: 0.002582358975170387\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03605913763741652\n",
      "Average test loss: 0.0025434670065426165\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03595445484585232\n",
      "Average test loss: 0.0025859765625662274\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03608602085378435\n",
      "Average test loss: 0.002553438846435812\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0359138217303488\n",
      "Average test loss: 0.002561997424190243\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03576477637555864\n",
      "Average test loss: 0.002606411338162919\n",
      "Epoch 95/300\n",
      "Average training loss: 0.035710883246527776\n",
      "Average test loss: 0.002561794449161324\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03569664549496439\n",
      "Average test loss: 0.0025778268130703104\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0358005329436726\n",
      "Average test loss: 0.002562505743569798\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03561829429202609\n",
      "Average test loss: 0.0026316518204079734\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03545193043185605\n",
      "Average test loss: 0.0026014122243763673\n",
      "Epoch 100/300\n",
      "Average training loss: 0.035451089729865395\n",
      "Average test loss: 0.0025783456828859116\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03539446559134457\n",
      "Average test loss: 0.0027016190031750333\n",
      "Epoch 102/300\n",
      "Average training loss: 0.035326203195585146\n",
      "Average test loss: 0.002625737161272102\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03526314575639036\n",
      "Average test loss: 0.0025796952408014074\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03521747325857481\n",
      "Average test loss: 0.002617044708588057\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03520006910132037\n",
      "Average test loss: 0.002601034647474686\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03518827258878284\n",
      "Average test loss: 0.002713089492482444\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03507998679743873\n",
      "Average test loss: 0.0025358228188835912\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03501385019885169\n",
      "Average test loss: 0.0025915906847351127\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03504512526757187\n",
      "Average test loss: 0.002595474489654104\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03493770620557997\n",
      "Average test loss: 0.002617394673431085\n",
      "Epoch 111/300\n",
      "Average training loss: 0.035055762031012114\n",
      "Average test loss: 0.0026104878077490463\n",
      "Epoch 112/300\n",
      "Average training loss: 0.034817027217812005\n",
      "Average test loss: 0.0026010701637715103\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03477254445023007\n",
      "Average test loss: 0.002564313588043054\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03475669786830743\n",
      "Average test loss: 0.0026295466216074096\n",
      "Epoch 115/300\n",
      "Average training loss: 0.034859321039583946\n",
      "Average test loss: 0.0025997239392664697\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0346822163661321\n",
      "Average test loss: 0.002643965101076497\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0346491822136773\n",
      "Average test loss: 0.002584133646554417\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03458190562162134\n",
      "Average test loss: 0.0027335548726841806\n",
      "Epoch 119/300\n",
      "Average training loss: 0.034694872516724796\n",
      "Average test loss: 0.0026116688017629917\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03453439346286986\n",
      "Average test loss: 0.002772476215950317\n",
      "Epoch 121/300\n",
      "Average training loss: 0.034396213703685334\n",
      "Average test loss: 0.002653629846043057\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03440518711176183\n",
      "Average test loss: 0.0026891426210188204\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0344257359471586\n",
      "Average test loss: 0.002611191315162513\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03438642798198594\n",
      "Average test loss: 0.0027213393453922536\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03434276229308711\n",
      "Average test loss: 0.0026252880247516763\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03423969037002987\n",
      "Average test loss: 0.0026401910743572648\n",
      "Epoch 127/300\n",
      "Average training loss: 0.034308510834972064\n",
      "Average test loss: 0.002603345604820384\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03414176560938358\n",
      "Average test loss: 0.002667825337085459\n",
      "Epoch 129/300\n",
      "Average training loss: 0.034161760482523174\n",
      "Average test loss: 0.0026140288785099983\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0340955702331331\n",
      "Average test loss: 0.0026407584415541756\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03407780470450719\n",
      "Average test loss: 0.002587515769008961\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03405796618428495\n",
      "Average test loss: 0.0025876524930612908\n",
      "Epoch 133/300\n",
      "Average training loss: 0.034048059985041616\n",
      "Average test loss: 0.002606795395517515\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03397278106212616\n",
      "Average test loss: 0.002694086023916801\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03392540518939495\n",
      "Average test loss: 0.002640017105680373\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03390644361575445\n",
      "Average test loss: 0.002770787881480323\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03389508402016428\n",
      "Average test loss: 0.0026127917561680077\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03389301432006889\n",
      "Average test loss: 0.0027383387725179393\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03381478654676014\n",
      "Average test loss: 0.0026515032630413772\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03374693975845973\n",
      "Average test loss: 0.0028648912879741853\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03379339592158794\n",
      "Average test loss: 0.0026758762653917075\n",
      "Epoch 142/300\n",
      "Average training loss: 0.033720937533511054\n",
      "Average test loss: 0.0026316244347641864\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03368283413516151\n",
      "Average test loss: 0.002658306693347792\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03375591578417354\n",
      "Average test loss: 0.002651632889898287\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03363263488809268\n",
      "Average test loss: 0.002650907160817749\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03358484537071652\n",
      "Average test loss: 0.0026434061587270762\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03352118351393276\n",
      "Average test loss: 0.002683710936146478\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03358014106088215\n",
      "Average test loss: 0.0026508619708733427\n",
      "Epoch 149/300\n",
      "Average training loss: 0.033520728944076435\n",
      "Average test loss: 0.0026438364926725626\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03365485427445836\n",
      "Average test loss: 0.0026545055146432587\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03350101015634007\n",
      "Average test loss: 0.0027504849806427954\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03342683697077963\n",
      "Average test loss: 0.00263853371847007\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03339588326878018\n",
      "Average test loss: 0.002638320449222293\n",
      "Epoch 154/300\n",
      "Average training loss: 0.033357749932342105\n",
      "Average test loss: 0.0026285230064143736\n",
      "Epoch 155/300\n",
      "Average training loss: 0.033363739371299744\n",
      "Average test loss: 0.002704070825336708\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03335154644979371\n",
      "Average test loss: 0.002662205505081349\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03330312460164229\n",
      "Average test loss: 0.0027213553873201212\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03340368688437674\n",
      "Average test loss: 0.0027364192097965212\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03326234173609151\n",
      "Average test loss: 0.0026294502996736103\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03330683501561483\n",
      "Average test loss: 0.0026373838723326723\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03313971550265948\n",
      "Average test loss: 0.002668628704113265\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03318930761681663\n",
      "Average test loss: 0.0027257829318857855\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03320721943676472\n",
      "Average test loss: 0.0027130088737855355\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03312059531112512\n",
      "Average test loss: 0.0026648107105866074\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03304772713780403\n",
      "Average test loss: 0.0026888957957012785\n",
      "Epoch 166/300\n",
      "Average training loss: 0.033056812236706416\n",
      "Average test loss: 0.0026968456057624686\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03306875499751833\n",
      "Average test loss: 0.002665391352234615\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0330644969890515\n",
      "Average test loss: 0.002751140629251798\n",
      "Epoch 169/300\n",
      "Average training loss: 0.033049623447987765\n",
      "Average test loss: 0.002638858989088072\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03304664564463827\n",
      "Average test loss: 0.002691790896778305\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03299577095111211\n",
      "Average test loss: 0.0027651769440207217\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03298531185421679\n",
      "Average test loss: 0.002744718862283561\n",
      "Epoch 173/300\n",
      "Average training loss: 0.032896140750911504\n",
      "Average test loss: 0.0027307979222387074\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03294151019387775\n",
      "Average test loss: 0.0027409615415251915\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03289629493488206\n",
      "Average test loss: 0.0027149797135757074\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03293378208577633\n",
      "Average test loss: 0.0026310933126757544\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03289872050285339\n",
      "Average test loss: 0.0026595975171981585\n",
      "Epoch 178/300\n",
      "Average training loss: 0.032859306633472446\n",
      "Average test loss: 0.00346093538382815\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03285127490096622\n",
      "Average test loss: 0.002703340093087819\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03282903796434403\n",
      "Average test loss: 0.0027156011658824152\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03277641833987501\n",
      "Average test loss: 0.0027000654335651133\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03274051906830735\n",
      "Average test loss: 0.0026624165394653876\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03274042348894808\n",
      "Average test loss: 0.0026852564595432745\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03273164855109321\n",
      "Average test loss: 0.0027215021414061387\n",
      "Epoch 185/300\n",
      "Average training loss: 0.032737147693832715\n",
      "Average test loss: 0.002809176565665338\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0326891975187593\n",
      "Average test loss: 0.0027110493299033907\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03276345575021373\n",
      "Average test loss: 0.0026949092318407364\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03271266897022724\n",
      "Average test loss: 0.0026998823487924205\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03265168884893258\n",
      "Average test loss: 0.0026730705042266184\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0325847011771467\n",
      "Average test loss: 0.002694386267413696\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03257555347515477\n",
      "Average test loss: 0.002753072820086446\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03256687037812339\n",
      "Average test loss: 0.002714910968724224\n",
      "Epoch 193/300\n",
      "Average training loss: 0.032577750232484604\n",
      "Average test loss: 0.0026972630895260308\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03255059757663144\n",
      "Average test loss: 0.0027146331154637867\n",
      "Epoch 195/300\n",
      "Average training loss: 0.032511256482866076\n",
      "Average test loss: 0.002730280269972152\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03256946687234773\n",
      "Average test loss: 0.0027456772952444025\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03248666492270099\n",
      "Average test loss: 0.0026828371305018662\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0325128713481956\n",
      "Average test loss: 0.002792742113996711\n",
      "Epoch 199/300\n",
      "Average training loss: 0.032439124261339505\n",
      "Average test loss: 0.0027269534286525512\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03243650592366854\n",
      "Average test loss: 0.0027303456811027397\n",
      "Epoch 201/300\n",
      "Average training loss: 0.032446574726038506\n",
      "Average test loss: 0.002731584953972035\n",
      "Epoch 202/300\n",
      "Average training loss: 0.032407431662082674\n",
      "Average test loss: 0.0026659733270191486\n",
      "Epoch 203/300\n",
      "Average training loss: 0.032461817930142085\n",
      "Average test loss: 0.002742874504894846\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0323863168127007\n",
      "Average test loss: 0.0027141909252645243\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03232392285267512\n",
      "Average test loss: 0.002663545209914446\n",
      "Epoch 206/300\n",
      "Average training loss: 0.032297769496838255\n",
      "Average test loss: 0.0027372029394739203\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03227993516789542\n",
      "Average test loss: 0.002753750499751833\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03240481160581112\n",
      "Average test loss: 0.0027378286404742135\n",
      "Epoch 209/300\n",
      "Average training loss: 0.032388365146186614\n",
      "Average test loss: 0.0027078981794830827\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03224693376157019\n",
      "Average test loss: 0.0026983105171885756\n",
      "Epoch 211/300\n",
      "Average training loss: 0.032331837818026544\n",
      "Average test loss: 0.0027270368575635882\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03224106849564446\n",
      "Average test loss: 0.002701390768194364\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03228408015602165\n",
      "Average test loss: 0.002734483245242801\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03247052820192443\n",
      "Average test loss: 0.00270241774742802\n",
      "Epoch 215/300\n",
      "Average training loss: 0.032151953137583206\n",
      "Average test loss: 0.00274339522048831\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0322372302280532\n",
      "Average test loss: 0.002760148511371679\n",
      "Epoch 217/300\n",
      "Average training loss: 0.032185985253916846\n",
      "Average test loss: 0.002698080679712196\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0322351923700836\n",
      "Average test loss: 0.00273295787597696\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03219171693589952\n",
      "Average test loss: 0.002835109000404676\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03208521915806664\n",
      "Average test loss: 0.0027793498179978795\n",
      "Epoch 221/300\n",
      "Average training loss: 0.032143525070614284\n",
      "Average test loss: 0.002705715226009488\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03208080195221636\n",
      "Average test loss: 0.0026952766080697377\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03208976483179463\n",
      "Average test loss: 0.0026934433821588755\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03203735948271222\n",
      "Average test loss: 0.002673287114335431\n",
      "Epoch 225/300\n",
      "Average training loss: 0.032090977903869417\n",
      "Average test loss: 0.0027270535259611077\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0321243851499425\n",
      "Average test loss: 0.0028166939340945746\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03203775450090567\n",
      "Average test loss: 0.0026952372563795912\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03201365805003378\n",
      "Average test loss: 0.002938314481534892\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03205135749114884\n",
      "Average test loss: 0.002775979932397604\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03202895655565792\n",
      "Average test loss: 0.00275229093639387\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03193600004580286\n",
      "Average test loss: 0.002682017672393057\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03196442352235317\n",
      "Average test loss: 0.0027683104134889115\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0319559423973163\n",
      "Average test loss: 0.0027026378797988097\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03202083482179377\n",
      "Average test loss: 0.0027708595676554574\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03197824537754059\n",
      "Average test loss: 0.0027806774350918003\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0319682401518027\n",
      "Average test loss: 0.0027046992712550693\n",
      "Epoch 237/300\n",
      "Average training loss: 0.031892020548383394\n",
      "Average test loss: 0.002742647252149052\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03189102023012108\n",
      "Average test loss: 0.002699652522595392\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03186193942361408\n",
      "Average test loss: 0.0027631305368203255\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03189796135160658\n",
      "Average test loss: 0.0027316610800723236\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03189092530475723\n",
      "Average test loss: 0.002738175094748537\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03183880439069536\n",
      "Average test loss: 0.0027414057648016345\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03184714259869523\n",
      "Average test loss: 0.002734789105132222\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03186658739050229\n",
      "Average test loss: 0.009901787617140346\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03204840718044175\n",
      "Average test loss: 0.0027033013806988795\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03181584208872583\n",
      "Average test loss: 0.002718869692232046\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03183128521674209\n",
      "Average test loss: 0.0027314481453763113\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03174323968754875\n",
      "Average test loss: 0.002838079491423236\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03178574036889606\n",
      "Average test loss: 0.0026924443180776306\n",
      "Epoch 250/300\n",
      "Average training loss: 0.031754217988914916\n",
      "Average test loss: 0.002791304521366126\n",
      "Epoch 251/300\n",
      "Average training loss: 0.031742780776487456\n",
      "Average test loss: 0.0028795772778491178\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03182689846058687\n",
      "Average test loss: 0.002802318794445859\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03169342688553863\n",
      "Average test loss: 0.002763491615032156\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03172703873945607\n",
      "Average test loss: 0.0027743249158892367\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03170619094040659\n",
      "Average test loss: 0.0027201496267484295\n",
      "Epoch 256/300\n",
      "Average training loss: 0.031736688097318014\n",
      "Average test loss: 0.0027754823236415785\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03167126711540752\n",
      "Average test loss: 0.002763382169107596\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03166893312500583\n",
      "Average test loss: 0.00277027549698121\n",
      "Epoch 259/300\n",
      "Average training loss: 0.031689890056848524\n",
      "Average test loss: 0.0027414326140036186\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03164168968300025\n",
      "Average test loss: 0.002828501114828719\n",
      "Epoch 261/300\n",
      "Average training loss: 0.031699580813447634\n",
      "Average test loss: 0.002745556031871173\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03166440632111497\n",
      "Average test loss: 0.0028203223136564095\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03164494565460417\n",
      "Average test loss: 0.002725944159552455\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03158045042140616\n",
      "Average test loss: 0.002725102256155676\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03165852803985278\n",
      "Average test loss: 0.0027547290687345797\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03156480727593104\n",
      "Average test loss: 0.002927806111673514\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03161841899487707\n",
      "Average test loss: 0.0028090590784947077\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03160093720257282\n",
      "Average test loss: 0.0027461542497492497\n",
      "Epoch 269/300\n",
      "Average training loss: 0.031593240333928005\n",
      "Average test loss: 0.002885371654190951\n",
      "Epoch 270/300\n",
      "Average training loss: 0.031544431424803204\n",
      "Average test loss: 0.002820381333637569\n",
      "Epoch 271/300\n",
      "Average training loss: 0.031604538806610634\n",
      "Average test loss: 0.002716057480002443\n",
      "Epoch 272/300\n",
      "Average training loss: 0.031587289578384824\n",
      "Average test loss: 0.002671874981580509\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03155656581454807\n",
      "Average test loss: 0.0027281073010009195\n",
      "Epoch 274/300\n",
      "Average training loss: 0.031620652910735875\n",
      "Average test loss: 0.0028476438578218222\n",
      "Epoch 275/300\n",
      "Average training loss: 0.031527672802408535\n",
      "Average test loss: 0.002870313343902429\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03145029918352763\n",
      "Average test loss: 0.002786749101140433\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03152482350170612\n",
      "Average test loss: 0.002749825469735596\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03145327104793655\n",
      "Average test loss: 0.0026920222224046785\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03143373536732462\n",
      "Average test loss: 0.002821817887533042\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03150050727857484\n",
      "Average test loss: 0.0027942302005572453\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03143342727588283\n",
      "Average test loss: 0.002728785956485404\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03140504606730408\n",
      "Average test loss: 0.002737742410351833\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03160802569323116\n",
      "Average test loss: 0.0027395517161736887\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03142157266537349\n",
      "Average test loss: 0.002799861735560828\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03142840378483137\n",
      "Average test loss: 0.0027446961720577544\n",
      "Epoch 286/300\n",
      "Average training loss: 0.031408181003398365\n",
      "Average test loss: 0.0027186331876243156\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03139854757322205\n",
      "Average test loss: 0.0028155995461468897\n",
      "Epoch 288/300\n",
      "Average training loss: 0.031395968437194825\n",
      "Average test loss: 0.002785574135267072\n",
      "Epoch 289/300\n",
      "Average training loss: 0.031429504212405944\n",
      "Average test loss: 0.0027968624898542962\n",
      "Epoch 290/300\n",
      "Average training loss: 0.031351906741658844\n",
      "Average test loss: 0.0026822457013444766\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0313430079271396\n",
      "Average test loss: 0.002745451988445388\n",
      "Epoch 292/300\n",
      "Average training loss: 0.031374001380470064\n",
      "Average test loss: 0.0027747555668983194\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03137965827931961\n",
      "Average test loss: 0.0027765898658997485\n",
      "Epoch 294/300\n",
      "Average training loss: 0.031305226372347936\n",
      "Average test loss: 0.0027179468619740674\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03131047732465797\n",
      "Average test loss: 0.0027695516225778394\n",
      "Epoch 296/300\n",
      "Average training loss: 0.031322922509577536\n",
      "Average test loss: 0.0027670227721747426\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03128818945421113\n",
      "Average test loss: 0.0027833843632704683\n",
      "Epoch 298/300\n",
      "Average training loss: 0.031285478098524944\n",
      "Average test loss: 0.0027351295167787207\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03131181488434474\n",
      "Average test loss: 0.002761728747851319\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03131117585301399\n",
      "Average test loss: 0.002783412281009886\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7597811540762583\n",
      "Average test loss: 0.005084543712437153\n",
      "Epoch 2/300\n",
      "Average training loss: 0.16039152726862166\n",
      "Average test loss: 0.003487923479742474\n",
      "Epoch 3/300\n",
      "Average training loss: 0.09977279981970787\n",
      "Average test loss: 0.003241550277504656\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07807172011004554\n",
      "Average test loss: 0.0031364474420746166\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06642339608404371\n",
      "Average test loss: 0.002963652713017331\n",
      "Epoch 6/300\n",
      "Average training loss: 0.059074868182341256\n",
      "Average test loss: 0.0028629743346116608\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05387489879462454\n",
      "Average test loss: 0.0027720548175275326\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05062001482976807\n",
      "Average test loss: 0.0026774499256991676\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04829646280407906\n",
      "Average test loss: 0.002692073425485028\n",
      "Epoch 10/300\n",
      "Average training loss: 0.046491966956191594\n",
      "Average test loss: 0.0025556839168485667\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04504177834590276\n",
      "Average test loss: 0.0024754129824125105\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04379014390707016\n",
      "Average test loss: 0.002518664760515094\n",
      "Epoch 13/300\n",
      "Average training loss: 0.042723502172364126\n",
      "Average test loss: 0.002350070129148662\n",
      "Epoch 14/300\n",
      "Average training loss: 0.041788457678423985\n",
      "Average test loss: 0.002379485365313788\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04089513699875937\n",
      "Average test loss: 0.002383322572335601\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04012735626763768\n",
      "Average test loss: 0.0022114045810368325\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0393612516630027\n",
      "Average test loss: 0.0022383387794511185\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03869443557659785\n",
      "Average test loss: 0.0021452951106346314\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03802867964572377\n",
      "Average test loss: 0.002125150933344331\n",
      "Epoch 20/300\n",
      "Average training loss: 0.037360310332642664\n",
      "Average test loss: 0.0021053807209763264\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03680240540703138\n",
      "Average test loss: 0.0021445556166064406\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03619990507264932\n",
      "Average test loss: 0.002083517418346471\n",
      "Epoch 23/300\n",
      "Average training loss: 0.035676250447829565\n",
      "Average test loss: 0.0020338425503836736\n",
      "Epoch 24/300\n",
      "Average training loss: 0.035153010725975035\n",
      "Average test loss: 0.0020911866939730115\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03468497878147496\n",
      "Average test loss: 0.0020836415804094737\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03425256355272399\n",
      "Average test loss: 0.002058817263175216\n",
      "Epoch 27/300\n",
      "Average training loss: 0.033897341158654955\n",
      "Average test loss: 0.002030440629977319\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03349745737678475\n",
      "Average test loss: 0.0019949402600112887\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03310987714264128\n",
      "Average test loss: 0.0019583520211486354\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03281681400537491\n",
      "Average test loss: 0.001966344255229665\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03249282618860404\n",
      "Average test loss: 0.0019223916663063897\n",
      "Epoch 32/300\n",
      "Average training loss: 0.032181275132629604\n",
      "Average test loss: 0.0019350241950402657\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03194308694038126\n",
      "Average test loss: 0.0019363102573487494\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03167227768732442\n",
      "Average test loss: 0.0019379732449435526\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03150244312981765\n",
      "Average test loss: 0.0019099410050031213\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03125504666070143\n",
      "Average test loss: 0.0018998305851386653\n",
      "Epoch 37/300\n",
      "Average training loss: 0.031079934861924913\n",
      "Average test loss: 0.0018943248188330068\n",
      "Epoch 38/300\n",
      "Average training loss: 0.030929807995756467\n",
      "Average test loss: 0.0019069863957249457\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03071665736867322\n",
      "Average test loss: 0.0019388547570755085\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03054289681547218\n",
      "Average test loss: 0.001869641605247226\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03036672592163086\n",
      "Average test loss: 0.0018735437722255787\n",
      "Epoch 42/300\n",
      "Average training loss: 0.030222107077638307\n",
      "Average test loss: 0.0018667031534843974\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03005438129769431\n",
      "Average test loss: 0.001868077949103382\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0300085559686025\n",
      "Average test loss: 0.0018605030017594497\n",
      "Epoch 45/300\n",
      "Average training loss: 0.029833500249518287\n",
      "Average test loss: 0.0018558513081322113\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02971984708805879\n",
      "Average test loss: 0.001862951518036425\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02953660193251239\n",
      "Average test loss: 0.0018580635285211935\n",
      "Epoch 48/300\n",
      "Average training loss: 0.029481827290521727\n",
      "Average test loss: 0.001854793825911151\n",
      "Epoch 49/300\n",
      "Average training loss: 0.029383010192877715\n",
      "Average test loss: 0.0018900993328748478\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02925379432241122\n",
      "Average test loss: 0.0018471279146356715\n",
      "Epoch 51/300\n",
      "Average training loss: 0.029145665630698203\n",
      "Average test loss: 0.0018633095269194907\n",
      "Epoch 52/300\n",
      "Average training loss: 0.029091829783386654\n",
      "Average test loss: 0.0018472304670140148\n",
      "Epoch 53/300\n",
      "Average training loss: 0.028976812642481593\n",
      "Average test loss: 0.001847994391909904\n",
      "Epoch 54/300\n",
      "Average training loss: 0.028849346068170335\n",
      "Average test loss: 0.0018494448740449217\n",
      "Epoch 55/300\n",
      "Average training loss: 0.028776409639252556\n",
      "Average test loss: 0.00184537758367757\n",
      "Epoch 56/300\n",
      "Average training loss: 0.028678045420183075\n",
      "Average test loss: 0.0018681388944387436\n",
      "Epoch 57/300\n",
      "Average training loss: 0.028609648033976556\n",
      "Average test loss: 0.00184884769614372\n",
      "Epoch 58/300\n",
      "Average training loss: 0.028542136467165417\n",
      "Average test loss: 0.0018404576269288857\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02841468590994676\n",
      "Average test loss: 0.0018321748500068982\n",
      "Epoch 60/300\n",
      "Average training loss: 0.028384779915213584\n",
      "Average test loss: 0.0018275594822027617\n",
      "Epoch 61/300\n",
      "Average training loss: 0.028263776833812395\n",
      "Average test loss: 0.0018415789574177728\n",
      "Epoch 62/300\n",
      "Average training loss: 0.028140543565154077\n",
      "Average test loss: 0.0018416673704567882\n",
      "Epoch 63/300\n",
      "Average training loss: 0.028033233932322925\n",
      "Average test loss: 0.0018368923827591869\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02802371472120285\n",
      "Average test loss: 0.0018351516104820702\n",
      "Epoch 65/300\n",
      "Average training loss: 0.027919134873482916\n",
      "Average test loss: 0.0018777451722158326\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02783882209824191\n",
      "Average test loss: 0.0018704320694216423\n",
      "Epoch 67/300\n",
      "Average training loss: 0.027796221959922048\n",
      "Average test loss: 0.0018764116153534916\n",
      "Epoch 68/300\n",
      "Average training loss: 0.027742897831731374\n",
      "Average test loss: 0.001861584405000839\n",
      "Epoch 69/300\n",
      "Average training loss: 0.027649130928847524\n",
      "Average test loss: 0.0018899378092545602\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02753199031121201\n",
      "Average test loss: 0.001830472373093168\n",
      "Epoch 71/300\n",
      "Average training loss: 0.027487292807963158\n",
      "Average test loss: 0.001845579424045152\n",
      "Epoch 72/300\n",
      "Average training loss: 0.027391319771607716\n",
      "Average test loss: 0.0018607699770687356\n",
      "Epoch 73/300\n",
      "Average training loss: 0.027335964961184397\n",
      "Average test loss: 0.0019064510704742538\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02727212241788705\n",
      "Average test loss: 0.0018441552271445591\n",
      "Epoch 75/300\n",
      "Average training loss: 0.027222513890928692\n",
      "Average test loss: 0.0018808902131600513\n",
      "Epoch 76/300\n",
      "Average training loss: 0.027171781793236732\n",
      "Average test loss: 0.0018589937396140562\n",
      "Epoch 77/300\n",
      "Average training loss: 0.027173388216230606\n",
      "Average test loss: 0.0018565401934708158\n",
      "Epoch 78/300\n",
      "Average training loss: 0.027054088330931134\n",
      "Average test loss: 0.0018601051142646207\n",
      "Epoch 79/300\n",
      "Average training loss: 0.027004345822665428\n",
      "Average test loss: 0.0018496950252188578\n",
      "Epoch 80/300\n",
      "Average training loss: 0.027002689765559303\n",
      "Average test loss: 0.00185334289767262\n",
      "Epoch 81/300\n",
      "Average training loss: 0.026782291321290865\n",
      "Average test loss: 0.0020258362436046205\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02674717815220356\n",
      "Average test loss: 0.0018516071853745314\n",
      "Epoch 83/300\n",
      "Average training loss: 0.026766121925579176\n",
      "Average test loss: 0.0018775174787475004\n",
      "Epoch 84/300\n",
      "Average training loss: 0.026752637306849163\n",
      "Average test loss: 0.002204741265417801\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02666262464887566\n",
      "Average test loss: 0.0018437127535127931\n",
      "Epoch 86/300\n",
      "Average training loss: 0.026498808041214943\n",
      "Average test loss: 0.0018305409544457992\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02647600787712468\n",
      "Average test loss: 0.0018672514522655142\n",
      "Epoch 88/300\n",
      "Average training loss: 0.026424392607476976\n",
      "Average test loss: 0.0019411617047153413\n",
      "Epoch 89/300\n",
      "Average training loss: 0.026372070492969617\n",
      "Average training loss: 0.026334085231026014\n",
      "Average test loss: 0.001941879071916143\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02628362491892444\n",
      "Average test loss: 0.001874871076602075\n",
      "Epoch 92/300\n",
      "Average training loss: 0.026255278506212763\n",
      "Average test loss: 0.0018517573428236775\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02617398232387172\n",
      "Average test loss: 0.0018683844710596733\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0261399310313993\n",
      "Average test loss: 0.0018734715695803365\n",
      "Epoch 95/300\n",
      "Average training loss: 0.026050605884856647\n",
      "Average test loss: 0.001870128628694349\n",
      "Epoch 96/300\n",
      "Average training loss: 0.026044091368714967\n",
      "Average test loss: 0.0018638160235972868\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0259384755641222\n",
      "Average test loss: 0.001868397088514434\n",
      "Epoch 98/300\n",
      "Average training loss: 0.025944413979848226\n",
      "Average test loss: 0.001996042261624502\n",
      "Epoch 99/300\n",
      "Average training loss: 0.025898872074153687\n",
      "Average test loss: 0.0018434492726292875\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02585371573931641\n",
      "Average test loss: 0.0018539588110935356\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02578144274486436\n",
      "Average test loss: 0.0019144561637399924\n",
      "Epoch 102/300\n",
      "Average training loss: 0.025773192245099278\n",
      "Average test loss: 0.001915621561308702\n",
      "Epoch 103/300\n",
      "Average training loss: 0.025769688768519296\n",
      "Average test loss: 0.0019065654891957011\n",
      "Epoch 104/300\n",
      "Average training loss: 0.025660969477560785\n",
      "Average test loss: 0.001873065311409947\n",
      "Epoch 105/300\n",
      "Average training loss: 0.025619613571299448\n",
      "Average test loss: 0.0018797565997681683\n",
      "Epoch 106/300\n",
      "Average training loss: 0.025607787074314225\n",
      "Average test loss: 0.0019052575963238874\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02556942717730999\n",
      "Average test loss: 0.001914502988052037\n",
      "Epoch 108/300\n",
      "Average training loss: 0.025511611307660737\n",
      "Average test loss: 0.0019042810409640272\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02543902141186926\n",
      "Average test loss: 0.0019486690329180824\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02540223716944456\n",
      "Average test loss: 0.0019304642223028673\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02542907219297356\n",
      "Average test loss: 0.001877367875021365\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02539615427619881\n",
      "Average test loss: 0.001884509830839104\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02533594072692924\n",
      "Average test loss: 0.0019028794312228759\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02528608631094297\n",
      "Average test loss: 0.0018910910784163408\n",
      "Epoch 115/300\n",
      "Average training loss: 0.025228533850775826\n",
      "Average test loss: 0.0019780703546065422\n",
      "Epoch 116/300\n",
      "Average training loss: 0.025184989949067434\n",
      "Average test loss: 0.001886525633227494\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02526995560195711\n",
      "Average test loss: 0.0018767306334856483\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02512023084031211\n",
      "Average test loss: 0.0018874133746657107\n",
      "Epoch 119/300\n",
      "Average training loss: 0.025117363712853856\n",
      "Average test loss: 0.0019501543154733048\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02507900754776266\n",
      "Average test loss: 0.0019025006021062532\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025026481297281054\n",
      "Average test loss: 0.0018876687798442112\n",
      "Epoch 122/300\n",
      "Average training loss: 0.025008332611785995\n",
      "Average test loss: 0.002018527239561081\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02499455400970247\n",
      "Average test loss: 0.001892502817325294\n",
      "Epoch 124/300\n",
      "Average training loss: 0.024988317989640765\n",
      "Average test loss: 0.0019193923516819874\n",
      "Epoch 125/300\n",
      "Average training loss: 0.024921152242355877\n",
      "Average test loss: 0.001908295014148785\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0248835873901844\n",
      "Average test loss: 0.00190986576055487\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024857200677196184\n",
      "Average test loss: 0.0019349818128264612\n",
      "Epoch 128/300\n",
      "Average training loss: 0.024876400298542448\n",
      "Average test loss: 0.0019616061865041653\n",
      "Epoch 129/300\n",
      "Average training loss: 0.024819045099947187\n",
      "Average test loss: 0.0019057232123903102\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024720062211155892\n",
      "Average test loss: 0.001922377366469138\n",
      "Epoch 131/300\n",
      "Average training loss: 0.024751139112644725\n",
      "Average test loss: 0.0018873544346748128\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024727848885787856\n",
      "Average test loss: 0.0020189688810043865\n",
      "Epoch 133/300\n",
      "Average training loss: 0.024768751035134\n",
      "Average test loss: 0.0019241279802388615\n",
      "Epoch 134/300\n",
      "Average training loss: 0.024652021168006792\n",
      "Average test loss: 0.0019254050138923858\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02470261944995986\n",
      "Average test loss: 0.0020093909667597875\n",
      "Epoch 136/300\n",
      "Average training loss: 0.024618711198369662\n",
      "Average test loss: 0.001948801870147387\n",
      "Epoch 137/300\n",
      "Average training loss: 0.024551721841096878\n",
      "Average test loss: 0.001976836313183109\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02453556966284911\n",
      "Average test loss: 0.0019609392707546553\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02452376992503802\n",
      "Average test loss: 0.001931933378179868\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02449134100145764\n",
      "Average test loss: 0.0019864296624436976\n",
      "Epoch 141/300\n",
      "Average training loss: 0.024500511104861896\n",
      "Average test loss: 0.001995955498682128\n",
      "Epoch 142/300\n",
      "Average training loss: 0.024483247488737106\n",
      "Average test loss: 0.0019120931134869654\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02445640807184908\n",
      "Average test loss: 0.0019389546927478579\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024455733188324506\n",
      "Average test loss: 0.0019317429482729898\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02444481397834089\n",
      "Average test loss: 0.0019859378633813726\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024456255811784\n",
      "Average test loss: 0.002007614257021083\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024321414578292105\n",
      "Average test loss: 0.001983688522544172\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02432629490726524\n",
      "Average test loss: 0.0019581432887870406\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02432640300028854\n",
      "Average test loss: 0.0019496695350648629\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02426480804218186\n",
      "Average test loss: 0.0019255208258206645\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024318369393547375\n",
      "Average test loss: 0.001922351996310883\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024242905085285503\n",
      "Average test loss: 0.0019179112375196483\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024243272599246767\n",
      "Average test loss: 0.0019276437600039774\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02417963311407301\n",
      "Average test loss: 0.0019282705415454176\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024176058150000044\n",
      "Average test loss: 0.001972973016504612\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02414836096101337\n",
      "Average test loss: 0.001986657552835014\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024133269000384542\n",
      "Average test loss: 0.0019602769662936527\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02411502811478244\n",
      "Average test loss: 0.0019197748069547944\n",
      "Epoch 159/300\n",
      "Average training loss: 0.024088304799464014\n",
      "Average test loss: 0.002028706257748935\n",
      "Epoch 160/300\n",
      "Average training loss: 0.024061727921168008\n",
      "Average test loss: 0.0019266453847909966\n",
      "Epoch 161/300\n",
      "Average training loss: 0.024055811893608836\n",
      "Average test loss: 0.001964980473741889\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024053727780779202\n",
      "Average test loss: 0.0019672285293539367\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0240082698530621\n",
      "Average test loss: 0.0019393688320285744\n",
      "Epoch 164/300\n",
      "Average training loss: 0.024021673623886373\n",
      "Average test loss: 0.001951798929936356\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024014494041601817\n",
      "Average test loss: 0.002014301607178317\n",
      "Epoch 166/300\n",
      "Average training loss: 0.023951367399758762\n",
      "Average test loss: 0.001978970136389964\n",
      "Epoch 167/300\n",
      "Average training loss: 0.023963318586349487\n",
      "Average test loss: 0.001984686729291247\n",
      "Epoch 168/300\n",
      "Average training loss: 0.023915726009342407\n",
      "Average test loss: 0.0019507764451619652\n",
      "Epoch 169/300\n",
      "Average training loss: 0.023910266420907444\n",
      "Average test loss: 0.001970956203424268\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023919127431180742\n",
      "Average test loss: 0.001987611049061848\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02399234605166647\n",
      "Average test loss: 0.001921034243164791\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02389563992785083\n",
      "Average test loss: 0.0019617462779084843\n",
      "Epoch 173/300\n",
      "Average training loss: 0.023825703610976537\n",
      "Average test loss: 0.001997238669337498\n",
      "Epoch 174/300\n",
      "Average training loss: 0.023825212524996864\n",
      "Average test loss: 0.0019440531955204076\n",
      "Epoch 175/300\n",
      "Average training loss: 0.023801232266757223\n",
      "Average test loss: 0.0020187812844912213\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02380676746037271\n",
      "Average test loss: 0.002019239814745055\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023831967214743295\n",
      "Average test loss: 0.0019241614710125657\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023747846300403278\n",
      "Average test loss: 0.001999747770010597\n",
      "Epoch 179/300\n",
      "Average training loss: 0.023787363227870728\n",
      "Average test loss: 0.0019986644310669766\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02374837272365888\n",
      "Average test loss: 0.0020721553043565816\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023733168356948428\n",
      "Average test loss: 0.0020174499408652383\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023706767472955914\n",
      "Average test loss: 0.001948964728249444\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02368346537484063\n",
      "Average test loss: 0.0019287021426070067\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02369417647189564\n",
      "Average test loss: 0.0019660412140397564\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02367719471289052\n",
      "Average test loss: 0.001979068468635281\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02363884437746472\n",
      "Average test loss: 0.0019744124242828953\n",
      "Epoch 187/300\n",
      "Average training loss: 0.023656524469455084\n",
      "Average test loss: 0.0019766104650787183\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023627671175532872\n",
      "Average test loss: 0.0019754141233861446\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02363132593366835\n",
      "Average test loss: 0.0024795524696302083\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02358247106936243\n",
      "Average test loss: 0.0020213531318327618\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023616229726208582\n",
      "Average test loss: 0.002013516324417045\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02359797394110097\n",
      "Average test loss: 0.0019649678229664764\n",
      "Epoch 193/300\n",
      "Average training loss: 0.023524423382348485\n",
      "Average test loss: 0.001959006587250365\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023515463632014063\n",
      "Average test loss: 0.0019819284458127287\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023531018565098445\n",
      "Average test loss: 0.001986986119921009\n",
      "Epoch 196/300\n",
      "Average training loss: 0.023593280982640054\n",
      "Average test loss: 0.0019758401390992933\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023537057230869927\n",
      "Average test loss: 0.0019817192872158356\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023462547714511554\n",
      "Average test loss: 0.0019854047160802616\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02345641892734501\n",
      "Average test loss: 0.00202478016850849\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023470201747284994\n",
      "Average test loss: 0.001999221968774994\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023493183313144577\n",
      "Average test loss: 0.001990812140206496\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02348885490000248\n",
      "Average test loss: 0.0020014922923098006\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023422573414113787\n",
      "Average test loss: 0.0020365438701895376\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02337813081012832\n",
      "Average test loss: 0.002006034683229195\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02342167185081376\n",
      "Average test loss: 0.002002735166086091\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023413639994131194\n",
      "Average test loss: 0.002017661007690347\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02341434989372889\n",
      "Average test loss: 0.0020293962011734643\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023339586779475213\n",
      "Average test loss: 0.0019659639100233716\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023366446011596254\n",
      "Average test loss: 0.001984229485400849\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02338610678083367\n",
      "Average test loss: 0.001979031964826087\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023329959922366673\n",
      "Average test loss: 0.001941456772490508\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02330932962728871\n",
      "Average test loss: 0.0019798006135970355\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0233140980998675\n",
      "Average test loss: 0.0020370078064087366\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02328572439816263\n",
      "Average test loss: 0.0020156241605679194\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02326401701403989\n",
      "Average test loss: 0.002020861665614777\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02326424717903137\n",
      "Average test loss: 0.0019814135146637756\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023271387389964527\n",
      "Average test loss: 0.0020015529429333076\n",
      "Epoch 218/300\n",
      "Average training loss: 0.023268751597238912\n",
      "Average test loss: 0.002000185492448509\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0232106235159768\n",
      "Average test loss: 0.0020048316746122305\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02319631333152453\n",
      "Average test loss: 0.0019964463763559857\n",
      "Epoch 221/300\n",
      "Average training loss: 0.023247203159663412\n",
      "Average test loss: 0.0019487234082900815\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02322114447421498\n",
      "Average test loss: 0.0020426703959496484\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023237805664539337\n",
      "Average test loss: 0.001999770785164502\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0231677956548002\n",
      "Average test loss: 0.0019587941160425543\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023174136499563854\n",
      "Average test loss: 0.0020123632069056234\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023225257913271586\n",
      "Average test loss: 0.0019784689237260155\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023204913397630055\n",
      "Average test loss: 0.0020116750453081397\n",
      "Epoch 228/300\n",
      "Average training loss: 0.023125422646601994\n",
      "Average test loss: 0.0020046032887573043\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023120637210706868\n",
      "Average test loss: 0.0019716589736441773\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023088598041070833\n",
      "Average test loss: 0.001991850749692983\n",
      "Epoch 231/300\n",
      "Average training loss: 0.023148031721512478\n",
      "Average test loss: 0.00205086822497348\n",
      "Epoch 232/300\n",
      "Average training loss: 0.023086147685845694\n",
      "Average test loss: 0.0020194612002621096\n",
      "Epoch 233/300\n",
      "Average training loss: 0.023140005669660037\n",
      "Average test loss: 0.002013440949531893\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023113972395658492\n",
      "Average test loss: 0.001984269692148599\n",
      "Epoch 235/300\n",
      "Average training loss: 0.023083807268076472\n",
      "Average test loss: 0.002010570963534216\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023057845609055626\n",
      "Average test loss: 0.00199639909936943\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023103790956238907\n",
      "Average test loss: 0.0019936024153398143\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022990802662240135\n",
      "Average test loss: 0.002012780564526717\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023067496260007224\n",
      "Average test loss: 0.002010878305364814\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02307235590616862\n",
      "Average test loss: 0.001955150227786766\n",
      "Epoch 241/300\n",
      "Average training loss: 0.023011914138992628\n",
      "Average test loss: 0.0020174594022747544\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02297967556036181\n",
      "Average test loss: 0.001990325530577037\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023017355596025784\n",
      "Average test loss: 0.0020067686221251885\n",
      "Epoch 244/300\n",
      "Average training loss: 0.022995727435582213\n",
      "Average test loss: 0.002035159906372428\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022999192179905044\n",
      "Average test loss: 0.0019927617131421963\n",
      "Epoch 246/300\n",
      "Average training loss: 0.022997099761333732\n",
      "Average test loss: 0.0020541132789933018\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02297366385989719\n",
      "Average test loss: 0.00204215352382097\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02298432480626636\n",
      "Average test loss: 0.002018188168242988\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023017956028381983\n",
      "Average test loss: 0.001999991896458798\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022936749890446664\n",
      "Average test loss: 0.001986833739818798\n",
      "Epoch 251/300\n",
      "Average training loss: 0.022908615867296856\n",
      "Average test loss: 0.0019807256732342973\n",
      "Epoch 252/300\n",
      "Average training loss: 0.022943577589260206\n",
      "Average test loss: 0.002080426544882357\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022981067577997845\n",
      "Average test loss: 0.0020121456711656516\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02291955801182323\n",
      "Average test loss: 0.0020831592720415857\n",
      "Epoch 255/300\n",
      "Average training loss: 0.022870933052566315\n",
      "Average test loss: 0.002044897112891906\n",
      "Epoch 256/300\n",
      "Average training loss: 0.022935883533623483\n",
      "Average test loss: 0.002044403655661477\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022883178073498937\n",
      "Average test loss: 0.00203586301776684\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02288112763232655\n",
      "Average test loss: 0.001996123521588743\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02283112360205915\n",
      "Average test loss: 0.001995918501996332\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022877883553504945\n",
      "Average test loss: 0.0020597893132103815\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022853622317314148\n",
      "Average test loss: 0.0019739323935161032\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022836199624670876\n",
      "Average test loss: 0.002152353671586348\n",
      "Epoch 263/300\n",
      "Average training loss: 0.022832511625356145\n",
      "Average test loss: 0.0020002083320998483\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022849015365044276\n",
      "Average test loss: 0.002030772667792108\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02285971074965265\n",
      "Average test loss: 0.0019908353907780517\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022841174986627365\n",
      "Average test loss: 0.002003131258715358\n",
      "Epoch 267/300\n",
      "Average training loss: 0.022813923954963684\n",
      "Average test loss: 0.0020188134936615825\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02279064618051052\n",
      "Average test loss: 0.0020299926373279755\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02281427367693848\n",
      "Average test loss: 0.0020247525225600433\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02278198134733571\n",
      "Average test loss: 0.002010240059501181\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022767091607054073\n",
      "Average test loss: 0.001958858052475585\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022774096145398087\n",
      "Average test loss: 0.0020736570860155753\n",
      "Epoch 273/300\n",
      "Average training loss: 0.022757917907502916\n",
      "Average test loss: 0.0020349405985325576\n",
      "Epoch 274/300\n",
      "Average training loss: 0.022830612343218593\n",
      "Average test loss: 0.0019982653346119657\n",
      "Epoch 275/300\n",
      "Average training loss: 0.022749030381441115\n",
      "Average test loss: 0.002035442873628603\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022755889852841694\n",
      "Average test loss: 0.002031184297365447\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022771351877186034\n",
      "Average test loss: 0.002027681385477384\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02271371806330151\n",
      "Average test loss: 0.002003961975996693\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022737996046741805\n",
      "Average test loss: 0.001996695630562802\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022734133422374726\n",
      "Average test loss: 0.002053541113519006\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022671063433090846\n",
      "Average test loss: 0.0020339441795109046\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022717987792359458\n",
      "Average test loss: 0.0020494524610953197\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02272881252070268\n",
      "Average test loss: 0.0020073850387707353\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022687542532881102\n",
      "Average test loss: 0.002019158188667562\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02269487784968482\n",
      "Average test loss: 0.0020797923689501153\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022655493564075892\n",
      "Average test loss: 0.0019860010128468276\n",
      "Epoch 287/300\n",
      "Average training loss: 0.022655280891391967\n",
      "Average test loss: 0.0019948570465462075\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02264740945233239\n",
      "Average test loss: 0.0020527990041300653\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022658373513155514\n",
      "Average test loss: 0.002042537399360703\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02263194806377093\n",
      "Average test loss: 0.002042918738702105\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022637812337941595\n",
      "Average test loss: 0.001989113321527839\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022606863957312373\n",
      "Average test loss: 0.002067019756262501\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022622995181216132\n",
      "Average test loss: 0.002015704942142798\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022616111967298718\n",
      "Average test loss: 0.002010209932198955\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022616705821620092\n",
      "Average test loss: 0.0020557893021032215\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02262855798502763\n",
      "Average test loss: 0.002043355674586362\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02260837574137582\n",
      "Average test loss: 0.002032264303416014\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02263495362467236\n",
      "Average test loss: 0.002014616205667456\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02258082168466515\n",
      "Average test loss: 0.0020432283147755596\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022632655723227396\n",
      "Average test loss: 0.0020578733678493235\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7490433311992222\n",
      "Average test loss: 0.0035098103595276675\n",
      "Epoch 2/300\n",
      "Average training loss: 0.1653168343173133\n",
      "Average test loss: 0.0029987362960560452\n",
      "Epoch 3/300\n",
      "Average training loss: 0.09628853585322698\n",
      "Average test loss: 0.002685810327529907\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07132789704534742\n",
      "Average test loss: 0.0025428566976139944\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05933754512336519\n",
      "Average test loss: 0.002393599360456897\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05202077127827538\n",
      "Average test loss: 0.002291894211951229\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04699928369787004\n",
      "Average test loss: 0.0022105167779243656\n",
      "Epoch 8/300\n",
      "Average training loss: 0.043332170950041875\n",
      "Average test loss: 0.002064968032555448\n",
      "Epoch 9/300\n",
      "Average training loss: 0.040556828399499256\n",
      "Average test loss: 0.002040733130855693\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03846703032652537\n",
      "Average test loss: 0.00205156087440749\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03677834918101629\n",
      "Average test loss: 0.0018198487971805863\n",
      "Epoch 12/300\n",
      "Average training loss: 0.035371821179986\n",
      "Average test loss: 0.0017995743047859933\n",
      "Epoch 13/300\n",
      "Average training loss: 0.034251817921797434\n",
      "Average test loss: 0.0017535986153201925\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03325872259504265\n",
      "Average test loss: 0.001698472592048347\n",
      "Epoch 15/300\n",
      "Average training loss: 0.032330755210585066\n",
      "Average test loss: 0.001657764916929106\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03154887977490822\n",
      "Average test loss: 0.0016344419092250368\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030778095945715903\n",
      "Average test loss: 0.001593437524098489\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03011725520094236\n",
      "Average test loss: 0.0016394276341630354\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02945428688161903\n",
      "Average test loss: 0.0015479748956859112\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02885932982299063\n",
      "Average test loss: 0.0015354541123120322\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02826909194389979\n",
      "Average test loss: 0.0015648500845871038\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027725746288895608\n",
      "Average test loss: 0.0014807624753771557\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0271629515753852\n",
      "Average test loss: 0.001472062754496518\n",
      "Epoch 24/300\n",
      "Average training loss: 0.026685052778985766\n",
      "Average test loss: 0.0014592380269120137\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026245887301034398\n",
      "Average test loss: 0.0014489574434442653\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025759079673224026\n",
      "Average test loss: 0.001426133150441779\n",
      "Epoch 27/300\n",
      "Average training loss: 0.025424238928490214\n",
      "Average test loss: 0.0014298826713735858\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025020533081558015\n",
      "Average test loss: 0.001399726795653502\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02467489132285118\n",
      "Average test loss: 0.0014004259730378786\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02434165457222197\n",
      "Average test loss: 0.0013824031613767147\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02410833701822493\n",
      "Average test loss: 0.0013802045272249314\n",
      "Epoch 32/300\n",
      "Average training loss: 0.023805374334255853\n",
      "Average test loss: 0.0013566449872321553\n",
      "Epoch 33/300\n",
      "Average training loss: 0.023601697329017853\n",
      "Average test loss: 0.001346568503210114\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02338561350024409\n",
      "Average test loss: 0.0013481780955981878\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0232184595796797\n",
      "Average test loss: 0.0013535791544450654\n",
      "Epoch 36/300\n",
      "Average training loss: 0.022982456387745008\n",
      "Average test loss: 0.0013360565653484729\n",
      "Epoch 37/300\n",
      "Average training loss: 0.022803898930549623\n",
      "Average test loss: 0.0013340491915328635\n",
      "Epoch 38/300\n",
      "Average training loss: 0.022610742602083417\n",
      "Average test loss: 0.0013316286731925276\n",
      "Epoch 39/300\n",
      "Average training loss: 0.022450897998279994\n",
      "Average test loss: 0.0013288160390737985\n",
      "Epoch 40/300\n",
      "Average training loss: 0.022330265627966986\n",
      "Average test loss: 0.0013166849751853281\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02217382403380341\n",
      "Average test loss: 0.0013272475527806415\n",
      "Epoch 42/300\n",
      "Average training loss: 0.022084570245610342\n",
      "Average test loss: 0.0013361565242004064\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02197510948942767\n",
      "Average test loss: 0.0013100032106352349\n",
      "Epoch 44/300\n",
      "Average training loss: 0.021825784139335155\n",
      "Average test loss: 0.001312448395933542\n",
      "Epoch 45/300\n",
      "Average training loss: 0.021703903923432032\n",
      "Average test loss: 0.00130315090291616\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02161909258696768\n",
      "Average test loss: 0.0013042672054014272\n",
      "Epoch 47/300\n",
      "Average training loss: 0.021521615132689475\n",
      "Average test loss: 0.0012949326808253923\n",
      "Epoch 48/300\n",
      "Average training loss: 0.021438147569696107\n",
      "Average test loss: 0.001311102366902762\n",
      "Epoch 49/300\n",
      "Average training loss: 0.021354780664874447\n",
      "Average test loss: 0.001386789429260211\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0212499133290516\n",
      "Average test loss: 0.0013231433886620734\n",
      "Epoch 51/300\n",
      "Average training loss: 0.021125251149137814\n",
      "Average test loss: 0.0012843538094311953\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02108313076198101\n",
      "Average test loss: 0.0013107035134712027\n",
      "Epoch 53/300\n",
      "Average training loss: 0.020960784824358092\n",
      "Average test loss: 0.0012842328493586845\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02091397260626157\n",
      "Average test loss: 0.001291411381525298\n",
      "Epoch 55/300\n",
      "Average training loss: 0.020824787674678696\n",
      "Average test loss: 0.001281700398048593\n",
      "Epoch 56/300\n",
      "Average training loss: 0.020821251187059613\n",
      "Average test loss: 0.0013004781173335182\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02071904416216744\n",
      "Average test loss: 0.0012852787127097447\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02064192944433954\n",
      "Average test loss: 0.0012902689831745294\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02054477475417985\n",
      "Average test loss: 0.0012923629828211333\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020449251832233534\n",
      "Average test loss: 0.001270311005310052\n",
      "Epoch 61/300\n",
      "Average training loss: 0.020407540503475402\n",
      "Average test loss: 0.0012808429847160974\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020344025891688135\n",
      "Average test loss: 0.0012921862814885875\n",
      "Epoch 63/300\n",
      "Average training loss: 0.020258745595812797\n",
      "Average test loss: 0.0012785828361908595\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02022679390509923\n",
      "Average test loss: 0.0012952845388402542\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020207358742753666\n",
      "Average test loss: 0.001281308729408516\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02011339971754286\n",
      "Average test loss: 0.001273751894529495\n",
      "Epoch 67/300\n",
      "Average training loss: 0.020106679151455562\n",
      "Average test loss: 0.0012873230905582507\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019967589527368546\n",
      "Average test loss: 0.0012713626390323043\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019888537612226274\n",
      "Average test loss: 0.0012965855398732755\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01987128747668531\n",
      "Average test loss: 0.001309072012702624\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019843842080897754\n",
      "Average test loss: 0.001272172905659924\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019773016596833864\n",
      "Average test loss: 0.0012920340841843022\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01977248553103871\n",
      "Average test loss: 0.0012722238201854958\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01963683683839109\n",
      "Average test loss: 0.001276796933470501\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019600780460569595\n",
      "Average test loss: 0.0013452970889500447\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019548664155933593\n",
      "Average test loss: 0.001279457296865682\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01949462591442797\n",
      "Average test loss: 0.0013009246044999195\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019429164957669047\n",
      "Average test loss: 0.001298982717614207\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01941748748885261\n",
      "Average test loss: 0.0012778221955264193\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01936626811656687\n",
      "Average test loss: 0.0012872647906964024\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019282505407101577\n",
      "Average test loss: 0.0012842113960327374\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01923388319545322\n",
      "Average test loss: 0.0012872184430145555\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019250999331474304\n",
      "Average test loss: 0.001295793305358125\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01916983170145088\n",
      "Average test loss: 0.0012892100500563781\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019156567904684278\n",
      "Average test loss: 0.0013302095147470633\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019117128433452712\n",
      "Average test loss: 0.0013884228283746376\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019043232211636174\n",
      "Average test loss: 0.0012973559489473702\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01898953917870919\n",
      "Average test loss: 0.001290565242059529\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018942684934371047\n",
      "Average test loss: 0.0013494095764019422\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01890610711524884\n",
      "Average test loss: 0.0013741281499258346\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01885310069306029\n",
      "Average test loss: 0.0013102241503074766\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01880266523361206\n",
      "Average test loss: 0.0012909323846300442\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018782303911944232\n",
      "Average test loss: 0.0013904706064818634\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01875822821756204\n",
      "Average test loss: 0.0013254118988083468\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018751161974337367\n",
      "Average test loss: 0.0013083216914803618\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01881422127907475\n",
      "Average test loss: 0.0013611565142249067\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018644754111766814\n",
      "Average test loss: 0.0012996616425613563\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018603331302603086\n",
      "Average test loss: 0.001289122823625803\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018569984704256057\n",
      "Average test loss: 0.006761454489082098\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018530790598028236\n",
      "Average test loss: 0.0012937341192737221\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018497551436225574\n",
      "Average test loss: 0.0013069605070890652\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018450799795488516\n",
      "Average test loss: 0.001329391817872723\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018448575569523704\n",
      "Average test loss: 0.0012933151193170085\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01839085527923372\n",
      "Average test loss: 0.0013149119953935345\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01837103904535373\n",
      "Average test loss: 0.001319624917788638\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018330376288129224\n",
      "Average test loss: 0.001330745332667397\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018290257781744004\n",
      "Average test loss: 0.001317483820952475\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018275413336025344\n",
      "Average test loss: 0.0013494701943256788\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018279710800283486\n",
      "Average test loss: 0.0013343941911330654\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018208864897075627\n",
      "Average test loss: 0.00135147152064989\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018161786888208655\n",
      "Average test loss: 0.0013485141340643168\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01812529480581482\n",
      "Average test loss: 0.0013537189858034253\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018116773172385164\n",
      "Average test loss: 0.0014963161745626065\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01808924545844396\n",
      "Average test loss: 0.0013117712189753851\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018067371961143282\n",
      "Average test loss: 0.0013390192641980119\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01803794669277138\n",
      "Average test loss: 0.001353222720324993\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018020684035287962\n",
      "Average test loss: 0.0013166184277377195\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018004152287211684\n",
      "Average test loss: 0.001356057094772243\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017970445508758228\n",
      "Average test loss: 0.0013336665017737283\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017914444719751677\n",
      "Average test loss: 0.0013233785318831602\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017901330850190588\n",
      "Average test loss: 0.0013538625662008093\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017878680818610722\n",
      "Average test loss: 0.0013378367692025172\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017880225635237165\n",
      "Average test loss: 0.0013256939831707212\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017856323406100272\n",
      "Average test loss: 0.0013489609240657752\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017818735487759113\n",
      "Average test loss: 0.0027168164471578268\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01779181843333774\n",
      "Average test loss: 0.0013275529891252518\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017768128897580837\n",
      "Average test loss: 0.0013558695054509573\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017749921064409944\n",
      "Average test loss: 0.0013783327895734046\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017721781633794308\n",
      "Average test loss: 0.001360658181986461\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01769833521462149\n",
      "Average test loss: 0.0013284724262646503\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017704809678097566\n",
      "Average test loss: 0.0013336701976756256\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017678843720091715\n",
      "Average test loss: 0.0013546029954320854\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017678423608342805\n",
      "Average test loss: 0.001325064772636526\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017605885307821964\n",
      "Average test loss: 0.0013760789758525788\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01761191677633259\n",
      "Average test loss: 0.0013682713101928433\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017543005605538686\n",
      "Average test loss: 0.0013232833746199806\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0175611261319783\n",
      "Average test loss: 0.001346101473706464\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017541185377372637\n",
      "Average test loss: 0.0013602104980705513\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017515781198110845\n",
      "Average test loss: 0.0013585204400329127\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017523932906488578\n",
      "Average test loss: 0.0013285925032363998\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01755336091087924\n",
      "Average test loss: 0.001365917307531668\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01746021587484413\n",
      "Average test loss: 0.0013449094706835846\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017418773553437655\n",
      "Average test loss: 0.0014948641160089109\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01739928798874219\n",
      "Average test loss: 0.001323068369179964\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017438610965179072\n",
      "Average test loss: 0.0013350007721843818\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017431846842997603\n",
      "Average test loss: 0.0013298230942131745\n",
      "Epoch 147/300\n",
      "Average training loss: 0.017375533236397636\n",
      "Average test loss: 0.0013326266740138332\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017335959889822537\n",
      "Average test loss: 0.0013937090714979503\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017336555630796485\n",
      "Average test loss: 0.0013790752625920707\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01734573382553127\n",
      "Average test loss: 0.0013878810697028207\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01731033614774545\n",
      "Average test loss: 0.0013678700386856994\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017646254447599253\n",
      "Average test loss: 0.0013410209306619235\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017296470310952928\n",
      "Average test loss: 0.0013584920073755912\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017242901246580813\n",
      "Average test loss: 0.0013411388636256258\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01721209616959095\n",
      "Average test loss: 0.002116207888556851\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01723092875546879\n",
      "Average test loss: 0.0013535987137713367\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017220153037044737\n",
      "Average test loss: 0.0013906394335855212\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01721372141275141\n",
      "Average test loss: 0.0013881964764247338\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01717329222626156\n",
      "Average test loss: 0.0013756862161681056\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01717682529654768\n",
      "Average test loss: 0.001369366267696023\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017159577616386944\n",
      "Average test loss: 0.0013655635484804709\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017155444813271362\n",
      "Average test loss: 0.0013647428463316627\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017115214810603196\n",
      "Average test loss: 0.10052549130386776\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01717771802428696\n",
      "Average test loss: 0.0013719136108540826\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017085287807716263\n",
      "Average test loss: 0.001357373790918953\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01709266221937206\n",
      "Average test loss: 0.0013967601589651572\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017080119495590528\n",
      "Average test loss: 0.0014206732351125942\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01704071224398083\n",
      "Average test loss: 0.0013777417124559482\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01705643719683091\n",
      "Average test loss: 0.0013899945891979668\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016995833516120912\n",
      "Average test loss: 0.002433176364335749\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017027425836357805\n",
      "Average test loss: 0.00146412037147416\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017019227455059686\n",
      "Average test loss: 0.0013669778200694257\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016988383861051667\n",
      "Average test loss: 0.0013846612534382278\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01697319497499201\n",
      "Average test loss: 0.0013744618594129052\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016964287413491142\n",
      "Average test loss: 0.0013660050360485912\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0169376736101177\n",
      "Average test loss: 0.001369485380438467\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016950974146525064\n",
      "Average test loss: 0.0013710007215332655\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01696436871505446\n",
      "Average test loss: 0.0014268080019909475\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0168853722512722\n",
      "Average test loss: 0.0013552713622856471\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01687699077443944\n",
      "Average test loss: 0.0013901294463624556\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016929788672261768\n",
      "Average test loss: 0.0014949746371971237\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0168738453131583\n",
      "Average test loss: 0.001397528329346743\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016863887218965423\n",
      "Average test loss: 0.0014122832624448671\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01688841387298372\n",
      "Average test loss: 0.0013887778693396184\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016832444260517755\n",
      "Average test loss: 0.0013998659507681926\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016865169301629065\n",
      "Average test loss: 0.0014509491976350546\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016831894257002405\n",
      "Average test loss: 0.0014102855477896002\n",
      "Epoch 188/300\n",
      "Average training loss: 0.016809084876543947\n",
      "Average test loss: 0.0013383355830899543\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01685623543626732\n",
      "Average test loss: 0.0014216083094684615\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016792901338802443\n",
      "Average test loss: 0.001387309181296991\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0167572955555386\n",
      "Average test loss: 0.001413147657074862\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01676571542190181\n",
      "Average test loss: 0.0016081675601502259\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016754313883682093\n",
      "Average test loss: 0.0013820646952630744\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016730540808704165\n",
      "Average test loss: 0.0013871673768282764\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016734014674193328\n",
      "Average test loss: 0.0014230382019239996\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016735267895791266\n",
      "Average test loss: 0.001449664042589979\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01673404458661874\n",
      "Average test loss: 0.0013848217487749126\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016732380541248454\n",
      "Average test loss: 0.001381394636662056\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016699663624167443\n",
      "Average test loss: 0.0014312007795605395\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016664931944674916\n",
      "Average test loss: 0.0013851649346244004\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016673288841214443\n",
      "Average test loss: 0.0013919282826698489\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016715490396651957\n",
      "Average test loss: 0.0013863774679808154\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016665239674349627\n",
      "Average test loss: 0.0015700879057662354\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01664412085711956\n",
      "Average test loss: 0.001369373998346014\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016668123742772472\n",
      "Average test loss: 0.0013993242105676067\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01661819965640704\n",
      "Average test loss: 0.00139507653667695\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016640218751298057\n",
      "Average test loss: 0.0014153218805376027\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016609006588657698\n",
      "Average test loss: 0.001407266203676247\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016607674249344402\n",
      "Average test loss: 0.0014417850356549025\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016622900812990136\n",
      "Average test loss: 0.001404685188188321\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016596743106014198\n",
      "Average test loss: 0.0014214314470688502\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016567358188331126\n",
      "Average test loss: 0.001814907149101297\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016561812647514874\n",
      "Average test loss: 0.0014189574450461401\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016590018197894095\n",
      "Average test loss: 0.0013942668065428734\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016556656242244772\n",
      "Average test loss: 0.0013902383831640084\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016533204280667835\n",
      "Average test loss: 0.0013895631429946256\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016536251405874888\n",
      "Average test loss: 0.0015749919802571337\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016542353195448715\n",
      "Average test loss: 0.0014305016675239637\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01653420722981294\n",
      "Average test loss: 0.00143911005411711\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016493911986549696\n",
      "Average test loss: 0.0013621873290588459\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016547125910719235\n",
      "Average test loss: 0.0013918437858422598\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016504531933201685\n",
      "Average test loss: 0.0013716575188769235\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01648144841194153\n",
      "Average test loss: 0.0014102742007623117\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01646284532505605\n",
      "Average test loss: 0.0014304602530060542\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0164847568952375\n",
      "Average test loss: 0.0014573684425817596\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01643831206030316\n",
      "Average test loss: 0.0014632688791801531\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016477208336194357\n",
      "Average test loss: 0.0013972858225719796\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016435758663548364\n",
      "Average test loss: 0.0014268543215778967\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016448562232984437\n",
      "Average test loss: 0.001406750975901054\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016428418878051968\n",
      "Average test loss: 0.0014225679512860046\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01644009843799803\n",
      "Average test loss: 0.001488040317263868\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016429314457707934\n",
      "Average test loss: 0.0014219406293705106\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016434967123799854\n",
      "Average test loss: 0.0014157335761313637\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016419988421930206\n",
      "Average test loss: 0.0013985957775471938\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01636984509560797\n",
      "Average test loss: 0.0015093544252320296\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016417105336983998\n",
      "Average test loss: 0.001428104519740575\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01636462527513504\n",
      "Average test loss: 0.0014104396146722138\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01635366118699312\n",
      "Average test loss: 0.001403440251532528\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016397309058242375\n",
      "Average test loss: 0.0013996805428630776\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01637946468094985\n",
      "Average test loss: 0.0014233715451943377\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016350005842745305\n",
      "Average test loss: 0.0014146267044771877\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01635133442075716\n",
      "Average test loss: 0.001471926708291802\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016398414954543115\n",
      "Average test loss: 0.0014139129457891816\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016334108793901073\n",
      "Average test loss: 0.0014018284546521802\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016315753140383298\n",
      "Average test loss: 0.0014066293854266404\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016324143251611127\n",
      "Average test loss: 0.0013912093373429444\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016310021460884148\n",
      "Average test loss: 0.0014215053082443773\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016380873978965812\n",
      "Average test loss: 0.0014044064754206273\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01630011398676369\n",
      "Average test loss: 0.001382570747906963\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016312483595477212\n",
      "Average test loss: 0.0014284186908561322\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016293424845569664\n",
      "Average test loss: 0.0014048342728573415\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016312276264031728\n",
      "Average test loss: 0.0013864106203739842\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016270200352701877\n",
      "Average test loss: 0.00138340666403787\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016269904933869837\n",
      "Average test loss: 0.0014055224378696745\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016268214689360724\n",
      "Average test loss: 0.0026047889335701862\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016280145349601906\n",
      "Average test loss: 0.001492041155592435\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016283802861140832\n",
      "Average test loss: 0.001413043861794803\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01622757694621881\n",
      "Average test loss: 0.0014236798396127092\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016281638738181857\n",
      "Average test loss: 0.0014243440142729215\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016255724855595163\n",
      "Average test loss: 0.0013988008366690743\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016209606469505363\n",
      "Average test loss: 0.001492696703515119\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016206695354647107\n",
      "Average test loss: 0.0014372296146531072\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01622637986391783\n",
      "Average test loss: 0.0014387829957736864\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016214955444137254\n",
      "Average test loss: 0.0014049539417028427\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016198580847846138\n",
      "Average test loss: 0.0014476860005718966\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016201621502637865\n",
      "Average test loss: 0.0014125070349416799\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01618330113010274\n",
      "Average test loss: 0.0013935204455628992\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016194400232699184\n",
      "Average test loss: 0.0014259660415765311\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016188062564366393\n",
      "Average test loss: 0.0014208145841128296\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016183736988239817\n",
      "Average test loss: 0.0014137442670762539\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016189179215166303\n",
      "Average test loss: 0.0014736137568122812\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01616068012929625\n",
      "Average test loss: 0.0016045370638991395\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01616688200334708\n",
      "Average test loss: 0.0014651622740137907\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0161724151753717\n",
      "Average test loss: 0.001382795104963912\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016160830712980693\n",
      "Average test loss: 0.0014589606902251642\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016135791323251193\n",
      "Average test loss: 0.001399213889406787\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016155012174612945\n",
      "Average test loss: 0.0014357699621468782\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016127182059817845\n",
      "Average test loss: 0.0014093894774301185\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016135446159376038\n",
      "Average test loss: 0.00214340572949085\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016138877080546486\n",
      "Average test loss: 0.0014060430611587233\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016134852935870487\n",
      "Average test loss: 0.001388796606515017\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016134008852144083\n",
      "Average test loss: 0.0014367928374041286\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01612888350089391\n",
      "Average test loss: 0.0014430837094680303\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0160997495452563\n",
      "Average test loss: 0.0014514976281465755\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016087864701946578\n",
      "Average test loss: 0.0014941433023454415\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016092052271796598\n",
      "Average test loss: 0.001413246461500724\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016075981615318192\n",
      "Average test loss: 0.001406600873813861\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016125392543772855\n",
      "Average test loss: 0.0013999908772400684\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016052390053868295\n",
      "Average test loss: 0.0014315789916242163\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01606137872901228\n",
      "Average test loss: 0.0014005553795852594\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016064362689852715\n",
      "Average test loss: 0.0014253830220550298\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01607083585444424\n",
      "Average test loss: 0.0014486916033654576\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016062039436565506\n",
      "Average test loss: 0.0014371343772961861\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016073741283681657\n",
      "Average test loss: 0.001585745473495788\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016035519319276013\n",
      "Average test loss: 0.0014475442338217463\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016040477875206207\n",
      "Average test loss: 0.0014128189242134492\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016056449682348303\n",
      "Average test loss: 0.0014181007558169464\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016057954569657643\n",
      "Average test loss: 0.0014007973089400264\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01603435380425718\n",
      "Average test loss: 0.0014435932798725036\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016015976492729452\n",
      "Average test loss: 0.0014572214020623102\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_64_Depth5/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.59\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.65\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.39\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.66\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.11\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.41\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.46\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.28\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.65\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.54\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.99\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 33.27\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.04\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.64\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.94\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 34.04\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.27\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.32\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.078387171851265\n",
      "Average test loss: 0.0052490746279557545\n",
      "Epoch 2/300\n",
      "Average training loss: 4.137498062557644\n",
      "Average test loss: 0.2496575379371643\n",
      "Epoch 3/300\n",
      "Average training loss: 3.024800004747179\n",
      "Average test loss: 0.0044553301942845186\n",
      "Epoch 4/300\n",
      "Average training loss: 2.346044056150648\n",
      "Average test loss: 0.004626139213641484\n",
      "Epoch 5/300\n",
      "Average training loss: 1.9232318012449476\n",
      "Average test loss: 0.0042338348047600855\n",
      "Epoch 6/300\n",
      "Average training loss: 1.5976563958062067\n",
      "Average test loss: 0.004172481550110711\n",
      "Epoch 7/300\n",
      "Average training loss: 1.3425885925292969\n",
      "Average test loss: 0.004205736429740985\n",
      "Epoch 8/300\n",
      "Average training loss: 1.1375107164382934\n",
      "Average test loss: 0.0041070586445017\n",
      "Epoch 9/300\n",
      "Average training loss: 0.9693000681665208\n",
      "Average test loss: 0.004093284447780914\n",
      "Epoch 10/300\n",
      "Average training loss: 0.83380724785063\n",
      "Average test loss: 0.004090668128803372\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6863805802133348\n",
      "Average test loss: 0.004020793549923433\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5678800729380714\n",
      "Average test loss: 0.003975381518403689\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4709026091628604\n",
      "Average test loss: 0.003946684416383505\n",
      "Epoch 14/300\n",
      "Average training loss: 0.3991420447561476\n",
      "Average test loss: 0.003957137546191613\n",
      "Epoch 15/300\n",
      "Average training loss: 0.343544460773468\n",
      "Average test loss: 0.003919628544400136\n",
      "Epoch 16/300\n",
      "Average training loss: 0.30118830818600123\n",
      "Average test loss: 0.0039026331996752156\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2669736788272858\n",
      "Average test loss: 0.00391230317308671\n",
      "Epoch 18/300\n",
      "Average training loss: 0.23985433615578544\n",
      "Average test loss: 0.0038766165075616705\n",
      "Epoch 19/300\n",
      "Average training loss: 0.21812618430455527\n",
      "Average test loss: 0.003908030241313908\n",
      "Epoch 20/300\n",
      "Average training loss: 0.19987332582473755\n",
      "Average test loss: 0.0038475408487849763\n",
      "Epoch 21/300\n",
      "Average training loss: 0.18513386274708643\n",
      "Average test loss: 0.0038213301536937556\n",
      "Epoch 22/300\n",
      "Average training loss: 0.17300576472282408\n",
      "Average test loss: 0.003818876867492994\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1630938591029909\n",
      "Average test loss: 0.0038647199341406424\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15507527305020227\n",
      "Average test loss: 0.003835726609453559\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1480382981962628\n",
      "Average test loss: 0.003804252484192451\n",
      "Epoch 26/300\n",
      "Average training loss: 0.14238249378734164\n",
      "Average test loss: 0.003813225288151039\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1377004079553816\n",
      "Average test loss: 0.0037767304215166305\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1337554572025935\n",
      "Average test loss: 0.003803641384260522\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13048478729195065\n",
      "Average test loss: 0.0037809721384611393\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1277120919889874\n",
      "Average test loss: 0.003761430766640438\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12543746463457744\n",
      "Average test loss: 0.0037520183918790685\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12350838512182236\n",
      "Average test loss: 0.0037488133930083777\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12199055083592733\n",
      "Average test loss: 0.00373338301314248\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12070364928245544\n",
      "Average test loss: 0.0037490582081178823\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11952117458979289\n",
      "Average test loss: 0.0037127638906240463\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1187401081191169\n",
      "Average test loss: 0.003710769652078549\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11794651339451472\n",
      "Average test loss: 0.0037191347550186845\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11744421664211485\n",
      "Average test loss: 0.003703320964343018\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11681003878513972\n",
      "Average test loss: 0.0037013539369735453\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11633823518620597\n",
      "Average test loss: 0.003717430232092738\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11589637172222138\n",
      "Average test loss: 0.0037033100190262\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11585182401206759\n",
      "Average test loss: 0.003685957009386685\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11508813816971249\n",
      "Average test loss: 0.003686780251562595\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11480709946155548\n",
      "Average test loss: 0.003727054906388124\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11453322523170047\n",
      "Average test loss: 0.0036877997347878086\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11416282094187206\n",
      "Average test loss: 0.0036897965870383714\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11397584939665265\n",
      "Average test loss: 0.003669837597136696\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11358363077375624\n",
      "Average test loss: 0.0036702711985756955\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11336812170346577\n",
      "Average test loss: 0.0037190930489450695\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11307107863823573\n",
      "Average test loss: 0.0036670254001186955\n",
      "Epoch 51/300\n",
      "Average training loss: 0.112923877765735\n",
      "Average test loss: 0.0036750335209071636\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11258353747924169\n",
      "Average test loss: 0.003662013545839323\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11234742035468419\n",
      "Average test loss: 0.0037811064662204848\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11213949225346247\n",
      "Average test loss: 0.003652186362279786\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1119446590145429\n",
      "Average test loss: 0.0036697167220215003\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11165184328622288\n",
      "Average test loss: 0.003696997681218717\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11159931200080447\n",
      "Average test loss: 0.003652681932681137\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11126246143049665\n",
      "Average test loss: 0.0036457734310792553\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11120164651340908\n",
      "Average test loss: 0.003655624653523167\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11106062501006657\n",
      "Average test loss: 0.0036541677955538037\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1106736612452401\n",
      "Average test loss: 0.0036438555299407906\n",
      "Epoch 62/300\n",
      "Average training loss: 0.1104587739109993\n",
      "Average test loss: 0.003640716760108868\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11029961520764563\n",
      "Average test loss: 0.003652053778991103\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11012199153502782\n",
      "Average test loss: 0.0036657580085512663\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11002752363019519\n",
      "Average test loss: 0.003661928921730982\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10985712516970104\n",
      "Average test loss: 0.0036256090816524293\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10968007006910112\n",
      "Average test loss: 0.003649126066929764\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10950727611117893\n",
      "Average test loss: 0.003643792535074883\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10931656572553847\n",
      "Average test loss: 0.003636020477861166\n",
      "Epoch 70/300\n",
      "Average training loss: 0.10909531807899475\n",
      "Average test loss: 0.0036688734795898197\n",
      "Epoch 71/300\n",
      "Average training loss: 0.10896235413683786\n",
      "Average test loss: 0.0036486233228610622\n",
      "Epoch 72/300\n",
      "Average training loss: 0.10888403424951765\n",
      "Average test loss: 0.00364728933862514\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10873926245503955\n",
      "Average test loss: 0.003669529888571964\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10847796303033828\n",
      "Average test loss: 0.0036643410565124616\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10840588465001848\n",
      "Average test loss: 0.003625401889698373\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10825044391552607\n",
      "Average test loss: 0.0036387927124483716\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10803630823890369\n",
      "Average test loss: 0.0036524286003162465\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10797974606355032\n",
      "Average test loss: 0.0036609693490382697\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1076897101799647\n",
      "Average test loss: 0.0036412080364922684\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10757894613345464\n",
      "Average test loss: 0.003640604496209158\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10746168585618338\n",
      "Average test loss: 0.0037568547104795775\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10727229482597775\n",
      "Average test loss: 0.003640716239396069\n",
      "Epoch 83/300\n",
      "Average training loss: 0.107159894545873\n",
      "Average test loss: 0.0036526550853417977\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10689272326893276\n",
      "Average test loss: 0.003695048928467764\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10679392496082518\n",
      "Average test loss: 0.0036707104858424927\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10660089216629665\n",
      "Average test loss: 0.003647723030298948\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10656925300094816\n",
      "Average test loss: 0.0036614365958505205\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10626604455047184\n",
      "Average test loss: 0.003744160804897547\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10612620582183202\n",
      "Average test loss: 0.0036313476875010466\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10592195241981082\n",
      "Average test loss: 0.003671102022545205\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10594890758064059\n",
      "Average test loss: 0.003671467495461305\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10560398587253358\n",
      "Average test loss: 0.0036599551335804994\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10541168269846174\n",
      "Average test loss: 0.0036626314150376453\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10537461548381381\n",
      "Average test loss: 0.003645744668940703\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10517894223001269\n",
      "Average test loss: 0.003713051856184999\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10493839409616258\n",
      "Average test loss: 0.0036898896594842277\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10483172125286526\n",
      "Average test loss: 0.003662028491910961\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10462691390514374\n",
      "Average test loss: 0.0036912702458600202\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10447750332620409\n",
      "Average test loss: 0.0037085695502658686\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10420321573813757\n",
      "Average test loss: 0.003666430571426948\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10406254767047034\n",
      "Average test loss: 0.003651665447693732\n",
      "Epoch 102/300\n",
      "Average training loss: 0.1039961685207155\n",
      "Average test loss: 0.003704729150152869\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10382784147726165\n",
      "Average test loss: 0.003698992960568931\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10372151021824942\n",
      "Average test loss: 0.003743468107862605\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10344901120000416\n",
      "Average test loss: 0.0037204135871595806\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10337223433123695\n",
      "Average test loss: 0.0037610545547472106\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10322853320174748\n",
      "Average test loss: 0.0037132981051173474\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10291188428799311\n",
      "Average test loss: 0.0037000156831410195\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10276686786943012\n",
      "Average test loss: 0.003714126746273703\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10257546473873987\n",
      "Average test loss: 0.0037143478975113896\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10247218302223418\n",
      "Average test loss: 0.0037465154483086534\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10229375422000885\n",
      "Average test loss: 0.003706230127563079\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10244394985834758\n",
      "Average test loss: 0.0037567910394734807\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10202265365256204\n",
      "Average test loss: 0.0037775012345777617\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10192416505681144\n",
      "Average test loss: 0.0037137203535272017\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10162975489430957\n",
      "Average test loss: 0.0037122812072436016\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10167501425743103\n",
      "Average test loss: 0.0037872917503118516\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10141123422649172\n",
      "Average test loss: 0.0037400761989669668\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10118122582965428\n",
      "Average test loss: 0.0037045986404021583\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10117063585254882\n",
      "Average test loss: 0.0037808205168694256\n",
      "Epoch 121/300\n",
      "Average training loss: 0.1009610286090109\n",
      "Average test loss: 0.003706420417047209\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10071522036525939\n",
      "Average test loss: 0.003764010624753104\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10074186784029007\n",
      "Average test loss: 0.003719339234961404\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10048359877533383\n",
      "Average test loss: 0.0037602432196338974\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10031106179290347\n",
      "Average test loss: 0.0037601537456115088\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10020360149277581\n",
      "Average test loss: 0.003754372940916154\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10009962150785658\n",
      "Average test loss: 0.003766056054375238\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09991325378417969\n",
      "Average test loss: 0.0038060750232802496\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09977970550457636\n",
      "Average test loss: 0.0038377529388914505\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09959521609544754\n",
      "Average test loss: 0.0037990454557455247\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09945002561807632\n",
      "Average test loss: 0.0037652130102117857\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09962301479445564\n",
      "Average test loss: 0.0037491089397420486\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0991838909122679\n",
      "Average test loss: 0.003743742513159911\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09918235757615831\n",
      "Average test loss: 0.003785876334127453\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0991171811554167\n",
      "Average test loss: 0.003832187866171201\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0989097289244334\n",
      "Average test loss: 0.003818707376718521\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09891999679141575\n",
      "Average test loss: 0.003854730994751056\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09866799763176176\n",
      "Average test loss: 0.003764020127761695\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09845984347661337\n",
      "Average test loss: 0.003792029515736633\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09835440413157145\n",
      "Average test loss: 0.0038394810180697177\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0982435161670049\n",
      "Average test loss: 0.0037723115239706303\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09810182498892148\n",
      "Average test loss: 0.0038406950169139436\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09818121050463782\n",
      "Average test loss: 0.003847971547394991\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09790757187869814\n",
      "Average test loss: 0.003864093787761198\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09775849419832229\n",
      "Average test loss: 0.0037772393702632853\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09787229061788982\n",
      "Average test loss: 0.0038699137959629297\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09758382068077723\n",
      "Average test loss: 0.003823921486528383\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09755344992876053\n",
      "Average test loss: 0.0037821523770689962\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09735753269990285\n",
      "Average test loss: 0.003784468851569626\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0971969472501013\n",
      "Average test loss: 0.003811031949809856\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09722446013159222\n",
      "Average test loss: 0.003843772705230448\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09704516958528095\n",
      "Average test loss: 0.004180533451959491\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09701890748739242\n",
      "Average test loss: 0.0038701668741802373\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09688710159063339\n",
      "Average test loss: 0.003859091328870919\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09682304749886195\n",
      "Average test loss: 0.003956279809069302\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09663805458943049\n",
      "Average test loss: 0.003794176912970013\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09645744071735277\n",
      "Average test loss: 0.003818070644926694\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09647973167233997\n",
      "Average test loss: 0.0039004365069170792\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09680602683623632\n",
      "Average test loss: 0.003817552193171448\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0961887918445799\n",
      "Average test loss: 0.0037797742682612605\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09615867351161109\n",
      "Average test loss: 0.003927925899831785\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09589897206756803\n",
      "Average test loss: 0.003869352988484833\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09608341987927754\n",
      "Average test loss: 0.003862265206873417\n",
      "Epoch 164/300\n",
      "Average training loss: 0.096205197930336\n",
      "Average test loss: 0.0038221462793234322\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09572726654344135\n",
      "Average test loss: 0.0038549778186198736\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09573693628443612\n",
      "Average test loss: 0.0038928911214073498\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09567104117075602\n",
      "Average test loss: 0.003803329359119137\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09555261237091488\n",
      "Average test loss: 0.003827023573633697\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09536802301141951\n",
      "Average test loss: 0.0038660619031223985\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09529512084854974\n",
      "Average test loss: 0.0038342835667232672\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09526400921742122\n",
      "Average test loss: 0.0038784902489019763\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09529935285117891\n",
      "Average test loss: 0.003822772935239805\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09515164697832532\n",
      "Average test loss: 0.0038435110217995114\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0950247154434522\n",
      "Average test loss: 0.003853188352659345\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09503450561894311\n",
      "Average test loss: 0.0039023282629334264\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09489891150924895\n",
      "Average test loss: 0.003851103658684426\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09477732515997357\n",
      "Average test loss: 0.00387621392061313\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09481949792305629\n",
      "Average test loss: 0.0038214758729769125\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0946244804461797\n",
      "Average test loss: 0.003797342391891612\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09469341150257322\n",
      "Average test loss: 0.003932167596080237\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09452536477645239\n",
      "Average test loss: 0.003869779710140493\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09439892350302802\n",
      "Average test loss: 0.0038784720746593343\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09427434260315365\n",
      "Average test loss: 0.0038782544587221413\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09430229713519414\n",
      "Average test loss: 0.0038971414365288285\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09417623430490493\n",
      "Average test loss: 0.003926494164806274\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09411136877536774\n",
      "Average test loss: 0.003914151838670174\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09406646889448166\n",
      "Average test loss: 0.0039266820860405765\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0940082602765825\n",
      "Average test loss: 0.003887194467915429\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0938660672571924\n",
      "Average test loss: 0.003870436343467898\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09407517355018191\n",
      "Average test loss: 0.003845523616919915\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09375762674543593\n",
      "Average test loss: 0.0038513699558873973\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09372436119450463\n",
      "Average test loss: 0.003896892436676555\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09349314562148518\n",
      "Average test loss: 0.003938465408980846\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09352230508459938\n",
      "Average test loss: 0.0038573995892786316\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09347796246740553\n",
      "Average test loss: 0.003867607650657495\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09333676638205846\n",
      "Average test loss: 0.003943404218388929\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09342825325992372\n",
      "Average test loss: 0.003887445448173417\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09345011110438241\n",
      "Average test loss: 0.003867114933828513\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09311414682202869\n",
      "Average test loss: 0.003919648916357093\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09312782015403112\n",
      "Average test loss: 0.0039036245176361666\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09319320168097814\n",
      "Average test loss: 0.0038972189728584553\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09301074267095989\n",
      "Average test loss: 0.0038823259065134657\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09301968529489306\n",
      "Average test loss: 0.003944997744841708\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09283607866366704\n",
      "Average test loss: 0.003981975556040803\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0929112991756863\n",
      "Average test loss: 0.003953767778144942\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09277414729860094\n",
      "Average test loss: 0.003920212003505892\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09287926151686245\n",
      "Average test loss: 0.004136996949298514\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09278785402907265\n",
      "Average test loss: 0.0038813937509225476\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09262756623824438\n",
      "Average test loss: 0.0038772658788495595\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09245702337556415\n",
      "Average test loss: 0.0039041807937125364\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09254067609707514\n",
      "Average test loss: 0.004296880824284421\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09265596280495325\n",
      "Average test loss: 0.003997300359523959\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09244701810346709\n",
      "Average test loss: 0.003953012777285443\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09228588941362169\n",
      "Average test loss: 0.004039073204725153\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09223804818259346\n",
      "Average test loss: 0.00390466036212941\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09207300509346857\n",
      "Average test loss: 0.0038899203687906263\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09216312330298954\n",
      "Average test loss: 0.004029768254607916\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09219019265307321\n",
      "Average test loss: 0.003893345441876186\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09210028937127855\n",
      "Average test loss: 0.0038791048804091084\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09200277920564015\n",
      "Average test loss: 0.003940770109701488\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09184362700250413\n",
      "Average test loss: 0.00389762045443058\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09183732997046577\n",
      "Average test loss: 0.0038926780029303496\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09181681974728902\n",
      "Average test loss: 0.003966787666082382\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09169838174846437\n",
      "Average test loss: 0.0038981732204556464\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09171384486887191\n",
      "Average test loss: 0.003916897926893499\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09170992082357407\n",
      "Average test loss: 0.0038769505582749845\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0914862576921781\n",
      "Average test loss: 0.0038280352407859433\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09158156274424659\n",
      "Average test loss: 0.0039431806618554725\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09141395492023892\n",
      "Average test loss: 0.00388768376244439\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09146058419015672\n",
      "Average test loss: 0.003945378004262845\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09134049378501044\n",
      "Average test loss: 0.0038711828330738676\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09137382281488843\n",
      "Average test loss: 0.0038854675537182224\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09128498475088014\n",
      "Average test loss: 0.003950654610784517\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09118634767664803\n",
      "Average test loss: 0.003904677267186344\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09124416502316793\n",
      "Average test loss: 0.004003125455437435\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09114846904410256\n",
      "Average test loss: 0.003925763073791232\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0912036241028044\n",
      "Average test loss: 0.003865562746508254\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0911042536828253\n",
      "Average test loss: 0.0039438037950959475\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09100541688336267\n",
      "Average test loss: 0.003957090068608522\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09114199713865916\n",
      "Average test loss: 0.00383804827907847\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09085234934091568\n",
      "Average test loss: 0.003909796832750241\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09088515512148539\n",
      "Average test loss: 0.00401481034523911\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09081976579295264\n",
      "Average test loss: 0.0039001013442046114\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09073165384266112\n",
      "Average test loss: 0.00390841635171738\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09072233580218421\n",
      "Average test loss: 0.003952101458071007\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09055983361932966\n",
      "Average test loss: 0.0038952728838970263\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09106327301926083\n",
      "Average test loss: 0.003898648843169212\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09055817293458515\n",
      "Average test loss: 0.003936247399904662\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09048973578214646\n",
      "Average test loss: 0.003923611513856384\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09046202719211578\n",
      "Average test loss: 0.004194683689624071\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0904554556409518\n",
      "Average test loss: 0.003964128638721175\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0905120012693935\n",
      "Average test loss: 0.0038282560942073662\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09032535815901227\n",
      "Average test loss: 0.003963129873077074\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0903691473338339\n",
      "Average test loss: 0.003935759261664417\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09018381606870228\n",
      "Average test loss: 0.004024584885997077\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09018914365106159\n",
      "Average test loss: 0.003860901190381911\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09016239709324307\n",
      "Average test loss: 0.0039037931578026876\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09020480402972963\n",
      "Average test loss: 0.003961708978025449\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0902121862106853\n",
      "Average test loss: 0.00386278493826588\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09004635593626234\n",
      "Average test loss: 0.004016525946143601\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08996486976411608\n",
      "Average test loss: 0.003922816805955437\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08993493649694655\n",
      "Average test loss: 0.00391666763731175\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08987219056155947\n",
      "Average test loss: 0.0038895433499581285\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0898439872595999\n",
      "Average test loss: 0.0038900840481122336\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08986897301011615\n",
      "Average test loss: 0.003972062422169579\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08975084118710625\n",
      "Average test loss: 0.003962833025596208\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08972655255264705\n",
      "Average test loss: 0.003961826011124585\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08970479228099187\n",
      "Average test loss: 0.003919112493594487\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08966955758796798\n",
      "Average test loss: 0.004084235353809264\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08968780185116662\n",
      "Average test loss: 0.0039715078740070265\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0896389137506485\n",
      "Average test loss: 0.003913408332607812\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08960740340418286\n",
      "Average test loss: 0.0039298030831333665\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08962959689564175\n",
      "Average test loss: 0.004006836840055055\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08950681259234747\n",
      "Average test loss: 0.0040433283949063885\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08945646742317412\n",
      "Average test loss: 0.00399599043569631\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08938959563440747\n",
      "Average test loss: 0.003980092274025082\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08947141130765279\n",
      "Average test loss: 0.003889443672158652\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08935671401686139\n",
      "Average test loss: 0.00399457114479608\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08920661846796672\n",
      "Average test loss: 0.004001180426114135\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08925277955002255\n",
      "Average test loss: 0.004052110198264321\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0892447395556503\n",
      "Average test loss: 0.003926615392582284\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08933631357219485\n",
      "Average test loss: 0.004013753715074724\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0890645367834303\n",
      "Average test loss: 0.004058046951476071\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08912738805015882\n",
      "Average test loss: 0.003956425107187695\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08907385932074653\n",
      "Average test loss: 0.003881970444901122\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08911844133668476\n",
      "Average test loss: 0.00423837548494339\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08915350918306245\n",
      "Average test loss: 0.003963921242910955\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08895867242415746\n",
      "Average test loss: 0.0038618420081006155\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08883580201201968\n",
      "Average test loss: 0.004052295909987556\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08894006535742018\n",
      "Average test loss: 0.004080542047818502\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08882749501201842\n",
      "Average test loss: 0.004010538334647815\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08887660508023368\n",
      "Average test loss: 0.003873783083425628\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08874717779292\n",
      "Average test loss: 0.003919856936360399\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08884469877349006\n",
      "Average test loss: 0.004036991089375483\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08857223226626713\n",
      "Average test loss: 0.004011424519742529\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0886906657881207\n",
      "Average test loss: 0.003914658151360021\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08861616215772099\n",
      "Average test loss: 0.0039732054695487025\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0885411358806822\n",
      "Average test loss: 0.003928355935340126\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08861263730128606\n",
      "Average test loss: 0.003996413396878375\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08856799673371844\n",
      "Average test loss: 0.0040127595238801505\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 9.474903651979234\n",
      "Average test loss: 0.005291011056966252\n",
      "Epoch 2/300\n",
      "Average training loss: 3.6666247344546847\n",
      "Average test loss: 0.004454137405380607\n",
      "Epoch 3/300\n",
      "Average training loss: 2.709092856089274\n",
      "Average test loss: 0.003902582034882572\n",
      "Epoch 4/300\n",
      "Average training loss: 2.1328156293233236\n",
      "Average test loss: 0.003852895926891102\n",
      "Epoch 5/300\n",
      "Average training loss: 1.7938724263509114\n",
      "Average test loss: 0.003661230837719308\n",
      "Epoch 6/300\n",
      "Average training loss: 1.520341364754571\n",
      "Average test loss: 0.0036060106415922444\n",
      "Epoch 7/300\n",
      "Average training loss: 1.2498087565104166\n",
      "Average test loss: 0.0034762356161243384\n",
      "Epoch 8/300\n",
      "Average training loss: 1.0630022163391113\n",
      "Average test loss: 0.0033978738581968678\n",
      "Epoch 9/300\n",
      "Average training loss: 0.9016013290617201\n",
      "Average test loss: 0.003329770409398609\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7660490593380398\n",
      "Average test loss: 0.003281956654869848\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6499905277358161\n",
      "Average test loss: 0.003279602605228623\n",
      "Epoch 12/300\n",
      "Average training loss: 0.548680093076494\n",
      "Average test loss: 0.0031589533127844333\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4634774210717943\n",
      "Average test loss: 0.0031125556592726043\n",
      "Epoch 14/300\n",
      "Average training loss: 0.39303712060716417\n",
      "Average test loss: 0.003098751567925016\n",
      "Epoch 15/300\n",
      "Average training loss: 0.336909382475747\n",
      "Average test loss: 0.0030020906500932243\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2915264560646481\n",
      "Average test loss: 0.0030811719867504307\n",
      "Epoch 17/300\n",
      "Average training loss: 0.25554257599512736\n",
      "Average test loss: 0.0029747814022832446\n",
      "Epoch 18/300\n",
      "Average training loss: 0.22661874887678357\n",
      "Average test loss: 0.0028958498465104236\n",
      "Epoch 19/300\n",
      "Average training loss: 0.20320544356769984\n",
      "Average test loss: 0.0029234491458369626\n",
      "Epoch 20/300\n",
      "Average training loss: 0.184453272567855\n",
      "Average test loss: 0.0028907753042876723\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16895762574672699\n",
      "Average test loss: 0.002816643468621704\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1557389810681343\n",
      "Average test loss: 0.0028109174720529055\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1447278123696645\n",
      "Average test loss: 0.0027992964235858783\n",
      "Epoch 24/300\n",
      "Average training loss: 0.13576136006249323\n",
      "Average test loss: 0.002763539875547091\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1279437182744344\n",
      "Average test loss: 0.0027399301199863353\n",
      "Epoch 26/300\n",
      "Average training loss: 0.121206125345495\n",
      "Average test loss: 0.002715463801390595\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11561638649304708\n",
      "Average test loss: 0.0027082745366626315\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1101201965411504\n",
      "Average test loss: 0.0027076114024966957\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10598798733287387\n",
      "Average test loss: 0.002644717190414667\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10276345364252726\n",
      "Average test loss: 0.002659543085222443\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10019265547063616\n",
      "Average test loss: 0.002673838501278725\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09811487032969793\n",
      "Average test loss: 0.00264145834164487\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09642358910706308\n",
      "Average test loss: 0.0026321290135383605\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09488624176714155\n",
      "Average test loss: 0.002629769993532035\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09350488659408357\n",
      "Average test loss: 0.0026044249484936397\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0924372014535798\n",
      "Average test loss: 0.0025848983097821474\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09131552788946364\n",
      "Average test loss: 0.00260886783256299\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09030127763085895\n",
      "Average test loss: 0.0025722717673828206\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08941328588459227\n",
      "Average test loss: 0.0025789564978331327\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08855216759443284\n",
      "Average test loss: 0.002586553206874265\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08765969240003162\n",
      "Average test loss: 0.002572858126420114\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08692340923017926\n",
      "Average test loss: 0.0025825547789120012\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08616883909702301\n",
      "Average test loss: 0.0025836116136569117\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08581204106410345\n",
      "Average test loss: 0.0025908769360846943\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0848793564107683\n",
      "Average test loss: 0.002529603228283425\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08425449311733246\n",
      "Average test loss: 0.002545544023315112\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08383261738883124\n",
      "Average test loss: 0.002553637867069079\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08335115425454245\n",
      "Average test loss: 0.0025503333657979966\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08269212925102976\n",
      "Average test loss: 0.00253876332545446\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08217617834276623\n",
      "Average test loss: 0.0025271716641469136\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08172623960177104\n",
      "Average test loss: 0.00254189153181182\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08157719579670164\n",
      "Average test loss: 0.0025139111133499277\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08077613585525088\n",
      "Average test loss: 0.002507111603601111\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08037291201617983\n",
      "Average test loss: 0.002509113607514236\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0800329998201794\n",
      "Average test loss: 0.002520318428468373\n",
      "Epoch 56/300\n",
      "Average training loss: 0.079548545897007\n",
      "Average test loss: 0.0025420814806388485\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07938748911354276\n",
      "Average test loss: 0.0025087555733819804\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07896292503674825\n",
      "Average test loss: 0.002555738331543075\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07855914978848563\n",
      "Average test loss: 0.002529522226192057\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07813857419292132\n",
      "Average test loss: 0.002504790206750234\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07794026169512007\n",
      "Average test loss: 0.0025021222996421985\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07751547840568754\n",
      "Average test loss: 0.002509295481360621\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07720197212033801\n",
      "Average test loss: 0.002493741898280051\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07682618188195758\n",
      "Average test loss: 0.0024905593262778388\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07652779370215204\n",
      "Average test loss: 0.0025268997457913228\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07639245032601887\n",
      "Average test loss: 0.002616790755962332\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07601803898149066\n",
      "Average test loss: 0.0025099820682985916\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07606326631373829\n",
      "Average test loss: 0.00269359109012617\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07548428563939201\n",
      "Average test loss: 0.0025035816488994493\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0751882632639673\n",
      "Average test loss: 0.002552550670897795\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07490315780705877\n",
      "Average test loss: 0.00253957925312635\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0745798067384296\n",
      "Average test loss: 0.002587468661988775\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07434747444258796\n",
      "Average test loss: 0.0025471149447063605\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07413264416986041\n",
      "Average test loss: 0.0025027220299881367\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07387808387478192\n",
      "Average test loss: 0.0025285477384717927\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07387581076224645\n",
      "Average test loss: 0.0025254193474021224\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07334571252597703\n",
      "Average test loss: 0.002498989226710465\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07298630831307835\n",
      "Average test loss: 0.002583416831369201\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07292110256354015\n",
      "Average test loss: 0.0025261105879520376\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07265802646676699\n",
      "Average test loss: 0.002553947290819552\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07232676239146127\n",
      "Average test loss: 0.0025067991595715284\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0721452586054802\n",
      "Average test loss: 0.0025044746523102124\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07184232047200204\n",
      "Average test loss: 0.0025672507108085683\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07163605211178461\n",
      "Average test loss: 0.0026504385124685036\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07155834599998262\n",
      "Average test loss: 0.0025470879367656177\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0711747318804264\n",
      "Average test loss: 0.0025349651240329776\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07084918541378445\n",
      "Average test loss: 0.0025190030451243124\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07079125489460097\n",
      "Average test loss: 0.002511349350007044\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07046249314480357\n",
      "Average test loss: 0.009727281832653615\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07029710714684592\n",
      "Average test loss: 0.002580695321162542\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07005700783597099\n",
      "Average test loss: 0.002536030675595005\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06991406491729948\n",
      "Average test loss: 0.0025878887383474244\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06972060844302178\n",
      "Average test loss: 0.002520586513603727\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0696740184724331\n",
      "Average test loss: 0.002536774290725589\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06925758958856265\n",
      "Average test loss: 0.0026102567917356888\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06897019882996877\n",
      "Average test loss: 0.0025934764246145882\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0691884061396122\n",
      "Average test loss: 0.0025371007720629375\n",
      "Epoch 98/300\n",
      "Average training loss: 0.068828906138738\n",
      "Average test loss: 0.0025714371018111704\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06842283249232504\n",
      "Average test loss: 0.0025586586480753288\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06833754970298873\n",
      "Average test loss: 0.0025638663245158063\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0680826637049516\n",
      "Average test loss: 0.0025967737570818925\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06804850284920798\n",
      "Average test loss: 0.0026446180931395955\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06787203497025701\n",
      "Average test loss: 0.0025458571149243247\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06760919722252422\n",
      "Average test loss: 0.002640230949761139\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06756137502524588\n",
      "Average test loss: 0.0025653563721312416\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06746042043301795\n",
      "Average test loss: 0.0027301244891973007\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06703079192505942\n",
      "Average test loss: 0.0025928726560539668\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06685250978006257\n",
      "Average test loss: 0.0026049844463252357\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06697624053557714\n",
      "Average test loss: 0.0025764471716764902\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06736839047405455\n",
      "Average test loss: 0.002754575644930204\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06659215453267098\n",
      "Average test loss: 0.0032471787540449036\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06633337648047341\n",
      "Average test loss: 0.0026095435032621028\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06619360657201873\n",
      "Average test loss: 0.0027639582419974936\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0661977521147993\n",
      "Average test loss: 0.002590990020169152\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0659925050801701\n",
      "Average test loss: 0.0027291400324967172\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06580901921457714\n",
      "Average test loss: 0.002546045370813873\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0658722985651758\n",
      "Average test loss: 0.002599259975676735\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06548821731739574\n",
      "Average test loss: 0.0026091568149212334\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06553215395410855\n",
      "Average test loss: 0.002683245985768735\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06540208526121245\n",
      "Average test loss: 0.002597509016489817\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06524182589517699\n",
      "Average test loss: 0.0025828286523206364\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0651949034333229\n",
      "Average test loss: 0.002655832814673583\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06503289290600353\n",
      "Average test loss: 0.0025775782941944068\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06469745534658432\n",
      "Average test loss: 0.002577467144570417\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06476241811447674\n",
      "Average test loss: 0.0025883878205592435\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06504401207632489\n",
      "Average test loss: 0.002586697889595396\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06451111572649744\n",
      "Average test loss: 0.0026125057568359704\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0644980863696999\n",
      "Average test loss: 0.002642081471367015\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06432959341009457\n",
      "Average test loss: 0.0026079709293941656\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0642186571624544\n",
      "Average test loss: 0.002679229375802808\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0642035242749585\n",
      "Average test loss: 0.003966820201526085\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06397450098726484\n",
      "Average test loss: 0.0026836640319476525\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06390806606743071\n",
      "Average test loss: 0.002589359741864933\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06375594288110734\n",
      "Average test loss: 0.0027074808296230104\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06391585412952636\n",
      "Average test loss: 0.0026503183647162386\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06376078986128171\n",
      "Average test loss: 0.0026900981149325766\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06370383301046159\n",
      "Average test loss: 0.0026324714779232938\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0635192803144455\n",
      "Average test loss: 0.0026333539059592618\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0632184248831537\n",
      "Average test loss: 0.002635977269874679\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06323323226968447\n",
      "Average test loss: 0.0026816626513997714\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06317210957407951\n",
      "Average test loss: 0.002654558866802189\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06308834377759033\n",
      "Average test loss: 0.0026864189040950604\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06296824822160933\n",
      "Average test loss: 0.0027202065215549535\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0628848067952527\n",
      "Average test loss: 0.002623513394138879\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06286610044704544\n",
      "Average test loss: 0.002706397403238548\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06275174463126394\n",
      "Average test loss: 0.0026368801459256148\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06268623663981755\n",
      "Average test loss: 0.0026345706323368683\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06254285991854137\n",
      "Average test loss: 0.002688305123605662\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06251104508837065\n",
      "Average test loss: 0.002752589505372776\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06273587029675642\n",
      "Average test loss: 0.0027021425563014214\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06253734210133552\n",
      "Average test loss: 0.002713722623263796\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06241142516003715\n",
      "Average test loss: 0.002730058546488484\n",
      "Epoch 153/300\n",
      "Average training loss: 0.062140292124615776\n",
      "Average test loss: 0.0026448532936887608\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06215389381514655\n",
      "Average test loss: 0.0026504717116347617\n",
      "Epoch 155/300\n",
      "Average training loss: 0.062049883564313256\n",
      "Average test loss: 0.0026062428731885223\n",
      "Epoch 156/300\n",
      "Average training loss: 0.062041034688552224\n",
      "Average test loss: 0.002697972746565938\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06193413550655047\n",
      "Average test loss: 0.00278850477333698\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06186759958333439\n",
      "Average test loss: 0.0026483616517442798\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06165019553568628\n",
      "Average test loss: 0.002648842759637369\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06181274651818805\n",
      "Average test loss: 0.0026841829176992176\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06154753865136041\n",
      "Average test loss: 0.002693382014830907\n",
      "Epoch 162/300\n",
      "Average training loss: 0.061586399969127445\n",
      "Average test loss: 0.0033114355638002354\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06150864118668768\n",
      "Average test loss: 0.0026571486350148916\n",
      "Epoch 164/300\n",
      "Average training loss: 0.061390661375390156\n",
      "Average test loss: 0.002767700024156107\n",
      "Epoch 165/300\n",
      "Average training loss: 0.061544003599219854\n",
      "Average test loss: 0.0026174704142742687\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06131717570622762\n",
      "Average test loss: 0.002657485287429558\n",
      "Epoch 167/300\n",
      "Average training loss: 0.061233037269777725\n",
      "Average test loss: 0.002706746763239304\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06129351999693447\n",
      "Average test loss: 0.0026754336539242004\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06120971632666058\n",
      "Average test loss: 0.002649148565820522\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06128822760780652\n",
      "Average test loss: 0.0027513261609193353\n",
      "Epoch 171/300\n",
      "Average training loss: 0.061179504156112674\n",
      "Average test loss: 0.002698983655104207\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0610241877204842\n",
      "Average test loss: 0.0026460961788478824\n",
      "Epoch 173/300\n",
      "Average training loss: 0.060923965301778585\n",
      "Average test loss: 0.0026797181974268624\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06080122150646316\n",
      "Average test loss: 0.0027085017811300026\n",
      "Epoch 175/300\n",
      "Average training loss: 0.060738460348712074\n",
      "Average test loss: 0.0026808218229562044\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06073054247101148\n",
      "Average test loss: 0.0026472098461041847\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06081057189570533\n",
      "Average test loss: 0.002677597949074374\n",
      "Epoch 178/300\n",
      "Average training loss: 0.060582040306594635\n",
      "Average test loss: 0.0027002965135292873\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06050400400161743\n",
      "Average test loss: 0.00278627068321738\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06058367028170162\n",
      "Average test loss: 0.0026914376610269147\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06054840599497159\n",
      "Average test loss: 0.00266712423455384\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06039702368776004\n",
      "Average test loss: 0.0027117921269188324\n",
      "Epoch 183/300\n",
      "Average training loss: 0.060375015199184416\n",
      "Average test loss: 0.002678075640151898\n",
      "Epoch 184/300\n",
      "Average training loss: 0.060303169399499895\n",
      "Average test loss: 0.003070507095505794\n",
      "Epoch 185/300\n",
      "Average training loss: 0.060293782624933456\n",
      "Average test loss: 0.0026494838069710466\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06019105890724394\n",
      "Average test loss: 0.002706836501757304\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06015199462572734\n",
      "Average test loss: 0.002761432082702716\n",
      "Epoch 188/300\n",
      "Average training loss: 0.060077786525090535\n",
      "Average test loss: 0.0028385617609860167\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06032108074757788\n",
      "Average test loss: 0.0026412325048198303\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05992142412397596\n",
      "Average test loss: 0.0033120078990856805\n",
      "Epoch 191/300\n",
      "Average training loss: 0.059999143461386364\n",
      "Average test loss: 0.0026728531279497677\n",
      "Epoch 192/300\n",
      "Average training loss: 0.059978230204847126\n",
      "Average test loss: 0.002686924947011802\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05984719699952337\n",
      "Average test loss: 0.002663983761332929\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06001643544435501\n",
      "Average test loss: 0.002759436371218827\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05965310511655278\n",
      "Average test loss: 0.002743655641252796\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05975655363665687\n",
      "Average test loss: 0.0026962850724036496\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05971776756975386\n",
      "Average test loss: 0.002787492354503936\n",
      "Epoch 198/300\n",
      "Average training loss: 0.059561583366658954\n",
      "Average test loss: 0.0027372283961416945\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05955367974771394\n",
      "Average test loss: 0.0027142782904621627\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05958334619469113\n",
      "Average test loss: 0.002653090671532684\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05948146625028716\n",
      "Average test loss: 0.009328168012822668\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05957698228292995\n",
      "Average test loss: 0.015942899060332114\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05951270224319564\n",
      "Average test loss: 0.0027191781238135363\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0594195387131638\n",
      "Average test loss: 0.0027199912706596984\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05936840295460489\n",
      "Average test loss: 0.002829880493382613\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05922243083516757\n",
      "Average test loss: 0.0026851314269006254\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05930091909567515\n",
      "Average test loss: 0.0026979109545548757\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05927557303508123\n",
      "Average test loss: 0.002764707356898321\n",
      "Epoch 209/300\n",
      "Average training loss: 0.059386930267016094\n",
      "Average test loss: 0.002714546216651797\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05912964063220554\n",
      "Average test loss: 0.002713773573231366\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0591899833381176\n",
      "Average test loss: 0.002727099276251263\n",
      "Epoch 212/300\n",
      "Average training loss: 0.058976703312661916\n",
      "Average test loss: 0.002694178338886963\n",
      "Epoch 213/300\n",
      "Average training loss: 0.059036037656995985\n",
      "Average test loss: 0.002735265270496408\n",
      "Epoch 214/300\n",
      "Average training loss: 0.058950260586208766\n",
      "Average test loss: 0.002764780868155261\n",
      "Epoch 215/300\n",
      "Average training loss: 0.059027729349003896\n",
      "Average test loss: 0.0027248507504247957\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05893313861224386\n",
      "Average test loss: 0.002722038154800733\n",
      "Epoch 217/300\n",
      "Average training loss: 0.058881715423531\n",
      "Average test loss: 0.0026885526147153644\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05878020150959492\n",
      "Average test loss: 0.002761580891907215\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05888756065567335\n",
      "Average test loss: 0.0030297666556305357\n",
      "Epoch 220/300\n",
      "Average training loss: 0.058764585609237356\n",
      "Average test loss: 0.0027435792212684947\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05873141271207068\n",
      "Average test loss: 0.002746213828006552\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05869137444761064\n",
      "Average test loss: 0.0027304510074771114\n",
      "Epoch 223/300\n",
      "Average training loss: 0.058630421813991335\n",
      "Average test loss: 0.002790896518362893\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05860810553365284\n",
      "Average test loss: 0.002808455120668643\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05860256252355046\n",
      "Average test loss: 0.002698654769402411\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05869729162255923\n",
      "Average test loss: 0.002735788859426975\n",
      "Epoch 227/300\n",
      "Average training loss: 0.058605232682493\n",
      "Average test loss: 0.0027168581523001195\n",
      "Epoch 228/300\n",
      "Average training loss: 0.058498134367995794\n",
      "Average test loss: 0.0027585044401801295\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05835912813742956\n",
      "Average test loss: 0.002783493536627955\n",
      "Epoch 230/300\n",
      "Average training loss: 0.058421499934461385\n",
      "Average test loss: 0.0029660625205271776\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05841202158398098\n",
      "Average test loss: 0.0028096542247674533\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05826832761367162\n",
      "Average test loss: 0.0027339098416268824\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05834553079141511\n",
      "Average test loss: 0.0027781097477094993\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05848389710651504\n",
      "Average test loss: 0.0027324476554575895\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05817109294070138\n",
      "Average test loss: 0.002746876218977074\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0581567806104819\n",
      "Average test loss: 0.0027683056228690675\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05820444547467762\n",
      "Average test loss: 0.0029409737698733807\n",
      "Epoch 238/300\n",
      "Average training loss: 0.058120865437719556\n",
      "Average test loss: 0.0027283772584050896\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05821925765938229\n",
      "Average test loss: 0.0027117511600049005\n",
      "Epoch 240/300\n",
      "Average training loss: 0.058065270023213496\n",
      "Average test loss: 0.0026816481796817647\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05797283058365186\n",
      "Average test loss: 0.0027622445643775994\n",
      "Epoch 242/300\n",
      "Average training loss: 0.058037780304749806\n",
      "Average test loss: 0.0027650175504386424\n",
      "Epoch 243/300\n",
      "Average training loss: 0.057987455623017416\n",
      "Average test loss: 0.002746161120426324\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05789410190449821\n",
      "Average test loss: 0.0027736432786203092\n",
      "Epoch 245/300\n",
      "Average training loss: 0.057905653678708605\n",
      "Average test loss: 0.0027434614106184906\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05790497500035498\n",
      "Average test loss: 0.002695565425066484\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05786373600363731\n",
      "Average test loss: 0.002774305862892005\n",
      "Epoch 248/300\n",
      "Average training loss: 0.058000681069162154\n",
      "Average test loss: 0.0027231176584545107\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05781696197224988\n",
      "Average test loss: 0.0027896801609959866\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05780500339799457\n",
      "Average test loss: 0.0027657416380114028\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05773667142788569\n",
      "Average test loss: 0.0027630976037018827\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05767928009231885\n",
      "Average test loss: 0.0027578334430646565\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05760559154219098\n",
      "Average test loss: 0.002707987728011277\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05770935103297234\n",
      "Average test loss: 0.002756802522473865\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05770481316248576\n",
      "Average test loss: 0.0027182939896980922\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05755956084860696\n",
      "Average test loss: 0.0027208505804753964\n",
      "Epoch 257/300\n",
      "Average training loss: 0.057538206087218394\n",
      "Average test loss: 0.003422857094142172\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05749982748760117\n",
      "Average test loss: 0.002797945596070753\n",
      "Epoch 259/300\n",
      "Average training loss: 0.057636503756046294\n",
      "Average test loss: 0.002758855355437845\n",
      "Epoch 260/300\n",
      "Average training loss: 0.057459417068296006\n",
      "Average test loss: 0.00277779934865733\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05771382080184089\n",
      "Average test loss: 0.002853393592043883\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05742005215088526\n",
      "Average test loss: 0.0027928282947589952\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05733388002382384\n",
      "Average test loss: 0.002837580883047647\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05725181103083823\n",
      "Average test loss: 0.0027588400909056266\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05737636338339912\n",
      "Average test loss: 0.00273668126296252\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05723402523332172\n",
      "Average test loss: 0.0027728382740169763\n",
      "Epoch 267/300\n",
      "Average training loss: 0.057242546174261305\n",
      "Average test loss: 0.002767408584865431\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05725183270043797\n",
      "Average test loss: 0.002729037216761046\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05722085498770078\n",
      "Average test loss: 0.002807336044187347\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05714718088838789\n",
      "Average test loss: 0.0027206455504314767\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05708747918241554\n",
      "Average test loss: 0.0027808945232795346\n",
      "Epoch 272/300\n",
      "Average training loss: 0.057083200729555555\n",
      "Average test loss: 0.0027644757427689104\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05714889908830325\n",
      "Average test loss: 0.0027864327637685668\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05718619424435827\n",
      "Average test loss: 0.0027726476924079986\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0572376890414291\n",
      "Average test loss: 0.002731596275543173\n",
      "Epoch 276/300\n",
      "Average training loss: 0.057102853947215614\n",
      "Average test loss: 0.0028047050775753127\n",
      "Epoch 277/300\n",
      "Average training loss: 0.056983886894252565\n",
      "Average test loss: 0.0027580815524690682\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05705941454900636\n",
      "Average test loss: 0.0027213565009749597\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05704582049118148\n",
      "Average test loss: 0.0027428360922882953\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05692850871218575\n",
      "Average test loss: 0.0027201744336634876\n",
      "Epoch 281/300\n",
      "Average training loss: 0.056981721778710685\n",
      "Average test loss: 0.0027606863747868275\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05688200180398093\n",
      "Average test loss: 0.0028197051857908567\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05691919131742584\n",
      "Average test loss: 0.002746375027630064\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05677061250474718\n",
      "Average test loss: 0.0028382074683904646\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05676999782853656\n",
      "Average test loss: 0.002865129817898075\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05677070708076159\n",
      "Average test loss: 0.00282561703854137\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05675416307316886\n",
      "Average test loss: 0.00284996418406566\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05678258203466733\n",
      "Average test loss: 0.0027556829928523964\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05676942785580953\n",
      "Average test loss: 0.0027405018627436623\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05661878651380539\n",
      "Average test loss: 0.0027413901812914344\n",
      "Epoch 291/300\n",
      "Average training loss: 0.056700494842396844\n",
      "Average test loss: 0.0027453741865853466\n",
      "Epoch 292/300\n",
      "Average training loss: 0.056764177123705545\n",
      "Average test loss: 0.0027700756187033324\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05668450879719522\n",
      "Average test loss: 0.0027487842920753693\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05665389585163858\n",
      "Average test loss: 0.0027286928965606625\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05666752476162381\n",
      "Average test loss: 0.0027382055583099526\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05643800385130776\n",
      "Average test loss: 0.002731470039942198\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05658034892214669\n",
      "Average test loss: 0.002791172939663132\n",
      "Epoch 298/300\n",
      "Average training loss: 0.056558348033163286\n",
      "Average test loss: 0.002814095305899779\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05647685761253039\n",
      "Average test loss: 0.0027633547609051067\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0564621327718099\n",
      "Average test loss: 0.002759953516845902\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7.232871232562595\n",
      "Average test loss: 0.005045218766977389\n",
      "Epoch 2/300\n",
      "Average training loss: 2.9598501987457277\n",
      "Average test loss: 0.004039547864140736\n",
      "Epoch 3/300\n",
      "Average training loss: 2.125731448067559\n",
      "Average test loss: 0.004034529379258553\n",
      "Epoch 4/300\n",
      "Average training loss: 1.6305348079469468\n",
      "Average test loss: 0.005441107246610853\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2825463174184164\n",
      "Average test loss: 0.010867256921405593\n",
      "Epoch 6/300\n",
      "Average training loss: 1.0491508676211039\n",
      "Average test loss: 0.004747779570312963\n",
      "Epoch 7/300\n",
      "Average training loss: 0.8657423075570001\n",
      "Average test loss: 0.0031030583601031037\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7276019051339891\n",
      "Average test loss: 0.003101620235790809\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6135952325397067\n",
      "Average test loss: 0.0028500528447329997\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5170124380323622\n",
      "Average test loss: 0.002696588995349076\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4384854707452986\n",
      "Average test loss: 0.0026584367650664513\n",
      "Epoch 12/300\n",
      "Average training loss: 0.37309317045741613\n",
      "Average test loss: 0.0026333350123216707\n",
      "Epoch 13/300\n",
      "Average training loss: 0.31894359482659235\n",
      "Average test loss: 0.0025083886803024345\n",
      "Epoch 14/300\n",
      "Average training loss: 0.27397587650352057\n",
      "Average test loss: 0.002443801614559359\n",
      "Epoch 15/300\n",
      "Average training loss: 0.2369090487294727\n",
      "Average test loss: 0.002417396508364214\n",
      "Epoch 16/300\n",
      "Average training loss: 0.20642295138041178\n",
      "Average test loss: 0.002309469639841053\n",
      "Epoch 17/300\n",
      "Average training loss: 0.18042096545961167\n",
      "Average test loss: 0.002345499620669418\n",
      "Epoch 18/300\n",
      "Average training loss: 0.15919596758815976\n",
      "Average test loss: 0.002252179696948992\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1417035101917055\n",
      "Average test loss: 0.002206022410343091\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1280688242316246\n",
      "Average test loss: 0.002151429889516698\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11708960786130693\n",
      "Average test loss: 0.0021749827437516716\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10871345207426283\n",
      "Average test loss: 0.0020881119736780723\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10212520740098423\n",
      "Average test loss: 0.0021097502092727355\n",
      "Epoch 24/300\n",
      "Average training loss: 0.09712790573967828\n",
      "Average test loss: 0.0020983587658136254\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0928391220966975\n",
      "Average test loss: 0.0020352516430947516\n",
      "Epoch 26/300\n",
      "Average training loss: 0.08917713530858358\n",
      "Average test loss: 0.002021008746491538\n",
      "Epoch 27/300\n",
      "Average training loss: 0.08598962942096922\n",
      "Average test loss: 0.002041121454185082\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08333578572670619\n",
      "Average test loss: 0.0020022650423149268\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0809389048881001\n",
      "Average test loss: 0.001977177735418081\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0788404706882106\n",
      "Average test loss: 0.0020055961149434247\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07702044983704885\n",
      "Average test loss: 0.0019821695308718415\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07544840248756939\n",
      "Average test loss: 0.001950566002788643\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07402555159065459\n",
      "Average test loss: 0.0019422164659740197\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07274167082044813\n",
      "Average test loss: 0.0019299354792262117\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07163398465183046\n",
      "Average test loss: 0.0019125225585367946\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07051469887627496\n",
      "Average test loss: 0.001906481439454688\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06948243375288116\n",
      "Average test loss: 0.001901337443643974\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06861559111542172\n",
      "Average test loss: 0.0018864687154483465\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0676205555829737\n",
      "Average test loss: 0.0019004238059537278\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06678772848513391\n",
      "Average test loss: 0.001906077465766834\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06608479078279601\n",
      "Average test loss: 0.0018907411494809721\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06527933627367019\n",
      "Average test loss: 0.0018746527236782843\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0645070822470718\n",
      "Average test loss: 0.0018557814651479325\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06395916836129295\n",
      "Average test loss: 0.0018948565251711341\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06334980135162671\n",
      "Average test loss: 0.0018743164607634146\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06263254931237963\n",
      "Average test loss: 0.0018346332271272938\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06216718437274297\n",
      "Average test loss: 0.0018282735642666619\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06158612386054463\n",
      "Average test loss: 0.0018603177308622334\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0612340553369787\n",
      "Average test loss: 0.0018239936060789558\n",
      "Epoch 50/300\n",
      "Average training loss: 0.060604568971527946\n",
      "Average test loss: 0.001848953971432315\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06025304820140203\n",
      "Average test loss: 0.0018591126282182006\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05980502516031265\n",
      "Average test loss: 0.0018320402400568128\n",
      "Epoch 53/300\n",
      "Average training loss: 0.059335595922337635\n",
      "Average test loss: 0.0018344814603527388\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0589740723338392\n",
      "Average test loss: 0.001853622880971266\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05871714725428157\n",
      "Average test loss: 0.0018316032743702331\n",
      "Epoch 56/300\n",
      "Average training loss: 0.058279374417331484\n",
      "Average test loss: 0.0018465140152515637\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05807369210322698\n",
      "Average test loss: 0.0018468544165500336\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05769570560587777\n",
      "Average test loss: 0.00181915506027225\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05725230178236961\n",
      "Average test loss: 0.0018045365870412854\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05695737813247575\n",
      "Average test loss: 0.001819709846439461\n",
      "Epoch 61/300\n",
      "Average training loss: 0.056783561127053364\n",
      "Average test loss: 0.001889894017121858\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05651014591587914\n",
      "Average test loss: 0.0018053116630762816\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05604578767551316\n",
      "Average test loss: 0.0018243211555398172\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05595796031422085\n",
      "Average test loss: 0.0018380287452083495\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05550729686021805\n",
      "Average test loss: 0.0018122727003776365\n",
      "Epoch 66/300\n",
      "Average training loss: 0.055343718856573104\n",
      "Average test loss: 0.0018170270678690738\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05502392814225621\n",
      "Average test loss: 0.0018031622089652551\n",
      "Epoch 68/300\n",
      "Average training loss: 0.054733153770367304\n",
      "Average test loss: 0.001809438336847557\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05451974111133152\n",
      "Average test loss: 0.001813696806319058\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05450474506782161\n",
      "Average test loss: 0.0018297406646112602\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05399337385760413\n",
      "Average test loss: 0.0018044562532256046\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05390606573886342\n",
      "Average test loss: 0.0018452279230372773\n",
      "Epoch 73/300\n",
      "Average training loss: 0.053528913928402796\n",
      "Average test loss: 0.001792642023310893\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05327061175306638\n",
      "Average test loss: 0.0018148692439620693\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05326855617099338\n",
      "Average test loss: 0.0018166261228422325\n",
      "Epoch 76/300\n",
      "Average training loss: 0.052883791681793\n",
      "Average test loss: 0.001840041326151954\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05271853011515405\n",
      "Average test loss: 0.0018163229653404818\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05259162366059091\n",
      "Average test loss: 0.0018008288802165125\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05213892584708002\n",
      "Average test loss: 0.001834257303737104\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05201665239201652\n",
      "Average test loss: 0.0018302041481559476\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05179974153306749\n",
      "Average test loss: 0.001835234150911371\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05188374035888248\n",
      "Average test loss: 0.002000840432734953\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05136416784260008\n",
      "Average test loss: 0.0018212834691835773\n",
      "Epoch 84/300\n",
      "Average training loss: 0.051097513251834446\n",
      "Average test loss: 0.0018390084183257487\n",
      "Epoch 85/300\n",
      "Average training loss: 0.051037479986747104\n",
      "Average test loss: 0.0018358596812726722\n",
      "Epoch 86/300\n",
      "Average training loss: 0.050977939613991316\n",
      "Average test loss: 0.0018634248357266188\n",
      "Epoch 87/300\n",
      "Average training loss: 0.050621762149863775\n",
      "Average test loss: 0.0019018968621061908\n",
      "Epoch 88/300\n",
      "Average training loss: 0.050381038152509265\n",
      "Average test loss: 0.001841362701315019\n",
      "Epoch 89/300\n",
      "Average training loss: 0.050302984618478354\n",
      "Average test loss: 0.0018385590759830343\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05009952700800366\n",
      "Average test loss: 0.0018650223674873511\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04988871616125107\n",
      "Average test loss: 0.001956926438336571\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04980683545271555\n",
      "Average test loss: 0.0018365353659416238\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04978752439220746\n",
      "Average test loss: 0.002041687887782852\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04934598506655958\n",
      "Average test loss: 0.0018885766942467954\n",
      "Epoch 95/300\n",
      "Average training loss: 0.049329452875587675\n",
      "Average test loss: 0.001860924551056491\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04899910963575045\n",
      "Average test loss: 0.001860633868103226\n",
      "Epoch 97/300\n",
      "Average training loss: 0.049228297819693886\n",
      "Average test loss: 0.0018988828167526258\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0488030899796221\n",
      "Average test loss: 0.0018665895776616202\n",
      "Epoch 99/300\n",
      "Average training loss: 0.048853728976514604\n",
      "Average test loss: 0.0018576608662390047\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04845068601767222\n",
      "Average test loss: 0.0018952420471856992\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04858609769741694\n",
      "Average test loss: 0.0019198860333611567\n",
      "Epoch 102/300\n",
      "Average training loss: 0.048183644264936445\n",
      "Average test loss: 0.0018633777110113037\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04806158442629708\n",
      "Average test loss: 0.0018861584312592943\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04794739656315909\n",
      "Average test loss: 0.001923999984541701\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04777607931693395\n",
      "Average test loss: 0.0020305671060664786\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04762538613213433\n",
      "Average test loss: 0.0019412675344695647\n",
      "Epoch 107/300\n",
      "Average training loss: 0.047557965341541504\n",
      "Average test loss: 0.0018589849167813858\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0475261985262235\n",
      "Average test loss: 0.001849478598477112\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04743167263931698\n",
      "Average test loss: 0.0019532363827650745\n",
      "Epoch 110/300\n",
      "Average training loss: 0.047127593103382326\n",
      "Average test loss: 0.0018885019021108746\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04719819798403316\n",
      "Average test loss: 0.00191900417726073\n",
      "Epoch 112/300\n",
      "Average training loss: 0.047072763085365295\n",
      "Average test loss: 0.0018863826646573015\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04681367578109105\n",
      "Average test loss: 0.001913984550577071\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04665469394127528\n",
      "Average test loss: 0.0018769404040649533\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0466301320625676\n",
      "Average test loss: 0.0018746096928500466\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04649046118557453\n",
      "Average test loss: 0.0018993077500619822\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04644076289070977\n",
      "Average test loss: 0.0018908941976519095\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04624530309769842\n",
      "Average test loss: 0.00201434289643334\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04625699817140897\n",
      "Average test loss: 0.0019276773006551796\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04611815167797936\n",
      "Average test loss: 0.0019022363608496057\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04623070384396447\n",
      "Average test loss: 0.001922693401782049\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04593829938934909\n",
      "Average test loss: 0.0019181754551827908\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04584435174862544\n",
      "Average test loss: 0.0019155366922625238\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0457044813964102\n",
      "Average test loss: 0.002480498777081569\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04572557152642144\n",
      "Average test loss: 0.0018841478946722216\n",
      "Epoch 126/300\n",
      "Average training loss: 0.045530048509438835\n",
      "Average test loss: 0.0019105752548202871\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04548241448402405\n",
      "Average test loss: 0.0019371307621606523\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04535995210541619\n",
      "Average test loss: 0.0019114810869925551\n",
      "Epoch 129/300\n",
      "Average training loss: 0.045278065078788336\n",
      "Average test loss: 0.0018972411139143838\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04520478554897838\n",
      "Average test loss: 0.001934286969403426\n",
      "Epoch 131/300\n",
      "Average training loss: 0.045170946614609825\n",
      "Average test loss: 0.0019413275373064808\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04508096103204621\n",
      "Average test loss: 0.0019328966369438502\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04510231696565946\n",
      "Average test loss: 0.001923305551107559\n",
      "Epoch 134/300\n",
      "Average training loss: 0.044919466690884695\n",
      "Average test loss: 0.0019961805479187106\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04490441137883398\n",
      "Average test loss: 0.0019215771132666203\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0447694730758667\n",
      "Average test loss: 0.0019920334524164596\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04476417622301314\n",
      "Average test loss: 0.002012307066573865\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04464087931977378\n",
      "Average test loss: 0.002710273466279937\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04468384665581915\n",
      "Average test loss: 0.0019337051827460526\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04456283075941934\n",
      "Average test loss: 0.0019869589012944037\n",
      "Epoch 141/300\n",
      "Average training loss: 0.044813579243090415\n",
      "Average test loss: 0.00193223770097312\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04445303306976954\n",
      "Average test loss: 0.0019173226603824232\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04422121033734745\n",
      "Average test loss: 0.001915242795056353\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04413220883409182\n",
      "Average test loss: 0.0019553544170533618\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04417340433266428\n",
      "Average test loss: 0.0019383944583435853\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04411294444898764\n",
      "Average test loss: 0.0020814444533445767\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04395576450559828\n",
      "Average test loss: 0.0019310654830187559\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04395188503464063\n",
      "Average test loss: 0.0019067016109410259\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04392024591896269\n",
      "Average test loss: 0.0019410299124817054\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04392368287841479\n",
      "Average test loss: 0.0019393791891634464\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04384417820970218\n",
      "Average test loss: 0.001918946067078246\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04384727791614003\n",
      "Average test loss: 0.0019265864274154107\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04363145510686768\n",
      "Average test loss: 0.001988725548506611\n",
      "Epoch 154/300\n",
      "Average training loss: 0.043766949115528\n",
      "Average test loss: 0.0019405070037270585\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0435790906978978\n",
      "Average test loss: 0.0019036975822721918\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04343030544453197\n",
      "Average test loss: 0.002150733409345978\n",
      "Epoch 157/300\n",
      "Average training loss: 0.043444989250765904\n",
      "Average test loss: 0.0019403041879543\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04355740675330162\n",
      "Average test loss: 0.010526340307253931\n",
      "Epoch 159/300\n",
      "Average training loss: 0.043553044398625694\n",
      "Average test loss: 0.002672186771925125\n",
      "Epoch 160/300\n",
      "Average training loss: 0.043371488700310386\n",
      "Average test loss: 0.001974543664811386\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04321447431544463\n",
      "Average test loss: 0.0020166956008308462\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04321589153673914\n",
      "Average test loss: 0.0019693152878640426\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04309204974439409\n",
      "Average test loss: 0.0019480032076438268\n",
      "Epoch 164/300\n",
      "Average training loss: 0.043077953779035144\n",
      "Average test loss: 0.0019946615314111113\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0431185350500875\n",
      "Average test loss: 0.002669025482609868\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04304831619064013\n",
      "Average test loss: 0.001985837744962838\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04311227306392458\n",
      "Average test loss: 0.0019491776163793272\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04299121354354753\n",
      "Average test loss: 0.0019254426028993395\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0428841700984372\n",
      "Average test loss: 0.0019302933628981313\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04281227567129665\n",
      "Average test loss: 0.001967949057618777\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04277588523096509\n",
      "Average test loss: 0.0019825030418319837\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04276195943024423\n",
      "Average test loss: 0.0019541379826971228\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04283559929993418\n",
      "Average test loss: 0.002023307049853934\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04273945539527469\n",
      "Average test loss: 0.0019127139836135837\n",
      "Epoch 175/300\n",
      "Average training loss: 0.042687490224838254\n",
      "Average test loss: 0.0019673162411070534\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04260283724798097\n",
      "Average test loss: 0.002031486394090785\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04249116106828054\n",
      "Average test loss: 0.0019061037531743446\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04251919430494309\n",
      "Average test loss: 0.001972850167916881\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04246962944666544\n",
      "Average test loss: 0.0019900967571884393\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04242399097151226\n",
      "Average test loss: 0.001978285240112907\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04236597301562627\n",
      "Average test loss: 0.0019453987452305026\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04240994698140356\n",
      "Average test loss: 0.0019865446840412913\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04241828219095866\n",
      "Average test loss: 0.0019624635281248223\n",
      "Epoch 184/300\n",
      "Average training loss: 0.042308886528015135\n",
      "Average test loss: 0.001984276404397355\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04220317902498775\n",
      "Average test loss: 0.0019311816023869647\n",
      "Epoch 186/300\n",
      "Average training loss: 0.042282247172461616\n",
      "Average test loss: 0.0020861056935456064\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04219807074467341\n",
      "Average test loss: 0.002024252005128397\n",
      "Epoch 188/300\n",
      "Average training loss: 0.042071727971235914\n",
      "Average test loss: 0.001985351218635009\n",
      "Epoch 189/300\n",
      "Average training loss: 0.042092740250958334\n",
      "Average test loss: 0.0019748672633431853\n",
      "Epoch 190/300\n",
      "Average training loss: 0.042080454445547526\n",
      "Average test loss: 0.0019408532261020607\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04199690008163452\n",
      "Average test loss: 0.0019629299148089356\n",
      "Epoch 192/300\n",
      "Average training loss: 0.041991310781902734\n",
      "Average test loss: 0.001991859118764599\n",
      "Epoch 193/300\n",
      "Average training loss: 0.042043171940578355\n",
      "Average test loss: 0.002018920864495966\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04189327759212918\n",
      "Average test loss: 0.001992305096652773\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04190773071183099\n",
      "Average test loss: 0.002035615284823709\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04189979573753145\n",
      "Average test loss: 0.002005127479839656\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04180790954497125\n",
      "Average test loss: 0.001984892789998816\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0418047599626912\n",
      "Average test loss: 0.00326152211200032\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04172543491257562\n",
      "Average test loss: 0.0019911383923350108\n",
      "Epoch 200/300\n",
      "Average training loss: 0.041855910072724024\n",
      "Average test loss: 0.0019556820407095883\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04175543138384819\n",
      "Average test loss: 0.0019967949094457757\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04155643015768793\n",
      "Average test loss: 0.0019634512467309835\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04157021605306201\n",
      "Average test loss: 0.0020826291326019497\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04162735720806652\n",
      "Average test loss: 0.0019792713642948203\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04149584644039472\n",
      "Average test loss: 0.0020618960946384404\n",
      "Epoch 206/300\n",
      "Average training loss: 0.041592165337668525\n",
      "Average test loss: 0.001995630109268758\n",
      "Epoch 207/300\n",
      "Average training loss: 0.041428281025754084\n",
      "Average test loss: 0.002027812152894007\n",
      "Epoch 208/300\n",
      "Average training loss: 0.041352929923269485\n",
      "Average test loss: 0.0020116242985758516\n",
      "Epoch 209/300\n",
      "Average training loss: 0.041412700166304904\n",
      "Average test loss: 0.0019772084816876384\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04143564298252265\n",
      "Average test loss: 0.001978401066735387\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04132385162181324\n",
      "Average test loss: 0.001985617600174414\n",
      "Epoch 212/300\n",
      "Average training loss: 0.041421210729413564\n",
      "Average test loss: 0.002002148540587061\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04131306470930576\n",
      "Average test loss: 0.0021359730827518637\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04125103118684557\n",
      "Average test loss: 0.0019723179975731507\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04128863156504101\n",
      "Average test loss: 0.0019835585057735444\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0412842943933275\n",
      "Average test loss: 0.001977201581725644\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04124760291311476\n",
      "Average test loss: 0.0019719359040674235\n",
      "Epoch 218/300\n",
      "Average training loss: 0.041253319601217905\n",
      "Average test loss: 0.0019978085216134788\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04129928999145826\n",
      "Average test loss: 0.0019738124700056183\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04108778039283223\n",
      "Average test loss: 0.0024055029108292526\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04098074917991956\n",
      "Average test loss: 0.0019852966527558035\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04109104761812422\n",
      "Average test loss: 0.0020150718992162082\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04096596489350001\n",
      "Average test loss: 0.0019696454970786967\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04103340441319678\n",
      "Average test loss: 0.0019742884466217624\n",
      "Epoch 225/300\n",
      "Average training loss: 0.040956268045637345\n",
      "Average test loss: 0.002008196353498432\n",
      "Epoch 226/300\n",
      "Average training loss: 0.040976293077071506\n",
      "Average test loss: 0.0020223206910822125\n",
      "Epoch 227/300\n",
      "Average training loss: 0.040978537874089345\n",
      "Average test loss: 0.001993149337048332\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04086188291178809\n",
      "Average test loss: 0.002018901027014686\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04093325290083885\n",
      "Average test loss: 0.00197563378750864\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04083446384138531\n",
      "Average test loss: 0.001992720758749379\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04081838112407261\n",
      "Average test loss: 0.0020081107578136855\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0408004930847221\n",
      "Average test loss: 0.002175232394494944\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04069783033596145\n",
      "Average test loss: 0.0020176368755184942\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04076763360699018\n",
      "Average test loss: 0.0020165596993433105\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04073203735219108\n",
      "Average test loss: 0.00223945241017888\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04068702745768759\n",
      "Average test loss: 0.0021423965632501574\n",
      "Epoch 237/300\n",
      "Average training loss: 0.040707738469044366\n",
      "Average test loss: 0.0019903967149245243\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04065236486660109\n",
      "Average test loss: 0.0020100344995864564\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04064314768380589\n",
      "Average test loss: 0.0019782722281912963\n",
      "Epoch 240/300\n",
      "Average training loss: 0.040597228656212486\n",
      "Average test loss: 0.001958101864386764\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0405001958641741\n",
      "Average test loss: 0.00198242945327527\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04060785313612885\n",
      "Average test loss: 0.0020171166486624213\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04056477020515336\n",
      "Average test loss: 0.004653622372075916\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04052461706267463\n",
      "Average test loss: 0.00195524681188787\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04049485623670949\n",
      "Average test loss: 0.002050075432492627\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04042464422848489\n",
      "Average test loss: 0.0019725063962654937\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04041554133097331\n",
      "Average test loss: 0.0020018126817627087\n",
      "Epoch 248/300\n",
      "Average training loss: 0.040510163605213166\n",
      "Average test loss: 0.002057362446664936\n",
      "Epoch 249/300\n",
      "Average training loss: 0.040401801500055526\n",
      "Average test loss: 0.0020006429055291746\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04036476504471567\n",
      "Average test loss: 0.0020272538465344243\n",
      "Epoch 251/300\n",
      "Average training loss: 0.040281901346312626\n",
      "Average test loss: 0.0019970432355379064\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04034501373767853\n",
      "Average test loss: 0.0020367350273558663\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04029238635963864\n",
      "Average test loss: 0.0020304624784944787\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0403280241108603\n",
      "Average test loss: 0.0020090762635485994\n",
      "Epoch 255/300\n",
      "Average training loss: 0.040211575514740416\n",
      "Average test loss: 0.0020235309704310363\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04014885006182724\n",
      "Average test loss: 0.0020458181003729504\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04022489508986473\n",
      "Average test loss: 0.002026946752332151\n",
      "Epoch 258/300\n",
      "Average training loss: 0.040212915841076106\n",
      "Average test loss: 0.0019919084974875053\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04018317432536019\n",
      "Average test loss: 0.0020326949222427278\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0401623885879914\n",
      "Average test loss: 0.002014787786743707\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04013207091225518\n",
      "Average test loss: 0.0020614180751144886\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04010185849004322\n",
      "Average test loss: 0.002027306036816703\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04007649448182848\n",
      "Average test loss: 0.002019738533223669\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04004262591401736\n",
      "Average test loss: 0.0020130377583619623\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04010449699560801\n",
      "Average test loss: 0.002048281847809752\n",
      "Epoch 266/300\n",
      "Average training loss: 0.040028980367713504\n",
      "Average test loss: 0.0019872030539231167\n",
      "Epoch 267/300\n",
      "Average training loss: 0.040072944677538344\n",
      "Average test loss: 0.0019960062281332083\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04008106858862771\n",
      "Average test loss: 0.002017642246352302\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03997271217405796\n",
      "Average test loss: 0.0020564022756492098\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03996225735876295\n",
      "Average test loss: 0.0020098330679660043\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03988542937239011\n",
      "Average test loss: 0.0020390223032898374\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03992630937033229\n",
      "Average test loss: 0.0020231706951227453\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03986988849441211\n",
      "Average test loss: 0.0020488768373098636\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03988014268875122\n",
      "Average test loss: 0.002025943960994482\n",
      "Epoch 275/300\n",
      "Average training loss: 0.039895542167954975\n",
      "Average test loss: 0.0020817289693901937\n",
      "Epoch 276/300\n",
      "Average training loss: 0.039924907479021284\n",
      "Average test loss: 0.002013052133636342\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03984816654854351\n",
      "Average test loss: 0.0020002411032716432\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03985372752282355\n",
      "Average test loss: 0.0020636099160959323\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03990737701455752\n",
      "Average test loss: 0.0020269727491670185\n",
      "Epoch 280/300\n",
      "Average training loss: 0.039711564981275135\n",
      "Average test loss: 0.001976240747504764\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03979015502333641\n",
      "Average test loss: 0.002045885070744488\n",
      "Epoch 282/300\n",
      "Average training loss: 0.039800503777133095\n",
      "Average test loss: 0.0020398100569016405\n",
      "Epoch 283/300\n",
      "Average training loss: 0.039770484452446304\n",
      "Average test loss: 0.00202847466017637\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03972804894712236\n",
      "Average test loss: 0.0020633512943362196\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03971128934952948\n",
      "Average test loss: 0.001977551576267514\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03968625364204248\n",
      "Average test loss: 0.0020260040857311753\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03967927195959621\n",
      "Average test loss: 0.0024023646581918003\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03957463467452261\n",
      "Average test loss: 0.0020630525923851463\n",
      "Epoch 289/300\n",
      "Average training loss: 0.039635416229565935\n",
      "Average test loss: 0.0020339054572913384\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03959495900240209\n",
      "Average test loss: 0.002034294294607308\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03961243817044629\n",
      "Average test loss: 0.0021621992697732316\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03957916404306889\n",
      "Average test loss: 0.002071311782217688\n",
      "Epoch 293/300\n",
      "Average training loss: 0.039537860506110724\n",
      "Average test loss: 0.0021479536292867527\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03952872983614604\n",
      "Average test loss: 0.002021411613975134\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03949874375926124\n",
      "Average test loss: 0.0020191041946204173\n",
      "Epoch 296/300\n",
      "Average training loss: 0.039433076722754375\n",
      "Average test loss: 0.0020079887356712588\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03942540965808763\n",
      "Average test loss: 0.0020431964728567335\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0395355237490601\n",
      "Average test loss: 0.0020033920179638597\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03955314971672164\n",
      "Average test loss: 0.0020610502620951997\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03941727515392833\n",
      "Average test loss: 0.002028060955202414\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.461568320592244\n",
      "Average test loss: 2.872093763507489\n",
      "Epoch 2/300\n",
      "Average training loss: 2.7611176488664415\n",
      "Average test loss: 0.0031856116368952723\n",
      "Epoch 3/300\n",
      "Average training loss: 2.1574382702509562\n",
      "Average test loss: 0.005455349465211232\n",
      "Epoch 4/300\n",
      "Average training loss: 1.6696539180543688\n",
      "Average test loss: 0.0026716951893435586\n",
      "Epoch 5/300\n",
      "Average training loss: 1.3039609292348227\n",
      "Average test loss: 0.00398346276446763\n",
      "Epoch 6/300\n",
      "Average training loss: 1.1078096657858953\n",
      "Average test loss: 0.0024154776673143107\n",
      "Epoch 7/300\n",
      "Average training loss: 0.9310098225275676\n",
      "Average test loss: 0.0023292428909076584\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7624870753818088\n",
      "Average test loss: 0.0022731011044234035\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6401474510298835\n",
      "Average test loss: 0.0021159937545243237\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5285529801050822\n",
      "Average test loss: 0.0020366744334912963\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4292323381900787\n",
      "Average test loss: 0.0020478714396142297\n",
      "Epoch 12/300\n",
      "Average training loss: 0.34762590021557277\n",
      "Average test loss: 0.0018678575060847734\n",
      "Epoch 13/300\n",
      "Average training loss: 0.2829161034292645\n",
      "Average test loss: 0.001806678152229223\n",
      "Epoch 14/300\n",
      "Average training loss: 0.23225490682654912\n",
      "Average test loss: 0.0017719813891583019\n",
      "Epoch 15/300\n",
      "Average training loss: 0.19244245666927762\n",
      "Average test loss: 0.0017185997790139583\n",
      "Epoch 16/300\n",
      "Average training loss: 0.16317133135265774\n",
      "Average test loss: 0.0017721327234887414\n",
      "Epoch 17/300\n",
      "Average training loss: 0.14145928752422332\n",
      "Average test loss: 0.0016254796572029591\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12490601992607117\n",
      "Average test loss: 0.001604918903981646\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11191567622290717\n",
      "Average test loss: 0.001627873866301444\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10164085927274492\n",
      "Average test loss: 0.0015445468272082508\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09306852520836724\n",
      "Average test loss: 0.001534907026630309\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08602904139624702\n",
      "Average test loss: 0.0015171693393753634\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08043501191006766\n",
      "Average test loss: 0.0015006233150553373\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07565019889010323\n",
      "Average test loss: 0.0014940265232386688\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07193742179208332\n",
      "Average test loss: 0.0014717093841690157\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06880709934565755\n",
      "Average test loss: 0.0014666537875826988\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06632393010457356\n",
      "Average test loss: 0.0014445032264726856\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06409365375505553\n",
      "Average test loss: 0.001408615141072207\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06214741658502155\n",
      "Average test loss: 0.0014211094615360102\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06037204895748032\n",
      "Average test loss: 0.001392310235235426\n",
      "Epoch 31/300\n",
      "Average training loss: 0.058758449150456325\n",
      "Average test loss: 0.0014548330571916368\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0573299691942003\n",
      "Average test loss: 0.0013867866804616318\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05600797931353251\n",
      "Average test loss: 0.0013644016163630618\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05480290218194326\n",
      "Average test loss: 0.001353056483488116\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05364776011970308\n",
      "Average test loss: 0.0013578075202595856\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05267672625184059\n",
      "Average test loss: 0.0013505506225758128\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05173137994276153\n",
      "Average test loss: 0.0013413611587343945\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05074134065376388\n",
      "Average test loss: 0.0013246667814544506\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04987345472309324\n",
      "Average test loss: 0.001326520659857326\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04913423587878545\n",
      "Average test loss: 0.001351718444377184\n",
      "Epoch 41/300\n",
      "Average training loss: 0.048616111187471285\n",
      "Average test loss: 0.0013172989189624785\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04774597144126892\n",
      "Average test loss: 0.0013409054208960798\n",
      "Epoch 43/300\n",
      "Average training loss: 0.047052286505699155\n",
      "Average test loss: 0.0013147614795404176\n",
      "Epoch 44/300\n",
      "Average training loss: 0.046438851767116124\n",
      "Average test loss: 0.0012965963812958863\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04591927819781833\n",
      "Average test loss: 0.0012978163762018085\n",
      "Epoch 46/300\n",
      "Average training loss: 0.045336128963364496\n",
      "Average test loss: 0.0013501487773222229\n",
      "Epoch 47/300\n",
      "Average training loss: 0.045041376171840564\n",
      "Average test loss: 0.0013017051419657137\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04440985756450229\n",
      "Average test loss: 0.00128521223904358\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04397038189570109\n",
      "Average test loss: 0.0012902718697571092\n",
      "Epoch 50/300\n",
      "Average training loss: 0.043634977016184066\n",
      "Average test loss: 0.0012829454019665718\n",
      "Epoch 51/300\n",
      "Average training loss: 0.043256972981823814\n",
      "Average test loss: 0.0012816398104963202\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04302352827787399\n",
      "Average test loss: 0.0013682377599438446\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04264566743042734\n",
      "Average test loss: 0.0012910016246346964\n",
      "Epoch 54/300\n",
      "Average training loss: 0.042297239237361485\n",
      "Average test loss: 0.0012753061775325073\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04195238387087981\n",
      "Average test loss: 0.0013203787008403904\n",
      "Epoch 56/300\n",
      "Average training loss: 0.041638161578112176\n",
      "Average test loss: 0.0012741072338281407\n",
      "Epoch 57/300\n",
      "Average training loss: 0.041463266639245884\n",
      "Average test loss: 0.001361614429909322\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04120089744528135\n",
      "Average test loss: 0.0012766414693453246\n",
      "Epoch 59/300\n",
      "Average training loss: 0.040872632392578656\n",
      "Average test loss: 0.0012744901304443676\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04064768909745746\n",
      "Average test loss: 0.0012569553316053416\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04052152406010363\n",
      "Average test loss: 0.0012755836268059082\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04011488106350104\n",
      "Average test loss: 0.001258897391665313\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03988101906908883\n",
      "Average test loss: 0.0012643075017258526\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03971284861697091\n",
      "Average test loss: 0.0012596749790633717\n",
      "Epoch 65/300\n",
      "Average training loss: 0.039405499484803944\n",
      "Average test loss: 0.0012591437304185495\n",
      "Epoch 66/300\n",
      "Average training loss: 0.039146580460998746\n",
      "Average test loss: 0.001282726758884059\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03905763247112433\n",
      "Average test loss: 0.001264045199068884\n",
      "Epoch 68/300\n",
      "Average training loss: 0.038918101214700275\n",
      "Average test loss: 0.001296060572223117\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03861250116758876\n",
      "Average test loss: 0.0012553062778380183\n",
      "Epoch 70/300\n",
      "Average training loss: 0.038407253675990634\n",
      "Average test loss: 0.0012525325500820246\n",
      "Epoch 71/300\n",
      "Average training loss: 0.038330400561292964\n",
      "Average test loss: 0.0012736839229861895\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03795273390743468\n",
      "Average test loss: 0.0012467898085920348\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03776803123454253\n",
      "Average test loss: 0.0012977832616824243\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03769082113934888\n",
      "Average test loss: 0.0012530073198593326\n",
      "Epoch 75/300\n",
      "Average training loss: 0.037451636004779076\n",
      "Average test loss: 0.001248681145409743\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03752431515190337\n",
      "Average test loss: 0.001277844506315887\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0372761266860697\n",
      "Average test loss: 0.0012584833319609363\n",
      "Epoch 78/300\n",
      "Average training loss: 0.036792750184734664\n",
      "Average test loss: 0.001289441894636386\n",
      "Epoch 79/300\n",
      "Average training loss: 0.036720018385185134\n",
      "Average test loss: 0.0012774514629402094\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0365681339138084\n",
      "Average test loss: 0.0012944559701200987\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03645561266607708\n",
      "Average test loss: 0.001258528732539465\n",
      "Epoch 82/300\n",
      "Average training loss: 0.036240606023205654\n",
      "Average test loss: 0.0012744286531685956\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03624609953496191\n",
      "Average test loss: 0.0012815366515682802\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03599755995306704\n",
      "Average test loss: 0.0013018039466502766\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03568811775578393\n",
      "Average test loss: 0.0012691004466679361\n",
      "Epoch 86/300\n",
      "Average training loss: 0.035832390601436295\n",
      "Average test loss: 0.001273349685439219\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03546068438225322\n",
      "Average test loss: 0.0014634958745704757\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03533029144754012\n",
      "Average test loss: 0.0013709623169981771\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03512347020705541\n",
      "Average test loss: 0.0013420732110324832\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0350561433583498\n",
      "Average test loss: 0.0012804605600734553\n",
      "Epoch 91/300\n",
      "Average training loss: 0.034968030106690196\n",
      "Average test loss: 0.0013167272339471513\n",
      "Epoch 92/300\n",
      "Average training loss: 0.034800933313038615\n",
      "Average test loss: 0.0013302270220075217\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03459605027569665\n",
      "Average test loss: 0.0013082940188857417\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03451921900775697\n",
      "Average test loss: 0.0013068213685312204\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03434565586182806\n",
      "Average test loss: 0.0012881850188391076\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03426320303148694\n",
      "Average test loss: 0.0013422358054667712\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03411811752782928\n",
      "Average test loss: 0.0012837607960941063\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03409850368731552\n",
      "Average test loss: 0.0012915696034518382\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03388304905427827\n",
      "Average test loss: 0.0013035263772019081\n",
      "Epoch 100/300\n",
      "Average training loss: 0.033807898296250236\n",
      "Average test loss: 0.0012848622943274677\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03362852569586701\n",
      "Average test loss: 0.001331218134663585\n",
      "Epoch 102/300\n",
      "Average training loss: 0.033563811283972525\n",
      "Average test loss: 0.001388723963768118\n",
      "Epoch 103/300\n",
      "Average training loss: 0.033463363862699935\n",
      "Average test loss: 0.0013330187258414096\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03351955965989166\n",
      "Average test loss: 0.0012995544780149229\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03319601111776299\n",
      "Average test loss: 0.00134330596950733\n",
      "Epoch 106/300\n",
      "Average training loss: 0.033079112395644185\n",
      "Average test loss: 0.0013018203745078709\n",
      "Epoch 107/300\n",
      "Average training loss: 0.033000147961907915\n",
      "Average test loss: 0.0013019477999769151\n",
      "Epoch 108/300\n",
      "Average training loss: 0.032927825714151066\n",
      "Average test loss: 0.0013276287618403633\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03292510085801283\n",
      "Average test loss: 0.001358194278863569\n",
      "Epoch 110/300\n",
      "Average training loss: 0.032764761176374224\n",
      "Average test loss: 0.001328484813682735\n",
      "Epoch 111/300\n",
      "Average training loss: 0.032723415818479325\n",
      "Average test loss: 0.0013482837076816295\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03270657795170943\n",
      "Average test loss: 0.0014238542571870817\n",
      "Epoch 113/300\n",
      "Average training loss: 0.032374323945906426\n",
      "Average test loss: 0.0013278160478091903\n",
      "Epoch 114/300\n",
      "Average training loss: 0.032332532316446304\n",
      "Average test loss: 0.0013145401812055045\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03242021816637781\n",
      "Average test loss: 0.001311566260850264\n",
      "Epoch 116/300\n",
      "Average training loss: 0.032215234842565325\n",
      "Average test loss: 0.0013318582643858262\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03246112026439773\n",
      "Average test loss: 0.0013000736565639576\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03203073478572899\n",
      "Average test loss: 0.0013096661923660173\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03193602330817116\n",
      "Average test loss: 0.0013295562961656187\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03191882612142298\n",
      "Average test loss: 0.0013280365521916085\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03185173643959893\n",
      "Average test loss: 0.0013911005143697063\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0317516682114866\n",
      "Average test loss: 0.0014079009075649083\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03184362580213282\n",
      "Average test loss: 0.001350775540702873\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03170213560263316\n",
      "Average test loss: 0.0013296371592829625\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03152995416190889\n",
      "Average test loss: 0.0013563579366438919\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03136393221384949\n",
      "Average test loss: 0.0013386857398889132\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03139495264324877\n",
      "Average test loss: 0.001324308516147236\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03150356154971653\n",
      "Average test loss: 0.0013637505425657663\n",
      "Epoch 129/300\n",
      "Average training loss: 0.031412331700325015\n",
      "Average test loss: 0.0014046931229531765\n",
      "Epoch 130/300\n",
      "Average training loss: 0.031233801735772025\n",
      "Average test loss: 0.001370687363255355\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03124436354637146\n",
      "Average test loss: 0.0014334315003620255\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03111496396197213\n",
      "Average test loss: 0.0013304877412608927\n",
      "Epoch 133/300\n",
      "Average training loss: 0.031069463532831933\n",
      "Average test loss: 0.0013489957199328475\n",
      "Epoch 134/300\n",
      "Average training loss: 0.030997066436542407\n",
      "Average test loss: 0.001340989914101859\n",
      "Epoch 135/300\n",
      "Average training loss: 0.030955310228798126\n",
      "Average test loss: 0.0013458629813976586\n",
      "Epoch 136/300\n",
      "Average training loss: 0.030858880538079475\n",
      "Average test loss: 0.001373799342662096\n",
      "Epoch 137/300\n",
      "Average training loss: 0.030847721644573743\n",
      "Average test loss: 0.001356276394902832\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03078603192501598\n",
      "Average test loss: 0.0013835642747581005\n",
      "Epoch 139/300\n",
      "Average training loss: 0.030724601017104256\n",
      "Average test loss: 0.001354530192601184\n",
      "Epoch 140/300\n",
      "Average training loss: 0.030623102385136818\n",
      "Average test loss: 0.001384386130091217\n",
      "Epoch 141/300\n",
      "Average training loss: 0.030620970570378832\n",
      "Average test loss: 0.0013579512789017624\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03055367488000128\n",
      "Average test loss: 0.001400765262854596\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03052800120247735\n",
      "Average test loss: 0.0015541858344028394\n",
      "Epoch 144/300\n",
      "Average training loss: 0.030542177556289567\n",
      "Average test loss: 0.0013314807587820623\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03044737663699521\n",
      "Average test loss: 0.0013681803090601333\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0305430964993106\n",
      "Average test loss: 0.0013471601990362007\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03036025861071216\n",
      "Average test loss: 0.001372479018341336\n",
      "Epoch 148/300\n",
      "Average training loss: 0.030379373575250307\n",
      "Average test loss: 0.0013580321847564644\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030156862013869815\n",
      "Average test loss: 0.0013606670183233089\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030145207996169727\n",
      "Average test loss: 0.0013337685769009922\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030278906083769267\n",
      "Average test loss: 0.0013693319034969642\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03005491951107979\n",
      "Average test loss: 0.001352704228522877\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03002036471499337\n",
      "Average test loss: 0.001373022105027404\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03006023122370243\n",
      "Average test loss: 0.0013672309606336057\n",
      "Epoch 155/300\n",
      "Average training loss: 0.029967325606279904\n",
      "Average test loss: 0.0013736894147263633\n",
      "Epoch 156/300\n",
      "Average training loss: 0.029961573372284573\n",
      "Average test loss: 0.0013451898865815665\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02989547361433506\n",
      "Average test loss: 0.001384354644972417\n",
      "Epoch 158/300\n",
      "Average training loss: 0.029911344399054844\n",
      "Average test loss: 0.0014082283314524426\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02981104947792159\n",
      "Average test loss: 0.0014576701474272543\n",
      "Epoch 160/300\n",
      "Average training loss: 0.029853660157985157\n",
      "Average test loss: 0.0013810779522690508\n",
      "Epoch 161/300\n",
      "Average training loss: 0.029724073433213764\n",
      "Average test loss: 0.001386324693655802\n",
      "Epoch 162/300\n",
      "Average training loss: 0.029645490951008265\n",
      "Average test loss: 0.0013813387804871631\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029623014039463467\n",
      "Average test loss: 0.001346834011686345\n",
      "Epoch 164/300\n",
      "Average training loss: 0.029634043378962412\n",
      "Average test loss: 0.001389351502371331\n",
      "Epoch 165/300\n",
      "Average training loss: 0.029632053398423723\n",
      "Average test loss: 0.0013700428425023952\n",
      "Epoch 166/300\n",
      "Average training loss: 0.029520516753196717\n",
      "Average test loss: 0.0013684971415851678\n",
      "Epoch 167/300\n",
      "Average training loss: 0.029536767212880982\n",
      "Average test loss: 0.0013745532616869444\n",
      "Epoch 168/300\n",
      "Average training loss: 0.029535928724540604\n",
      "Average test loss: 0.0014253500745528274\n",
      "Epoch 169/300\n",
      "Average training loss: 0.029534265293015373\n",
      "Average test loss: 0.0013930140795807044\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02944289850857523\n",
      "Average test loss: 0.0013816040615654654\n",
      "Epoch 171/300\n",
      "Average training loss: 0.029345717446671592\n",
      "Average test loss: 0.0014017384831483165\n",
      "Epoch 172/300\n",
      "Average training loss: 0.029335071345170338\n",
      "Average test loss: 0.0013824181366297935\n",
      "Epoch 173/300\n",
      "Average training loss: 0.029359830391075876\n",
      "Average test loss: 0.0013822529345699068\n",
      "Epoch 174/300\n",
      "Average training loss: 0.029304860999186833\n",
      "Average test loss: 0.001376995659329825\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02922823039525085\n",
      "Average test loss: 0.0013868956077429983\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02918285960621304\n",
      "Average test loss: 0.001375638925159971\n",
      "Epoch 177/300\n",
      "Average training loss: 0.029346488417850602\n",
      "Average test loss: 0.0014123256543858184\n",
      "Epoch 178/300\n",
      "Average training loss: 0.029151568310128318\n",
      "Average test loss: 0.0013772953526220387\n",
      "Epoch 179/300\n",
      "Average training loss: 0.029070823493931027\n",
      "Average test loss: 0.0014049009904265404\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02911759015255504\n",
      "Average test loss: 0.0013746833290076918\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0291034125238657\n",
      "Average test loss: 0.001360113154579368\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02901719091501501\n",
      "Average test loss: 0.0013878978969943192\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0289586944596635\n",
      "Average test loss: 0.0013961068811412488\n",
      "Epoch 184/300\n",
      "Average training loss: 0.028920332403646574\n",
      "Average test loss: 0.0013789132356436715\n",
      "Epoch 185/300\n",
      "Average training loss: 0.028934371289279726\n",
      "Average test loss: 0.001412256342979769\n",
      "Epoch 186/300\n",
      "Average training loss: 0.028952619711558023\n",
      "Average test loss: 0.0014510426885551877\n",
      "Epoch 187/300\n",
      "Average training loss: 0.028906814250681135\n",
      "Average test loss: 0.0013669577891317506\n",
      "Epoch 188/300\n",
      "Average training loss: 0.028877081195513406\n",
      "Average test loss: 0.0014199594158886208\n",
      "Epoch 189/300\n",
      "Average training loss: 0.028854479626648957\n",
      "Average test loss: 0.0015838918287513985\n",
      "Epoch 190/300\n",
      "Average training loss: 0.028767114121052952\n",
      "Average test loss: 0.0014129961936010255\n",
      "Epoch 191/300\n",
      "Average training loss: 0.028751052343183092\n",
      "Average test loss: 0.0013781571729729572\n",
      "Epoch 192/300\n",
      "Average training loss: 0.028758394704924688\n",
      "Average test loss: 0.001424308668408129\n",
      "Epoch 193/300\n",
      "Average training loss: 0.028765810231367747\n",
      "Average test loss: 0.0014249968078608314\n",
      "Epoch 194/300\n",
      "Average training loss: 0.028721750974655152\n",
      "Average test loss: 0.0014460438419547346\n",
      "Epoch 195/300\n",
      "Average training loss: 0.028595444727275106\n",
      "Average test loss: 0.0014127715374343097\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02864369421203931\n",
      "Average test loss: 0.0014345553612543477\n",
      "Epoch 197/300\n",
      "Average training loss: 0.028630794038375217\n",
      "Average test loss: 0.001373239019782179\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02860079346762763\n",
      "Average test loss: 0.0014123083477218946\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02867307930853632\n",
      "Average test loss: 0.0013854997423994873\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02859085792965359\n",
      "Average test loss: 0.0013872057816738056\n",
      "Epoch 201/300\n",
      "Average training loss: 0.028537275455064244\n",
      "Average test loss: 0.0013897940883826879\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0285914463698864\n",
      "Average test loss: 0.0014092505739277436\n",
      "Epoch 203/300\n",
      "Average training loss: 0.028442191556096078\n",
      "Average test loss: 0.0014089708056093918\n",
      "Epoch 204/300\n",
      "Average training loss: 0.028436817419197825\n",
      "Average test loss: 0.0014057180889778668\n",
      "Epoch 205/300\n",
      "Average training loss: 0.028493457512723076\n",
      "Average test loss: 0.0014136310403442217\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02840191090438101\n",
      "Average test loss: 0.0013897555167786777\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028411738099323378\n",
      "Average test loss: 0.0014032149228991734\n",
      "Epoch 208/300\n",
      "Average training loss: 0.028367226052615376\n",
      "Average test loss: 0.001385382113409125\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02836152826746305\n",
      "Average test loss: 0.0014019827818394536\n",
      "Epoch 210/300\n",
      "Average training loss: 0.028306088907023273\n",
      "Average test loss: 0.001494709736150172\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02838710374136766\n",
      "Average test loss: 0.0014062836842300992\n",
      "Epoch 212/300\n",
      "Average training loss: 0.028281296579374206\n",
      "Average test loss: 0.0014390091509040859\n",
      "Epoch 213/300\n",
      "Average training loss: 0.028207923861013517\n",
      "Average test loss: 0.0014387148157176044\n",
      "Epoch 214/300\n",
      "Average training loss: 0.028213195646802584\n",
      "Average test loss: 0.0013677912601787183\n",
      "Epoch 215/300\n",
      "Average training loss: 0.028162353585163752\n",
      "Average test loss: 0.0013821932200549377\n",
      "Epoch 216/300\n",
      "Average training loss: 0.028197534668776725\n",
      "Average test loss: 0.0013888520470096005\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028121509921219614\n",
      "Average test loss: 0.0014178268718533218\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028167281460430887\n",
      "Average test loss: 0.0014179507409118944\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028117804386549525\n",
      "Average test loss: 0.0014444113713171748\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028137314947942894\n",
      "Average test loss: 0.0014829890039852924\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028074673059913846\n",
      "Average test loss: 0.0013935824077472919\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028081035794483292\n",
      "Average test loss: 0.0014040169374396404\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02800596146782239\n",
      "Average test loss: 0.001434207105336504\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028060244747334057\n",
      "Average test loss: 0.0014052031275091899\n",
      "Epoch 225/300\n",
      "Average training loss: 0.028029855479796726\n",
      "Average test loss: 0.0014116469331913524\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02796911763648192\n",
      "Average test loss: 0.001423742332547489\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02799719075858593\n",
      "Average test loss: 0.0014092712129155794\n",
      "Epoch 228/300\n",
      "Average training loss: 0.027987725075748232\n",
      "Average test loss: 0.0013966069815473424\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02797604448431068\n",
      "Average test loss: 0.0013849899623956945\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02789571643869082\n",
      "Average test loss: 0.0014048660692189718\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0278664144774278\n",
      "Average test loss: 0.0014063531932317548\n",
      "Epoch 232/300\n",
      "Average training loss: 0.027853956235779655\n",
      "Average test loss: 0.0014213060057825513\n",
      "Epoch 233/300\n",
      "Average training loss: 0.027814202793770365\n",
      "Average test loss: 0.001404836469847295\n",
      "Epoch 234/300\n",
      "Average training loss: 0.027818103838298056\n",
      "Average test loss: 0.0013881136182074746\n",
      "Epoch 235/300\n",
      "Average training loss: 0.027866947076386876\n",
      "Average test loss: 0.001420189692535334\n",
      "Epoch 236/300\n",
      "Average training loss: 0.027860647646917237\n",
      "Average test loss: 0.0014130107527598739\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02778070986436473\n",
      "Average test loss: 0.0013933319088278546\n",
      "Epoch 238/300\n",
      "Average training loss: 0.027799911008940803\n",
      "Average test loss: 0.0014479757340935368\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02777418162756496\n",
      "Average test loss: 0.00143931761632363\n",
      "Epoch 240/300\n",
      "Average training loss: 0.027786743357777595\n",
      "Average test loss: 0.0014057295291374128\n",
      "Epoch 241/300\n",
      "Average training loss: 0.027725631427433756\n",
      "Average test loss: 0.0015021042011471258\n",
      "Epoch 242/300\n",
      "Average training loss: 0.027660530606077777\n",
      "Average test loss: 0.0013975815038300224\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02771721140212483\n",
      "Average test loss: 0.0014024839837931925\n",
      "Epoch 244/300\n",
      "Average training loss: 0.027638571596807902\n",
      "Average test loss: 0.0014117205106239353\n",
      "Epoch 245/300\n",
      "Average training loss: 0.027620959497160383\n",
      "Average test loss: 0.0013989256968618268\n",
      "Epoch 246/300\n",
      "Average training loss: 0.027578567231694856\n",
      "Average test loss: 0.0014671495546483332\n",
      "Epoch 247/300\n",
      "Average training loss: 0.027658310484555033\n",
      "Average test loss: 0.0014147561460526453\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02777445720301734\n",
      "Average test loss: 0.0014934719373575515\n",
      "Epoch 249/300\n",
      "Average training loss: 0.027537972703576087\n",
      "Average test loss: 0.0014172095175211629\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027590241812997393\n",
      "Average test loss: 0.0014261337004394996\n",
      "Epoch 251/300\n",
      "Average training loss: 0.027497725867562824\n",
      "Average test loss: 0.001423557476244039\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02753618571990066\n",
      "Average test loss: 0.0013932911493918963\n",
      "Epoch 253/300\n",
      "Average training loss: 0.027512659887472788\n",
      "Average test loss: 0.0014682574659172032\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02753471637268861\n",
      "Average test loss: 0.001404285260476172\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02747233739991983\n",
      "Average test loss: 0.001405795589192874\n",
      "Epoch 256/300\n",
      "Average training loss: 0.027496000794900787\n",
      "Average test loss: 0.001482696843230062\n",
      "Epoch 257/300\n",
      "Average training loss: 0.027445645375384225\n",
      "Average test loss: 0.001433069510747575\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02745249928534031\n",
      "Average test loss: 0.0014284303727456265\n",
      "Epoch 259/300\n",
      "Average training loss: 0.027475275479257107\n",
      "Average test loss: 0.0014422654785836736\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02742836481001642\n",
      "Average test loss: 0.0014334425969670217\n",
      "Epoch 261/300\n",
      "Average training loss: 0.027374541338947084\n",
      "Average test loss: 0.00143355590912203\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027381652858522204\n",
      "Average test loss: 0.001440347667886979\n",
      "Epoch 263/300\n",
      "Average training loss: 0.027430441313319735\n",
      "Average test loss: 0.001382672554295924\n",
      "Epoch 264/300\n",
      "Average training loss: 0.027326530444953177\n",
      "Average test loss: 0.0014090847679310375\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02731207467781173\n",
      "Average test loss: 0.0014496321462922626\n",
      "Epoch 266/300\n",
      "Average training loss: 0.027374918387995825\n",
      "Average test loss: 0.0014190409431854884\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02732143537203471\n",
      "Average test loss: 0.0014107836369011138\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02729123004939821\n",
      "Average test loss: 0.0014236450946579376\n",
      "Epoch 269/300\n",
      "Average training loss: 0.027278803492585817\n",
      "Average test loss: 0.0014233737364411354\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027248865564664205\n",
      "Average test loss: 0.0017280725865728326\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027232715230849053\n",
      "Average test loss: 0.001421830769524806\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027197890763481457\n",
      "Average test loss: 0.0014246234599397413\n",
      "Epoch 273/300\n",
      "Average training loss: 0.027225259312325053\n",
      "Average test loss: 0.0014542841466350688\n",
      "Epoch 274/300\n",
      "Average training loss: 0.027297566749983362\n",
      "Average test loss: 0.0014219409284285374\n",
      "Epoch 275/300\n",
      "Average training loss: 0.027188796783487002\n",
      "Average test loss: 0.0014583817235090666\n",
      "Epoch 276/300\n",
      "Average training loss: 0.027149329364299774\n",
      "Average test loss: 0.0014279609223206838\n",
      "Epoch 277/300\n",
      "Average training loss: 0.027143782147102887\n",
      "Average test loss: 0.0013962335501176615\n",
      "Epoch 278/300\n",
      "Average training loss: 0.027182018347912364\n",
      "Average test loss: 0.001451363246784442\n",
      "Epoch 279/300\n",
      "Average training loss: 0.027157295788327852\n",
      "Average test loss: 0.001399884842750099\n",
      "Epoch 280/300\n",
      "Average training loss: 0.027148956254952485\n",
      "Average test loss: 0.001427393860856278\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0271027377396822\n",
      "Average test loss: 0.001519479071514474\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027082593844996557\n",
      "Average test loss: 0.001470476577989757\n",
      "Epoch 283/300\n",
      "Average training loss: 0.027135464039113787\n",
      "Average test loss: 0.001399167846940044\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0270556314057774\n",
      "Average test loss: 0.0014228265191324884\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02712065573533376\n",
      "Average test loss: 0.0014140511616650554\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02715209099319246\n",
      "Average test loss: 0.0014469150274785028\n",
      "Epoch 287/300\n",
      "Average training loss: 0.027035531798998515\n",
      "Average test loss: 0.0014533394014255869\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02700185185836421\n",
      "Average test loss: 0.0014565841308794916\n",
      "Epoch 289/300\n",
      "Average training loss: 0.027021427270438936\n",
      "Average test loss: 0.0014427316425782112\n",
      "Epoch 290/300\n",
      "Average training loss: 0.027020806691712803\n",
      "Average test loss: 0.001456535665732291\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02713822019762463\n",
      "Average test loss: 0.0014323351140030556\n",
      "Epoch 292/300\n",
      "Average training loss: 0.026978824478056697\n",
      "Average test loss: 0.0013964531481679943\n",
      "Epoch 293/300\n",
      "Average training loss: 0.026933502543303702\n",
      "Average test loss: 0.001444996017548773\n",
      "Epoch 294/300\n",
      "Average training loss: 0.027027395380867854\n",
      "Average test loss: 0.0016053321985527874\n",
      "Epoch 295/300\n",
      "Average training loss: 0.026928133442997933\n",
      "Average test loss: 0.0014990989033960634\n",
      "Epoch 296/300\n",
      "Average training loss: 0.026951595538192326\n",
      "Average test loss: 0.0013957786208743024\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02686477260126008\n",
      "Average test loss: 0.0014273666233445208\n",
      "Epoch 298/300\n",
      "Average training loss: 0.026931407175130314\n",
      "Average test loss: 0.0014504358446639446\n",
      "Epoch 299/300\n",
      "Average training loss: 0.026934857909878096\n",
      "Average test loss: 0.001427196256402466\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02693017763727241\n",
      "Average test loss: 0.0014224741513737373\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_64_Depth5/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.60\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.59\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.01\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.23\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.48\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.49\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.68\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.81\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.25\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.49\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.75\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.80\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.84\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.93\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.01\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.10\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.11\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.14\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.30\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.34\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.50\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.51\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.66\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.50\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.44\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.53\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.90\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.30\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.63\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.59\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 33.57\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 33.59\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.63\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.65\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.55\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.87\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.80\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.88\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 34.09\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 34.06\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 34.21\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 34.30\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 34.37\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 34.40\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.36\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
