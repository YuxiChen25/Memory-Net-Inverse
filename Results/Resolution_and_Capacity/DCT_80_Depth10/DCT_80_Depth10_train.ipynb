{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f629ed4f-ff48-414b-8931-c04449ab0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea188b14-1a2d-4acf-99ad-4deccbea1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588ed776-1f23-464d-ad09-0054d8c05e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a770293-62ad-4e57-beb9-5d990eea4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_80x80_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557a5109-c40e-47a7-8499-46f555b49a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5856e7ad-e221-420f-bcb5-277d6236a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c327900a-a43c-4cf4-b765-1d34d6558c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.09859668471415838\n",
      "Average test loss: 0.0046017511681550076\n",
      "Epoch 2/300\n",
      "Average training loss: 0.025237096832030348\n",
      "Average test loss: 0.004154084361882674\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022402093955212168\n",
      "Average test loss: 0.004135282625754674\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02140487400856283\n",
      "Average test loss: 0.0039202661474959715\n",
      "Epoch 5/300\n",
      "Average training loss: 0.020873026927312216\n",
      "Average test loss: 0.0038801372939099867\n",
      "Epoch 6/300\n",
      "Average training loss: 0.020538976851436828\n",
      "Average test loss: 0.0038703964365025363\n",
      "Epoch 7/300\n",
      "Average training loss: 0.020297817630900276\n",
      "Average test loss: 0.0038147166180941793\n",
      "Epoch 8/300\n",
      "Average training loss: 0.020089807959066498\n",
      "Average test loss: 0.0038288691975176333\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01991921059290568\n",
      "Average test loss: 0.0037837144188168975\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0197758090843757\n",
      "Average test loss: 0.0037592498229609595\n",
      "Epoch 11/300\n",
      "Average training loss: 0.019634081544147596\n",
      "Average test loss: 0.0037357267832590474\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019498614481753773\n",
      "Average test loss: 0.003715658610065778\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01936974379171928\n",
      "Average test loss: 0.0037168235931959418\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019250786375668315\n",
      "Average test loss: 0.0037075094874534343\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01913321917090151\n",
      "Average test loss: 0.0036954513804780113\n",
      "Epoch 16/300\n",
      "Average training loss: 0.019028825853433875\n",
      "Average test loss: 0.0036917172914577856\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01892923852139049\n",
      "Average test loss: 0.0036685984534107975\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018832371599144405\n",
      "Average test loss: 0.0036189184250930945\n",
      "Epoch 19/300\n",
      "Average training loss: 0.018748091247346665\n",
      "Average test loss: 0.0036242438447144296\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018665618340174356\n",
      "Average test loss: 0.0036090238015684816\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01859082519511382\n",
      "Average test loss: 0.00360745061106152\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01852093089454704\n",
      "Average test loss: 0.0036094098852740395\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018459582808944913\n",
      "Average test loss: 0.003592466399901443\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01840338593390253\n",
      "Average test loss: 0.003565925454099973\n",
      "Epoch 25/300\n",
      "Average training loss: 0.018336712438199256\n",
      "Average test loss: 0.003543651717818446\n",
      "Epoch 26/300\n",
      "Average training loss: 0.018288250090347395\n",
      "Average test loss: 0.0035453778654336928\n",
      "Epoch 27/300\n",
      "Average training loss: 0.018236417808466486\n",
      "Average test loss: 0.003549985198511018\n",
      "Epoch 28/300\n",
      "Average training loss: 0.018184582418865627\n",
      "Average test loss: 0.003539400717450513\n",
      "Epoch 29/300\n",
      "Average training loss: 0.018145652671655018\n",
      "Average test loss: 0.0035383733662052285\n",
      "Epoch 30/300\n",
      "Average training loss: 0.018101273672448265\n",
      "Average test loss: 0.0035324023241798085\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01805907103419304\n",
      "Average test loss: 0.0035243176004538935\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01800251223809189\n",
      "Average test loss: 0.0035153698465890354\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01798408883313338\n",
      "Average test loss: 0.0035217229241712228\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017938520867791442\n",
      "Average test loss: 0.003527098231431511\n",
      "Epoch 35/300\n",
      "Average training loss: 0.017915330928232935\n",
      "Average test loss: 0.0035168169463674227\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01787336726900604\n",
      "Average test loss: 0.0035064751371327372\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01784179326891899\n",
      "Average test loss: 0.003497921396460798\n",
      "Epoch 38/300\n",
      "Average training loss: 0.017808404023448625\n",
      "Average test loss: 0.0035054963019986946\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01777312131722768\n",
      "Average test loss: 0.0034921223434309164\n",
      "Epoch 40/300\n",
      "Average training loss: 0.017743401896622445\n",
      "Average test loss: 0.0034940179843041632\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01772236233452956\n",
      "Average test loss: 0.003505615867053469\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01768994741141796\n",
      "Average test loss: 0.0034938830760204128\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0176599953878257\n",
      "Average test loss: 0.003502140314214759\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01763587401807308\n",
      "Average test loss: 0.0035046399728291566\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01760705356299877\n",
      "Average test loss: 0.003487327482137415\n",
      "Epoch 46/300\n",
      "Average training loss: 0.017587225468622313\n",
      "Average test loss: 0.0034989151666975685\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017555352000726592\n",
      "Average test loss: 0.003507885438286596\n",
      "Epoch 48/300\n",
      "Average training loss: 0.017535254740880597\n",
      "Average test loss: 0.0034931241311132906\n",
      "Epoch 49/300\n",
      "Average training loss: 0.017522959396243095\n",
      "Average test loss: 0.0034892595461052324\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01749045852985647\n",
      "Average test loss: 0.0034908220983213846\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01746515534735388\n",
      "Average test loss: 0.0035312904614127343\n",
      "Epoch 52/300\n",
      "Average training loss: 0.017436218024955857\n",
      "Average test loss: 0.00347976715862751\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017423329633143213\n",
      "Average test loss: 0.003506579232505626\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01739316484746006\n",
      "Average test loss: 0.003501127246560322\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017380860960317983\n",
      "Average test loss: 0.003486441992637184\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017356419793433612\n",
      "Average test loss: 0.00349885568043424\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017321560783518684\n",
      "Average test loss: 0.003487503547221422\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01730525830636422\n",
      "Average test loss: 0.0034776674509048463\n",
      "Epoch 59/300\n",
      "Average training loss: 0.017291122233702078\n",
      "Average test loss: 0.0034986109539038605\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017260052712427244\n",
      "Average test loss: 0.0034991832528677254\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017238530468609597\n",
      "Average test loss: 0.003532939539394445\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01722238546775447\n",
      "Average test loss: 0.003511344507129656\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017199974252118003\n",
      "Average test loss: 0.0035096441532174744\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017182834348744815\n",
      "Average test loss: 0.0035188695366183916\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01716121143599351\n",
      "Average test loss: 0.0034915597467786736\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01715415818244219\n",
      "Average test loss: 0.003496363361262613\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0171098805649413\n",
      "Average test loss: 0.0035157529099120034\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01709591486884488\n",
      "Average test loss: 0.003511343314415879\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017077479176637197\n",
      "Average test loss: 0.0035989946093824177\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01705570321612888\n",
      "Average test loss: 0.00348892929053141\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017037625402212143\n",
      "Average test loss: 0.003510267412910859\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017008896123203967\n",
      "Average test loss: 0.0035308158840570186\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016990186658170488\n",
      "Average test loss: 0.0035432877366741496\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016981194402608608\n",
      "Average test loss: 0.0035388799127605225\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01696012788348728\n",
      "Average test loss: 0.00350267181545496\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01694585861845149\n",
      "Average test loss: 0.0035091532257695993\n",
      "Epoch 77/300\n",
      "Average training loss: 0.016925677755640613\n",
      "Average test loss: 0.0035831191701193653\n",
      "Epoch 78/300\n",
      "Average training loss: 0.016902972266077995\n",
      "Average test loss: 0.0035440472434792253\n",
      "Epoch 79/300\n",
      "Average training loss: 0.016883841516243087\n",
      "Average test loss: 0.0035145673112322888\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0168642465621233\n",
      "Average test loss: 0.00352602066554957\n",
      "Epoch 81/300\n",
      "Average training loss: 0.016851966241995493\n",
      "Average test loss: 0.0035060341056022378\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01682554889221986\n",
      "Average test loss: 0.0035087688782562813\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01682251994725731\n",
      "Average test loss: 0.0034988510496914388\n",
      "Epoch 84/300\n",
      "Average training loss: 0.016789475141300095\n",
      "Average test loss: 0.003548946970452865\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01677665170199341\n",
      "Average test loss: 0.003532312933769491\n",
      "Epoch 86/300\n",
      "Average training loss: 0.016766665778226324\n",
      "Average test loss: 0.0035038221809599135\n",
      "Epoch 87/300\n",
      "Average training loss: 0.016751445868776905\n",
      "Average test loss: 0.003538051704151763\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01672800938288371\n",
      "Average test loss: 0.003601821655821469\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016720421539412604\n",
      "Average test loss: 0.003507129462228881\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016695558765696155\n",
      "Average test loss: 0.0035896155128462446\n",
      "Epoch 91/300\n",
      "Average training loss: 0.016683575573894712\n",
      "Average test loss: 0.0036014231499284506\n",
      "Epoch 92/300\n",
      "Average training loss: 0.016663563273847104\n",
      "Average test loss: 0.0035304204641530912\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01664859943671359\n",
      "Average test loss: 0.003539766929215855\n",
      "Epoch 94/300\n",
      "Average training loss: 0.016641020180450546\n",
      "Average test loss: 0.0035476476001656716\n",
      "Epoch 95/300\n",
      "Average training loss: 0.016631514115466012\n",
      "Average test loss: 0.003575675613143378\n",
      "Epoch 96/300\n",
      "Average training loss: 0.016609047677781846\n",
      "Average test loss: 0.003528136178644167\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016589681968092917\n",
      "Average test loss: 0.0035528396508759923\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01657306379907661\n",
      "Average test loss: 0.003520085765255822\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016566752735111448\n",
      "Average test loss: 0.0035364031170805293\n",
      "Epoch 100/300\n",
      "Average training loss: 0.016553475516537824\n",
      "Average test loss: 0.0036148188766092063\n",
      "Epoch 101/300\n",
      "Average training loss: 0.016536511833469072\n",
      "Average test loss: 0.0035240691916810143\n",
      "Epoch 102/300\n",
      "Average training loss: 0.016521997665365537\n",
      "Average test loss: 0.0035740425665345458\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016510824968417485\n",
      "Average test loss: 0.00355527275097039\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016508219529357222\n",
      "Average test loss: 0.0035840044065068165\n",
      "Epoch 105/300\n",
      "Average training loss: 0.016476985154880417\n",
      "Average test loss: 0.0035645495077802074\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01646475852529208\n",
      "Average test loss: 0.003629455028515723\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016462297328644328\n",
      "Average test loss: 0.003623664279364877\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016447399405969513\n",
      "Average test loss: 0.003566631409029166\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01643739739391539\n",
      "Average test loss: 0.0035758611284610296\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016417266584104962\n",
      "Average test loss: 0.0035841616325908237\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016403309949570233\n",
      "Average test loss: 0.0037423873738282256\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016380021478566858\n",
      "Average test loss: 0.0035744055004583463\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016385663136839868\n",
      "Average test loss: 0.0036142792385071517\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016364037752979333\n",
      "Average test loss: 0.0035474623234735595\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01635935549189647\n",
      "Average test loss: 0.0035950753049304087\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01634530140956243\n",
      "Average test loss: 0.0036659012200931707\n",
      "Epoch 117/300\n",
      "Average training loss: 0.016335218058692084\n",
      "Average test loss: 0.0036507617529067727\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016325185399916436\n",
      "Average test loss: 0.003560613701120019\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01631032923857371\n",
      "Average test loss: 0.0035707580517563555\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016300378998120625\n",
      "Average test loss: 0.003646120997145772\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016294295428527727\n",
      "Average test loss: 0.0035795792875190574\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016276997938752175\n",
      "Average test loss: 0.0037133587300777434\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016270955829984612\n",
      "Average test loss: 0.003541511215062605\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0162524976266755\n",
      "Average test loss: 0.003609209390150176\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016241321383251083\n",
      "Average test loss: 0.0036407319959253074\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01623776197930177\n",
      "Average test loss: 0.0036232290625986124\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01623494187080198\n",
      "Average test loss: 0.0035713253426882955\n",
      "Epoch 128/300\n",
      "Average training loss: 0.016216679848316644\n",
      "Average test loss: 0.003661291288211942\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016207410292492973\n",
      "Average test loss: 0.0037014844318230948\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016183929761250816\n",
      "Average test loss: 0.0035723617809514203\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016200277456806765\n",
      "Average test loss: 0.0035781222190707923\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01617027424607012\n",
      "Average test loss: 0.003609362209422721\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016158052562839455\n",
      "Average test loss: 0.0036107180710468027\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016149666344126064\n",
      "Average test loss: 0.0036000476193924747\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016156128454539512\n",
      "Average test loss: 0.003596132179308269\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01614072085585859\n",
      "Average test loss: 0.00366671657976177\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016132903830872642\n",
      "Average test loss: 0.003588326700238718\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016118908944229286\n",
      "Average test loss: 0.003668713973214229\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0161197334461742\n",
      "Average test loss: 0.003628497670508093\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016101938255959087\n",
      "Average test loss: 0.0035595854388342963\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01608082795060343\n",
      "Average test loss: 0.0036981614886058703\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01608844105899334\n",
      "Average test loss: 0.003596860889138447\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01607705671257443\n",
      "Average test loss: 0.0035684347730129957\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01606433750854598\n",
      "Average test loss: 0.00359620991618269\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01605594341788027\n",
      "Average test loss: 0.00364374908266796\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01604524425499969\n",
      "Average test loss: 0.0036932105285425983\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016048535126778816\n",
      "Average test loss: 0.003612606510830422\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016024977568123076\n",
      "Average test loss: 0.003555655773315165\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01601809465719594\n",
      "Average test loss: 0.0037365544939206704\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016024749028186003\n",
      "Average test loss: 0.0036327328661249743\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01600943784084585\n",
      "Average test loss: 0.0036483505263717637\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01599348279916578\n",
      "Average test loss: 0.00365563866454694\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01599635348220666\n",
      "Average test loss: 0.003618128203269508\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01597684979190429\n",
      "Average test loss: 0.003583760561214553\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015974516049027443\n",
      "Average test loss: 0.00362011994028257\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015978650249540805\n",
      "Average test loss: 0.0036141891514675484\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01594941179205974\n",
      "Average test loss: 0.003746092905393905\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015947836842801835\n",
      "Average test loss: 0.003699365032629834\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01594576503833135\n",
      "Average test loss: 0.0036877488129668764\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01593539946443505\n",
      "Average test loss: 0.0036047370502104363\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01593543144563834\n",
      "Average test loss: 0.0036462540158794985\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015922420951227347\n",
      "Average test loss: 0.0036429907642304897\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015921783844629923\n",
      "Average test loss: 0.0036630419716238974\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01590414516793357\n",
      "Average test loss: 0.0036344846532576614\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01589587549865246\n",
      "Average test loss: 0.0036455937046557664\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0158911547727055\n",
      "Average test loss: 0.0036767996909717717\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015889311234984133\n",
      "Average test loss: 0.0036452555602623357\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01588817614565293\n",
      "Average test loss: 0.0037982392633954683\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01586984584480524\n",
      "Average test loss: 0.003631096239512165\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015863968308601116\n",
      "Average test loss: 0.0036929689484337964\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015861943071915043\n",
      "Average test loss: 0.0036592631687720617\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015854157813721233\n",
      "Average test loss: 0.003671570740226242\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015841710122923056\n",
      "Average test loss: 0.0036139202782263357\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015844985789722866\n",
      "Average test loss: 0.0035928464213179218\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015837402648395962\n",
      "Average test loss: 0.0036635403773850866\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015823905453913743\n",
      "Average test loss: 0.003702386481273505\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015812748717765012\n",
      "Average test loss: 0.0036707237840940556\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015797212182647652\n",
      "Average test loss: 0.003614579228684306\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015804359027081067\n",
      "Average test loss: 0.0036470870673656463\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015802515706254378\n",
      "Average test loss: 0.003657387055042717\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01579155429949363\n",
      "Average test loss: 0.00366671280687054\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015788419942061108\n",
      "Average test loss: 0.0036400113757699727\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015783590886327956\n",
      "Average test loss: 0.003635776363727119\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015778115674853325\n",
      "Average test loss: 0.003700700044631958\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01576827413505978\n",
      "Average test loss: 0.0036725622843950985\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015764702868958314\n",
      "Average test loss: 0.003630168797241317\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01576966350691186\n",
      "Average test loss: 0.0036816837129493556\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015755559918781123\n",
      "Average test loss: 0.0036778304042915503\n",
      "Epoch 189/300\n",
      "Average training loss: 0.015744664028286934\n",
      "Average test loss: 0.0037145942815889916\n",
      "Epoch 190/300\n",
      "Average training loss: 0.015723143610689376\n",
      "Average test loss: 0.003662559040097727\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01572952162143257\n",
      "Average test loss: 0.003649318046040005\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01572475117693345\n",
      "Average test loss: 0.003657269893421067\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01572762545115418\n",
      "Average test loss: 0.0036383270625438956\n",
      "Epoch 194/300\n",
      "Average training loss: 0.015720085508293576\n",
      "Average test loss: 0.003762763219161166\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015712752260267734\n",
      "Average test loss: 0.003625762505042884\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01569948001785411\n",
      "Average test loss: 0.0036623683555258644\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01569710008551677\n",
      "Average test loss: 0.003669407545071509\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015690852851503425\n",
      "Average test loss: 0.003717284076122774\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01568254373719295\n",
      "Average test loss: 0.0036800248213112352\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015681432743867237\n",
      "Average test loss: 0.003636781384961473\n",
      "Epoch 201/300\n",
      "Average training loss: 0.015674681236346563\n",
      "Average test loss: 0.0037144031222495766\n",
      "Epoch 202/300\n",
      "Average training loss: 0.015681045999957455\n",
      "Average test loss: 0.0036127235583133166\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01567310723910729\n",
      "Average test loss: 0.00366899752492706\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015655435051355096\n",
      "Average test loss: 0.0036822026690675152\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01566366203212076\n",
      "Average test loss: 0.003756627068337467\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01565037728846073\n",
      "Average test loss: 0.003690100103823675\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01565024168209897\n",
      "Average test loss: 0.003680027612381511\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01562500944485267\n",
      "Average test loss: 0.003654960244894028\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01564196938276291\n",
      "Average test loss: 0.003682574615503351\n",
      "Epoch 210/300\n",
      "Average training loss: 0.015632514913876853\n",
      "Average test loss: 0.003652853247606092\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015623457826673985\n",
      "Average test loss: 0.003732131469166941\n",
      "Epoch 212/300\n",
      "Average training loss: 0.015618990237514178\n",
      "Average test loss: 0.003612962409026093\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01561566355244981\n",
      "Average test loss: 0.0036766041587624285\n",
      "Epoch 214/300\n",
      "Average training loss: 0.015602221786148018\n",
      "Average test loss: 0.003773001967618863\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015598132679031955\n",
      "Average test loss: 0.0036580521244969633\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015606787971324391\n",
      "Average test loss: 0.0036336744289017387\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015590979616675112\n",
      "Average test loss: 0.0037020408478048114\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015596884979969925\n",
      "Average test loss: 0.003678913204206361\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015577683207061556\n",
      "Average test loss: 0.003740025502940019\n",
      "Epoch 220/300\n",
      "Average training loss: 0.015581321377721098\n",
      "Average test loss: 0.0036791297019355827\n",
      "Epoch 221/300\n",
      "Average training loss: 0.015589346989161439\n",
      "Average test loss: 0.003730684709217813\n",
      "Epoch 222/300\n",
      "Average training loss: 0.015573856159216828\n",
      "Average test loss: 0.003626149720615811\n",
      "Epoch 223/300\n",
      "Average training loss: 0.015565073788166047\n",
      "Average test loss: 0.0037521650838769145\n",
      "Epoch 224/300\n",
      "Average training loss: 0.015564254852632681\n",
      "Average test loss: 0.003795445557476746\n",
      "Epoch 225/300\n",
      "Average training loss: 0.015564354687929154\n",
      "Average test loss: 0.003705853558248944\n",
      "Epoch 226/300\n",
      "Average training loss: 0.015548081383109093\n",
      "Average test loss: 0.003693126786914137\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015548171944088406\n",
      "Average test loss: 0.003798289924239119\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015545076633493105\n",
      "Average test loss: 0.0037177646685805587\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015539225603143374\n",
      "Average test loss: 0.003663187995346056\n",
      "Epoch 230/300\n",
      "Average training loss: 0.015531823398338423\n",
      "Average test loss: 0.003738906125641531\n",
      "Epoch 231/300\n",
      "Average training loss: 0.015537372498876519\n",
      "Average test loss: 0.003869888700958755\n",
      "Epoch 232/300\n",
      "Average training loss: 0.015521406550374296\n",
      "Average test loss: 0.0037014101333916185\n",
      "Epoch 233/300\n",
      "Average training loss: 0.015521785562237104\n",
      "Average test loss: 0.0037524212035867904\n",
      "Epoch 234/300\n",
      "Average training loss: 0.015529342356655334\n",
      "Average test loss: 0.0036709164215458765\n",
      "Epoch 235/300\n",
      "Average training loss: 0.015519706059661176\n",
      "Average test loss: 0.0036621263114114603\n",
      "Epoch 236/300\n",
      "Average training loss: 0.015506800402369764\n",
      "Average test loss: 0.0036710541850576797\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015514785257478556\n",
      "Average test loss: 0.003704560753578941\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015502377186384466\n",
      "Average test loss: 0.003735222007665369\n",
      "Epoch 239/300\n",
      "Average training loss: 0.015498015515506268\n",
      "Average test loss: 0.0036965869679633115\n",
      "Epoch 240/300\n",
      "Average training loss: 0.015484982715712654\n",
      "Average test loss: 0.003689176204510861\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015485766499406762\n",
      "Average test loss: 0.00370805391534749\n",
      "Epoch 242/300\n",
      "Average training loss: 0.015488959026833376\n",
      "Average test loss: 0.003697407762830456\n",
      "Epoch 243/300\n",
      "Average training loss: 0.015471152305603027\n",
      "Average test loss: 0.0037470757220354344\n",
      "Epoch 244/300\n",
      "Average training loss: 0.015478355024423865\n",
      "Average test loss: 0.0037189969807449315\n",
      "Epoch 245/300\n",
      "Average training loss: 0.015473986041214732\n",
      "Average test loss: 0.0036745709081490835\n",
      "Epoch 246/300\n",
      "Average training loss: 0.015464384169214302\n",
      "Average test loss: 0.0037531746543116038\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015468640128771464\n",
      "Average test loss: 0.003755716052527229\n",
      "Epoch 248/300\n",
      "Average training loss: 0.015454358546270265\n",
      "Average test loss: 0.0036812073054413\n",
      "Epoch 249/300\n",
      "Average training loss: 0.015465489026572968\n",
      "Average test loss: 0.0036934158843424584\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015447365174690883\n",
      "Average test loss: 0.0037472487321744364\n",
      "Epoch 251/300\n",
      "Average training loss: 0.015448396212524838\n",
      "Average test loss: 0.0036902612191107536\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01545297141538726\n",
      "Average test loss: 0.003669996762648225\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0154380938783288\n",
      "Average test loss: 0.0036826237458735703\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015431836292147637\n",
      "Average test loss: 0.0036680322804798684\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015435960761374897\n",
      "Average test loss: 0.0036923772267376384\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015434211822847526\n",
      "Average test loss: 0.003665490910410881\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015418653650416269\n",
      "Average test loss: 0.003815077147963974\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015420289851725102\n",
      "Average test loss: 0.0037253691421614754\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01542637314564652\n",
      "Average test loss: 0.0037312785589860547\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015412188935610983\n",
      "Average test loss: 0.003696994507892264\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015405682460301452\n",
      "Average test loss: 0.0037923710855344932\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015402346932225757\n",
      "Average test loss: 0.0037335487066043746\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015395364077554808\n",
      "Average test loss: 0.0038534008614304993\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01540810884700881\n",
      "Average test loss: 0.0037249032201038465\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015397608048386044\n",
      "Average test loss: 0.0037903263146678605\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015385960421628422\n",
      "Average test loss: 0.003662477808487084\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015397764124804073\n",
      "Average test loss: 0.003633877817955282\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015382133389512697\n",
      "Average test loss: 0.0037567456331517963\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015377260253661209\n",
      "Average test loss: 0.0037023574316667187\n",
      "Epoch 270/300\n",
      "Average training loss: 0.015379411939945485\n",
      "Average test loss: 0.003694435758723153\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015375221498310566\n",
      "Average test loss: 0.0036678573828604487\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015372220985591411\n",
      "Average test loss: 0.003758608601573441\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015361838831669755\n",
      "Average test loss: 0.0036993600372225047\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01537547018378973\n",
      "Average test loss: 0.0037341317302650875\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015361492596566677\n",
      "Average test loss: 0.0037051832096444237\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015351661031444867\n",
      "Average test loss: 0.0037886976533465914\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015347364792393313\n",
      "Average test loss: 0.0037159184027049275\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015348103308843241\n",
      "Average test loss: 0.0037073927904582686\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015345865393678347\n",
      "Average test loss: 0.00371145085659292\n",
      "Epoch 280/300\n",
      "Average training loss: 0.015341011736955908\n",
      "Average test loss: 0.0037171926096909577\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015351060607367092\n",
      "Average test loss: 0.003705383938219812\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015344070553779602\n",
      "Average test loss: 0.003744263814141353\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015345243916743332\n",
      "Average test loss: 0.003719160321686003\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015329238980180687\n",
      "Average test loss: 0.0037849249475532106\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015327523153689173\n",
      "Average test loss: 0.0037032350897789\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015328624463743635\n",
      "Average test loss: 0.0037793557960540056\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015321213417583041\n",
      "Average test loss: 0.003768359146805273\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015306601392726103\n",
      "Average test loss: 0.0037553051091316673\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015307505250804954\n",
      "Average test loss: 0.0037283773560904793\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015321475264098909\n",
      "Average test loss: 0.003776059889545043\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015303046218223043\n",
      "Average test loss: 0.003748274134265052\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015309330367379718\n",
      "Average test loss: 0.003806327985599637\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015313730742368433\n",
      "Average test loss: 0.003777449635995759\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015295511978367964\n",
      "Average test loss: 0.003781392720424467\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015295719203021791\n",
      "Average test loss: 0.0037849749614381127\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015295157710711162\n",
      "Average test loss: 0.00371002579977115\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015295128519336383\n",
      "Average test loss: 0.0038026843306918938\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015297949235472414\n",
      "Average test loss: 0.0037247962063799303\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015280386851065688\n",
      "Average test loss: 0.0037383602182898255\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01528800560037295\n",
      "Average test loss: 0.0037595662060711118\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.09096059236427148\n",
      "Average test loss: 0.006377004251711898\n",
      "Epoch 2/300\n",
      "Average training loss: 0.022972966777781646\n",
      "Average test loss: 0.0036463445727196006\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02003829770286878\n",
      "Average test loss: 0.003569470554176304\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018942115608188842\n",
      "Average test loss: 0.0034005457535386087\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018349825854930613\n",
      "Average test loss: 0.0033023918914712137\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017936356176932653\n",
      "Average test loss: 0.00324863417694966\n",
      "Epoch 7/300\n",
      "Average training loss: 0.017610316729380026\n",
      "Average test loss: 0.003203885859499375\n",
      "Epoch 8/300\n",
      "Average training loss: 0.017324210302697287\n",
      "Average test loss: 0.0031169987838301394\n",
      "Epoch 9/300\n",
      "Average training loss: 0.017079307604167197\n",
      "Average test loss: 0.0030935744817058245\n",
      "Epoch 10/300\n",
      "Average training loss: 0.016836773097515106\n",
      "Average test loss: 0.003044565758978327\n",
      "Epoch 11/300\n",
      "Average training loss: 0.016599319208827283\n",
      "Average test loss: 0.0029782693969706694\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01637585358818372\n",
      "Average test loss: 0.002930149351143175\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01615794370571772\n",
      "Average test loss: 0.0029242593281798893\n",
      "Epoch 14/300\n",
      "Average training loss: 0.015968634982075958\n",
      "Average test loss: 0.0029348328537825083\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01578169619043668\n",
      "Average test loss: 0.0028626467742853693\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015593858956462807\n",
      "Average test loss: 0.002816441730285684\n",
      "Epoch 17/300\n",
      "Average training loss: 0.015419182317952316\n",
      "Average test loss: 0.0028385523154089848\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015260215544038348\n",
      "Average test loss: 0.002856116251813041\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015106870738168557\n",
      "Average test loss: 0.0027662632446736096\n",
      "Epoch 20/300\n",
      "Average training loss: 0.014948801121777959\n",
      "Average test loss: 0.002725408335423304\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014818191473682721\n",
      "Average test loss: 0.0027181006005654734\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01468006946063704\n",
      "Average test loss: 0.0026781874522566794\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014561330522927973\n",
      "Average test loss: 0.0026808177092009122\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014431489400565625\n",
      "Average test loss: 0.002656504635595613\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014342021405696868\n",
      "Average test loss: 0.0027227514795958997\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014238673421243827\n",
      "Average test loss: 0.0026154358384923803\n",
      "Epoch 27/300\n",
      "Average training loss: 0.014133529611759716\n",
      "Average test loss: 0.002622674010280106\n",
      "Epoch 28/300\n",
      "Average training loss: 0.014049103245139123\n",
      "Average test loss: 0.0026173675143056447\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013980650702284441\n",
      "Average test loss: 0.0026031375500477024\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013882905538297362\n",
      "Average test loss: 0.0025802694687412846\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013814779774182372\n",
      "Average test loss: 0.0025637856100996334\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013762060899701384\n",
      "Average test loss: 0.00260020420824488\n",
      "Epoch 33/300\n",
      "Average training loss: 0.013700601893994543\n",
      "Average test loss: 0.0026069284938275814\n",
      "Epoch 34/300\n",
      "Average training loss: 0.013626587678988774\n",
      "Average test loss: 0.0025583137849138843\n",
      "Epoch 35/300\n",
      "Average training loss: 0.013588511989348465\n",
      "Average test loss: 0.002572485289019015\n",
      "Epoch 36/300\n",
      "Average training loss: 0.013530234407219621\n",
      "Average test loss: 0.0025400516192118326\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013491024481753508\n",
      "Average test loss: 0.0025333986431360245\n",
      "Epoch 38/300\n",
      "Average training loss: 0.013432632851103942\n",
      "Average test loss: 0.0025616395852218073\n",
      "Epoch 39/300\n",
      "Average training loss: 0.013378104242185752\n",
      "Average test loss: 0.002519527616393235\n",
      "Epoch 40/300\n",
      "Average training loss: 0.013334354086054696\n",
      "Average test loss: 0.0025185429871910146\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013299960946043332\n",
      "Average test loss: 0.0025414548025776943\n",
      "Epoch 42/300\n",
      "Average training loss: 0.013274889508883158\n",
      "Average test loss: 0.0025450081643131044\n",
      "Epoch 43/300\n",
      "Average training loss: 0.013230926550924779\n",
      "Average test loss: 0.0025162935016883746\n",
      "Epoch 44/300\n",
      "Average training loss: 0.013187068783574634\n",
      "Average test loss: 0.002520299334803389\n",
      "Epoch 45/300\n",
      "Average training loss: 0.013138763121432728\n",
      "Average test loss: 0.0025818998279670876\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013102844738297993\n",
      "Average test loss: 0.002555334892641339\n",
      "Epoch 47/300\n",
      "Average training loss: 0.013080430780020025\n",
      "Average test loss: 0.0025454702437337903\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01303945078369644\n",
      "Average test loss: 0.002518501960361997\n",
      "Epoch 49/300\n",
      "Average training loss: 0.013014621732963456\n",
      "Average test loss: 0.002538797625237041\n",
      "Epoch 50/300\n",
      "Average training loss: 0.012987866695970298\n",
      "Average test loss: 0.0025569986659619544\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012942831017076968\n",
      "Average test loss: 0.002533973505306575\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012900408862365617\n",
      "Average test loss: 0.0025044114529672594\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012878664593315787\n",
      "Average test loss: 0.002549419505107734\n",
      "Epoch 54/300\n",
      "Average training loss: 0.012864559147920873\n",
      "Average test loss: 0.0024961758972042135\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012833459773825275\n",
      "Average test loss: 0.002505323684463898\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012790393102500174\n",
      "Average test loss: 0.0025138419705132645\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01277273655600018\n",
      "Average test loss: 0.0024989309697929355\n",
      "Epoch 58/300\n",
      "Average training loss: 0.012743904067410362\n",
      "Average test loss: 0.0025578345424599118\n",
      "Epoch 59/300\n",
      "Average training loss: 0.012734324692024125\n",
      "Average test loss: 0.0025311246348751915\n",
      "Epoch 60/300\n",
      "Average training loss: 0.012690274845394823\n",
      "Average test loss: 0.0025205254356066385\n",
      "Epoch 61/300\n",
      "Average training loss: 0.012680588877035512\n",
      "Average test loss: 0.002504447399535113\n",
      "Epoch 62/300\n",
      "Average training loss: 0.012632313800354799\n",
      "Average test loss: 0.00252606134261522\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012612385524643792\n",
      "Average test loss: 0.0025366049758676025\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012587160136964586\n",
      "Average test loss: 0.0025117282970911928\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01255505590637525\n",
      "Average test loss: 0.0025084302682015632\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012540931781133016\n",
      "Average test loss: 0.002519595239518417\n",
      "Epoch 67/300\n",
      "Average training loss: 0.012503057834174897\n",
      "Average test loss: 0.0025595404809961716\n",
      "Epoch 68/300\n",
      "Average training loss: 0.012501533675524923\n",
      "Average test loss: 0.002549944010666675\n",
      "Epoch 69/300\n",
      "Average training loss: 0.012467097871005535\n",
      "Average test loss: 0.0025164689785904353\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012424804147746828\n",
      "Average test loss: 0.0025253771210296287\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01242066105372376\n",
      "Average test loss: 0.002527931470423937\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01240348932147026\n",
      "Average test loss: 0.002567166403763824\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012387636778255304\n",
      "Average test loss: 0.0026227876941363015\n",
      "Epoch 74/300\n",
      "Average training loss: 0.012358555900553863\n",
      "Average test loss: 0.002513843543206652\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012359396465122699\n",
      "Average test loss: 0.002538057260629204\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012324405109716786\n",
      "Average test loss: 0.002569365259259939\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012294361278414725\n",
      "Average test loss: 0.0025831857507841454\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01227982381068998\n",
      "Average test loss: 0.00250874700749086\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012256298816038503\n",
      "Average test loss: 0.002561773891871174\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012238850175506539\n",
      "Average test loss: 0.0025655225351866747\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01222041655662987\n",
      "Average test loss: 0.002552431755595737\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01219434661914905\n",
      "Average test loss: 0.0025663011798428164\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01217598966591888\n",
      "Average test loss: 0.0025295500771866904\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012158491365197631\n",
      "Average test loss: 0.002590789843143688\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012154934596270322\n",
      "Average test loss: 0.002614013974658317\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012143040954238838\n",
      "Average test loss: 0.0025355183862977556\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012146136373281479\n",
      "Average test loss: 0.002608678526348538\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01209705116185877\n",
      "Average test loss: 0.0025294563006609677\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012086330324411393\n",
      "Average test loss: 0.002602855915410651\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012066654191248947\n",
      "Average test loss: 0.0026571484944886634\n",
      "Epoch 91/300\n",
      "Average training loss: 0.012043872141175799\n",
      "Average test loss: 0.0025425030605660546\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012047055194775263\n",
      "Average test loss: 0.002603725341459115\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012016200933191511\n",
      "Average test loss: 0.002551945776989063\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01202577888344725\n",
      "Average test loss: 0.0026506507691616814\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011982663730780283\n",
      "Average test loss: 0.0025650261375639175\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011971378576424387\n",
      "Average test loss: 0.0025332822223297423\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011965072443087896\n",
      "Average test loss: 0.0025932222424695887\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011947500573264228\n",
      "Average test loss: 0.002556040592491627\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011940641613884105\n",
      "Average test loss: 0.002589935975563195\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0119106044727895\n",
      "Average test loss: 0.0025608528585483632\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011930601869192389\n",
      "Average test loss: 0.002617305932773484\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011883923841847313\n",
      "Average test loss: 0.002577448764195045\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011889302209847503\n",
      "Average test loss: 0.002589589067010416\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011879844299621052\n",
      "Average test loss: 0.002808765964789523\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011877863362431526\n",
      "Average test loss: 0.0026665988324417007\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01184048977908161\n",
      "Average test loss: 0.0025706618735566737\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011850112613704469\n",
      "Average test loss: 0.0026061489776604703\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011823435612022877\n",
      "Average test loss: 0.0025463246777653694\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01181120438542631\n",
      "Average test loss: 0.0026122061290467778\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011795208897855547\n",
      "Average test loss: 0.002625606626065241\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011791875677804152\n",
      "Average test loss: 0.002603600282635954\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011767756856564018\n",
      "Average test loss: 0.002651111295032832\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011781595667203267\n",
      "Average test loss: 0.002572855067749818\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011777421718670262\n",
      "Average test loss: 0.0026293006355149876\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011749319774823056\n",
      "Average test loss: 0.0026643734272155497\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01173886400875118\n",
      "Average test loss: 0.0025877666895588237\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011709960204859574\n",
      "Average test loss: 0.0026530967415827845\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011737350461383661\n",
      "Average test loss: 0.002675860815164116\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011759297541446155\n",
      "Average test loss: 0.0025898887794464824\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011685075175431039\n",
      "Average test loss: 0.002634462866932154\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011677396709720294\n",
      "Average test loss: 0.002621791544680794\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011675038803782728\n",
      "Average test loss: 0.0026452506840642957\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011667731999523109\n",
      "Average test loss: 0.002651921461025874\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011678392984800869\n",
      "Average test loss: 0.0026011077283571164\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011632051677339607\n",
      "Average test loss: 0.002645201489329338\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01164425421340598\n",
      "Average test loss: 0.0025751908386333123\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011638239921794998\n",
      "Average test loss: 0.0026182098856402766\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01161198686228858\n",
      "Average test loss: 0.0025652078228692215\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011607079343663321\n",
      "Average test loss: 0.0025802600702477824\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01160455998364422\n",
      "Average test loss: 0.0026311387113398975\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011608337732652824\n",
      "Average test loss: 0.002653065158882075\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011601218226883146\n",
      "Average test loss: 0.002649604749141468\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011579784119294749\n",
      "Average test loss: 0.002624731985748642\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011565881537066565\n",
      "Average test loss: 0.002652527255627016\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011564179173774189\n",
      "Average test loss: 0.002617608843371272\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011575752013259465\n",
      "Average test loss: 0.0026069584012859397\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011534837596946292\n",
      "Average test loss: 0.002600196317666107\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011542059966259533\n",
      "Average test loss: 0.002638407584486736\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011546569202509191\n",
      "Average test loss: 0.002644025486583511\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011530203920271662\n",
      "Average test loss: 0.002686470573561059\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011514472478793726\n",
      "Average test loss: 0.002617120457606183\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011526599251561695\n",
      "Average test loss: 0.0026467525454031094\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011505091749959522\n",
      "Average test loss: 0.0025699053561935824\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011492710984415478\n",
      "Average test loss: 0.002643938951401247\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011488875322043896\n",
      "Average test loss: 0.0026988764790197213\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011482823799053828\n",
      "Average test loss: 0.0026770420669474534\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011482115668555101\n",
      "Average test loss: 0.0026234697020716138\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011476563438773155\n",
      "Average test loss: 0.0026576219478415116\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011465670197374291\n",
      "Average test loss: 0.002592662487592962\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011447183502217134\n",
      "Average test loss: 0.0025630167248762315\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011464970415665044\n",
      "Average test loss: 0.0027143994917472204\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011468656567235789\n",
      "Average test loss: 0.0026299791445748674\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011424230214622286\n",
      "Average test loss: 0.002698408819010688\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0114346695434716\n",
      "Average test loss: 0.002719964179727766\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011418804701831606\n",
      "Average test loss: 0.0026034668816460505\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011416829783883359\n",
      "Average test loss: 0.0026350611024018793\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011412158496677876\n",
      "Average test loss: 0.002750502026018997\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011430514112114907\n",
      "Average test loss: 0.0026972157437768246\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011407581923736466\n",
      "Average test loss: 0.002627079799460868\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011398254739741485\n",
      "Average test loss: 0.0026940556493484312\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011388240854773257\n",
      "Average test loss: 0.002666602737373776\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011380662283963627\n",
      "Average test loss: 0.0026354634960492453\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011373903719915285\n",
      "Average test loss: 0.002679068523976538\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011366669868429501\n",
      "Average test loss: 0.002604885654523969\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0113605625099606\n",
      "Average test loss: 0.002610012238431308\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011347706937127644\n",
      "Average test loss: 0.0026762312919729287\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011357911875678433\n",
      "Average test loss: 0.002696730125385026\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011343974398242102\n",
      "Average test loss: 0.0027526728419793975\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011348551174004873\n",
      "Average test loss: 0.0026422242296652663\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011359094848235449\n",
      "Average test loss: 0.0026586772758099768\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011328367962605423\n",
      "Average test loss: 0.0026912538210550944\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0112935948661632\n",
      "Average test loss: 0.0027067798181540435\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011341971332000362\n",
      "Average test loss: 0.002733102836542659\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01132429433779584\n",
      "Average test loss: 0.0026715592371506823\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0113000178999371\n",
      "Average test loss: 0.0026412740270089774\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011304977941016356\n",
      "Average test loss: 0.0026202275738534\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011301012983752622\n",
      "Average test loss: 0.0027118930775258277\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011311760910683208\n",
      "Average test loss: 0.00265561021429797\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011297415424138308\n",
      "Average test loss: 0.0026123547181487084\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011285769820213318\n",
      "Average test loss: 0.002619926170135538\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011289719895356232\n",
      "Average test loss: 0.0026822592930661306\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011282591174046199\n",
      "Average test loss: 0.0027394990099387038\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011273377685911126\n",
      "Average test loss: 0.0026696975539541906\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011252545502450731\n",
      "Average test loss: 0.0027770379460934137\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011270692395667236\n",
      "Average test loss: 0.00267044148242308\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011251797741485967\n",
      "Average test loss: 0.002657241969472832\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011260268594655726\n",
      "Average test loss: 0.0026168641178972193\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011248877798517545\n",
      "Average test loss: 0.002803704032260511\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011238709833886888\n",
      "Average test loss: 0.002670247142099672\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011235352330737644\n",
      "Average test loss: 0.0027755108532599277\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01124132363167074\n",
      "Average test loss: 0.0026565630396621093\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011233086907201343\n",
      "Average test loss: 0.002728346407620443\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01122621206773652\n",
      "Average test loss: 0.002725953199383285\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011227145137886207\n",
      "Average test loss: 0.0026863548801177076\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011216974606944456\n",
      "Average test loss: 0.002715228572798272\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011203045326802465\n",
      "Average test loss: 0.0027510341997775767\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011206014646424187\n",
      "Average test loss: 0.0026732359073228305\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011202368777659205\n",
      "Average test loss: 0.0027499721329659224\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01120814108186298\n",
      "Average test loss: 0.0026472242248968946\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011193312178055446\n",
      "Average test loss: 0.0026987826352318127\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011198564818335904\n",
      "Average test loss: 0.0026930274226599268\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011183581079045931\n",
      "Average test loss: 0.0027960488475445243\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011178699847724703\n",
      "Average test loss: 0.0026738394202871457\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01118669675042232\n",
      "Average test loss: 0.002746855346330752\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011196641305254565\n",
      "Average test loss: 0.0026367843250433604\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011183499031596714\n",
      "Average test loss: 0.002708849335089326\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011171404062873787\n",
      "Average test loss: 0.0027036426038377815\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011158796791401174\n",
      "Average test loss: 0.0027224002583987185\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011162723591758145\n",
      "Average test loss: 0.002803772141949998\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01114607898477051\n",
      "Average test loss: 0.002615206349641085\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011144778903987672\n",
      "Average test loss: 0.0026894437012573084\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011173160131606791\n",
      "Average test loss: 0.0027518004704680707\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01113436993666821\n",
      "Average test loss: 0.00272393418641554\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011132294219401147\n",
      "Average test loss: 0.0027454980618009965\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011142291304965813\n",
      "Average test loss: 0.002711583639598555\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011131182039777438\n",
      "Average test loss: 0.0027682022319899663\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011136197748283545\n",
      "Average test loss: 0.002766933078153266\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011122698252399763\n",
      "Average test loss: 0.0028474551467224957\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01112113611234559\n",
      "Average test loss: 0.002625226435251534\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011128157531221707\n",
      "Average test loss: 0.0026714462108082242\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011104615022738774\n",
      "Average test loss: 0.002718411675757832\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011118220892631345\n",
      "Average test loss: 0.00263671300932765\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011108895257115364\n",
      "Average test loss: 0.002724543408387237\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01110596988019016\n",
      "Average test loss: 0.0027342404491371578\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011114666922224892\n",
      "Average test loss: 0.002702080317100303\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011120791073060698\n",
      "Average test loss: 0.0029100508828543955\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011090307877295548\n",
      "Average test loss: 0.0026690309117030765\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011083093454854356\n",
      "Average test loss: 0.0027040015279004973\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011100140963991484\n",
      "Average test loss: 0.0027305924403998585\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011077352608243624\n",
      "Average test loss: 0.0026868187366053463\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011087071385648515\n",
      "Average test loss: 0.0028151920733766422\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0110874158003264\n",
      "Average test loss: 0.0027208009554694095\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01107797823432419\n",
      "Average test loss: 0.0026998494702080884\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011070134440229999\n",
      "Average test loss: 0.002761811094255083\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011069144239028295\n",
      "Average test loss: 0.002660930022597313\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011070829304556051\n",
      "Average test loss: 0.002720997495576739\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011065855894651679\n",
      "Average test loss: 0.0026974374444948303\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0110540982807676\n",
      "Average test loss: 0.0027482447874628835\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011071492210858397\n",
      "Average test loss: 0.0027680644298800163\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011061222203903728\n",
      "Average test loss: 0.002826822522510257\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011064965683552953\n",
      "Average test loss: 0.0027773628381805287\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011051333311531278\n",
      "Average test loss: 0.002722984217728178\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01105146776056952\n",
      "Average test loss: 0.0027262905177970727\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01104041827428672\n",
      "Average test loss: 0.0027830679786081114\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011050267465412617\n",
      "Average test loss: 0.0027063643189354073\n",
      "Epoch 246/300\n",
      "Average training loss: 0.011028989615539709\n",
      "Average test loss: 0.0026819860097020863\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011033840061889755\n",
      "Average test loss: 0.0027008792402015793\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011022924066417746\n",
      "Average test loss: 0.0026945960787642335\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011031338132090038\n",
      "Average test loss: 0.002713189585548308\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01101892236702972\n",
      "Average test loss: 0.002735400506812665\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01101945012062788\n",
      "Average test loss: 0.0027856645892477697\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011010048982997735\n",
      "Average test loss: 0.002682762621384528\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011012787495222356\n",
      "Average test loss: 0.002847608427206675\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011018356658518315\n",
      "Average test loss: 0.002723104684303204\n",
      "Epoch 255/300\n",
      "Average training loss: 0.011042551637523703\n",
      "Average test loss: 0.0026807997781369422\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011010532480974992\n",
      "Average test loss: 0.002736496119863457\n",
      "Epoch 257/300\n",
      "Average training loss: 0.011009406762818495\n",
      "Average test loss: 0.002718077941280272\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01100549584047662\n",
      "Average test loss: 0.0027195387014912233\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011010574786199464\n",
      "Average test loss: 0.0028367700320151117\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01099669299978349\n",
      "Average test loss: 0.0027248298389216264\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01097818543513616\n",
      "Average test loss: 0.0026808445586098567\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01100346717155642\n",
      "Average test loss: 0.002740036367956135\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010982961847550338\n",
      "Average test loss: 0.0027654067306882805\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010987415531443225\n",
      "Average test loss: 0.002809524834362997\n",
      "Epoch 265/300\n",
      "Average training loss: 0.011001123123698765\n",
      "Average test loss: 0.002742422382036845\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010976434512270821\n",
      "Average test loss: 0.0026678565309072536\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010962728271053898\n",
      "Average test loss: 0.0027697812871386607\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010977811620467239\n",
      "Average test loss: 0.002705318991301788\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010965650973220666\n",
      "Average test loss: 0.0027791261178337864\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010982999776800473\n",
      "Average test loss: 0.0027712927711092765\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01097027200087905\n",
      "Average test loss: 0.0026663970711330574\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010959861165119543\n",
      "Average test loss: 0.0027366904717766576\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010968386690649722\n",
      "Average test loss: 0.0026856461334973576\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010955846345259083\n",
      "Average test loss: 0.002760079896905356\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010954965699878003\n",
      "Average test loss: 0.0027849087439891364\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01095181357115507\n",
      "Average test loss: 0.0027596152954631383\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010958492894967397\n",
      "Average test loss: 0.002734353420221143\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010942149727708763\n",
      "Average test loss: 0.0026597377150836916\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01095390539119641\n",
      "Average test loss: 0.002723875903420978\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010943327209187878\n",
      "Average test loss: 0.002840699060095681\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010937866370711062\n",
      "Average test loss: 0.0027160419745163784\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010953618608829048\n",
      "Average training loss: 0.01094307630509138\n",
      "Average test loss: 0.0027658491434736383\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010931332646972602\n",
      "Average test loss: 0.0026931797905514637\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010928387499517864\n",
      "Average test loss: 0.0027059184008588395\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01093593841046095\n",
      "Average test loss: 0.0027497207265761163\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01091063414596849\n",
      "Average test loss: 0.0028675324939605265\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010919127739965916\n",
      "Average test loss: 0.002741160224709246\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01091643951750464\n",
      "Average test loss: 0.0027244683125366765\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01091469839711984\n",
      "Average test loss: 0.002712977762437529\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010920344953735669\n",
      "Average test loss: 0.0026917155008349155\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0109246932880746\n",
      "Average test loss: 0.002751805391162634\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010912880283262994\n",
      "Average test loss: 0.002760061989641852\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010905267969601683\n",
      "Average test loss: 0.0027480460581266217\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010903480300472843\n",
      "Average test loss: 0.0026868850524640747\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01091103339774741\n",
      "Average test loss: 0.002763607267083393\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010907236503230201\n",
      "Average test loss: 0.002714995682860414\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010907516323029994\n",
      "Average test loss: 0.0026879293897913562\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.08294283957779408\n",
      "Average test loss: 0.00405830461324917\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01968483776681953\n",
      "Average test loss: 0.002995756378604306\n",
      "Epoch 3/300\n",
      "Average training loss: 0.016942863379915556\n",
      "Average test loss: 0.002816389783182078\n",
      "Epoch 4/300\n",
      "Average training loss: 0.015841306943032478\n",
      "Average test loss: 0.002695496347422401\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0151903689743744\n",
      "Average test loss: 0.002623833012767136\n",
      "Epoch 6/300\n",
      "Average training loss: 0.014718380722734663\n",
      "Average test loss: 0.0024982228436403802\n",
      "Epoch 7/300\n",
      "Average training loss: 0.014353202273862229\n",
      "Average test loss: 0.002478468940593302\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014036197540660699\n",
      "Average test loss: 0.002356075567917691\n",
      "Epoch 9/300\n",
      "Average training loss: 0.013738093160920672\n",
      "Average test loss: 0.002293326425883505\n",
      "Epoch 10/300\n",
      "Average training loss: 0.013479050905340248\n",
      "Average test loss: 0.0022930088732391594\n",
      "Epoch 11/300\n",
      "Average training loss: 0.013213603161275386\n",
      "Average test loss: 0.0022352650560852555\n",
      "Epoch 12/300\n",
      "Average training loss: 0.012971747086279923\n",
      "Average test loss: 0.0021659218680320515\n",
      "Epoch 13/300\n",
      "Average training loss: 0.012737581027878656\n",
      "Average test loss: 0.0021565797117849192\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012510093912482262\n",
      "Average test loss: 0.0021104649891042046\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01228014935966995\n",
      "Average test loss: 0.0020878213875823552\n",
      "Epoch 16/300\n",
      "Average training loss: 0.012076401164134344\n",
      "Average test loss: 0.0020364983717186583\n",
      "Epoch 17/300\n",
      "Average training loss: 0.011885749032100042\n",
      "Average test loss: 0.0020187037323291103\n",
      "Epoch 18/300\n",
      "Average training loss: 0.011174480080604553\n",
      "Average test loss: 0.001977473419987493\n",
      "Epoch 22/300\n",
      "Average training loss: 0.011031608930064572\n",
      "Average test loss: 0.0019454780236507455\n",
      "Epoch 23/300\n",
      "Average training loss: 0.010893501470072403\n",
      "Average test loss: 0.0020075253943602246\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01077287923792998\n",
      "Average test loss: 0.0019220357849780057\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010662177208397124\n",
      "Average test loss: 0.0018516554962843656\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010545862513283889\n",
      "Average test loss: 0.0018469339811967478\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010464263695809576\n",
      "Average test loss: 0.0018364127654996183\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01035957087079684\n",
      "Average test loss: 0.0018121252635286914\n",
      "Epoch 29/300\n",
      "Average training loss: 0.010290238116350438\n",
      "Average test loss: 0.0018288220391712254\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010227029770612716\n",
      "Average test loss: 0.0018056596053971185\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01015273033413622\n",
      "Average test loss: 0.001781837062082357\n",
      "Epoch 32/300\n",
      "Average training loss: 0.010080405906670623\n",
      "Average test loss: 0.0018787246386831006\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010034644149243831\n",
      "Average test loss: 0.0017842077685313092\n",
      "Epoch 34/300\n",
      "Average training loss: 0.009976021837029192\n",
      "Average test loss: 0.0017711778059601785\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009940721097091834\n",
      "Average test loss: 0.0017577156879318256\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009885427937739425\n",
      "Average test loss: 0.0017696069698366853\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009843848063300053\n",
      "Average test loss: 0.001755061646302541\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009810403393374549\n",
      "Average test loss: 0.0017705080546438694\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00976788145220942\n",
      "Average test loss: 0.001766796478173799\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009737562283873558\n",
      "Average test loss: 0.0017640963376810152\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009686226184997294\n",
      "Average test loss: 0.0017541395012910168\n",
      "Epoch 42/300\n",
      "Average training loss: 0.00966084655581249\n",
      "Average test loss: 0.0017505900839136707\n",
      "Epoch 43/300\n",
      "Average training loss: 0.00964413243614965\n",
      "Average test loss: 0.0017446733352003826\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009608376188410653\n",
      "Average test loss: 0.0017434197311393088\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009582475137379434\n",
      "Average test loss: 0.0018045001211058763\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009545801771183809\n",
      "Average test loss: 0.001719928675900317\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00951219850861364\n",
      "Average test loss: 0.0017276103451020189\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009491601719624466\n",
      "Average test loss: 0.0017355640702363517\n",
      "Epoch 49/300\n",
      "Average training loss: 0.00938678090646863\n",
      "Average test loss: 0.0017337042211244503\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009374852989696794\n",
      "Average test loss: 0.0017144678725550572\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009347151358094481\n",
      "Average test loss: 0.0017212254826186432\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00932628544420004\n",
      "Average test loss: 0.0017414248288195166\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009354430694133043\n",
      "Average test loss: 0.0017205709823303753\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009287435269190205\n",
      "Average test loss: 0.0017377645176731879\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009270482772754298\n",
      "Average test loss: 0.0017337751950447758\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009252640842563576\n",
      "Average test loss: 0.0017055495651438833\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009225151430815459\n",
      "Average test loss: 0.0017528701881981557\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00921690916766723\n",
      "Average test loss: 0.0017095353148049777\n",
      "Epoch 62/300\n",
      "Average training loss: 0.00918995473285516\n",
      "Average test loss: 0.0017078312952071427\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009167630950609842\n",
      "Average test loss: 0.0017174210564957725\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009173254788749748\n",
      "Average test loss: 0.0017108679417934683\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009136217021279865\n",
      "Average test loss: 0.001701733418636852\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009119267523288727\n",
      "Average test loss: 0.001773467350854642\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009109008484416538\n",
      "Average test loss: 0.0016977005269792345\n",
      "Epoch 68/300\n",
      "Average training loss: 0.00908356936275959\n",
      "Average test loss: 0.0018907580302200383\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009058085809151331\n",
      "Average test loss: 0.0017242647582251165\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009058581677575906\n",
      "Average test loss: 0.0017119241415833434\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009041645026869244\n",
      "Average test loss: 0.0017158648597283496\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00900683772067229\n",
      "Average test loss: 0.0017294166894215677\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008990687168720696\n",
      "Average test loss: 0.0017221411135461596\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008983101906047926\n",
      "Average test loss: 0.001725815007980499\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008976890126864116\n",
      "Average test loss: 0.0017355480330685773\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008960275867333015\n",
      "Average test loss: 0.001983790086685783\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008959578280647596\n",
      "Average test loss: 0.0016975701309533582\n",
      "Epoch 78/300\n",
      "Average training loss: 0.00892574332281947\n",
      "Average test loss: 0.0017236010877208577\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008919461847179466\n",
      "Average test loss: 0.0017035228236474925\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008906577565189864\n",
      "Average test loss: 0.001765800766232941\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00892200538267692\n",
      "Average test loss: 0.0017113974493824773\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008888654514319368\n",
      "Average test loss: 0.0019271637606951926\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008862052479551899\n",
      "Average test loss: 0.0017099563071711196\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008837560751371914\n",
      "Average test loss: 0.001735163140628073\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008829025834384892\n",
      "Average test loss: 0.001707424295652244\n",
      "Epoch 86/300\n",
      "Average training loss: 0.008809889727168613\n",
      "Average test loss: 0.001718709142671691\n",
      "Epoch 87/300\n",
      "Average training loss: 0.008803392965346575\n",
      "Average test loss: 0.0017270246694485345\n",
      "Epoch 88/300\n",
      "Average training loss: 0.008793070207039516\n",
      "Average test loss: 0.0017254067769067155\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008786638990044594\n",
      "Average test loss: 0.0017254429755525456\n",
      "Epoch 90/300\n",
      "Average training loss: 0.008766194391167825\n",
      "Average test loss: 0.001748915589104096\n",
      "Epoch 91/300\n",
      "Average training loss: 0.008742555109990967\n",
      "Average test loss: 0.0017157083032652736\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008755779903795984\n",
      "Average test loss: 0.0017413900680840015\n",
      "Epoch 93/300\n",
      "Average training loss: 0.008766285658296612\n",
      "Average test loss: 0.0017130499991277854\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008724198442366387\n",
      "Average test loss: 0.001770985196241074\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008708594270050525\n",
      "Average test loss: 0.0017763390626965297\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008693111540542708\n",
      "Average test loss: 0.0017248141712819538\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008685315924386183\n",
      "Average test loss: 0.0017407824078367817\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008714637798981534\n",
      "Average test loss: 0.0017986001577228307\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008666803197728263\n",
      "Average test loss: 0.0017391490458200376\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008654470442897744\n",
      "Average test loss: 0.0017475049108680752\n",
      "Epoch 101/300\n",
      "Average training loss: 0.008653684039082793\n",
      "Average test loss: 0.0017284891463609205\n",
      "Epoch 102/300\n",
      "Average training loss: 0.008653795552336507\n",
      "Average test loss: 0.0017373317408685882\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008628856115871006\n",
      "Average test loss: 0.0017332699013770454\n",
      "Epoch 104/300\n",
      "Average training loss: 0.008642078780465657\n",
      "Average test loss: 0.0017395468444253008\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008613912661042479\n",
      "Average test loss: 0.001729522217479017\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008603725078619189\n",
      "Average test loss: 0.0017201609052717687\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008588296535528369\n",
      "Average test loss: 0.0017351267918840878\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008573790887163745\n",
      "Average test loss: 0.0017571114525198937\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008577929495109451\n",
      "Average test loss: 0.0017286856698079242\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008562924205842945\n",
      "Average test loss: 0.001739360151398513\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008562593220008744\n",
      "Average test loss: 0.0017636531852185727\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008554771042532392\n",
      "Average test loss: 0.0017352374733115236\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008546069365822606\n",
      "Average test loss: 0.0017974072807571955\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008537431298858589\n",
      "Average test loss: 0.001792575681168172\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008518761434488827\n",
      "Average test loss: 0.0017495901451135675\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008513445584310426\n",
      "Average test loss: 0.0017226191189967923\n",
      "Epoch 117/300\n",
      "Average training loss: 0.008517396020806497\n",
      "Average test loss: 0.0017891628365549776\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008501349257926146\n",
      "Average test loss: 0.0017729445790044135\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008505195233556959\n",
      "Average test loss: 0.001750035408987767\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008478450478365023\n",
      "Average test loss: 0.0017500327989045117\n",
      "Epoch 121/300\n",
      "Average training loss: 0.008474427772892847\n",
      "Average test loss: 0.0017632851141194503\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008465583660536341\n",
      "Average test loss: 0.0017513070723248854\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008463449214895566\n",
      "Average test loss: 0.0017383145893820457\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008454421153499021\n",
      "Average test loss: 0.0019252223604255252\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008443926877859565\n",
      "Average test loss: 0.0017554054385465053\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00843322962688075\n",
      "Average test loss: 0.001773778788331482\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008433176186763577\n",
      "Average test loss: 0.0017766955390365587\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008457313714341984\n",
      "Average test loss: 0.0017435720426340897\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008414272715648015\n",
      "Average test loss: 0.00174845367235442\n",
      "Epoch 130/300\n",
      "Average training loss: 0.008415080714970827\n",
      "Average test loss: 0.0017484860757572783\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008416915449003378\n",
      "Average test loss: 0.0018187351033298505\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008405285178787178\n",
      "Average test loss: 0.0017942424923595454\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008392838604334327\n",
      "Average test loss: 0.0017628237953823474\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008378205167336596\n",
      "Average test loss: 0.001781794166813294\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008377636628432406\n",
      "Average test loss: 0.0017920015700575377\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008366188348167472\n",
      "Average test loss: 0.00177119183819741\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008373395003792312\n",
      "Average test loss: 0.0018213871477378739\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008393254288989637\n",
      "Average test loss: 0.0017800439989401234\n",
      "Epoch 139/300\n",
      "Average training loss: 0.008377853751182556\n",
      "Average test loss: 0.001795484025341769\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008335592420564758\n",
      "Average test loss: 0.0017863940465160542\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00833715038994948\n",
      "Average test loss: 0.0018262858672274485\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008364073772811228\n",
      "Average test loss: 0.0017845262221785054\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008334197977350818\n",
      "Average test loss: 0.0017522098562783666\n",
      "Epoch 144/300\n",
      "Average training loss: 0.00832546176844173\n",
      "Average test loss: 0.0017671453062858846\n",
      "Epoch 145/300\n",
      "Average training loss: 0.008323705128497548\n",
      "Average test loss: 0.0018009151888804303\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008302156876358721\n",
      "Average test loss: 0.0017540373178198934\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008318436575432618\n",
      "Average test loss: 0.0019111239167137277\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00830636343235771\n",
      "Average test loss: 0.0018197497071491347\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008305825681736071\n",
      "Average test loss: 0.0017738817483186722\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008293520972132682\n",
      "Average test loss: 0.0017659994348262747\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008301771934661601\n",
      "Average test loss: 0.0017826345883723762\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00831251090599431\n",
      "Average test loss: 0.0017480355675021807\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008276651866734028\n",
      "Average test loss: 0.0017735407448684175\n",
      "Epoch 154/300\n",
      "Average training loss: 0.00827774714719918\n",
      "Average test loss: 0.0018400922474554843\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008285355294744173\n",
      "Average test loss: 0.0018622273285355832\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008269863739195797\n",
      "Average test loss: 0.00179328979137871\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00826208255150252\n",
      "Average test loss: 0.0017922010203409527\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00826987985521555\n",
      "Average test loss: 0.0018401873583594958\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008256139881494972\n",
      "Average test loss: 0.0017770084957074788\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00824175531292955\n",
      "Average test loss: 0.001828590186416275\n",
      "Epoch 161/300\n",
      "Average training loss: 0.00823833355307579\n",
      "Average test loss: 0.0018504316244895259\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008261979203257296\n",
      "Average test loss: 0.0017933519401897987\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008241532729317744\n",
      "Average test loss: 0.0018178985256494748\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008233305847479238\n",
      "Average test loss: 0.0018024179650884535\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008231939583188958\n",
      "Average test loss: 0.0017858594043387308\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008222736520071824\n",
      "Average test loss: 0.0017778232667284707\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008223305980778403\n",
      "Average test loss: 0.0018110714380939802\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008207531044466627\n",
      "Average test loss: 0.0018322665250549714\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008209217690759236\n",
      "Average test loss: 0.0017973358465565574\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00820923763513565\n",
      "Average test loss: 0.0017880381064282523\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008212387217001783\n",
      "Average test loss: 0.0017975371942544978\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0081967432031201\n",
      "Average test loss: 0.0017690948262396786\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00819789109668798\n",
      "Average test loss: 0.0017738009793683888\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008192326110270287\n",
      "Average test loss: 0.001780511665261454\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008189186235268911\n",
      "Average test loss: 0.0018777095075282785\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008191084624164634\n",
      "Average test loss: 0.0018295471931083335\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008168911312603288\n",
      "Average test loss: 0.0017866880028612085\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0081750121778912\n",
      "Average test loss: 0.0018675501473868887\n",
      "Epoch 179/300\n",
      "Average training loss: 0.008175919620527161\n",
      "Average test loss: 0.0018219721235541834\n",
      "Epoch 180/300\n",
      "Average training loss: 0.00815538447143303\n",
      "Average test loss: 0.0018329256500841842\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00816827453714278\n",
      "Average test loss: 0.0017827854661477936\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008169005604253874\n",
      "Average test loss: 0.0018671580391625563\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008162211127993133\n",
      "Average test loss: 0.0018234974732622504\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008142735296239455\n",
      "Average test loss: 0.0018169566579163074\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008153827459447914\n",
      "Average test loss: 0.001814164307848033\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008140982140683466\n",
      "Average test loss: 0.0017921466052325235\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008145288530530202\n",
      "Average test loss: 0.0018596770224264926\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008142113618552684\n",
      "Average test loss: 0.0018209921865620547\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00813074935393201\n",
      "Average test loss: 0.0018298407521926694\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008146125134908491\n",
      "Average test loss: 0.0018206138188640276\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008123611205981837\n",
      "Average test loss: 0.0017937509005682337\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008125785746508175\n",
      "Average test loss: 0.0018275349601689311\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008122041062348418\n",
      "Average test loss: 0.0018113472427551945\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008133008950700363\n",
      "Average test loss: 0.0018621114177836313\n",
      "Epoch 195/300\n",
      "Average training loss: 0.00811535615970691\n",
      "Average test loss: 0.0017899446485357153\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008135189373874002\n",
      "Average test loss: 0.0018366374804948766\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00809511418102516\n",
      "Average test loss: 0.0018227911012040244\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00809722286545568\n",
      "Average test loss: 0.0017899189492066702\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008111422858304448\n",
      "Average test loss: 0.0018491226291904848\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008112256395734018\n",
      "Average test loss: 0.0018186249556019901\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008091000675327248\n",
      "Average test loss: 0.0018435357266830073\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00809112543819679\n",
      "Average test loss: 0.001779648874575893\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008084017322295241\n",
      "Average test loss: 0.0018314224740283358\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008086322614302238\n",
      "Average test loss: 0.00181355766300112\n",
      "Epoch 205/300\n",
      "Average training loss: 0.00808218047560917\n",
      "Average test loss: 0.001836262108447651\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008083637671338188\n",
      "Average test loss: 0.001773611747349302\n",
      "Epoch 207/300\n",
      "Average training loss: 0.008074561892284287\n",
      "Average test loss: 0.0018331580426957873\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008085956296159161\n",
      "Average test loss: 0.0018557857897960478\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008069817800074815\n",
      "Average test loss: 0.0018213622538993757\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008065109024445216\n",
      "Average test loss: 0.0018104554382670256\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008052774038579728\n",
      "Average test loss: 0.0017886706108434334\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008077815274397533\n",
      "Average test loss: 0.0018204979640949104\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008064161556876368\n",
      "Average test loss: 0.0018326026235396663\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008065318892813391\n",
      "Average test loss: 0.0018969563758000731\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008053402590668864\n",
      "Average test loss: 0.0018740522491021289\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008043252160151799\n",
      "Average test loss: 0.0018372935335048371\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008038481016539865\n",
      "Average test loss: 0.0018545413787166279\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008039780149029362\n",
      "Average test loss: 0.0017957708310956757\n",
      "Epoch 222/300\n",
      "Average training loss: 0.008032775539904833\n",
      "Average test loss: 0.0018458424798316426\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008032286754084957\n",
      "Average test loss: 0.0018093064963403676\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008029105661229954\n",
      "Average test loss: 0.001832915267182721\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008036429182522826\n",
      "Average test loss: 0.0018605849074406755\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008037177167005008\n",
      "Average test loss: 0.001820511181321409\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008020590514358546\n",
      "Average test loss: 0.0019692694660690094\n",
      "Epoch 228/300\n",
      "Average training loss: 0.008026954826795392\n",
      "Average test loss: 0.0018465524404827091\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008023266788985994\n",
      "Average test loss: 0.0018012042695449459\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008034485238707728\n",
      "Average test loss: 0.0019102255354324976\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008006814308878448\n",
      "Average test loss: 0.001820470625327693\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008010414135952791\n",
      "Average test loss: 0.001803346307016909\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008012173619949155\n",
      "Average test loss: 0.0018556116610351535\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008002593913426002\n",
      "Average test loss: 0.0018456707044194143\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007996369677699275\n",
      "Average test loss: 0.001828021887379388\n",
      "Epoch 236/300\n",
      "Average training loss: 0.00801195316347811\n",
      "Average test loss: 0.0018121180089397562\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00799973893745078\n",
      "Average test loss: 0.001839301849508451\n",
      "Epoch 238/300\n",
      "Average training loss: 0.008001569754133622\n",
      "Average test loss: 0.0018367004520777198\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007995586656033993\n",
      "Average test loss: 0.0018718472050709856\n",
      "Epoch 240/300\n",
      "Average training loss: 0.007990196366690927\n",
      "Average test loss: 0.001813427630191048\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007993941913462347\n",
      "Average test loss: 0.0018645348695831166\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00797882125692235\n",
      "Average test loss: 0.0019468510614501105\n",
      "Epoch 243/300\n",
      "Average training loss: 0.007980907722479768\n",
      "Average test loss: 0.001820395842504998\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007975300759077071\n",
      "Average test loss: 0.0018996462271445326\n",
      "Epoch 245/300\n",
      "Average training loss: 0.007988279632396168\n",
      "Average test loss: 0.0018265437320288684\n",
      "Epoch 246/300\n",
      "Average training loss: 0.007992064781900909\n",
      "Average test loss: 0.001873591512441635\n",
      "Epoch 247/300\n",
      "Average training loss: 0.00797400171847807\n",
      "Average test loss: 0.0019937164400600726\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007985358748171065\n",
      "Average test loss: 0.0018445002094118132\n",
      "Epoch 252/300\n",
      "Average training loss: 0.007963076578246223\n",
      "Average test loss: 0.0018544787660034166\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007973566519717375\n",
      "Average test loss: 0.0018276879485282632\n",
      "Epoch 254/300\n",
      "Average training loss: 0.007967105552140209\n",
      "Average test loss: 0.0018416501095311509\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00796507674538427\n",
      "Average test loss: 0.001862573547495736\n",
      "Epoch 256/300\n",
      "Average training loss: 0.007957008662323156\n",
      "Average test loss: 0.001843016946895255\n",
      "Epoch 257/300\n",
      "Average training loss: 0.00796308800412549\n",
      "Average test loss: 0.0018181524001475837\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007956683048357565\n",
      "Average test loss: 0.0018732637809589505\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00796834889964925\n",
      "Average test loss: 0.0018788076244915526\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007955712305588855\n",
      "Average test loss: 0.0018492374336346983\n",
      "Epoch 261/300\n",
      "Average training loss: 0.007946141301343839\n",
      "Average test loss: 0.0018887607531829012\n",
      "Epoch 262/300\n",
      "Average training loss: 0.007943837468822797\n",
      "Average test loss: 0.0018407420240756538\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00794062109084593\n",
      "Average test loss: 0.0018591630632678667\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00793737179868751\n",
      "Average test loss: 0.001849961039506727\n",
      "Epoch 265/300\n",
      "Average training loss: 0.007950056369933817\n",
      "Average test loss: 0.001891821113311582\n",
      "Epoch 266/300\n",
      "Average training loss: 0.007932892262935639\n",
      "Average test loss: 0.0018562659161786238\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0079292148678667\n",
      "Average test loss: 0.0018888957726044787\n",
      "Epoch 268/300\n",
      "Average training loss: 0.007946404871841272\n",
      "Average test loss: 0.0018789372532111075\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007934010562797387\n",
      "Average test loss: 0.0018324695701400439\n",
      "Epoch 270/300\n",
      "Average training loss: 0.007936326822059022\n",
      "Average test loss: 0.0018125548019177385\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007930506515006224\n",
      "Average test loss: 0.0018299798609481917\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007934837646368476\n",
      "Average test loss: 0.0018537858844631249\n",
      "Epoch 273/300\n",
      "Average training loss: 0.007912435021665361\n",
      "Average test loss: 0.0018233077015934718\n",
      "Epoch 274/300\n",
      "Average training loss: 0.007927198753174809\n",
      "Average test loss: 0.0019044134341594245\n",
      "Epoch 275/300\n",
      "Average training loss: 0.007910322045286496\n",
      "Average test loss: 0.0018338039765755335\n",
      "Epoch 279/300\n",
      "Average training loss: 0.007904382510317696\n",
      "Average test loss: 0.0018731716929210557\n",
      "Epoch 280/300\n",
      "Average training loss: 0.007932089721577036\n",
      "Average test loss: 0.0018130561264438762\n",
      "Epoch 281/300\n",
      "Average training loss: 0.007924919495566023\n",
      "Average test loss: 0.001868096788931224\n",
      "Epoch 282/300\n",
      "Average training loss: 0.007907158291174306\n",
      "Average test loss: 0.001893233818312486\n",
      "Epoch 283/300\n",
      "Average training loss: 0.007896123143533865\n",
      "Average test loss: 0.0018148982340676918\n",
      "Epoch 284/300\n",
      "Average training loss: 0.007899059565116962\n",
      "Average test loss: 0.001844179904088378\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00790731430798769\n",
      "Average test loss: 0.0018613687686415182\n",
      "Epoch 286/300\n",
      "Average training loss: 0.007918630508499013\n",
      "Average test loss: 0.001858410006389022\n",
      "Epoch 287/300\n",
      "Average training loss: 0.00789644419029355\n",
      "Average test loss: 0.001873395868577063\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007901535228722625\n",
      "Average test loss: 0.0018487430036895805\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007897282492369413\n",
      "Average test loss: 0.001867196039400167\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007899593640118838\n",
      "Average test loss: 0.0018069006531602806\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007900370246834225\n",
      "Average test loss: 0.0018412104149659475\n",
      "Epoch 292/300\n",
      "Average training loss: 0.007888517660399278\n",
      "Average test loss: 0.001877025302240832\n",
      "Epoch 293/300\n",
      "Average training loss: 0.007892026849918896\n",
      "Average test loss: 0.001843658813378877\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007895906982322533\n",
      "Average test loss: 0.0018404074583926963\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007880051440662807\n",
      "Average test loss: 0.0018713128172482053\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007892798050410217\n",
      "Average test loss: 0.0020214179205811688\n",
      "Epoch 297/300\n",
      "Average training loss: 0.00788063287941946\n",
      "Average test loss: 0.001885735665137569\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0078862710479233\n",
      "Average test loss: 0.0018499754286474652\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00788578624650836\n",
      "Average test loss: 0.0018993793945345614\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007881111671941148\n",
      "Average test loss: 0.0020302489056355424\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.08381497253146436\n",
      "Average test loss: 0.002828070074112879\n",
      "Epoch 2/300\n",
      "Average training loss: 0.016854283317923546\n",
      "Average test loss: 0.00250833588010735\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014281068368090524\n",
      "Average test loss: 0.002256683150927226\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013277696990304523\n",
      "Average test loss: 0.0021121225996563833\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012678875016669432\n",
      "Average test loss: 0.0021216627003418074\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012236822187900544\n",
      "Average test loss: 0.001956712665553722\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01187152404917611\n",
      "Average test loss: 0.00188415823240454\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011558200838665168\n",
      "Average test loss: 0.0018481565084722308\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0112638756243719\n",
      "Average test loss: 0.0017482562552516658\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01100390307770835\n",
      "Average test loss: 0.0017211203320572773\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010759052019980219\n",
      "Average test loss: 0.0016628487080128658\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010526663820362754\n",
      "Average test loss: 0.0016706081542910802\n",
      "Epoch 13/300\n",
      "Average training loss: 0.010298405220939054\n",
      "Average test loss: 0.001634932903572917\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01008864628440804\n",
      "Average test loss: 0.0015727923010579413\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009878935610254606\n",
      "Average test loss: 0.001575471390866571\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00967204375151131\n",
      "Average test loss: 0.0015244343406003382\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009483215645783477\n",
      "Average test loss: 0.001617246709143122\n",
      "Epoch 18/300\n",
      "Average training loss: 0.009303894645637936\n",
      "Average test loss: 0.001481618013853828\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00912663745135069\n",
      "Average test loss: 0.0014482644688751962\n",
      "Epoch 20/300\n",
      "Average training loss: 0.008954389394985305\n",
      "Average test loss: 0.0014474886916577816\n",
      "Epoch 21/300\n",
      "Average training loss: 0.008790860408710108\n",
      "Average test loss: 0.001807327328560253\n",
      "Epoch 22/300\n",
      "Average training loss: 0.008652778329948584\n",
      "Average test loss: 0.001409038452224599\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008513919013655847\n",
      "Average test loss: 0.0015370511101144883\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008395885466287533\n",
      "Average test loss: 0.0013874723302821318\n",
      "Epoch 25/300\n",
      "Average training loss: 0.008278087502966325\n",
      "Average test loss: 0.0013541581249381933\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00818243868690398\n",
      "Average test loss: 0.0013422105587604972\n",
      "Epoch 27/300\n",
      "Average training loss: 0.008086428043329054\n",
      "Average test loss: 0.0013201179428853922\n",
      "Epoch 28/300\n",
      "Average training loss: 0.008007945836418204\n",
      "Average test loss: 0.001342699258070853\n",
      "Epoch 29/300\n",
      "Average training loss: 0.007938239149749279\n",
      "Average test loss: 0.0013490540576684806\n",
      "Epoch 30/300\n",
      "Average training loss: 0.007882424986196889\n",
      "Average test loss: 0.0013274655038904813\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007803066465589735\n",
      "Average test loss: 0.0012886250890377495\n",
      "Epoch 32/300\n",
      "Average training loss: 0.007616977179216014\n",
      "Average test loss: 0.001279824193049636\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007574470133003261\n",
      "Average test loss: 0.0012698854768855704\n",
      "Epoch 37/300\n",
      "Average training loss: 0.00753862902108166\n",
      "Average test loss: 0.001280436306260526\n",
      "Epoch 38/300\n",
      "Average training loss: 0.007522316661145952\n",
      "Average test loss: 0.0012525585163384676\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007458663293470939\n",
      "Average test loss: 0.001257320903862516\n",
      "Epoch 40/300\n",
      "Average training loss: 0.007453324760827753\n",
      "Average test loss: 0.0012589971521972782\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0074157904572784905\n",
      "Average test loss: 0.0012708143255052468\n",
      "Epoch 42/300\n",
      "Average training loss: 0.007372429604745574\n",
      "Average test loss: 0.0012378156022944799\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007355365308622519\n",
      "Average test loss: 0.0014302273416995175\n",
      "Epoch 44/300\n",
      "Average training loss: 0.007329834263771772\n",
      "Average test loss: 0.0012671597824535437\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007297204137676292\n",
      "Average test loss: 0.001254683847953048\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007277728575385279\n",
      "Average test loss: 0.001241473669703636\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007251600596225924\n",
      "Average test loss: 0.0012301898630749848\n",
      "Epoch 48/300\n",
      "Average training loss: 0.007232290722015831\n",
      "Average test loss: 0.0012310023962830503\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007207441276146306\n",
      "Average test loss: 0.001233562873572939\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007210554427156846\n",
      "Average test loss: 0.0012371230252707997\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007186955565793646\n",
      "Average test loss: 0.0012736879008718664\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007146102733082241\n",
      "Average test loss: 0.001218286433050202\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007135320521063275\n",
      "Average test loss: 0.0013362070181303554\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00710867903340194\n",
      "Average test loss: 0.0012242761880366338\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007096835180289215\n",
      "Average test loss: 0.001244906063708994\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007082911619709598\n",
      "Average test loss: 0.0012530793599370453\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007065242380317714\n",
      "Average test loss: 0.0012134315561399692\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007036788721879323\n",
      "Average test loss: 0.0012320553695576058\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007034439684202274\n",
      "Average test loss: 0.001256756597974648\n",
      "Epoch 60/300\n",
      "Average training loss: 0.007016810611718231\n",
      "Average test loss: 0.0012039761422978092\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006997203599661589\n",
      "Average test loss: 0.0012477818015031517\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006980794708761904\n",
      "Average test loss: 0.0012572635496035218\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0069743958132134545\n",
      "Average test loss: 0.0012125330760350658\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006963242570973105\n",
      "Average test loss: 0.001238917119645824\n",
      "Epoch 65/300\n",
      "Average training loss: 0.006899062258087926\n",
      "Average test loss: 0.0012438100075556173\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006889304885019858\n",
      "Average test loss: 0.0012101237248732812\n",
      "Epoch 70/300\n",
      "Average training loss: 0.006864170310811864\n",
      "Average test loss: 0.0012292588760869372\n",
      "Epoch 71/300\n",
      "Average training loss: 0.00685978735735019\n",
      "Average test loss: 0.0012051304957001574\n",
      "Epoch 72/300\n",
      "Average training loss: 0.006842206719434924\n",
      "Average test loss: 0.0012204114540169636\n",
      "Epoch 73/300\n",
      "Average training loss: 0.006841105087763733\n",
      "Average test loss: 0.001223839043173939\n",
      "Epoch 74/300\n",
      "Average training loss: 0.006826921675354243\n",
      "Average test loss: 0.001242383918362773\n",
      "Epoch 75/300\n",
      "Average training loss: 0.00681766649418407\n",
      "Average test loss: 0.001209521996167799\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00680081641425689\n",
      "Average test loss: 0.0012217590248522659\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0067929978614879975\n",
      "Average test loss: 0.0012252273781018124\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006780096764779753\n",
      "Average test loss: 0.0012879909207630488\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006764289710256789\n",
      "Average test loss: 0.001208267812203202\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0067547464213437505\n",
      "Average test loss: 0.0012366722118523385\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006744411385307709\n",
      "Average test loss: 0.0012404260123148562\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006737082211507692\n",
      "Average test loss: 0.0012335040325091944\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0067200901322066785\n",
      "Average test loss: 0.0012220755171858603\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006734434049990442\n",
      "Average test loss: 0.0013513425953893198\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006769484472771485\n",
      "Average test loss: 0.0012146002704070673\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006688649555461274\n",
      "Average test loss: 0.0012141164839267731\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006684137124568224\n",
      "Average test loss: 0.0012873158617359068\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006676674026168055\n",
      "Average test loss: 0.001224283506628126\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00666732084337208\n",
      "Average test loss: 0.001217286411051949\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006658923211197059\n",
      "Average test loss: 0.0012486857940028938\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006646138817899757\n",
      "Average test loss: 0.0012572561032138765\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006616353003722098\n",
      "Average test loss: 0.0012842189393316706\n",
      "Epoch 95/300\n",
      "Average training loss: 0.006617685570485062\n",
      "Average test loss: 0.0012307508672691053\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006604678679671553\n",
      "Average test loss: 0.0012319374738468064\n",
      "Epoch 97/300\n",
      "Average training loss: 0.006584275815221998\n",
      "Average test loss: 0.001353328766197794\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006585450383938021\n",
      "Average test loss: 0.0012210426789501475\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006573609614951743\n",
      "Average test loss: 0.0012439933406809965\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00656028820739852\n",
      "Average test loss: 0.0012275629262439908\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0065654313866462975\n",
      "Average test loss: 0.0012305869864196414\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006561704937368631\n",
      "Average test loss: 0.0012467125833241475\n",
      "Epoch 103/300\n",
      "Average training loss: 0.006544901909099685\n",
      "Average test loss: 0.0012213513197170363\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0065398011927803356\n",
      "Average test loss: 0.0012379223082421553\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006529280150102244\n",
      "Average test loss: 0.0012285365238785743\n",
      "Epoch 106/300\n",
      "Average training loss: 0.00652095190808177\n",
      "Average test loss: 0.0012458757681565152\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006539187800967031\n",
      "Average test loss: 0.0012333624773244892\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006512488488935762\n",
      "Average test loss: 0.001264409721415076\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00649087194353342\n",
      "Average test loss: 0.001262484568843825\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006488375311924352\n",
      "Average test loss: 0.0012421528080788751\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00648990926063723\n",
      "Average test loss: 0.0013022696501058008\n",
      "Epoch 112/300\n",
      "Average training loss: 0.006484551765024662\n",
      "Average test loss: 0.0012623118446725937\n",
      "Epoch 113/300\n",
      "Average training loss: 0.006481524256368478\n",
      "Average test loss: 0.0013148617359499137\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0064716524539722335\n",
      "Average test loss: 0.001242144801860882\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0064757571638458305\n",
      "Average test loss: 0.0012408789056870672\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006450617640796635\n",
      "Average test loss: 0.0012683315916607778\n",
      "Epoch 117/300\n",
      "Average training loss: 0.00644765109113521\n",
      "Average test loss: 0.0012470122548855014\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006417368137588104\n",
      "Average test loss: 0.0012706555268830724\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006414988533490234\n",
      "Average test loss: 0.0012704298025928438\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006405499610635969\n",
      "Average test loss: 0.0012616750699881879\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006411310883445872\n",
      "Average test loss: 0.0012492709497196807\n",
      "Epoch 125/300\n",
      "Average training loss: 0.006394727009038131\n",
      "Average test loss: 0.001262219462543726\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00642957944340176\n",
      "Average test loss: 0.0012662102043007812\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006374323356068796\n",
      "Average test loss: 0.001264891362335119\n",
      "Epoch 128/300\n",
      "Average training loss: 0.006377193585038185\n",
      "Average test loss: 0.0012685738525663813\n",
      "Epoch 129/300\n",
      "Average training loss: 0.006376485841969649\n",
      "Average test loss: 0.001272588490922418\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006366431959387329\n",
      "Average test loss: 0.0012775405214892493\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0063680748562845915\n",
      "Average test loss: 0.0012564373524445627\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006361765210827192\n",
      "Average test loss: 0.001278486657784217\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006352902344945404\n",
      "Average test loss: 0.0012491176983134614\n",
      "Epoch 134/300\n",
      "Average training loss: 0.006355080099155506\n",
      "Average test loss: 0.0012776743984367284\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006347367659211158\n",
      "Average test loss: 0.0012639715453195903\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0063363761090570025\n",
      "Average test loss: 0.0012825409916953907\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006343504218591584\n",
      "Average test loss: 0.0013515647298966845\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006331406489221586\n",
      "Average test loss: 0.0013266267369811734\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006318160643594133\n",
      "Average test loss: 0.0012634941142880254\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006318001788109541\n",
      "Average test loss: 0.0013140277120595176\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0063180094249546525\n",
      "Average test loss: 0.0012681231827785572\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006310295678675175\n",
      "Average test loss: 0.001281688649362574\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0063185866040488085\n",
      "Average test loss: 0.001308388949630575\n",
      "Epoch 144/300\n",
      "Average training loss: 0.006305834200647142\n",
      "Average test loss: 0.0012758207465004589\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006294823006623321\n",
      "Average test loss: 0.0012427317449409101\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0062967829807764955\n",
      "Average test loss: 0.001281895610710813\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0062958711783091224\n",
      "Average test loss: 0.001290606622584164\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0062832411320673095\n",
      "Average test loss: 0.0012656284799385403\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006257986036025815\n",
      "Average test loss: 0.0012748179291892383\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0062688913928965724\n",
      "Average test loss: 0.001303942129222883\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006273058131337166\n",
      "Average test loss: 0.0012884666830715206\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006266467205352253\n",
      "Average test loss: 0.001271292052956091\n",
      "Epoch 156/300\n",
      "Average training loss: 0.006247483289904065\n",
      "Average test loss: 0.001272009035055008\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0062604280677106645\n",
      "Average test loss: 0.001281972504220903\n",
      "Epoch 158/300\n",
      "Average training loss: 0.006306350313334002\n",
      "Average test loss: 0.0012833419009629222\n",
      "Epoch 159/300\n",
      "Average training loss: 0.006235532519924971\n",
      "Average test loss: 0.0013060031095519663\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0062251470941636295\n",
      "Average test loss: 0.0012872364103483657\n",
      "Epoch 161/300\n",
      "Average training loss: 0.006233622672657172\n",
      "Average test loss: 0.0013015521129386292\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006232210673391819\n",
      "Average test loss: 0.0012994840778410436\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006224634521951278\n",
      "Average test loss: 0.0012628188881919616\n",
      "Epoch 164/300\n",
      "Average training loss: 0.006215457757314046\n",
      "Average test loss: 0.0012659471911481686\n",
      "Epoch 165/300\n",
      "Average training loss: 0.006210954773757193\n",
      "Average test loss: 0.0013432801802539163\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0062173677053716445\n",
      "Average test loss: 0.0012862570932548906\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00620734981364674\n",
      "Average test loss: 0.0012854843293834063\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006208783403038979\n",
      "Average test loss: 0.001313063404419356\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0062043777393798035\n",
      "Average test loss: 0.0012853854260303909\n",
      "Epoch 170/300\n",
      "Average training loss: 0.006200093591792716\n",
      "Average test loss: 0.0012879939996120002\n",
      "Epoch 171/300\n",
      "Average training loss: 0.006192715554601616\n",
      "Average test loss: 0.00127628855764245\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006200132571160794\n",
      "Average test loss: 0.0013306539942406945\n",
      "Epoch 173/300\n",
      "Average training loss: 0.006186402830812666\n",
      "Average test loss: 0.0013280934994626377\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006178361938645442\n",
      "Average test loss: 0.0012824561435522304\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00618599556841784\n",
      "Average test loss: 0.0013257283280707069\n",
      "Epoch 176/300\n",
      "Average training loss: 0.006189005478802655\n",
      "Average test loss: 0.0012940560282311505\n",
      "Epoch 177/300\n",
      "Average training loss: 0.006177707955655125\n",
      "Average test loss: 0.0012844004016886984\n",
      "Epoch 178/300\n",
      "Average training loss: 0.006169530837072267\n",
      "Average test loss: 0.0012942046699010664\n",
      "Epoch 179/300\n",
      "Average training loss: 0.006178784559170405\n",
      "Average test loss: 0.0013004106995132235\n",
      "Epoch 180/300\n",
      "Average training loss: 0.00616847014054656\n",
      "Average test loss: 0.0012813875013444986\n",
      "Epoch 181/300\n",
      "Average training loss: 0.006162471862716807\n",
      "Average test loss: 0.0012993321529486113\n",
      "Epoch 182/300\n",
      "Average training loss: 0.006161608232392205\n",
      "Average test loss: 0.0012718383636739518\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006172000269509024\n",
      "Average test loss: 0.0013092065195863445\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00616054908807079\n",
      "Average test loss: 0.0013014835186509622\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006151216035087903\n",
      "Average test loss: 0.0013377879359241988\n",
      "Epoch 186/300\n",
      "Average training loss: 0.006163036598927445\n",
      "Average test loss: 0.0013141307994309398\n",
      "Epoch 187/300\n",
      "Average training loss: 0.006139998207075728\n",
      "Average test loss: 0.0013392774585841431\n",
      "Epoch 188/300\n",
      "Average training loss: 0.00614009990543127\n",
      "Average test loss: 0.0012942174473363492\n",
      "Epoch 189/300\n",
      "Average training loss: 0.006137113874364231\n",
      "Average test loss: 0.001305068626275493\n",
      "Epoch 190/300\n",
      "Average training loss: 0.006131168283522129\n",
      "Average test loss: 0.0013215706575041015\n",
      "Epoch 191/300\n",
      "Average training loss: 0.006138819683757093\n",
      "Average test loss: 0.0012873122763509552\n",
      "Epoch 192/300\n",
      "Average training loss: 0.006139028571132157\n",
      "Average test loss: 0.001302925642579794\n",
      "Epoch 193/300\n",
      "Average training loss: 0.006143073536455631\n",
      "Average test loss: 0.001318041564586262\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006130080335049166\n",
      "Average test loss: 0.0013673312107308044\n",
      "Epoch 195/300\n",
      "Average training loss: 0.006107886905057563\n",
      "Average test loss: 0.0012797167852727903\n",
      "Epoch 199/300\n",
      "Average training loss: 0.006116209571146303\n",
      "Average test loss: 0.0012997149198522998\n",
      "Epoch 200/300\n",
      "Average training loss: 0.006114351638903221\n",
      "Average test loss: 0.0012735263284088837\n",
      "Epoch 201/300\n",
      "Average training loss: 0.006109572714401616\n",
      "Average test loss: 0.0013031217363766497\n",
      "Epoch 202/300\n",
      "Average training loss: 0.006116786723335584\n",
      "Average test loss: 0.0013301266228987112\n",
      "Epoch 203/300\n",
      "Average training loss: 0.006105043390144904\n",
      "Average test loss: 0.0013118078782119685\n",
      "Epoch 204/300\n",
      "Average training loss: 0.006118150513205263\n",
      "Average test loss: 0.0012708128885262542\n",
      "Epoch 205/300\n",
      "Average training loss: 0.006099072064376539\n",
      "Average test loss: 0.0012942644798507293\n",
      "Epoch 206/300\n",
      "Average training loss: 0.006100020658224821\n",
      "Average test loss: 0.001335957549771087\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0060904643771549066\n",
      "Average test loss: 0.0013292898897909456\n",
      "Epoch 208/300\n",
      "Average training loss: 0.006092787510818905\n",
      "Average test loss: 0.0013497018828574155\n",
      "Epoch 209/300\n",
      "Average training loss: 0.006091761157744461\n",
      "Average test loss: 0.001286336196379529\n",
      "Epoch 210/300\n",
      "Average training loss: 0.006093386857252982\n",
      "Average test loss: 0.0013317611555361912\n",
      "Epoch 211/300\n",
      "Average training loss: 0.006086121359633075\n",
      "Average test loss: 0.0013046856821100745\n",
      "Epoch 212/300\n",
      "Average training loss: 0.006082443383832773\n",
      "Average test loss: 0.001300877208614515\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0060809935000207685\n",
      "Average test loss: 0.0013252772649543154\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006090375435228149\n",
      "Average test loss: 0.0013365074979762236\n",
      "Epoch 215/300\n",
      "Average training loss: 0.006085534914500183\n",
      "Average test loss: 0.0013707915067465769\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00606579034941064\n",
      "Average test loss: 0.0013127404193704327\n",
      "Epoch 217/300\n",
      "Average training loss: 0.006080133769247267\n",
      "Average test loss: 0.0013292554100561473\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006075943104922771\n",
      "Average test loss: 0.0013390162894502283\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00607287882723742\n",
      "Average test loss: 0.00131812182875971\n",
      "Epoch 220/300\n",
      "Average training loss: 0.006066140141752031\n",
      "Average test loss: 0.001291282899470793\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00605592444497678\n",
      "Average test loss: 0.0013386588729918003\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006058783221575949\n",
      "Average test loss: 0.0013145100991759035\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00606906978206502\n",
      "Average test loss: 0.001336925801510612\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006065748828980658\n",
      "Average test loss: 0.0013211226117693716\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006056227238641845\n",
      "Average test loss: 0.0013148326978294385\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0060530294962227345\n",
      "Average test loss: 0.0013640959687117073\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0060543029639456005\n",
      "Average test loss: 0.0013105773728133905\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006050957688440879\n",
      "Average test loss: 0.0013325739496697982\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006043096288210816\n",
      "Average test loss: 0.0013256059125479724\n",
      "Epoch 230/300\n",
      "Average training loss: 0.006047609943896532\n",
      "Average test loss: 0.001355983311517371\n",
      "Epoch 231/300\n",
      "Average training loss: 0.006044847370849715\n",
      "Average test loss: 0.0012965945029217336\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006038741996718777\n",
      "Average test loss: 0.0013073270701699786\n",
      "Epoch 233/300\n",
      "Average training loss: 0.006043455524163114\n",
      "Average test loss: 0.001335662653669715\n",
      "Epoch 234/300\n",
      "Average training loss: 0.006036752647823758\n",
      "Average test loss: 0.0013036847714748647\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006036671579711967\n",
      "Average test loss: 0.0013015341270301078\n",
      "Epoch 236/300\n",
      "Average training loss: 0.00603373512170381\n",
      "Average test loss: 0.001313784280481438\n",
      "Epoch 237/300\n",
      "Average training loss: 0.006029835195591053\n",
      "Average test loss: 0.001343304521507687\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0060383483829597635\n",
      "Average test loss: 0.0013350466791954305\n",
      "Epoch 239/300\n",
      "Average training loss: 0.006044008217338059\n",
      "Average test loss: 0.0013231365682764185\n",
      "Epoch 240/300\n",
      "Average training loss: 0.00602699150558975\n",
      "Average test loss: 0.00130433882224477\n",
      "Epoch 241/300\n",
      "Average training loss: 0.006023617267401682\n",
      "Average test loss: 0.0013442641211052736\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0060194384650223785\n",
      "Average test loss: 0.0013480489977842403\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0060203316277927825\n",
      "Average test loss: 0.0013177528588825629\n",
      "Epoch 244/300\n",
      "Average training loss: 0.006018462870270014\n",
      "Average test loss: 0.0013296972330556149\n",
      "Epoch 245/300\n",
      "Average training loss: 0.006020468514412641\n",
      "Average test loss: 0.0013396500615506536\n",
      "Epoch 246/300\n",
      "Average training loss: 0.006024922390778859\n",
      "Average test loss: 0.0013349520915912256\n",
      "Epoch 247/300\n",
      "Average test loss: 0.0013264578218675322\n",
      "Epoch 251/300\n",
      "Average training loss: 0.006013349057899581\n",
      "Average test loss: 0.001342202145399319\n",
      "Epoch 252/300\n",
      "Average training loss: 0.006011734024518066\n",
      "Average test loss: 0.001321433386972381\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0060114572656651336\n",
      "Average test loss: 0.0012958908784720634\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0059980828743428\n",
      "Average test loss: 0.0013333894004527893\n",
      "Epoch 255/300\n",
      "Average training loss: 0.006007023268275791\n",
      "Average test loss: 0.0013278025128982134\n",
      "Epoch 256/300\n",
      "Average training loss: 0.005995958693325519\n",
      "Average test loss: 0.0013251473341758053\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0060019872871538\n",
      "Average test loss: 0.0013203639078678356\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006007109431342946\n",
      "Average test loss: 0.0013247523182589147\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0060048218063182305\n",
      "Average test loss: 0.0013356968211010098\n",
      "Epoch 260/300\n",
      "Average training loss: 0.005998024076223373\n",
      "Average test loss: 0.0012840155824604962\n",
      "Epoch 261/300\n",
      "Average training loss: 0.005985963514695565\n",
      "Average test loss: 0.0013789171477158865\n",
      "Epoch 262/300\n",
      "Average training loss: 0.005987196964936124\n",
      "Average test loss: 0.0013002185608363814\n",
      "Epoch 263/300\n",
      "Average training loss: 0.005994929856724209\n",
      "Average test loss: 0.0013668387586043941\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0059864073561297524\n",
      "Average test loss: 0.0013207022642923727\n",
      "Epoch 265/300\n",
      "Average training loss: 0.005991686153329081\n",
      "Average test loss: 0.0013341760347700782\n",
      "Epoch 266/300\n",
      "Average training loss: 0.005984096345802149\n",
      "Average test loss: 0.0012989826198253366\n",
      "Epoch 267/300\n",
      "Average training loss: 0.005977177758183744\n",
      "Average test loss: 0.0013638948468077514\n",
      "Epoch 268/300\n",
      "Average training loss: 0.005989291254844931\n",
      "Average test loss: 0.0013359704259783029\n",
      "Epoch 269/300\n",
      "Average training loss: 0.005991061580677827\n",
      "Average test loss: 0.0013634163841812147\n",
      "Epoch 270/300\n",
      "Average training loss: 0.005983104702085257\n",
      "Average test loss: 0.0013343562025369868\n",
      "Epoch 271/300\n",
      "Average training loss: 0.005988898881193664\n",
      "Average test loss: 0.0013631955963145528\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005974183361977339\n",
      "Average test loss: 0.001350587892346084\n",
      "Epoch 273/300\n",
      "Average training loss: 0.005978019446134567\n",
      "Average test loss: 0.0013492148361272283\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005979870252104269\n",
      "Average test loss: 0.001336895946620239\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005964851972957452\n",
      "Average test loss: 0.0013326771028029422\n",
      "Epoch 276/300\n",
      "Average training loss: 0.005973869066685438\n",
      "Average test loss: 0.001311296769624783\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005972519278112385\n",
      "Average test loss: 0.0013132892997366273\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0059667266089883115\n",
      "Average test loss: 0.0013570090702010526\n",
      "Epoch 279/300\n",
      "Average training loss: 0.005955028475986586\n",
      "Average test loss: 0.0013245171813501251\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005968165691941977\n",
      "Average test loss: 0.001327518210435907\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00596582112254368\n",
      "Average test loss: 0.0013318410665831633\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005961611085053947\n",
      "Average test loss: 0.0013270908111913337\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00596106182411313\n",
      "Average test loss: 0.0013194746378188333\n",
      "Epoch 284/300\n",
      "Average training loss: 0.005968041155487299\n",
      "Average test loss: 0.0013179734316137101\n",
      "Epoch 288/300\n",
      "Average training loss: 0.005960587440058589\n",
      "Average test loss: 0.0013572454396635293\n",
      "Epoch 289/300\n",
      "Average training loss: 0.005948108029448324\n",
      "Average test loss: 0.0013304811831977633\n",
      "Epoch 290/300\n",
      "Average training loss: 0.005959824591875077\n",
      "Average test loss: 0.0013381293744055762\n",
      "Epoch 291/300\n",
      "Average training loss: 0.005950225429816379\n",
      "Average test loss: 0.0013285610272238651\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00595419463266929\n",
      "Average test loss: 0.001361226433991558\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005942261271178723\n",
      "Average test loss: 0.00132695210704373\n",
      "Epoch 294/300\n",
      "Average training loss: 0.005951793023281627\n",
      "Average test loss: 0.0013430866741885742\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0059415937438607215\n",
      "Average test loss: 0.001356238665887051\n",
      "Epoch 296/300\n",
      "Average training loss: 0.005940255493753486\n",
      "Average test loss: 0.0013359904607964886\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005943110671515266\n",
      "Average test loss: 0.001355020407276849\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005938464244206746\n",
      "Average test loss: 0.0013407934487072958\n",
      "Epoch 299/300\n",
      "Average training loss: 0.005941929298556513\n",
      "Average test loss: 0.0013510002829134463\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0059371260930266645\n",
      "Average test loss: 0.0013386489358316693\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_80_Depth10/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e7981d8-392a-4490-b70e-8b5b20f0e56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Memory-Net-Inverse/Denoising_Algorithms/DL_Training/evaluation.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(directory, map_location=model.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.78\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.46\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 32.12\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 33.07\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 33.65\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 34.18\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87ec7f-3c67-4e95-bec1-061d87255384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.2599781551361082\n",
      "Average test loss: 0.005569210176666578\n",
      "Epoch 2/300\n",
      "Average training loss: 1.3555583081775242\n",
      "Average test loss: 0.004351347727494108\n",
      "Epoch 3/300\n",
      "Average training loss: 0.681897623538971\n",
      "Average test loss: 0.004115056646366914\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5119002437326643\n",
      "Average test loss: 0.0040648712815923825\n",
      "Epoch 6/300\n",
      "Average training loss: 0.3916253232691023\n",
      "Average test loss: 0.003907966848048899\n",
      "Epoch 7/300\n",
      "Average training loss: 0.3133636778725518\n",
      "Average test loss: 0.0038822512266536552\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2569971037573285\n",
      "Average test loss: 0.0038309214224831927\n",
      "Epoch 9/300\n",
      "Average training loss: 0.21334095497926076\n",
      "Average test loss: 0.003832660989628898\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18008068283398945\n",
      "Average test loss: 0.003775796103808615\n",
      "Epoch 11/300\n",
      "Average training loss: 0.15441800106896295\n",
      "Average test loss: 0.0038514404185116292\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13385534563329485\n",
      "Average test loss: 0.0037673294929166635\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11832346034049988\n",
      "Average test loss: 0.0037193370113770166\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1059874852101008\n",
      "Average test loss: 0.003706296447250578\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09637569391065173\n",
      "Average test loss: 0.0036917255197962125\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08219060515032874\n",
      "Average test loss: 0.003665542984795239\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07735399935642878\n",
      "Average test loss: 0.0036569630802712506\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07351430453194513\n",
      "Average test loss: 0.003631161825524436\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07039506206247542\n",
      "Average test loss: 0.0036248251907527447\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06780278726087677\n",
      "Average test loss: 0.003601372126696838\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06568632607327567\n",
      "Average test loss: 0.0036828677381078403\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06396271848016315\n",
      "Average test loss: 0.0036304756539563337\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06146009581618839\n",
      "Average test loss: 0.0035803273055288526\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06051851425568263\n",
      "Average test loss: 0.0035575842197156614\n",
      "Epoch 27/300\n",
      "Average training loss: 0.060037787798378206\n",
      "Average test loss: 0.0035852786449508535\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05932476473848025\n",
      "Average test loss: 0.0035529662834273444\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05861775552895334\n",
      "Average test loss: 0.0035498448287447294\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05792372990979089\n",
      "Average test loss: 0.00353211486670706\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05756951157251994\n",
      "Average test loss: 0.0035269969049841164\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0570975675019953\n",
      "Average test loss: 0.003509837744136651\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05716943828596009\n",
      "Average test loss: 0.0035285381343629625\n",
      "Epoch 34/300\n",
      "Average training loss: 0.056584679073757596\n",
      "Average test loss: 0.003499540790708529\n",
      "Epoch 35/300\n",
      "Average training loss: 0.056303187602096134\n",
      "Average test loss: 0.0035224435209400126\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05560900905397203\n",
      "Average test loss: 0.0034925261282672485\n",
      "Epoch 38/300\n",
      "Average training loss: 0.055400485191080304\n",
      "Average test loss: 0.003502954171763526\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05513023273977968\n",
      "Average test loss: 0.003513574822909302\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05467782962653372\n",
      "Average test loss: 0.0034792921936346423\n",
      "Epoch 42/300\n",
      "Average training loss: 0.054518476287523904\n",
      "Average test loss: 0.0034777408122188516\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05431191583805614\n",
      "Average test loss: 0.0034942305942790375\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05416790806253751\n",
      "Average test loss: 0.0035206149626109337\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05401249623298645\n",
      "Average test loss: 0.0034803200190265974\n",
      "Epoch 46/300\n",
      "Average training loss: 0.053927090697818335\n",
      "Average test loss: 0.0034673423086189563\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05363022815518909\n",
      "Average test loss: 0.003461349130504661\n",
      "Epoch 48/300\n",
      "Average training loss: 0.053490351382229066\n",
      "Average test loss: 0.0034814751309653123\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05334318967991405\n",
      "Average test loss: 0.0034947621936185494\n",
      "Epoch 50/300\n",
      "Average training loss: 0.053226954069402484\n",
      "Average test loss: 0.0034838154800236223\n",
      "Epoch 51/300\n",
      "Average training loss: 0.053182766444153255\n",
      "Average test loss: 0.0034604884650972154\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05286831604441007\n",
      "Average test loss: 0.003454305023368862\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0526763562394513\n",
      "Average test loss: 0.0034584957897249194\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05256207060813904\n",
      "Average test loss: 0.0034519135742965674\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05245849655071894\n",
      "Average test loss: 0.0035780008499407107\n",
      "Epoch 57/300\n",
      "Average training loss: 0.052400907301240494\n",
      "Average test loss: 0.0034587836919559374\n",
      "Epoch 58/300\n",
      "Average training loss: 0.052244062135616935\n",
      "Average test loss: 0.003435459152277973\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05209382329053349\n",
      "Average test loss: 0.0034746044801755084\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05206619409057829\n",
      "Average test loss: 0.0034479710434873897\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05191639375686646\n",
      "Average test loss: 0.0034419053254856\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05170650853051079\n",
      "Average test loss: 0.0034647901252739957\n",
      "Epoch 64/300\n",
      "Average training loss: 0.051644881029923755\n",
      "Average test loss: 0.003441488409207927\n",
      "Epoch 65/300\n",
      "Average training loss: 0.051528452032142215\n",
      "Average test loss: 0.0034413643094400566\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0513594064861536\n",
      "Average test loss: 0.003554601372943984\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05128231568800078\n",
      "Average test loss: 0.0034437272519701055\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05118810151351823\n",
      "Average test loss: 0.003470555968168709\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05108544308940569\n",
      "Average test loss: 0.003463938032587369\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05100020577841335\n",
      "Average test loss: 0.003464437260395951\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05085697097910775\n",
      "Average test loss: 0.0034775333375566537\n",
      "Epoch 72/300\n",
      "Average training loss: 0.050704432590140236\n",
      "Average test loss: 0.0035472786472075515\n",
      "Epoch 74/300\n",
      "Average test loss: 0.0034782676071756415\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05038396257493231\n",
      "Average test loss: 0.0034668315104726287\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05033442215124766\n",
      "Average test loss: 0.003472052491787407\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05017407355705897\n",
      "Average test loss: 0.0034673741306695672\n",
      "Epoch 79/300\n",
      "Average training loss: 0.050200108263227676\n",
      "Average test loss: 0.003480087688813607\n",
      "Epoch 80/300\n",
      "Average training loss: 0.050003709319565035\n",
      "Average test loss: 0.003511102684463064\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04993398642208841\n",
      "Average test loss: 0.0034714416853255698\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04982647399769889\n",
      "Average test loss: 0.0035043159189323586\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04975410404470232\n",
      "Average test loss: 0.0034592795479628773\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04973389900061819\n",
      "Average training loss: 0.049495386660099026\n",
      "Average test loss: 0.0034978292385737103\n",
      "Epoch 87/300\n",
      "Average training loss: 0.049400095967782866\n",
      "Average test loss: 0.0034984599521590606\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04937897693779734\n",
      "Average test loss: 0.003526074673773514\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04921982876459757\n",
      "Average test loss: 0.0035168172212110627\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04915155866742134\n",
      "Average test loss: 0.003506149242942532\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04908922658695115\n",
      "Average test loss: 0.003507511242396302\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04913304543164041\n",
      "Average test loss: 0.003505951291984982\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04887000452313158\n",
      "Average test loss: 0.0035492428067243763\n",
      "Epoch 94/300\n",
      "Average training loss: 0.048807097603877386\n",
      "Average test loss: 0.003528994395915005\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04871313234832552\n",
      "Average test loss: 0.0034964628999845848\n",
      "Epoch 97/300\n",
      "Average training loss: 0.048670699503686694\n",
      "Average test loss: 0.0034980454800857437\n",
      "Epoch 98/300\n",
      "Average training loss: 0.048527872522672016\n",
      "Average test loss: 0.0035529666965206466\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04845812678337097\n",
      "Average test loss: 0.0035854134236772855\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04840716038809882\n",
      "Average test loss: 0.0034573521783782374\n",
      "Epoch 101/300\n",
      "Average training loss: 0.048339530030886334\n",
      "Average test loss: 0.003495765332132578\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04826250153779983\n",
      "Average test loss: 0.003512180831697252\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04819375893473625\n",
      "Average test loss: 0.0035895693562924863\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04810655387904909\n",
      "Average test loss: 0.0035439755730330945\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04798108378383848\n",
      "Average test loss: 0.003537779480115407\n",
      "Epoch 107/300\n",
      "Average training loss: 0.047864937484264375\n",
      "Average test loss: 0.003490612232954138\n",
      "Epoch 108/300\n",
      "Average training loss: 0.047845402856667836\n",
      "Average test loss: 0.0035390494730737476\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04780380773213175\n",
      "Average test loss: 0.0035765941200984848\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04776378434234195\n",
      "Average test loss: 0.003615720054962569\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04761106072697375\n",
      "Average test loss: 0.003586599096862806\n",
      "Epoch 113/300\n",
      "Average training loss: 0.047540305743614834\n",
      "Average test loss: 0.0035034490066270033\n",
      "Epoch 114/300\n",
      "Average training loss: 0.047474857098526425\n",
      "Average test loss: 0.0035475188992503616\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04742353976103995\n",
      "Average test loss: 0.003555198724071185\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04733008910218875\n",
      "Average test loss: 0.0035988787712736264\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04734804759091801\n",
      "Average test loss: 0.00354841689641277\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04724585798382759\n",
      "Average test loss: 0.0036047056805756355\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04719300459159745\n",
      "Average test loss: 0.003542316246363852\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04705122763911883\n",
      "Average test loss: 0.0035877836185197035\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04702807048625416\n",
      "Average test loss: 0.003560152347303099\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04693946819504102\n",
      "Average test loss: 0.0035751476714180577\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04687951273057196\n",
      "Average test loss: 0.0035906574330810044\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04692182696196768\n",
      "Average test loss: 0.0035649805778844488\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04678952921761407\n",
      "Average test loss: 0.0036553605045709344\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04675199199385113\n",
      "Average test loss: 0.0035653215278353954\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04671171252926191\n",
      "Average test loss: 0.003583353191614151\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04658127122455173\n",
      "Average test loss: 0.0036147722953723538\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04656333162387212\n",
      "Average test loss: 0.0036681495091567436\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04650462415979968\n",
      "Average test loss: 0.003701130543731981\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04640007684297032\n",
      "Average test loss: 0.0035815955315613083\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04642480084631178\n",
      "Average test loss: 0.0036208163561920327\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04633339826928245\n",
      "Average test loss: 0.0036147077423002985\n",
      "Epoch 136/300\n",
      "Average training loss: 0.046353747594687675\n",
      "Average test loss: 0.003565901112639242\n",
      "Epoch 137/300\n",
      "Average training loss: 0.046242164704534745\n",
      "Average test loss: 0.00358882582332525\n",
      "Epoch 138/300\n",
      "Average training loss: 0.046220579110913804\n",
      "Average test loss: 0.0036614835009806685\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04617745134896702\n",
      "Average test loss: 0.0036729021155171926\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04613185516993205\n",
      "Average test loss: 0.003625807317180766\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0460186976061927\n",
      "Average test loss: 0.003675448360542456\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04589977894557847\n",
      "Average test loss: 0.0035964016916437283\n",
      "Epoch 145/300\n",
      "Average training loss: 0.045929264144765\n",
      "Average test loss: 0.0036374744342433083\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0458728453748756\n",
      "Average test loss: 0.003607508981600404\n",
      "Epoch 147/300\n",
      "Average training loss: 0.045788039922714234\n",
      "Average test loss: 0.0036697621749093136\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04576777915490998\n",
      "Average test loss: 0.0037112784224251905\n",
      "Epoch 149/300\n",
      "Average training loss: 0.045797513561116325\n",
      "Average test loss: 0.003581918978235788\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04574838662147522\n",
      "Average test loss: 0.003628816306591034\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0456353627079063\n",
      "Average test loss: 0.0036048950132810407\n",
      "Epoch 152/300\n",
      "Average training loss: 0.045635277976592385\n",
      "Average test loss: 0.003593378814144267\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04550253329674403\n",
      "Average test loss: 0.0036782212934146326\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04547904965115918\n",
      "Average test loss: 0.0036919849548074936\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04550001140435537\n",
      "Average test loss: 0.003576095608373483\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04542865759796567\n",
      "Average test loss: 0.003616567476756043\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04542899555961291\n",
      "Average test loss: 0.00363460079414977\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04536743978659312\n",
      "Average test loss: 0.003634905546489689\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04528571782840623\n",
      "Average test loss: 0.003718091013944811\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04524898887508445\n",
      "Average test loss: 0.0035954776387661694\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04517304180728065\n",
      "Average test loss: 0.003690013991047939\n",
      "Epoch 165/300\n",
      "Average training loss: 0.045138407645954025\n",
      "Average test loss: 0.0036028439874450364\n",
      "Epoch 166/300\n",
      "Average training loss: 0.045098153243462245\n",
      "Average test loss: 0.003682233246250285\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04506823607948091\n",
      "Average test loss: 0.003696639071736071\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04503691425588396\n",
      "Average test loss: 0.003686261491643058\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04500244146419896\n",
      "Average test loss: 0.0036948239225894214\n",
      "Epoch 170/300\n",
      "Average training loss: 0.044983294400903914\n",
      "Average test loss: 0.0036443760726186966\n",
      "Epoch 171/300\n",
      "Average training loss: 0.044928811613056396\n",
      "Average test loss: 0.00377325151157048\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04491795581248072\n",
      "Average test loss: 0.0036091865305271412\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0448387052251233\n",
      "Average test loss: 0.0037449494985242683\n",
      "Epoch 175/300\n",
      "Average training loss: 0.044774691743983165\n",
      "Average test loss: 0.003685281264492207\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04474743519061142\n",
      "Average test loss: 0.0036652373961276476\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04472580242156982\n",
      "Average test loss: 0.0036652555767860678\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04468886119127274\n",
      "Average test loss: 0.0036360765205075344\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04465643288691839\n",
      "Average test loss: 0.0037012000179125204\n",
      "Epoch 181/300\n",
      "Average training loss: 0.044640400489171346\n",
      "Average test loss: 0.003690052496476306\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04461171279682054\n",
      "Average test loss: 0.003728988350679477\n",
      "Epoch 183/300\n",
      "Average training loss: 0.044542365954981906\n",
      "Average test loss: 0.003742721089058452\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04455062922504213\n",
      "Average test loss: 0.003625534435113271\n",
      "Epoch 185/300\n",
      "Average training loss: 0.044542862435181935\n",
      "Average test loss: 0.003704415033467942\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0444962470597691\n",
      "Average test loss: 0.0037796990144997834\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04445927105678452\n",
      "Average test loss: 0.003697818857514196\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0444004615313477\n",
      "Average test loss: 0.0036123269790162642\n",
      "Epoch 190/300\n",
      "Average training loss: 0.044384961234198675\n",
      "Average test loss: 0.0036493177513281503\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04431387218170696\n",
      "Average test loss: 0.003637203600257635\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04430122744374805\n",
      "Average test loss: 0.0036635421593156125\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04426449869407548\n",
      "Average test loss: 0.003708923085903128\n",
      "Epoch 194/300\n",
      "Average training loss: 0.044272496438688705\n",
      "Average test loss: 0.00363679777044389\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04422603871093856\n",
      "Average test loss: 0.003649852802976966\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04417988955643442\n",
      "Average test loss: 0.00367895899568167\n",
      "Epoch 197/300\n",
      "Average training loss: 0.044170480771197215\n",
      "Average test loss: 0.0036879553701728582\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04413110222419103\n",
      "Average test loss: 0.0036839739953478177\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04412417812479867\n",
      "Average test loss: 0.0037299537402060295\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04408326328463025\n",
      "Average test loss: 0.0037828449284036956\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04411439002222485\n",
      "Average test loss: 0.003742734706029296\n",
      "Epoch 202/300\n",
      "Average training loss: 0.044032328633798495\n",
      "Average test loss: 0.0037243797021607556\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04398104555077023\n",
      "Average test loss: 0.003640068990902768\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0439996142619186\n",
      "Average test loss: 0.003702671813468138\n",
      "Epoch 205/300\n",
      "Average training loss: 0.043960077679819534\n",
      "Average test loss: 0.0037681532144132586\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04395851087901327\n",
      "Average test loss: 0.0037376253294448057\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04391618083582984\n",
      "Average test loss: 0.0036882265901399982\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04392721122172144\n",
      "Average test loss: 0.003743173549572627\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04385018862287204\n",
      "Average test loss: 0.0037481772493984963\n",
      "Epoch 210/300\n",
      "Average training loss: 0.043856059670448304\n",
      "Average test loss: 0.0036432633323387966\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04382577772935232\n",
      "Average test loss: 0.003919281392668684\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04381326594948769\n",
      "Average test loss: 0.003648253518053227\n",
      "Epoch 213/300\n",
      "Average training loss: 0.043760237326224645\n",
      "Average test loss: 0.0037053592627247175\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04379767516917653\n",
      "Average test loss: 0.0037032358021371896\n",
      "Epoch 215/300\n",
      "Average training loss: 0.043787250876426695\n",
      "Average test loss: 0.003664578956241409\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0436820410920514\n",
      "Average test loss: 0.003669157189006607\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04366508283383316\n",
      "Average test loss: 0.003710871415833632\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04363969354000356\n",
      "Average test loss: 0.0036779339959224066\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04366301057073805\n",
      "Average test loss: 0.003825892851791448\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04362873549924957\n",
      "Average test loss: 0.0037925796192139387\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04364309649169445\n",
      "Average test loss: 0.0037823221913228433\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04356529163983133\n",
      "Average test loss: 0.003678333497295777\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04352049409515328\n",
      "Average test loss: 0.0037724842006961507\n",
      "Epoch 224/300\n",
      "Average training loss: 0.043510533640782036\n",
      "Average test loss: 0.003729553202994996\n",
      "Epoch 225/300\n",
      "Average training loss: 0.043497793502277796\n",
      "Average test loss: 0.003728452075065838\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04347044201029671\n",
      "Average test loss: 0.003849648184246487\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04348090788722038\n",
      "Average test loss: 0.003643081727541155\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04342863440844748\n",
      "Average test loss: 0.0037438572496175768\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04343894720408652\n",
      "Average test loss: 0.0037408214712308513\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04336551659637027\n",
      "Average test loss: 0.0036557111812548504\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0433972515364488\n",
      "Average test loss: 0.0037720748010194962\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04338059511780739\n",
      "Average test loss: 0.0037055546792431012\n",
      "Epoch 233/300\n",
      "Average training loss: 0.043348517038755946\n",
      "Average test loss: 0.0039777162037789825\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04334742362962829\n",
      "Average test loss: 0.0037358449779243933\n",
      "Epoch 235/300\n",
      "Average training loss: 0.043301984068420195\n",
      "Average test loss: 0.003751741796731949\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04328034752276209\n",
      "Average test loss: 0.0036702456271482838\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04321756954987844\n",
      "Average test loss: 0.0037331738142917553\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04320098873641756\n",
      "Average test loss: 0.0037154224204520385\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04319640801350276\n",
      "Average test loss: 0.0037649284226612912\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04323006598485841\n",
      "Average test loss: 0.0037259897101256582\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04317151655753453\n",
      "Average test loss: 0.0037634152724511094\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0431505559153027\n",
      "Average test loss: 0.0036329982301427258\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04312188623348872\n",
      "Average test loss: 0.0038781361174252296\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04312456706166267\n",
      "Average test loss: 0.003725061418695582\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04309719132714802\n",
      "Average test loss: 0.003862425772473216\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04310973047216733\n",
      "Average test loss: 0.0037130716693484123\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04303813081648614\n",
      "Average test loss: 0.0037490994880596796\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04305685380929046\n",
      "Average test loss: 0.0037515440341085197\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04301218789981471\n",
      "Average test loss: 0.0037661647649688853\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04304908925626013\n",
      "Average test loss: 0.0037452548156595893\n",
      "Epoch 251/300\n",
      "Average training loss: 0.042995171815156934\n",
      "Average test loss: 0.003684268854558468\n",
      "Epoch 252/300\n",
      "Average training loss: 0.042933472716146046\n",
      "Average test loss: 0.0036972597659462028\n",
      "Epoch 253/300\n",
      "Average training loss: 0.042944699813922245\n",
      "Average test loss: 0.0036967996089822715\n",
      "Epoch 254/300\n",
      "Average training loss: 0.042928182707892525\n",
      "Average test loss: 0.0037750143135587373\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04288821318745613\n",
      "Average test loss: 0.0037831070512119266\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04293174944486883\n",
      "Average test loss: 0.0037909410345471567\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04289886868331168\n",
      "Average test loss: 0.003714172065258026\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04285547393229273\n",
      "Average test loss: 0.0036818036321136687\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0428445251054234\n",
      "Average test loss: 0.0037111986428499224\n",
      "Epoch 260/300\n",
      "Average training loss: 0.042806368768215176\n",
      "Average test loss: 0.00376967692333791\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04281099847621388\n",
      "Average test loss: 0.003761418239523967\n",
      "Epoch 262/300\n",
      "Average training loss: 0.042834480990966164\n",
      "Average test loss: 0.003769296739457382\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04276782289809651\n",
      "Average test loss: 0.0037225530900888973\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04274012827542093\n",
      "Average test loss: 0.0036583546867801084\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04272534825735622\n",
      "Average test loss: 0.003697411814497577\n",
      "Epoch 266/300\n",
      "Average training loss: 0.042751478536261454\n",
      "Average test loss: 0.003728614411006371\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04268841636512015\n",
      "Average test loss: 0.0037411303019358054\n",
      "Epoch 268/300\n",
      "Average training loss: 0.042682666824923624\n",
      "Average test loss: 0.003794941280451086\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04268967192537255\n",
      "Average test loss: 0.003811697734312879\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04261175270544158\n",
      "Average test loss: 0.003755927413908972\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04265159479114745\n",
      "Average test loss: 0.0037989239804446695\n",
      "Epoch 272/300\n",
      "Average training loss: 0.042583927472432455\n",
      "Average test loss: 0.0037083344016638066\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04263775751325819\n",
      "Average test loss: 0.0037941329247421687\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04256624724798733\n",
      "Average test loss: 0.0037790868170559408\n",
      "Epoch 275/300\n",
      "Average training loss: 0.042577751467625304\n",
      "Average test loss: 0.0037102389145228596\n",
      "Epoch 276/300\n",
      "Average training loss: 0.042581868966420494\n",
      "Average test loss: 0.003782204516025053\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04256184740861257\n",
      "Average test loss: 0.003713854017150071\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04249703961610794\n",
      "Average test loss: 0.0037725438765353627\n",
      "Epoch 279/300\n",
      "Average training loss: 0.042496543165710235\n",
      "Average test loss: 0.003761115773891409\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0424758639368746\n",
      "Average test loss: 0.003748341562847296\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04248601059118907\n",
      "Average test loss: 0.0036811384979842437\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04247421293788486\n",
      "Average test loss: 0.003734097725401322\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04242691599991587\n",
      "Average test loss: 0.003740923503620757\n",
      "Epoch 284/300\n",
      "Average training loss: 0.042391164312760035\n",
      "Average test loss: 0.0036636136788874866\n",
      "Epoch 285/300\n",
      "Average training loss: 0.042427707576089435\n",
      "Average test loss: 0.0037397005218598577\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04237949288553662\n",
      "Average test loss: 0.0038236793329318366\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04232736124595006\n",
      "Average test loss: 0.003750921133905649\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04241975865761439\n",
      "Average test loss: 0.0037193963778101736\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04235802200602161\n",
      "Average test loss: 0.003728836723913749\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04235044221248892\n",
      "Average test loss: 0.0037693688029216397\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04229741536577543\n",
      "Average test loss: 0.0037011687664522063\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04233975809150272\n",
      "Average test loss: 0.0037867920943018465\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04230004320873155\n",
      "Average test loss: 0.0037335577284296354\n",
      "Epoch 294/300\n",
      "Average training loss: 0.042298788189888\n",
      "Average test loss: 0.0037112082000821827\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04224447281824218\n",
      "Average test loss: 0.0038518187614778676\n",
      "Epoch 296/300\n",
      "Average training loss: 0.042250912692811754\n",
      "Average test loss: 0.0037488337581356368\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04226016849610541\n",
      "Average test loss: 0.0036579148566557303\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04219277219639884\n",
      "Average test loss: 0.0037412306571172342\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04222316376368205\n",
      "Average test loss: 0.0037087192322231\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04223809120059013\n",
      "Average test loss: 0.0038327089386681716\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.5099027819103665\n",
      "Average test loss: 0.028455272691945234\n",
      "Epoch 2/300\n",
      "Average training loss: 1.367890089140998\n",
      "Average test loss: 0.007550367451376386\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8947451042069329\n",
      "Average test loss: 0.0038001165750126045\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6395182458029853\n",
      "Average test loss: 0.00371632376851307\n",
      "Epoch 5/300\n",
      "Average training loss: 0.4894546003871494\n",
      "Average test loss: 0.0034978068934546575\n",
      "Epoch 6/300\n",
      "Average training loss: 0.3890950494077471\n",
      "Average test loss: 0.003411102120247152\n",
      "Epoch 7/300\n",
      "Average training loss: 0.31934171602461076\n",
      "Average test loss: 0.0033796852299322684\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2668992380566067\n",
      "Average test loss: 0.0032382771538363563\n",
      "Epoch 9/300\n",
      "Average training loss: 0.22649422913127476\n",
      "Average test loss: 0.0032569799361129603\n",
      "Epoch 10/300\n",
      "Average training loss: 0.19484474674860636\n",
      "Average test loss: 0.0031706778429862526\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1688092721303304\n",
      "Average test loss: 0.003091540492967599\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14743252885341646\n",
      "Average test loss: 0.003057093205758267\n",
      "Epoch 13/300\n",
      "Average training loss: 0.12967529012097254\n",
      "Average test loss: 0.0030014669119069973\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11464489955372281\n",
      "Average test loss: 0.014672003369985356\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10216466924217012\n",
      "Average test loss: 0.0029415503941062422\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0919879700144132\n",
      "Average test loss: 0.0031641862771163384\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08317104261451297\n",
      "Average test loss: 0.004129616574694713\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07622406357857917\n",
      "Average test loss: 0.0028547309606025617\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07054252827829785\n",
      "Average test loss: 0.00281694865723451\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06578278103470803\n",
      "Average test loss: 0.002783421007709371\n",
      "Epoch 21/300\n",
      "Average training loss: 0.062132280535168116\n",
      "Average test loss: 0.002729867954634958\n",
      "Epoch 22/300\n",
      "Average training loss: 0.059034110897117194\n",
      "Average test loss: 0.002690249125783642\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05683188536763191\n",
      "Average test loss: 0.0028484778321451612\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05473225301504135\n",
      "Average test loss: 0.0026606072809340227\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05270856559607718\n",
      "Average test loss: 0.002627352090448969\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05132647125257386\n",
      "Average test loss: 0.002625155371096399\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05171321198344231\n",
      "Average test loss: 0.002600273710572057\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04917161973979738\n",
      "Average test loss: 0.002602138840696878\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04835413020518091\n",
      "Average test loss: 0.002586186604056921\n",
      "Epoch 30/300\n",
      "Average training loss: 0.047695462597741024\n",
      "Average test loss: 0.002585544288986259\n",
      "Epoch 31/300\n",
      "Average training loss: 0.047200001726547876\n",
      "Average test loss: 0.0025607547683434355\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04635576353636053\n",
      "Average test loss: 0.0025970312050647205\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04581561383273867\n",
      "Average test loss: 0.0025508320428844957\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04523717314336035\n",
      "Average test loss: 0.002526681436639693\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04471526285012563\n",
      "Average test loss: 0.002511830142802662\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04430733646617995\n",
      "Average test loss: 0.002522347437010871\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04383129911621412\n",
      "Average test loss: 0.002504126013153129\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04343917724821303\n",
      "Average test loss: 0.0024991396720417673\n",
      "Epoch 39/300\n",
      "Average training loss: 0.042971727463934156\n",
      "Average test loss: 0.002508448829460475\n",
      "Epoch 40/300\n",
      "Average training loss: 0.042596262488100264\n",
      "Average test loss: 0.002502629600465298\n",
      "Epoch 41/300\n",
      "Average training loss: 0.042223391219973566\n",
      "Average test loss: 0.002509396276022825\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04186269617577394\n",
      "Average test loss: 0.0024848471867541474\n",
      "Epoch 43/300\n",
      "Average training loss: 0.041614515079392324\n",
      "Average test loss: 0.0024823924222340185\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04123361151251528\n",
      "Average test loss: 0.002442026094223062\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04093442436390453\n",
      "Average test loss: 0.0024327508968611557\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04063678698241711\n",
      "Average test loss: 0.0024338749231149755\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04039237320423126\n",
      "Average test loss: 0.0024380239699449803\n",
      "Epoch 48/300\n",
      "Average training loss: 0.040086676432026756\n",
      "Average test loss: 0.0024219934531591004\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03990611593425274\n",
      "Average test loss: 0.0024488356622556847\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03962089111904303\n",
      "Average test loss: 0.0025545325953927303\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0393129973742697\n",
      "Average test loss: 0.002469293837228583\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03912234781351354\n",
      "Average test loss: 0.0024440215647013653\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03900460259947512\n",
      "Average test loss: 0.002508665194734931\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03867734204067124\n",
      "Average test loss: 0.0024448439832776784\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03844119060370657\n",
      "Average test loss: 0.002422165244196852\n",
      "Epoch 56/300\n",
      "Average training loss: 0.038263524807161754\n",
      "Average test loss: 0.0023989369317682255\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03805316287279129\n",
      "Average test loss: 0.0023931041616532538\n",
      "Epoch 58/300\n",
      "Average training loss: 0.037932688868708084\n",
      "Average test loss: 0.0023953157210101685\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03770201599266794\n",
      "Average test loss: 0.0024000555948457784\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03754646759066317\n",
      "Average test loss: 0.0024286095150229\n",
      "Epoch 61/300\n",
      "Average training loss: 0.037323487473858726\n",
      "Average test loss: 0.002499163382893635\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03718188675244649\n",
      "Average test loss: 0.0023964321590546106\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03698455948630969\n",
      "Average test loss: 0.0023915040652371113\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03686560679641035\n",
      "Average test loss: 0.0024400631323870684\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03667246163388093\n",
      "Average test loss: 0.002411314955395129\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03653139260990752\n",
      "Average test loss: 0.0024348658095631336\n",
      "Epoch 67/300\n",
      "Average training loss: 0.036349118798971174\n",
      "Average test loss: 0.002400519467993743\n",
      "Epoch 68/300\n",
      "Average training loss: 0.036225031384163435\n",
      "Average test loss: 0.0024244497290088073\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03610328575637606\n",
      "Average test loss: 0.0024378704651155407\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03601505355205801\n",
      "Average test loss: 0.0023919951737754875\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03590248076783286\n",
      "Average test loss: 0.0024081728712966044\n",
      "Epoch 72/300\n",
      "Average training loss: 0.035709294890364014\n",
      "Average test loss: 0.0024194208215922116\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03551271775364876\n",
      "Average test loss: 0.0024237553079922993\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03540699874692493\n",
      "Average test loss: 0.0024015211769276195\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03530884361929364\n",
      "Average test loss: 0.0024115385809499357\n",
      "Epoch 76/300\n",
      "Average training loss: 0.035149756285879345\n",
      "Average test loss: 0.002401188045947088\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0350284383677774\n",
      "Average test loss: 0.002465298027627998\n",
      "Epoch 78/300\n",
      "Average training loss: 0.034917773867646856\n",
      "Average test loss: 0.0024284065926654472\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03494174871676498\n",
      "Average test loss: 0.002425475678510136\n",
      "Epoch 80/300\n",
      "Average training loss: 0.034704653304484155\n",
      "Average test loss: 0.002481403418920106\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03461126570900281\n",
      "Average test loss: 0.002453984582796693\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034564124683539074\n",
      "Average test loss: 0.0024324297540717654\n",
      "Epoch 83/300\n",
      "Average training loss: 0.034430626995033686\n",
      "Average test loss: 0.002444708072890838\n",
      "Epoch 84/300\n",
      "Average training loss: 0.034253613942199286\n",
      "Average test loss: 0.0024717158627592857\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03423013061616156\n",
      "Average test loss: 0.002484668127571543\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03417709750268194\n",
      "Average test loss: 0.0024415073910107215\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03403842042055395\n",
      "Average test loss: 0.002554988493108087\n",
      "Epoch 88/300\n",
      "Average training loss: 0.033939766075876024\n",
      "Average test loss: 0.0024611443664050766\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03387134519716104\n",
      "Average test loss: 0.00261202427951826\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03374999177290334\n",
      "Average test loss: 0.0024914000147125788\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03370012698570887\n",
      "Average test loss: 0.0024609157156406177\n",
      "Epoch 92/300\n",
      "Average training loss: 0.033583688995904394\n",
      "Average test loss: 0.0025888473571588594\n",
      "Epoch 93/300\n",
      "Average training loss: 0.033526452875799605\n",
      "Average test loss: 0.002484040590830975\n",
      "Epoch 94/300\n",
      "Average training loss: 0.033391239288780426\n",
      "Average test loss: 0.0025418227650225163\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03334334039356973\n",
      "Average test loss: 0.002474978744569752\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03329909706777996\n",
      "Average test loss: 0.0025095392927113507\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03318985176251994\n",
      "Average test loss: 0.002460061503160331\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03320299878219764\n",
      "Average test loss: 0.0024527369048446416\n",
      "Epoch 99/300\n",
      "Average training loss: 0.033051240048474734\n",
      "Average test loss: 0.0025540551791588466\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03295293740265899\n",
      "Average test loss: 0.0024218193990074924\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03302547948724694\n",
      "Average test loss: 0.002468314190912578\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03280788891514142\n",
      "Average test loss: 0.0024695899111943113\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03278065255863799\n",
      "Average test loss: 0.0025432029959435265\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03267396270400948\n",
      "Average test loss: 0.002469161635885636\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03266742998030451\n",
      "Average test loss: 0.0024887982006702157\n",
      "Epoch 106/300\n",
      "Average training loss: 0.032597319619523155\n",
      "Average test loss: 0.002484632827548517\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03256693301929368\n",
      "Average test loss: 0.0024758660877123475\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03243449322382609\n",
      "Average test loss: 0.0024957193336966966\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03248671319584052\n",
      "Average test loss: 0.0024978699046704503\n",
      "Epoch 110/300\n",
      "Average training loss: 0.032390464703241986\n",
      "Average test loss: 0.0024559827651828526\n",
      "Epoch 111/300\n",
      "Average training loss: 0.032303881395194266\n",
      "Average test loss: 0.002486806052426497\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03227200083765719\n",
      "Average test loss: 0.002662348631148537\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03219563515318764\n",
      "Average test loss: 0.0025235922374866075\n",
      "Epoch 114/300\n",
      "Average training loss: 0.032203108774291146\n",
      "Average test loss: 0.0024962888409694036\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0321251678566138\n",
      "Average test loss: 0.0025467303346635565\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0320642318609688\n",
      "Average test loss: 0.002472235647858017\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03203730797270934\n",
      "Average test loss: 0.002497169377696183\n",
      "Epoch 118/300\n",
      "Average training loss: 0.031889219317171306\n",
      "Average test loss: 0.002665374284196231\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03190511421528128\n",
      "Average test loss: 0.0025420260568045907\n",
      "Epoch 120/300\n",
      "Average training loss: 0.031856077988942466\n",
      "Average test loss: 0.002532675055683487\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03180124006503158\n",
      "Average test loss: 0.0025139976044495902\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03174054758747419\n",
      "Average test loss: 0.002532100789869825\n",
      "Epoch 123/300\n",
      "Average training loss: 0.031865552263127435\n",
      "Average test loss: 0.0024746004156768323\n",
      "Epoch 124/300\n",
      "Average training loss: 0.031702626066075434\n",
      "Average test loss: 0.002516644829677211\n",
      "Epoch 125/300\n",
      "Average training loss: 0.031641931167907186\n",
      "Average test loss: 0.0025253918659355907\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03157601949572563\n",
      "Average test loss: 0.002516672455188301\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03156064365473058\n",
      "Average test loss: 0.002529241906892922\n",
      "Epoch 128/300\n",
      "Average training loss: 0.031516249040762584\n",
      "Average test loss: 0.002493381925341156\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03149652595818043\n",
      "Average test loss: 0.0025508558700482052\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03150108321838909\n",
      "Average test loss: 0.0025064638822029036\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03139761716789669\n",
      "Average test loss: 0.0024968686430818505\n",
      "Epoch 132/300\n",
      "Average training loss: 0.031322510509027374\n",
      "Average test loss: 0.002560610656109121\n",
      "Epoch 133/300\n",
      "Average training loss: 0.031242235536376634\n",
      "Average test loss: 0.0025206399168819188\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0312624275005526\n",
      "Average test loss: 0.0025234847305756476\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031262019485235215\n",
      "Average test loss: 0.0025378495059493514\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031232191362314754\n",
      "Average test loss: 0.002604851859104302\n",
      "Epoch 137/300\n",
      "Average training loss: 0.031248152236143748\n",
      "Average test loss: 0.0025685154510041077\n",
      "Epoch 138/300\n",
      "Average training loss: 0.031182252728276783\n",
      "Average test loss: 0.0025590423876419664\n",
      "Epoch 139/300\n",
      "Average training loss: 0.031104336279961797\n",
      "Average test loss: 0.002522742642503646\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03107438720100456\n",
      "Average test loss: 0.002541221675566501\n",
      "Epoch 141/300\n",
      "Average training loss: 0.031039715243710412\n",
      "Average test loss: 0.0026102887627979117\n",
      "Epoch 142/300\n",
      "Average training loss: 0.030905383398135502\n",
      "Average test loss: 0.0025308278166792457\n",
      "Epoch 143/300\n",
      "Average training loss: 0.030940137455860772\n",
      "Average test loss: 0.0025436988818562695\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03096196765369839\n",
      "Average test loss: 0.002512075875368383\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03085346347424719\n",
      "Average test loss: 0.0025091085417403114\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030843495382202997\n",
      "Average test loss: 0.0024993094634264707\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030878350175089306\n",
      "Average test loss: 0.0025275501507437893\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030765104891525376\n",
      "Average test loss: 0.0025509346185459032\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030755221550663313\n",
      "Average test loss: 0.0025612669616109796\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030811248951488072\n",
      "Average test loss: 0.002604554748162627\n",
      "Epoch 152/300\n",
      "Average training loss: 0.030711390448941125\n",
      "Average test loss: 0.002513944495883253\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030651897307899263\n",
      "Average test loss: 0.0025269310395750733\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030577827043003505\n",
      "Average test loss: 0.0025198505143117574\n",
      "Epoch 155/300\n",
      "Average training loss: 0.030561134494013258\n",
      "Average test loss: 0.0026079281895524924\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03055968297521273\n",
      "Average test loss: 0.0025635507750428384\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030497127450174754\n",
      "Average test loss: 0.002498761881763736\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030534571634398565\n",
      "Average test loss: 0.002532198262090484\n",
      "Epoch 160/300\n",
      "Average training loss: 0.030437520100010766\n",
      "Average test loss: 0.0025239578415122295\n",
      "Epoch 161/300\n",
      "Average training loss: 0.030459058458606404\n",
      "Average test loss: 0.0025192493377253415\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030387498789363438\n",
      "Average test loss: 0.002599256181054645\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03038216846022341\n",
      "Average test loss: 0.0025663819809754688\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030315522872739368\n",
      "Average test loss: 0.0026037444211542606\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030310687748922243\n",
      "Average test loss: 0.0025433827632417283\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03031801009674867\n",
      "Average test loss: 0.0025724783877117768\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030285241486297715\n",
      "Average test loss: 0.002605766777363088\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030235427131255467\n",
      "Average test loss: 0.002606608392774231\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030235385201043553\n",
      "Average test loss: 0.002541910803773337\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030146721974015234\n",
      "Average test loss: 0.002603729829709563\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030145810498131646\n",
      "Average test loss: 0.0025506523704777163\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030139977471696007\n",
      "Average test loss: 0.0025847550539506805\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030105769990219012\n",
      "Average test loss: 0.0025356315531664423\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03008788948920038\n",
      "Average test loss: 0.002556491756190856\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030128601120577917\n",
      "Average test loss: 0.0026021038910581007\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03000777931511402\n",
      "Average test loss: 0.002622156626648373\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03001623372733593\n",
      "Average test loss: 0.0025628297726313274\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0300073830154207\n",
      "Average test loss: 0.002588343180095156\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02992875196867519\n",
      "Average test loss: 0.0026076775876184306\n",
      "Epoch 181/300\n",
      "Average training loss: 0.029994663957092497\n",
      "Average test loss: 0.0025858783887492288\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02990581930180391\n",
      "Average test loss: 0.0025924775844646825\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029925039028127987\n",
      "Average test loss: 0.002585469376295805\n",
      "Epoch 184/300\n",
      "Average training loss: 0.029923666202359728\n",
      "Average test loss: 0.0025751927237336832\n",
      "Epoch 185/300\n",
      "Average training loss: 0.029863074264592594\n",
      "Average test loss: 0.00258347570275267\n",
      "Epoch 186/300\n",
      "Average training loss: 0.029848625835445192\n",
      "Average test loss: 0.0026093933868946303\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02981975247959296\n",
      "Average test loss: 0.0025440253979629943\n",
      "Epoch 188/300\n",
      "Average training loss: 0.029761328069700135\n",
      "Average test loss: 0.0025790902943246896\n",
      "Epoch 189/300\n",
      "Average training loss: 0.029842047484384644\n",
      "Average test loss: 0.0025798676759004593\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02973305205007394\n",
      "Average test loss: 0.0026375762607074448\n",
      "Epoch 191/300\n",
      "Average training loss: 0.029749734025862482\n",
      "Average test loss: 0.002587353093549609\n",
      "Epoch 192/300\n",
      "Average training loss: 0.029745137626926103\n",
      "Average test loss: 0.0026118730684328412\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029698440278569858\n",
      "Average test loss: 0.0026234279750949807\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02964896557894018\n",
      "Average test loss: 0.0026129544294542736\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029638222980830405\n",
      "Average test loss: 0.0025993230433927644\n",
      "Epoch 197/300\n",
      "Average training loss: 0.029630445210470095\n",
      "Average test loss: 0.0025814382773306633\n",
      "Epoch 198/300\n",
      "Average training loss: 0.029666820945011246\n",
      "Average test loss: 0.00260035448645552\n",
      "Epoch 199/300\n",
      "Average training loss: 0.029568680880798233\n",
      "Average test loss: 0.0025315342920107973\n",
      "Epoch 200/300\n",
      "Average training loss: 0.029642853202091323\n",
      "Average test loss: 0.002595498986542225\n",
      "Epoch 201/300\n",
      "Average training loss: 0.029580541195140943\n",
      "Average test loss: 0.0025762137696146966\n",
      "Epoch 202/300\n",
      "Average training loss: 0.029564023681812816\n",
      "Average test loss: 0.0026313171581261686\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02948689510756069\n",
      "Average test loss: 0.0026505425564116902\n",
      "Epoch 204/300\n",
      "Average training loss: 0.029503971010446547\n",
      "Average test loss: 0.00265224890379856\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029509849240382514\n",
      "Average test loss: 0.0025474553210660814\n",
      "Epoch 206/300\n",
      "Average training loss: 0.029487829253077507\n",
      "Average test loss: 0.0026137075527674623\n",
      "Epoch 207/300\n",
      "Average training loss: 0.029391522965497442\n",
      "Average test loss: 0.002619388121490677\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029421843613187473\n",
      "Average test loss: 0.0026323425399346484\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02940542480349541\n",
      "Average test loss: 0.0026123044463909335\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029411175282465087\n",
      "Average test loss: 0.0026128975339233876\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029388312892781362\n",
      "Average test loss: 0.0025867903644426\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029357608117991024\n",
      "Average test loss: 0.0026652687521030506\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02937822819749514\n",
      "Average test loss: 0.0026589723067979017\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02930455664296945\n",
      "Average test loss: 0.0026340788594550555\n",
      "Epoch 215/300\n",
      "Average training loss: 0.029292941600084305\n",
      "Average test loss: 0.0026764648273173305\n",
      "Epoch 216/300\n",
      "Average training loss: 0.029305954136782223\n",
      "Average test loss: 0.0026586486219118037\n",
      "Epoch 217/300\n",
      "Average training loss: 0.029313372724586062\n",
      "Average test loss: 0.002592602225434449\n",
      "Epoch 218/300\n",
      "Average training loss: 0.029259947415855195\n",
      "Average test loss: 0.0025723578395942847\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02925598557293415\n",
      "Average test loss: 0.0025922137782391575\n",
      "Epoch 220/300\n",
      "Average training loss: 0.029191840537720257\n",
      "Average test loss: 0.0025675626479917104\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029234078081117738\n",
      "Average test loss: 0.0026716496013104917\n",
      "Epoch 222/300\n",
      "Average training loss: 0.029205437141988012\n",
      "Average test loss: 0.002593426737313469\n",
      "Epoch 223/300\n",
      "Average training loss: 0.029211700013942188\n",
      "Average test loss: 0.00262457788694236\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029141164526343345\n",
      "Average test loss: 0.002622353080660105\n",
      "Epoch 225/300\n",
      "Average training loss: 0.029154671745167837\n",
      "Average test loss: 0.0025685538553322355\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02916664102839099\n",
      "Average test loss: 0.0026212864497469532\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0291649898307191\n",
      "Average test loss: 0.0026485174294147225\n",
      "Epoch 228/300\n",
      "Average training loss: 0.029101767798264823\n",
      "Average test loss: 0.002618963749044471\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02914264978799555\n",
      "Average test loss: 0.002599076829229792\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029090286289652188\n",
      "Average test loss: 0.002598877995999323\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029043855860829353\n",
      "Average test loss: 0.002634323643313514\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029085358889566527\n",
      "Average test loss: 0.00265080017151518\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02902304751839903\n",
      "Average test loss: 0.00259675006609824\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029051993519067763\n",
      "Average test loss: 0.0026036822337450255\n",
      "Epoch 235/300\n",
      "Average training loss: 0.028995147887203428\n",
      "Average test loss: 0.0026577546341965594\n",
      "Epoch 236/300\n",
      "Average training loss: 0.028978019050425952\n",
      "Average test loss: 0.0026862613235910733\n",
      "Epoch 237/300\n",
      "Average training loss: 0.028997966152098445\n",
      "Average test loss: 0.0026470160968601704\n",
      "Epoch 238/300\n",
      "Average training loss: 0.028965334316094715\n",
      "Average test loss: 0.0027700391469730272\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02895226810872555\n",
      "Average test loss: 0.0026280944254249332\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0289432099941704\n",
      "Average test loss: 0.0026572318273699945\n",
      "Epoch 241/300\n",
      "Average training loss: 0.028970748411284553\n",
      "Average test loss: 0.002599471888711883\n",
      "Epoch 242/300\n",
      "Average training loss: 0.028928618606593872\n",
      "Average test loss: 0.0026453495176715983\n",
      "Epoch 243/300\n",
      "Average training loss: 0.028882646469606293\n",
      "Average test loss: 0.0026050201101849475\n",
      "Epoch 244/300\n",
      "Average training loss: 0.028827312620149717\n",
      "Average test loss: 0.002642401452279753\n",
      "Epoch 245/300\n",
      "Average training loss: 0.028870491484800975\n",
      "Average test loss: 0.0025946320777551993\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02884908107171456\n",
      "Average test loss: 0.0026532753820841513\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02885974213646518\n",
      "Average test loss: 0.0026025728933099243\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02885256922079457\n",
      "Average test loss: 0.002654381880847116\n",
      "Epoch 249/300\n",
      "Average training loss: 0.028816098913550376\n",
      "Average test loss: 0.0026383880205038523\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02878603912062115\n",
      "Average test loss: 0.0026745634439090888\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02883590866625309\n",
      "Average test loss: 0.0025820477925654916\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02874418334662914\n",
      "Average test loss: 0.0026412033043387863\n",
      "Epoch 253/300\n",
      "Average training loss: 0.028781324350171618\n",
      "Average test loss: 0.002627093122444219\n",
      "Epoch 254/300\n",
      "Average training loss: 0.028792684432533053\n",
      "Average test loss: 0.002649501060239143\n",
      "Epoch 255/300\n",
      "Average training loss: 0.028723076093528005\n",
      "Average test loss: 0.002690621237994896\n",
      "Epoch 256/300\n",
      "Average training loss: 0.028725151360034944\n",
      "Average test loss: 0.0026216647438704966\n",
      "Epoch 257/300\n",
      "Average training loss: 0.028700230040484005\n",
      "Average test loss: 0.002629252197013961\n",
      "Epoch 258/300\n",
      "Average training loss: 0.028756854302353328\n",
      "Average test loss: 0.0026097405501123933\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02867583365076118\n",
      "Average test loss: 0.002669131990108225\n",
      "Epoch 260/300\n",
      "Average training loss: 0.028694037357966107\n",
      "Average test loss: 0.002655104704408182\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02866984456280867\n",
      "Average test loss: 0.0026684896329210862\n",
      "Epoch 262/300\n",
      "Average training loss: 0.028669420580069223\n",
      "Average test loss: 0.0026434226534846755\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0286338938275973\n",
      "Average test loss: 0.0026371970182905596\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02866464980112182\n",
      "Average test loss: 0.002616140203550458\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02859237555000517\n",
      "Average test loss: 0.0026032867944902846\n",
      "Epoch 267/300\n",
      "Average training loss: 0.028625787599219216\n",
      "Average test loss: 0.0026707355361431837\n",
      "Epoch 268/300\n",
      "Average training loss: 0.028698753368523386\n",
      "Average test loss: 0.0026162176496452756\n",
      "Epoch 269/300\n",
      "Average training loss: 0.028622213624417782\n",
      "Average test loss: 0.0026546451890220243\n",
      "Epoch 270/300\n",
      "Average training loss: 0.028579883067144287\n",
      "Average test loss: 0.002695675722219878\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02853434285852644\n",
      "Average test loss: 0.002640333417389128\n",
      "Epoch 272/300\n",
      "Average training loss: 0.028508364025089477\n",
      "Average test loss: 0.0026442599677377276\n",
      "Epoch 273/300\n",
      "Average training loss: 0.028515650560458503\n",
      "Average test loss: 0.002643500479973025\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02854637065033118\n",
      "Average test loss: 0.002650591486857997\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02848284025821421\n",
      "Average test loss: 0.002654551279834575\n",
      "Epoch 277/300\n",
      "Average training loss: 0.028469713273975586\n",
      "Average test loss: 0.0026632633958425787\n",
      "Epoch 278/300\n",
      "Average training loss: 0.028491807578338517\n",
      "Average test loss: 0.002681408547071947\n",
      "Epoch 279/300\n",
      "Average training loss: 0.028479496479034423\n",
      "Average test loss: 0.00264974612225261\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02844701531694995\n",
      "Average test loss: 0.002666117974039581\n",
      "Epoch 281/300\n",
      "Average training loss: 0.028419995749990146\n",
      "Average test loss: 0.0026274795902685986\n",
      "Epoch 282/300\n",
      "Average training loss: 0.028421653234296376\n",
      "Average test loss: 0.002638898687230216\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02843334187567234\n",
      "Average test loss: 0.0026076029514273008\n",
      "Epoch 284/300\n",
      "Average training loss: 0.028398799631330702\n",
      "Average test loss: 0.0026828108767254486\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02841840371158388\n",
      "Average test loss: 0.0026202793680131435\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02839525172776646\n",
      "Average test loss: 0.0026831270601186488\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02838455288608869\n",
      "Average test loss: 0.0026362472161029775\n",
      "Epoch 288/300\n",
      "Average training loss: 0.028364680517050954\n",
      "Average test loss: 0.002685632923307518\n",
      "Epoch 289/300\n",
      "Average training loss: 0.028364164665341376\n",
      "Average test loss: 0.0026565572215865056\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02836299218899674\n",
      "Average test loss: 0.0026559238736000327\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02835415084163348\n",
      "Average test loss: 0.002707217905256483\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02831507071521547\n",
      "Average test loss: 0.002608920505684283\n",
      "Epoch 294/300\n",
      "Average training loss: 0.028342812892463473\n",
      "Average test loss: 0.002646827982635134\n",
      "Epoch 295/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_80_Depth10/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88d0e5-0d9e-4bca-843f-af4fc42dac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c0213-f705-469e-8591-ebb3c6c1280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_80_Depth10/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fff5f8-b924-4454-bcfb-61b2d5b924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
