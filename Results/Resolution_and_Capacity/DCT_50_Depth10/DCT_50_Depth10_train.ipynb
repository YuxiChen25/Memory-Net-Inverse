{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11474574074149131\n",
      "Average test loss: 0.004948930737045076\n",
      "Epoch 2/300\n",
      "Average training loss: 0.029081824431816738\n",
      "Average test loss: 0.004740047305408451\n",
      "Epoch 3/300\n",
      "Average training loss: 0.025111968035499255\n",
      "Average test loss: 0.004566040409108003\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02370139875842465\n",
      "Average test loss: 0.004366567508627971\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022978223396672142\n",
      "Average test loss: 0.004329186004069116\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022528026926848622\n",
      "Average test loss: 0.00427273164027267\n",
      "Epoch 7/300\n",
      "Average training loss: 0.022205803745322758\n",
      "Average test loss: 0.004250242801176177\n",
      "Epoch 8/300\n",
      "Average training loss: 0.021946196383900114\n",
      "Average test loss: 0.004218114578889476\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021740217453903622\n",
      "Average test loss: 0.00422006281485988\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02156093107495043\n",
      "Average test loss: 0.0041747085762520635\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021405313073760934\n",
      "Average test loss: 0.004157326908989085\n",
      "Epoch 12/300\n",
      "Average training loss: 0.021263089552521704\n",
      "Average test loss: 0.004126063481387165\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021139645589722526\n",
      "Average test loss: 0.004124668919377857\n",
      "Epoch 14/300\n",
      "Average training loss: 0.021018513234125243\n",
      "Average test loss: 0.0041079916689130995\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020902319931321675\n",
      "Average test loss: 0.004081960776406858\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020778277039527893\n",
      "Average test loss: 0.0041349516635139786\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02067786578171783\n",
      "Average test loss: 0.004081684517777628\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02058139578666952\n",
      "Average test loss: 0.0040387474675145415\n",
      "Epoch 19/300\n",
      "Average training loss: 0.020483097409208617\n",
      "Average test loss: 0.004051650914053122\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02038160764508777\n",
      "Average test loss: 0.0040216684941616326\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020304295582903756\n",
      "Average test loss: 0.004015601472101278\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02021122581263383\n",
      "Average test loss: 0.004009598465015491\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020144960772660044\n",
      "Average test loss: 0.004000763412151072\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02006341992980904\n",
      "Average test loss: 0.0039719158584872885\n",
      "Epoch 25/300\n",
      "Average training loss: 0.019996869484583537\n",
      "Average test loss: 0.003966450651693675\n",
      "Epoch 26/300\n",
      "Average training loss: 0.019933061480522155\n",
      "Average test loss: 0.003960901179557873\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019876626311077013\n",
      "Average test loss: 0.0039653073158115145\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019817471217777993\n",
      "Average test loss: 0.0039738476444035765\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019755717928210893\n",
      "Average test loss: 0.0039822434708476065\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019703448191285132\n",
      "Average test loss: 0.003950726560834381\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01964614792664846\n",
      "Average test loss: 0.003936051640659571\n",
      "Epoch 32/300\n",
      "Average training loss: 0.019584116119477486\n",
      "Average test loss: 0.003925333188639747\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019555780462920665\n",
      "Average test loss: 0.003933401257420579\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019492055327528054\n",
      "Average test loss: 0.003940024616196752\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019451300110254022\n",
      "Average test loss: 0.00393717770692375\n",
      "Epoch 36/300\n",
      "Average training loss: 0.019410915775431525\n",
      "Average test loss: 0.003932286766875122\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0193506804448035\n",
      "Average test loss: 0.00395257713790569\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019309471199909847\n",
      "Average test loss: 0.003922172507892052\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019273086628980106\n",
      "Average test loss: 0.0039167099661297266\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019231237954563565\n",
      "Average test loss: 0.003916327425382203\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019186914270122847\n",
      "Average test loss: 0.003976121019572019\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01914502646939622\n",
      "Average test loss: 0.003928453559676806\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01910502389901214\n",
      "Average test loss: 0.003919787988066673\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019049887095888455\n",
      "Average test loss: 0.003949236595382293\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019005112934443687\n",
      "Average test loss: 0.003923158878667487\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01897807000743018\n",
      "Average test loss: 0.003924111819929547\n",
      "Epoch 47/300\n",
      "Average training loss: 0.018915611561801698\n",
      "Average test loss: 0.003939865715387795\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018879322098361122\n",
      "Average test loss: 0.003992010548296902\n",
      "Epoch 49/300\n",
      "Average training loss: 0.018859665802783435\n",
      "Average test loss: 0.0039377745389938355\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018794311289158133\n",
      "Average test loss: 0.003935676062272655\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01876345900694529\n",
      "Average test loss: 0.003926747529870934\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018738244642814\n",
      "Average test loss: 0.003924652972155147\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01867073766224914\n",
      "Average test loss: 0.003968962600661648\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018627687785360548\n",
      "Average test loss: 0.003940937767012252\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018595926836133002\n",
      "Average test loss: 0.003925586639179124\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018552723875476254\n",
      "Average test loss: 0.003970401687547564\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01852584132303794\n",
      "Average test loss: 0.003937223313583268\n",
      "Epoch 58/300\n",
      "Average training loss: 0.018461523233188525\n",
      "Average test loss: 0.00397597277826733\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018424838768111334\n",
      "Average test loss: 0.0039971324648294185\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018382861668864887\n",
      "Average test loss: 0.0039783289991319176\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01835517970720927\n",
      "Average test loss: 0.004024136245250702\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018318615721331703\n",
      "Average test loss: 0.003951220487347908\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018281109402577083\n",
      "Average test loss: 0.004005010640869538\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01823272724284066\n",
      "Average test loss: 0.003992867184182008\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018204175354705917\n",
      "Average test loss: 0.003963379435655143\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018162031615773837\n",
      "Average test loss: 0.003969203256898456\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018113634395930504\n",
      "Average test loss: 0.00409124602129062\n",
      "Epoch 68/300\n",
      "Average training loss: 0.018107793203658528\n",
      "Average test loss: 0.004008703165584141\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018063381827539866\n",
      "Average test loss: 0.004138591697232591\n",
      "Epoch 70/300\n",
      "Average training loss: 0.018013698829544914\n",
      "Average test loss: 0.0039801822797291804\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017987827572557662\n",
      "Average test loss: 0.00400534993244542\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017949262529611588\n",
      "Average test loss: 0.004002010728749964\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01791296570499738\n",
      "Average test loss: 0.004027393173012469\n",
      "Epoch 74/300\n",
      "Average training loss: 0.017889615723656282\n",
      "Average test loss: 0.004018419604334566\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017840459146433407\n",
      "Average test loss: 0.0039802249744534494\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017817325259248416\n",
      "Average test loss: 0.004002861220389605\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01779294651084476\n",
      "Average test loss: 0.004119276640522811\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01773714024325212\n",
      "Average test loss: 0.004071844835248258\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017712894009219275\n",
      "Average test loss: 0.004028330004877514\n",
      "Epoch 80/300\n",
      "Average training loss: 0.017674699453843964\n",
      "Average test loss: 0.004036090228499638\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017657747888730632\n",
      "Average test loss: 0.004045037765469816\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017628470672501458\n",
      "Average test loss: 0.004031681873732143\n",
      "Epoch 83/300\n",
      "Average training loss: 0.017603590211934514\n",
      "Average test loss: 0.004002156067225668\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017578059698972436\n",
      "Average test loss: 0.004076947858764066\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017558380401796764\n",
      "Average test loss: 0.00405385380031334\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017517370911108124\n",
      "Average test loss: 0.003997347354681956\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017482372344368034\n",
      "Average test loss: 0.004063647621621688\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017456256146232287\n",
      "Average test loss: 0.00412104528852635\n",
      "Epoch 89/300\n",
      "Average training loss: 0.017412211464511024\n",
      "Average test loss: 0.004017297544413143\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017402470774120753\n",
      "Average test loss: 0.00413216397476693\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01737460777329074\n",
      "Average test loss: 0.004149744680151344\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017339697387483383\n",
      "Average test loss: 0.004048818237044745\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017326273342801464\n",
      "Average test loss: 0.004047757171094417\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017299386681781874\n",
      "Average test loss: 0.004072604635109504\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017272110091315376\n",
      "Average test loss: 0.004025109411941634\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01724699543747637\n",
      "Average test loss: 0.004045157954510715\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017216204664773412\n",
      "Average test loss: 0.004114897340950038\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01719361675116751\n",
      "Average test loss: 0.0040906346196101775\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017175888643496565\n",
      "Average test loss: 0.004078027991371022\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017168093275692728\n",
      "Average test loss: 0.004122600234217114\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017125589956839878\n",
      "Average test loss: 0.0041223454591300755\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017116108900970883\n",
      "Average test loss: 0.004139136516178647\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017079497286015086\n",
      "Average test loss: 0.004058025628328323\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017055346801877022\n",
      "Average test loss: 0.004157980121672154\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01703634107692374\n",
      "Average test loss: 0.0041343189647628205\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017008873737520643\n",
      "Average test loss: 0.004245601067526473\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017016909514864287\n",
      "Average test loss: 0.004149142175498936\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016979527038004663\n",
      "Average test loss: 0.004078783441334963\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01694917478164037\n",
      "Average test loss: 0.004084121491760015\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016922515662180053\n",
      "Average test loss: 0.004273254418538677\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01689249471988943\n",
      "Average test loss: 0.004258103975405296\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016891764941314856\n",
      "Average test loss: 0.00409305956463019\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016860729831788275\n",
      "Average test loss: 0.004181292698201206\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016838500829206574\n",
      "Average test loss: 0.004084621836741765\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01684244247029225\n",
      "Average test loss: 0.004191126123484638\n",
      "Epoch 116/300\n",
      "Average training loss: 0.016807496953341695\n",
      "Average test loss: 0.004183832826713721\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01679215551084942\n",
      "Average test loss: 0.004190075299185183\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016783610731363297\n",
      "Average test loss: 0.00407518709409568\n",
      "Epoch 119/300\n",
      "Average training loss: 0.016749926400681337\n",
      "Average test loss: 0.004113154040649533\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016747577184604272\n",
      "Average test loss: 0.004194931328917543\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01672880922920174\n",
      "Average test loss: 0.004153517654372586\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016689308930602338\n",
      "Average test loss: 0.004279298600637251\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016697671992911233\n",
      "Average test loss: 0.004059903832773368\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01666741258237097\n",
      "Average test loss: 0.004137166805565357\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016649033750096957\n",
      "Average test loss: 0.00418731712239484\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01662025827417771\n",
      "Average test loss: 0.004251659187177817\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01662498852858941\n",
      "Average test loss: 0.004204446016914314\n",
      "Epoch 128/300\n",
      "Average training loss: 0.016611182598604095\n",
      "Average test loss: 0.004284081964857049\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01659334772080183\n",
      "Average test loss: 0.004261581574049261\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016560872744354936\n",
      "Average test loss: 0.00415377779222197\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016545281935069295\n",
      "Average test loss: 0.004210513137694862\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016537102277080218\n",
      "Average test loss: 0.004118657479269637\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01652385145260228\n",
      "Average test loss: 0.004231636288679308\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01649662685808208\n",
      "Average test loss: 0.004214614911004901\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016497556120157242\n",
      "Average test loss: 0.004211497401197751\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016475548846854104\n",
      "Average test loss: 0.0041496593376828565\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016469949382874702\n",
      "Average test loss: 0.00409136092538635\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016424802674187555\n",
      "Average test loss: 0.004249149961604012\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01643728240413798\n",
      "Average test loss: 0.004203571227068702\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016411677906910577\n",
      "Average test loss: 0.004143305912199947\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016397267333335346\n",
      "Average test loss: 0.004248708347893424\n",
      "Epoch 142/300\n",
      "Average training loss: 0.016380379769537184\n",
      "Average test loss: 0.004206192120288809\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01635407675637139\n",
      "Average test loss: 0.0041055800759543975\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016354618964923753\n",
      "Average test loss: 0.004204061117644112\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016336296419302624\n",
      "Average test loss: 0.0041266006943252356\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01631763252698713\n",
      "Average test loss: 0.004270768966525793\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016323299296200276\n",
      "Average test loss: 0.004169245611462328\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016304297898378638\n",
      "Average test loss: 0.004232798412856129\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01629423531062073\n",
      "Average test loss: 0.004346701317363315\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01627995701216989\n",
      "Average test loss: 0.004224079631682899\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016258913830750517\n",
      "Average test loss: 0.004230758608422346\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016237019094328085\n",
      "Average test loss: 0.004325409773323271\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016247272334165043\n",
      "Average test loss: 0.004191643454548385\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016228465372489558\n",
      "Average test loss: 0.004161242437031534\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0162235738866859\n",
      "Average test loss: 0.0041930092061973285\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016197471294966008\n",
      "Average test loss: 0.004193046032968495\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01617934411764145\n",
      "Average test loss: 0.004355241139729818\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01616180534329679\n",
      "Average test loss: 0.004382695416195525\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01616637112448613\n",
      "Average test loss: 0.00429145817955335\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016154854921831024\n",
      "Average test loss: 0.004122824661019776\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01613899356292354\n",
      "Average test loss: 0.004280007907499869\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016125329047441482\n",
      "Average test loss: 0.004279898117399878\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01613076683961683\n",
      "Average test loss: 0.004247832129812903\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016107499707076284\n",
      "Average test loss: 0.004234898122234477\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01609509086029397\n",
      "Average test loss: 0.004245109632818236\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016072712289790313\n",
      "Average test loss: 0.0042839086109565366\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016090441207091015\n",
      "Average test loss: 0.004186594247197112\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01605909768657552\n",
      "Average test loss: 0.004382838294323948\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01605368157227834\n",
      "Average test loss: 0.004266498669981957\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016035970820320978\n",
      "Average test loss: 0.00418592947969834\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016047992147505283\n",
      "Average test loss: 0.004320661786943674\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016012872584991986\n",
      "Average test loss: 0.004238145393215948\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015996816665762\n",
      "Average test loss: 0.004212479684915807\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015993621370858618\n",
      "Average test loss: 0.004246969518148237\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015982706101404297\n",
      "Average test loss: 0.004314956349838111\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015994876310229302\n",
      "Average test loss: 0.004259015598023931\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01597454572882917\n",
      "Average test loss: 0.004302257013403707\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01595341213626994\n",
      "Average test loss: 0.004184027795783348\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01595921713113785\n",
      "Average test loss: 0.004288400054805809\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015944668877455923\n",
      "Average test loss: 0.0043133120317426\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015927412519024478\n",
      "Average test loss: 0.004246260358641545\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01591921288354529\n",
      "Average test loss: 0.00429558689519763\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015915565555294355\n",
      "Average test loss: 0.004241592208958335\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015891110396219623\n",
      "Average test loss: 0.004237096355193191\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015877046603295537\n",
      "Average test loss: 0.004185432250301043\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015900483950144716\n",
      "Average test loss: 0.004175992044102814\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01586314948813783\n",
      "Average test loss: 0.0043022049872411625\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015858584740095667\n",
      "Average test loss: 0.004309630702353186\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01585889498889446\n",
      "Average test loss: 0.004427085484067599\n",
      "Epoch 190/300\n",
      "Average training loss: 0.015848664994041126\n",
      "Average test loss: 0.004247115743449993\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015855772604544956\n",
      "Average test loss: 0.004216380918191539\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015818159775601495\n",
      "Average test loss: 0.004239549026307132\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01581359955254528\n",
      "Average test loss: 0.004194606533894936\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01580369703968366\n",
      "Average test loss: 0.004389236608727111\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015805957714716594\n",
      "Average test loss: 0.004172819024158849\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015776787655221093\n",
      "Average test loss: 0.0042595144742065\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01577591973543167\n",
      "Average test loss: 0.004247751915620433\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015765243951645164\n",
      "Average test loss: 0.004428484000576039\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01576345522287819\n",
      "Average test loss: 0.004289701166459256\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015754048648807736\n",
      "Average test loss: 0.004241452539960543\n",
      "Epoch 201/300\n",
      "Average training loss: 0.015740800662173166\n",
      "Average test loss: 0.004365974150184128\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01575294036666552\n",
      "Average test loss: 0.004114264921181732\n",
      "Epoch 203/300\n",
      "Average training loss: 0.015733061586817105\n",
      "Average test loss: 0.004250546810113722\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015713828747471174\n",
      "Average test loss: 0.004291036270144913\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015709437581400077\n",
      "Average test loss: 0.004336452485993504\n",
      "Epoch 206/300\n",
      "Average training loss: 0.015709885070721307\n",
      "Average test loss: 0.004403227623759044\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015687927901744843\n",
      "Average test loss: 0.004244951518045531\n",
      "Epoch 208/300\n",
      "Average training loss: 0.015679127864539625\n",
      "Average test loss: 0.00434168812715345\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01568168466207054\n",
      "Average test loss: 0.004355856066983607\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01566597517248657\n",
      "Average test loss: 0.004213958843093779\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015670172873470518\n",
      "Average test loss: 0.004298637267202139\n",
      "Epoch 212/300\n",
      "Average training loss: 0.015647574285666148\n",
      "Average test loss: 0.0042191773292919\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0156557384605209\n",
      "Average test loss: 0.004221573327564531\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01562277398010095\n",
      "Average test loss: 0.004430816591406862\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015629110934833684\n",
      "Average test loss: 0.004198868193767137\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015629617103272014\n",
      "Average test loss: 0.004215318440149228\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015617695928447776\n",
      "Average test loss: 0.004423538463397159\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015621425074007777\n",
      "Average test loss: 0.0042326985009842446\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015601241531471412\n",
      "Average test loss: 0.004579406475855244\n",
      "Epoch 220/300\n",
      "Average training loss: 0.015590567756030295\n",
      "Average test loss: 0.004330704532356726\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01559869985944695\n",
      "Average test loss: 0.0042477339675856965\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01558239095740848\n",
      "Average test loss: 0.004200346027811368\n",
      "Epoch 223/300\n",
      "Average training loss: 0.015566543921000427\n",
      "Average test loss: 0.0045512139420542455\n",
      "Epoch 224/300\n",
      "Average training loss: 0.015565896304117309\n",
      "Average test loss: 0.004434396663680672\n",
      "Epoch 225/300\n",
      "Average training loss: 0.015560249196158515\n",
      "Average test loss: 0.0043489397408233745\n",
      "Epoch 226/300\n",
      "Average training loss: 0.015548078952564133\n",
      "Average test loss: 0.004315948993381526\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015539200372993946\n",
      "Average test loss: 0.004500913897736205\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015525733138124147\n",
      "Average test loss: 0.004283880034668578\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015535211385952102\n",
      "Average test loss: 0.004208802768339713\n",
      "Epoch 230/300\n",
      "Average training loss: 0.015527918761803044\n",
      "Average test loss: 0.004351417757363783\n",
      "Epoch 231/300\n",
      "Average training loss: 0.015522327671448389\n",
      "Average test loss: 0.004471038848161697\n",
      "Epoch 232/300\n",
      "Average training loss: 0.015520604144367907\n",
      "Average test loss: 0.004403815456148651\n",
      "Epoch 233/300\n",
      "Average training loss: 0.015516205653548241\n",
      "Average test loss: 0.004449553412902686\n",
      "Epoch 234/300\n",
      "Average training loss: 0.015498039145436552\n",
      "Average test loss: 0.004413187781141864\n",
      "Epoch 235/300\n",
      "Average training loss: 0.015493471534715758\n",
      "Average test loss: 0.004208537608799007\n",
      "Epoch 236/300\n",
      "Average training loss: 0.015482265830039978\n",
      "Average test loss: 0.0043088679562012355\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015482646293110318\n",
      "Average test loss: 0.004310590486559603\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015458999454975128\n",
      "Average test loss: 0.004267488738728894\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01545763986888859\n",
      "Average test loss: 0.004280244348777665\n",
      "Epoch 240/300\n",
      "Average training loss: 0.015473631709814072\n",
      "Average test loss: 0.004346240270055003\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015461067027515835\n",
      "Average test loss: 0.004448029341590073\n",
      "Epoch 242/300\n",
      "Average training loss: 0.015437425722678503\n",
      "Average test loss: 0.004308716968529754\n",
      "Epoch 243/300\n",
      "Average training loss: 0.015438046842813491\n",
      "Average test loss: 0.004309907734807995\n",
      "Epoch 244/300\n",
      "Average training loss: 0.015432839784357283\n",
      "Average test loss: 0.004282027544660701\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01540579620997111\n",
      "Average test loss: 0.004235351695782609\n",
      "Epoch 246/300\n",
      "Average training loss: 0.015415969784061114\n",
      "Average test loss: 0.004270688717770908\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015423756312164996\n",
      "Average test loss: 0.004313812469028764\n",
      "Epoch 248/300\n",
      "Average training loss: 0.015402700427505705\n",
      "Average test loss: 0.004263273820281029\n",
      "Epoch 249/300\n",
      "Average training loss: 0.015402543742623594\n",
      "Average test loss: 0.004282011723766724\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01539262140625053\n",
      "Average test loss: 0.004354859431791636\n",
      "Epoch 251/300\n",
      "Average training loss: 0.015385017854472001\n",
      "Average test loss: 0.004382222548541095\n",
      "Epoch 252/300\n",
      "Average training loss: 0.015397087315718334\n",
      "Average test loss: 0.00431663964357641\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01537050657802158\n",
      "Average test loss: 0.00422923137827052\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015370786392026478\n",
      "Average test loss: 0.004318593621460928\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015366879659394423\n",
      "Average test loss: 0.004378776316324042\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015354358660678068\n",
      "Average test loss: 0.0043455167334112855\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015345724058647951\n",
      "Average test loss: 0.004479127829273542\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015359358307388094\n",
      "Average test loss: 0.0045096205398440364\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015358349004553424\n",
      "Average test loss: 0.004313357372043861\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015342697154316636\n",
      "Average test loss: 0.004353225925730334\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015341191565824879\n",
      "Average test loss: 0.004525782877993253\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015324314162962966\n",
      "Average test loss: 0.004463524474038018\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015309379356602829\n",
      "Average test loss: 0.004424198464386993\n",
      "Epoch 264/300\n",
      "Average training loss: 0.015325024760431714\n",
      "Average test loss: 0.0045056371432211665\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015298451051529912\n",
      "Average test loss: 0.004508114115645488\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015307337462074227\n",
      "Average test loss: 0.0042978300183183615\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015301461559202936\n",
      "Average test loss: 0.004236800179713302\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015284395606153542\n",
      "Average test loss: 0.00432078146810333\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015282162461843756\n",
      "Average test loss: 0.004341305579369267\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01528344936006599\n",
      "Average test loss: 0.004259828819168939\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015253091039756934\n",
      "Average test loss: 0.00431159905778865\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015274792835116386\n",
      "Average test loss: 0.004411099430587557\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01527540922164917\n",
      "Average test loss: 0.00433035672083497\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015265311407546202\n",
      "Average test loss: 0.004289387054741383\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01524523331804408\n",
      "Average test loss: 0.004346682331628269\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015237969400568142\n",
      "Average test loss: 0.0043879627991053795\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015231752526428964\n",
      "Average test loss: 0.00431777996942401\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01524954636560546\n",
      "Average test loss: 0.004323585935557882\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015215811727775468\n",
      "Average test loss: 0.004271333301646842\n",
      "Epoch 280/300\n",
      "Average training loss: 0.015232816548811064\n",
      "Average test loss: 0.004395273739265071\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015220213690565691\n",
      "Average test loss: 0.004429335206333134\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015214358454777135\n",
      "Average test loss: 0.004356755859115057\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015212113441692458\n",
      "Average test loss: 0.004348858284867472\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015194672620958753\n",
      "Average test loss: 0.0043505443102783625\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01520836202469137\n",
      "Average test loss: 0.004341494490702947\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015188701142867406\n",
      "Average test loss: 0.004354644326493144\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015201559454202653\n",
      "Average test loss: 0.004329062629905012\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015175875688592593\n",
      "Average test loss: 0.00441197603671915\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01517390895800458\n",
      "Average test loss: 0.004366578590538767\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015172161949177584\n",
      "Average test loss: 0.004472464039921761\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015178814681867759\n",
      "Average test loss: 0.004450820012225045\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015155361430512534\n",
      "Average test loss: 0.004434878300875425\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015172085120446151\n",
      "Average test loss: 0.004365771099097199\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015153178869850105\n",
      "Average test loss: 0.004246683573971192\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015153420440852642\n",
      "Average test loss: 0.004367647556381093\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015150744770136145\n",
      "Average test loss: 0.004427730074773232\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015138345464236206\n",
      "Average test loss: 0.004285828194270531\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015149690179361238\n",
      "Average test loss: 0.0043984707527690466\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01513609818700287\n",
      "Average test loss: 0.004367309911383523\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01513544927785794\n",
      "Average test loss: 0.004331093895973431\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11089172870251868\n",
      "Average test loss: 0.00475693564530876\n",
      "Epoch 2/300\n",
      "Average training loss: 0.026537705476913188\n",
      "Average test loss: 0.004111942984991603\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02252017737428347\n",
      "Average test loss: 0.004079284191959434\n",
      "Epoch 4/300\n",
      "Average training loss: 0.021042853264345063\n",
      "Average test loss: 0.0037942564549545447\n",
      "Epoch 5/300\n",
      "Average training loss: 0.020277833346691396\n",
      "Average test loss: 0.0038243390512135295\n",
      "Epoch 6/300\n",
      "Average training loss: 0.019789205637243058\n",
      "Average test loss: 0.0036360094050566357\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019412253691090477\n",
      "Average test loss: 0.003622266388601727\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01912268607815107\n",
      "Average test loss: 0.003542676796929704\n",
      "Epoch 9/300\n",
      "Average training loss: 0.018858094832963412\n",
      "Average test loss: 0.0035159369400805896\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01862670654306809\n",
      "Average test loss: 0.003491103447559807\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01840031839079327\n",
      "Average test loss: 0.0034046224877238274\n",
      "Epoch 12/300\n",
      "Average training loss: 0.018209331378340722\n",
      "Average test loss: 0.003359994892237915\n",
      "Epoch 13/300\n",
      "Average training loss: 0.018000100998414887\n",
      "Average test loss: 0.0033194412442131174\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01782517006082667\n",
      "Average test loss: 0.0033546589352190493\n",
      "Epoch 15/300\n",
      "Average training loss: 0.017633555703692967\n",
      "Average test loss: 0.003293465855013993\n",
      "Epoch 16/300\n",
      "Average training loss: 0.017447732186979717\n",
      "Average test loss: 0.003237973474793964\n",
      "Epoch 17/300\n",
      "Average training loss: 0.017276677024861176\n",
      "Average test loss: 0.003233988364537557\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017105768283622133\n",
      "Average test loss: 0.0033325338599582512\n",
      "Epoch 19/300\n",
      "Average training loss: 0.016950104335943857\n",
      "Average test loss: 0.003227501537029942\n",
      "Epoch 20/300\n",
      "Average training loss: 0.016779389979938667\n",
      "Average test loss: 0.0031507548318348\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016626944532824886\n",
      "Average test loss: 0.0031590439381284845\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016481474873920283\n",
      "Average test loss: 0.0030944734240571656\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0163476927710904\n",
      "Average test loss: 0.003120099643038379\n",
      "Epoch 24/300\n",
      "Average training loss: 0.016201223629216353\n",
      "Average test loss: 0.0031014959745936923\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016073153961863783\n",
      "Average test loss: 0.003050351548319062\n",
      "Epoch 26/300\n",
      "Average training loss: 0.015952314044038456\n",
      "Average test loss: 0.0030393906744817893\n",
      "Epoch 27/300\n",
      "Average training loss: 0.015832615629666383\n",
      "Average test loss: 0.00303296590430869\n",
      "Epoch 28/300\n",
      "Average training loss: 0.015716672025620937\n",
      "Average test loss: 0.003017584044072363\n",
      "Epoch 29/300\n",
      "Average training loss: 0.015617908496823575\n",
      "Average test loss: 0.0030322792316890427\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015509460471570492\n",
      "Average test loss: 0.0029942095366617045\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0154087409923474\n",
      "Average test loss: 0.0029780349890804955\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015320934067997668\n",
      "Average test loss: 0.003026735170640879\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015240117450555165\n",
      "Average test loss: 0.003023944019443459\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015145441016389264\n",
      "Average test loss: 0.0029765447626511257\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01507475765960084\n",
      "Average test loss: 0.002945752000436187\n",
      "Epoch 36/300\n",
      "Average training loss: 0.014975880979663797\n",
      "Average test loss: 0.002952295707124803\n",
      "Epoch 37/300\n",
      "Average training loss: 0.014902437683608797\n",
      "Average test loss: 0.003034833763622575\n",
      "Epoch 38/300\n",
      "Average training loss: 0.014826029111113814\n",
      "Average test loss: 0.003013975691671173\n",
      "Epoch 39/300\n",
      "Average training loss: 0.014759359863069322\n",
      "Average test loss: 0.002933208249716295\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0146568084574408\n",
      "Average test loss: 0.0029857625650862854\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01459985292951266\n",
      "Average test loss: 0.0030525428750034834\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014527244158089161\n",
      "Average test loss: 0.002946847209913863\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01447371778719955\n",
      "Average test loss: 0.0029513393472880125\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014395833132167657\n",
      "Average test loss: 0.002959453530816568\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014327728112538655\n",
      "Average test loss: 0.003027373910157217\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014267644168602095\n",
      "Average test loss: 0.0030650679180398584\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014212906354831325\n",
      "Average test loss: 0.0030091552904082668\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014139045846958955\n",
      "Average test loss: 0.003028434694434206\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01406825924043854\n",
      "Average test loss: 0.0030470028101570075\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014021443471312523\n",
      "Average test loss: 0.0031247641967816486\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013949250797430673\n",
      "Average test loss: 0.0029533585059560007\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013890173094968001\n",
      "Average test loss: 0.003002729051332507\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013849762064715226\n",
      "Average test loss: 0.003049790227164825\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013779226076271798\n",
      "Average test loss: 0.0029517700266506936\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01374821781201495\n",
      "Average test loss: 0.002930902302886049\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01367833277500338\n",
      "Average test loss: 0.0029752323544687694\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0136243269170324\n",
      "Average test loss: 0.002979441759693954\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01357515442950858\n",
      "Average test loss: 0.0031406341141296755\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013557276777095265\n",
      "Average test loss: 0.003038812867469258\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013476623416360882\n",
      "Average test loss: 0.003045182989289363\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013424502490295304\n",
      "Average test loss: 0.002991760286192099\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01342049435691701\n",
      "Average test loss: 0.0031063222574690976\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013341822176343865\n",
      "Average test loss: 0.002998006313625309\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01331415485839049\n",
      "Average test loss: 0.00300939212159978\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01326065890242656\n",
      "Average test loss: 0.0029921733542448946\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013225515921082762\n",
      "Average test loss: 0.003020287546846602\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013168132132954067\n",
      "Average test loss: 0.0030586814851396612\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013152382691701252\n",
      "Average test loss: 0.0030577055722888972\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013098454451395406\n",
      "Average test loss: 0.002990280224217309\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01307352645529641\n",
      "Average test loss: 0.0031348017352736657\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013032956653998957\n",
      "Average test loss: 0.0030612072555555236\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013018196387423409\n",
      "Average test loss: 0.003253271594022711\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012979375892215305\n",
      "Average test loss: 0.0032727997762461505\n",
      "Epoch 74/300\n",
      "Average training loss: 0.012927695273525185\n",
      "Average test loss: 0.0030363228056165905\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012917801113592253\n",
      "Average test loss: 0.0030902553108624288\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012876052874657843\n",
      "Average test loss: 0.0030474366512563494\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012834194301731057\n",
      "Average test loss: 0.0030980139279531108\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01278897659563356\n",
      "Average test loss: 0.00307844810684522\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012786034169296424\n",
      "Average test loss: 0.0031508612719674907\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012760033412939973\n",
      "Average test loss: 0.003092908286386066\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012733278445071643\n",
      "Average test loss: 0.003212081982029809\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012677305846578545\n",
      "Average test loss: 0.0032044152071078616\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01267159407834212\n",
      "Average test loss: 0.0030383120017747085\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012659598968923092\n",
      "Average test loss: 0.0031383405495435\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012614133251623975\n",
      "Average test loss: 0.0031350115285151536\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012633762997885545\n",
      "Average test loss: 0.003047237148301469\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012579571140309175\n",
      "Average test loss: 0.0030697363530182175\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012540267470810149\n",
      "Average test loss: 0.0030301883419354757\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012536927969091468\n",
      "Average test loss: 0.0032705032181822593\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01250466444508897\n",
      "Average test loss: 0.003279329570838147\n",
      "Epoch 91/300\n",
      "Average training loss: 0.012480426688989004\n",
      "Average test loss: 0.0030905046173267895\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012449139848351479\n",
      "Average test loss: 0.003123817471373412\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012437506155835257\n",
      "Average test loss: 0.0030815036731461683\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012391648401402765\n",
      "Average test loss: 0.0033162965333710112\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01238629112061527\n",
      "Average test loss: 0.0030890464232199723\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012362508243156803\n",
      "Average test loss: 0.003164896113694542\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01235456283059385\n",
      "Average test loss: 0.0031693727403051325\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012321855418384075\n",
      "Average test loss: 0.0030421101198428208\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012299565648039181\n",
      "Average test loss: 0.0030941294613811704\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012276775015725029\n",
      "Average test loss: 0.0031669369182652897\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01226812522775597\n",
      "Average test loss: 0.0031380990400082533\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012234479243556659\n",
      "Average test loss: 0.0030706721651885243\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012232782629628977\n",
      "Average test loss: 0.0031857028388314777\n",
      "Epoch 104/300\n",
      "Average training loss: 0.012199264911313852\n",
      "Average test loss: 0.003161421279112498\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012184427291982703\n",
      "Average test loss: 0.0031744082358976205\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012170810105072127\n",
      "Average test loss: 0.003091388895900713\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012198519882228639\n",
      "Average test loss: 0.0031400721410496366\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012127296591798464\n",
      "Average test loss: 0.00308221050310466\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012133701952795188\n",
      "Average test loss: 0.0031495705749839545\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012098841841850016\n",
      "Average test loss: 0.0031451484279500113\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01210152826209863\n",
      "Average test loss: 0.0031902815827892887\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012057204999857478\n",
      "Average test loss: 0.003135705573691262\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012068995479908255\n",
      "Average test loss: 0.003204554524479641\n",
      "Epoch 114/300\n",
      "Average training loss: 0.012041967888673146\n",
      "Average test loss: 0.003220548107392258\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012036041574759616\n",
      "Average test loss: 0.0033585299147913853\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012023827210896546\n",
      "Average test loss: 0.003223810047325161\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012001965098910862\n",
      "Average test loss: 0.0031970120291743014\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011983914429114924\n",
      "Average test loss: 0.003205402975695001\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011980302818947368\n",
      "Average test loss: 0.003184208531760507\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01193601139055358\n",
      "Average test loss: 0.003171398187470105\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011939745946062936\n",
      "Average test loss: 0.0030992766206877098\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011929416280653742\n",
      "Average test loss: 0.003189159960589475\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01190614223231872\n",
      "Average test loss: 0.003170580928524335\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01190755220833752\n",
      "Average test loss: 0.0032598432790901925\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011895590329335796\n",
      "Average test loss: 0.0032172565288427803\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011868424992594453\n",
      "Average test loss: 0.0031560296941962507\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01187733493745327\n",
      "Average test loss: 0.0032063400256964894\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011847225009567208\n",
      "Average test loss: 0.0031111417238910994\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011834881966312726\n",
      "Average test loss: 0.003204353802733951\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011818797852430079\n",
      "Average test loss: 0.0031664694829119577\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011808201859394709\n",
      "Average test loss: 0.0033968012957937186\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011797948943244086\n",
      "Average test loss: 0.003259126652446058\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011797639341817961\n",
      "Average test loss: 0.0032311202574314343\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01177159414274825\n",
      "Average test loss: 0.0031943611322591704\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011764447583920427\n",
      "Average test loss: 0.003306838644047578\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01177482712186045\n",
      "Average test loss: 0.0031758576381123726\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011748480814198653\n",
      "Average test loss: 0.0031319613059361774\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011744672964016597\n",
      "Average test loss: 0.0031965476725664406\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011751670928464995\n",
      "Average test loss: 0.003338418659961058\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011692350590394604\n",
      "Average test loss: 0.0036226593496070966\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011696471206843853\n",
      "Average test loss: 0.003209652774863773\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011708645756873819\n",
      "Average test loss: 0.0032396995338300864\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011683717113816076\n",
      "Average test loss: 0.003166841440109743\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011663229338824748\n",
      "Average test loss: 0.0031803039440678224\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011653140899207857\n",
      "Average test loss: 0.0032355280123237107\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01165830913434426\n",
      "Average test loss: 0.0031847874793327516\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011631803886757957\n",
      "Average test loss: 0.0031361128472619584\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011615870593322648\n",
      "Average test loss: 0.0032385238168968094\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011639458290818664\n",
      "Average test loss: 0.0032359978039231563\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011614568381673759\n",
      "Average test loss: 0.00317172101388375\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011599653960929976\n",
      "Average test loss: 0.0032807857741912207\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011612025119364261\n",
      "Average test loss: 0.003259859286869566\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011577830865151352\n",
      "Average test loss: 0.003209318440821436\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011566047954890463\n",
      "Average test loss: 0.0032779661292831105\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011553543432719178\n",
      "Average test loss: 0.0032193551282915806\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011569463612304794\n",
      "Average test loss: 0.0033054126540405884\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011552531406283378\n",
      "Average test loss: 0.0031444595605134964\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011551404458781084\n",
      "Average test loss: 0.0032633617114689616\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011518353801634577\n",
      "Average test loss: 0.003280209003844195\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011505792059004307\n",
      "Average test loss: 0.003215714477416542\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011521225353082021\n",
      "Average test loss: 0.0031775186306072607\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011491180948085255\n",
      "Average test loss: 0.003202845471807652\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011506860050890182\n",
      "Average test loss: 0.003228441217293342\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011482187470628156\n",
      "Average test loss: 0.003199830269234048\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011472344372007582\n",
      "Average test loss: 0.003187948010861874\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011466021274526913\n",
      "Average test loss: 0.0032181856679833597\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011459258136649926\n",
      "Average test loss: 0.0032845167803267636\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0114585759209262\n",
      "Average test loss: 0.003365158726875153\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011432377205954658\n",
      "Average test loss: 0.003308993917786413\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011448329800532923\n",
      "Average test loss: 0.003198368791490793\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011424642716844877\n",
      "Average test loss: 0.003325683796985282\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011408277597692277\n",
      "Average test loss: 0.00333816787869566\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011430743166969883\n",
      "Average test loss: 0.003405199135136273\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011418139755725861\n",
      "Average test loss: 0.0031790915611717434\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011415165757967366\n",
      "Average test loss: 0.0031926511871731944\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011384276185598638\n",
      "Average test loss: 0.003282605561324292\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011361268670194678\n",
      "Average test loss: 0.003376328984482421\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011365526395539442\n",
      "Average test loss: 0.0033129606623616484\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01139136196176211\n",
      "Average test loss: 0.0033573770162959892\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01135467506531212\n",
      "Average test loss: 0.0032502464877648485\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011341031384964784\n",
      "Average test loss: 0.0032264195579207607\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011346269527657165\n",
      "Average test loss: 0.003330933207852973\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011359886049396462\n",
      "Average test loss: 0.0032566537130624055\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011324838681353463\n",
      "Average test loss: 0.0033727808482944964\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011338787281678782\n",
      "Average test loss: 0.0032552273391435542\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011327130647169219\n",
      "Average test loss: 0.003206838260508246\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011323337582664357\n",
      "Average test loss: 0.003294216822211941\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011303343909482162\n",
      "Average test loss: 0.0035146069750189783\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011280773674448331\n",
      "Average test loss: 0.003212516014153759\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011300644253691037\n",
      "Average test loss: 0.003367699063072602\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011291500487261349\n",
      "Average test loss: 0.0033633149119300975\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011272254676454598\n",
      "Average test loss: 0.0033327707855237854\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011261925133566062\n",
      "Average test loss: 0.003386617185134027\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011261360819968912\n",
      "Average test loss: 0.003308723761803574\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011285303049617343\n",
      "Average test loss: 0.003282108350760407\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011241694923076365\n",
      "Average test loss: 0.0032946061189803814\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011243774339556693\n",
      "Average test loss: 0.0031526787566641965\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011234939608309003\n",
      "Average test loss: 0.0033365523581289584\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011235184899634786\n",
      "Average test loss: 0.0032950055419156948\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011226030718949107\n",
      "Average test loss: 0.003420196197099156\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011228210440112485\n",
      "Average test loss: 0.003255827117918266\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01122103295640813\n",
      "Average test loss: 0.003470255761510796\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011200298294425011\n",
      "Average test loss: 0.0031940457611862156\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011197524883680874\n",
      "Average test loss: 0.003353939721826464\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011219354583985276\n",
      "Average test loss: 0.003167436321162515\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011189278669655323\n",
      "Average test loss: 0.003383177112581001\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011194251123401854\n",
      "Average test loss: 0.003285791140877538\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011200214505195618\n",
      "Average test loss: 0.0032926536748806635\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011182571131322119\n",
      "Average test loss: 0.003404020621130864\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011171071021921105\n",
      "Average test loss: 0.0032340118748446305\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01115092253850566\n",
      "Average test loss: 0.0031717561491661603\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011167011308173339\n",
      "Average test loss: 0.0032778358954108425\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011150966006020705\n",
      "Average test loss: 0.003289549702157577\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011137286530600653\n",
      "Average test loss: 0.003403274084544844\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011154897907541858\n",
      "Average test loss: 0.0032854463300771184\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011134353305730555\n",
      "Average test loss: 0.0033494616707579958\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0111335828602314\n",
      "Average test loss: 0.0033531282405472465\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011125007349583839\n",
      "Average test loss: 0.0035289060198184516\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011119056692553891\n",
      "Average test loss: 0.0032702616134451497\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011115069934063488\n",
      "Average test loss: 0.0032766836018611988\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011108105121387376\n",
      "Average test loss: 0.003277281513230668\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011097896051489645\n",
      "Average test loss: 0.003252673742464847\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011099395173291365\n",
      "Average test loss: 0.0032259584565957388\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0110745201309522\n",
      "Average test loss: 0.003478341867526372\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011095329417122735\n",
      "Average test loss: 0.0032951700136375924\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011098264258768824\n",
      "Average test loss: 0.0035830021974527174\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011092812966141436\n",
      "Average test loss: 0.0033273662941323387\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011075068797088332\n",
      "Average test loss: 0.0032485581098331347\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01107932316015164\n",
      "Average test loss: 0.003235765154576964\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011071644441121154\n",
      "Average test loss: 0.0034360091081923907\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0110614750377006\n",
      "Average test loss: 0.0036019951117535432\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011070557098421786\n",
      "Average test loss: 0.0032423760158320266\n",
      "Epoch 233/300\n",
      "Average training loss: 0.011064143040113978\n",
      "Average test loss: 0.003362486094650295\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011037060987618235\n",
      "Average test loss: 0.0033064483154772056\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011034953237407738\n",
      "Average test loss: 0.0033100745499961906\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011047029178175662\n",
      "Average test loss: 0.003362378649827507\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011034185348285569\n",
      "Average test loss: 0.003290296532213688\n",
      "Epoch 238/300\n",
      "Average training loss: 0.011015496544539928\n",
      "Average test loss: 0.003370921610751086\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011006131181286441\n",
      "Average test loss: 0.003330698613077402\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011022864957650503\n",
      "Average test loss: 0.0032715332959261205\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011038048858443896\n",
      "Average test loss: 0.0034830314113448065\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011028180736634467\n",
      "Average test loss: 0.003363965433297886\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011005352355539798\n",
      "Average test loss: 0.003313007282714049\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010986571614113119\n",
      "Average test loss: 0.003402790218798651\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011013891395595339\n",
      "Average test loss: 0.003257462746153275\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010994128970636261\n",
      "Average test loss: 0.003231358685427242\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01097824212246471\n",
      "Average test loss: 0.003311890597972605\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010978937370081742\n",
      "Average test loss: 0.00336484147546192\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010978937610156007\n",
      "Average test loss: 0.003379681514369117\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010972289983597066\n",
      "Average test loss: 0.0035002278869764674\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010964875336322519\n",
      "Average test loss: 0.0034480998292565346\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010964308544165559\n",
      "Average test loss: 0.0033274912565118735\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01095063960717784\n",
      "Average test loss: 0.0034938346311036085\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010944824284977383\n",
      "Average test loss: 0.0033782498294280634\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010975609994596906\n",
      "Average test loss: 0.003353247771246566\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010948288405521048\n",
      "Average test loss: 0.0034248506269521184\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010954459625399775\n",
      "Average test loss: 0.0033591835846503576\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010944347103436789\n",
      "Average test loss: 0.003283365333866742\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010945209022197458\n",
      "Average test loss: 0.0034378281951778465\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01093100554909971\n",
      "Average test loss: 0.003358102942092551\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0109076929009623\n",
      "Average test loss: 0.0033935378179368045\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010923962427510156\n",
      "Average test loss: 0.0033812338444921706\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01092250811143054\n",
      "Average test loss: 0.003363813562732604\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01092951880974902\n",
      "Average test loss: 0.003363196672871709\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01091828284660975\n",
      "Average test loss: 0.003458076482017835\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01093296173049344\n",
      "Average test loss: 0.0034952427384754023\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010897104577057892\n",
      "Average test loss: 0.00331810505170789\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01088221726566553\n",
      "Average test loss: 0.0033078748306466475\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010902001159058677\n",
      "Average test loss: 0.0033535435684025287\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01089587090909481\n",
      "Average test loss: 0.003341988380998373\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01087894387046496\n",
      "Average test loss: 0.003312807304577695\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010875092854102453\n",
      "Average test loss: 0.003456006163317296\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010880868029263285\n",
      "Average test loss: 0.0032535565230581496\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010877167781607973\n",
      "Average test loss: 0.003482295069222649\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010856051050954395\n",
      "Average test loss: 0.0034297115724119874\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010867448440028562\n",
      "Average test loss: 0.003444387964076466\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010862413494951196\n",
      "Average test loss: 0.0033312754422012304\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01087470869637198\n",
      "Average test loss: 0.003291408300813701\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010874139534930388\n",
      "Average test loss: 0.0034274665779537626\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010845591650240951\n",
      "Average test loss: 0.0033565895735389656\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010849535697864161\n",
      "Average test loss: 0.003340654254787498\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010853330708626244\n",
      "Average test loss: 0.0033150436320445605\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01084254882733027\n",
      "Average test loss: 0.003328721203116907\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010853339982529481\n",
      "Average test loss: 0.0033374869740671583\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01084194335507022\n",
      "Average test loss: 0.0034570885221991273\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010831902046998342\n",
      "Average test loss: 0.0032541847938878667\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010819548921452628\n",
      "Average test loss: 0.0033076209548032944\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010823205024831825\n",
      "Average test loss: 0.0033997185685568387\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010822833025207122\n",
      "Average test loss: 0.0033357788825200663\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010819150345193014\n",
      "Average test loss: 0.0033653719193405575\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010817821014258596\n",
      "Average test loss: 0.003193613451802068\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01081597840703196\n",
      "Average test loss: 0.003377146107869016\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010819588257620732\n",
      "Average test loss: 0.0033583906011448966\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010798616351352798\n",
      "Average test loss: 0.0034507177310685315\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010801146917045116\n",
      "Average test loss: 0.003296262703008122\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010806902297668987\n",
      "Average test loss: 0.003431566391968065\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010796903457906511\n",
      "Average test loss: 0.0033306835471755927\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010803800422284338\n",
      "Average test loss: 0.0033305052117341095\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010797579470607969\n",
      "Average test loss: 0.003485830630693171\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010815504513680935\n",
      "Average test loss: 0.003362046213199695\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10764771337310473\n",
      "Average test loss: 0.004628591929872831\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02420222140351931\n",
      "Average test loss: 0.003559865096377002\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020080548428826863\n",
      "Average test loss: 0.003345904879892866\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018528877743416362\n",
      "Average test loss: 0.00318850335602959\n",
      "Epoch 5/300\n",
      "Average training loss: 0.017661563222606976\n",
      "Average test loss: 0.00308498786120779\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017083181521130933\n",
      "Average test loss: 0.0030432058316138054\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01660951039360629\n",
      "Average test loss: 0.0029078946797591117\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01623494055204921\n",
      "Average test loss: 0.002792374795302749\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015901346556842326\n",
      "Average test loss: 0.0027290050103846524\n",
      "Epoch 10/300\n",
      "Average training loss: 0.015622416539324654\n",
      "Average test loss: 0.002766341168847349\n",
      "Epoch 11/300\n",
      "Average training loss: 0.015356033133963744\n",
      "Average test loss: 0.002647869102656841\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015119556317726771\n",
      "Average test loss: 0.0026178556428187423\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014874478807051977\n",
      "Average test loss: 0.0025895398784842754\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014648221936490801\n",
      "Average test loss: 0.002560638615447614\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014416922415296237\n",
      "Average test loss: 0.0025333351786765786\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01420355623960495\n",
      "Average test loss: 0.0024624931428374516\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013985423213077917\n",
      "Average test loss: 0.002449093145111369\n",
      "Epoch 18/300\n",
      "Average training loss: 0.013768284805946879\n",
      "Average test loss: 0.0023992005882577765\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01357502695835299\n",
      "Average test loss: 0.0023780770645373396\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013370599763260948\n",
      "Average test loss: 0.002501470140284962\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013180178421239058\n",
      "Average test loss: 0.0023762432142264314\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012989291417929861\n",
      "Average test loss: 0.002428301304889222\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01284705074586802\n",
      "Average test loss: 0.0026599160480416484\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012686262814535035\n",
      "Average test loss: 0.0022803787489732107\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01253762547340658\n",
      "Average test loss: 0.0022481793881290488\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012396196215516991\n",
      "Average test loss: 0.00223233028066655\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01228687607910898\n",
      "Average test loss: 0.0022321293682066932\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012158367073370351\n",
      "Average test loss: 0.002195355313519637\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01205229535781675\n",
      "Average test loss: 0.002187722661635942\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01194753138307068\n",
      "Average test loss: 0.0021679102283798986\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011843093506164021\n",
      "Average test loss: 0.0021719854571339158\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01176280050062471\n",
      "Average test loss: 0.0022968910957376163\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0116751097291708\n",
      "Average test loss: 0.002254507915220327\n",
      "Epoch 34/300\n",
      "Average training loss: 0.011570786544018321\n",
      "Average test loss: 0.0021463045246071285\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011498303458094597\n",
      "Average test loss: 0.0021604876826620766\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011429210816820462\n",
      "Average test loss: 0.0021660285885963175\n",
      "Epoch 37/300\n",
      "Average training loss: 0.011366080428991053\n",
      "Average test loss: 0.0021584688124971258\n",
      "Epoch 38/300\n",
      "Average training loss: 0.011289728547135988\n",
      "Average test loss: 0.002170339189676775\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01120270517302884\n",
      "Average test loss: 0.002140954045475357\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011141484532091352\n",
      "Average test loss: 0.0021668735885371766\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011071062174108293\n",
      "Average test loss: 0.002190947340697878\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01101827846798632\n",
      "Average test loss: 0.0022413893896672463\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010966539794372187\n",
      "Average test loss: 0.002109171316234602\n",
      "Epoch 44/300\n",
      "Average training loss: 0.010906833506292768\n",
      "Average test loss: 0.0021381686731345123\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010849064405593608\n",
      "Average test loss: 0.002219826116330094\n",
      "Epoch 46/300\n",
      "Average training loss: 0.010791835774978002\n",
      "Average test loss: 0.002150278992123074\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01073683177679777\n",
      "Average test loss: 0.0021298400482369795\n",
      "Epoch 48/300\n",
      "Average training loss: 0.010669661907686128\n",
      "Average test loss: 0.0021424969360232353\n",
      "Epoch 49/300\n",
      "Average training loss: 0.010637699100706312\n",
      "Average test loss: 0.0021340514951282076\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01057822483446863\n",
      "Average test loss: 0.00226261046818561\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010526099503040313\n",
      "Average test loss: 0.0021932371794763537\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010485599774453375\n",
      "Average test loss: 0.0021234521263589464\n",
      "Epoch 53/300\n",
      "Average training loss: 0.010414216001414591\n",
      "Average test loss: 0.0022102520332361263\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010381530591183238\n",
      "Average test loss: 0.0021510508954524993\n",
      "Epoch 55/300\n",
      "Average training loss: 0.010356292002316978\n",
      "Average test loss: 0.0022363150695131886\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0102995159017543\n",
      "Average test loss: 0.0021682606662313145\n",
      "Epoch 57/300\n",
      "Average training loss: 0.010252557451526324\n",
      "Average test loss: 0.0022352960242165458\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0102116956917776\n",
      "Average test loss: 0.0021887203893727726\n",
      "Epoch 59/300\n",
      "Average training loss: 0.010158284732864963\n",
      "Average test loss: 0.0021460836591819924\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010147128357655473\n",
      "Average test loss: 0.002211836610092885\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01009958894054095\n",
      "Average test loss: 0.002167546362305681\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01006491287300984\n",
      "Average test loss: 0.0021767534665349457\n",
      "Epoch 63/300\n",
      "Average training loss: 0.010029460036920177\n",
      "Average test loss: 0.002152145192854934\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009993071508076455\n",
      "Average test loss: 0.002183172542187903\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009971866223547194\n",
      "Average test loss: 0.002185516559208433\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009926949640115102\n",
      "Average test loss: 0.0022424877083135975\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009896217634280523\n",
      "Average test loss: 0.0021706687421020536\n",
      "Epoch 68/300\n",
      "Average training loss: 0.00985729219019413\n",
      "Average test loss: 0.0022652937865091697\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00982238138301505\n",
      "Average test loss: 0.0021751949760235017\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009789330794579453\n",
      "Average test loss: 0.0021882647590504753\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009805270296418004\n",
      "Average test loss: 0.0022389029813930394\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009738481514155864\n",
      "Average test loss: 0.002185986520205107\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009724696106380886\n",
      "Average test loss: 0.00224509521573782\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00969446670429574\n",
      "Average test loss: 0.0022702663145545456\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009657482211788495\n",
      "Average test loss: 0.002325044114039176\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009639028683304787\n",
      "Average test loss: 0.00221938587176717\n",
      "Epoch 77/300\n",
      "Average training loss: 0.00962862039440208\n",
      "Average test loss: 0.002146103546126849\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009572730258935028\n",
      "Average test loss: 0.0021876082018845613\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009558432484666507\n",
      "Average test loss: 0.0022016474842611287\n",
      "Epoch 80/300\n",
      "Average training loss: 0.00955299295567804\n",
      "Average test loss: 0.002224657689014243\n",
      "Epoch 81/300\n",
      "Average training loss: 0.009514958577023612\n",
      "Average test loss: 0.002266123141058617\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009493026726361778\n",
      "Average test loss: 0.00232719473561479\n",
      "Epoch 83/300\n",
      "Average training loss: 0.009495788383815023\n",
      "Average test loss: 0.0022286013875984483\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009446917430394226\n",
      "Average test loss: 0.0022386263455781673\n",
      "Epoch 85/300\n",
      "Average training loss: 0.00943017100956705\n",
      "Average test loss: 0.0022157718195683425\n",
      "Epoch 86/300\n",
      "Average training loss: 0.009422946583065722\n",
      "Average test loss: 0.0022375867857287327\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009408565846582255\n",
      "Average test loss: 0.0022194659345679815\n",
      "Epoch 88/300\n",
      "Average training loss: 0.009385979577071137\n",
      "Average test loss: 0.002201713106284539\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009362788355184925\n",
      "Average test loss: 0.002223768633272913\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009331990814871257\n",
      "Average test loss: 0.0022871697558504013\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00931832824067937\n",
      "Average test loss: 0.0022231885824973386\n",
      "Epoch 92/300\n",
      "Average training loss: 0.009288506793479125\n",
      "Average test loss: 0.0023378856310413945\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00927968759338061\n",
      "Average test loss: 0.0022836751124511163\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009282239569971958\n",
      "Average test loss: 0.00232949878482355\n",
      "Epoch 95/300\n",
      "Average training loss: 0.009233490184777312\n",
      "Average test loss: 0.002293518052953813\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009244838911212153\n",
      "Average test loss: 0.0022133826077398327\n",
      "Epoch 97/300\n",
      "Average training loss: 0.009227038047379918\n",
      "Average test loss: 0.002260411852763759\n",
      "Epoch 98/300\n",
      "Average training loss: 0.009212654002010823\n",
      "Average test loss: 0.0023291354454639886\n",
      "Epoch 99/300\n",
      "Average training loss: 0.009183713829351797\n",
      "Average test loss: 0.0023111115300820935\n",
      "Epoch 100/300\n",
      "Average training loss: 0.009177529133856296\n",
      "Average test loss: 0.002286049105433954\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009151851375483805\n",
      "Average test loss: 0.0022722815081684124\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009144327454268932\n",
      "Average test loss: 0.0022919049705896113\n",
      "Epoch 103/300\n",
      "Average training loss: 0.00912869563491808\n",
      "Average test loss: 0.0022788278381857606\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00911524952451388\n",
      "Average test loss: 0.002323013939584295\n",
      "Epoch 105/300\n",
      "Average training loss: 0.009095053541991445\n",
      "Average test loss: 0.0022725060211701525\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009105657950043679\n",
      "Average test loss: 0.002274411274327172\n",
      "Epoch 107/300\n",
      "Average training loss: 0.009073319669812919\n",
      "Average test loss: 0.0022934807173700797\n",
      "Epoch 108/300\n",
      "Average training loss: 0.009059681110497979\n",
      "Average test loss: 0.00228585305934151\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00905371601631244\n",
      "Average test loss: 0.002280873670346207\n",
      "Epoch 110/300\n",
      "Average training loss: 0.009042737483978271\n",
      "Average test loss: 0.0022787417919478484\n",
      "Epoch 111/300\n",
      "Average training loss: 0.009026554289791319\n",
      "Average test loss: 0.0023828557452393903\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008996270052260823\n",
      "Average test loss: 0.0022964294206144083\n",
      "Epoch 113/300\n",
      "Average training loss: 0.00898192954560121\n",
      "Average test loss: 0.0023726668057756292\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00900857447501686\n",
      "Average test loss: 0.0022758628018200396\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008979210402402613\n",
      "Average test loss: 0.0023237453827427493\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008966881148517132\n",
      "Average test loss: 0.0022802497709376946\n",
      "Epoch 117/300\n",
      "Average training loss: 0.00897807854000065\n",
      "Average test loss: 0.002271534305686752\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008934841020239724\n",
      "Average test loss: 0.0023235295089996523\n",
      "Epoch 119/300\n",
      "Average training loss: 0.00893882094323635\n",
      "Average test loss: 0.0023262627188944153\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008912350523389049\n",
      "Average test loss: 0.0022820510825970106\n",
      "Epoch 121/300\n",
      "Average training loss: 0.00891397863543696\n",
      "Average test loss: 0.0023244415320869947\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008896868486371305\n",
      "Average test loss: 0.002285513396271401\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008891988391263617\n",
      "Average test loss: 0.0022740698736160993\n",
      "Epoch 124/300\n",
      "Average training loss: 0.00887872210641702\n",
      "Average test loss: 0.002411299897047381\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008864262129697535\n",
      "Average test loss: 0.0022810021457779737\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008867597569194105\n",
      "Average test loss: 0.0024357918127336435\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008851207716597451\n",
      "Average test loss: 0.002309878455888894\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008852785665127967\n",
      "Average test loss: 0.0022894825237906643\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008822590014172925\n",
      "Average test loss: 0.0023183921395490566\n",
      "Epoch 130/300\n",
      "Average training loss: 0.008838320618288384\n",
      "Average test loss: 0.0023151104498861565\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008823004498249954\n",
      "Average test loss: 0.002334501901538008\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008804043038437764\n",
      "Average test loss: 0.0023656429056492117\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008793838500562642\n",
      "Average test loss: 0.002356361822432114\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008783693369064066\n",
      "Average test loss: 0.0023671947293397453\n",
      "Epoch 135/300\n",
      "Average training loss: 0.00877353890488545\n",
      "Average test loss: 0.002357975408848789\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008766037203371525\n",
      "Average test loss: 0.002287399961095717\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008765357576310634\n",
      "Average test loss: 0.0023692555320966573\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00876421578331954\n",
      "Average test loss: 0.002414460602733824\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00874336534904109\n",
      "Average test loss: 0.0024233281494428715\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008734585589004888\n",
      "Average test loss: 0.002379518028555645\n",
      "Epoch 141/300\n",
      "Average training loss: 0.008745007345245944\n",
      "Average test loss: 0.0024172620417343246\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008728531022866567\n",
      "Average test loss: 0.002380287711405092\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008716839025004043\n",
      "Average test loss: 0.002269223752948973\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008727532674041059\n",
      "Average test loss: 0.002360709507018328\n",
      "Epoch 145/300\n",
      "Average training loss: 0.008685790816115008\n",
      "Average test loss: 0.0024238318589826424\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008688178050435251\n",
      "Average test loss: 0.0023060221142239042\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008685684321241246\n",
      "Average test loss: 0.002525693515729573\n",
      "Epoch 148/300\n",
      "Average training loss: 0.008668284407092466\n",
      "Average test loss: 0.002370165514552759\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0086815634386407\n",
      "Average test loss: 0.0024182600654247734\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008663526076409552\n",
      "Average test loss: 0.002349500837839312\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008649615300612317\n",
      "Average test loss: 0.0024092007159358924\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008665631623731719\n",
      "Average test loss: 0.002270499150372214\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008651982725494438\n",
      "Average test loss: 0.0023650532366914882\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008631480135437516\n",
      "Average test loss: 0.00246181602206909\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008636535715725686\n",
      "Average test loss: 0.0023457295927736496\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008623062644981676\n",
      "Average test loss: 0.0023719513221747346\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008601768225431442\n",
      "Average test loss: 0.00230993295316067\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00861531625274155\n",
      "Average test loss: 0.0024360074295351904\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008587219609982438\n",
      "Average test loss: 0.0022991428723972704\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00861551930093103\n",
      "Average test loss: 0.002479666620079014\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008583068867731426\n",
      "Average test loss: 0.002484180674577753\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008584754265016979\n",
      "Average test loss: 0.0023413822394278317\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00857964461379581\n",
      "Average test loss: 0.0025257282773446708\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008565247780746884\n",
      "Average test loss: 0.002393786579370499\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008570639358626472\n",
      "Average test loss: 0.0023970044375293785\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008556909617450502\n",
      "Average test loss: 0.0023939600304389995\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008565570095347034\n",
      "Average test loss: 0.0023556731563682356\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008558812654680677\n",
      "Average test loss: 0.0025753557335378395\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008567400833384859\n",
      "Average test loss: 0.002342731497147017\n",
      "Epoch 170/300\n",
      "Average training loss: 0.008520043458375665\n",
      "Average test loss: 0.0023917275253269405\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008530671606461208\n",
      "Average test loss: 0.0024203793946653606\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00852594878607326\n",
      "Average test loss: 0.0023530584687574043\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008507193206912941\n",
      "Average test loss: 0.002372425604404675\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008519734176496666\n",
      "Average test loss: 0.002427305815120538\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008495769797099961\n",
      "Average test loss: 0.0024482492388536534\n",
      "Epoch 176/300\n",
      "Average training loss: 0.00851740686264303\n",
      "Average test loss: 0.0023600974707967707\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008492742420898543\n",
      "Average test loss: 0.0023568471267612443\n",
      "Epoch 178/300\n",
      "Average training loss: 0.008493047615306245\n",
      "Average test loss: 0.002406943155038688\n",
      "Epoch 179/300\n",
      "Average training loss: 0.00848953093836705\n",
      "Average test loss: 0.0024139661569562224\n",
      "Epoch 180/300\n",
      "Average training loss: 0.008483892219348086\n",
      "Average test loss: 0.002402190768159926\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00847949099375142\n",
      "Average test loss: 0.0023885957731140986\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008479677068276539\n",
      "Average test loss: 0.0024297115517159305\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008479517375429471\n",
      "Average test loss: 0.002430762440173162\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008454433121201066\n",
      "Average test loss: 0.0024800839316513803\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008457984775718715\n",
      "Average test loss: 0.0023573319235195714\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008454163130786684\n",
      "Average test loss: 0.002438216760237184\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008458289248454902\n",
      "Average test loss: 0.0024614964704960584\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008435468045373758\n",
      "Average test loss: 0.0024222013977252773\n",
      "Epoch 189/300\n",
      "Average training loss: 0.008438175395958954\n",
      "Average test loss: 0.0027712919894191954\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00844041486001677\n",
      "Average test loss: 0.0024066351392409866\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008432044638527765\n",
      "Average test loss: 0.00238465944925944\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00843436517400874\n",
      "Average test loss: 0.002452989356385337\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008411674650179016\n",
      "Average test loss: 0.002406566398218274\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00841620982479718\n",
      "Average test loss: 0.0024017499395542676\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008413725501133337\n",
      "Average test loss: 0.0023828794192522766\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008409229059186247\n",
      "Average test loss: 0.002416654230405887\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008393591868380705\n",
      "Average test loss: 0.0024963151319987243\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00840361095716556\n",
      "Average test loss: 0.00242858639297386\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008381080956094794\n",
      "Average test loss: 0.0024167075368265313\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008383884337627226\n",
      "Average test loss: 0.002475946061209672\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00837748275945584\n",
      "Average test loss: 0.0024133300847477383\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00838715367929803\n",
      "Average test loss: 0.0023511437982734707\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008373812353859346\n",
      "Average test loss: 0.002465402885650595\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008375091663251321\n",
      "Average test loss: 0.0024080687797524863\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008361375816994244\n",
      "Average test loss: 0.0024711670538203586\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00838527762889862\n",
      "Average test loss: 0.0023448584870331816\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0083730884955989\n",
      "Average test loss: 0.002472650679035319\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008341678081287279\n",
      "Average test loss: 0.0023859345018863677\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008355950327383148\n",
      "Average test loss: 0.0025227157703290384\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008350602675643233\n",
      "Average test loss: 0.002392105492763221\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008346111349761485\n",
      "Average test loss: 0.0023832414459644094\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008337432022723888\n",
      "Average test loss: 0.002431197285859121\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008327417643533813\n",
      "Average test loss: 0.0024242934352821776\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008341062345024613\n",
      "Average test loss: 0.0025264324537581867\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008334857553243638\n",
      "Average test loss: 0.0024624915808025335\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008330993094378047\n",
      "Average test loss: 0.0024844174300216967\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008311215942104657\n",
      "Average test loss: 0.0024893694289235607\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008323598099251588\n",
      "Average test loss: 0.002452027431378762\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008323274386839734\n",
      "Average test loss: 0.0025095602696140608\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008312192651960586\n",
      "Average test loss: 0.0025032694499111836\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008300243555257717\n",
      "Average test loss: 0.0023802515969922147\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00830105813841025\n",
      "Average test loss: 0.002582737810909748\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008305678009986878\n",
      "Average test loss: 0.002403236293544372\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008294040477938122\n",
      "Average test loss: 0.002513283915610777\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008291151890324222\n",
      "Average test loss: 0.0025043335430738\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008281023662951258\n",
      "Average test loss: 0.002450900726227297\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008290957866443528\n",
      "Average test loss: 0.002507715724201666\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00828298114405738\n",
      "Average test loss: 0.0024332861783396866\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008275256095661057\n",
      "Average test loss: 0.0024789504216363033\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00828264337612523\n",
      "Average test loss: 0.0024813602790236472\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008277310661143726\n",
      "Average test loss: 0.002426535840663645\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008256400771439075\n",
      "Average test loss: 0.0023578609439233937\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008260057101647058\n",
      "Average test loss: 0.002421622817094127\n",
      "Epoch 234/300\n",
      "Average training loss: 0.00825497027569347\n",
      "Average test loss: 0.002450135013916426\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00825104467322429\n",
      "Average test loss: 0.0024132035963444244\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008263521903091008\n",
      "Average test loss: 0.0023533953552444774\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008246521882712842\n",
      "Average test loss: 0.0024820132561855844\n",
      "Epoch 238/300\n",
      "Average training loss: 0.008245489295985963\n",
      "Average test loss: 0.002425183986210161\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008244731086823675\n",
      "Average test loss: 0.0024815825772368247\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008237206837369336\n",
      "Average test loss: 0.002425003733692898\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008235148049890995\n",
      "Average test loss: 0.0024255993641499017\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008235515728592872\n",
      "Average test loss: 0.0025336533373014796\n",
      "Epoch 243/300\n",
      "Average training loss: 0.008226709720161226\n",
      "Average test loss: 0.002418369839588801\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008223709956639343\n",
      "Average test loss: 0.002491384417646461\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008220075916084979\n",
      "Average test loss: 0.0024609112785094313\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008221654694113466\n",
      "Average test loss: 0.002571011322033074\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008216146062645648\n",
      "Average test loss: 0.00238961579836905\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008214050182037884\n",
      "Average test loss: 0.0024522430018211402\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008227804886798064\n",
      "Average test loss: 0.002489430305444532\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008206084774600135\n",
      "Average test loss: 0.0024156789939022726\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008205101523962286\n",
      "Average test loss: 0.0024993332837604815\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008198172999752892\n",
      "Average test loss: 0.002411757913728555\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008212596064640415\n",
      "Average test loss: 0.002373944010378586\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008190758102469975\n",
      "Average test loss: 0.0024410146118866074\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008191469411055247\n",
      "Average test loss: 0.0024794306959956885\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008198063195579582\n",
      "Average test loss: 0.002488148799062603\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008193802011509736\n",
      "Average test loss: 0.002424131061260899\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008195538849466376\n",
      "Average test loss: 0.0024509044086767568\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008181845236362683\n",
      "Average test loss: 0.0024906949436085094\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008174535142464771\n",
      "Average test loss: 0.002627523561525676\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008190935948242745\n",
      "Average test loss: 0.002452046358337005\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00817130513323678\n",
      "Average test loss: 0.00241292259345452\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008170170993440681\n",
      "Average test loss: 0.0023967124263031617\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008175796211593681\n",
      "Average test loss: 0.002481822249583072\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008155755159341627\n",
      "Average test loss: 0.002480695478618145\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008151900821261935\n",
      "Average test loss: 0.0024320831710679665\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008160415531860457\n",
      "Average test loss: 0.002528388311672542\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008172505906886524\n",
      "Average test loss: 0.0024794913979454174\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008143286101520061\n",
      "Average test loss: 0.002467114204333888\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008160407080004612\n",
      "Average test loss: 0.0024259416753840113\n",
      "Epoch 271/300\n",
      "Average training loss: 0.00814799792236752\n",
      "Average test loss: 0.0024290561921273667\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0081509522654944\n",
      "Average test loss: 0.002403101505090793\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008149868820276525\n",
      "Average test loss: 0.002485976835919751\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008145776085141632\n",
      "Average test loss: 0.0025465955409324833\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008137484334823157\n",
      "Average test loss: 0.0025129249745772944\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008131930383543174\n",
      "Average test loss: 0.0025690177127511964\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008118169030795494\n",
      "Average test loss: 0.002446886539045307\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008131004903051589\n",
      "Average test loss: 0.0024102288412137162\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008127119248112043\n",
      "Average test loss: 0.0026313435117610627\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008128465563886696\n",
      "Average test loss: 0.002453127222963505\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00813160240650177\n",
      "Average test loss: 0.0024976475892795457\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008128577371852266\n",
      "Average test loss: 0.0025812127501186398\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008107006743550301\n",
      "Average test loss: 0.002391408297440244\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008105746957990859\n",
      "Average test loss: 0.0024149200771417883\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008119551440080006\n",
      "Average test loss: 0.0024729813068277307\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008114978777451648\n",
      "Average test loss: 0.0025778634312252206\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008114309143688943\n",
      "Average test loss: 0.0024364086927639115\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008105540581047534\n",
      "Average test loss: 0.002439320931211114\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00811198056075308\n",
      "Average test loss: 0.0024573370334174897\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008100185103714467\n",
      "Average test loss: 0.002461729196831584\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008099344696021742\n",
      "Average test loss: 0.002577993688484033\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008091789030366473\n",
      "Average test loss: 0.0024817925388407377\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008085749206857548\n",
      "Average test loss: 0.00250054708454344\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008093809463911587\n",
      "Average test loss: 0.002461494147622337\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008086036442468563\n",
      "Average test loss: 0.002453316860418353\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008089307018866142\n",
      "Average test loss: 0.0025296950509978667\n",
      "Epoch 297/300\n",
      "Average training loss: 0.00808610099967983\n",
      "Average test loss: 0.00254644831456244\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00808811015718513\n",
      "Average test loss: 0.002502577847490708\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008071570756948657\n",
      "Average test loss: 0.0025086451108670897\n",
      "Epoch 300/300\n",
      "Average training loss: 0.00806026632959644\n",
      "Average test loss: 0.0025075076485259664\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10340179633266396\n",
      "Average test loss: 0.0034632339663803577\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02150538783437676\n",
      "Average test loss: 0.003151696600433853\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017118734369675318\n",
      "Average test loss: 0.0026785359436439144\n",
      "Epoch 4/300\n",
      "Average training loss: 0.015488541929258241\n",
      "Average test loss: 0.0025100591122690172\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014597620344824262\n",
      "Average test loss: 0.0025016642446733183\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013991858748926056\n",
      "Average test loss: 0.0023606065982538795\n",
      "Epoch 7/300\n",
      "Average training loss: 0.013539581251641114\n",
      "Average test loss: 0.0022407762772507136\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01316846848693159\n",
      "Average test loss: 0.0022352533667451806\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01281546061154869\n",
      "Average test loss: 0.0020882695789138477\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01251257601628701\n",
      "Average test loss: 0.002051569655123684\n",
      "Epoch 11/300\n",
      "Average training loss: 0.012223345001538594\n",
      "Average test loss: 0.001992357329568929\n",
      "Epoch 12/300\n",
      "Average training loss: 0.011960201805664433\n",
      "Average test loss: 0.0020377577848525513\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011701742668118742\n",
      "Average test loss: 0.0019659462637371485\n",
      "Epoch 14/300\n",
      "Average training loss: 0.011459808305733734\n",
      "Average test loss: 0.0018540847488782472\n",
      "Epoch 15/300\n",
      "Average training loss: 0.011222264730268055\n",
      "Average test loss: 0.0018331413705729776\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010985946709083186\n",
      "Average test loss: 0.0017823586744359799\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010765300642699004\n",
      "Average test loss: 0.0022282873812235066\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01054864946876963\n",
      "Average test loss: 0.0017766264006495475\n",
      "Epoch 19/300\n",
      "Average training loss: 0.010349647478097015\n",
      "Average test loss: 0.0017112259039034445\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010142129928701454\n",
      "Average test loss: 0.0016978588472637865\n",
      "Epoch 21/300\n",
      "Average training loss: 0.009962085651854673\n",
      "Average test loss: 0.001805044401747485\n",
      "Epoch 22/300\n",
      "Average training loss: 0.009786993357870314\n",
      "Average test loss: 0.0016476504537794325\n",
      "Epoch 23/300\n",
      "Average training loss: 0.00963022053738435\n",
      "Average test loss: 0.0017699967211940223\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009478166466785802\n",
      "Average test loss: 0.001613225983041856\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00935764566477802\n",
      "Average test loss: 0.0016172916601515479\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009233561664819718\n",
      "Average test loss: 0.0015714068309817877\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009127717564503353\n",
      "Average test loss: 0.0015556614397921496\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009012041877541278\n",
      "Average test loss: 0.0015757757079684072\n",
      "Epoch 29/300\n",
      "Average training loss: 0.008933444182078044\n",
      "Average test loss: 0.0015790188883741698\n",
      "Epoch 30/300\n",
      "Average training loss: 0.008849228992230363\n",
      "Average test loss: 0.0015591099843796756\n",
      "Epoch 31/300\n",
      "Average training loss: 0.008758457551813788\n",
      "Average test loss: 0.0015269076471320457\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008683122597220871\n",
      "Average test loss: 0.001525651832525101\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008622114684432745\n",
      "Average test loss: 0.001500766649738782\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00855318698618147\n",
      "Average test loss: 0.0015189588070950574\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008505023176057471\n",
      "Average test loss: 0.0014974904033458894\n",
      "Epoch 36/300\n",
      "Average training loss: 0.008421483640869459\n",
      "Average test loss: 0.0014949043041302098\n",
      "Epoch 37/300\n",
      "Average training loss: 0.00837853798104657\n",
      "Average test loss: 0.0015000319609211551\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00832786835109194\n",
      "Average test loss: 0.001619583673361275\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008271776604983542\n",
      "Average test loss: 0.0014815232494018143\n",
      "Epoch 40/300\n",
      "Average training loss: 0.008221590550823344\n",
      "Average test loss: 0.001535637582031389\n",
      "Epoch 41/300\n",
      "Average training loss: 0.008195087645202876\n",
      "Average test loss: 0.0015285150383909543\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008118951305747033\n",
      "Average test loss: 0.0014946745745320286\n",
      "Epoch 43/300\n",
      "Average training loss: 0.008105015916542875\n",
      "Average test loss: 0.0015320630787561337\n",
      "Epoch 44/300\n",
      "Average training loss: 0.00805198959964845\n",
      "Average test loss: 0.0014976798689199818\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007996181883745724\n",
      "Average test loss: 0.0015304296438892683\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007955201166371504\n",
      "Average test loss: 0.0015121620526123377\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007929501608014106\n",
      "Average test loss: 0.0014810744332563546\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00789710329969724\n",
      "Average test loss: 0.001515961728990078\n",
      "Epoch 49/300\n",
      "Average training loss: 0.007838287276112372\n",
      "Average test loss: 0.0014791540421752467\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007809686772111389\n",
      "Average test loss: 0.0015156463001751237\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0077838188476032685\n",
      "Average test loss: 0.0015744676566165354\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007754274629056454\n",
      "Average test loss: 0.0015017902229188217\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007696666640953885\n",
      "Average test loss: 0.0014870533293320073\n",
      "Epoch 54/300\n",
      "Average training loss: 0.007675591919571161\n",
      "Average test loss: 0.0014909581850386328\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007643077208764023\n",
      "Average test loss: 0.001486332338510288\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007603155628260639\n",
      "Average test loss: 0.0014935729609181483\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007579488163606988\n",
      "Average test loss: 0.0015334612973448303\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0075455626659095285\n",
      "Average test loss: 0.0015310816880729464\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007522518249021636\n",
      "Average test loss: 0.0016148999547585844\n",
      "Epoch 60/300\n",
      "Average training loss: 0.007504679056505362\n",
      "Average test loss: 0.001477213064674288\n",
      "Epoch 61/300\n",
      "Average training loss: 0.007464552771093117\n",
      "Average test loss: 0.0017196907656681205\n",
      "Epoch 62/300\n",
      "Average training loss: 0.007432126856926415\n",
      "Average test loss: 0.0015016008090848723\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007405806791037321\n",
      "Average test loss: 0.0015109152633489833\n",
      "Epoch 64/300\n",
      "Average training loss: 0.007384876002454095\n",
      "Average test loss: 0.0015288516815958751\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007367780249151919\n",
      "Average test loss: 0.0015172123064597447\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00732763377784027\n",
      "Average test loss: 0.001506206454605692\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0073081875070929525\n",
      "Average test loss: 0.0015014086390535038\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007298116462512149\n",
      "Average test loss: 0.0015615193720700013\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007269842731869883\n",
      "Average test loss: 0.0015146693104050226\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007231725192732281\n",
      "Average test loss: 0.001569472700978319\n",
      "Epoch 71/300\n",
      "Average training loss: 0.007210174172702762\n",
      "Average test loss: 0.0015064506785323222\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00721132836656438\n",
      "Average test loss: 0.0015220596114587453\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007191548458817932\n",
      "Average test loss: 0.0015209264220255945\n",
      "Epoch 74/300\n",
      "Average training loss: 0.007152612148887581\n",
      "Average test loss: 0.0015690744984894992\n",
      "Epoch 75/300\n",
      "Average training loss: 0.00714403496724036\n",
      "Average test loss: 0.0015771014408932792\n",
      "Epoch 76/300\n",
      "Average training loss: 0.007131452658938037\n",
      "Average test loss: 0.001628494672373765\n",
      "Epoch 77/300\n",
      "Average training loss: 0.007117342354936732\n",
      "Average test loss: 0.0015314308174161448\n",
      "Epoch 78/300\n",
      "Average training loss: 0.007077430540074905\n",
      "Average test loss: 0.001609472572286096\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007074535835120413\n",
      "Average test loss: 0.0015803789363967049\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007036943802403079\n",
      "Average test loss: 0.0015849906892205278\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007044079741670026\n",
      "Average test loss: 0.001622718632221222\n",
      "Epoch 82/300\n",
      "Average training loss: 0.007014368726147546\n",
      "Average test loss: 0.0015779254281272491\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007011267532904943\n",
      "Average test loss: 0.001601784697630339\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006994369451784425\n",
      "Average test loss: 0.0016419592622874512\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006987129881564114\n",
      "Average test loss: 0.001571091697861751\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006942551395545403\n",
      "Average test loss: 0.0015572580297787985\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006935986100385586\n",
      "Average test loss: 0.0015986918031962382\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006919432953000068\n",
      "Average test loss: 0.0015727623723861244\n",
      "Epoch 89/300\n",
      "Average training loss: 0.006923664055350754\n",
      "Average test loss: 0.001572406372676293\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006886254432714648\n",
      "Average test loss: 0.001582802338629133\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006867056851585706\n",
      "Average test loss: 0.0015408303161255188\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0068638862640493445\n",
      "Average test loss: 0.0015611810568306181\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0068533878007696735\n",
      "Average test loss: 0.001581844402373665\n",
      "Epoch 94/300\n",
      "Average training loss: 0.006844253210143911\n",
      "Average test loss: 0.0016479854345735577\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0068178122436834705\n",
      "Average test loss: 0.001585902755976551\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006821352589875459\n",
      "Average test loss: 0.0015631180176925328\n",
      "Epoch 97/300\n",
      "Average training loss: 0.006810624718252155\n",
      "Average test loss: 0.0015994942827771107\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006796611823969417\n",
      "Average test loss: 0.0016208949310498105\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006774439495056867\n",
      "Average test loss: 0.0015460818190541532\n",
      "Epoch 100/300\n",
      "Average training loss: 0.006759642451173729\n",
      "Average test loss: 0.00157999973392321\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0067521535779039065\n",
      "Average test loss: 0.0015734877995111877\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006747727685918411\n",
      "Average test loss: 0.0016509184845619731\n",
      "Epoch 103/300\n",
      "Average training loss: 0.006733962930738926\n",
      "Average test loss: 0.0015788362009657754\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006733517916252216\n",
      "Average test loss: 0.001577298541035917\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006707890925308068\n",
      "Average test loss: 0.001623947521050771\n",
      "Epoch 106/300\n",
      "Average training loss: 0.00670151100680232\n",
      "Average test loss: 0.0016360501207204328\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006698894314467907\n",
      "Average test loss: 0.001626993821002543\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006684107472085291\n",
      "Average test loss: 0.0016806622230344348\n",
      "Epoch 109/300\n",
      "Average training loss: 0.006663265818936957\n",
      "Average test loss: 0.0016005592989838785\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006663074367576175\n",
      "Average test loss: 0.0015955245855988729\n",
      "Epoch 111/300\n",
      "Average training loss: 0.006657538384199142\n",
      "Average test loss: 0.0016415472268126905\n",
      "Epoch 112/300\n",
      "Average training loss: 0.006638393717921442\n",
      "Average test loss: 0.001642189798048801\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0066292737755510546\n",
      "Average test loss: 0.0016238730025167267\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0066229316037562155\n",
      "Average test loss: 0.0017001480387730731\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0066155750813583535\n",
      "Average test loss: 0.0016169924487670262\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006608818285581138\n",
      "Average test loss: 0.001666381303841869\n",
      "Epoch 117/300\n",
      "Average training loss: 0.006590207611521085\n",
      "Average test loss: 0.0016428971896982854\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006587021405912108\n",
      "Average test loss: 0.0016176826727266112\n",
      "Epoch 119/300\n",
      "Average training loss: 0.006577297312931882\n",
      "Average test loss: 0.0016692514999045267\n",
      "Epoch 120/300\n",
      "Average training loss: 0.006572763978193203\n",
      "Average test loss: 0.0015807918059743114\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006570346321082777\n",
      "Average test loss: 0.00160721561136759\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006558294422510598\n",
      "Average test loss: 0.0016715375240892172\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006566051265431775\n",
      "Average test loss: 0.0016502647671020694\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006550142631348637\n",
      "Average test loss: 0.001629558425396681\n",
      "Epoch 125/300\n",
      "Average training loss: 0.006537771809018321\n",
      "Average test loss: 0.0016331313107576634\n",
      "Epoch 126/300\n",
      "Average training loss: 0.006533087226665683\n",
      "Average test loss: 0.0016527504440810945\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006513463230596648\n",
      "Average test loss: 0.001620885615857939\n",
      "Epoch 128/300\n",
      "Average training loss: 0.006506330310470528\n",
      "Average test loss: 0.001629303860788544\n",
      "Epoch 129/300\n",
      "Average training loss: 0.006513251870042748\n",
      "Average test loss: 0.001634599092002544\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006502728981690274\n",
      "Average test loss: 0.0016530213803052903\n",
      "Epoch 131/300\n",
      "Average training loss: 0.006497667188031806\n",
      "Average test loss: 0.001615829484537244\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006484581480423609\n",
      "Average test loss: 0.0016254936285937825\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006478508136338657\n",
      "Average test loss: 0.0016393372399939432\n",
      "Epoch 134/300\n",
      "Average training loss: 0.006470018025487662\n",
      "Average test loss: 0.0017401199979293677\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006483132760971785\n",
      "Average test loss: 0.0016212857571016584\n",
      "Epoch 136/300\n",
      "Average training loss: 0.006457804727057616\n",
      "Average test loss: 0.0017083319545620017\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00645948396747311\n",
      "Average test loss: 0.0017396814540649454\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006446738908274306\n",
      "Average test loss: 0.001665145308089753\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006435040968573755\n",
      "Average test loss: 0.0016824227322099936\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006439627825385995\n",
      "Average test loss: 0.0017027756877036559\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0064327573308514225\n",
      "Average test loss: 0.0016218315416740047\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006434727026770512\n",
      "Average test loss: 0.0017127142606510057\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006414910890161991\n",
      "Average test loss: 0.0017164893828125463\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0064128165716926255\n",
      "Average test loss: 0.0017125938911404875\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006413905368083053\n",
      "Average test loss: 0.001642041953591009\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0064037635744445855\n",
      "Average test loss: 0.00163383206890689\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006400855994886822\n",
      "Average test loss: 0.0016695457972172234\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0063898629074295365\n",
      "Average test loss: 0.0016501524102770619\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006382996485879024\n",
      "Average test loss: 0.0016110936102146903\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006393522487746345\n",
      "Average test loss: 0.001680016683621539\n",
      "Epoch 151/300\n",
      "Average training loss: 0.00637223784915275\n",
      "Average test loss: 0.0017060245248592563\n",
      "Epoch 152/300\n",
      "Average training loss: 0.006374079609496726\n",
      "Average test loss: 0.0016704994740171565\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006358849515517553\n",
      "Average test loss: 0.0016817812636080716\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006366742201149463\n",
      "Average test loss: 0.001727947938359446\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006352489956137207\n",
      "Average test loss: 0.0016457048698017994\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00635030964968933\n",
      "Average test loss: 0.0016913900701329113\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0063417270473308035\n",
      "Average test loss: 0.0016362264342606068\n",
      "Epoch 158/300\n",
      "Average training loss: 0.006338868118615614\n",
      "Average test loss: 0.001744454302514593\n",
      "Epoch 159/300\n",
      "Average training loss: 0.006325457248008913\n",
      "Average test loss: 0.001734395139126314\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0063337464965879916\n",
      "Average test loss: 0.0017072580749582912\n",
      "Epoch 161/300\n",
      "Average training loss: 0.006325606618490484\n",
      "Average test loss: 0.0016729430748770634\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006331230367223422\n",
      "Average test loss: 0.001697928171346171\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006319408321132263\n",
      "Average test loss: 0.001651642331129147\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0063080402302245295\n",
      "Average test loss: 0.0016471725911315945\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0063011150426334804\n",
      "Average test loss: 0.0017871638851033316\n",
      "Epoch 166/300\n",
      "Average training loss: 0.006306848308692376\n",
      "Average test loss: 0.0017105502616614103\n",
      "Epoch 167/300\n",
      "Average training loss: 0.006301431187325054\n",
      "Average test loss: 0.001661703904469808\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006288918341613478\n",
      "Average test loss: 0.0017966124307778146\n",
      "Epoch 169/300\n",
      "Average training loss: 0.006301781166758802\n",
      "Average test loss: 0.0016679056150217851\n",
      "Epoch 170/300\n",
      "Average training loss: 0.006278721859057744\n",
      "Average test loss: 0.0017131609068148665\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0062771681373318036\n",
      "Average test loss: 0.0016897188483013046\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006271114042235746\n",
      "Average test loss: 0.0017140877722866005\n",
      "Epoch 173/300\n",
      "Average training loss: 0.006268809147179127\n",
      "Average test loss: 0.0017085888949740263\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006269921214630206\n",
      "Average test loss: 0.0017167315388926202\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0062721216699315446\n",
      "Average test loss: 0.0018210181391073597\n",
      "Epoch 176/300\n",
      "Average training loss: 0.006256018267323573\n",
      "Average test loss: 0.0016558605918867721\n",
      "Epoch 177/300\n",
      "Average training loss: 0.006259995691478252\n",
      "Average test loss: 0.001657535306798915\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0062549862509800325\n",
      "Average test loss: 0.0017172490608774954\n",
      "Epoch 179/300\n",
      "Average training loss: 0.006242830612179306\n",
      "Average test loss: 0.0017130390937543578\n",
      "Epoch 180/300\n",
      "Average training loss: 0.006243959748910533\n",
      "Average test loss: 0.0016635924886084265\n",
      "Epoch 181/300\n",
      "Average training loss: 0.006252649663637082\n",
      "Average test loss: 0.00169738555389146\n",
      "Epoch 182/300\n",
      "Average training loss: 0.006229114192227522\n",
      "Average test loss: 0.001661932942457497\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006237172871414158\n",
      "Average test loss: 0.0016768331344549855\n",
      "Epoch 184/300\n",
      "Average training loss: 0.006225718412962225\n",
      "Average test loss: 0.0017218934761153327\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006229971768541468\n",
      "Average test loss: 0.0018262130932675468\n",
      "Epoch 186/300\n",
      "Average training loss: 0.006218591308842103\n",
      "Average test loss: 0.001709810620898174\n",
      "Epoch 187/300\n",
      "Average training loss: 0.006210631165239546\n",
      "Average test loss: 0.0017507344159401125\n",
      "Epoch 188/300\n",
      "Average training loss: 0.006206412630776564\n",
      "Average test loss: 0.0016690076603036788\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0062237017696930306\n",
      "Average test loss: 0.0016805974319577218\n",
      "Epoch 190/300\n",
      "Average training loss: 0.006205985432697667\n",
      "Average test loss: 0.001743446319260531\n",
      "Epoch 191/300\n",
      "Average training loss: 0.006213371785150634\n",
      "Average test loss: 0.0017145836752735905\n",
      "Epoch 192/300\n",
      "Average training loss: 0.006201099057164457\n",
      "Average test loss: 0.0016899925355489055\n",
      "Epoch 193/300\n",
      "Average training loss: 0.006202267068955634\n",
      "Average test loss: 0.0017514099149654309\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006191603063709206\n",
      "Average test loss: 0.0017678543993582328\n",
      "Epoch 195/300\n",
      "Average training loss: 0.006192158483796649\n",
      "Average test loss: 0.0017128872514391939\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0061899032160225844\n",
      "Average test loss: 0.001737120197671983\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00619347195327282\n",
      "Average test loss: 0.0017333833873271942\n",
      "Epoch 198/300\n",
      "Average training loss: 0.006174879690839185\n",
      "Average test loss: 0.0016887118785331647\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0061788119069404075\n",
      "Average test loss: 0.0017160583230563335\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0061758966582516825\n",
      "Average test loss: 0.0016969123422685597\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0061734015891949335\n",
      "Average test loss: 0.001748675271247824\n",
      "Epoch 202/300\n",
      "Average training loss: 0.006166597684017486\n",
      "Average test loss: 0.0018406646824959251\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0061676526334550645\n",
      "Average test loss: 0.001687658769182033\n",
      "Epoch 204/300\n",
      "Average training loss: 0.00618790400483542\n",
      "Average test loss: 0.0016664263730247816\n",
      "Epoch 205/300\n",
      "Average training loss: 0.006159521827267276\n",
      "Average test loss: 0.0016749885725892253\n",
      "Epoch 206/300\n",
      "Average training loss: 0.006155477857838074\n",
      "Average test loss: 0.0016870460980054406\n",
      "Epoch 207/300\n",
      "Average training loss: 0.006155510110159715\n",
      "Average test loss: 0.0017367771543148492\n",
      "Epoch 208/300\n",
      "Average training loss: 0.006141188221673171\n",
      "Average test loss: 0.0017060536348985301\n",
      "Epoch 209/300\n",
      "Average training loss: 0.006139373927066724\n",
      "Average test loss: 0.0017041179769568973\n",
      "Epoch 210/300\n",
      "Average training loss: 0.006156250433375438\n",
      "Average test loss: 0.0017458504513940877\n",
      "Epoch 211/300\n",
      "Average training loss: 0.006136529907170269\n",
      "Average test loss: 0.00170977865314732\n",
      "Epoch 212/300\n",
      "Average training loss: 0.006138298805803061\n",
      "Average test loss: 0.0017059758211382561\n",
      "Epoch 213/300\n",
      "Average training loss: 0.006132192980084154\n",
      "Average test loss: 0.0016440760075218147\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006131615368442403\n",
      "Average test loss: 0.0017690631825890805\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00614005550617973\n",
      "Average test loss: 0.0016613950974618396\n",
      "Epoch 216/300\n",
      "Average training loss: 0.006128569164623817\n",
      "Average test loss: 0.0016875808627034227\n",
      "Epoch 217/300\n",
      "Average training loss: 0.006138026232934661\n",
      "Average test loss: 0.0016987415020250612\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006117451479037602\n",
      "Average test loss: 0.001710303230314619\n",
      "Epoch 219/300\n",
      "Average training loss: 0.006126735764245192\n",
      "Average test loss: 0.0017339574567352732\n",
      "Epoch 220/300\n",
      "Average training loss: 0.006115987517353561\n",
      "Average test loss: 0.001682553436089721\n",
      "Epoch 221/300\n",
      "Average training loss: 0.006109617889755302\n",
      "Average test loss: 0.0017354354984644386\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006108570460644033\n",
      "Average test loss: 0.0016695596944126818\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00610817905391256\n",
      "Average test loss: 0.001692384758964181\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0061164787862863805\n",
      "Average test loss: 0.001735642145284348\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006114394384125869\n",
      "Average test loss: 0.0017076453981507156\n",
      "Epoch 226/300\n",
      "Average training loss: 0.006101177262763182\n",
      "Average test loss: 0.0017417159245039025\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00609959528181288\n",
      "Average test loss: 0.0017326020335571633\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006085436984482739\n",
      "Average test loss: 0.0016783491545874212\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006094051241874695\n",
      "Average test loss: 0.0017759790786852439\n",
      "Epoch 230/300\n",
      "Average training loss: 0.006091415625479486\n",
      "Average test loss: 0.0017145401696778005\n",
      "Epoch 231/300\n",
      "Average training loss: 0.006082445946418577\n",
      "Average test loss: 0.0016886994602779547\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006083944940732585\n",
      "Average test loss: 0.0017078163981851604\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00608570147678256\n",
      "Average test loss: 0.0017087176808466514\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0060805103584296175\n",
      "Average test loss: 0.001735739890899923\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006075470066318909\n",
      "Average test loss: 0.001740224981887473\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0060807681274082925\n",
      "Average test loss: 0.0017110853591519924\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00607768254685733\n",
      "Average test loss: 0.0017176574520352814\n",
      "Epoch 238/300\n",
      "Average training loss: 0.006074015285405848\n",
      "Average test loss: 0.0017441055457004242\n",
      "Epoch 239/300\n",
      "Average training loss: 0.006070661802258757\n",
      "Average test loss: 0.0017860846138662762\n",
      "Epoch 240/300\n",
      "Average training loss: 0.006064078759402037\n",
      "Average test loss: 0.001708222343793346\n",
      "Epoch 241/300\n",
      "Average training loss: 0.006059955474817091\n",
      "Average test loss: 0.0017519493090609709\n",
      "Epoch 242/300\n",
      "Average training loss: 0.006056785007731782\n",
      "Average test loss: 0.0017483479571528734\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006061092865963777\n",
      "Average test loss: 0.0017487042628021704\n",
      "Epoch 244/300\n",
      "Average training loss: 0.006067069871972005\n",
      "Average test loss: 0.001686112030202316\n",
      "Epoch 245/300\n",
      "Average training loss: 0.006062938064336777\n",
      "Average test loss: 0.0017311811235008968\n",
      "Epoch 246/300\n",
      "Average training loss: 0.006047686195207967\n",
      "Average test loss: 0.0018020071041666798\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0060487094186246395\n",
      "Average test loss: 0.0017683547497209576\n",
      "Epoch 248/300\n",
      "Average training loss: 0.006048488655024105\n",
      "Average test loss: 0.001677073046565056\n",
      "Epoch 249/300\n",
      "Average training loss: 0.006045283264170091\n",
      "Average test loss: 0.0017761044518815145\n",
      "Epoch 250/300\n",
      "Average training loss: 0.006045545886374182\n",
      "Average test loss: 0.0016947194608963197\n",
      "Epoch 251/300\n",
      "Average training loss: 0.006040858296884431\n",
      "Average test loss: 0.001763019647035334\n",
      "Epoch 252/300\n",
      "Average training loss: 0.006039062399417162\n",
      "Average test loss: 0.001735483235058685\n",
      "Epoch 253/300\n",
      "Average training loss: 0.00603605969912476\n",
      "Average test loss: 0.001760207053170436\n",
      "Epoch 254/300\n",
      "Average training loss: 0.006033403467386961\n",
      "Average test loss: 0.001828700949334436\n",
      "Epoch 255/300\n",
      "Average training loss: 0.006030197230892049\n",
      "Average test loss: 0.001731695188002454\n",
      "Epoch 256/300\n",
      "Average training loss: 0.006030894455396466\n",
      "Average test loss: 0.0017314914667771921\n",
      "Epoch 257/300\n",
      "Average training loss: 0.006035978306084871\n",
      "Average test loss: 0.001767815048392448\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006038440655503008\n",
      "Average test loss: 0.0017851199275917477\n",
      "Epoch 259/300\n",
      "Average training loss: 0.006022832719402181\n",
      "Average test loss: 0.001776609665558984\n",
      "Epoch 260/300\n",
      "Average training loss: 0.006018099865565697\n",
      "Average test loss: 0.0016797034533487425\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00602503218419022\n",
      "Average test loss: 0.0017961067776713106\n",
      "Epoch 262/300\n",
      "Average training loss: 0.006009498183925947\n",
      "Average test loss: 0.0017183943122832312\n",
      "Epoch 263/300\n",
      "Average training loss: 0.006014180242187447\n",
      "Average test loss: 0.0017115223778204785\n",
      "Epoch 264/300\n",
      "Average training loss: 0.006016006075673633\n",
      "Average test loss: 0.0017458497883958948\n",
      "Epoch 265/300\n",
      "Average training loss: 0.006014874409884214\n",
      "Average test loss: 0.0017478812699102692\n",
      "Epoch 266/300\n",
      "Average training loss: 0.006010892365127802\n",
      "Average test loss: 0.0016620676721342735\n",
      "Epoch 267/300\n",
      "Average training loss: 0.006007700450718403\n",
      "Average test loss: 0.0017857666292952167\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006006659605436855\n",
      "Average test loss: 0.0017663014270365238\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006006034089045392\n",
      "Average test loss: 0.0017342773604517182\n",
      "Epoch 270/300\n",
      "Average training loss: 0.006007251592559947\n",
      "Average test loss: 0.0017835403374499746\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006008812456909153\n",
      "Average test loss: 0.0018426025133166048\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005997846875339746\n",
      "Average test loss: 0.0017030057261387507\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0059914737910860116\n",
      "Average test loss: 0.0017355110792236195\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005991493305398358\n",
      "Average test loss: 0.001699486658287545\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005984789521329933\n",
      "Average test loss: 0.001748905106033716\n",
      "Epoch 276/300\n",
      "Average training loss: 0.006003899959226449\n",
      "Average test loss: 0.001708550564944744\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005997218201971717\n",
      "Average test loss: 0.0017587942874266042\n",
      "Epoch 278/300\n",
      "Average training loss: 0.005984765539152755\n",
      "Average test loss: 0.001786359549396568\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0059839585837390685\n",
      "Average test loss: 0.0017206919967300363\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0059735355658663645\n",
      "Average test loss: 0.00174873370408184\n",
      "Epoch 281/300\n",
      "Average training loss: 0.005982530031560196\n",
      "Average test loss: 0.0017713226777915326\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005982660289025969\n",
      "Average test loss: 0.0017439681228457226\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0059801476382546955\n",
      "Average test loss: 0.0017919696006509992\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0059790096738272245\n",
      "Average test loss: 0.001685304501093924\n",
      "Epoch 285/300\n",
      "Average training loss: 0.005985116065790256\n",
      "Average test loss: 0.0017468224087109168\n",
      "Epoch 286/300\n",
      "Average training loss: 0.005968954022973776\n",
      "Average test loss: 0.0017523926441661186\n",
      "Epoch 287/300\n",
      "Average training loss: 0.005975827183574438\n",
      "Average test loss: 0.001729830678552389\n",
      "Epoch 288/300\n",
      "Average training loss: 0.005973991411427656\n",
      "Average test loss: 0.0017297058473858568\n",
      "Epoch 289/300\n",
      "Average training loss: 0.005969717526601421\n",
      "Average test loss: 0.001670390299003985\n",
      "Epoch 290/300\n",
      "Average training loss: 0.005978888687988122\n",
      "Average test loss: 0.001749547020946112\n",
      "Epoch 291/300\n",
      "Average training loss: 0.005953988254070282\n",
      "Average test loss: 0.0017456049976042574\n",
      "Epoch 292/300\n",
      "Average training loss: 0.005965004621694485\n",
      "Average test loss: 0.0018027839548885821\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005954290192574262\n",
      "Average test loss: 0.001703826534975734\n",
      "Epoch 294/300\n",
      "Average training loss: 0.005957445385555427\n",
      "Average test loss: 0.0017294120032133329\n",
      "Epoch 295/300\n",
      "Average training loss: 0.005951820611953735\n",
      "Average test loss: 0.001747197320063909\n",
      "Epoch 296/300\n",
      "Average training loss: 0.005964617885649204\n",
      "Average test loss: 0.0017354068619509539\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005961428831434912\n",
      "Average test loss: 0.0017814846620894968\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005941043283376429\n",
      "Average test loss: 0.0017861933662659591\n",
      "Epoch 299/300\n",
      "Average training loss: 0.005954169206321239\n",
      "Average test loss: 0.001721497131097648\n",
      "Epoch 300/300\n",
      "Average training loss: 0.005944993624670638\n",
      "Average test loss: 0.0017283832954449786\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_50_Depth10/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.94\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.56\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 32.21\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.63\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.23\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.5788667881223892\n",
      "Average test loss: 0.0065891156937513086\n",
      "Epoch 2/300\n",
      "Average training loss: 1.3390401363372804\n",
      "Average test loss: 0.0050671883328921264\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8602399866316054\n",
      "Average test loss: 0.00465571296090881\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6112462402979533\n",
      "Average test loss: 0.004919682640996244\n",
      "Epoch 5/300\n",
      "Average training loss: 0.47217513065867955\n",
      "Average test loss: 0.0044380103920896845\n",
      "Epoch 6/300\n",
      "Average training loss: 0.38007955524656506\n",
      "Average test loss: 0.004389212381922536\n",
      "Epoch 7/300\n",
      "Average training loss: 0.31576552128791807\n",
      "Average test loss: 0.004326452308644851\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2651755509906345\n",
      "Average test loss: 0.004278537878559695\n",
      "Epoch 9/300\n",
      "Average training loss: 0.22658382666110993\n",
      "Average test loss: 0.00425004103117519\n",
      "Epoch 10/300\n",
      "Average training loss: 0.19597880334324308\n",
      "Average test loss: 0.004214634016570118\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17200246919525994\n",
      "Average test loss: 0.004287085121704472\n",
      "Epoch 12/300\n",
      "Average training loss: 0.15199546111292309\n",
      "Average test loss: 0.00421214009117749\n",
      "Epoch 13/300\n",
      "Average training loss: 0.13579462052053876\n",
      "Average test loss: 0.004146310194498963\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12188987912072076\n",
      "Average test loss: 0.004135330501943826\n",
      "Epoch 15/300\n",
      "Average training loss: 0.11078275157345666\n",
      "Average test loss: 0.004279962651845481\n",
      "Epoch 16/300\n",
      "Average training loss: 0.10124499962727229\n",
      "Average test loss: 0.004108371972623799\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09388576018810273\n",
      "Average test loss: 0.004120060873942242\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0876603131029341\n",
      "Average test loss: 0.0040703669450142315\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08247792975770103\n",
      "Average test loss: 0.004052404016256332\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07851209247774547\n",
      "Average test loss: 0.004055510176966588\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07499215551217397\n",
      "Average test loss: 0.004743739737818638\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0723479998641544\n",
      "Average test loss: 0.004065097209480074\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07013971677753661\n",
      "Average test loss: 0.004058258259461986\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06856938051184018\n",
      "Average test loss: 0.00399724454825951\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0670666954451137\n",
      "Average test loss: 0.003990523782041338\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06590208951632182\n",
      "Average test loss: 0.003978133980184794\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06487704350219832\n",
      "Average test loss: 0.0040367135775999895\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06413908101452721\n",
      "Average test loss: 0.003952474218896693\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06340479870637258\n",
      "Average test loss: 0.003965681500732899\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06288387154539427\n",
      "Average test loss: 0.0039568622534473736\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06228990839587317\n",
      "Average test loss: 0.003941484013158414\n",
      "Epoch 32/300\n",
      "Average training loss: 0.061907161427868736\n",
      "Average test loss: 0.003927099394301573\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06151100089814928\n",
      "Average test loss: 0.0039365570325818324\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06113235737548934\n",
      "Average test loss: 0.0039380479115578865\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06084196335077286\n",
      "Average test loss: 0.003982561233970854\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06060395502050717\n",
      "Average test loss: 0.003922630226446523\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06029329166478581\n",
      "Average test loss: 0.003899409681558609\n",
      "Epoch 38/300\n",
      "Average training loss: 0.060105262531174555\n",
      "Average test loss: 0.003928802753902144\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05982505513893233\n",
      "Average test loss: 0.003918179191235039\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05958695516983668\n",
      "Average test loss: 0.0038965785482691392\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05935208363665475\n",
      "Average test loss: 0.0038925276139958036\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05916590373383628\n",
      "Average test loss: 0.00390428952106999\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05910574139489068\n",
      "Average test loss: 0.003915263847758373\n",
      "Epoch 44/300\n",
      "Average training loss: 0.058776073058446246\n",
      "Average test loss: 0.0039645284588138265\n",
      "Epoch 45/300\n",
      "Average training loss: 0.058535879327191245\n",
      "Average test loss: 0.003871176099611653\n",
      "Epoch 46/300\n",
      "Average training loss: 0.058400211387210424\n",
      "Average test loss: 0.003878883809472124\n",
      "Epoch 47/300\n",
      "Average training loss: 0.058177713284889854\n",
      "Average test loss: 0.003907933868467808\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05794586865107219\n",
      "Average test loss: 0.003919342972131239\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0578041234711806\n",
      "Average test loss: 0.0039290703895191355\n",
      "Epoch 50/300\n",
      "Average training loss: 0.057603462600045736\n",
      "Average test loss: 0.0038989813754128086\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0574355193707678\n",
      "Average test loss: 0.0038763713294433224\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05728734078672197\n",
      "Average test loss: 0.003906632677548461\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05719326149755054\n",
      "Average test loss: 0.0038595172348949644\n",
      "Epoch 54/300\n",
      "Average training loss: 0.056930126623974904\n",
      "Average test loss: 0.0039062340011199316\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05673963438802295\n",
      "Average test loss: 0.003920390534318156\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05661754051182005\n",
      "Average test loss: 0.003911915588710044\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0565403969751464\n",
      "Average test loss: 0.003923932193468015\n",
      "Epoch 58/300\n",
      "Average training loss: 0.056219400859541364\n",
      "Average test loss: 0.0038878214359283447\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05613236351807912\n",
      "Average test loss: 0.003940489316773084\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05594124266836378\n",
      "Average test loss: 0.0038788742133312754\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0558211341533396\n",
      "Average test loss: 0.0039092351069880855\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05557985245850351\n",
      "Average test loss: 0.003876901410933998\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05548241763975885\n",
      "Average test loss: 0.003921501603391436\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05530926796131664\n",
      "Average test loss: 0.0038820871772865455\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05517444971203804\n",
      "Average test loss: 0.0038710294779804016\n",
      "Epoch 66/300\n",
      "Average training loss: 0.054972924595077834\n",
      "Average test loss: 0.004013784616771671\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05480801600880093\n",
      "Average test loss: 0.0039305831742369466\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05470723762114843\n",
      "Average test loss: 0.003959143316373229\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0545869847006268\n",
      "Average test loss: 0.00388758433661941\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05447652680012915\n",
      "Average test loss: 0.003959835280146864\n",
      "Epoch 71/300\n",
      "Average training loss: 0.054231444963150556\n",
      "Average test loss: 0.003983338062134054\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05404121665159861\n",
      "Average test loss: 0.003931637179520395\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05395138203435474\n",
      "Average test loss: 0.004036136417339246\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05374433365629779\n",
      "Average test loss: 0.0040822874220709\n",
      "Epoch 75/300\n",
      "Average training loss: 0.053688499914275274\n",
      "Average test loss: 0.00397117355465889\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05346036902732319\n",
      "Average test loss: 0.00394379123672843\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05334252358145184\n",
      "Average test loss: 0.0040060557025588224\n",
      "Epoch 78/300\n",
      "Average training loss: 0.053150997151931126\n",
      "Average test loss: 0.0039690291546285155\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05312111360496945\n",
      "Average test loss: 0.003967758865613076\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05291849428084162\n",
      "Average test loss: 0.003965754097741511\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0528335917658276\n",
      "Average test loss: 0.004006747327744961\n",
      "Epoch 82/300\n",
      "Average training loss: 0.052646768152713776\n",
      "Average test loss: 0.0039650436921252145\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05246485117740101\n",
      "Average test loss: 0.0038872904342909654\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05247765996721056\n",
      "Average test loss: 0.004027642469853163\n",
      "Epoch 85/300\n",
      "Average training loss: 0.052277432627148096\n",
      "Average test loss: 0.003928386869529883\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05219054176078902\n",
      "Average test loss: 0.00397372276087602\n",
      "Epoch 87/300\n",
      "Average training loss: 0.052024876717064114\n",
      "Average test loss: 0.00398862092072765\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05190482312109735\n",
      "Average test loss: 0.004040446584216422\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05171102037363582\n",
      "Average test loss: 0.004065945714712143\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05161147391796112\n",
      "Average test loss: 0.003998696077615022\n",
      "Epoch 91/300\n",
      "Average training loss: 0.051492839306592944\n",
      "Average test loss: 0.0040939813885423875\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05134804325964716\n",
      "Average test loss: 0.004021351668569777\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05124062700072924\n",
      "Average test loss: 0.004172497937869694\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05115860898296038\n",
      "Average test loss: 0.00406366704735491\n",
      "Epoch 95/300\n",
      "Average training loss: 0.051052546915080814\n",
      "Average test loss: 0.004071223526365227\n",
      "Epoch 96/300\n",
      "Average training loss: 0.050970821496513156\n",
      "Average test loss: 0.004010451751450698\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0508365418083138\n",
      "Average test loss: 0.004056248801036014\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05068680248988999\n",
      "Average test loss: 0.004023271488853627\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05061647253235181\n",
      "Average test loss: 0.004217591916728351\n",
      "Epoch 100/300\n",
      "Average training loss: 0.050498972280157935\n",
      "Average test loss: 0.003967793825599883\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05029438610540496\n",
      "Average test loss: 0.003983156136754486\n",
      "Epoch 102/300\n",
      "Average training loss: 0.050278803429669804\n",
      "Average test loss: 0.00408030145532555\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05020080661442545\n",
      "Average test loss: 0.004129416895823346\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05005567474828826\n",
      "Average test loss: 0.0041129458020958635\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04994336269299189\n",
      "Average test loss: 0.004028001030286153\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04986915762556924\n",
      "Average test loss: 0.003992686927939455\n",
      "Epoch 107/300\n",
      "Average training loss: 0.049785480830404495\n",
      "Average test loss: 0.00403730779265364\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04965704236096806\n",
      "Average test loss: 0.004056369482539594\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04955226014388932\n",
      "Average test loss: 0.004151706636365917\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04938469733463393\n",
      "Average test loss: 0.004082632924119632\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04931709326638116\n",
      "Average test loss: 0.004086659009671874\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04920680945780542\n",
      "Average test loss: 0.004191094817386733\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04924386600984467\n",
      "Average test loss: 0.0040434160211847885\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04908229639463955\n",
      "Average test loss: 0.004052823019317454\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04901099156339963\n",
      "Average test loss: 0.0040502231253518\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04887527359525363\n",
      "Average test loss: 0.0041684140484366155\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04883776234587034\n",
      "Average test loss: 0.004120169536934958\n",
      "Epoch 118/300\n",
      "Average training loss: 0.048696199811167185\n",
      "Average test loss: 0.004157476733542151\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04856841720806228\n",
      "Average test loss: 0.004145110654127267\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04850533730453915\n",
      "Average test loss: 0.00412822709315353\n",
      "Epoch 121/300\n",
      "Average training loss: 0.048513155145777594\n",
      "Average test loss: 0.004097411651164293\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04840267540017764\n",
      "Average test loss: 0.004058093371490637\n",
      "Epoch 123/300\n",
      "Average training loss: 0.048288522101110884\n",
      "Average test loss: 0.004193592997267842\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04819555629293124\n",
      "Average test loss: 0.004021161786797974\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04814308258228832\n",
      "Average test loss: 0.004180466313329008\n",
      "Epoch 126/300\n",
      "Average training loss: 0.047998072746727204\n",
      "Average test loss: 0.004249253330131372\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04792409583926201\n",
      "Average test loss: 0.004131162649641434\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04794689711266094\n",
      "Average test loss: 0.004106777332102259\n",
      "Epoch 129/300\n",
      "Average training loss: 0.047792830381128526\n",
      "Average test loss: 0.0041990466047492295\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04771331797043483\n",
      "Average test loss: 0.004191820515526665\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0478959069914288\n",
      "Average test loss: 0.004193473699192206\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04763107833928532\n",
      "Average test loss: 0.004182274347792069\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04747813931769795\n",
      "Average test loss: 0.004181670238160425\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04741543887058894\n",
      "Average test loss: 0.00419685313767857\n",
      "Epoch 135/300\n",
      "Average training loss: 0.047339831057522035\n",
      "Average test loss: 0.0041731341050730815\n",
      "Epoch 136/300\n",
      "Average training loss: 0.047382959427105055\n",
      "Average test loss: 0.004230234532720513\n",
      "Epoch 137/300\n",
      "Average training loss: 0.047174671994315256\n",
      "Average test loss: 0.00409979170271092\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04719646445910136\n",
      "Average test loss: 0.004235170718696383\n",
      "Epoch 139/300\n",
      "Average training loss: 0.047067458464039695\n",
      "Average test loss: 0.004247415129302276\n",
      "Epoch 140/300\n",
      "Average training loss: 0.046983198036750155\n",
      "Average test loss: 0.004144463703036308\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04690442064735625\n",
      "Average test loss: 0.004181992147324814\n",
      "Epoch 142/300\n",
      "Average training loss: 0.046857245928711364\n",
      "Average test loss: 0.0042797852017813255\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04680088379979134\n",
      "Average test loss: 0.004146970451705986\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04669156972898377\n",
      "Average test loss: 0.004226184726175334\n",
      "Epoch 145/300\n",
      "Average training loss: 0.046659382455878785\n",
      "Average test loss: 0.004145439124148753\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04658744334512287\n",
      "Average test loss: 0.00420097553357482\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04653743248184522\n",
      "Average test loss: 0.004251108500278659\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04647976845171716\n",
      "Average test loss: 0.004169683483946655\n",
      "Epoch 149/300\n",
      "Average training loss: 0.046353848381174935\n",
      "Average test loss: 0.0041510460765825374\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04632896059420374\n",
      "Average test loss: 0.004249307086898221\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04625407435827785\n",
      "Average test loss: 0.004131119470215506\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04624193833602799\n",
      "Average test loss: 0.004194668605923653\n",
      "Epoch 153/300\n",
      "Average training loss: 0.046188288182020185\n",
      "Average test loss: 0.004151954639703036\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04604457617799441\n",
      "Average test loss: 0.004223532827157113\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04600467353065808\n",
      "Average test loss: 0.004239321495923731\n",
      "Epoch 156/300\n",
      "Average training loss: 0.046022074490785596\n",
      "Average test loss: 0.004131896286581953\n",
      "Epoch 157/300\n",
      "Average training loss: 0.045905251377158694\n",
      "Average test loss: 0.00413061634865072\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04581532793243726\n",
      "Average test loss: 0.004183141100737784\n",
      "Epoch 159/300\n",
      "Average training loss: 0.045785985377099776\n",
      "Average test loss: 0.004077190248088705\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04573246377044254\n",
      "Average test loss: 0.004342544460048278\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04569517627689573\n",
      "Average test loss: 0.004240980270836088\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04561812230613497\n",
      "Average test loss: 0.004157350152317021\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04554360379775365\n",
      "Average test loss: 0.0042648347411304716\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04559851298729579\n",
      "Average test loss: 0.004250786621123552\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04543605770336257\n",
      "Average test loss: 0.0041706329404066\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04539346951246261\n",
      "Average test loss: 0.0042228752463642095\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04532535351978408\n",
      "Average test loss: 0.004302835079530875\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04526685910423597\n",
      "Average test loss: 0.0042650510478350855\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04525660632385148\n",
      "Average test loss: 0.004283947103346388\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04525997463862101\n",
      "Average test loss: 0.004226177297946479\n",
      "Epoch 171/300\n",
      "Average training loss: 0.045077340417438085\n",
      "Average test loss: 0.004319249567886194\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04508946038948165\n",
      "Average test loss: 0.004125699284589953\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04501232029663192\n",
      "Average test loss: 0.004186471103380124\n",
      "Epoch 174/300\n",
      "Average training loss: 0.044969840043120914\n",
      "Average test loss: 0.0042019877864254845\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04491620727380117\n",
      "Average test loss: 0.004263214003708628\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04484293936689695\n",
      "Average test loss: 0.004245324726733896\n",
      "Epoch 177/300\n",
      "Average training loss: 0.044818187137444815\n",
      "Average test loss: 0.004284624588572317\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04476090838180648\n",
      "Average test loss: 0.0042238631422321\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04472848820024067\n",
      "Average test loss: 0.004209105711430311\n",
      "Epoch 180/300\n",
      "Average training loss: 0.044644184678792956\n",
      "Average test loss: 0.004232768043461773\n",
      "Epoch 181/300\n",
      "Average training loss: 0.044632492245899304\n",
      "Average test loss: 0.004323694705549214\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04465774713953336\n",
      "Average test loss: 0.00431717804281248\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04452452045679092\n",
      "Average test loss: 0.004300437363485495\n",
      "Epoch 184/300\n",
      "Average training loss: 0.044531567149692114\n",
      "Average test loss: 0.004207726373440689\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04445128345489502\n",
      "Average test loss: 0.004175486078278886\n",
      "Epoch 186/300\n",
      "Average training loss: 0.044388503048155045\n",
      "Average test loss: 0.004334002536618047\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04432246501909362\n",
      "Average test loss: 0.004189745103319486\n",
      "Epoch 188/300\n",
      "Average training loss: 0.044290959616502125\n",
      "Average test loss: 0.004258962980161111\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04426854175329208\n",
      "Average test loss: 0.004171105743282371\n",
      "Epoch 190/300\n",
      "Average training loss: 0.044231397363874644\n",
      "Average test loss: 0.004233000569045544\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04420649254322052\n",
      "Average test loss: 0.004219257075753477\n",
      "Epoch 192/300\n",
      "Average training loss: 0.044117815961440404\n",
      "Average test loss: 0.004164037410583761\n",
      "Epoch 193/300\n",
      "Average training loss: 0.044064253664679\n",
      "Average test loss: 0.00427198933954868\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04402411263187726\n",
      "Average test loss: 0.004391091375922163\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04400483824147119\n",
      "Average test loss: 0.00417753635160625\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04393447822994656\n",
      "Average test loss: 0.004139001955174738\n",
      "Epoch 197/300\n",
      "Average training loss: 0.043922122021516165\n",
      "Average test loss: 0.00442779960255656\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0438355209297604\n",
      "Average test loss: 0.004214000533438391\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04386722027593189\n",
      "Average test loss: 0.004195139026062356\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04382290855050087\n",
      "Average test loss: 0.004251497998833656\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0437990846865707\n",
      "Average test loss: 0.004279489944378535\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04372388767037127\n",
      "Average test loss: 0.004177855184922616\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04369166018234359\n",
      "Average test loss: 0.004214082678572999\n",
      "Epoch 204/300\n",
      "Average training loss: 0.043691050906976066\n",
      "Average test loss: 0.004305101948893732\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04351946093307601\n",
      "Average test loss: 0.004378337473091152\n",
      "Epoch 206/300\n",
      "Average training loss: 0.043517744451761244\n",
      "Average test loss: 0.004326947824408611\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0434257135325008\n",
      "Average test loss: 0.004262112942006853\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04348159673810005\n",
      "Average test loss: 0.004219217189898094\n",
      "Epoch 209/300\n",
      "Average training loss: 0.043411032805840175\n",
      "Average test loss: 0.004225432403178678\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04346709582871861\n",
      "Average test loss: 0.004279722747703394\n",
      "Epoch 211/300\n",
      "Average training loss: 0.043321616113185885\n",
      "Average test loss: 0.004269184194919136\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04328476185599963\n",
      "Average test loss: 0.004280621664391623\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04325080905523565\n",
      "Average test loss: 0.004285832385429077\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04324587519301309\n",
      "Average test loss: 0.004237808376964596\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04325819901956452\n",
      "Average test loss: 0.004170577133488324\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04321495703359445\n",
      "Average test loss: 0.004256893523451355\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04302247149248918\n",
      "Average test loss: 0.004211555153752366\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04324386126796405\n",
      "Average test loss: 0.004231389467087057\n",
      "Epoch 219/300\n",
      "Average training loss: 0.043042958190043765\n",
      "Average test loss: 0.00440053028634025\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04309953220685323\n",
      "Average test loss: 0.0042644902803003785\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04300862118270662\n",
      "Average test loss: 0.00456023701487316\n",
      "Epoch 222/300\n",
      "Average training loss: 0.042931051154931385\n",
      "Average test loss: 0.004208597417092986\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04295396908620993\n",
      "Average test loss: 0.00427500400878489\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04289843882123629\n",
      "Average test loss: 0.004227648667991161\n",
      "Epoch 225/300\n",
      "Average training loss: 0.042793799536095724\n",
      "Average test loss: 0.004210321229365137\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04273674535751343\n",
      "Average test loss: 0.004397993012848827\n",
      "Epoch 227/300\n",
      "Average training loss: 0.042730177634292175\n",
      "Average test loss: 0.0042399083065489925\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04269681990312205\n",
      "Average test loss: 0.0042535710388587585\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0428735656870736\n",
      "Average test loss: 0.00422882899766167\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04262192826469739\n",
      "Average test loss: 0.004272489397062196\n",
      "Epoch 231/300\n",
      "Average training loss: 0.042597313741842904\n",
      "Average test loss: 0.004220452842820022\n",
      "Epoch 232/300\n",
      "Average training loss: 0.042619061105781134\n",
      "Average test loss: 0.0042940749157634045\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04255618382162518\n",
      "Average test loss: 0.004709465520663394\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04261064314511087\n",
      "Average test loss: 0.004455158738626374\n",
      "Epoch 235/300\n",
      "Average training loss: 0.042520647091997994\n",
      "Average test loss: 0.0043776794949339495\n",
      "Epoch 236/300\n",
      "Average training loss: 0.042417434195677436\n",
      "Average test loss: 0.004240828486366404\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04239195430609915\n",
      "Average test loss: 0.004237649372675353\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04238091394636366\n",
      "Average test loss: 0.004260575543675158\n",
      "Epoch 239/300\n",
      "Average training loss: 0.042436313837766644\n",
      "Average test loss: 0.004334514908078644\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04230494976374838\n",
      "Average test loss: 0.004213947877701786\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04236574175953865\n",
      "Average test loss: 0.004331488047829932\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04222449373867777\n",
      "Average test loss: 0.0041987730057703124\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04222115771472454\n",
      "Average test loss: 0.004650043137992422\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04220388811826706\n",
      "Average test loss: 0.0042905235994193285\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04218784550163481\n",
      "Average test loss: 0.004333983323226372\n",
      "Epoch 246/300\n",
      "Average training loss: 0.042131456898318397\n",
      "Average test loss: 0.004250678160124355\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0420553810497125\n",
      "Average test loss: 0.004334412705774109\n",
      "Epoch 248/300\n",
      "Average training loss: 0.042066342065731684\n",
      "Average test loss: 0.004345447806020578\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04205528673529625\n",
      "Average test loss: 0.004248543982704481\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04207179182436731\n",
      "Average test loss: 0.0043347066272464065\n",
      "Epoch 251/300\n",
      "Average training loss: 0.042003940727975635\n",
      "Average test loss: 0.004145186072422398\n",
      "Epoch 252/300\n",
      "Average training loss: 0.042000209675894844\n",
      "Average test loss: 0.004348976427068313\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04189106126295196\n",
      "Average test loss: 0.0042960108684168925\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04191353315114975\n",
      "Average test loss: 0.004285564805898401\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04183092256387075\n",
      "Average test loss: 0.004322394731558031\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04184696153302987\n",
      "Average test loss: 0.00437333444899155\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04184986670149697\n",
      "Average test loss: 0.004309148158050246\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04176423321167628\n",
      "Average test loss: 0.004258570009428594\n",
      "Epoch 259/300\n",
      "Average training loss: 0.041712284240457746\n",
      "Average test loss: 0.004222043783714374\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04177944355209669\n",
      "Average test loss: 0.004385220028460026\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04174835780925221\n",
      "Average test loss: 0.00433776666637924\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0416844842268361\n",
      "Average test loss: 0.004542624225839973\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04162679209311803\n",
      "Average test loss: 0.004432971272203657\n",
      "Epoch 264/300\n",
      "Average training loss: 0.041575651168823245\n",
      "Average test loss: 0.004204615435252587\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04158066481682989\n",
      "Average test loss: 0.004295978526274363\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04157333653668563\n",
      "Average test loss: 0.0043008548683590355\n",
      "Epoch 267/300\n",
      "Average training loss: 0.041522151990069284\n",
      "Average test loss: 0.004336404917140802\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04154515012436443\n",
      "Average test loss: 0.004502928985903661\n",
      "Epoch 269/300\n",
      "Average training loss: 0.041564653118451436\n",
      "Average test loss: 0.0044617572219835385\n",
      "Epoch 270/300\n",
      "Average training loss: 0.041422257446580464\n",
      "Average test loss: 0.004194723954631222\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04138343790173531\n",
      "Average test loss: 0.004351432700537973\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04138867317636808\n",
      "Average test loss: 0.004341654885353314\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04138799719015757\n",
      "Average test loss: 0.004417094499700599\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04136758451329337\n",
      "Average test loss: 0.004346918972829978\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04133961022562451\n",
      "Average test loss: 0.0042410014582177\n",
      "Epoch 276/300\n",
      "Average training loss: 0.041323299788766435\n",
      "Average test loss: 0.004349918942898512\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04122685139377912\n",
      "Average test loss: 0.004268982174081935\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04124857164091534\n",
      "Average test loss: 0.0043075161708725825\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04124853956037097\n",
      "Average test loss: 0.004290566838035981\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04125064011414846\n",
      "Average test loss: 0.004323285167002016\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04123504141966502\n",
      "Average test loss: 0.0043756298919518785\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04126492939392726\n",
      "Average test loss: 0.004220432763919234\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04108364363180266\n",
      "Average test loss: 0.004358413026564651\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04106295793586307\n",
      "Average test loss: 0.004160733941735493\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04113178590602345\n",
      "Average test loss: 0.004292076369126638\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04104721166690191\n",
      "Average test loss: 0.004324829150405195\n",
      "Epoch 287/300\n",
      "Average training loss: 0.041041377269559434\n",
      "Average test loss: 0.00435064390508665\n",
      "Epoch 288/300\n",
      "Average training loss: 0.041046969811121624\n",
      "Average test loss: 0.0043145844866004255\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04098038432333204\n",
      "Average test loss: 0.004294189687818289\n",
      "Epoch 290/300\n",
      "Average training loss: 0.040930650330252116\n",
      "Average test loss: 0.004387427630523841\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04093229721321\n",
      "Average test loss: 0.004287730141646332\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04093436922960811\n",
      "Average test loss: 0.004446376657320394\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04089545651276906\n",
      "Average test loss: 0.004342155449920231\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04088177550832431\n",
      "Average test loss: 0.004302245843741629\n",
      "Epoch 295/300\n",
      "Average training loss: 0.040825423141320545\n",
      "Average test loss: 0.004389441740595632\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0408077884680695\n",
      "Average test loss: 0.0042567676347162985\n",
      "Epoch 297/300\n",
      "Average training loss: 0.040791112340158885\n",
      "Average test loss: 0.004164141338318586\n",
      "Epoch 298/300\n",
      "Average training loss: 0.040751950714323255\n",
      "Average test loss: 0.004335795647982094\n",
      "Epoch 299/300\n",
      "Average test loss: 0.004505599358429512\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04075348653395971\n",
      "Average test loss: 0.0044651819399247564\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.9582437540690103\n",
      "Average test loss: 0.006295042063213057\n",
      "Epoch 2/300\n",
      "Average training loss: 1.3665612046983506\n",
      "Average test loss: 0.004780117393781741\n",
      "Epoch 3/300\n",
      "Average training loss: 0.9247850103378296\n",
      "Average test loss: 0.011225517650445303\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6553550803926256\n",
      "Average test loss: 0.004008084101809395\n",
      "Epoch 5/300\n",
      "Average training loss: 0.47731333518028257\n",
      "Average test loss: 0.003913542518185245\n",
      "Epoch 6/300\n",
      "Average training loss: 0.3715126663313972\n",
      "Average test loss: 0.0041392605341970925\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2983457244369719\n",
      "Average test loss: 0.00376947015337646\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2458878604835934\n",
      "Average test loss: 0.0036568327848282125\n",
      "Epoch 9/300\n",
      "Average training loss: 0.20685194741355048\n",
      "Average test loss: 0.003807944883695907\n",
      "Epoch 10/300\n",
      "Average training loss: 0.17668096894688076\n",
      "Average test loss: 0.006641150489656461\n",
      "Epoch 11/300\n",
      "Average training loss: 0.15260494507683647\n",
      "Average test loss: 0.003512913745517532\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1333150710132387\n",
      "Average test loss: 0.003462174733893739\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11774387361605962\n",
      "Average test loss: 0.0034034971007042463\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10480940218766531\n",
      "Average test loss: 0.011625180158764124\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09458615560001797\n",
      "Average test loss: 0.0033794154406835636\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08634019539753596\n",
      "Average test loss: 0.003320061596317424\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07959260599480734\n",
      "Average test loss: 0.0032422277571426497\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07424957568446795\n",
      "Average test loss: 0.0032195104586167467\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06992904877000385\n",
      "Average test loss: 0.003195128472728862\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06628880187537935\n",
      "Average test loss: 0.003162946470081806\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06339036579264534\n",
      "Average test loss: 0.0031322766817692255\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06105503437585301\n",
      "Average test loss: 0.003089298399993115\n",
      "Epoch 23/300\n",
      "Average training loss: 0.059032551950878566\n",
      "Average test loss: 0.0032255733956893283\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05747063102655941\n",
      "Average test loss: 0.0030616248936081924\n",
      "Epoch 25/300\n",
      "Average training loss: 0.056025951928562585\n",
      "Average test loss: 0.0030347857009619473\n",
      "Epoch 26/300\n",
      "Average training loss: 0.054894469443294736\n",
      "Average test loss: 0.0030174095405058727\n",
      "Epoch 27/300\n",
      "Average training loss: 0.054449454661872655\n",
      "Average test loss: 0.0029916893848114545\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05309430327018102\n",
      "Average test loss: 0.0030033663345707786\n",
      "Epoch 29/300\n",
      "Average training loss: 0.052240251859029134\n",
      "Average test loss: 0.003003068459323711\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0516011214322514\n",
      "Average test loss: 0.0029803572793801626\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05087735826108191\n",
      "Average test loss: 0.002976335445832875\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05027379331654973\n",
      "Average test loss: 0.002954306800332334\n",
      "Epoch 33/300\n",
      "Average training loss: 0.049703970279958515\n",
      "Average test loss: 0.002939796688449052\n",
      "Epoch 34/300\n",
      "Average training loss: 0.049194612489806284\n",
      "Average test loss: 0.002911047180700633\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04868451181716389\n",
      "Average test loss: 0.002901061473828223\n",
      "Epoch 36/300\n",
      "Average training loss: 0.048314696494075986\n",
      "Average test loss: 0.0029471237315899794\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04787373650405142\n",
      "Average test loss: 0.0028907642865346537\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04747182684474521\n",
      "Average test loss: 0.002878012598181764\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04702693250113063\n",
      "Average test loss: 0.0030146524007949563\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04663249515493711\n",
      "Average test loss: 0.0029125407634096015\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04624336742526955\n",
      "Average test loss: 0.0029377072354157766\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0459553216430876\n",
      "Average test loss: 0.0028619164013200336\n",
      "Epoch 43/300\n",
      "Average training loss: 0.045606160627471075\n",
      "Average test loss: 0.002891805844588412\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04525332269072533\n",
      "Average test loss: 0.00286310557089746\n",
      "Epoch 45/300\n",
      "Average training loss: 0.044833476785156465\n",
      "Average test loss: 0.0028796422185583246\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04461318594879574\n",
      "Average test loss: 0.002922326128722893\n",
      "Epoch 47/300\n",
      "Average training loss: 0.044249070992072426\n",
      "Average test loss: 0.0029421292572385737\n",
      "Epoch 48/300\n",
      "Average training loss: 0.043990390522612464\n",
      "Average test loss: 0.00283622163016763\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04357359770933787\n",
      "Average test loss: 0.002898295542018281\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04335874957011806\n",
      "Average test loss: 0.003033086571428511\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04298224619030953\n",
      "Average test loss: 0.0028584555170188346\n",
      "Epoch 52/300\n",
      "Average training loss: 0.042655562390883764\n",
      "Average test loss: 0.002859758796584275\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04239134392639001\n",
      "Average test loss: 0.002956513847120934\n",
      "Epoch 54/300\n",
      "Average training loss: 0.042098087463114\n",
      "Average test loss: 0.002934363495144579\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04177132930689388\n",
      "Average test loss: 0.0028844295946053333\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04156024690800243\n",
      "Average test loss: 0.0028378305438285074\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04131187699569596\n",
      "Average test loss: 0.0028660732192090817\n",
      "Epoch 58/300\n",
      "Average training loss: 0.041083040323522356\n",
      "Average test loss: 0.0028205535070349773\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04090256472594208\n",
      "Average test loss: 0.0028276874197440014\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04053798260622554\n",
      "Average test loss: 0.0028274824522021742\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04019572029842271\n",
      "Average test loss: 0.0029277162473234867\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04000678144395351\n",
      "Average test loss: 0.0028921824285967485\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03977595793207486\n",
      "Average test loss: 0.0028699468187987805\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03953585441410541\n",
      "Average test loss: 0.002898940770369437\n",
      "Epoch 65/300\n",
      "Average training loss: 0.039337058901786806\n",
      "Average test loss: 0.002949699714365933\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03906057364245256\n",
      "Average test loss: 0.0030085122456981078\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03886346923311551\n",
      "Average test loss: 0.002874571158240239\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03868192123538918\n",
      "Average test loss: 0.002915008867366446\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03856062877178192\n",
      "Average test loss: 0.0029035904658958316\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03828732964727614\n",
      "Average test loss: 0.0028709170118802123\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03810202660825517\n",
      "Average test loss: 0.0029449263299918837\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03790783456630177\n",
      "Average test loss: 0.0028908510704835255\n",
      "Epoch 73/300\n",
      "Average training loss: 0.037741463234027224\n",
      "Average test loss: 0.002917899484021796\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03750512804918819\n",
      "Average test loss: 0.002966114647479521\n",
      "Epoch 75/300\n",
      "Average training loss: 0.037449854569302665\n",
      "Average test loss: 0.002890707518181039\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03728539017836253\n",
      "Average test loss: 0.0028904144850869975\n",
      "Epoch 77/300\n",
      "Average training loss: 0.037045245501730176\n",
      "Average test loss: 0.003104104663969742\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03686263801985317\n",
      "Average test loss: 0.0029596491562616494\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0367348660942581\n",
      "Average test loss: 0.0029738362760593495\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03664639231065909\n",
      "Average test loss: 0.002913631655482782\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03654709122081597\n",
      "Average test loss: 0.0029449077660424843\n",
      "Epoch 82/300\n",
      "Average training loss: 0.036325204226705765\n",
      "Average test loss: 0.002902726398780942\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03625124932659997\n",
      "Average test loss: 0.0029097199264085955\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0360810674627622\n",
      "Average test loss: 0.003007840343233612\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03599670065773858\n",
      "Average test loss: 0.002944225086727076\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03582349050707287\n",
      "Average test loss: 0.002979501131715046\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03571843008365896\n",
      "Average test loss: 0.003150179244991806\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03559354767534468\n",
      "Average test loss: 0.0029433949968467156\n",
      "Epoch 89/300\n",
      "Average training loss: 0.035519412948025596\n",
      "Average test loss: 0.003079839392668671\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03542437603738573\n",
      "Average test loss: 0.0029270326437221632\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03520496649543444\n",
      "Average test loss: 0.002976634562636415\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0350622824612591\n",
      "Average test loss: 0.0031235144997222557\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03500103543202082\n",
      "Average test loss: 0.0030352211762219665\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03493875298566288\n",
      "Average test loss: 0.00303981270102991\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03480236166053348\n",
      "Average test loss: 0.0030768818685577974\n",
      "Epoch 96/300\n",
      "Average training loss: 0.034792659964826375\n",
      "Average test loss: 0.0030029209442436694\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03458415038221412\n",
      "Average test loss: 0.002998671077812711\n",
      "Epoch 98/300\n",
      "Average training loss: 0.034488447063499024\n",
      "Average test loss: 0.0029068447028597197\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03443698792656263\n",
      "Average test loss: 0.003023510081900491\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03436933654546738\n",
      "Average test loss: 0.0029629057637519307\n",
      "Epoch 101/300\n",
      "Average training loss: 0.034259787950250836\n",
      "Average test loss: 0.002954975869713558\n",
      "Epoch 102/300\n",
      "Average training loss: 0.034184834443860584\n",
      "Average test loss: 0.00297198036333753\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03404115658005079\n",
      "Average test loss: 0.0031547748426803282\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03392604582342837\n",
      "Average test loss: 0.002951030034261445\n",
      "Epoch 105/300\n",
      "Average training loss: 0.033837078086204\n",
      "Average test loss: 0.003076150876780351\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03374979599151347\n",
      "Average test loss: 0.0029711382240056993\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03369832984606425\n",
      "Average test loss: 0.0030367426427288188\n",
      "Epoch 108/300\n",
      "Average training loss: 0.033648584882418314\n",
      "Average test loss: 0.003009433409406079\n",
      "Epoch 109/300\n",
      "Average training loss: 0.033654039753807916\n",
      "Average test loss: 0.003035734806623724\n",
      "Epoch 110/300\n",
      "Average training loss: 0.033409502683414354\n",
      "Average test loss: 0.0030098137454026277\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03341870289047559\n",
      "Average test loss: 0.0030219912193715573\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0333782947825061\n",
      "Average test loss: 0.003175348211907678\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03329189212785827\n",
      "Average test loss: 0.0030490494368390904\n",
      "Epoch 114/300\n",
      "Average training loss: 0.033181986009081205\n",
      "Average test loss: 0.0030034185339593224\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0330690285017093\n",
      "Average test loss: 0.003109803338224689\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03298058992458714\n",
      "Average test loss: 0.0030042829539419875\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03299452214770847\n",
      "Average test loss: 0.003090313280829125\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03296907769474718\n",
      "Average test loss: 0.0032218290713305273\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03284961440662543\n",
      "Average test loss: 0.002990644130234917\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03272116458581554\n",
      "Average test loss: 0.0030856794027818574\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03267206262714333\n",
      "Average test loss: 0.0031094570176468956\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03257080082264211\n",
      "Average test loss: 0.0031112085996816558\n",
      "Epoch 123/300\n",
      "Average training loss: 0.032558420177963045\n",
      "Average test loss: 0.0030291986337138547\n",
      "Epoch 124/300\n",
      "Average training loss: 0.032516150045726035\n",
      "Average test loss: 0.0030282643257329863\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03245207290351391\n",
      "Average test loss: 0.0030621248634739055\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03235315523048242\n",
      "Average test loss: 0.0030244526432620157\n",
      "Epoch 127/300\n",
      "Average training loss: 0.032321742819415196\n",
      "Average test loss: 0.003017690064592494\n",
      "Epoch 128/300\n",
      "Average training loss: 0.032185823791556885\n",
      "Average test loss: 0.0030144593924697903\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03225443878769874\n",
      "Average test loss: 0.003180577817062537\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03219407545361254\n",
      "Average test loss: 0.0030676683669702874\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03210280647046036\n",
      "Average test loss: 0.0031084179853399594\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0321186467293236\n",
      "Average test loss: 0.003071535194499625\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0319412955807315\n",
      "Average test loss: 0.003053519365688165\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03206448971231778\n",
      "Average test loss: 0.002974629977821476\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03187547323935562\n",
      "Average test loss: 0.003119080275297165\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031822464138269425\n",
      "Average test loss: 0.003111683066934347\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03178250541455216\n",
      "Average test loss: 0.0031205591505600347\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03186578094628122\n",
      "Average test loss: 0.003033210864911477\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03174934659567144\n",
      "Average test loss: 0.003131853621866968\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03157732694182131\n",
      "Average test loss: 0.0031047942735668685\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0315791309343444\n",
      "Average test loss: 0.003132571428393324\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03150067205230395\n",
      "Average test loss: 0.0031322866727908454\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03143979295425945\n",
      "Average test loss: 0.0031173554913451274\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0314150720089674\n",
      "Average test loss: 0.00310435293842521\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03138046901590294\n",
      "Average test loss: 0.0030204266127612854\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03139569204714563\n",
      "Average test loss: 0.00302426773806413\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03131228734718429\n",
      "Average test loss: 0.003153237065921227\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03127795156339804\n",
      "Average test loss: 0.0030711031450579565\n",
      "Epoch 149/300\n",
      "Average training loss: 0.031170893806550238\n",
      "Average test loss: 0.0030757197736658985\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03118692176706261\n",
      "Average test loss: 0.0030646200536025895\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03117981595959928\n",
      "Average test loss: 0.003113585980402099\n",
      "Epoch 152/300\n",
      "Average training loss: 0.031042894621690114\n",
      "Average test loss: 0.0030390912940104803\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03104564948214425\n",
      "Average test loss: 0.003069304571383529\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03105307420094808\n",
      "Average test loss: 0.0030824760877423817\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03096485265592734\n",
      "Average test loss: 0.003169971029791567\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030886164536078772\n",
      "Average test loss: 0.003183893385860655\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030861887140406503\n",
      "Average test loss: 0.0030820799209177493\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030805372262994447\n",
      "Average test loss: 0.0031211703543861705\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03080937355922328\n",
      "Average test loss: 0.00310343241919246\n",
      "Epoch 160/300\n",
      "Average training loss: 0.030767951764994198\n",
      "Average test loss: 0.0030931133925914765\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03063886435329914\n",
      "Average test loss: 0.003035766400500304\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030686556342575284\n",
      "Average test loss: 0.003164678739797738\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030671898507409624\n",
      "Average test loss: 0.003182687936143743\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03058023397458924\n",
      "Average test loss: 0.0031368462985588443\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030622928139236238\n",
      "Average test loss: 0.003149179576171769\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0304879899604453\n",
      "Average test loss: 0.0030867494580646354\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030586965155270363\n",
      "Average test loss: 0.0031889319291545284\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030542593671215906\n",
      "Average test loss: 0.0031900980952713227\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030399888002210194\n",
      "Average test loss: 0.003254985612920589\n",
      "Epoch 170/300\n",
      "Average training loss: 0.030270721859402126\n",
      "Average test loss: 0.003052172702116271\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03033786808947722\n",
      "Average test loss: 0.0030878585715674693\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030289198365476395\n",
      "Average test loss: 0.0031168965908388298\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03023611149688562\n",
      "Average test loss: 0.003132892026255528\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03025330324967702\n",
      "Average test loss: 0.003065564701230162\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03020457042256991\n",
      "Average test loss: 0.003107518735445208\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030244584595163663\n",
      "Average test loss: 0.003194111140444875\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03021237392226855\n",
      "Average test loss: 0.003144453762513068\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030129761904478074\n",
      "Average test loss: 0.003144895817463597\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03003928267624643\n",
      "Average test loss: 0.0031379510952780645\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03004770247803794\n",
      "Average test loss: 0.003115600583453973\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02994956756134828\n",
      "Average test loss: 0.0033164977677580384\n",
      "Epoch 182/300\n",
      "Average training loss: 0.029963459495041107\n",
      "Average test loss: 0.0031227338191949657\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029961733938919172\n",
      "Average test loss: 0.0031229204398890336\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0299214299602641\n",
      "Average test loss: 0.003120986781186528\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0298757607307699\n",
      "Average test loss: 0.0031414715074416662\n",
      "Epoch 186/300\n",
      "Average training loss: 0.029839630416697925\n",
      "Average test loss: 0.003135006546560261\n",
      "Epoch 187/300\n",
      "Average training loss: 0.029780984186463886\n",
      "Average test loss: 0.003189114401323928\n",
      "Epoch 188/300\n",
      "Average training loss: 0.029767485769258605\n",
      "Average test loss: 0.003130015359984504\n",
      "Epoch 189/300\n",
      "Average training loss: 0.029766649888621436\n",
      "Average test loss: 0.0031116052435504068\n",
      "Epoch 190/300\n",
      "Average training loss: 0.029686114927132926\n",
      "Average test loss: 0.003144748071829478\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02964870064291689\n",
      "Average test loss: 0.003146823734872871\n",
      "Epoch 192/300\n",
      "Average training loss: 0.029707721991671456\n",
      "Average test loss: 0.0031875457918892304\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029678967227538426\n",
      "Average test loss: 0.0032065754437612163\n",
      "Epoch 194/300\n",
      "Average training loss: 0.029662758795751466\n",
      "Average test loss: 0.0031040097669594816\n",
      "Epoch 195/300\n",
      "Average training loss: 0.029672764821185006\n",
      "Average test loss: 0.003175048517891102\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029524639926022955\n",
      "Average test loss: 0.003193439584639337\n",
      "Epoch 197/300\n",
      "Average training loss: 0.029528543043467734\n",
      "Average test loss: 0.003112737597483728\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02951384858124786\n",
      "Average test loss: 0.0031423620502981876\n",
      "Epoch 199/300\n",
      "Average training loss: 0.029428891211748125\n",
      "Average test loss: 0.003094856517596377\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02945745456384288\n",
      "Average test loss: 0.0031605920655032\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02935412439207236\n",
      "Average test loss: 0.0032036481214066347\n",
      "Epoch 202/300\n",
      "Average training loss: 0.029357701763510703\n",
      "Average test loss: 0.0031718579168534943\n",
      "Epoch 203/300\n",
      "Average training loss: 0.029320111581020884\n",
      "Average test loss: 0.003172410074207518\n",
      "Epoch 204/300\n",
      "Average training loss: 0.029411128393477863\n",
      "Average test loss: 0.00317181859869096\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029376152472363578\n",
      "Average test loss: 0.003110596815124154\n",
      "Epoch 206/300\n",
      "Average training loss: 0.029326921903424792\n",
      "Average test loss: 0.0031562623685846727\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02921291824016306\n",
      "Average test loss: 0.003194987597150935\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029235887063874137\n",
      "Average test loss: 0.0031349589292787843\n",
      "Epoch 209/300\n",
      "Average training loss: 0.029243401570452583\n",
      "Average test loss: 0.003114998082940777\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029147191633780798\n",
      "Average test loss: 0.003218214931173457\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029103187912040286\n",
      "Average test loss: 0.003166087871624364\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02911613840361436\n",
      "Average test loss: 0.0032340548572440944\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0291259353177415\n",
      "Average test loss: 0.003269671443849802\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02900872928235266\n",
      "Average test loss: 0.0031193781660662757\n",
      "Epoch 215/300\n",
      "Average training loss: 0.029090755686163903\n",
      "Average test loss: 0.003226963018791543\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02903479154739115\n",
      "Average test loss: 0.003186204401569234\n",
      "Epoch 217/300\n",
      "Average training loss: 0.029037401735782623\n",
      "Average test loss: 0.0030845537580963638\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02903806844353676\n",
      "Average test loss: 0.0032202678790522947\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02894674742884106\n",
      "Average test loss: 0.003225932576176193\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028903454133205945\n",
      "Average test loss: 0.0032060661773300834\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028897361882858807\n",
      "Average test loss: 0.003243773136494888\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028880662532316314\n",
      "Average test loss: 0.0031308478354993793\n",
      "Epoch 223/300\n",
      "Average training loss: 0.028918036681082514\n",
      "Average test loss: 0.003164240152057674\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028798775987492666\n",
      "Average test loss: 0.0032051796979374356\n",
      "Epoch 225/300\n",
      "Average training loss: 0.028785175338387488\n",
      "Average test loss: 0.0031359491929825807\n",
      "Epoch 226/300\n",
      "Average training loss: 0.028797579578227466\n",
      "Average test loss: 0.0031726002374456988\n",
      "Epoch 227/300\n",
      "Average training loss: 0.028824718841248088\n",
      "Average test loss: 0.003133704888116982\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02877311744292577\n",
      "Average test loss: 0.0031863560072249836\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02874396112561226\n",
      "Average test loss: 0.0032618626647939286\n",
      "Epoch 230/300\n",
      "Average training loss: 0.028781196407145923\n",
      "Average test loss: 0.0030587269783847863\n",
      "Epoch 231/300\n",
      "Average training loss: 0.028712995317247178\n",
      "Average test loss: 0.0031824467368423938\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02870985083281994\n",
      "Average test loss: 0.003243275469375981\n",
      "Epoch 233/300\n",
      "Average training loss: 0.028657320456372367\n",
      "Average test loss: 0.0031926457656340466\n",
      "Epoch 234/300\n",
      "Average training loss: 0.028621876882182228\n",
      "Average test loss: 0.003174381013131804\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02860309178299374\n",
      "Average test loss: 0.0032274633476303684\n",
      "Epoch 236/300\n",
      "Average training loss: 0.028537491167585055\n",
      "Average test loss: 0.0032010954341126813\n",
      "Epoch 237/300\n",
      "Average training loss: 0.028525837134983804\n",
      "Average test loss: 0.0031834791110207637\n",
      "Epoch 238/300\n",
      "Average training loss: 0.028542343339986273\n",
      "Average test loss: 0.003166861952179008\n",
      "Epoch 239/300\n",
      "Average training loss: 0.028508903611037466\n",
      "Average test loss: 0.003268769863165087\n",
      "Epoch 240/300\n",
      "Average training loss: 0.028457610623704062\n",
      "Average test loss: 0.003205096031228701\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02849102484848764\n",
      "Average test loss: 0.0031871852794041235\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02848132511973381\n",
      "Average test loss: 0.003247730968727006\n",
      "Epoch 243/300\n",
      "Average training loss: 0.028442360619703927\n",
      "Average test loss: 0.0031275852173566817\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02840121916267607\n",
      "Average test loss: 0.0031348546147346498\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02843999636835522\n",
      "Average test loss: 0.0031699007730931045\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02836284585131539\n",
      "Average test loss: 0.0032318241120212607\n",
      "Epoch 247/300\n",
      "Average training loss: 0.028343166212240856\n",
      "Average test loss: 0.0031397736725500888\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0283935689760579\n",
      "Average test loss: 0.0032015536760704386\n",
      "Epoch 249/300\n",
      "Average training loss: 0.028247016357051003\n",
      "Average test loss: 0.0031337841076569425\n",
      "Epoch 250/300\n",
      "Average training loss: 0.028276151004764768\n",
      "Average test loss: 0.0032289791732198663\n",
      "Epoch 251/300\n",
      "Average training loss: 0.028257153302431108\n",
      "Average test loss: 0.003166064598908027\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02821694094936053\n",
      "Average test loss: 0.0031486051339242194\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02823057523369789\n",
      "Average test loss: 0.003176671812310815\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02835595292846362\n",
      "Average test loss: 0.0031736380046026576\n",
      "Epoch 255/300\n",
      "Average training loss: 0.028199515551328658\n",
      "Average test loss: 0.0033139714237509503\n",
      "Epoch 256/300\n",
      "Average training loss: 0.028115024934212366\n",
      "Average test loss: 0.003227221882591645\n",
      "Epoch 257/300\n",
      "Average training loss: 0.028140757569008402\n",
      "Average test loss: 0.0032077456133233176\n",
      "Epoch 258/300\n",
      "Average training loss: 0.028184784885909822\n",
      "Average test loss: 0.0031392791769984697\n",
      "Epoch 259/300\n",
      "Average training loss: 0.028112501410974395\n",
      "Average test loss: 0.0032567739085190826\n",
      "Epoch 260/300\n",
      "Average training loss: 0.028114477237065633\n",
      "Average test loss: 0.003236551589849922\n",
      "Epoch 261/300\n",
      "Average training loss: 0.028151799412237274\n",
      "Average test loss: 0.003249378768520223\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02808917014963097\n",
      "Average test loss: 0.0032312953921241894\n",
      "Epoch 263/300\n",
      "Average training loss: 0.028032994841535885\n",
      "Average test loss: 0.0032998625300824642\n",
      "Epoch 264/300\n",
      "Average training loss: 0.028043624445796014\n",
      "Average test loss: 0.0032105910579363506\n",
      "Epoch 265/300\n",
      "Average training loss: 0.028003425493836404\n",
      "Average test loss: 0.003196193971878125\n",
      "Epoch 266/300\n",
      "Average training loss: 0.028068343427446154\n",
      "Average test loss: 0.003229337445149819\n",
      "Epoch 267/300\n",
      "Average training loss: 0.027940141010615562\n",
      "Average test loss: 0.003157296287102832\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02794195792078972\n",
      "Average test loss: 0.003158595398068428\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02792542181743516\n",
      "Average test loss: 0.003191689948240916\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02801736492415269\n",
      "Average test loss: 0.0033500327687296604\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027949436381459237\n",
      "Average test loss: 0.0032710144279731643\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027865173942512936\n",
      "Average test loss: 0.003208532930041353\n",
      "Epoch 273/300\n",
      "Average training loss: 0.027841607289181815\n",
      "Average test loss: 0.003237764746985502\n",
      "Epoch 274/300\n",
      "Average training loss: 0.027890081024832196\n",
      "Average test loss: 0.003244240137437979\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02782785974111822\n",
      "Average test loss: 0.0032634116661631398\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02778637962705559\n",
      "Average test loss: 0.003221620821290546\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02780365486939748\n",
      "Average test loss: 0.003279601015150547\n",
      "Epoch 278/300\n",
      "Average training loss: 0.027792009542385736\n",
      "Average test loss: 0.0032328073136094542\n",
      "Epoch 279/300\n",
      "Average training loss: 0.027794521000650194\n",
      "Average test loss: 0.0031619612069593538\n",
      "Epoch 280/300\n",
      "Average training loss: 0.027747400497396785\n",
      "Average test loss: 0.0032289594581557646\n",
      "Epoch 281/300\n",
      "Average training loss: 0.027689201010598078\n",
      "Average test loss: 0.003276514611310429\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027732438524564106\n",
      "Average test loss: 0.0032197584273914496\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02773268844683965\n",
      "Average test loss: 0.00313924807537761\n",
      "Epoch 284/300\n",
      "Average training loss: 0.027739820102850594\n",
      "Average test loss: 0.0032819969231883683\n",
      "Epoch 285/300\n",
      "Average training loss: 0.027697338490022552\n",
      "Average test loss: 0.0031829549252159065\n",
      "Epoch 286/300\n",
      "Average training loss: 0.027621043192015755\n",
      "Average test loss: 0.0032366411427242887\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02766229615939988\n",
      "Average test loss: 0.003195905798632238\n",
      "Epoch 288/300\n",
      "Average training loss: 0.027596221019824344\n",
      "Average test loss: 0.0032543764122658306\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02752897901667489\n",
      "Average test loss: 0.003230883319965667\n",
      "Epoch 290/300\n",
      "Average training loss: 0.027520736277103424\n",
      "Average test loss: 0.0032459577061235904\n",
      "Epoch 291/300\n",
      "Average training loss: 0.027638788104057312\n",
      "Average test loss: 0.003265943856082029\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02755327339304818\n",
      "Average test loss: 0.003259462986141443\n",
      "Epoch 293/300\n",
      "Average training loss: 0.027532534857590992\n",
      "Average test loss: 0.0031822505835443736\n",
      "Epoch 294/300\n",
      "Average training loss: 0.027559904643230968\n",
      "Average test loss: 0.0031843738700780603\n",
      "Epoch 295/300\n",
      "Average training loss: 0.027539274174306128\n",
      "Average test loss: 0.0031917803645547895\n",
      "Epoch 296/300\n",
      "Average training loss: 0.027471201653281846\n",
      "Average test loss: 0.003234469700190756\n",
      "Epoch 297/300\n",
      "Average training loss: 0.027466441387931508\n",
      "Average test loss: 0.0033012244486146504\n",
      "Epoch 298/300\n",
      "Average training loss: 0.027464900437328552\n",
      "Average test loss: 0.0031590081428488096\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02746286828815937\n",
      "Average test loss: 0.0032948122214939857\n",
      "Epoch 300/300\n",
      "Average training loss: 0.027456809886627728\n",
      "Average test loss: 0.0031744682600514757\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average test loss: 0.005761169815642966\n",
      "Epoch 2/300\n",
      "Average training loss: 1.4105463268491958\n",
      "Average test loss: 0.003988725944732627\n",
      "Epoch 3/300\n",
      "Average training loss: 0.9786767387390136\n",
      "Average test loss: 0.0037988818856991\n",
      "Epoch 4/300\n",
      "Average training loss: 0.7410352929963006\n",
      "Average test loss: 0.0034113283921033146\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5765056131415897\n",
      "Average test loss: 0.0033138476198332177\n",
      "Epoch 6/300\n",
      "Average training loss: 0.4562992307609982\n",
      "Average test loss: 0.0032054569392154612\n",
      "Epoch 7/300\n",
      "Average training loss: 0.3692886596520742\n",
      "Average test loss: 0.0030730753048426574\n",
      "Epoch 8/300\n",
      "Average training loss: 0.30316672656271193\n",
      "Average test loss: 0.0033006017100479868\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2514427226119571\n",
      "Average test loss: 0.0030873472642981344\n",
      "Epoch 10/300\n",
      "Average training loss: 0.20938763937685226\n",
      "Average test loss: 0.0028550492777592606\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17722012668185763\n",
      "Average test loss: 0.002846115104026265\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1510790335337321\n",
      "Average test loss: 0.00276914805215266\n",
      "Epoch 13/300\n",
      "Average training loss: 0.13040723603963852\n",
      "Average test loss: 0.0026323530783669818\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11393754716714223\n",
      "Average test loss: 0.0025841677121611105\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10018667868773143\n",
      "Average test loss: 0.002551128821240531\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08897030127048493\n",
      "Average test loss: 0.0024706550227064227\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08010569398932987\n",
      "Average test loss: 0.002499074932394756\n",
      "Epoch 18/300\n",
      "Average training loss: 0.072629840473334\n",
      "Average test loss: 0.0024056939891436034\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06670631138152547\n",
      "Average test loss: 0.002387809906154871\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06189693725440237\n",
      "Average test loss: 0.0023342395587306886\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05795360491010878\n",
      "Average test loss: 0.0023901921781814763\n",
      "Epoch 22/300\n",
      "Average training loss: 0.054714563462469314\n",
      "Average test loss: 0.002315634264300267\n",
      "Epoch 23/300\n",
      "Average training loss: 0.051954717550012804\n",
      "Average test loss: 0.002252290062399374\n",
      "Epoch 24/300\n",
      "Average training loss: 0.049673576242393915\n",
      "Average test loss: 0.002207892236817214\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04775385546684265\n",
      "Average test loss: 0.002234417189533512\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04621990577710999\n",
      "Average test loss: 0.002208286648305754\n",
      "Epoch 27/300\n",
      "Average training loss: 0.044865358236763214\n",
      "Average test loss: 0.002209797334753805\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0437740200261275\n",
      "Average test loss: 0.002163989556332429\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04277869848410289\n",
      "Average test loss: 0.0021638498384919432\n",
      "Epoch 30/300\n",
      "Average training loss: 0.041866420464383235\n",
      "Average test loss: 0.0022100357053180535\n",
      "Epoch 31/300\n",
      "Average training loss: 0.041139655904637445\n",
      "Average test loss: 0.0021116097832305563\n",
      "Epoch 32/300\n",
      "Average training loss: 0.040512505561113354\n",
      "Average test loss: 0.00213786147803896\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03975195539246003\n",
      "Average test loss: 0.002119549422421389\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03910819806655248\n",
      "Average test loss: 0.0021201119700239766\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03865379219584995\n",
      "Average test loss: 0.0021001309775229957\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03809955360160933\n",
      "Average test loss: 0.002081013563606474\n",
      "Epoch 37/300\n",
      "Average training loss: 0.037579119278324975\n",
      "Average test loss: 0.00208820133532087\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03745516407489777\n",
      "Average test loss: 0.0020796260094890994\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03670669807824824\n",
      "Average test loss: 0.0020547222855190434\n",
      "Epoch 40/300\n",
      "Average training loss: 0.036324721700615355\n",
      "Average test loss: 0.0020456384607694215\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03582602128055361\n",
      "Average test loss: 0.0020580024766839213\n",
      "Epoch 42/300\n",
      "Average training loss: 0.035411277492841085\n",
      "Average test loss: 0.0020608135127565927\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03503089677128527\n",
      "Average test loss: 0.002089399619234933\n",
      "Epoch 44/300\n",
      "Average training loss: 0.034606648504734036\n",
      "Average test loss: 0.0020909645112438336\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03419994334214264\n",
      "Average test loss: 0.0020480886238316695\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03388736208279928\n",
      "Average test loss: 0.002016510550243159\n",
      "Epoch 47/300\n",
      "Average training loss: 0.033594497371051045\n",
      "Average test loss: 0.0020182783657477963\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03315193307565318\n",
      "Average test loss: 0.002069657457578513\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03280434327655368\n",
      "Average test loss: 0.0020225169538623756\n",
      "Epoch 50/300\n",
      "Average training loss: 0.032588531702756884\n",
      "Average test loss: 0.0020351421646773813\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03217637379633056\n",
      "Average test loss: 0.0020513080113256973\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03191082462337282\n",
      "Average test loss: 0.0020583359172774685\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03157187047104041\n",
      "Average test loss: 0.0020517622764325803\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03132906224661403\n",
      "Average test loss: 0.0020318932958568138\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03099921850197845\n",
      "Average test loss: 0.0020234893374145033\n",
      "Epoch 56/300\n",
      "Average training loss: 0.030737340816193157\n",
      "Average test loss: 0.0020109832903577223\n",
      "Epoch 57/300\n",
      "Average training loss: 0.030571397205193838\n",
      "Average test loss: 0.0020297233532700276\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03023278918862343\n",
      "Average test loss: 0.002034909084232317\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02998215326584048\n",
      "Average test loss: 0.0020731685921135876\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02971450400683615\n",
      "Average test loss: 0.0020369563998861445\n",
      "Epoch 61/300\n",
      "Average training loss: 0.029537851120034854\n",
      "Average test loss: 0.0020349848514629736\n",
      "Epoch 62/300\n",
      "Average training loss: 0.029316066198050977\n",
      "Average test loss: 0.0020968131764481463\n",
      "Epoch 63/300\n",
      "Average training loss: 0.029135474170247713\n",
      "Average test loss: 0.0020235845945361587\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02885665220850044\n",
      "Average test loss: 0.0021531764359937772\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02863121568163236\n",
      "Average test loss: 0.002063000247710281\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02841644479168786\n",
      "Average test loss: 0.002073135990752942\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02823230257961485\n",
      "Average test loss: 0.0022725645676255225\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02803789478374852\n",
      "Average test loss: 0.002147403622873955\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0279046877125899\n",
      "Average test loss: 0.002081410786861347\n",
      "Epoch 70/300\n",
      "Average training loss: 0.027733052983880045\n",
      "Average test loss: 0.0020455804204361308\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02762897539138794\n",
      "Average test loss: 0.002077479564274351\n",
      "Epoch 72/300\n",
      "Average training loss: 0.027433119883139928\n",
      "Average test loss: 0.002077435185511907\n",
      "Epoch 73/300\n",
      "Average training loss: 0.027194000836875704\n",
      "Average test loss: 0.002135843823974331\n",
      "Epoch 74/300\n",
      "Average training loss: 0.027075133825341862\n",
      "Average test loss: 0.002076116817692916\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02685542343888018\n",
      "Average test loss: 0.0020475879123227464\n",
      "Epoch 76/300\n",
      "Average training loss: 0.026794281012482112\n",
      "Average test loss: 0.0020647332217130394\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02669747407734394\n",
      "Average test loss: 0.0020615258725980917\n",
      "Epoch 78/300\n",
      "Average training loss: 0.026548804034789403\n",
      "Average test loss: 0.0020995098296552896\n",
      "Epoch 79/300\n",
      "Average training loss: 0.026321485044227707\n",
      "Average test loss: 0.002093041638429794\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02617792387637827\n",
      "Average test loss: 0.0021215395212897826\n",
      "Epoch 81/300\n",
      "Average training loss: 0.026242315328783457\n",
      "Average test loss: 0.002082645281321473\n",
      "Epoch 82/300\n",
      "Average training loss: 0.026018911985887423\n",
      "Average test loss: 0.0020702490081182783\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025851122493545216\n",
      "Average test loss: 0.0021658289610511725\n",
      "Epoch 84/300\n",
      "Average training loss: 0.025721596712867417\n",
      "Average test loss: 0.0022086983025074005\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02563625426259306\n",
      "Average test loss: 0.002310309125213987\n",
      "Epoch 86/300\n",
      "Average training loss: 0.025548922126491864\n",
      "Average test loss: 0.0020906221109131973\n",
      "Epoch 87/300\n",
      "Average training loss: 0.025437711190846233\n",
      "Average test loss: 0.0021638971894151634\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025298226517107752\n",
      "Average test loss: 0.0020690924815005727\n",
      "Epoch 89/300\n",
      "Average training loss: 0.025247227917114895\n",
      "Average test loss: 0.0020566720532046424\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02506507612930404\n",
      "Average test loss: 0.0021137108783134155\n",
      "Epoch 91/300\n",
      "Average training loss: 0.025073838882976106\n",
      "Average test loss: 0.0021068238651172984\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02493953164666891\n",
      "Average test loss: 0.002230374909006059\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02482407721877098\n",
      "Average test loss: 0.002168564808037546\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02473397161066532\n",
      "Average test loss: 0.0022016633529629973\n",
      "Epoch 95/300\n",
      "Average training loss: 0.024667228781514696\n",
      "Average test loss: 0.002181794565791885\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02455861470931106\n",
      "Average test loss: 0.002113747951471143\n",
      "Epoch 97/300\n",
      "Average training loss: 0.024506567533645364\n",
      "Average test loss: 0.0021316211192558207\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024456667783359688\n",
      "Average test loss: 0.002243712313266264\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02433434051109685\n",
      "Average test loss: 0.002121116140650378\n",
      "Epoch 100/300\n",
      "Average training loss: 0.024300505171219507\n",
      "Average test loss: 0.0021641929590453706\n",
      "Epoch 101/300\n",
      "Average training loss: 0.024170127413339085\n",
      "Average test loss: 0.0021535037844959233\n",
      "Epoch 102/300\n",
      "Average training loss: 0.024096353559030427\n",
      "Average test loss: 0.002196933606432544\n",
      "Epoch 103/300\n",
      "Average training loss: 0.024056711302863226\n",
      "Average test loss: 0.002099212111491296\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02405324876639578\n",
      "Average test loss: 0.002258998337512215\n",
      "Epoch 105/300\n",
      "Average training loss: 0.023909007355570795\n",
      "Average test loss: 0.00224388857330713\n",
      "Epoch 106/300\n",
      "Average training loss: 0.023889436268972025\n",
      "Average test loss: 0.0022038254398438666\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023825803284843763\n",
      "Average test loss: 0.0022151148127805857\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02374348168075085\n",
      "Average test loss: 0.0022160423257284695\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02368848739233282\n",
      "Average test loss: 0.0021762477725537285\n",
      "Epoch 110/300\n",
      "Average training loss: 0.023571686506271363\n",
      "Average test loss: 0.0022188558290816016\n",
      "Epoch 111/300\n",
      "Average training loss: 0.023525040030479432\n",
      "Average test loss: 0.0021062711221683356\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02349299767613411\n",
      "Average test loss: 0.002251859925066431\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023428353572885197\n",
      "Average test loss: 0.0021684318667070735\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023335121924678483\n",
      "Average test loss: 0.0022631727078308663\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023331612919767698\n",
      "Average test loss: 0.0021281839532570706\n",
      "Epoch 116/300\n",
      "Average training loss: 0.023317248125871024\n",
      "Average test loss: 0.002187712638742394\n",
      "Epoch 117/300\n",
      "Average training loss: 0.023231200371351506\n",
      "Average test loss: 0.0022634243769571187\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023148181852367188\n",
      "Average test loss: 0.002280082410408391\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023125569752520985\n",
      "Average test loss: 0.0021595789258264833\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023059039678838517\n",
      "Average test loss: 0.0022143452353775503\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02305300267206298\n",
      "Average test loss: 0.0022339158296171163\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0229428141216437\n",
      "Average test loss: 0.002123168644391828\n",
      "Epoch 123/300\n",
      "Average training loss: 0.022920948311686517\n",
      "Average test loss: 0.0022285991896771724\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02287011947731177\n",
      "Average test loss: 0.002230696976184845\n",
      "Epoch 125/300\n",
      "Average training loss: 0.022757218365867932\n",
      "Average test loss: 0.0022388622591065035\n",
      "Epoch 126/300\n",
      "Average training loss: 0.022807416955629985\n",
      "Average test loss: 0.0022102554028646812\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02271489100323783\n",
      "Average test loss: 0.0022787882511814436\n",
      "Epoch 128/300\n",
      "Average training loss: 0.022685448305474386\n",
      "Average test loss: 0.0022479127794504165\n",
      "Epoch 129/300\n",
      "Average training loss: 0.022675050566593805\n",
      "Average test loss: 0.0021961773599808417\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02259008129272196\n",
      "Average test loss: 0.002219783866364095\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02252416303091579\n",
      "Average test loss: 0.0022406093581683107\n",
      "Epoch 132/300\n",
      "Average training loss: 0.022514323242836528\n",
      "Average test loss: 0.0022054959171348147\n",
      "Epoch 133/300\n",
      "Average training loss: 0.022499960564904744\n",
      "Average test loss: 0.0022524985226078167\n",
      "Epoch 134/300\n",
      "Average training loss: 0.022427091719375715\n",
      "Average test loss: 0.0022494128302981455\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02240854329864184\n",
      "Average test loss: 0.0022467212924319837\n",
      "Epoch 136/300\n",
      "Average training loss: 0.022323489159345625\n",
      "Average test loss: 0.0023215606127762133\n",
      "Epoch 137/300\n",
      "Average training loss: 0.022346872609522606\n",
      "Average test loss: 0.0021633985573425887\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022342569693922995\n",
      "Average test loss: 0.002170062044635415\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022232710854874715\n",
      "Average test loss: 0.0022155687622725964\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022210983316103616\n",
      "Average test loss: 0.002289953508310848\n",
      "Epoch 141/300\n",
      "Average training loss: 0.022142109523216883\n",
      "Average test loss: 0.002185587565932009\n",
      "Epoch 142/300\n",
      "Average training loss: 0.022154417367445098\n",
      "Average test loss: 0.0022328418259405427\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022148964433206454\n",
      "Average test loss: 0.0022150945091206167\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02206678761707412\n",
      "Average test loss: 0.0022183272341887157\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02204834491511186\n",
      "Average test loss: 0.00222211465291265\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021970283039742047\n",
      "Average test loss: 0.0022347277605699167\n",
      "Epoch 147/300\n",
      "Average training loss: 0.022066081383162076\n",
      "Average test loss: 0.0022940492767633664\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02188393668499258\n",
      "Average test loss: 0.002199095032695267\n",
      "Epoch 149/300\n",
      "Average training loss: 0.021906077105138037\n",
      "Average test loss: 0.002232922561777135\n",
      "Epoch 150/300\n",
      "Average training loss: 0.021890961623854108\n",
      "Average test loss: 0.0022150479979399177\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021829390360249414\n",
      "Average test loss: 0.0023350677116670544\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021818908143374654\n",
      "Average test loss: 0.0022736327795104846\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021769085332751275\n",
      "Average test loss: 0.0023300198064082197\n",
      "Epoch 154/300\n",
      "Average training loss: 0.021755681441062026\n",
      "Average test loss: 0.0022777226821829874\n",
      "Epoch 155/300\n",
      "Average training loss: 0.021756898363431296\n",
      "Average test loss: 0.002293607446572019\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021681994857059586\n",
      "Average test loss: 0.0021840951593799724\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021673608071274227\n",
      "Average test loss: 0.002316476632013089\n",
      "Epoch 158/300\n",
      "Average training loss: 0.021660159283214145\n",
      "Average test loss: 0.0023046417263232997\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021601493706305822\n",
      "Average test loss: 0.0021656470715792644\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021556832815210026\n",
      "Average test loss: 0.002238671122222311\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02149666705562009\n",
      "Average test loss: 0.0022661237499366203\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021519931343694527\n",
      "Average test loss: 0.0022422242619925077\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021555169398585955\n",
      "Average test loss: 0.0022223797797535855\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021451584522922833\n",
      "Average test loss: 0.002316456569565667\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021449720814824106\n",
      "Average test loss: 0.0022601682539615366\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021404233405987422\n",
      "Average test loss: 0.002277104354153077\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021312635269429947\n",
      "Average test loss: 0.0023026402082501184\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021323426875803207\n",
      "Average test loss: 0.0022720742570753725\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021335339658790163\n",
      "Average test loss: 0.0022494950793269606\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0213066968239016\n",
      "Average test loss: 0.0022702254398415484\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02130118186937438\n",
      "Average test loss: 0.00224700990671085\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0212499479022291\n",
      "Average test loss: 0.0022139878136416278\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021233928743335937\n",
      "Average test loss: 0.002295006593896283\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021140821594330998\n",
      "Average test loss: 0.002299724418669939\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021178301884068385\n",
      "Average test loss: 0.0022401625745826297\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021181220317880313\n",
      "Average test loss: 0.0023191776093509464\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021135986987087463\n",
      "Average test loss: 0.0022507397217883004\n",
      "Epoch 178/300\n",
      "Average training loss: 0.021073859083983633\n",
      "Average test loss: 0.0022538859823511706\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021125709575083522\n",
      "Average test loss: 0.002232858305176099\n",
      "Epoch 180/300\n",
      "Average training loss: 0.021048880786531502\n",
      "Average test loss: 0.0023040736984047626\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02100953507092264\n",
      "Average test loss: 0.002323457329430514\n",
      "Epoch 182/300\n",
      "Average training loss: 0.020997760286761653\n",
      "Average test loss: 0.002346514563386639\n",
      "Epoch 183/300\n",
      "Average training loss: 0.020974278756313854\n",
      "Average test loss: 0.0022952064401987526\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021022624156541294\n",
      "Average test loss: 0.002228214142223199\n",
      "Epoch 185/300\n",
      "Average training loss: 0.020984485867950653\n",
      "Average test loss: 0.002207541775165333\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02094657781968514\n",
      "Average test loss: 0.002222459404832787\n",
      "Epoch 187/300\n",
      "Average training loss: 0.020944739340080155\n",
      "Average test loss: 0.0022659379457020097\n",
      "Epoch 188/300\n",
      "Average training loss: 0.020849964312381213\n",
      "Average test loss: 0.0022526547368615867\n",
      "Epoch 189/300\n",
      "Average training loss: 0.020856324156125386\n",
      "Average test loss: 0.002242747291094727\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020847294391857254\n",
      "Average test loss: 0.0022681810898292397\n",
      "Epoch 191/300\n",
      "Average training loss: 0.020770051843590206\n",
      "Average test loss: 0.0022677309855611786\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02078552538818783\n",
      "Average test loss: 0.002319826586378945\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02083467889825503\n",
      "Average test loss: 0.0023038927155236404\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02076383443342315\n",
      "Average test loss: 0.0022598481877810425\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020729067592157258\n",
      "Average test loss: 0.0022113895072705214\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02066428211496936\n",
      "Average test loss: 0.0022473984466244777\n",
      "Epoch 197/300\n",
      "Average training loss: 0.020687854366170034\n",
      "Average test loss: 0.002285455930564139\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020741057223743864\n",
      "Average test loss: 0.00225482673673994\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020669603005051614\n",
      "Average test loss: 0.002217969966017538\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020593181894885168\n",
      "Average test loss: 0.0023234215991364583\n",
      "Epoch 201/300\n",
      "Average training loss: 0.020590380435188613\n",
      "Average test loss: 0.0022762251561507583\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02062726260887252\n",
      "Average test loss: 0.002254621840185589\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020561007723212243\n",
      "Average test loss: 0.0023121899380866024\n",
      "Epoch 204/300\n",
      "Average training loss: 0.020574798342254428\n",
      "Average test loss: 0.0022555038111491334\n",
      "Epoch 205/300\n",
      "Average training loss: 0.020522230237308477\n",
      "Average test loss: 0.002313912898302078\n",
      "Epoch 206/300\n",
      "Average training loss: 0.020543887640866967\n",
      "Average test loss: 0.0023564582724745074\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02048376905421416\n",
      "Average test loss: 0.002318871003161702\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020487668186426163\n",
      "Average test loss: 0.002249073352250788\n",
      "Epoch 209/300\n",
      "Average training loss: 0.020459703215294413\n",
      "Average test loss: 0.0022801064018987947\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02042536206709014\n",
      "Average test loss: 0.002271907366087867\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02055081525279416\n",
      "Average test loss: 0.002368744557309482\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02042383458548122\n",
      "Average test loss: 0.0023537577138178878\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020425327133801248\n",
      "Average test loss: 0.0023534503117617635\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020338086056212585\n",
      "Average test loss: 0.0023058909312304522\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0203549761424462\n",
      "Average test loss: 0.0023675396841847233\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02032867334286372\n",
      "Average test loss: 0.002318051187838945\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020331996457444296\n",
      "Average test loss: 0.0022980321187319025\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02031941941877206\n",
      "Average test loss: 0.0022617017962038516\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020263733479711745\n",
      "Average test loss: 0.0023373110143260823\n",
      "Epoch 220/300\n",
      "Average training loss: 0.020285638719797133\n",
      "Average test loss: 0.002355634504515264\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02022142912613021\n",
      "Average test loss: 0.0022828293657447727\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020280334629946283\n",
      "Average test loss: 0.002300163745880127\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02022697244418992\n",
      "Average test loss: 0.002346057487651706\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02022961405499114\n",
      "Average test loss: 0.0023336621604652867\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020210584643814297\n",
      "Average test loss: 0.0023426738207538923\n",
      "Epoch 226/300\n",
      "Average training loss: 0.020172680002119806\n",
      "Average test loss: 0.0023155906587425204\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02020601464973556\n",
      "Average test loss: 0.002247725498966045\n",
      "Epoch 228/300\n",
      "Average training loss: 0.020182883492774435\n",
      "Average test loss: 0.0022409194951049155\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02012255749106407\n",
      "Average test loss: 0.0023467608872387146\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020120586605535614\n",
      "Average test loss: 0.0023909952296978895\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02009514283802774\n",
      "Average test loss: 0.0023419570152958236\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020067519590258598\n",
      "Average test loss: 0.0022858895652203095\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020100295123126773\n",
      "Average test loss: 0.002307844842887587\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02005091012434827\n",
      "Average test loss: 0.002370020205155015\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020031924774249393\n",
      "Average test loss: 0.002404129717292057\n",
      "Epoch 236/300\n",
      "Average training loss: 0.020001515097088283\n",
      "Average test loss: 0.0023378494795825745\n",
      "Epoch 237/300\n",
      "Average training loss: 0.020012353029516008\n",
      "Average test loss: 0.0022927786961404813\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02001277716871765\n",
      "Average test loss: 0.0023599903161327044\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02000097996327612\n",
      "Average test loss: 0.0022299352400004862\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0199506762507889\n",
      "Average test loss: 0.0022740122175051104\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01995466986629698\n",
      "Average test loss: 0.0022874393458995556\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019994343274169497\n",
      "Average test loss: 0.0023329140277993346\n",
      "Epoch 243/300\n",
      "Average training loss: 0.019890976463754973\n",
      "Average test loss: 0.0022675281033540766\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01988462729834848\n",
      "Average test loss: 0.0023710161990796526\n",
      "Epoch 245/300\n",
      "Average training loss: 0.019896018857757252\n",
      "Average test loss: 0.0022658952408366735\n",
      "Epoch 246/300\n",
      "Average training loss: 0.019885312368472418\n",
      "Average test loss: 0.0023788564196891254\n",
      "Epoch 247/300\n",
      "Average training loss: 0.019886970437235304\n",
      "Average test loss: 0.002368284719892674\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01985280276586612\n",
      "Average test loss: 0.002294141720359524\n",
      "Epoch 249/300\n",
      "Average training loss: 0.019840810257527562\n",
      "Average test loss: 0.002344427278265357\n",
      "Epoch 250/300\n",
      "Average training loss: 0.019791761949658395\n",
      "Average test loss: 0.002283489198010001\n",
      "Epoch 251/300\n",
      "Average training loss: 0.019869257887204488\n",
      "Average test loss: 0.0023885410986840723\n",
      "Epoch 252/300\n",
      "Average training loss: 0.019802815995282597\n",
      "Average test loss: 0.0023537158345182736\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01978975812594096\n",
      "Average test loss: 0.0023126971726823184\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019773146983649997\n",
      "Average test loss: 0.0023085395329528386\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019742059383955266\n",
      "Average test loss: 0.002376079242779977\n",
      "Epoch 256/300\n",
      "Average training loss: 0.019719520406590568\n",
      "Average test loss: 0.002661075134451191\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019744599440031582\n",
      "Average test loss: 0.0022841714390863977\n",
      "Epoch 258/300\n",
      "Average training loss: 0.019724214695394038\n",
      "Average test loss: 0.002391576164919469\n",
      "Epoch 259/300\n",
      "Average training loss: 0.019674700963828298\n",
      "Average test loss: 0.0023587636488179367\n",
      "Epoch 260/300\n",
      "Average training loss: 0.019718085365162955\n",
      "Average test loss: 0.0023074504911071722\n",
      "Epoch 261/300\n",
      "Average training loss: 0.019680638975567287\n",
      "Average test loss: 0.002311519710967938\n",
      "Epoch 262/300\n",
      "Average training loss: 0.019769691535168225\n",
      "Average test loss: 0.0023154372978541585\n",
      "Epoch 263/300\n",
      "Average training loss: 0.019686028657688034\n",
      "Average test loss: 0.002304313074383471\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01964488840930992\n",
      "Average test loss: 0.002457350061999427\n",
      "Epoch 265/300\n",
      "Average training loss: 0.019667056957880655\n",
      "Average test loss: 0.0023965533348835177\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01962865814401044\n",
      "Average test loss: 0.00236311628177969\n",
      "Epoch 267/300\n",
      "Average training loss: 0.019620991362465753\n",
      "Average test loss: 0.0023150084041472937\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01962770124938753\n",
      "Average test loss: 0.0023971468900433847\n",
      "Epoch 269/300\n",
      "Average training loss: 0.019571673039760856\n",
      "Average test loss: 0.002349793227803376\n",
      "Epoch 270/300\n",
      "Average training loss: 0.019620135560631753\n",
      "Average test loss: 0.0023250189744349983\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01960835805369748\n",
      "Average test loss: 0.0023381008700364167\n",
      "Epoch 272/300\n",
      "Average training loss: 0.019532219257619644\n",
      "Average test loss: 0.0022967068314966228\n",
      "Epoch 273/300\n",
      "Average training loss: 0.019561916285090977\n",
      "Average test loss: 0.002287456793503629\n",
      "Epoch 274/300\n",
      "Average training loss: 0.019537932724588446\n",
      "Average test loss: 0.0023769976326988803\n",
      "Epoch 275/300\n",
      "Average training loss: 0.019544940589202774\n",
      "Average test loss: 0.002283418608829379\n",
      "Epoch 276/300\n",
      "Average training loss: 0.019488418372968833\n",
      "Average test loss: 0.002334664571719865\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01951908472345935\n",
      "Average test loss: 0.0022922733736534914\n",
      "Epoch 278/300\n",
      "Average training loss: 0.019467360554469956\n",
      "Average test loss: 0.002421352634516855\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01952728036377165\n",
      "Average test loss: 0.0023186200711255273\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019456245006786454\n",
      "Average test loss: 0.002367650647337238\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01947573332819674\n",
      "Average test loss: 0.002403445705357525\n",
      "Epoch 282/300\n",
      "Average training loss: 0.019447791465454632\n",
      "Average test loss: 0.0022733401229812038\n",
      "Epoch 283/300\n",
      "Average training loss: 0.019426801562309266\n",
      "Average test loss: 0.0023605074382697542\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019441312504311404\n",
      "Average test loss: 0.002440526511313187\n",
      "Epoch 285/300\n",
      "Average training loss: 0.019433741074469355\n",
      "Average test loss: 0.002417476785265737\n",
      "Epoch 286/300\n",
      "Average training loss: 0.019380089966787233\n",
      "Average test loss: 0.0023818652170399823\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019378171662489573\n",
      "Average test loss: 0.002319625854285227\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01938130947947502\n",
      "Average test loss: 0.0022933333890719546\n",
      "Epoch 289/300\n",
      "Average training loss: 0.019388804992039998\n",
      "Average test loss: 0.002353231639911731\n",
      "Epoch 290/300\n",
      "Average training loss: 0.019346413642168044\n",
      "Average test loss: 0.002308518514037132\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01932249742580785\n",
      "Average test loss: 0.002282967517359389\n",
      "Epoch 292/300\n",
      "Average training loss: 0.019367903630766604\n",
      "Average test loss: 0.002307521090325382\n",
      "Epoch 293/300\n",
      "Average training loss: 0.019369703095820215\n",
      "Average test loss: 0.0023748699689490928\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0193143774577313\n",
      "Average test loss: 0.002315848963128196\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01932350552909904\n",
      "Average test loss: 0.0023299500787009797\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01927430263410012\n",
      "Average test loss: 0.00236336465097136\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019290674106942282\n",
      "Average test loss: 0.002392747011863523\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019267876001695793\n",
      "Average test loss: 0.002388054680596623\n",
      "Epoch 299/300\n",
      "Average training loss: 0.019224738433129256\n",
      "Average test loss: 0.0023555057756602764\n",
      "Epoch 300/300\n",
      "Average training loss: 0.019300628151330684\n",
      "Average test loss: 0.002429864844100343\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.344762702200148\n",
      "Average test loss: 0.02535460611515575\n",
      "Epoch 2/300\n",
      "Average training loss: 1.126209354188707\n",
      "Average test loss: 0.0033917367752227517\n",
      "Epoch 3/300\n",
      "Average training loss: 0.7437125708791945\n",
      "Average test loss: 0.00319989499470426\n",
      "Epoch 4/300\n",
      "Average training loss: 0.529089930958218\n",
      "Average test loss: 0.0028343942268855044\n",
      "Epoch 5/300\n",
      "Average training loss: 0.40179371746381126\n",
      "Average test loss: 0.002684812383001877\n",
      "Epoch 6/300\n",
      "Average training loss: 0.32012720121277705\n",
      "Average test loss: 0.0026784491849442325\n",
      "Epoch 7/300\n",
      "Average training loss: 0.261591340303421\n",
      "Average test loss: 0.0025281462060908477\n",
      "Epoch 8/300\n",
      "Average training loss: 0.21601206334431966\n",
      "Average test loss: 0.0023733286801725624\n",
      "Epoch 9/300\n",
      "Average training loss: 0.18159272368748983\n",
      "Average test loss: 0.0022515844987291427\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15252062720722623\n",
      "Average test loss: 0.004794714296857516\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13037276951471966\n",
      "Average test loss: 0.0021671624099835756\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11131718521647983\n",
      "Average test loss: 0.004251119391785727\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09586498097578684\n",
      "Average test loss: 0.0020411121589649053\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08322051364845699\n",
      "Average test loss: 0.0019391332086589601\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07287482898765141\n",
      "Average test loss: 0.001876103850081563\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06487744702233209\n",
      "Average test loss: 0.001995836883990301\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05833244875735707\n",
      "Average test loss: 0.0018021010180107422\n",
      "Epoch 18/300\n",
      "Average training loss: 0.053147803558243646\n",
      "Average test loss: 0.001957441501526369\n",
      "Epoch 19/300\n",
      "Average training loss: 0.049289308345980117\n",
      "Average test loss: 0.001722994502944251\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04584050479862425\n",
      "Average test loss: 0.0016909797578636143\n",
      "Epoch 21/300\n",
      "Average training loss: 0.043090297386050226\n",
      "Average test loss: 0.0017103738623360793\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04076130245460404\n",
      "Average test loss: 0.001664490997998251\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03901852175924513\n",
      "Average test loss: 0.0016399759062462383\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03742644606696235\n",
      "Average test loss: 0.0016153618133523397\n",
      "Epoch 25/300\n",
      "Average training loss: 0.036073514809211095\n",
      "Average test loss: 0.0016123368178183833\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03513399598664708\n",
      "Average test loss: 0.0015556088639423252\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03399726639522446\n",
      "Average test loss: 0.0015222168479942612\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03314021980762482\n",
      "Average test loss: 0.0015165322977635596\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03238769626783\n",
      "Average test loss: 0.0015027234407348766\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03157489031718837\n",
      "Average test loss: 0.0014930766733984152\n",
      "Epoch 31/300\n",
      "Average training loss: 0.030923496338228385\n",
      "Average test loss: 0.0015998253412544726\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03026726328664356\n",
      "Average test loss: 0.0014944064903797375\n",
      "Epoch 33/300\n",
      "Average training loss: 0.029649010646674367\n",
      "Average test loss: 0.0014822270372985965\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02908441463775105\n",
      "Average test loss: 0.001470733887102041\n",
      "Epoch 35/300\n",
      "Average training loss: 0.028526130948629644\n",
      "Average test loss: 0.0015554243143544428\n",
      "Epoch 36/300\n",
      "Average training loss: 0.028042031011647647\n",
      "Average test loss: 0.001439004325411386\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02756419229838583\n",
      "Average test loss: 0.0014269102573808696\n",
      "Epoch 38/300\n",
      "Average training loss: 0.027085455512007077\n",
      "Average test loss: 0.0014176556053054\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02663652486933602\n",
      "Average test loss: 0.0014282625026793944\n",
      "Epoch 40/300\n",
      "Average training loss: 0.026206203705734676\n",
      "Average test loss: 0.0015236106018225353\n",
      "Epoch 41/300\n",
      "Average training loss: 0.025907526860634487\n",
      "Average test loss: 0.0014034202056419519\n",
      "Epoch 42/300\n",
      "Average training loss: 0.025439105042152934\n",
      "Average test loss: 0.0014096793449587292\n",
      "Epoch 43/300\n",
      "Average training loss: 0.025071942513187725\n",
      "Average test loss: 0.001419989847785069\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02469852614402771\n",
      "Average test loss: 0.001429363875132468\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02436023510247469\n",
      "Average test loss: 0.001505916740331385\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02405191714896096\n",
      "Average test loss: 0.001475996164760242\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02371962808072567\n",
      "Average test loss: 0.0014970299562232362\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023430680773324438\n",
      "Average test loss: 0.0013888349127438332\n",
      "Epoch 49/300\n",
      "Average training loss: 0.023198490998811192\n",
      "Average test loss: 0.0014396407997442615\n",
      "Epoch 50/300\n",
      "Average training loss: 0.022887597428427803\n",
      "Average test loss: 0.00142016715866824\n",
      "Epoch 51/300\n",
      "Average training loss: 0.022658146142959596\n",
      "Average test loss: 0.0013840004480961297\n",
      "Epoch 52/300\n",
      "Average training loss: 0.022357652650939093\n",
      "Average test loss: 0.001403372298201753\n",
      "Epoch 53/300\n",
      "Average training loss: 0.022121615833706327\n",
      "Average test loss: 0.0013883507950231432\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02199473962187767\n",
      "Average test loss: 0.0013869738491355545\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021691207273138894\n",
      "Average test loss: 0.001395837944932282\n",
      "Epoch 56/300\n",
      "Average training loss: 0.021491450442208185\n",
      "Average test loss: 0.0013925886110713085\n",
      "Epoch 57/300\n",
      "Average training loss: 0.021219611884819137\n",
      "Average test loss: 0.0013856779868817992\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02102446982264519\n",
      "Average test loss: 0.0014344710908416245\n",
      "Epoch 59/300\n",
      "Average training loss: 0.020837468839353987\n",
      "Average test loss: 0.0013883059804534748\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020696888741519717\n",
      "Average test loss: 0.0013831847390780847\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02050844058725569\n",
      "Average test loss: 0.0014023016720182365\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020384917714529566\n",
      "Average test loss: 0.001431736750735177\n",
      "Epoch 63/300\n",
      "Average training loss: 0.020086990853150685\n",
      "Average test loss: 0.0014201629656987885\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01998949735197756\n",
      "Average test loss: 0.00140193195713477\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019839003610114255\n",
      "Average test loss: 0.0015537429422967965\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019662759602069854\n",
      "Average test loss: 0.0014216747267378701\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01954155957947175\n",
      "Average test loss: 0.0014277689574907224\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019385576965080368\n",
      "Average test loss: 0.0013924367783798113\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019207063991162513\n",
      "Average test loss: 0.0014225472920677729\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01918325324025419\n",
      "Average test loss: 0.0014692407963383528\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019003860460387337\n",
      "Average test loss: 0.0015686966423980064\n",
      "Epoch 72/300\n",
      "Average training loss: 0.018894715231325892\n",
      "Average test loss: 0.0014496197283474935\n",
      "Epoch 73/300\n",
      "Average training loss: 0.018711134111715686\n",
      "Average test loss: 0.0014460856165322993\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018583974038561185\n",
      "Average test loss: 0.001429303222749796\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01849124889075756\n",
      "Average test loss: 0.0014505642821701864\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018387870025303628\n",
      "Average test loss: 0.0014128017888093989\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018350744353400335\n",
      "Average test loss: 0.0014470035582263437\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01814540684885449\n",
      "Average test loss: 0.0015589169251422088\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018077629289693302\n",
      "Average test loss: 0.0014410860265294712\n",
      "Epoch 80/300\n",
      "Average training loss: 0.017970429946978887\n",
      "Average test loss: 0.0014062415504207213\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017849884473615223\n",
      "Average test loss: 0.0014674569824710488\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01787069709930155\n",
      "Average test loss: 0.001435182922312783\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0177431885658039\n",
      "Average test loss: 0.001488590097779201\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017633183342715103\n",
      "Average test loss: 0.0014503444480813213\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01752575364212195\n",
      "Average test loss: 0.0014366958108213213\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017475609030160637\n",
      "Average test loss: 0.0014899195474055078\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017447763981918493\n",
      "Average test loss: 0.0014447825655548108\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017490408729347918\n",
      "Average test loss: 0.0014818222501004736\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01730109381179015\n",
      "Average test loss: 0.0014614563066926267\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017168893026808896\n",
      "Average test loss: 0.0014830952920019626\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017117611767517197\n",
      "Average test loss: 0.0014504366223182944\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01704953955196672\n",
      "Average test loss: 0.0015165559666024315\n",
      "Epoch 93/300\n",
      "Average training loss: 0.016966260510186355\n",
      "Average test loss: 0.0015361319922117724\n",
      "Epoch 94/300\n",
      "Average training loss: 0.016890256645778815\n",
      "Average test loss: 0.001474487743443913\n",
      "Epoch 95/300\n",
      "Average training loss: 0.016834825355145668\n",
      "Average test loss: 0.0014882379704051548\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01684071033447981\n",
      "Average test loss: 0.0014975908969839414\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0167394591089752\n",
      "Average test loss: 0.0014621992677243218\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016672356482181284\n",
      "Average test loss: 0.0014995850608166721\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016648965005245472\n",
      "Average test loss: 0.001499833018725945\n",
      "Epoch 100/300\n",
      "Average training loss: 0.016583431735634802\n",
      "Average test loss: 0.001495003092620108\n",
      "Epoch 101/300\n",
      "Average training loss: 0.016542264118790628\n",
      "Average test loss: 0.0014755333537856737\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01649499622070127\n",
      "Average test loss: 0.0015653372964200875\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016504714586668544\n",
      "Average test loss: 0.0015100045313851701\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016354345142013497\n",
      "Average test loss: 0.001504317447129223\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01635389556735754\n",
      "Average test loss: 0.0015303980668799745\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01632272051523129\n",
      "Average test loss: 0.001540328518797954\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0162377918039759\n",
      "Average test loss: 0.0015184534846080673\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016165520244174534\n",
      "Average test loss: 0.001512469001321329\n",
      "Epoch 109/300\n",
      "Average training loss: 0.016167544306152398\n",
      "Average test loss: 0.0015034962522590327\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01616644504583544\n",
      "Average test loss: 0.0015181042334685723\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016065057133634885\n",
      "Average test loss: 0.0015029329325382907\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01603145184285111\n",
      "Average test loss: 0.001497729431423876\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01598436681429545\n",
      "Average test loss: 0.0015480256672534677\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015979170008665985\n",
      "Average test loss: 0.0015939962239935994\n",
      "Epoch 115/300\n",
      "Average training loss: 0.015965816617012026\n",
      "Average test loss: 0.0015118815607080857\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01589257520602809\n",
      "Average test loss: 0.0015357224511810476\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015840393280817402\n",
      "Average test loss: 0.001537626661360264\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015819166690939003\n",
      "Average test loss: 0.0015047059162623352\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015788053109414047\n",
      "Average test loss: 0.001541784785170522\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015723014847271972\n",
      "Average test loss: 0.001559795279883676\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015696226544678213\n",
      "Average test loss: 0.0015816954855496685\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015674056208795972\n",
      "Average test loss: 0.0015191952573756376\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01567460931920343\n",
      "Average test loss: 0.0015330703660017915\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015562245576745935\n",
      "Average test loss: 0.0015285789298100604\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01560404075930516\n",
      "Average test loss: 0.0015248085457003778\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01559671072082387\n",
      "Average test loss: 0.0015123388901766804\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015554367067085371\n",
      "Average test loss: 0.0015017172014340759\n",
      "Epoch 128/300\n",
      "Average training loss: 0.015500302753514713\n",
      "Average test loss: 0.0015510992726518048\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015456222472091516\n",
      "Average test loss: 0.0014956727270036935\n",
      "Epoch 130/300\n",
      "Average training loss: 0.015428721173769897\n",
      "Average test loss: 0.001532704978560408\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015377143180204762\n",
      "Average test loss: 0.0015801782757043838\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015349169876840379\n",
      "Average test loss: 0.001591470566474729\n",
      "Epoch 133/300\n",
      "Average training loss: 0.015354773028029336\n",
      "Average test loss: 0.0015279580402291484\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01527853553659386\n",
      "Average test loss: 0.0015952592057486374\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015293939087953833\n",
      "Average test loss: 0.0015129732900920013\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015241965969403585\n",
      "Average test loss: 0.001558610032312572\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015221851718094613\n",
      "Average test loss: 0.0015282221374412377\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015204636260039277\n",
      "Average test loss: 0.0015479207001626492\n",
      "Epoch 139/300\n",
      "Average training loss: 0.015188147057261733\n",
      "Average test loss: 0.00156927142250869\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015165293485754065\n",
      "Average test loss: 0.0015725300926715136\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015132508892979887\n",
      "Average test loss: 0.0015585245463169283\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015110019469426738\n",
      "Average test loss: 0.001599733921026604\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015102056005762683\n",
      "Average test loss: 0.0015311822657369904\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015067802982197868\n",
      "Average test loss: 0.00154185344858302\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015018783164521059\n",
      "Average test loss: 0.0016010453423692121\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01499437467339966\n",
      "Average test loss: 0.0015180749863179194\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015000434425969919\n",
      "Average test loss: 0.0016388164968747232\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014981463656657272\n",
      "Average test loss: 0.0015391397753523456\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014973715456823508\n",
      "Average test loss: 0.0015843335563937822\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014916739308999644\n",
      "Average test loss: 0.0016110904289202558\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014934670905272165\n",
      "Average test loss: 0.001612040771274931\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014851670683258109\n",
      "Average test loss: 0.001569425756836103\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014860544850428899\n",
      "Average test loss: 0.0015568854969201817\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01480683999011914\n",
      "Average test loss: 0.001539286240004003\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01480388740532928\n",
      "Average test loss: 0.0016239909388952784\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014799757367206944\n",
      "Average test loss: 0.0015910811784366767\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014808603178295824\n",
      "Average test loss: 0.0015562281825890144\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014762216872639126\n",
      "Average test loss: 0.0015861649996497566\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014706840596265263\n",
      "Average test loss: 0.0015338566698547868\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014697127375337812\n",
      "Average test loss: 0.0015768776742948426\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014700569385455714\n",
      "Average test loss: 0.0015512513438136213\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014684828670488464\n",
      "Average test loss: 0.001568928441653649\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014662208935452833\n",
      "Average test loss: 0.0015393666894071632\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014652064685192373\n",
      "Average test loss: 0.0015737763359728787\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014616885740723874\n",
      "Average test loss: 0.0015827659993535943\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014635620163546668\n",
      "Average test loss: 0.0015483143425857027\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014590592381854853\n",
      "Average test loss: 0.0015731679919279283\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014581504513820013\n",
      "Average test loss: 0.0015456211550368202\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01452037640619609\n",
      "Average test loss: 0.0016622912144909302\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014543467834591865\n",
      "Average test loss: 0.0016214900890158283\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014543358691036701\n",
      "Average test loss: 0.0016084453330064813\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01451094595922364\n",
      "Average test loss: 0.0016077848150291376\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014484682818253835\n",
      "Average test loss: 0.0015485946288746263\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014465636188785234\n",
      "Average test loss: 0.0016221834568099843\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014449995929996172\n",
      "Average test loss: 0.0015806824584595031\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014444683986405532\n",
      "Average test loss: 0.0016227472745296027\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014411283646192815\n",
      "Average test loss: 0.0015956684068466225\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014390845947795444\n",
      "Average test loss: 0.0015867849634960294\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014433136366307735\n",
      "Average test loss: 0.0015943229586506884\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014368535792662038\n",
      "Average test loss: 0.0015787959266453982\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014458162053591675\n",
      "Average test loss: 0.0015984509718707867\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014306878190901545\n",
      "Average test loss: 0.0016073818998411297\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014319482433299224\n",
      "Average test loss: 0.0016053274114512736\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01430964557826519\n",
      "Average test loss: 0.0016471321605559853\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014337285586529308\n",
      "Average test loss: 0.0015719179523487885\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014278930178946918\n",
      "Average test loss: 0.0015628895937568612\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014262952085998324\n",
      "Average test loss: 0.001580452954603566\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014279796885119544\n",
      "Average test loss: 0.001604146688607418\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014232156125207743\n",
      "Average test loss: 0.0015759283485304977\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014237127507726351\n",
      "Average test loss: 0.0016303098282466333\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014190409529540274\n",
      "Average test loss: 0.001577085946790046\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014182204932802252\n",
      "Average test loss: 0.0016000697701755498\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014197089345918762\n",
      "Average test loss: 0.0015884096527265178\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014159200511872768\n",
      "Average test loss: 0.0015793497566547658\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01417860297444794\n",
      "Average test loss: 0.0016158752389666107\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014152168283859888\n",
      "Average test loss: 0.0016029382621248563\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014140981184939543\n",
      "Average test loss: 0.0015877767739196618\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014104895673692226\n",
      "Average test loss: 0.0016133796049074994\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014094655465748574\n",
      "Average test loss: 0.0015880418638181356\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014091802742746141\n",
      "Average test loss: 0.0016100045467416445\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014073389910161495\n",
      "Average test loss: 0.001641067372635007\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014066668136252297\n",
      "Average test loss: 0.0015731863242884477\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014039488131801288\n",
      "Average test loss: 0.0015702280246963104\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014031827508575387\n",
      "Average test loss: 0.001572307506058779\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01405145420713557\n",
      "Average test loss: 0.0015946367922135525\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014058880606873167\n",
      "Average test loss: 0.0016149675690879425\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013996662184596062\n",
      "Average test loss: 0.0015836524257125954\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014003432874878248\n",
      "Average test loss: 0.0016178623425463835\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01398433424780766\n",
      "Average test loss: 0.0016442256094887852\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013967677808470197\n",
      "Average test loss: 0.0015727113884770208\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013940978001389239\n",
      "Average test loss: 0.0015808710770474539\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013948341119620535\n",
      "Average test loss: 0.0015959981808231936\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013956474002864625\n",
      "Average test loss: 0.0016019853739999235\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013933233524362246\n",
      "Average test loss: 0.0016373815004610353\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013891340389019914\n",
      "Average test loss: 0.001605864483035273\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013899890378945404\n",
      "Average test loss: 0.001589095037120084\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013888631688223945\n",
      "Average test loss: 0.0016558649664123854\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013876140578753419\n",
      "Average test loss: 0.0016572696204400724\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013853213506440322\n",
      "Average test loss: 0.0016320987394493488\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013859143108129501\n",
      "Average test loss: 0.0017704080304958754\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013837404353751076\n",
      "Average test loss: 0.0016309480065893796\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013839453208777639\n",
      "Average test loss: 0.0016829059744357235\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013806085841523276\n",
      "Average test loss: 0.001647762509373327\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013812631385193931\n",
      "Average test loss: 0.0015837354545171062\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013806052068869272\n",
      "Average test loss: 0.0016252028035620848\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013810466938548618\n",
      "Average test loss: 0.0016442312287787597\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013786496392554706\n",
      "Average test loss: 0.0016674373994270961\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013738961106373205\n",
      "Average test loss: 0.00162931211768753\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013794051228298082\n",
      "Average test loss: 0.0015921161021623346\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013742306073506674\n",
      "Average test loss: 0.0016951649081375865\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013743951160874632\n",
      "Average test loss: 0.0016137830168008805\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013718450039625168\n",
      "Average test loss: 0.0016364893321361806\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013726898397008578\n",
      "Average test loss: 0.0016548471609130501\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013690590758290556\n",
      "Average test loss: 0.0016615684380133946\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013707934159371589\n",
      "Average test loss: 0.0015964051672878365\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013667503253453308\n",
      "Average test loss: 0.0016308353590882487\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013681458456648721\n",
      "Average test loss: 0.0016517041332812772\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013692092857427068\n",
      "Average test loss: 0.0016680450943402118\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013672087002131674\n",
      "Average test loss: 0.0016192663098271523\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013649565135439237\n",
      "Average test loss: 0.0016141119866321485\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01364631806148423\n",
      "Average test loss: 0.0015994595434102748\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013657420907583501\n",
      "Average test loss: 0.0016778344790347747\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013603348611129655\n",
      "Average test loss: 0.001698661538461844\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013643369568718804\n",
      "Average test loss: 0.0016889671018968026\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013602273994021946\n",
      "Average test loss: 0.0016192609678126043\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013585615084403091\n",
      "Average test loss: 0.0016622769575979975\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013575259946286678\n",
      "Average test loss: 0.0016241238423519665\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013568939316603872\n",
      "Average test loss: 0.0015960238427958555\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01359034771968921\n",
      "Average test loss: 0.0016193676224599282\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013612599455647998\n",
      "Average test loss: 0.0016160872284736898\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013566769933534993\n",
      "Average test loss: 0.0016672061258513067\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01352942822376887\n",
      "Average test loss: 0.0016279848168293636\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013528837107949786\n",
      "Average test loss: 0.0016738292071968317\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013527992453840045\n",
      "Average test loss: 0.0016228355647375187\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013504269859857029\n",
      "Average test loss: 0.0016707935185275144\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013554638038906787\n",
      "Average test loss: 0.0016459981039580371\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013510693376262983\n",
      "Average test loss: 0.001652139532690247\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013480724458065299\n",
      "Average test loss: 0.0016940096536030372\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013480184863011042\n",
      "Average test loss: 0.0016090891564057932\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01350039529055357\n",
      "Average test loss: 0.0016331593804061412\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013463997167017724\n",
      "Average test loss: 0.0015908693702270588\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013456578260494604\n",
      "Average test loss: 0.0017029187654455504\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013457621448569828\n",
      "Average test loss: 0.0016464488653259144\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013452428549528122\n",
      "Average test loss: 0.0015935570106117261\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013423888220555253\n",
      "Average test loss: 0.0016645649661206536\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01344502783815066\n",
      "Average test loss: 0.001653213715698156\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01342569132314788\n",
      "Average test loss: 0.0016681252974602911\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01337850021902058\n",
      "Average test loss: 0.001610976342836188\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013387187536391947\n",
      "Average test loss: 0.0016266061991660132\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01342760530528095\n",
      "Average test loss: 0.001629864581550161\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013376677984992664\n",
      "Average test loss: 0.0016301884815717736\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013372081947823366\n",
      "Average test loss: 0.0017058403802414734\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013365681764152315\n",
      "Average test loss: 0.0016482359218514629\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013356457451979319\n",
      "Average test loss: 0.0016494885654085212\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01331969046427144\n",
      "Average test loss: 0.0016523687434382736\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01331226553933488\n",
      "Average test loss: 0.0017003088942211535\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013330290333264404\n",
      "Average test loss: 0.0016684534835318724\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01333158383684026\n",
      "Average test loss: 0.0016190238305264048\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013337054706282086\n",
      "Average test loss: 0.0016515006789316734\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013328955591552788\n",
      "Average test loss: 0.0016214680225691861\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013309927077756988\n",
      "Average test loss: 0.001637105504464772\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013329182431929642\n",
      "Average test loss: 0.0016920202682829566\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013260928379164802\n",
      "Average test loss: 0.0015875516550408468\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013277541742556624\n",
      "Average test loss: 0.0016949925311944552\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013293018288082546\n",
      "Average test loss: 0.0016204272947377629\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013291581131103966\n",
      "Average test loss: 0.0016920095753545563\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01328557226061821\n",
      "Average test loss: 0.0016837259534125527\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013270366567704413\n",
      "Average test loss: 0.0016906355252075526\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013265363050831688\n",
      "Average test loss: 0.001629098381743663\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01326701967128449\n",
      "Average test loss: 0.001675782628564371\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01322128665778372\n",
      "Average test loss: 0.0016439279512398773\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01323037008775605\n",
      "Average test loss: 0.0016884662432389127\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013242068395846419\n",
      "Average test loss: 0.001665772979458173\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013197007723152637\n",
      "Average test loss: 0.0017595001585367653\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013243195366528298\n",
      "Average test loss: 0.0016606764857553774\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013182444823284943\n",
      "Average test loss: 0.0016795552456751466\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013204685643315315\n",
      "Average test loss: 0.0016045672284025285\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013215780819455783\n",
      "Average test loss: 0.001655841647543841\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013186839391787848\n",
      "Average test loss: 0.001623455045796517\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01319458497232861\n",
      "Average test loss: 0.0016755474913451407\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_50_Depth10/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.65\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.09\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.94\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.72\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.21\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.69\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.83\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.90\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.12\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.63\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.84\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.43\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.53\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.81\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.98\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.41\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.20\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.65\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.86\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 33.459253119574655\n",
      "Average test loss: 0.0073249077329205145\n",
      "Epoch 2/300\n",
      "Average training loss: 18.557746944851345\n",
      "Average test loss: 0.01070184255308575\n",
      "Epoch 3/300\n",
      "Average training loss: 15.705365370856391\n",
      "Average test loss: 0.0247858456985818\n",
      "Epoch 4/300\n",
      "Average training loss: 13.461928529527452\n",
      "Average test loss: 0.08859686212903924\n",
      "Epoch 5/300\n",
      "Average training loss: 11.958123305426703\n",
      "Average test loss: 0.0046904668956995015\n",
      "Epoch 6/300\n",
      "Average training loss: 9.51011745707194\n",
      "Average test loss: 0.9985720879501767\n",
      "Epoch 7/300\n",
      "Average training loss: 8.933740901523166\n",
      "Average test loss: 0.005484416634258297\n",
      "Epoch 8/300\n",
      "Average training loss: 8.399603394402398\n",
      "Average test loss: 0.004372637375775311\n",
      "Epoch 9/300\n",
      "Average training loss: 7.10397168774075\n",
      "Average test loss: 0.004355277381423447\n",
      "Epoch 10/300\n",
      "Average training loss: 5.847796313815647\n",
      "Average test loss: 0.014865149163123634\n",
      "Epoch 11/300\n",
      "Average training loss: 5.215333211686876\n",
      "Average test loss: 0.0042543967362079355\n",
      "Epoch 12/300\n",
      "Average training loss: 4.287979562123617\n",
      "Average test loss: 0.004217208438863357\n",
      "Epoch 13/300\n",
      "Average training loss: 3.8901441340976293\n",
      "Average test loss: 0.004210059461494287\n",
      "Epoch 14/300\n",
      "Average training loss: 3.029944607840644\n",
      "Average test loss: 0.004285526742537816\n",
      "Epoch 15/300\n",
      "Average training loss: 2.3891596592797173\n",
      "Average test loss: 0.009786351937386724\n",
      "Epoch 16/300\n",
      "Average training loss: 2.069928760740492\n",
      "Average test loss: 0.00790139478465749\n",
      "Epoch 17/300\n",
      "Average training loss: 1.8306745007832845\n",
      "Average test loss: 0.00453887214884162\n",
      "Epoch 18/300\n",
      "Average training loss: 1.427351942062378\n",
      "Average test loss: 0.004797872650954459\n",
      "Epoch 19/300\n",
      "Average training loss: 1.2691495361328125\n",
      "Average test loss: 0.0040876680951979424\n",
      "Epoch 20/300\n",
      "Average training loss: 1.06975001584159\n",
      "Average test loss: 0.004636825966752238\n",
      "Epoch 21/300\n",
      "Average training loss: 0.892970177491506\n",
      "Average test loss: 0.008618940583947632\n",
      "Epoch 22/300\n",
      "Average training loss: 0.741375968032413\n",
      "Average test loss: 0.006030463551067644\n",
      "Epoch 23/300\n",
      "Average training loss: 0.6316940633985731\n",
      "Average test loss: 0.004629111860568325\n",
      "Epoch 24/300\n",
      "Average training loss: 0.5177424654430813\n",
      "Average test loss: 0.014148020622630915\n",
      "Epoch 25/300\n",
      "Average training loss: 0.4307714236312442\n",
      "Average test loss: 0.004015128150996234\n",
      "Epoch 26/300\n",
      "Average training loss: 0.3681586607562171\n",
      "Average test loss: 0.011423623823043372\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3218009866078695\n",
      "Average test loss: 0.004378077019626896\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2844187386300829\n",
      "Average test loss: 1.316460567240086\n",
      "Epoch 29/300\n",
      "Average training loss: 0.25239976927969193\n",
      "Average test loss: 0.05085102939274576\n",
      "Epoch 30/300\n",
      "Average training loss: 0.22860898068216112\n",
      "Average test loss: 0.004001545855154593\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2085520678758621\n",
      "Average test loss: 0.004080335318007403\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1921817405488756\n",
      "Average test loss: 0.003956891637295484\n",
      "Epoch 33/300\n",
      "Average training loss: 0.17882833545737797\n",
      "Average test loss: 0.003939452737569809\n",
      "Epoch 34/300\n",
      "Average training loss: 0.16825336303975846\n",
      "Average test loss: 0.004020495798852709\n",
      "Epoch 35/300\n",
      "Average training loss: 0.15950489078627691\n",
      "Average test loss: 0.0039389564297679395\n",
      "Epoch 36/300\n",
      "Average training loss: 0.152560785346561\n",
      "Average test loss: 0.03632147907382912\n",
      "Epoch 37/300\n",
      "Average training loss: 0.14865728645854526\n",
      "Average test loss: 0.00727275024023321\n",
      "Epoch 38/300\n",
      "Average training loss: 0.14253406896856097\n",
      "Average test loss: 0.003907692408396138\n",
      "Epoch 39/300\n",
      "Average training loss: 0.137866316722499\n",
      "Average test loss: 0.00392691545188427\n",
      "Epoch 40/300\n",
      "Average training loss: 0.13461719718244342\n",
      "Average test loss: 0.003887484865470065\n",
      "Epoch 41/300\n",
      "Average training loss: 0.13201901903417376\n",
      "Average test loss: 0.003906394977536466\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12976024319728216\n",
      "Average test loss: 0.003911221660259697\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1278804576926761\n",
      "Average test loss: 0.003879813120596939\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12627987287441889\n",
      "Average test loss: 0.003911248167562816\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12480318380726708\n",
      "Average test loss: 0.003979208128733767\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12362115960650974\n",
      "Average test loss: 0.003955424284769429\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1227886731757058\n",
      "Average test loss: 0.003892610825184319\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12189814274840885\n",
      "Average test loss: 0.0038864396404888896\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12113724420468013\n",
      "Average test loss: 0.003920646134556996\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12052372856272592\n",
      "Average test loss: 0.003914594685658812\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11987711240185632\n",
      "Average test loss: 0.003877934488778313\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11933929772509469\n",
      "Average test loss: 0.0038943387038177916\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11883002410332362\n",
      "Average test loss: 0.003988666727311081\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11829840103785197\n",
      "Average test loss: 0.003921497688111332\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1178881403737598\n",
      "Average test loss: 0.003911469031953149\n",
      "Epoch 56/300\n",
      "Average training loss: 0.117283900141716\n",
      "Average test loss: 0.0039013608362939625\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11685772439506319\n",
      "Average test loss: 0.003914608269515965\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11638066320949131\n",
      "Average test loss: 0.0039069866111709015\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11603390604919857\n",
      "Average test loss: 0.003917970556558834\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11568789614571465\n",
      "Average test loss: 0.003976944414277871\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11540220532814661\n",
      "Average test loss: 0.003923402628137005\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11488863811890285\n",
      "Average test loss: 0.003959136525169015\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11454331890079711\n",
      "Average test loss: 0.003938286004381048\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11436133776108424\n",
      "Average test loss: 0.0038695636606878706\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11384335382117165\n",
      "Average test loss: 0.003912076804372999\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11336318517393536\n",
      "Average test loss: 0.003924548965982265\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1130890190800031\n",
      "Average test loss: 0.00403220537967152\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11285905738671621\n",
      "Average test loss: 0.003949841243111425\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11220247350136439\n",
      "Average test loss: 0.004000170937739312\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11197775355974833\n",
      "Average test loss: 0.0039027792734818325\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11176388214694129\n",
      "Average test loss: 0.0039055439417974816\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11131900978750653\n",
      "Average test loss: 0.0039933438853671155\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1112320176296764\n",
      "Average test loss: 0.003977689039996929\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11044215012921227\n",
      "Average test loss: 0.003904290310624573\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11020789419942432\n",
      "Average test loss: 0.003944597109324402\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10997871117459403\n",
      "Average test loss: 0.004035451227592098\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1094865171511968\n",
      "Average test loss: 0.003938772936041156\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10909801599052217\n",
      "Average test loss: 0.003963752423930499\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10897120928102069\n",
      "Average test loss: 0.0039151763344804445\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10823137798574235\n",
      "Average test loss: 0.004043056420981884\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10799998786052067\n",
      "Average test loss: 0.003966583811574512\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10753917749722798\n",
      "Average test loss: 0.003887423555470175\n",
      "Epoch 83/300\n",
      "Average training loss: 0.1071355326043235\n",
      "Average test loss: 0.004023714308937391\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10690123923619588\n",
      "Average test loss: 0.004032099355840021\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10648278707265854\n",
      "Average test loss: 0.003976588840285937\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10606497003634771\n",
      "Average test loss: 0.004019055796787143\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10600299925936593\n",
      "Average test loss: 0.003940205225721001\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10533894100454118\n",
      "Average test loss: 0.004071623633926114\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10489324065049489\n",
      "Average test loss: 0.003990202870633867\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10476407472954856\n",
      "Average test loss: 0.003988091426177157\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10416522361834844\n",
      "Average test loss: 0.004005503976510631\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1038976876007186\n",
      "Average test loss: 0.0039614280940343935\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10350708738962809\n",
      "Average test loss: 0.004076827569347289\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10326586516035928\n",
      "Average test loss: 0.003987705998122692\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10286311613188849\n",
      "Average test loss: 0.0039447649086101185\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10253582045104769\n",
      "Average test loss: 0.003959974334057834\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10236715935336219\n",
      "Average test loss: 0.004059195542087158\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10198767405748367\n",
      "Average test loss: 0.004008281408912606\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10137911000516679\n",
      "Average test loss: 0.004036443432999982\n",
      "Epoch 100/300\n",
      "Average training loss: 0.1011535935666826\n",
      "Average test loss: 0.004074578190636303\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10083536217610041\n",
      "Average test loss: 0.004121887034219172\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10056634238693449\n",
      "Average test loss: 0.003987808568610085\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10042389067676333\n",
      "Average test loss: 0.004035385083407164\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09976199002398385\n",
      "Average test loss: 0.004086971679495441\n",
      "Epoch 105/300\n",
      "Average training loss: 0.09950308518939548\n",
      "Average test loss: 0.004055482652452257\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09935423154963387\n",
      "Average test loss: 0.004049306496770846\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09903319081995222\n",
      "Average test loss: 0.004220997452735901\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09854303809669283\n",
      "Average test loss: 0.004044808756767048\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09822491391499837\n",
      "Average test loss: 0.004068571142852306\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09794403475522995\n",
      "Average test loss: 0.004201731166077985\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09769877253638373\n",
      "Average test loss: 0.004096948320666949\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09746682498521275\n",
      "Average test loss: 0.00418330234537522\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09717292226685419\n",
      "Average test loss: 0.004128313430688447\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09722084871927897\n",
      "Average test loss: 0.004224537192119493\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09670294705364439\n",
      "Average test loss: 0.004044870864186022\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09632774147722456\n",
      "Average test loss: 0.004144170378645261\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09596434427632226\n",
      "Average test loss: 0.0040980870173209245\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09644233707586924\n",
      "Average test loss: 0.004157929491665628\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09546001659499274\n",
      "Average test loss: 0.004087041929571165\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09519064460198084\n",
      "Average test loss: 0.004186385963112116\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09515704758299721\n",
      "Average test loss: 0.004091023706520597\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09475518549150891\n",
      "Average test loss: 0.0040610342025756835\n",
      "Epoch 123/300\n",
      "Average training loss: 0.094428264008628\n",
      "Average test loss: 0.00413768642478519\n",
      "Epoch 124/300\n",
      "Average training loss: 0.09448265771071117\n",
      "Average test loss: 0.0041941947593457166\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09419680204656389\n",
      "Average test loss: 0.004059989184141159\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09359284950627221\n",
      "Average test loss: 0.004072237220903238\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09343140458398395\n",
      "Average test loss: 0.00413532486351\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09321984131468666\n",
      "Average test loss: 0.004309534560475084\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0929865046342214\n",
      "Average test loss: 0.004282099356667863\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09287157062027189\n",
      "Average test loss: 0.0042343831022994384\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09266621488001611\n",
      "Average test loss: 0.004125334947887394\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09236562064621183\n",
      "Average test loss: 0.004222196257155803\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09201935680045022\n",
      "Average test loss: 0.004063926674218642\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09182725509007772\n",
      "Average test loss: 0.004071172491543823\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09174645816617542\n",
      "Average test loss: 0.004277685154643324\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09164811850587527\n",
      "Average test loss: 0.004372077354954348\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09131515584389369\n",
      "Average test loss: 0.0042395883856548204\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09106379906336466\n",
      "Average test loss: 0.004245635299219026\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09083958378103044\n",
      "Average test loss: 0.004307868604858716\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09055270834763844\n",
      "Average test loss: 0.0041412043530080055\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09047542765405443\n",
      "Average test loss: 0.004210262841855487\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09024171149068408\n",
      "Average test loss: 0.004169706862833765\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08993758202923668\n",
      "Average test loss: 0.004167692657146189\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08964722441302406\n",
      "Average test loss: 0.004207024626226889\n",
      "Epoch 145/300\n",
      "Average training loss: 0.08961028018262651\n",
      "Average test loss: 0.004104116731219821\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0893259763651424\n",
      "Average test loss: 0.004234524163934919\n",
      "Epoch 147/300\n",
      "Average training loss: 0.089211495452457\n",
      "Average test loss: 0.004216569596487615\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08904326728979746\n",
      "Average test loss: 0.00414839654084709\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08895301530096265\n",
      "Average test loss: 0.004267557581265767\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08862945938110352\n",
      "Average test loss: 0.004161827893720733\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08832881903648376\n",
      "Average test loss: 0.004089091232253445\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08840105736917919\n",
      "Average test loss: 0.00419385055369801\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08809909198019239\n",
      "Average test loss: 0.004201808732416895\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08799853783845901\n",
      "Average test loss: 0.004193895750989517\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08761875702275171\n",
      "Average test loss: 0.004141835288662049\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08754949465062883\n",
      "Average test loss: 0.004201100140810013\n",
      "Epoch 157/300\n",
      "Average training loss: 0.087363102250629\n",
      "Average test loss: 0.004115946932385365\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08723113483852811\n",
      "Average test loss: 0.004237796101719141\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0871095850997501\n",
      "Average test loss: 0.004209177806559536\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08677657993634542\n",
      "Average test loss: 0.0041037516887817115\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08659812561670939\n",
      "Average test loss: 0.004201898357727461\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0866765903631846\n",
      "Average test loss: 0.0043075927942991254\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0868594912091891\n",
      "Average test loss: 0.004264323373221689\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0862866238090727\n",
      "Average test loss: 0.004189390055421326\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0860445636941327\n",
      "Average test loss: 0.00414957096034454\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08597411001390881\n",
      "Average test loss: 0.004169577010389831\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08585571442047756\n",
      "Average test loss: 0.004335649621983369\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08558801267544429\n",
      "Average test loss: 0.004270607719818751\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08549042074547873\n",
      "Average test loss: 0.004529735194312202\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08548977353837756\n",
      "Average test loss: 0.004133938412285513\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08509959743089146\n",
      "Average test loss: 0.004530965265093578\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0849882214334276\n",
      "Average test loss: 0.004383167569215099\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08487737210922772\n",
      "Average test loss: 0.00421440514177084\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08492643138104015\n",
      "Average test loss: 0.004180938677241405\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08464506461222966\n",
      "Average test loss: 0.00427788504668408\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08445537078380584\n",
      "Average test loss: 0.004313888746624191\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08432228697008556\n",
      "Average test loss: 0.004171925688783328\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08404797060622109\n",
      "Average test loss: 0.004288028308086925\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08413042670488358\n",
      "Average test loss: 0.004102755495864484\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08392490481005775\n",
      "Average test loss: 0.004388527513792117\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08380918237235811\n",
      "Average test loss: 0.004300397934599055\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08363615302907096\n",
      "Average test loss: 0.0042579721589055325\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08336133951942126\n",
      "Average test loss: 0.004303959336959653\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08329933486713303\n",
      "Average test loss: 0.004239586208222641\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08323853772216373\n",
      "Average test loss: 0.004226802782052093\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08306048900551266\n",
      "Average test loss: 0.00419876331422064\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0830041141708692\n",
      "Average test loss: 0.0042743507785101735\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0829116616845131\n",
      "Average test loss: 0.004197041872888803\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0825952857600318\n",
      "Average test loss: 0.0041852969125741055\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08250735538866785\n",
      "Average test loss: 0.004145555520223247\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08239146503474977\n",
      "Average test loss: 0.0041522656078967785\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08216197929779688\n",
      "Average test loss: 0.0042459399188972185\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08222332467635472\n",
      "Average test loss: 0.004116627647437983\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08217170205712318\n",
      "Average test loss: 0.004441181029710505\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08179831128650242\n",
      "Average test loss: 0.004183787518077427\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08177855305539238\n",
      "Average test loss: 0.004253516440797183\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08160578176710341\n",
      "Average test loss: 0.004256312178240882\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0816555975874265\n",
      "Average test loss: 0.004314447157498863\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08139435817466842\n",
      "Average test loss: 0.0043226860049698085\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08127219140529633\n",
      "Average test loss: 0.004380984419749843\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08150074265400568\n",
      "Average test loss: 0.004302850312242906\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0810688412586848\n",
      "Average test loss: 0.004182342817799912\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0809166067706214\n",
      "Average test loss: 0.004240292980853054\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08079590051041709\n",
      "Average test loss: 0.004232859055201213\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08070134243700239\n",
      "Average test loss: 0.00415852021984756\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08088258859846327\n",
      "Average test loss: 0.004344922086844841\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08093562755319807\n",
      "Average test loss: 0.004334382308440076\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08030043488409784\n",
      "Average test loss: 0.004279072373691533\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08014645980464087\n",
      "Average test loss: 0.004170993846737676\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08014319615893893\n",
      "Average test loss: 0.0041278840580748185\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08009260093503529\n",
      "Average test loss: 0.0042006874208649\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08026987579133775\n",
      "Average test loss: 0.004290850958890385\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08029247863425149\n",
      "Average test loss: 0.004310900389941202\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0797168775598208\n",
      "Average test loss: 0.004458525563279788\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07961810378233591\n",
      "Average test loss: 0.004275963341610299\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07963725223143896\n",
      "Average test loss: 0.004262782472703192\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07952124534050624\n",
      "Average test loss: 0.0042339622332817975\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07936292539702522\n",
      "Average test loss: 0.004314865548577573\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07915245580010943\n",
      "Average test loss: 0.00418328158557415\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07907369007998043\n",
      "Average test loss: 0.004291580766025517\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07906483334302902\n",
      "Average test loss: 0.004234328186139464\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07921891113122305\n",
      "Average test loss: 0.004355337041533656\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07880618405342102\n",
      "Average test loss: 0.004281583494610257\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07884976423448986\n",
      "Average test loss: 0.004341305181177126\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07891368146737417\n",
      "Average test loss: 0.0042681321191291016\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07858786632617315\n",
      "Average test loss: 0.004204894539382722\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0784043373465538\n",
      "Average test loss: 0.004243499523856574\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07845201219121616\n",
      "Average test loss: 0.004365387595362134\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0783410070406066\n",
      "Average test loss: 0.004259211079320974\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07810128002696567\n",
      "Average test loss: 0.004287780455003182\n",
      "Epoch 231/300\n",
      "Average training loss: 0.078125283645259\n",
      "Average test loss: 0.004596234503719542\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07797768715686268\n",
      "Average test loss: 0.004275481429985828\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07792139801051881\n",
      "Average test loss: 0.004516118300457795\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07798437848024899\n",
      "Average test loss: 0.004289892138499353\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07774066652854283\n",
      "Average test loss: 0.004230598639696837\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07759929776191711\n",
      "Average test loss: 0.004231510025966499\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07771656978792614\n",
      "Average test loss: 0.004318661387181944\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07743657721413506\n",
      "Average test loss: 0.004512859827114476\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07752034638325374\n",
      "Average test loss: 0.0042712036553356385\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07714497505956226\n",
      "Average test loss: 0.004118015524620811\n",
      "Epoch 241/300\n",
      "Average training loss: 0.077252566450172\n",
      "Average test loss: 0.004323510753611723\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07704930853843689\n",
      "Average test loss: 0.004335489679748813\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0770293935139974\n",
      "Average test loss: 0.0042440099728604155\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07696468745337592\n",
      "Average test loss: 0.0042623390977581345\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0768245360718833\n",
      "Average test loss: 0.004286615558382538\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07679269976417223\n",
      "Average test loss: 0.004269441662149297\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07675513860914442\n",
      "Average test loss: 0.004451498323430617\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07642752579185698\n",
      "Average test loss: 0.00426974696955747\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07668823252121608\n",
      "Average test loss: 0.004321161107884513\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07648574639028972\n",
      "Average test loss: 0.0041905895806849\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07639097505807876\n",
      "Average test loss: 0.004295371691799826\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07637338280015521\n",
      "Average test loss: 0.004237771630494131\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07605119967460633\n",
      "Average test loss: 0.004272018496361044\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07601833230919308\n",
      "Average test loss: 0.004284665710810158\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07595117667648528\n",
      "Average test loss: 0.004335514081435071\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07594247151745691\n",
      "Average test loss: 0.00434186856697003\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07585625843869315\n",
      "Average test loss: 0.0041556908844245805\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07620183621512519\n",
      "Average test loss: 0.00420405964470572\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07566527625587252\n",
      "Average test loss: 0.004302219468272394\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07565917389922672\n",
      "Average test loss: 0.004314558673236105\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07542593727509181\n",
      "Average test loss: 0.004188943138966957\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07556914953059621\n",
      "Average test loss: 0.004332591315110525\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07541682935423322\n",
      "Average test loss: 0.004313586552109983\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07549234784973992\n",
      "Average test loss: 0.004237624581696259\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07511131486627791\n",
      "Average test loss: 0.004234409876167774\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07513700880606969\n",
      "Average test loss: 0.004277826163296898\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07523661932680342\n",
      "Average test loss: 0.004235448069249591\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07482235936323801\n",
      "Average test loss: 0.00418035015070604\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07498187915815248\n",
      "Average test loss: 0.004323463914295038\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07491183463401264\n",
      "Average test loss: 0.004374688291715251\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07485318217674891\n",
      "Average test loss: 0.0043354757490257426\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07464363082912233\n",
      "Average test loss: 0.004271339649955431\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0745935144159529\n",
      "Average test loss: 0.004239240191049046\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0745520015027788\n",
      "Average test loss: 0.004213878596925901\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07442396458652284\n",
      "Average test loss: 0.004233241191754738\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07436212055550681\n",
      "Average test loss: 0.004406854319489665\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07427662416299184\n",
      "Average test loss: 0.004275762486788961\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07439108168416553\n",
      "Average test loss: 0.0045952472674349945\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07423430264658398\n",
      "Average test loss: 0.00416700747071041\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07415701287322574\n",
      "Average test loss: 0.0043506172237296895\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07418346389796998\n",
      "Average test loss: 0.004220965016219351\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07420179243882497\n",
      "Average test loss: 0.004350049746533235\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07415056723356248\n",
      "Average test loss: 0.0043644065467847715\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07369344186782836\n",
      "Average test loss: 0.004297830346971751\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0738884434401989\n",
      "Average test loss: 0.004234250588549508\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07376611246003045\n",
      "Average test loss: 0.004243010307765669\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07362102539009518\n",
      "Average test loss: 0.004172173363880979\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07355204166968664\n",
      "Average test loss: 0.004414624088340336\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07361279708147049\n",
      "Average test loss: 0.004238783744060331\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07377633158034748\n",
      "Average test loss: 0.004255113230397304\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07325545769267612\n",
      "Average test loss: 0.0043201991431415085\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07332632876104779\n",
      "Average test loss: 0.004417595755722788\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07334585106041697\n",
      "Average test loss: 0.004296758091077209\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07320325454738405\n",
      "Average test loss: 0.004323723547160626\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07309391358163622\n",
      "Average test loss: 0.0041879425396521884\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0731324361231592\n",
      "Average test loss: 0.004308196836875545\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0730061252646976\n",
      "Average test loss: 0.004315178281524115\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07291385182407167\n",
      "Average test loss: 0.0042344511517634\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0728561942577362\n",
      "Average test loss: 0.004200877801204721\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07277187510331472\n",
      "Average test loss: 0.004374675440912445\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 33.082629104614256\n",
      "Average test loss: 1.861005929592583\n",
      "Epoch 2/300\n",
      "Average training loss: 17.330251815795897\n",
      "Average test loss: 0.004896840256121424\n",
      "Epoch 3/300\n",
      "Average training loss: 11.980308450486925\n",
      "Average test loss: 0.02326024391419358\n",
      "Epoch 4/300\n",
      "Average training loss: 9.19486222161187\n",
      "Average test loss: 0.00420496241748333\n",
      "Epoch 5/300\n",
      "Average training loss: 7.857945698632134\n",
      "Average test loss: 0.004265464355962144\n",
      "Epoch 6/300\n",
      "Average training loss: 6.173058551364475\n",
      "Average test loss: 0.004065821594662136\n",
      "Epoch 7/300\n",
      "Average training loss: 5.704183570014106\n",
      "Average test loss: 0.003903609865448541\n",
      "Epoch 8/300\n",
      "Average training loss: 4.689100708855523\n",
      "Average test loss: 0.0037428478584107428\n",
      "Epoch 9/300\n",
      "Average training loss: 3.786181795756022\n",
      "Average test loss: 0.316021118024985\n",
      "Epoch 10/300\n",
      "Average training loss: 3.2651378854115802\n",
      "Average test loss: 0.003629135937119524\n",
      "Epoch 11/300\n",
      "Average training loss: 2.6249234375423853\n",
      "Average test loss: 0.01265080764889717\n",
      "Epoch 12/300\n",
      "Average training loss: 2.1480076804690893\n",
      "Average test loss: 0.10400118890528877\n",
      "Epoch 13/300\n",
      "Average training loss: 1.7453557449976602\n",
      "Average test loss: 0.022034616039858925\n",
      "Epoch 14/300\n",
      "Average training loss: 1.7661289902793036\n",
      "Average test loss: 0.004088152443783151\n",
      "Epoch 15/300\n",
      "Average training loss: 1.5199342663023208\n",
      "Average test loss: 41.767262481557\n",
      "Epoch 16/300\n",
      "Average training loss: 1.2764375646379258\n",
      "Average test loss: 8.384328953184394\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0992622998025683\n",
      "Average test loss: 0.0393498655528658\n",
      "Epoch 18/300\n",
      "Average training loss: 0.8967410660319858\n",
      "Average test loss: 0.004781118007997672\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7655472482575311\n",
      "Average test loss: 0.1177325710033377\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6560339965290494\n",
      "Average test loss: 0.017136274056302178\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5565303498903911\n",
      "Average test loss: 0.004699833417932193\n",
      "Epoch 22/300\n",
      "Average training loss: 0.4680966650115119\n",
      "Average test loss: 0.0039041498491747513\n",
      "Epoch 23/300\n",
      "Average training loss: 0.4007405011124081\n",
      "Average test loss: 0.003109611187958055\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3462333796289232\n",
      "Average test loss: 0.003081358973764711\n",
      "Epoch 25/300\n",
      "Average training loss: 0.30217456150054933\n",
      "Average test loss: 0.007556448661204841\n",
      "Epoch 26/300\n",
      "Average training loss: 0.26747531153096094\n",
      "Average test loss: 0.00304419329472714\n",
      "Epoch 27/300\n",
      "Average training loss: 0.23831331462330288\n",
      "Average test loss: 0.0029954463037558726\n",
      "Epoch 28/300\n",
      "Average training loss: 0.21441182949807908\n",
      "Average test loss: 0.00297649480899175\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1942241145107481\n",
      "Average test loss: 0.0032818549962507354\n",
      "Epoch 30/300\n",
      "Average training loss: 0.17717203274038104\n",
      "Average test loss: 0.00296938201578127\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1629250743786494\n",
      "Average test loss: 0.0029533284437946146\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15091915958457522\n",
      "Average test loss: 0.002926559224517809\n",
      "Epoch 33/300\n",
      "Average training loss: 0.14085171557797327\n",
      "Average test loss: 0.0029194712477425733\n",
      "Epoch 34/300\n",
      "Average training loss: 0.13266993788878123\n",
      "Average test loss: 0.0029230770069277948\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1256799434290992\n",
      "Average test loss: 0.00290058009698987\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12022108422385322\n",
      "Average test loss: 0.002911538210593992\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11545924780766169\n",
      "Average test loss: 0.0029039129184352027\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11143203915490045\n",
      "Average test loss: 0.0029457745848016606\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10821366335286034\n",
      "Average test loss: 0.002875352640532785\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10563720207744175\n",
      "Average test loss: 0.002942138453428116\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10310196123520533\n",
      "Average test loss: 0.0028520410789383784\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10116109989749061\n",
      "Average test loss: 0.002919470310004221\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09949763694074419\n",
      "Average test loss: 0.002866108331415388\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09790117637647523\n",
      "Average test loss: 0.0029591529321753318\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09672161412570211\n",
      "Average test loss: 0.002935610071548985\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09547368837396304\n",
      "Average test loss: 0.0028435553997341128\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09438233051035139\n",
      "Average test loss: 0.0028869719434943463\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09337738539775213\n",
      "Average test loss: 0.0028716904695870148\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09251195266511705\n",
      "Average test loss: 0.002817139052889413\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09167872178885672\n",
      "Average test loss: 0.0028618008751008246\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09072940108511184\n",
      "Average test loss: 0.002970262746223145\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09011573102076849\n",
      "Average test loss: 0.0028535736728873516\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08948909785350163\n",
      "Average test loss: 0.0029475429898334874\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08847941139009263\n",
      "Average test loss: 0.0029357777217196095\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0878563474814097\n",
      "Average test loss: 0.0028542726669046614\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08715937527020773\n",
      "Average test loss: 0.0028103409183935987\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08628505507773823\n",
      "Average test loss: 0.0029637174798796575\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08571393357382881\n",
      "Average test loss: 0.0028320724316355256\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08519903801547156\n",
      "Average test loss: 0.002847854878132542\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08442818876107534\n",
      "Average test loss: 0.00283007044179572\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08386302670505312\n",
      "Average test loss: 0.0028841388171745672\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08325352498557832\n",
      "Average test loss: 0.0028050737803181013\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08233436510297987\n",
      "Average test loss: 0.002989219179169999\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08191614992751016\n",
      "Average test loss: 0.002926447222423222\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08127315661642287\n",
      "Average test loss: 0.0028558330877373615\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08078504410054949\n",
      "Average test loss: 0.0028534952828453645\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08005685216850704\n",
      "Average test loss: 0.0028812656162513625\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07951294444666969\n",
      "Average test loss: 0.003003255371418264\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07920212627119488\n",
      "Average test loss: 0.002898091631838017\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07842550770772828\n",
      "Average test loss: 0.002933007045752472\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07790840407212575\n",
      "Average test loss: 0.0028319965744805003\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07813546213176516\n",
      "Average test loss: 0.002859422852595647\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07733275623122851\n",
      "Average test loss: 0.0029519837126135828\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07646405004792743\n",
      "Average test loss: 0.002879617441445589\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07621191875802145\n",
      "Average test loss: 0.002936578162221445\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07580426902572314\n",
      "Average test loss: 0.002946165286625425\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07497102698352602\n",
      "Average test loss: 0.0029027620990657145\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07460378509759903\n",
      "Average test loss: 0.0029528533166481388\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0743212660253048\n",
      "Average test loss: 0.0029233259207879503\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07378761821322971\n",
      "Average test loss: 0.0028642909748272764\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07342236698336072\n",
      "Average test loss: 0.0029083229017754396\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07325610378384591\n",
      "Average test loss: 0.0029147950951009987\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07268724504113197\n",
      "Average test loss: 0.0029543787002977396\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07214349208606614\n",
      "Average test loss: 0.002987057599549492\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0725443433754974\n",
      "Average test loss: 0.002906351680556933\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0717658990820249\n",
      "Average test loss: 0.00291737477046748\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07122665517197715\n",
      "Average test loss: 0.0029097725564820897\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07058845621347427\n",
      "Average test loss: 0.0029776622170789376\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07044041575988133\n",
      "Average test loss: 0.002924334072921839\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0701800602806939\n",
      "Average test loss: 0.002998418237807022\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06970095775524775\n",
      "Average test loss: 0.0029355382836527295\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07023947483301163\n",
      "Average test loss: 0.0028575256243348122\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06924489107396868\n",
      "Average test loss: 0.002901689793707596\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06871960643927257\n",
      "Average test loss: 0.0029347354413734542\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06875087081061469\n",
      "Average test loss: 0.0029987300862040784\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06831904529531797\n",
      "Average test loss: 0.002967974733354317\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06885595873660512\n",
      "Average test loss: 0.003046212798708843\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06776057745350732\n",
      "Average test loss: 0.0028894973871194654\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06731933305329747\n",
      "Average test loss: 0.0029649184602830144\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06713877422610919\n",
      "Average test loss: 0.002980289804852671\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06684949607319302\n",
      "Average test loss: 0.003017932310907377\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06680699539515707\n",
      "Average test loss: 0.0029788588556564515\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06648866649137603\n",
      "Average test loss: 0.003027373016294506\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06655650501449903\n",
      "Average test loss: 0.002956582779685656\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06609042812056011\n",
      "Average test loss: 0.0029919077858535778\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06613322369919883\n",
      "Average test loss: 0.003080861666136318\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06607347418864568\n",
      "Average test loss: 0.00294241163900329\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06537652117345068\n",
      "Average test loss: 0.0029537740353908805\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0650326505402724\n",
      "Average test loss: 0.0030538965865141814\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06471603287922012\n",
      "Average test loss: 0.0029227415197011497\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06459863645831744\n",
      "Average test loss: 0.0030912851151078937\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06433365410897467\n",
      "Average test loss: 0.003119980288359026\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06413937345147133\n",
      "Average test loss: 0.0030495437756180763\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06410063157810106\n",
      "Average test loss: 0.002936861921929651\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06371205575929748\n",
      "Average test loss: 0.0030452047348436383\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06350350120663643\n",
      "Average test loss: 0.0030183042440977362\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06326583134796884\n",
      "Average test loss: 0.0030663737449795007\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06317984331978692\n",
      "Average test loss: 0.0030089571815398006\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06289320856663916\n",
      "Average test loss: 0.0031977828159514402\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06264676206641727\n",
      "Average test loss: 0.0029744185416234865\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06252812616361512\n",
      "Average test loss: 0.003066210688195295\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06262921398546961\n",
      "Average test loss: 0.003064369123842981\n",
      "Epoch 123/300\n",
      "Average training loss: 0.062128869546784295\n",
      "Average test loss: 0.0030574636528682376\n",
      "Epoch 124/300\n",
      "Average training loss: 0.062075942311022016\n",
      "Average test loss: 0.0030491139313413036\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06177046360903316\n",
      "Average test loss: 0.003145976250784265\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06160893065068457\n",
      "Average test loss: 0.003054458389783071\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06136902816096942\n",
      "Average test loss: 0.003104925709259179\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06140062252349324\n",
      "Average test loss: 0.003063065087215768\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06111836137043105\n",
      "Average test loss: 0.003069184701061911\n",
      "Epoch 130/300\n",
      "Average training loss: 0.061038953721523286\n",
      "Average test loss: 0.003101092655832569\n",
      "Epoch 131/300\n",
      "Average training loss: 0.060791416489415696\n",
      "Average test loss: 0.0030731938771075672\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06066183872355355\n",
      "Average test loss: 0.00304433127037353\n",
      "Epoch 133/300\n",
      "Average training loss: 0.060728673070669176\n",
      "Average test loss: 0.003052618111690713\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06042577874991629\n",
      "Average test loss: 0.0030074672748645146\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06028082893292109\n",
      "Average test loss: 0.003097236283537414\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0601853307320012\n",
      "Average test loss: 0.003076323504766656\n",
      "Epoch 137/300\n",
      "Average training loss: 0.059992236061228645\n",
      "Average test loss: 0.0030368020551072225\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05978641259339121\n",
      "Average test loss: 0.003062602218861381\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05962508643335766\n",
      "Average test loss: 0.003100347030080027\n",
      "Epoch 140/300\n",
      "Average training loss: 0.059352822459406325\n",
      "Average test loss: 0.003021571217932635\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05941898542642594\n",
      "Average test loss: 0.0030161916503889693\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05942729315161705\n",
      "Average test loss: 0.0031359358962832224\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05925023501780298\n",
      "Average test loss: 0.0030754188357128035\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0592522312104702\n",
      "Average test loss: 0.003077200301405456\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05872210855616464\n",
      "Average test loss: 0.003075851463402311\n",
      "Epoch 146/300\n",
      "Average training loss: 0.058910539557536445\n",
      "Average test loss: 0.003089781566626496\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05851331009467443\n",
      "Average test loss: 0.0029437861125916243\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0586138014362918\n",
      "Average test loss: 0.0030638935532834796\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0583600180943807\n",
      "Average test loss: 0.003099748321912355\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05869181752204895\n",
      "Average test loss: 0.0030670495513412687\n",
      "Epoch 151/300\n",
      "Average training loss: 0.058057348474860195\n",
      "Average test loss: 0.003022117760239376\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05780801628695594\n",
      "Average test loss: 0.003097993275150657\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0579115367896027\n",
      "Average test loss: 0.003142974687119325\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05779728897743755\n",
      "Average test loss: 0.0030217606634315515\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05760439662469758\n",
      "Average test loss: 0.0031140873303843867\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05762715486685435\n",
      "Average test loss: 0.003056747646795379\n",
      "Epoch 157/300\n",
      "Average training loss: 0.057393300728665456\n",
      "Average test loss: 0.0030817500253518424\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0573831665151649\n",
      "Average test loss: 0.003127387739924921\n",
      "Epoch 159/300\n",
      "Average training loss: 0.057162088904115886\n",
      "Average test loss: 0.003011295327709781\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05702397077613407\n",
      "Average test loss: 0.003068507248742713\n",
      "Epoch 161/300\n",
      "Average training loss: 0.056864578965637416\n",
      "Average test loss: 0.0031639325484219523\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05683586713340547\n",
      "Average test loss: 0.003058669768894712\n",
      "Epoch 163/300\n",
      "Average training loss: 0.056704473584890364\n",
      "Average test loss: 0.0030086613202260602\n",
      "Epoch 164/300\n",
      "Average training loss: 0.056601163254843816\n",
      "Average test loss: 0.003051451225247648\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05651361147562663\n",
      "Average test loss: 0.00303807426802814\n",
      "Epoch 166/300\n",
      "Average training loss: 0.056403999206092625\n",
      "Average test loss: 0.003093583355968197\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05646122030417124\n",
      "Average test loss: 0.003168114337242312\n",
      "Epoch 168/300\n",
      "Average training loss: 0.056499352471696004\n",
      "Average test loss: 0.003132224999368191\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05612141382694245\n",
      "Average test loss: 0.0030767222876764005\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05607835429575708\n",
      "Average test loss: 0.0030420923040558896\n",
      "Epoch 171/300\n",
      "Average training loss: 0.056291367265913224\n",
      "Average test loss: 0.0030414428061081304\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05580554603536924\n",
      "Average test loss: 0.0030655208879874813\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0556430469751358\n",
      "Average test loss: 0.0031878174419204394\n",
      "Epoch 174/300\n",
      "Average training loss: 0.055567796150843306\n",
      "Average test loss: 0.0031045622217158475\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05562909410066075\n",
      "Average test loss: 0.00303481262922287\n",
      "Epoch 176/300\n",
      "Average training loss: 0.055372730556461545\n",
      "Average test loss: 0.0031929901351945267\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0553616109126144\n",
      "Average test loss: 0.0030888054608884784\n",
      "Epoch 178/300\n",
      "Average training loss: 0.055251097387737695\n",
      "Average test loss: 0.0031313987175623577\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05524578010704782\n",
      "Average test loss: 0.0030660388910522065\n",
      "Epoch 180/300\n",
      "Average training loss: 0.055182828843593594\n",
      "Average test loss: 0.0031072769434087807\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05495770192808575\n",
      "Average test loss: 0.0030727893076837065\n",
      "Epoch 182/300\n",
      "Average training loss: 0.054899708489576975\n",
      "Average test loss: 0.003148728732433584\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05471152151955499\n",
      "Average test loss: 0.0030507005597982142\n",
      "Epoch 184/300\n",
      "Average training loss: 0.055455677453014586\n",
      "Average test loss: 0.0030823778729471897\n",
      "Epoch 185/300\n",
      "Average training loss: 0.054633185883363085\n",
      "Average test loss: 0.0030955939926207068\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05453964072797034\n",
      "Average test loss: 0.0031254404621819656\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05441319547096888\n",
      "Average test loss: 0.003135884342301223\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05435284555951754\n",
      "Average test loss: 0.003127098609589868\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05444729277822707\n",
      "Average test loss: 0.003084024879667494\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0541777858502335\n",
      "Average test loss: 0.003167756486684084\n",
      "Epoch 191/300\n",
      "Average training loss: 0.054228511648045645\n",
      "Average test loss: 0.0031267962459888723\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05408958779440986\n",
      "Average test loss: 0.0032093503760794798\n",
      "Epoch 193/300\n",
      "Average training loss: 0.054136197057035235\n",
      "Average test loss: 0.0030235747664959893\n",
      "Epoch 194/300\n",
      "Average training loss: 0.054757815092802045\n",
      "Average test loss: 0.0030689097858137555\n",
      "Epoch 195/300\n",
      "Average training loss: 0.053850675821304324\n",
      "Average test loss: 0.0031041863744871484\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05373227192295922\n",
      "Average test loss: 0.003145881232081188\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05364737652738889\n",
      "Average test loss: 0.0031459788754582404\n",
      "Epoch 198/300\n",
      "Average training loss: 0.053534444640080135\n",
      "Average test loss: 0.0031508756095750463\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05351286825868819\n",
      "Average test loss: 0.003062752410976423\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05349045330286026\n",
      "Average test loss: 0.003210624142239491\n",
      "Epoch 201/300\n",
      "Average training loss: 0.053455503390895\n",
      "Average test loss: 0.0031387929829458394\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05341368802719646\n",
      "Average test loss: 0.0031177558501561484\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05344581307967504\n",
      "Average test loss: 0.0032504120491858987\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0531514931221803\n",
      "Average test loss: 0.0032051616553217172\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05309081646468904\n",
      "Average test loss: 0.003158927526531948\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05304683797227012\n",
      "Average test loss: 0.0031455872416910197\n",
      "Epoch 207/300\n",
      "Average training loss: 0.052983412343594764\n",
      "Average test loss: 0.0032314804761360088\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05289166498184204\n",
      "Average test loss: 0.0030755411353376176\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05292575047744645\n",
      "Average test loss: 0.003124178399435348\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0528576151198811\n",
      "Average test loss: 0.0031373806214994855\n",
      "Epoch 211/300\n",
      "Average training loss: 0.052631604813867146\n",
      "Average test loss: 0.0031116017835835616\n",
      "Epoch 212/300\n",
      "Average training loss: 0.052703992545604705\n",
      "Average test loss: 0.003100100868071119\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05260815714465247\n",
      "Average test loss: 0.0031207021808044777\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05250627669029766\n",
      "Average test loss: 0.0031088589475386673\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05241144836942355\n",
      "Average test loss: 0.0031438663419750003\n",
      "Epoch 216/300\n",
      "Average training loss: 0.052262829499112234\n",
      "Average test loss: 0.003176294681719608\n",
      "Epoch 217/300\n",
      "Average training loss: 0.052282209979163274\n",
      "Average test loss: 0.0031678879737026162\n",
      "Epoch 218/300\n",
      "Average training loss: 0.052206593179040486\n",
      "Average test loss: 0.0031510162711557415\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05207085792885886\n",
      "Average test loss: 0.0031762948566012913\n",
      "Epoch 220/300\n",
      "Average training loss: 0.052215106854836144\n",
      "Average test loss: 0.0031744409648494587\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05224907923075888\n",
      "Average test loss: 0.003208699346623487\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05208404345975982\n",
      "Average test loss: 0.003163437615459164\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05209484250346819\n",
      "Average test loss: 0.003087914189013342\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05177900207373831\n",
      "Average test loss: 0.0031132242509888278\n",
      "Epoch 225/300\n",
      "Average training loss: 0.051755393538210126\n",
      "Average test loss: 0.003132191410702136\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0516617200507058\n",
      "Average test loss: 0.003080831768612067\n",
      "Epoch 227/300\n",
      "Average training loss: 0.051638690200116896\n",
      "Average test loss: 0.003077243364519543\n",
      "Epoch 228/300\n",
      "Average training loss: 0.051579188542233576\n",
      "Average test loss: 0.0031593550679584344\n",
      "Epoch 229/300\n",
      "Average training loss: 0.051528977890809374\n",
      "Average test loss: 0.0031473849357830153\n",
      "Epoch 230/300\n",
      "Average training loss: 0.051521158632304935\n",
      "Average test loss: 0.0032669392315049966\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05140377058254348\n",
      "Average test loss: 0.0031692682405312857\n",
      "Epoch 232/300\n",
      "Average training loss: 0.051457218954960506\n",
      "Average test loss: 0.003268180176201794\n",
      "Epoch 233/300\n",
      "Average training loss: 0.051310620168844856\n",
      "Average test loss: 0.0032005268453309936\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05135451310210758\n",
      "Average test loss: 0.0030851333070960308\n",
      "Epoch 235/300\n",
      "Average training loss: 0.051357178717851636\n",
      "Average test loss: 0.0031277410444907018\n",
      "Epoch 236/300\n",
      "Average training loss: 0.051284045351876154\n",
      "Average test loss: 0.00315605480906864\n",
      "Epoch 237/300\n",
      "Average training loss: 0.051036256780227024\n",
      "Average test loss: 0.003341259733463327\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05114382863044739\n",
      "Average test loss: 0.003208862291648984\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05103123570150799\n",
      "Average test loss: 0.0031191099480622343\n",
      "Epoch 240/300\n",
      "Average training loss: 0.050941111508342955\n",
      "Average test loss: 0.0031192173504581053\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05084296891093254\n",
      "Average test loss: 0.00321894940506253\n",
      "Epoch 242/300\n",
      "Average training loss: 0.050782335102558133\n",
      "Average test loss: 0.003181040289087428\n",
      "Epoch 243/300\n",
      "Average training loss: 0.050766152948141095\n",
      "Average test loss: 0.0031624362752255466\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05078849383857515\n",
      "Average test loss: 0.003175937749652399\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05066777453488774\n",
      "Average test loss: 0.003201961366666688\n",
      "Epoch 246/300\n",
      "Average training loss: 0.050589818951156405\n",
      "Average test loss: 0.003158661560465892\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0504444312552611\n",
      "Average test loss: 0.003297675227125486\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05044812676972813\n",
      "Average test loss: 0.0031438427979333535\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05059211116698053\n",
      "Average test loss: 0.0032305863197478984\n",
      "Epoch 250/300\n",
      "Average training loss: 0.050606953690449395\n",
      "Average test loss: 0.003128307185239262\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05026290104952123\n",
      "Average test loss: 0.003231083081000381\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05028974564539062\n",
      "Average test loss: 0.0031944668010498085\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05025759398606088\n",
      "Average test loss: 0.003201994756857554\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05016227335731188\n",
      "Average test loss: 0.003183953024033043\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05022806194755766\n",
      "Average test loss: 0.0032723789513111113\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05009257009625435\n",
      "Average test loss: 0.0031220405536393323\n",
      "Epoch 257/300\n",
      "Average training loss: 0.049977919449408846\n",
      "Average test loss: 0.003241629309331377\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04996508684092098\n",
      "Average test loss: 0.0032648997604846955\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05012301241358121\n",
      "Average test loss: 0.0032581503625131315\n",
      "Epoch 260/300\n",
      "Average training loss: 0.049902926001283855\n",
      "Average test loss: 0.003212369194254279\n",
      "Epoch 261/300\n",
      "Average training loss: 0.049867444780137805\n",
      "Average test loss: 0.0032520743631240393\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04973310793439547\n",
      "Average test loss: 0.0031570276239266\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04982886029283206\n",
      "Average test loss: 0.003125699904333386\n",
      "Epoch 264/300\n",
      "Average training loss: 0.049804895245366625\n",
      "Average test loss: 0.0032809335022336906\n",
      "Epoch 265/300\n",
      "Average training loss: 0.049656038284301755\n",
      "Average test loss: 0.003191033767743243\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04968320391575495\n",
      "Average test loss: 0.0032114388048648836\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04974775500430001\n",
      "Average test loss: 0.0033193966833253703\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04961703253454632\n",
      "Average test loss: 0.00319890183955431\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04944037506977717\n",
      "Average test loss: 0.0032157103584872352\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04941814573936992\n",
      "Average test loss: 0.003213587385912736\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04937767702672217\n",
      "Average test loss: 0.0030951281475524108\n",
      "Epoch 272/300\n",
      "Average training loss: 0.049424827999538845\n",
      "Average test loss: 0.003119777801964018\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04926630148291588\n",
      "Average test loss: 0.0031748554169510803\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04920206528902054\n",
      "Average test loss: 0.003231524953618646\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04930529104007615\n",
      "Average test loss: 0.003191120060367717\n",
      "Epoch 276/300\n",
      "Average training loss: 0.049150370044840706\n",
      "Average test loss: 0.0032124673273000452\n",
      "Epoch 277/300\n",
      "Average training loss: 0.049146258814467325\n",
      "Average test loss: 0.0031448796192804973\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04909515160322189\n",
      "Average test loss: 0.0031903198092348047\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04909227835469776\n",
      "Average test loss: 0.003205584165950616\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04883579188916418\n",
      "Average test loss: 0.0031590263831118744\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04897075877587\n",
      "Average test loss: 0.0032168391589075325\n",
      "Epoch 282/300\n",
      "Average training loss: 0.048997012101941635\n",
      "Average test loss: 0.003214544985857275\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04892999027834998\n",
      "Average test loss: 0.003174206185878979\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0488372202846739\n",
      "Average test loss: 0.0032515362844698957\n",
      "Epoch 285/300\n",
      "Average training loss: 0.048751926395628184\n",
      "Average test loss: 0.003203821731938256\n",
      "Epoch 286/300\n",
      "Average training loss: 0.048761216042770276\n",
      "Average test loss: 0.0032441866757969063\n",
      "Epoch 287/300\n",
      "Average training loss: 0.048659336659643386\n",
      "Average test loss: 0.0032760370628287395\n",
      "Epoch 288/300\n",
      "Average training loss: 0.048817549516757326\n",
      "Average test loss: 0.0032249550082617336\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04863460180494521\n",
      "Average test loss: 0.0031539311862240236\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04857966122031212\n",
      "Average test loss: 0.0031827565621998575\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04852414338787397\n",
      "Average test loss: 0.0031808836973375746\n",
      "Epoch 292/300\n",
      "Average training loss: 0.048540810306866966\n",
      "Average test loss: 0.003183046504441235\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04846881616446707\n",
      "Average test loss: 0.003166885958984494\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04843126041359372\n",
      "Average test loss: 0.003145456272073918\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04840591575370894\n",
      "Average test loss: 0.003116170453114642\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04863399395346642\n",
      "Average test loss: 0.0031604119448198215\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04834527586234941\n",
      "Average test loss: 0.0032763905446562503\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04809065671761831\n",
      "Average test loss: 0.003224897033224503\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04811217041479217\n",
      "Average test loss: 0.0032661234684702423\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04821273587147395\n",
      "Average test loss: 0.0031666440020004906\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 33.50889377848307\n",
      "Average test loss: 15.641633848157193\n",
      "Epoch 2/300\n",
      "Average training loss: 22.485586278279623\n",
      "Average test loss: 0.004486217727263768\n",
      "Epoch 3/300\n",
      "Average training loss: 18.851459988064235\n",
      "Average test loss: 0.35564334726499186\n",
      "Epoch 4/300\n",
      "Average training loss: 14.879434651692709\n",
      "Average test loss: 0.003767271345688237\n",
      "Epoch 5/300\n",
      "Average training loss: 13.453009368048773\n",
      "Average test loss: 0.00432483078373803\n",
      "Epoch 6/300\n",
      "Average training loss: 11.99076229349772\n",
      "Average test loss: 0.0034114443148589796\n",
      "Epoch 7/300\n",
      "Average training loss: 10.255138221740722\n",
      "Average test loss: 0.0033167732351770006\n",
      "Epoch 8/300\n",
      "Average training loss: 7.3066285133361815\n",
      "Average test loss: 0.0031679881809072363\n",
      "Epoch 9/300\n",
      "Average training loss: 7.1604175779554575\n",
      "Average test loss: 0.003776987434261375\n",
      "Epoch 10/300\n",
      "Average training loss: 6.199152813805474\n",
      "Average test loss: 0.03511737977123509\n",
      "Epoch 11/300\n",
      "Average training loss: 5.391998145209419\n",
      "Average test loss: 0.002967848910846644\n",
      "Epoch 12/300\n",
      "Average training loss: 4.32545695898268\n",
      "Average test loss: 0.013414463441405032\n",
      "Epoch 13/300\n",
      "Average training loss: 3.4754745945400662\n",
      "Average test loss: 0.016310783048470816\n",
      "Epoch 14/300\n",
      "Average training loss: 3.023272176106771\n",
      "Average test loss: 0.0026856067122684583\n",
      "Epoch 15/300\n",
      "Average training loss: 2.741254871580336\n",
      "Average test loss: 0.029206536525239548\n",
      "Epoch 16/300\n",
      "Average training loss: 2.452550121731228\n",
      "Average test loss: 0.3555119717357059\n",
      "Epoch 17/300\n",
      "Average training loss: 1.9323534522586399\n",
      "Average test loss: 0.06515505071553505\n",
      "Epoch 18/300\n",
      "Average training loss: 1.8133253036075168\n",
      "Average test loss: 0.002669712766384085\n",
      "Epoch 19/300\n",
      "Average training loss: 1.5726332669787937\n",
      "Average test loss: 0.0031463756151497366\n",
      "Epoch 20/300\n",
      "Average training loss: 1.3265830632315743\n",
      "Average test loss: 0.0025991521088613405\n",
      "Epoch 21/300\n",
      "Average training loss: 1.1578837729030185\n",
      "Average test loss: 0.21679755341261625\n",
      "Epoch 22/300\n",
      "Average training loss: 0.9163550846311781\n",
      "Average test loss: 0.0023611302121231952\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7719155459403991\n",
      "Average test loss: 0.17781411822140217\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6335900781419542\n",
      "Average test loss: 0.016478810749120183\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5512864890098572\n",
      "Average test loss: 0.002278352688997984\n",
      "Epoch 26/300\n",
      "Average training loss: 0.4606563461621602\n",
      "Average test loss: 0.002221165119877292\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3902660636636946\n",
      "Average test loss: 0.002268603364730047\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3370101929505666\n",
      "Average test loss: 0.008132773767742846\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2927587273915609\n",
      "Average test loss: 0.010194145128130912\n",
      "Epoch 30/300\n",
      "Average training loss: 0.25702703788545395\n",
      "Average test loss: 0.009844461042433977\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2260043457084232\n",
      "Average test loss: 0.0021722042688892946\n",
      "Epoch 32/300\n",
      "Average training loss: 0.20056546382109325\n",
      "Average test loss: 0.0021359560081942214\n",
      "Epoch 33/300\n",
      "Average training loss: 0.17988134038448333\n",
      "Average test loss: 0.005840646255347464\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1615200789504581\n",
      "Average test loss: 0.002260295081987149\n",
      "Epoch 35/300\n",
      "Average training loss: 0.14649674201673932\n",
      "Average test loss: 0.002088114529123737\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1339901192718082\n",
      "Average test loss: 0.002182141943420801\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12362341539727317\n",
      "Average test loss: 0.002110263971197936\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11502742089165581\n",
      "Average test loss: 0.002074099308707648\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10768791417943106\n",
      "Average test loss: 0.002041073300254842\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10174621635675431\n",
      "Average test loss: 0.002099899006386598\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09687237693866094\n",
      "Average test loss: 0.0020311953181193936\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0927669091158443\n",
      "Average test loss: 0.0020935868113819097\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08912331820196576\n",
      "Average test loss: 0.0020399322075148425\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08618147331476211\n",
      "Average test loss: 0.002024114438539578\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08363989133305019\n",
      "Average test loss: 0.0020701143518090247\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0813231592906846\n",
      "Average test loss: 0.0020431125001567933\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07969212616152234\n",
      "Average test loss: 0.0020452703724925716\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07780147966411378\n",
      "Average test loss: 0.002014069325808022\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07609388044145372\n",
      "Average test loss: 0.0020157697116956115\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07487616691655583\n",
      "Average test loss: 0.002038214909326699\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07346183151668972\n",
      "Average test loss: 0.0020676222073121205\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07244634799162547\n",
      "Average test loss: 0.002052682659899195\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07148964896466997\n",
      "Average test loss: 0.001985023550896181\n",
      "Epoch 54/300\n",
      "Average training loss: 0.070235299948189\n",
      "Average test loss: 0.002027381657105353\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06938432463672425\n",
      "Average test loss: 0.002062706295193897\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06850033177600967\n",
      "Average test loss: 0.0020273481600193515\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06749584983123673\n",
      "Average test loss: 0.002019435662569271\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0665967005888621\n",
      "Average test loss: 0.002014278715993795\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06578712235225571\n",
      "Average test loss: 0.001987272990246614\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06499009542332755\n",
      "Average test loss: 0.00210355261568394\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06426115748617384\n",
      "Average test loss: 0.002089978519413206\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06356481002105607\n",
      "Average test loss: 0.0020069941921780506\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06301795708802012\n",
      "Average test loss: 0.002091290984302759\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06198077694574992\n",
      "Average test loss: 0.002023021846595738\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06141532869802581\n",
      "Average test loss: 0.0020695459472222463\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0609525338212649\n",
      "Average test loss: 0.0020276737903348273\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06025289690825674\n",
      "Average test loss: 0.002013431859202683\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05961292673812972\n",
      "Average test loss: 0.002140685869173871\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05905196467373106\n",
      "Average test loss: 0.0020603787403346762\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05840715556343397\n",
      "Average test loss: 0.0020955955853892696\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05786721978916062\n",
      "Average test loss: 0.0021744867679145603\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05729570417602857\n",
      "Average test loss: 0.0020915452034937013\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05666944288214048\n",
      "Average test loss: 0.0020007495797342723\n",
      "Epoch 74/300\n",
      "Average training loss: 0.056246528807613584\n",
      "Average test loss: 0.0020605512719808353\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05611161444584529\n",
      "Average test loss: 0.0020358779644593598\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0552033846643236\n",
      "Average test loss: 0.002022744325403538\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0553338962925805\n",
      "Average test loss: 0.0020502176783565016\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05447901367147764\n",
      "Average test loss: 0.0021017102083812156\n",
      "Epoch 79/300\n",
      "Average training loss: 0.054099734650717844\n",
      "Average test loss: 0.0020513614794860284\n",
      "Epoch 80/300\n",
      "Average training loss: 0.053207380327913494\n",
      "Average test loss: 0.0020248892597026294\n",
      "Epoch 81/300\n",
      "Average training loss: 0.052991614547040725\n",
      "Average test loss: 0.0020292797421829567\n",
      "Epoch 82/300\n",
      "Average training loss: 0.052614542967743344\n",
      "Average test loss: 0.0020376350906574064\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05209164900581042\n",
      "Average test loss: 0.002195472491077251\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05181956747174263\n",
      "Average test loss: 0.0020095135722723273\n",
      "Epoch 85/300\n",
      "Average training loss: 0.051344257020288046\n",
      "Average test loss: 0.0020371194285237126\n",
      "Epoch 86/300\n",
      "Average training loss: 0.051141846881972416\n",
      "Average test loss: 0.0020786698239131105\n",
      "Epoch 87/300\n",
      "Average training loss: 0.050720371905300356\n",
      "Average test loss: 0.0020192330992884104\n",
      "Epoch 88/300\n",
      "Average training loss: 0.050728266490830315\n",
      "Average test loss: 0.0021175602895932067\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04991025494204627\n",
      "Average test loss: 0.0021078989365034633\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04944533511996269\n",
      "Average test loss: 0.0020494934781971905\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04936828264594078\n",
      "Average test loss: 0.0021079553609920872\n",
      "Epoch 92/300\n",
      "Average training loss: 0.049072480792800584\n",
      "Average test loss: 0.002118563398097952\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04869403229819404\n",
      "Average test loss: 0.002177197771767775\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04845545601182514\n",
      "Average test loss: 0.0020840370557788346\n",
      "Epoch 95/300\n",
      "Average training loss: 0.048047721988624996\n",
      "Average test loss: 0.0021114248366405565\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04776272669765684\n",
      "Average test loss: 0.00211090592864073\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04792180340488752\n",
      "Average test loss: 0.002160655890074041\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04737189072370529\n",
      "Average test loss: 0.0020858679374472964\n",
      "Epoch 99/300\n",
      "Average training loss: 0.046957416352298526\n",
      "Average test loss: 0.00210141350949804\n",
      "Epoch 100/300\n",
      "Average training loss: 0.046737875716553794\n",
      "Average test loss: 0.0021202596777843107\n",
      "Epoch 101/300\n",
      "Average training loss: 0.046588990370432536\n",
      "Average test loss: 0.0021039643937514887\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04655172971553272\n",
      "Average test loss: 0.0021018026682237782\n",
      "Epoch 103/300\n",
      "Average training loss: 0.046432686103714836\n",
      "Average test loss: 0.0021506903181887334\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04591340177588993\n",
      "Average test loss: 0.0021143920618212886\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04559178833497895\n",
      "Average test loss: 0.002076673492892749\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04529939207103517\n",
      "Average test loss: 0.0021441434499704175\n",
      "Epoch 107/300\n",
      "Average training loss: 0.045216174403826394\n",
      "Average test loss: 0.002063163243337638\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04505036959052086\n",
      "Average test loss: 0.002104827777379089\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04501141921016905\n",
      "Average test loss: 0.00211298037879169\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04469063980049557\n",
      "Average test loss: 0.0021068913891083662\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04440573222769631\n",
      "Average test loss: 0.0021411543418135906\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04439874502354198\n",
      "Average test loss: 0.0020923022743728425\n",
      "Epoch 113/300\n",
      "Average training loss: 0.044118348313702475\n",
      "Average test loss: 0.002155365022520224\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04387572235531277\n",
      "Average test loss: 0.0021578662312693067\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04379777533478207\n",
      "Average test loss: 0.002137520871953004\n",
      "Epoch 116/300\n",
      "Average training loss: 0.043611654897530876\n",
      "Average test loss: 0.0022293721369157234\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04360230088565085\n",
      "Average test loss: 0.002161608890630305\n",
      "Epoch 118/300\n",
      "Average training loss: 0.043241228024164836\n",
      "Average test loss: 0.002110588037719329\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04309942278265953\n",
      "Average test loss: 0.0021636129235848784\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04291187244322565\n",
      "Average test loss: 0.002128856720402837\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04284315953983201\n",
      "Average test loss: 0.0021309487749305036\n",
      "Epoch 122/300\n",
      "Average training loss: 0.042832317473159895\n",
      "Average test loss: 0.002185611800290644\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04252164033386442\n",
      "Average test loss: 0.0021727634518303807\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0423183695409033\n",
      "Average test loss: 0.0021539404917922285\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04222661619053947\n",
      "Average test loss: 0.0020617516084263723\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04224632689522372\n",
      "Average test loss: 0.0021319222742070755\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04188172197010782\n",
      "Average test loss: 0.0021965681314468385\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04187383909026782\n",
      "Average test loss: 0.0021590568298060033\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04171059500177701\n",
      "Average test loss: 0.0022033862931032976\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04154754287666745\n",
      "Average test loss: 0.0022298989032084743\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04141879932085673\n",
      "Average test loss: 0.0022292333169736797\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04136215982172224\n",
      "Average test loss: 0.0022976217383725776\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04113228699896071\n",
      "Average test loss: 0.0021413451867798965\n",
      "Epoch 134/300\n",
      "Average training loss: 0.040993048591746226\n",
      "Average test loss: 0.002165179945528507\n",
      "Epoch 135/300\n",
      "Average training loss: 0.040952553601728545\n",
      "Average test loss: 0.002168706345475382\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04082275567120976\n",
      "Average test loss: 0.0021811449548436537\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04082454840011067\n",
      "Average test loss: 0.002297588472978936\n",
      "Epoch 138/300\n",
      "Average training loss: 0.040721656067503824\n",
      "Average test loss: 0.0022214081421908405\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04056844871242841\n",
      "Average test loss: 0.002178213932034042\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04048402274979485\n",
      "Average test loss: 0.002136371035128832\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04028165067235629\n",
      "Average test loss: 0.002174846712499857\n",
      "Epoch 142/300\n",
      "Average training loss: 0.040246179872088964\n",
      "Average test loss: 0.0022138013645178743\n",
      "Epoch 143/300\n",
      "Average training loss: 0.040004083504279454\n",
      "Average test loss: 0.0021453077010810374\n",
      "Epoch 144/300\n",
      "Average training loss: 0.039950978676478066\n",
      "Average test loss: 0.0021684050653129816\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03981722479065259\n",
      "Average test loss: 0.0021798169795009827\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03983675033516354\n",
      "Average test loss: 0.0021866086849735844\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03963930426372422\n",
      "Average test loss: 0.00222016555764195\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03977010198103057\n",
      "Average test loss: 0.00220583469606936\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03951490116119385\n",
      "Average test loss: 0.0022362124412837955\n",
      "Epoch 150/300\n",
      "Average training loss: 0.039568588438961244\n",
      "Average test loss: 0.0022688596322097713\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03932044926285744\n",
      "Average test loss: 0.002178531875419948\n",
      "Epoch 152/300\n",
      "Average training loss: 0.039306032376156916\n",
      "Average test loss: 0.002268503529226614\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03923326126072142\n",
      "Average test loss: 0.0022384268862919673\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03903243117365572\n",
      "Average test loss: 0.0022287181516488395\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03894708904623985\n",
      "Average test loss: 0.002150114469644096\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03897233725587527\n",
      "Average test loss: 0.0022005529672735266\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03890420625276036\n",
      "Average test loss: 0.0021994371264138154\n",
      "Epoch 158/300\n",
      "Average training loss: 0.038741380941536695\n",
      "Average test loss: 0.002194935586199992\n",
      "Epoch 159/300\n",
      "Average training loss: 0.038687682428293756\n",
      "Average test loss: 0.0021925433177707922\n",
      "Epoch 160/300\n",
      "Average training loss: 0.038553020879626275\n",
      "Average test loss: 0.0022212668938769234\n",
      "Epoch 161/300\n",
      "Average training loss: 0.038429419832925\n",
      "Average test loss: 0.0022424627697716156\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03837000881301032\n",
      "Average test loss: 0.0022899517439719705\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03833058087362184\n",
      "Average test loss: 0.0022240878974811897\n",
      "Epoch 164/300\n",
      "Average training loss: 0.038278595795234044\n",
      "Average test loss: 0.002226030661414067\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03811944047941102\n",
      "Average test loss: 0.002214922682278686\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03818356266617775\n",
      "Average test loss: 0.002159606670546863\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03811222352584203\n",
      "Average test loss: 0.0022236223167015445\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03803770890666379\n",
      "Average test loss: 0.0022084698661540944\n",
      "Epoch 169/300\n",
      "Average training loss: 0.037875071028868355\n",
      "Average test loss: 0.002196192148555484\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03773607615960969\n",
      "Average test loss: 0.002239132034695811\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03796004295680258\n",
      "Average test loss: 0.0022333468933486275\n",
      "Epoch 172/300\n",
      "Average training loss: 0.037651115278402966\n",
      "Average test loss: 0.0022300622084488473\n",
      "Epoch 173/300\n",
      "Average training loss: 0.037622531639205085\n",
      "Average test loss: 0.002246990291815665\n",
      "Epoch 174/300\n",
      "Average training loss: 0.037584163202179804\n",
      "Average test loss: 0.0021645974379239812\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03743570408887333\n",
      "Average test loss: 0.002193047524533338\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03741245192951626\n",
      "Average test loss: 0.002271367456763983\n",
      "Epoch 177/300\n",
      "Average training loss: 0.037365926682949066\n",
      "Average test loss: 0.0022122284440944592\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03731430754065514\n",
      "Average test loss: 0.002229815057168404\n",
      "Epoch 179/300\n",
      "Average training loss: 0.037241793427202435\n",
      "Average test loss: 0.0022068591868090962\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03712896305322647\n",
      "Average test loss: 0.0021733190798097187\n",
      "Epoch 181/300\n",
      "Average training loss: 0.037131658537520304\n",
      "Average test loss: 0.00220430550020602\n",
      "Epoch 182/300\n",
      "Average training loss: 0.036964582888616454\n",
      "Average test loss: 0.002205704192734427\n",
      "Epoch 183/300\n",
      "Average training loss: 0.037075091709693274\n",
      "Average test loss: 0.0022447379958919354\n",
      "Epoch 184/300\n",
      "Average training loss: 0.036964704407585994\n",
      "Average test loss: 0.002240385841785206\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03684535222914484\n",
      "Average test loss: 0.0022394780501102407\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03684443763891856\n",
      "Average test loss: 0.002143327639024291\n",
      "Epoch 187/300\n",
      "Average training loss: 0.036818667789300284\n",
      "Average test loss: 0.002186533422519763\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03677635425991482\n",
      "Average test loss: 0.002152598439405362\n",
      "Epoch 189/300\n",
      "Average training loss: 0.036451036830743154\n",
      "Average test loss: 0.0022402365163175595\n",
      "Epoch 190/300\n",
      "Average training loss: 0.036494347290860284\n",
      "Average test loss: 0.002212619800534513\n",
      "Epoch 191/300\n",
      "Average training loss: 0.036653481473525366\n",
      "Average test loss: 0.002243029480903513\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0364612097243468\n",
      "Average test loss: 0.0024001113494030303\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03652239673170778\n",
      "Average test loss: 0.002198458275033368\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03629976558023029\n",
      "Average test loss: 0.002283665032022529\n",
      "Epoch 195/300\n",
      "Average training loss: 0.036227093302541306\n",
      "Average test loss: 0.0021735206285698546\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03629931908183628\n",
      "Average test loss: 0.00222416158620682\n",
      "Epoch 197/300\n",
      "Average training loss: 0.036082625223530666\n",
      "Average test loss: 0.002216365111991763\n",
      "Epoch 198/300\n",
      "Average training loss: 0.036119396246141855\n",
      "Average test loss: 0.0023060777530901962\n",
      "Epoch 199/300\n",
      "Average training loss: 0.036022378120157454\n",
      "Average test loss: 0.0022129155746143723\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03600635543300046\n",
      "Average test loss: 0.0022319511597355206\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03611493765976694\n",
      "Average test loss: 0.002166884718255864\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0358967518856128\n",
      "Average test loss: 0.0022919967960980205\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03587409255570836\n",
      "Average test loss: 0.0023114586319360466\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03586569133235348\n",
      "Average test loss: 0.0023090535101170343\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03584807460010052\n",
      "Average test loss: 0.0022282966247035396\n",
      "Epoch 206/300\n",
      "Average training loss: 0.035716891116566125\n",
      "Average test loss: 0.002303816108136541\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03570277871191502\n",
      "Average test loss: 0.002264120019765364\n",
      "Epoch 208/300\n",
      "Average training loss: 0.035580664990676775\n",
      "Average test loss: 0.0022626776579353545\n",
      "Epoch 209/300\n",
      "Average training loss: 0.035517022043466566\n",
      "Average test loss: 0.0022690374081333477\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03549212525288264\n",
      "Average test loss: 0.002265853812607626\n",
      "Epoch 211/300\n",
      "Average training loss: 0.035511611551046374\n",
      "Average test loss: 0.0024227593545284535\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03543949013286167\n",
      "Average test loss: 0.0022641362051169078\n",
      "Epoch 213/300\n",
      "Average training loss: 0.035368964145580925\n",
      "Average test loss: 0.0022695937697879143\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03531953949398465\n",
      "Average test loss: 0.002189686563383374\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03527119643158383\n",
      "Average test loss: 0.0021891716592427756\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0352911452849706\n",
      "Average test loss: 0.002310619130316708\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03528668828805288\n",
      "Average test loss: 0.002238453222024772\n",
      "Epoch 218/300\n",
      "Average training loss: 0.035191069430775115\n",
      "Average test loss: 0.0022710887926320235\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03507676227556335\n",
      "Average test loss: 0.0022417148484124077\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03500539934635162\n",
      "Average test loss: 0.002295908730166654\n",
      "Epoch 221/300\n",
      "Average training loss: 0.034960403444038494\n",
      "Average test loss: 0.002232110928123196\n",
      "Epoch 222/300\n",
      "Average training loss: 0.035001048053304354\n",
      "Average test loss: 0.0022409272748563024\n",
      "Epoch 223/300\n",
      "Average training loss: 0.034959459914101494\n",
      "Average test loss: 0.0022404775912356046\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03496023262209363\n",
      "Average test loss: 0.002215903785286678\n",
      "Epoch 225/300\n",
      "Average training loss: 0.034789468232128355\n",
      "Average test loss: 0.002289217794314027\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03478948231538137\n",
      "Average test loss: 0.002370058393726746\n",
      "Epoch 227/300\n",
      "Average training loss: 0.034855551140175925\n",
      "Average test loss: 0.0022386281993240116\n",
      "Epoch 228/300\n",
      "Average training loss: 0.034674977852238546\n",
      "Average test loss: 0.002392128018455373\n",
      "Epoch 229/300\n",
      "Average training loss: 0.034910980155070624\n",
      "Average test loss: 0.00223318978274862\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0345761404633522\n",
      "Average test loss: 0.002205931743193004\n",
      "Epoch 231/300\n",
      "Average training loss: 0.034449261031217045\n",
      "Average test loss: 0.002254118845073713\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03461075355443689\n",
      "Average test loss: 0.0023266666581233343\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0344855237123039\n",
      "Average test loss: 0.00229869618660046\n",
      "Epoch 234/300\n",
      "Average training loss: 0.034495978481239746\n",
      "Average test loss: 0.0022308689643525414\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03446701563563612\n",
      "Average test loss: 0.0022481457847687934\n",
      "Epoch 236/300\n",
      "Average training loss: 0.034518569762508076\n",
      "Average test loss: 0.002314166271334721\n",
      "Epoch 237/300\n",
      "Average training loss: 0.034466590487294724\n",
      "Average test loss: 0.002257373490681251\n",
      "Epoch 238/300\n",
      "Average training loss: 0.034362724605533815\n",
      "Average test loss: 0.0022876760208358367\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03429686068163978\n",
      "Average test loss: 0.002287807067959673\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03439240446024471\n",
      "Average test loss: 0.002288031451818016\n",
      "Epoch 241/300\n",
      "Average training loss: 0.034426357352071335\n",
      "Average test loss: 0.0022898850159512626\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03419144766198264\n",
      "Average test loss: 0.0022750215897750525\n",
      "Epoch 243/300\n",
      "Average training loss: 0.034153573797808756\n",
      "Average test loss: 0.0022117118416354062\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03399736026260588\n",
      "Average test loss: 0.002218803335809045\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03405636502802372\n",
      "Average test loss: 0.0023087392535267605\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03408137706915537\n",
      "Average test loss: 0.002233952277753916\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03402073314951526\n",
      "Average test loss: 0.002321898658759892\n",
      "Epoch 248/300\n",
      "Average training loss: 0.034008629196219976\n",
      "Average test loss: 0.0023132662201921146\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03396910024682681\n",
      "Average test loss: 0.002352498419375883\n",
      "Epoch 250/300\n",
      "Average training loss: 0.033943008604976864\n",
      "Average test loss: 0.002284537988404433\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03403479985230499\n",
      "Average test loss: 0.0022955658332341248\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03386405177248849\n",
      "Average test loss: 0.002346330763358209\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03375988224314319\n",
      "Average test loss: 0.0023486931203967995\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03373159983754158\n",
      "Average test loss: 0.0023224020128448804\n",
      "Epoch 255/300\n",
      "Average training loss: 0.033731042964590924\n",
      "Average test loss: 0.002308141173587905\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03368919532001018\n",
      "Average test loss: 0.0022683307346370485\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03363810284601317\n",
      "Average test loss: 0.0022540362087181873\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03364980045954386\n",
      "Average test loss: 0.0022902975270731583\n",
      "Epoch 259/300\n",
      "Average training loss: 0.033629068778620826\n",
      "Average test loss: 0.0022829887115707\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03357061076495382\n",
      "Average test loss: 0.002238974308543321\n",
      "Epoch 261/300\n",
      "Average training loss: 0.033544267664353054\n",
      "Average test loss: 0.0022352399670829376\n",
      "Epoch 262/300\n",
      "Average training loss: 0.033547903317544196\n",
      "Average test loss: 0.0023204188307540284\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03343290113740497\n",
      "Average test loss: 0.0022426662031147216\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03340663761893908\n",
      "Average test loss: 0.002365693582739267\n",
      "Epoch 265/300\n",
      "Average training loss: 0.033473368369870714\n",
      "Average test loss: 0.002354251030729049\n",
      "Epoch 266/300\n",
      "Average training loss: 0.033525427675909464\n",
      "Average test loss: 0.002266510215484434\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03336250039935112\n",
      "Average test loss: 0.002259519547224045\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03336434212327004\n",
      "Average test loss: 0.0022537328801635237\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03334925099545055\n",
      "Average test loss: 0.0022510500475764273\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03320352279974355\n",
      "Average test loss: 0.0022891615558829573\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03319673261377547\n",
      "Average test loss: 0.0022647858396586443\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03313586657577091\n",
      "Average test loss: 0.002316359936673608\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03314331685254971\n",
      "Average test loss: 0.0022844451747627724\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03314412817358971\n",
      "Average test loss: 0.0022644398032377164\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03311362222168181\n",
      "Average test loss: 0.002290941882982022\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03318168269263373\n",
      "Average test loss: 0.002266066120316585\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03316223367386394\n",
      "Average test loss: 0.002306498646736145\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03299655449224843\n",
      "Average test loss: 0.0022642860398110415\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0330157045159075\n",
      "Average test loss: 0.002253744180107282\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03289327809876866\n",
      "Average test loss: 0.0022986111686461503\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03298279537757238\n",
      "Average test loss: 0.002302527680372198\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03295635927716891\n",
      "Average test loss: 0.0022588865536575515\n",
      "Epoch 283/300\n",
      "Average training loss: 0.032937484376960334\n",
      "Average test loss: 0.002288999931472871\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03296501514646742\n",
      "Average test loss: 0.002356592849103941\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03278742807441288\n",
      "Average test loss: 0.0023350398153480557\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03283612471818924\n",
      "Average test loss: 0.0022608048621979024\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03287228375673294\n",
      "Average test loss: 0.0022479359132962094\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03272802372939057\n",
      "Average test loss: 0.0022921980546994343\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03270656657384501\n",
      "Average test loss: 0.0023187735293888384\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03270133563876152\n",
      "Average test loss: 0.0023396798370199073\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03264770307309098\n",
      "Average test loss: 0.0022944318308598466\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03267148186100854\n",
      "Average test loss: 0.0022344518590511546\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03257554441359308\n",
      "Average test loss: 0.0022957934228082497\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03256415995293194\n",
      "Average test loss: 0.002269379737579988\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0326130501528581\n",
      "Average test loss: 0.002284874022938311\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03255895466936959\n",
      "Average test loss: 0.0023114344434191783\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03248286445438862\n",
      "Average test loss: 0.002303195940123664\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03246524016062419\n",
      "Average test loss: 0.0022837133368270265\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03245009468992551\n",
      "Average test loss: 0.0023040531528078848\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03241877646413115\n",
      "Average test loss: 0.0023123390229625833\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 34.08214581976996\n",
      "Average test loss: 5853.493148959978\n",
      "Epoch 2/300\n",
      "Average training loss: 17.395954511854384\n",
      "Average test loss: 0.012801336684160763\n",
      "Epoch 3/300\n",
      "Average training loss: 10.066412760840523\n",
      "Average test loss: 227.89758392676296\n",
      "Epoch 4/300\n",
      "Average training loss: 8.109847627003987\n",
      "Average test loss: 0.254842496043692\n",
      "Epoch 5/300\n",
      "Average training loss: 6.685113891601563\n",
      "Average test loss: 2.848044299023019\n",
      "Epoch 6/300\n",
      "Average training loss: 5.473376316918267\n",
      "Average test loss: 0.014842566734800736\n",
      "Epoch 7/300\n",
      "Average training loss: 5.112790057288276\n",
      "Average test loss: 4.131602702631719\n",
      "Epoch 8/300\n",
      "Average training loss: 4.029095809300741\n",
      "Average test loss: 41.69045749666459\n",
      "Epoch 9/300\n",
      "Average training loss: 3.3276029408772785\n",
      "Average test loss: 3.8514185845487647\n",
      "Epoch 10/300\n",
      "Average training loss: 2.6400368054707846\n",
      "Average test loss: 0.25143266695737837\n",
      "Epoch 11/300\n",
      "Average training loss: 2.1235393012364705\n",
      "Average test loss: 71474.99890277778\n",
      "Epoch 12/300\n",
      "Average training loss: 1.7753114762836033\n",
      "Average test loss: 0.9358558786403802\n",
      "Epoch 13/300\n",
      "Average training loss: 1.4230485866334703\n",
      "Average test loss: 5.96556558441288\n",
      "Epoch 14/300\n",
      "Average training loss: 1.2648636976877847\n",
      "Average test loss: 27.355357509759564\n",
      "Epoch 15/300\n",
      "Average training loss: 1.0261410352389018\n",
      "Average test loss: 0.010930227448335952\n",
      "Epoch 16/300\n",
      "Average training loss: 0.8153939612706502\n",
      "Average test loss: 0.009092834062770836\n",
      "Epoch 17/300\n",
      "Average training loss: 0.6925551362567478\n",
      "Average test loss: 0.11273933616735869\n",
      "Epoch 18/300\n",
      "Average training loss: 0.5674559064441257\n",
      "Average test loss: 116.58985634818373\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4809135451846653\n",
      "Average test loss: 0.20259939489565376\n",
      "Epoch 20/300\n",
      "Average training loss: 0.41345624452167085\n",
      "Average test loss: 8463.700737738061\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3564554996755388\n",
      "Average test loss: 0.0017124825498192674\n",
      "Epoch 22/300\n",
      "Average training loss: 0.3090816395547655\n",
      "Average test loss: 0.008586318706369235\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2705110488997565\n",
      "Average test loss: 0.0024663975245008867\n",
      "Epoch 24/300\n",
      "Average training loss: 0.23660518866115146\n",
      "Average test loss: 0.5177039055211676\n",
      "Epoch 25/300\n",
      "Average training loss: 0.20856780811150868\n",
      "Average test loss: 1.4157211891238888\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1844535319275326\n",
      "Average test loss: 0.004448330806257824\n",
      "Epoch 27/300\n",
      "Average training loss: 0.16452096537748973\n",
      "Average test loss: 0.04184462612701787\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14714272456698949\n",
      "Average test loss: 0.0015837890117739637\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1321240426831775\n",
      "Average test loss: 0.0015285117565136818\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11930463486909866\n",
      "Average test loss: 0.4275460580057568\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10840414210822848\n",
      "Average test loss: 0.00251766309969955\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09991099202632904\n",
      "Average test loss: 0.0014953020702426633\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0922091351946195\n",
      "Average test loss: 0.0014867896690136857\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08559156870179706\n",
      "Average test loss: 0.0014621938279209038\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08060758664210638\n",
      "Average test loss: 0.001447143474800719\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07588091895977656\n",
      "Average test loss: 0.001433047592639923\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07220718381139968\n",
      "Average test loss: 0.00145712360036042\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06909704723291928\n",
      "Average test loss: 0.0014218992737846242\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06634025616778268\n",
      "Average test loss: 0.0014244503410946992\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06407290713654624\n",
      "Average test loss: 0.001434561589732766\n",
      "Epoch 41/300\n",
      "Average training loss: 0.062012085007296665\n",
      "Average test loss: 0.0014155245423316956\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06004919200804498\n",
      "Average test loss: 0.0014144096771876017\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05846781752175755\n",
      "Average test loss: 0.0014135708411534627\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05700690235031976\n",
      "Average test loss: 0.001390968250265966\n",
      "Epoch 45/300\n",
      "Average training loss: 0.055893501735395854\n",
      "Average test loss: 0.001409273811046862\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05465034009350671\n",
      "Average test loss: 0.0013871908126812842\n",
      "Epoch 47/300\n",
      "Average training loss: 0.053560908863941825\n",
      "Average test loss: 0.0013973635330589282\n",
      "Epoch 48/300\n",
      "Average training loss: 0.052457230367594296\n",
      "Average test loss: 0.0013864059585870967\n",
      "Epoch 49/300\n",
      "Average training loss: 0.051534885240925685\n",
      "Average test loss: 0.001391836111795985\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05068909786807166\n",
      "Average test loss: 0.0013946808052973615\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04975722117225329\n",
      "Average test loss: 0.0014039433753738801\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04891842852036158\n",
      "Average test loss: 0.0014685574880697661\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0481522814101643\n",
      "Average test loss: 0.0013919478443761666\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04735754593544536\n",
      "Average test loss: 0.0013918295793442263\n",
      "Epoch 55/300\n",
      "Average training loss: 0.046593690160248016\n",
      "Average test loss: 0.0014006128224233786\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04596229940983984\n",
      "Average test loss: 0.0013867370301029749\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04536387316054768\n",
      "Average test loss: 0.0013785337341121501\n",
      "Epoch 58/300\n",
      "Average training loss: 0.044659855011436676\n",
      "Average test loss: 0.001370069426173965\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04419672127564748\n",
      "Average test loss: 0.0014128452707082033\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04368395702375306\n",
      "Average test loss: 0.0014099992227016224\n",
      "Epoch 61/300\n",
      "Average training loss: 0.042698510782586206\n",
      "Average test loss: 0.0013914378862310616\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04226278496781985\n",
      "Average test loss: 0.0013734492063522338\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04174945971204175\n",
      "Average test loss: 0.0014076239348699649\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04185612458156215\n",
      "Average test loss: 0.0013923991938225098\n",
      "Epoch 65/300\n",
      "Average training loss: 0.041390120117200745\n",
      "Average test loss: 0.001482105576278021\n",
      "Epoch 66/300\n",
      "Average training loss: 0.040260951426294114\n",
      "Average test loss: 0.0013906069603334698\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03959856264955468\n",
      "Average test loss: 0.0014307446939249833\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03911151367922624\n",
      "Average test loss: 0.0014194468604400753\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03872108776205116\n",
      "Average test loss: 0.0014500624746498134\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03834471130702231\n",
      "Average test loss: 0.00140280014452421\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03787954331437747\n",
      "Average test loss: 0.0014294468469710814\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03766063852608204\n",
      "Average test loss: 0.0014530808168152969\n",
      "Epoch 73/300\n",
      "Average training loss: 0.037204805089367764\n",
      "Average test loss: 0.001393311409590145\n",
      "Epoch 74/300\n",
      "Average training loss: 0.036589136822356115\n",
      "Average test loss: 0.0014354486486150158\n",
      "Epoch 75/300\n",
      "Average training loss: 0.036285265781813195\n",
      "Average test loss: 0.0014321359124862486\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0359105888588561\n",
      "Average test loss: 0.0014805508252854148\n",
      "Epoch 77/300\n",
      "Average training loss: 0.035610853483279546\n",
      "Average test loss: 0.001391431879459156\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03509529089927673\n",
      "Average test loss: 0.0014535281561935942\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03488179740309715\n",
      "Average test loss: 0.0015029376099507015\n",
      "Epoch 80/300\n",
      "Average training loss: 0.034636282950639724\n",
      "Average test loss: 0.0014791860124096275\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03432390154898167\n",
      "Average test loss: 0.0014233871661126613\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034283619471722176\n",
      "Average test loss: 0.0014897679291251632\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0338408286107911\n",
      "Average test loss: 0.0014022439108747574\n",
      "Epoch 84/300\n",
      "Average training loss: 0.033409898277786045\n",
      "Average test loss: 0.0014745608881736795\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03321024289396074\n",
      "Average test loss: 0.0014994456528996428\n",
      "Epoch 86/300\n",
      "Average training loss: 0.033054113401307\n",
      "Average test loss: 0.0014105797189598282\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03273776318464014\n",
      "Average test loss: 0.0014204149605292413\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03246508820023802\n",
      "Average test loss: 0.0014189026311246884\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03238269902931319\n",
      "Average test loss: 0.0014005811167880893\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03222772935860687\n",
      "Average test loss: 0.0014639407045518359\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03188075943953461\n",
      "Average test loss: 0.0014464673046022654\n",
      "Epoch 92/300\n",
      "Average training loss: 0.031709396320912575\n",
      "Average test loss: 0.001460517369935082\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03155190020634068\n",
      "Average test loss: 0.0014384088646620511\n",
      "Epoch 94/300\n",
      "Average training loss: 0.031458088599973254\n",
      "Average test loss: 0.0015376472279636396\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03109401662316587\n",
      "Average test loss: 0.0015450119870818323\n",
      "Epoch 96/300\n",
      "Average training loss: 0.030905930855207972\n",
      "Average test loss: 0.001500270566282173\n",
      "Epoch 97/300\n",
      "Average training loss: 0.030926038194033834\n",
      "Average test loss: 0.0014809099693213485\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03095189234945509\n",
      "Average test loss: 0.0014695213154579203\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03041235210498174\n",
      "Average test loss: 0.0014695459948852658\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03030541178915236\n",
      "Average test loss: 0.001474266747219695\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03017665085527632\n",
      "Average test loss: 0.0014627036444014973\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03007039305071036\n",
      "Average test loss: 0.0014907286817000972\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02989136423336135\n",
      "Average test loss: 0.0014557060804218055\n",
      "Epoch 104/300\n",
      "Average training loss: 0.029710374002655346\n",
      "Average test loss: 0.0014679110906500783\n",
      "Epoch 105/300\n",
      "Average training loss: 0.029441645183497005\n",
      "Average test loss: 0.0015144706080771155\n",
      "Epoch 106/300\n",
      "Average training loss: 0.029536116265588337\n",
      "Average test loss: 0.0014966714003951185\n",
      "Epoch 107/300\n",
      "Average training loss: 0.029312740392155118\n",
      "Average test loss: 0.0014766056624551614\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02920308982829253\n",
      "Average test loss: 0.0014628039004488125\n",
      "Epoch 109/300\n",
      "Average training loss: 0.029157333074344528\n",
      "Average test loss: 0.0015065872898946207\n",
      "Epoch 110/300\n",
      "Average training loss: 0.028994849562644958\n",
      "Average test loss: 0.0014899749412304825\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02886035566859775\n",
      "Average test loss: 0.0014801127399421401\n",
      "Epoch 112/300\n",
      "Average training loss: 0.028768514076868694\n",
      "Average test loss: 0.0015426132201424075\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02866225921114286\n",
      "Average test loss: 0.0015070684870911969\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028439275545378527\n",
      "Average test loss: 0.0016608260296699074\n",
      "Epoch 115/300\n",
      "Average training loss: 0.028418315551347204\n",
      "Average test loss: 0.0015007397150620819\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02838452589843008\n",
      "Average test loss: 0.0015459399091907672\n",
      "Epoch 117/300\n",
      "Average training loss: 0.028190888550546436\n",
      "Average test loss: 0.0015071231195082266\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02804706187049548\n",
      "Average test loss: 0.0015024058672392533\n",
      "Epoch 119/300\n",
      "Average training loss: 0.028062637041012445\n",
      "Average test loss: 0.0014710699985217718\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02787200733191437\n",
      "Average test loss: 0.0015230873288172814\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027858471006155013\n",
      "Average test loss: 0.0014926557005900476\n",
      "Epoch 122/300\n",
      "Average training loss: 0.027681408834126262\n",
      "Average test loss: 0.0016486805910244583\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027691747551163038\n",
      "Average test loss: 0.0015726344464346766\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027548711660835477\n",
      "Average test loss: 0.001527291198985444\n",
      "Epoch 125/300\n",
      "Average training loss: 0.027421402177876895\n",
      "Average test loss: 0.001496322998466591\n",
      "Epoch 126/300\n",
      "Average training loss: 0.027464918575353093\n",
      "Average test loss: 0.0015247213123366237\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02730004748536481\n",
      "Average test loss: 0.0015076804555331667\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027193295136094094\n",
      "Average test loss: 0.0014861500654369593\n",
      "Epoch 129/300\n",
      "Average training loss: 0.027107352882623673\n",
      "Average test loss: 0.0014935370205591122\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02704501078857316\n",
      "Average test loss: 0.0015474169082525704\n",
      "Epoch 131/300\n",
      "Average training loss: 0.026952767079075176\n",
      "Average test loss: 0.0015545623858148852\n",
      "Epoch 132/300\n",
      "Average training loss: 0.026903339400887488\n",
      "Average test loss: 0.0015143241652597983\n",
      "Epoch 133/300\n",
      "Average training loss: 0.026835691296392017\n",
      "Average test loss: 0.0015907371711606781\n",
      "Epoch 134/300\n",
      "Average training loss: 0.026787215534183714\n",
      "Average test loss: 0.0015633833140341771\n",
      "Epoch 135/300\n",
      "Average training loss: 0.026779098578625257\n",
      "Average test loss: 0.0015518075325008896\n",
      "Epoch 136/300\n",
      "Average training loss: 0.026592240538862017\n",
      "Average test loss: 0.0015291112179143561\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02646277375188139\n",
      "Average test loss: 0.0015297816734140119\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02647222665945689\n",
      "Average test loss: 0.00151010964728064\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02637321015695731\n",
      "Average test loss: 0.001550042504341238\n",
      "Epoch 140/300\n",
      "Average training loss: 0.026310756123728223\n",
      "Average test loss: 0.0015175637955673867\n",
      "Epoch 141/300\n",
      "Average training loss: 0.026251632864276567\n",
      "Average test loss: 0.0015354044224239057\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02619339701367749\n",
      "Average test loss: 0.0015457398970093992\n",
      "Epoch 143/300\n",
      "Average training loss: 0.026160503114263215\n",
      "Average test loss: 0.0015126225005110932\n",
      "Epoch 144/300\n",
      "Average training loss: 0.026040457689099843\n",
      "Average test loss: 0.0015339583975987302\n",
      "Epoch 145/300\n",
      "Average training loss: 0.025951424873537486\n",
      "Average test loss: 0.0015565542473147313\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02596743868622515\n",
      "Average test loss: 0.001504423586030801\n",
      "Epoch 147/300\n",
      "Average training loss: 0.025887260850932865\n",
      "Average test loss: 0.0015457848226651549\n",
      "Epoch 148/300\n",
      "Average training loss: 0.025741148442029953\n",
      "Average test loss: 0.0015443720797904664\n",
      "Epoch 149/300\n",
      "Average training loss: 0.025782321403423944\n",
      "Average test loss: 0.0015616856606470214\n",
      "Epoch 150/300\n",
      "Average training loss: 0.025793026611208917\n",
      "Average test loss: 0.0015390653184925516\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0255681631565094\n",
      "Average test loss: 0.001659818128993114\n",
      "Epoch 152/300\n",
      "Average training loss: 0.025552513965302042\n",
      "Average test loss: 0.0015745552275329827\n",
      "Epoch 153/300\n",
      "Average training loss: 0.025545885188712016\n",
      "Average test loss: 0.0015283189617718259\n",
      "Epoch 154/300\n",
      "Average training loss: 0.025454937628573843\n",
      "Average test loss: 0.0015136364520423942\n",
      "Epoch 155/300\n",
      "Average training loss: 0.025387321379449632\n",
      "Average test loss: 0.0015771360580498974\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02547625861234135\n",
      "Average test loss: 0.001575251622384207\n",
      "Epoch 157/300\n",
      "Average training loss: 0.025349811510907278\n",
      "Average test loss: 0.001534574726389514\n",
      "Epoch 158/300\n",
      "Average training loss: 0.025219579835732777\n",
      "Average test loss: 0.0015399178615253832\n",
      "Epoch 159/300\n",
      "Average training loss: 0.025181685499019092\n",
      "Average test loss: 0.0015635917431985338\n",
      "Epoch 160/300\n",
      "Average training loss: 0.025135610027445687\n",
      "Average test loss: 0.0015394969132418435\n",
      "Epoch 161/300\n",
      "Average training loss: 0.025107815730902885\n",
      "Average test loss: 0.0016342801459961467\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02503261672457059\n",
      "Average test loss: 0.001552222309427129\n",
      "Epoch 163/300\n",
      "Average training loss: 0.025045371340380776\n",
      "Average test loss: 0.001652853939268324\n",
      "Epoch 164/300\n",
      "Average training loss: 0.024949775114655494\n",
      "Average test loss: 0.0016857596271567874\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024997579612665705\n",
      "Average test loss: 0.0015353183287920224\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02487983079916901\n",
      "Average test loss: 0.001536606619031065\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024858003538515833\n",
      "Average test loss: 0.0015445274261550771\n",
      "Epoch 168/300\n",
      "Average training loss: 0.024746293685502477\n",
      "Average test loss: 0.001543633454841458\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02472482716706064\n",
      "Average test loss: 0.0016286473928226364\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02463031934036149\n",
      "Average test loss: 0.0015554045097281536\n",
      "Epoch 171/300\n",
      "Average training loss: 0.024669272133045726\n",
      "Average test loss: 0.0015529101757953564\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02470544948677222\n",
      "Average test loss: 0.0015222058500059777\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024626862853765488\n",
      "Average test loss: 0.001552130102697346\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02450186439189646\n",
      "Average test loss: 0.0015737867093541557\n",
      "Epoch 175/300\n",
      "Average training loss: 0.024510142965449226\n",
      "Average test loss: 0.001531576578091416\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024443131158749264\n",
      "Average test loss: 0.0015249212166915337\n",
      "Epoch 177/300\n",
      "Average training loss: 0.024419758067362837\n",
      "Average test loss: 0.0016237140553486016\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02442561582889822\n",
      "Average test loss: 0.0015735469239039554\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02433374581568771\n",
      "Average test loss: 0.0016232761935227448\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0242845871647199\n",
      "Average test loss: 0.001562992068938911\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02416310005552239\n",
      "Average test loss: 0.0015604694277668993\n",
      "Epoch 182/300\n",
      "Average training loss: 0.024198717425266903\n",
      "Average test loss: 0.001577275606079234\n",
      "Epoch 183/300\n",
      "Average training loss: 0.024093132227659225\n",
      "Average test loss: 0.0015592695684689614\n",
      "Epoch 184/300\n",
      "Average training loss: 0.024067642589410146\n",
      "Average test loss: 0.0015947160999187165\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0241594129320648\n",
      "Average test loss: 0.0016023549841096003\n",
      "Epoch 186/300\n",
      "Average training loss: 0.024039845150378016\n",
      "Average test loss: 0.0015615665663240685\n",
      "Epoch 187/300\n",
      "Average training loss: 0.024044210374355317\n",
      "Average test loss: 0.001514429833739996\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023957868843442864\n",
      "Average test loss: 0.0016146360909980203\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023878268046511546\n",
      "Average test loss: 0.0015497151166200638\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023909073220358956\n",
      "Average test loss: 0.001633871969146033\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023837908618979983\n",
      "Average test loss: 0.0015741884823267658\n",
      "Epoch 192/300\n",
      "Average training loss: 0.023765252598457867\n",
      "Average test loss: 0.0016176738218507832\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02386410704917378\n",
      "Average test loss: 0.0015538258662240374\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023682182566987144\n",
      "Average test loss: 0.001526032874878082\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02375175829562876\n",
      "Average test loss: 0.0015831215936276649\n",
      "Epoch 196/300\n",
      "Average training loss: 0.023680793745650186\n",
      "Average test loss: 0.0015779051423693697\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023668824076652526\n",
      "Average test loss: 0.0015681610036020477\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02355735134250588\n",
      "Average test loss: 0.001563898859338628\n",
      "Epoch 199/300\n",
      "Average training loss: 0.023592220119304126\n",
      "Average test loss: 0.001572960103655027\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02353355187508795\n",
      "Average test loss: 0.0015450387782313757\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023448008346888753\n",
      "Average test loss: 0.0016054919537984663\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0235111653059721\n",
      "Average test loss: 0.0016209613321762947\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023440676394436095\n",
      "Average test loss: 0.001582575041283336\n",
      "Epoch 204/300\n",
      "Average training loss: 0.023467212569382456\n",
      "Average test loss: 0.0015899690584176116\n",
      "Epoch 205/300\n",
      "Average training loss: 0.023407522489627203\n",
      "Average test loss: 0.0015610699210729863\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023356066066357824\n",
      "Average test loss: 0.0015843062587082386\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023298464382688205\n",
      "Average test loss: 0.0015509177925479081\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02329486818280485\n",
      "Average test loss: 0.0015788808691625793\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023287694801886877\n",
      "Average test loss: 0.0016668372573331\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02329532509545485\n",
      "Average test loss: 0.0015936150669844614\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023211491531795924\n",
      "Average test loss: 0.0015885242155442634\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023146865197353893\n",
      "Average test loss: 0.0015847670940889252\n",
      "Epoch 213/300\n",
      "Average training loss: 0.023132887144883475\n",
      "Average test loss: 0.0015754342507570982\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02310355586475796\n",
      "Average test loss: 0.0015946715381513867\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02306839934653706\n",
      "Average test loss: 0.0015431593604799774\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023088757728536923\n",
      "Average test loss: 0.0015313171074829168\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023042255293991832\n",
      "Average test loss: 0.0016830390009822116\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02304478985733456\n",
      "Average test loss: 0.0015959883821714256\n",
      "Epoch 219/300\n",
      "Average training loss: 0.022942965269088744\n",
      "Average test loss: 0.0015615134499967099\n",
      "Epoch 220/300\n",
      "Average training loss: 0.022971609903706443\n",
      "Average test loss: 0.0016640733413191306\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02298586816754606\n",
      "Average test loss: 0.0015773504461265274\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022937270907892122\n",
      "Average test loss: 0.0015972978044818673\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022793610498309136\n",
      "Average test loss: 0.00158520943335154\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022903807565569877\n",
      "Average test loss: 0.0015757129161308209\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02278647590180238\n",
      "Average test loss: 0.001647396759233541\n",
      "Epoch 226/300\n",
      "Average training loss: 0.022739405070741972\n",
      "Average test loss: 0.0016276990378068553\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02276025220586194\n",
      "Average test loss: 0.0016769072552108102\n",
      "Epoch 228/300\n",
      "Average training loss: 0.022789272038473023\n",
      "Average test loss: 0.0016145460431774457\n",
      "Epoch 229/300\n",
      "Average training loss: 0.022679091685348086\n",
      "Average test loss: 0.001594278455608421\n",
      "Epoch 230/300\n",
      "Average training loss: 0.022709002925290003\n",
      "Average test loss: 0.0015875857729050849\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022652320144904984\n",
      "Average test loss: 0.0015825957039164171\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02264850313961506\n",
      "Average test loss: 0.001561886570519871\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022627449341946177\n",
      "Average test loss: 0.0015832031855566634\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022650674793455335\n",
      "Average test loss: 0.001601668131434255\n",
      "Epoch 235/300\n",
      "Average training loss: 0.022542947188019753\n",
      "Average test loss: 0.0015588843418906132\n",
      "Epoch 236/300\n",
      "Average training loss: 0.022492495545082623\n",
      "Average test loss: 0.001534251974326455\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02256514295935631\n",
      "Average test loss: 0.0016217211172398593\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022466276814540227\n",
      "Average test loss: 0.0016309564094990493\n",
      "Epoch 239/300\n",
      "Average training loss: 0.022509377974602913\n",
      "Average test loss: 0.001593037375042008\n",
      "Epoch 240/300\n",
      "Average training loss: 0.022445542032519975\n",
      "Average test loss: 0.0016053953351866867\n",
      "Epoch 241/300\n",
      "Average training loss: 0.022375550389289855\n",
      "Average test loss: 0.0015878391258108118\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02244967744251092\n",
      "Average test loss: 0.0016031530302845769\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02237583386235767\n",
      "Average test loss: 0.0016102640568796131\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02240058669116762\n",
      "Average test loss: 0.0017118087217936086\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02236675155493948\n",
      "Average test loss: 0.0016172820159958468\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02228253071175681\n",
      "Average test loss: 0.0016049824912721911\n",
      "Epoch 247/300\n",
      "Average training loss: 0.022272079043918185\n",
      "Average test loss: 0.0015827367282989953\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02224694432814916\n",
      "Average test loss: 0.0018757195822480652\n",
      "Epoch 249/300\n",
      "Average training loss: 0.022233868742982547\n",
      "Average test loss: 0.0016140719575600492\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022230434661110243\n",
      "Average test loss: 0.0015826628230926063\n",
      "Epoch 251/300\n",
      "Average training loss: 0.022173239002625147\n",
      "Average test loss: 0.0016312390851477782\n",
      "Epoch 252/300\n",
      "Average training loss: 0.022270826008584765\n",
      "Average test loss: 0.0016000797301530837\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022173111776510875\n",
      "Average test loss: 0.001613751707598567\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022130224992831547\n",
      "Average test loss: 0.0016061821065636145\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02206118043926027\n",
      "Average test loss: 0.0016305208955374028\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02208983002603054\n",
      "Average test loss: 0.001619683613276316\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022122254769007366\n",
      "Average test loss: 0.0016797464039797585\n",
      "Epoch 258/300\n",
      "Average training loss: 0.022042905444900195\n",
      "Average test loss: 0.001614829327704178\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022070068748460875\n",
      "Average test loss: 0.0015741082434025076\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021950000304314824\n",
      "Average test loss: 0.0015647766643928156\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022004706682430375\n",
      "Average test loss: 0.0016326352862848176\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022000512295299104\n",
      "Average test loss: 0.0016081869140681293\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02194518558349874\n",
      "Average test loss: 0.001578312828567707\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02191134690079424\n",
      "Average test loss: 0.001588657138041324\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02194080713060167\n",
      "Average test loss: 0.0015994106721546914\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0219222181836764\n",
      "Average test loss: 0.0015902207605540753\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02180477029747433\n",
      "Average test loss: 0.0016088009296605984\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021808416939444013\n",
      "Average test loss: 0.0015962867978960276\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02187921599878205\n",
      "Average test loss: 0.0016030824140956005\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021821160939004686\n",
      "Average test loss: 0.0015562099688169028\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021788932401272984\n",
      "Average test loss: 0.0016180369990567366\n",
      "Epoch 272/300\n",
      "Average training loss: 0.021737091852558985\n",
      "Average test loss: 0.0015986329078053435\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02174133193989595\n",
      "Average test loss: 0.00156882981557606\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021701041632228427\n",
      "Average test loss: 0.0016086286287237373\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02170255843963888\n",
      "Average test loss: 0.0016088532727832595\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021682933709687657\n",
      "Average test loss: 0.0016104376992831628\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0217002429448896\n",
      "Average test loss: 0.0015910542953966392\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02161038442949454\n",
      "Average test loss: 0.001686708049641715\n",
      "Epoch 279/300\n",
      "Average training loss: 0.021682505793041654\n",
      "Average test loss: 0.0016031975999681485\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021592469056447346\n",
      "Average test loss: 0.001599417604195575\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021611555871036317\n",
      "Average test loss: 0.0015970295272353622\n",
      "Epoch 282/300\n",
      "Average training loss: 0.021633896408809555\n",
      "Average test loss: 0.0016285163445605173\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021579346673356162\n",
      "Average test loss: 0.0015775775736611751\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021580292703376875\n",
      "Average test loss: 0.0016029556042825182\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021502811551094056\n",
      "Average test loss: 0.0016046744585037232\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021485126389397514\n",
      "Average test loss: 0.0016941732575909958\n",
      "Epoch 287/300\n",
      "Average training loss: 0.021472854317890273\n",
      "Average test loss: 0.0016255642912454075\n",
      "Epoch 288/300\n",
      "Average training loss: 0.021486633136868476\n",
      "Average test loss: 0.001595033158755137\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02151523381140497\n",
      "Average test loss: 0.001622089609089825\n",
      "Epoch 290/300\n",
      "Average training loss: 0.021426953936616578\n",
      "Average test loss: 0.0016223477024791969\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02142506864004665\n",
      "Average test loss: 0.0016255703858203358\n",
      "Epoch 292/300\n",
      "Average training loss: 0.021351327636175685\n",
      "Average test loss: 0.0016283213822171092\n",
      "Epoch 293/300\n",
      "Average training loss: 0.021422337296936246\n",
      "Average test loss: 0.0016315918217102686\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021379326252473727\n",
      "Average test loss: 0.0016072713820677665\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021383045113748975\n",
      "Average test loss: 0.0016544491449991862\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021376155086689524\n",
      "Average test loss: 0.0015923326974734665\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021356508370902805\n",
      "Average test loss: 0.0015998476358751457\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021299990040560563\n",
      "Average test loss: 0.001647585070071121\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021251957305603558\n",
      "Average test loss: 0.001638690680368907\n",
      "Epoch 300/300\n",
      "Average training loss: 0.021284739245971043\n",
      "Average test loss: 0.0016384512769679227\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_50_Depth10/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.33\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.58\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.68\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.88\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.04\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.36\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.36\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.78\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.75\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.04\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.83\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.15\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.28\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.29\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.64\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.57\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.38\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.02\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 33.15\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.13\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.19\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.36\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.49\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.35\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.56\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.73\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.82\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.95\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.03\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
