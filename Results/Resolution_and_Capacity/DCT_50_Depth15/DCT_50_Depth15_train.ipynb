{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 15)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.310047152267562\n",
      "Average test loss: 0.00614732422803839\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09308548979957898\n",
      "Average test loss: 0.004944191807674037\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05317840196026696\n",
      "Average test loss: 0.004740561888863643\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03888480807012982\n",
      "Average test loss: 0.0045258611821466024\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03231124147110515\n",
      "Average test loss: 0.004423601384792063\n",
      "Epoch 6/300\n",
      "Average training loss: 0.028804059548510445\n",
      "Average test loss: 0.004358560786065128\n",
      "Epoch 7/300\n",
      "Average training loss: 0.026671734124422072\n",
      "Average test loss: 0.004329979678202006\n",
      "Epoch 8/300\n",
      "Average training loss: 0.025272134231196508\n",
      "Average test loss: 0.004292834219626254\n",
      "Epoch 9/300\n",
      "Average training loss: 0.024322570244471233\n",
      "Average test loss: 0.004256613709032535\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02364642131494151\n",
      "Average test loss: 0.0042081894541366235\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02314831759697861\n",
      "Average test loss: 0.004259663138538599\n",
      "Epoch 12/300\n",
      "Average training loss: 0.022771933348642456\n",
      "Average test loss: 0.004164021929932965\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02247636229130957\n",
      "Average test loss: 0.004135631810459826\n",
      "Epoch 14/300\n",
      "Average training loss: 0.022235402645336258\n",
      "Average test loss: 0.004127138172586759\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02204846042725775\n",
      "Average test loss: 0.004101856750746568\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021879932491315736\n",
      "Average test loss: 0.004096630335268047\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02172792951265971\n",
      "Average test loss: 0.004067864777520299\n",
      "Epoch 18/300\n",
      "Average training loss: 0.021587684969935152\n",
      "Average test loss: 0.004053395846237739\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021456012864907583\n",
      "Average test loss: 0.004060025458741519\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02134164299401972\n",
      "Average test loss: 0.004074414986703131\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021226846544278994\n",
      "Average test loss: 0.004018791363057163\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021126227700048024\n",
      "Average test loss: 0.004019944300254186\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0210407432400518\n",
      "Average test loss: 0.004062632879863182\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02093508960141076\n",
      "Average test loss: 0.004078107131231162\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020838781111770205\n",
      "Average test loss: 0.0043036735037134755\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020731132353345554\n",
      "Average test loss: 0.003986297541608413\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02061767162465387\n",
      "Average test loss: 0.0039793476632071865\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020506653292311564\n",
      "Average test loss: 0.003959926377154059\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02039266158723169\n",
      "Average test loss: 0.003940603113215831\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02027262695464823\n",
      "Average test loss: 0.003978262285805411\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020164746097392505\n",
      "Average test loss: 0.003966680976251761\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020090720726384056\n",
      "Average test loss: 0.003945304341821207\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019967068891558384\n",
      "Average test loss: 0.003967323743220833\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01989793232911163\n",
      "Average test loss: 0.003936903239952193\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01980840014749103\n",
      "Average test loss: 0.003933947248384357\n",
      "Epoch 36/300\n",
      "Average training loss: 0.019729856265915766\n",
      "Average test loss: 0.003978124202746484\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019658348687820966\n",
      "Average test loss: 0.00404426933825016\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019605413286222353\n",
      "Average test loss: 0.003905516234950887\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019528576319416364\n",
      "Average test loss: 0.003914987462676234\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01945717105600569\n",
      "Average test loss: 0.0039039993033640912\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019409002491997347\n",
      "Average test loss: 0.003904470813150207\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01934980744454596\n",
      "Average test loss: 0.003929600705289179\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01929617027607229\n",
      "Average test loss: 0.003922002408653497\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019247453292210896\n",
      "Average test loss: 0.003906675250579914\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019194387258754837\n",
      "Average test loss: 0.003916441385530763\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019124047043422857\n",
      "Average test loss: 0.0039337057574755615\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019056273902455966\n",
      "Average test loss: 0.0039030727427452803\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019002682091461286\n",
      "Average test loss: 0.0039206319935619835\n",
      "Epoch 49/300\n",
      "Average training loss: 0.018952411987715297\n",
      "Average test loss: 0.003915385839011934\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01889667167597347\n",
      "Average test loss: 0.003996969441158904\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018859024823539787\n",
      "Average test loss: 0.003945621150235335\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018795317361752193\n",
      "Average test loss: 0.003925499501327674\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018741978569163216\n",
      "Average test loss: 0.003925073779705498\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01867975182003445\n",
      "Average test loss: 0.003924918887515863\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01864044792784585\n",
      "Average test loss: 0.003913270674645901\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018588645387026998\n",
      "Average test loss: 0.003933889999985695\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018513031965328587\n",
      "Average test loss: 0.003989440660095877\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01847730500996113\n",
      "Average test loss: 0.00393870544888907\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01842424988746643\n",
      "Average test loss: 0.003988111253413889\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01836945468849606\n",
      "Average test loss: 0.003942056090674466\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018309178202516502\n",
      "Average test loss: 0.0039724998341666325\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018269590970542697\n",
      "Average test loss: 0.003970420822294222\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018197986505097814\n",
      "Average test loss: 0.003994213032432728\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01800130339463552\n",
      "Average test loss: 0.004057663610618975\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017960376507706113\n",
      "Average test loss: 0.004142779872235325\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01787633209841119\n",
      "Average test loss: 0.003941466734434167\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017848896647493045\n",
      "Average test loss: 0.004096004027873278\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017785018606318367\n",
      "Average test loss: 0.003994456489673919\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01776519550134738\n",
      "Average test loss: 0.004012018283829093\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017683341055280632\n",
      "Average test loss: 0.004202657743046681\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01766361663573318\n",
      "Average test loss: 0.004102231782757574\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017616103859411345\n",
      "Average test loss: 0.004000812831852171\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017534760005772115\n",
      "Average test loss: 0.004084056715584463\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017504804090493254\n",
      "Average test loss: 0.004092538997944858\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01746313961181376\n",
      "Average test loss: 0.004041535296166936\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017411225338776905\n",
      "Average test loss: 0.004011963085581859\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01736118257045746\n",
      "Average test loss: 0.004260312971348564\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017317552372813225\n",
      "Average test loss: 0.004110377054247591\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017283537490500343\n",
      "Average test loss: 0.0040212864219728445\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0172324996623728\n",
      "Average test loss: 0.004078809722016255\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017176121751467387\n",
      "Average test loss: 0.004068260547187594\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017146249521109793\n",
      "Average test loss: 0.004037022539724907\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017109007138344977\n",
      "Average test loss: 0.00415246179782682\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017055589301718604\n",
      "Average test loss: 0.0040994464902000295\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01700213460955355\n",
      "Average test loss: 0.004029641192820337\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016982635228170288\n",
      "Average test loss: 0.004134790335471431\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016924701554907693\n",
      "Average test loss: 0.0040932475912074245\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01689563024457958\n",
      "Average test loss: 0.00400895877679189\n",
      "Epoch 92/300\n",
      "Average training loss: 0.016847867355578475\n",
      "Average test loss: 0.004127087232553297\n",
      "Epoch 93/300\n",
      "Average training loss: 0.016829032064312036\n",
      "Average test loss: 0.004132185906171799\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01676896218955517\n",
      "Average test loss: 0.004039619507475032\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01673526327146424\n",
      "Average test loss: 0.004169101957645681\n",
      "Epoch 96/300\n",
      "Average training loss: 0.016690876232253182\n",
      "Average test loss: 0.004168000979142056\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016571835824184946\n",
      "Average test loss: 0.0041881800277365575\n",
      "Epoch 101/300\n",
      "Average training loss: 0.016527300915784304\n",
      "Average test loss: 0.0041250896083398\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01649774827890926\n",
      "Average test loss: 0.004144940763711929\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016480035444100698\n",
      "Average test loss: 0.004066690824511978\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016430148164431255\n",
      "Average test loss: 0.004119666499810087\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01638380889635947\n",
      "Average test loss: 0.004118757308771213\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01638088056941827\n",
      "Average test loss: 0.004307202156633138\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0163239379806651\n",
      "Average test loss: 0.00419734560781055\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016296396707495053\n",
      "Average test loss: 0.004085867444674174\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01625609012113677\n",
      "Average test loss: 0.004149053193628788\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01623006016926633\n",
      "Average test loss: 0.0041371185344954335\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016196645321117507\n",
      "Average test loss: 0.004231492259229223\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01619099286529753\n",
      "Average test loss: 0.004154537451763948\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016150573784278498\n",
      "Average test loss: 0.004241244144737721\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016123395803074043\n",
      "Average test loss: 0.004236220944258902\n",
      "Epoch 115/300\n",
      "Average training loss: 0.016076347752577727\n",
      "Average test loss: 0.004228977457516723\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01605718688170115\n",
      "Average test loss: 0.004129251631804639\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01605183219909668\n",
      "Average test loss: 0.004083443033819397\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016004347537954648\n",
      "Average test loss: 0.0041819502293235725\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015975035563111306\n",
      "Average test loss: 0.004137369670801693\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015967629649572903\n",
      "Average test loss: 0.004256605356517765\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015923977243403595\n",
      "Average test loss: 0.0042576724117000895\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015902286366456086\n",
      "Average test loss: 0.004146964054968622\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01586939458631807\n",
      "Average test loss: 0.004228783689439296\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015848565162056023\n",
      "Average test loss: 0.004195658959034417\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01581602230336931\n",
      "Average test loss: 0.004196892818849948\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01579405043522517\n",
      "Average test loss: 0.004317189309746027\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015791126894454163\n",
      "Average test loss: 0.004148965920839045\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01575582589374648\n",
      "Average test loss: 0.004130032529019647\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015730936663846173\n",
      "Average test loss: 0.004147450858106216\n",
      "Epoch 130/300\n",
      "Average training loss: 0.015699761778116227\n",
      "Average test loss: 0.004157820221657555\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015697010189294817\n",
      "Average test loss: 0.004358901264352931\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01566000937587685\n",
      "Average test loss: 0.0042477073433498545\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01563785032182932\n",
      "Average test loss: 0.004222337156534195\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01560440867394209\n",
      "Average test loss: 0.004313418528892928\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015596864809592565\n",
      "Average test loss: 0.004371456599483887\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015560417503118516\n",
      "Average test loss: 0.004352403042631017\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015555852307213677\n",
      "Average test loss: 0.004139471148037248\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015536155452330907\n",
      "Average test loss: 0.004213381648477581\n",
      "Epoch 139/300\n",
      "Average training loss: 0.015514686448706521\n",
      "Average test loss: 0.004223452541770207\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015479248777031898\n",
      "Average test loss: 0.004327095648066865\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015466511206494437\n",
      "Average test loss: 0.004272474126476381\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015442211898664633\n",
      "Average test loss: 0.004230013066281875\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01541411711441146\n",
      "Average test loss: 0.004152782467090421\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015398962852855523\n",
      "Average test loss: 0.004293884989288118\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015383669894602563\n",
      "Average test loss: 0.004182905389202966\n",
      "Epoch 146/300\n",
      "Average training loss: 0.015386054108540216\n",
      "Average test loss: 0.004354072723951605\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015369863074686792\n",
      "Average test loss: 0.004272776599559519\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015316949418021574\n",
      "Average test loss: 0.004173314913279481\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015308531677557363\n",
      "Average test loss: 0.004217351465175549\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015282022829684947\n",
      "Average test loss: 0.004248018668343624\n",
      "Epoch 151/300\n",
      "Average training loss: 0.015281273803777164\n",
      "Average test loss: 0.00439209323293633\n",
      "Epoch 152/300\n",
      "Average training loss: 0.015265239998698235\n",
      "Average test loss: 0.0042233180778308045\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015234518370694585\n",
      "Average test loss: 0.004251141985257467\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015204184018075466\n",
      "Average test loss: 0.004270939934584829\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015197808538046148\n",
      "Average test loss: 0.004336516074008412\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015172362964186404\n",
      "Average test loss: 0.004243659308801094\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01518755660371648\n",
      "Average test loss: 0.004189814192553361\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015135284107592371\n",
      "Average test loss: 0.004276444411526124\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015133352717591657\n",
      "Average test loss: 0.004305086703350147\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015115498795277543\n",
      "Average test loss: 0.004344297471145789\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015099579514728652\n",
      "Average test loss: 0.004341526517851485\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015073388270205922\n",
      "Average test loss: 0.004255376075290971\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01508377659569184\n",
      "Average test loss: 0.00430844275198049\n",
      "Epoch 164/300\n",
      "Average training loss: 0.015038350890907977\n",
      "Average test loss: 0.004289564217130343\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015017977360222074\n",
      "Average test loss: 0.004295714972126815\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015020361128780578\n",
      "Average test loss: 0.004268646342472897\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015012206289503309\n",
      "Average test loss: 0.0044240275617274975\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014997140525115861\n",
      "Average test loss: 0.00422731734936436\n",
      "Epoch 169/300\n",
      "Average training loss: 0.014969334316750368\n",
      "Average test loss: 0.00430749774682853\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014943795843256844\n",
      "Average test loss: 0.004268807450516356\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014946063333087498\n",
      "Average test loss: 0.0042395003139972685\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014914255604147912\n",
      "Average test loss: 0.00428359059492747\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014903143720080456\n",
      "Average test loss: 0.004312244802713394\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014902354020211431\n",
      "Average test loss: 0.004238958310749796\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01488905714948972\n",
      "Average test loss: 0.004420868575986889\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014858777084284359\n",
      "Average test loss: 0.0042927799762951\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014841685976419184\n",
      "Average test loss: 0.004299641536341773\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01485685569461849\n",
      "Average test loss: 0.004244690871694021\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01483426739854945\n",
      "Average test loss: 0.004156618923362759\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014821640661193266\n",
      "Average test loss: 0.004386471750007735\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014778506530655756\n",
      "Average test loss: 0.004373155164221923\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014784885732664002\n",
      "Average test loss: 0.0042416215286486675\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014775253093904919\n",
      "Average test loss: 0.004349327006687721\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014780660236875217\n",
      "Average test loss: 0.004324067779713207\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014739348243508074\n",
      "Average test loss: 0.004181477994140651\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014725010977023178\n",
      "Average test loss: 0.004307857503493627\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01470853857199351\n",
      "Average test loss: 0.004316139998535315\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014703381323152118\n",
      "Average test loss: 0.0043943621108515395\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014701005952225791\n",
      "Average test loss: 0.00421823442603151\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014692495082815489\n",
      "Average test loss: 0.00447688804484076\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014660855227046543\n",
      "Average test loss: 0.004311603667835394\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014646479436920748\n",
      "Average test loss: 0.004283761423081159\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014644825220935874\n",
      "Average test loss: 0.004404914787246121\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014620972271594736\n",
      "Average test loss: 0.0042456988332172235\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014641015715897083\n",
      "Average test loss: 0.004346391015789575\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014612327848871549\n",
      "Average test loss: 0.004322414596875508\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014591988060209487\n",
      "Average test loss: 0.004362336699747377\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01458402834335963\n",
      "Average test loss: 0.004416757321192158\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014578644514911705\n",
      "Average test loss: 0.004393795916603671\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01455968407375945\n",
      "Average test loss: 0.004386738080117437\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014536513003210226\n",
      "Average test loss: 0.004371070913970471\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014532640704678164\n",
      "Average test loss: 0.004239248192144765\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014526117554141415\n",
      "Average test loss: 0.0043621954725434385\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014508800039688746\n",
      "Average test loss: 0.004320523851861557\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01449628909677267\n",
      "Average test loss: 0.00450728861346013\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014501802613337835\n",
      "Average test loss: 0.004376370777686437\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014489143869115246\n",
      "Average test loss: 0.004358495384040806\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014465328853991297\n",
      "Average test loss: 0.004279871486127376\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014461210850212309\n",
      "Average test loss: 0.004412176206707954\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014445837764276399\n",
      "Average test loss: 0.004324446327570412\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01442544051590893\n",
      "Average test loss: 0.004261802855051226\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01442295547740327\n",
      "Average test loss: 0.004485295636372434\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014428850966195266\n",
      "Average test loss: 0.004352035374691089\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014396490646733179\n",
      "Average test loss: 0.004360919896927145\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01439183750996987\n",
      "Average test loss: 0.004319945876176159\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014385801509022713\n",
      "Average test loss: 0.00429103935468528\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014362884637382296\n",
      "Average test loss: 0.004398395919965373\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014365519843995572\n",
      "Average test loss: 0.004291994150314066\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014355995592143801\n",
      "Average test loss: 0.004187153575321039\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014353594936430455\n",
      "Average test loss: 0.0043423325291110405\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014319084310697184\n",
      "Average test loss: 0.004435457122822602\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014329969615572029\n",
      "Average test loss: 0.004295972600993183\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01430843454764949\n",
      "Average test loss: 0.004412745768411292\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014307150379651122\n",
      "Average test loss: 0.004388404682899515\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014298884588811133\n",
      "Average test loss: 0.004236997987661097\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01429745714697573\n",
      "Average test loss: 0.0043695903621200054\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014308745281563865\n",
      "Average test loss: 0.004373035000430213\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014250644315448072\n",
      "Average test loss: 0.0044314831631879015\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014251851025554868\n",
      "Average test loss: 0.004308857183075614\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014247291829023095\n",
      "Average test loss: 0.004358997575110859\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0142470049833258\n",
      "Average test loss: 0.00448087631786863\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014231296671761406\n",
      "Average test loss: 0.004274723414124714\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014201390356653267\n",
      "Average test loss: 0.0043646064026074275\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014220978253417544\n",
      "Average test loss: 0.004398759415994088\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014184887384374937\n",
      "Average test loss: 0.004332963057069315\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014201025238467587\n",
      "Average test loss: 0.004399657770577404\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014207431148323749\n",
      "Average test loss: 0.004314904352443086\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014162174943420622\n",
      "Average test loss: 0.004527160274071826\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014154946179025704\n",
      "Average test loss: 0.004312995620278848\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01414250500086281\n",
      "Average test loss: 0.004544553577486012\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014145395131574737\n",
      "Average test loss: 0.004347407862130138\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014123957143889533\n",
      "Average test loss: 0.004235451816270748\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01412404555413458\n",
      "Average test loss: 0.004353928858000371\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014137961957189771\n",
      "Average test loss: 0.004488052527937624\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014119519362019169\n",
      "Average test loss: 0.004523251479284631\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014094661578536033\n",
      "Average test loss: 0.0043681605979800225\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014101480323407386\n",
      "Average test loss: 0.004420702703297138\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014090198124448458\n",
      "Average test loss: 0.0045213311767826475\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014082296723292934\n",
      "Average test loss: 0.004357440387416217\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014084890452524026\n",
      "Average test loss: 0.004388174161108004\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014056008744570945\n",
      "Average test loss: 0.004423303479949633\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014059920521246062\n",
      "Average test loss: 0.004286253561990128\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014057188464535608\n",
      "Average test loss: 0.004343934066593647\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01404232420027256\n",
      "Average test loss: 0.004422487605777052\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014034133288595412\n",
      "Average test loss: 0.004390619737820493\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014047709605760045\n",
      "Average test loss: 0.004296497755373518\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014019882874356376\n",
      "Average test loss: 0.0044853153634402485\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014004109775854482\n",
      "Average test loss: 0.004495480960028039\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01398475650863515\n",
      "Average test loss: 0.004368175518181589\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013997633860343032\n",
      "Average test loss: 0.004597799051553011\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013978732937739956\n",
      "Average test loss: 0.004281678259372711\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013973698625134097\n",
      "Average test loss: 0.004635120721740855\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013977019190788269\n",
      "Average test loss: 0.004372892558160755\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013963845296866364\n",
      "Average test loss: 0.004399493889055318\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013964575063023302\n",
      "Average test loss: 0.0045243664561874335\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013952268808252281\n",
      "Average test loss: 0.00438943348162704\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013943746080829038\n",
      "Average test loss: 0.004427866685307688\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013916507239970895\n",
      "Average test loss: 0.004399387497454881\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013923373666488463\n",
      "Average test loss: 0.0043735136911273\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013920500342216757\n",
      "Average test loss: 0.004538374778297213\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013913329165014955\n",
      "Average test loss: 0.004380132982300388\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013901658138467205\n",
      "Average test loss: 0.004354024438808362\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013906741509834925\n",
      "Average test loss: 0.0044138347204360695\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013888548423846563\n",
      "Average test loss: 0.004410441077417797\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01387739592542251\n",
      "Average test loss: 0.00442053217978941\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013900207750499249\n",
      "Average test loss: 0.004543788571117653\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01386091735545132\n",
      "Average test loss: 0.004341697098066409\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0138746073320508\n",
      "Average test loss: 0.004325941185777386\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013843174418641462\n",
      "Average test loss: 0.004378535607208809\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013855217201842202\n",
      "Average test loss: 0.004486992914436592\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013827415353722042\n",
      "Average test loss: 0.004432021348012819\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013827695099843874\n",
      "Average test loss: 0.00448556944148408\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013818115871813563\n",
      "Average test loss: 0.004382411086724864\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013820703907973237\n",
      "Average test loss: 0.004455572953033778\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01382079648723205\n",
      "Average test loss: 0.0043777397457096315\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013810258873634868\n",
      "Average test loss: 0.004408501282955209\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013795120391580793\n",
      "Average test loss: 0.0044649157478577565\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0138089190704955\n",
      "Average test loss: 0.004528872126506434\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013786245967778894\n",
      "Average test loss: 0.004421411290557848\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013794727810555034\n",
      "Average test loss: 0.004497546631015009\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013788204205532869\n",
      "Average test loss: 0.004357476817650928\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013754100327690442\n",
      "Average test loss: 0.004392867498513725\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013769765789310138\n",
      "Average test loss: 0.00432009963484274\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013742043551471498\n",
      "Average test loss: 0.004417293661584457\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013750129032466146\n",
      "Average test loss: 0.0044594300443099606\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013759716104302142\n",
      "Average test loss: 0.004365248007078965\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013719856269657611\n",
      "Average test loss: 0.004561834036476082\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01371200504567888\n",
      "Average test loss: 0.004595473316394621\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013732791219320563\n",
      "Average test loss: 0.004298750396404001\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013740761668317848\n",
      "Average test loss: 0.004421771828085184\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2789841252035565\n",
      "Average test loss: 0.007091554936849409\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08627617243925731\n",
      "Average test loss: 0.004665821758823263\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04846225908067491\n",
      "Average test loss: 0.0042301027890708715\n",
      "Epoch 4/300\n",
      "Average training loss: 0.034867546081542966\n",
      "Average test loss: 0.004130235586108433\n",
      "Epoch 5/300\n",
      "Average training loss: 0.028900890681478712\n",
      "Average test loss: 0.003930335583372248\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0256296667340729\n",
      "Average test loss: 0.0038337216457972924\n",
      "Epoch 7/300\n",
      "Average training loss: 0.023698302515678935\n",
      "Average test loss: 0.003790850427829557\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022468038098679647\n",
      "Average test loss: 0.0036870195195078848\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021544107432166736\n",
      "Average test loss: 0.0037027561126483813\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02088187304470274\n",
      "Average test loss: 0.0035840463168505163\n",
      "Epoch 11/300\n",
      "Average training loss: 0.020369952546225655\n",
      "Average test loss: 0.003523359939042065\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01996115393936634\n",
      "Average test loss: 0.003496120248817735\n",
      "Epoch 13/300\n",
      "Average training loss: 0.019638935638798607\n",
      "Average test loss: 0.0034314083924723996\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019353924186693298\n",
      "Average test loss: 0.0034910137351188393\n",
      "Epoch 15/300\n",
      "Average training loss: 0.019122858499487243\n",
      "Average test loss: 0.003413469437923696\n",
      "Epoch 16/300\n",
      "Average training loss: 0.018911313836773237\n",
      "Average test loss: 0.003358084869053629\n",
      "Epoch 17/300\n",
      "Average training loss: 0.018708249646756384\n",
      "Average test loss: 0.00332420976087451\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018536828313436775\n",
      "Average test loss: 0.003307930249720812\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01836775916814804\n",
      "Average test loss: 0.003556524536675877\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018193497005436154\n",
      "Average test loss: 0.003246169108276566\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018034335740738445\n",
      "Average test loss: 0.003192966783212291\n",
      "Epoch 22/300\n",
      "Average training loss: 0.017899401267369587\n",
      "Average test loss: 0.0031905165722386705\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01773347180750635\n",
      "Average test loss: 0.003232900817361143\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017571216545171207\n",
      "Average test loss: 0.0031687824392898217\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017422124889161852\n",
      "Average test loss: 0.00310865401973327\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017278680581185552\n",
      "Average test loss: 0.003116097204801109\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017120585579839018\n",
      "Average test loss: 0.003097387144135104\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016978541443745294\n",
      "Average test loss: 0.0031820282513896625\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0168538211873836\n",
      "Average test loss: 0.0030913842788173095\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016698041097985374\n",
      "Average test loss: 0.003074747348100775\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016557937116258675\n",
      "Average test loss: 0.003026149754722913\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016409472827282216\n",
      "Average test loss: 0.003125904479374488\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016289438743558194\n",
      "Average test loss: 0.0030193020624832974\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01615131568825907\n",
      "Average test loss: 0.0031216031714446013\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01603960670116875\n",
      "Average test loss: 0.003010874077263806\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015910225502318805\n",
      "Average test loss: 0.003060434539284971\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015819975623653993\n",
      "Average test loss: 0.002987373638070292\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015682294745412138\n",
      "Average test loss: 0.0031437833606162004\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015589334986276097\n",
      "Average test loss: 0.002982062843317787\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015487720376915402\n",
      "Average test loss: 0.003068622941772143\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015368408353792296\n",
      "Average test loss: 0.003025530726545387\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01529684783021609\n",
      "Average test loss: 0.0032706596081455547\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015192612056103018\n",
      "Average test loss: 0.00307061885504259\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015077001802623271\n",
      "Average test loss: 0.002976305303060346\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015003001660936408\n",
      "Average test loss: 0.002966058367242416\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014887143364383114\n",
      "Average test loss: 0.002987734787993961\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014822836268279288\n",
      "Average test loss: 0.003077335266603364\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014719109962383906\n",
      "Average test loss: 0.0030573365673836734\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014632165854175885\n",
      "Average test loss: 0.003065951306786802\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014532523493799899\n",
      "Average test loss: 0.0030480086050099797\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01448385996537076\n",
      "Average test loss: 0.002943698724317882\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01437682345840666\n",
      "Average test loss: 0.002999339688155386\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014312668027149306\n",
      "Average test loss: 0.003007184697729018\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01422365753683779\n",
      "Average test loss: 0.002966945598108901\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014156836250589954\n",
      "Average test loss: 0.002935750862583518\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014069836589197318\n",
      "Average test loss: 0.0031036977338501147\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013996805489063263\n",
      "Average test loss: 0.0031160598268939388\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01392115746024582\n",
      "Average test loss: 0.0029921298031177785\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013857886196010643\n",
      "Average test loss: 0.002984538206209739\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013792004740900464\n",
      "Average test loss: 0.0031178682299537793\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013717203117907048\n",
      "Average test loss: 0.002987922297169765\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013676839523017407\n",
      "Average test loss: 0.0031918299686577584\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013617025279336505\n",
      "Average test loss: 0.0030065384950074886\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013527849594751994\n",
      "Average test loss: 0.002959016513493326\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013485399283468723\n",
      "Average test loss: 0.003044812970691257\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013436493255198002\n",
      "Average test loss: 0.0029866723488602373\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01339167279087835\n",
      "Average test loss: 0.003113460721861985\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013304314687020249\n",
      "Average test loss: 0.0030410359712938467\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013260541011061933\n",
      "Average test loss: 0.003202654086260332\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013228577035996648\n",
      "Average test loss: 0.0031548947395963803\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013150294989347458\n",
      "Average test loss: 0.003064103387710121\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013117077675130632\n",
      "Average test loss: 0.003007694081299835\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013073255866765976\n",
      "Average test loss: 0.003039231902816229\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013013044229812093\n",
      "Average test loss: 0.0030406942015720737\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012977826085355547\n",
      "Average test loss: 0.003079288704113828\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012919346503913402\n",
      "Average test loss: 0.0030254514544374414\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01286817106190655\n",
      "Average test loss: 0.0030706453909062677\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012841893542971876\n",
      "Average test loss: 0.003101321785400311\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012802813654972447\n",
      "Average test loss: 0.003216229905063907\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012741784010496405\n",
      "Average test loss: 0.0030661588828596806\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012726295171512497\n",
      "Average test loss: 0.0032443082965910434\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012683067462510533\n",
      "Average test loss: 0.003008170264048709\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0126402583113975\n",
      "Average test loss: 0.0030328747241033446\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012598112466434637\n",
      "Average test loss: 0.0031979198262302414\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012578396317238609\n",
      "Average test loss: 0.003398874159902334\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012535333640873432\n",
      "Average test loss: 0.003027715940856271\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012520501989457342\n",
      "Average test loss: 0.003152203747795688\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012469699701501264\n",
      "Average test loss: 0.003052734579063124\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012441237290700277\n",
      "Average test loss: 0.003016303099070986\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012377307217982081\n",
      "Average test loss: 0.003099640658332242\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01236086462934812\n",
      "Average test loss: 0.003143872884205646\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012347796456681357\n",
      "Average test loss: 0.0030400446028345162\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01230087446504169\n",
      "Average test loss: 0.0032140563629153704\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012311223169995679\n",
      "Average test loss: 0.00321350485790107\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012242784964541594\n",
      "Average test loss: 0.0030784424634443387\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012201333374612862\n",
      "Average test loss: 0.0031164459880027506\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012177613097760413\n",
      "Average test loss: 0.0032053056336525413\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012183803504125939\n",
      "Average test loss: 0.00311732954511212\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012155510885020097\n",
      "Average test loss: 0.003117235755754842\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01212823935266998\n",
      "Average test loss: 0.0031552988392197423\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012084239115317662\n",
      "Average test loss: 0.003197620911937621\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012047270921369395\n",
      "Average test loss: 0.0033614132671306533\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012025647081434727\n",
      "Average test loss: 0.0032040769023199875\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011992139514949587\n",
      "Average test loss: 0.0031453129137969677\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011984549277358585\n",
      "Average test loss: 0.0032944791704002355\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011965617947280406\n",
      "Average test loss: 0.0033244268575476276\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01193797129061487\n",
      "Average test loss: 0.003093705539074209\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011897421076065964\n",
      "Average test loss: 0.003102790279727843\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011900715976953507\n",
      "Average test loss: 0.0031077651573965946\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011866971494423019\n",
      "Average test loss: 0.003202080584441622\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01183735266327858\n",
      "Average test loss: 0.003176310245361593\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011822060954239634\n",
      "Average test loss: 0.0031714270070402157\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011801756901045641\n",
      "Average test loss: 0.0032470216904249456\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011792691756453779\n",
      "Average test loss: 0.0031357802500327427\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011762831445369456\n",
      "Average test loss: 0.0031176007056815756\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011725334511862862\n",
      "Average test loss: 0.0031083833328965638\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011706772819989258\n",
      "Average test loss: 0.003156405271962285\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01168722477555275\n",
      "Average test loss: 0.003183975017319123\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01167235067155626\n",
      "Average test loss: 0.0032054813388321135\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011686972249713209\n",
      "Average test loss: 0.003184765398916271\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011652041272156768\n",
      "Average test loss: 0.003220334377967649\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011612864116827647\n",
      "Average test loss: 0.0032458177349633643\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011624745378063784\n",
      "Average test loss: 0.0033115827451563546\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01159025549226337\n",
      "Average test loss: 0.003176659331139591\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011558684402041965\n",
      "Average test loss: 0.003125728477206495\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011541481060286364\n",
      "Average test loss: 0.0031997451742904054\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011542704846296046\n",
      "Average test loss: 0.003168017549233304\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011497277482516235\n",
      "Average test loss: 0.0032665187799268297\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011508264865312312\n",
      "Average test loss: 0.003238845508131716\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011496972141166528\n",
      "Average test loss: 0.00316829067758388\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011462287685109509\n",
      "Average test loss: 0.003247910714811749\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011442550072239505\n",
      "Average test loss: 0.003219817404738731\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011429205519457659\n",
      "Average test loss: 0.003216639665265878\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011415824670758513\n",
      "Average test loss: 0.0031893473302738533\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011408382542431355\n",
      "Average test loss: 0.0030948254234260983\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011390352320339945\n",
      "Average test loss: 0.0031988677295545736\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011375047306219736\n",
      "Average test loss: 0.0032472671736031773\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011340174798336294\n",
      "Average test loss: 0.0031414611063276727\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011339269194338057\n",
      "Average test loss: 0.0031516120529009235\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011326990626752376\n",
      "Average test loss: 0.0032064171089894243\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011305686465568014\n",
      "Average test loss: 0.003205214492873185\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011295036050180594\n",
      "Average test loss: 0.0031630576321234304\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011279032306538688\n",
      "Average test loss: 0.003199884276009268\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011252533124552832\n",
      "Average test loss: 0.003154818994096584\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011234638121392991\n",
      "Average test loss: 0.00311071836306817\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011263397679560715\n",
      "Average test loss: 0.0032704383755723637\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011243175544672543\n",
      "Average test loss: 0.0032902610024644267\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011216967241631614\n",
      "Average test loss: 0.003220281609230571\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01121153602335188\n",
      "Average test loss: 0.003241354431750046\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011184648178517819\n",
      "Average test loss: 0.003220697500846452\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011175146396789286\n",
      "Average test loss: 0.0033617892424679463\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011157205315927664\n",
      "Average test loss: 0.0032170334400402176\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011141154598858621\n",
      "Average test loss: 0.003323864732351568\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011136566704346073\n",
      "Average test loss: 0.0033054336694379647\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01118528934650951\n",
      "Average test loss: 0.003211463291818897\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011123228752778637\n",
      "Average test loss: 0.003251744910246796\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011110809915595585\n",
      "Average test loss: 0.003241784189310339\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011071684044268396\n",
      "Average test loss: 0.0032002213493817384\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01107297034147713\n",
      "Average test loss: 0.003168901055223412\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011069885501431094\n",
      "Average test loss: 0.0032520588984092077\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011057859281698862\n",
      "Average test loss: 0.003209968789170186\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01103397781567441\n",
      "Average test loss: 0.003289593479803039\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011034724011189408\n",
      "Average test loss: 0.0032219734258121914\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01101766266508235\n",
      "Average test loss: 0.0032767023391400774\n",
      "Epoch 165/300\n",
      "Average training loss: 0.010994491250150734\n",
      "Average test loss: 0.0033445844606806835\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01098730351568924\n",
      "Average test loss: 0.003207504584143559\n",
      "Epoch 167/300\n",
      "Average training loss: 0.010972546622157096\n",
      "Average test loss: 0.0031944358336428802\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011056619967851374\n",
      "Average test loss: 0.0032605723006029924\n",
      "Epoch 169/300\n",
      "Average training loss: 0.010967737663123343\n",
      "Average test loss: 0.003302324870808257\n",
      "Epoch 170/300\n",
      "Average training loss: 0.010951437928610377\n",
      "Average test loss: 0.003261914151410262\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01096737228996224\n",
      "Average test loss: 0.0032102458686050443\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0109471052404907\n",
      "Average test loss: 0.0032730417502009205\n",
      "Epoch 173/300\n",
      "Average training loss: 0.010912072536846002\n",
      "Average test loss: 0.003264550966728065\n",
      "Epoch 174/300\n",
      "Average training loss: 0.010903274041910967\n",
      "Average test loss: 0.003120382870030072\n",
      "Epoch 175/300\n",
      "Average training loss: 0.010876341587967343\n",
      "Average test loss: 0.0032402158793475894\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01090546988033586\n",
      "Average test loss: 0.003399487835665544\n",
      "Epoch 177/300\n",
      "Average training loss: 0.010879108217027452\n",
      "Average test loss: 0.0033692883853283194\n",
      "Epoch 178/300\n",
      "Average training loss: 0.010860016101764309\n",
      "Average test loss: 0.0033394057198117178\n",
      "Epoch 179/300\n",
      "Average training loss: 0.010878341228597694\n",
      "Average test loss: 0.0031604520380496977\n",
      "Epoch 180/300\n",
      "Average training loss: 0.010881866681906912\n",
      "Average test loss: 0.0032286813375022676\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010847991512881384\n",
      "Average test loss: 0.003130463365879324\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010843376180777946\n",
      "Average test loss: 0.0034654266983270643\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01081489452222983\n",
      "Average test loss: 0.0033328057964228924\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010823369277848138\n",
      "Average test loss: 0.0032341219960815375\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01081306755873892\n",
      "Average test loss: 0.0032193595070598856\n",
      "Epoch 186/300\n",
      "Average training loss: 0.010815997034311294\n",
      "Average test loss: 0.003260347269889381\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010790250070393085\n",
      "Average test loss: 0.003322521010724207\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010776299442682002\n",
      "Average test loss: 0.003229833584692743\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010795653317537573\n",
      "Average test loss: 0.003298400363160504\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010745128060380619\n",
      "Average test loss: 0.0033257927424791786\n",
      "Epoch 191/300\n",
      "Average training loss: 0.010758969549503592\n",
      "Average test loss: 0.003268264546783434\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010754787783655856\n",
      "Average test loss: 0.0033721340373158454\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010735455326735973\n",
      "Average test loss: 0.0036722448671029674\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010724749541944927\n",
      "Average test loss: 0.0032924695832447872\n",
      "Epoch 195/300\n",
      "Average training loss: 0.010726514128347239\n",
      "Average test loss: 0.0032162567749619486\n",
      "Epoch 196/300\n",
      "Average training loss: 0.010758564482960436\n",
      "Average test loss: 0.003279007179248664\n",
      "Epoch 197/300\n",
      "Average training loss: 0.010707641277876165\n",
      "Average test loss: 0.003396162484255102\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010693171092619498\n",
      "Average test loss: 0.003353220643268691\n",
      "Epoch 199/300\n",
      "Average training loss: 0.010685208486186133\n",
      "Average test loss: 0.003294874500690235\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0106976250352131\n",
      "Average test loss: 0.003359539236045546\n",
      "Epoch 201/300\n",
      "Average training loss: 0.010659854021337296\n",
      "Average test loss: 0.003281995163195663\n",
      "Epoch 202/300\n",
      "Average training loss: 0.010664777634044489\n",
      "Average test loss: 0.0034888633321970702\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010683588567707274\n",
      "Average test loss: 0.0033494875486940147\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010648207443455855\n",
      "Average test loss: 0.0032704484566218324\n",
      "Epoch 205/300\n",
      "Average training loss: 0.010638562590711647\n",
      "Average test loss: 0.0032663694208280906\n",
      "Epoch 206/300\n",
      "Average training loss: 0.010626519741283523\n",
      "Average test loss: 0.003262671178827683\n",
      "Epoch 207/300\n",
      "Average training loss: 0.010626700134740935\n",
      "Average test loss: 0.0033460413772198887\n",
      "Epoch 208/300\n",
      "Average training loss: 0.010623778036899036\n",
      "Average test loss: 0.003265404340500633\n",
      "Epoch 209/300\n",
      "Average training loss: 0.010643687670429548\n",
      "Average test loss: 0.003301854194245405\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01059555272012949\n",
      "Average test loss: 0.003301101892358727\n",
      "Epoch 211/300\n",
      "Average training loss: 0.010596195404728254\n",
      "Average test loss: 0.003256605571963721\n",
      "Epoch 212/300\n",
      "Average training loss: 0.010581259414553642\n",
      "Average test loss: 0.0034104670617315504\n",
      "Epoch 213/300\n",
      "Average training loss: 0.010578578886886437\n",
      "Average test loss: 0.0032642301480389304\n",
      "Epoch 214/300\n",
      "Average training loss: 0.010575867095755207\n",
      "Average test loss: 0.003264856794849038\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010575536953906218\n",
      "Average test loss: 0.0033887013209362824\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010548652900589837\n",
      "Average test loss: 0.003313483398821619\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010561940941545698\n",
      "Average test loss: 0.0035989587826447352\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0105444965718521\n",
      "Average test loss: 0.0032339508663862945\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010557341290016968\n",
      "Average test loss: 0.0034051688636342684\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010519286506705814\n",
      "Average test loss: 0.003306056051618523\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010515898647407691\n",
      "Average test loss: 0.003205376860582166\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010507844775501225\n",
      "Average test loss: 0.003357595657101936\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010515508267614576\n",
      "Average test loss: 0.0033935918201588922\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010512850936088298\n",
      "Average test loss: 0.003609279855257935\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010488692914446195\n",
      "Average test loss: 0.0032424034585969317\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01052024704300695\n",
      "Average test loss: 0.0034005351898570855\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010493263308372762\n",
      "Average test loss: 0.003315615687519312\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010472540798286597\n",
      "Average test loss: 0.003331656864947743\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010458894655936295\n",
      "Average test loss: 0.0033420827243891026\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010462963481744131\n",
      "Average test loss: 0.0032362114211751357\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010457504363523589\n",
      "Average test loss: 0.003233900833254059\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010460942524174848\n",
      "Average test loss: 0.0033266044039693145\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010451060498754184\n",
      "Average test loss: 0.0033513946512507067\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010434473263720672\n",
      "Average test loss: 0.0034405560518304505\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010428129345178604\n",
      "Average test loss: 0.003310603577333192\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010437548702789677\n",
      "Average test loss: 0.003275792249995801\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010408603378468089\n",
      "Average test loss: 0.0033027544795638983\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010415662282870875\n",
      "Average test loss: 0.0032575650148921544\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010413534653683503\n",
      "Average test loss: 0.0033464598823338748\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01042733967718151\n",
      "Average test loss: 0.0033426206223666666\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01039007753961616\n",
      "Average test loss: 0.0032472397258712185\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01040147265046835\n",
      "Average test loss: 0.00328423583569626\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010371553101473384\n",
      "Average test loss: 0.003377083706566029\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01037830709086524\n",
      "Average test loss: 0.0033160876099848084\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010373874373733997\n",
      "Average test loss: 0.0034432247603933016\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010341855742865138\n",
      "Average test loss: 0.003353845999472671\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010352725272377333\n",
      "Average test loss: 0.003352611863985658\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010364976892040835\n",
      "Average test loss: 0.0033634870876040725\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01036349970764584\n",
      "Average test loss: 0.003311418496693174\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01034545848849747\n",
      "Average test loss: 0.0032837071468432744\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010313962345321974\n",
      "Average test loss: 0.003185376358528932\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01032772109988663\n",
      "Average test loss: 0.0033228554603540236\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010321688936816321\n",
      "Average test loss: 0.0033569214300562937\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01033059756623374\n",
      "Average test loss: 0.0033645266147537363\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010318533099773857\n",
      "Average test loss: 0.003327428481231133\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01030682689530982\n",
      "Average test loss: 0.0034066532254219055\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010303192855583296\n",
      "Average test loss: 0.003351267098966572\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010290744497544236\n",
      "Average test loss: 0.0033196054134103986\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010287584436436494\n",
      "Average test loss: 0.003380576058394379\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010304108511242601\n",
      "Average test loss: 0.003285908379488521\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010285801270769702\n",
      "Average test loss: 0.003290905710723665\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010284857508209016\n",
      "Average test loss: 0.003309558272982637\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010283501278195117\n",
      "Average test loss: 0.003394450816636284\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01025144397881296\n",
      "Average test loss: 0.0032208201674123606\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010277795343349378\n",
      "Average test loss: 0.0032920701445804702\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01024756283313036\n",
      "Average test loss: 0.003274768570644988\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010248242905570401\n",
      "Average test loss: 0.0033201923312412367\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010259427833060424\n",
      "Average test loss: 0.003313264196738601\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010230216467960013\n",
      "Average test loss: 0.003299371570969621\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010227126632299688\n",
      "Average test loss: 0.0033332791956555514\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010223511926001972\n",
      "Average test loss: 0.003433112358053525\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010220351230767038\n",
      "Average test loss: 0.003468165612883038\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010214944520344337\n",
      "Average test loss: 0.0033430085335340765\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010198360646764437\n",
      "Average test loss: 0.0033232091868089304\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010210720496873061\n",
      "Average test loss: 0.0033124631856464677\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010212073252432877\n",
      "Average test loss: 0.0032629012055695058\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010212585771249401\n",
      "Average test loss: 0.0035569145617385706\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010189913017882241\n",
      "Average test loss: 0.0033924907512134974\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010187000723348723\n",
      "Average test loss: 0.003378666727286246\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010172836567792628\n",
      "Average test loss: 0.0033979266948170133\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0101485705309444\n",
      "Average test loss: 0.003252005122395025\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010183183579809137\n",
      "Average test loss: 0.003374370625863473\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010168985616001817\n",
      "Average test loss: 0.0034543738013340366\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010172333557986551\n",
      "Average test loss: 0.003381240590165059\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01017585170848502\n",
      "Average test loss: 0.0033603921251164543\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010165067052675619\n",
      "Average test loss: 0.0034600750375539066\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010156840295427376\n",
      "Average test loss: 0.0032851652912795543\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010152399462130335\n",
      "Average test loss: 0.0035036438964307307\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010147361122071742\n",
      "Average test loss: 0.003436938363437851\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010145344484597444\n",
      "Average test loss: 0.003438798224346505\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010117848642170428\n",
      "Average test loss: 0.003355967556643817\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010128815151751042\n",
      "Average test loss: 0.003379119620141056\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010134821934832467\n",
      "Average test loss: 0.003523331951970855\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010130494088762336\n",
      "Average test loss: 0.00333730807258851\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010132172217799558\n",
      "Average test loss: 0.0033984229627789723\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01010859090462327\n",
      "Average test loss: 0.0033623297558062607\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010120291764951415\n",
      "Average test loss: 0.0033775490385790667\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010083873747951454\n",
      "Average test loss: 0.003369647417631414\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010090947152839766\n",
      "Average test loss: 0.0034674012429184386\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010108292999366919\n",
      "Average test loss: 0.0032862041979614233\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.28229776039388443\n",
      "Average test loss: 0.005262520650401711\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08985857778125339\n",
      "Average test loss: 0.004150794124851624\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04758070887128512\n",
      "Average test loss: 0.003912112396624353\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03305712462796105\n",
      "Average test loss: 0.0035518179591745138\n",
      "Epoch 5/300\n",
      "Average training loss: 0.026548001936740347\n",
      "Average test loss: 0.003572033812602361\n",
      "Epoch 6/300\n",
      "Average training loss: 0.023085361702574626\n",
      "Average test loss: 0.0032731008993254766\n",
      "Epoch 7/300\n",
      "Average training loss: 0.021037583379281893\n",
      "Average test loss: 0.0031030533982233867\n",
      "Epoch 8/300\n",
      "Average training loss: 0.019663716646532218\n",
      "Average test loss: 0.0031567219392293028\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01871725498470995\n",
      "Average test loss: 0.0029320606105029583\n",
      "Epoch 10/300\n",
      "Average training loss: 0.018010543103019395\n",
      "Average test loss: 0.002959553773618407\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017486368760466577\n",
      "Average test loss: 0.0027598535008728506\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01706765753030777\n",
      "Average test loss: 0.0027455102612988815\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016720279100040595\n",
      "Average test loss: 0.00274919293096496\n",
      "Epoch 14/300\n",
      "Average training loss: 0.016426767443617186\n",
      "Average test loss: 0.002643729377951887\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016171880847877927\n",
      "Average test loss: 0.002581499146711495\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015936695550878843\n",
      "Average test loss: 0.0025444405358284712\n",
      "Epoch 17/300\n",
      "Average training loss: 0.015703236035174793\n",
      "Average test loss: 0.0025630617164489294\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015495762320856253\n",
      "Average test loss: 0.0024832864267130693\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015295751272804208\n",
      "Average test loss: 0.0025469589688711697\n",
      "Epoch 20/300\n",
      "Average training loss: 0.015114556720687284\n",
      "Average test loss: 0.0024345578950726323\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014944747756752703\n",
      "Average test loss: 0.0024153959825634957\n",
      "Epoch 22/300\n",
      "Average training loss: 0.014773937120205827\n",
      "Average test loss: 0.0023973526243741316\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014588878543840514\n",
      "Average test loss: 0.0023866588164948755\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014412001964118746\n",
      "Average test loss: 0.0023605182768983973\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014250823362833924\n",
      "Average test loss: 0.00235964441485703\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014052384922073948\n",
      "Average test loss: 0.002299662866319219\n",
      "Epoch 27/300\n",
      "Average training loss: 0.013868974509338538\n",
      "Average test loss: 0.0022954608955317074\n",
      "Epoch 28/300\n",
      "Average training loss: 0.013690841803120243\n",
      "Average test loss: 0.002373440782022145\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013501289224459066\n",
      "Average test loss: 0.0023081268032805785\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013329011802872022\n",
      "Average test loss: 0.002341231938555009\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013155928907295068\n",
      "Average test loss: 0.0022730737928715015\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013014081911080413\n",
      "Average test loss: 0.002240242630450262\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012851587508287694\n",
      "Average test loss: 0.002264073175481624\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012696164895262983\n",
      "Average test loss: 0.0021859723962843417\n",
      "Epoch 35/300\n",
      "Average training loss: 0.012543067606786886\n",
      "Average test loss: 0.00236677418069707\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012401985293461216\n",
      "Average test loss: 0.0021706970758322212\n",
      "Epoch 37/300\n",
      "Average training loss: 0.012288638807833195\n",
      "Average test loss: 0.00218543335236609\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012159820525182618\n",
      "Average test loss: 0.0021788786120919717\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012004131564663517\n",
      "Average test loss: 0.002183123748128613\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011932607558866342\n",
      "Average test loss: 0.002200484505544106\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011790527562714285\n",
      "Average test loss: 0.0021866381812012856\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011696243352360195\n",
      "Average test loss: 0.0022931922285093203\n",
      "Epoch 43/300\n",
      "Average training loss: 0.011576665592690308\n",
      "Average test loss: 0.002245639190491703\n",
      "Epoch 44/300\n",
      "Average training loss: 0.011510998649729622\n",
      "Average test loss: 0.0021506341750630073\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01140254280798965\n",
      "Average test loss: 0.0021865166135960155\n",
      "Epoch 46/300\n",
      "Average training loss: 0.011327161208622986\n",
      "Average test loss: 0.002166573773862587\n",
      "Epoch 47/300\n",
      "Average training loss: 0.011206342937217818\n",
      "Average test loss: 0.002153120659084784\n",
      "Epoch 48/300\n",
      "Average training loss: 0.011128629848361016\n",
      "Average test loss: 0.0021525022260223824\n",
      "Epoch 49/300\n",
      "Average training loss: 0.011048339136772686\n",
      "Average test loss: 0.0021525212075147364\n",
      "Epoch 50/300\n",
      "Average training loss: 0.010988523359100024\n",
      "Average test loss: 0.002203142802024053\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010883309563000997\n",
      "Average test loss: 0.002146721824589703\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010822073251836829\n",
      "Average test loss: 0.0021521166070467895\n",
      "Epoch 53/300\n",
      "Average training loss: 0.010740247603919771\n",
      "Average test loss: 0.0021457814116858775\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010685853946126169\n",
      "Average test loss: 0.0021926036319798894\n",
      "Epoch 55/300\n",
      "Average training loss: 0.010613821861644586\n",
      "Average test loss: 0.0022309830770310428\n",
      "Epoch 56/300\n",
      "Average training loss: 0.010546124148699973\n",
      "Average test loss: 0.0021280383073414367\n",
      "Epoch 57/300\n",
      "Average training loss: 0.010475619239525662\n",
      "Average test loss: 0.002406264110571808\n",
      "Epoch 58/300\n",
      "Average training loss: 0.010406033688121371\n",
      "Average test loss: 0.002165685708634555\n",
      "Epoch 59/300\n",
      "Average training loss: 0.010346917675601112\n",
      "Average test loss: 0.0021189377415511343\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010294094214422836\n",
      "Average test loss: 0.002164719059752921\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01024433770775795\n",
      "Average test loss: 0.002187587015951673\n",
      "Epoch 62/300\n",
      "Average training loss: 0.010190181575715542\n",
      "Average test loss: 0.002133841591576735\n",
      "Epoch 63/300\n",
      "Average training loss: 0.010147493005626732\n",
      "Average test loss: 0.0021777978187633887\n",
      "Epoch 64/300\n",
      "Average training loss: 0.010052118765811125\n",
      "Average test loss: 0.002200600242293957\n",
      "Epoch 65/300\n",
      "Average training loss: 0.010016468941337533\n",
      "Average test loss: 0.0021568206004384492\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00994978641346097\n",
      "Average test loss: 0.0021393612926411958\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009937002013954851\n",
      "Average test loss: 0.0021508212176462017\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0099078430922495\n",
      "Average test loss: 0.0021341210036642023\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009829763930290938\n",
      "Average test loss: 0.0022780587583159408\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009772520915087726\n",
      "Average test loss: 0.0021796504352241753\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009754281690551175\n",
      "Average test loss: 0.0021681955804427463\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009681005454725689\n",
      "Average test loss: 0.0022272060712178548\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009659118000004026\n",
      "Average test loss: 0.0021668345590846404\n",
      "Epoch 74/300\n",
      "Average training loss: 0.009602119290994273\n",
      "Average test loss: 0.0022250423445883726\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009588494200052486\n",
      "Average test loss: 0.002471778452810314\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009565462263094054\n",
      "Average test loss: 0.002203419465985563\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009494043234321806\n",
      "Average test loss: 0.0021961529482569958\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009464473000003231\n",
      "Average test loss: 0.0022216570022412473\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009442668804691898\n",
      "Average test loss: 0.0022492580935359\n",
      "Epoch 80/300\n",
      "Average training loss: 0.009392196263704035\n",
      "Average test loss: 0.002234720543958247\n",
      "Epoch 81/300\n",
      "Average training loss: 0.009398406753523483\n",
      "Average test loss: 0.0022408295451766913\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009356640411747826\n",
      "Average test loss: 0.002221060808334086\n",
      "Epoch 83/300\n",
      "Average training loss: 0.009285926022463375\n",
      "Average test loss: 0.005272396861058143\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009262896523707443\n",
      "Average test loss: 0.0022755762392448055\n",
      "Epoch 85/300\n",
      "Average training loss: 0.009247160031563706\n",
      "Average test loss: 0.0022191911209374668\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00922611228707764\n",
      "Average test loss: 0.002325576339227458\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009189400045408143\n",
      "Average test loss: 0.002232547365128994\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00915749941021204\n",
      "Average test loss: 0.0023838084497385553\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009142667078309588\n",
      "Average test loss: 0.0022442720954616866\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009116632036036916\n",
      "Average test loss: 0.002328260015903248\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009094100400805473\n",
      "Average test loss: 0.002175129319127235\n",
      "Epoch 92/300\n",
      "Average training loss: 0.009076409147845374\n",
      "Average test loss: 0.0025804515108466147\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00902440183609724\n",
      "Average test loss: 0.0022080029061891967\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009016192196143998\n",
      "Average test loss: 0.002260991969663236\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008981746003445652\n",
      "Average test loss: 0.0022639945768233804\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008967137977066967\n",
      "Average test loss: 0.0022837751296659312\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008947026969658003\n",
      "Average test loss: 0.002267621951798598\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008919386764367421\n",
      "Average test loss: 0.0022655759361158642\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008891213143037426\n",
      "Average test loss: 0.0023992723011308247\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008875970330503252\n",
      "Average test loss: 0.002421100927102897\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0088609971685542\n",
      "Average test loss: 0.0022578800353739\n",
      "Epoch 102/300\n",
      "Average training loss: 0.00884150230925944\n",
      "Average test loss: 0.0024082798158956897\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008819283271001444\n",
      "Average test loss: 0.0023156678800781566\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00878231222430865\n",
      "Average test loss: 0.0022656465013407997\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008770031981170177\n",
      "Average test loss: 0.0021984230491022268\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008772053435444831\n",
      "Average test loss: 0.0022656275129152668\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008736624071995417\n",
      "Average test loss: 0.002319368110348781\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00872170463618305\n",
      "Average test loss: 0.0023612888313622937\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008718016736209392\n",
      "Average test loss: 0.002385506125787894\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008697545265158018\n",
      "Average test loss: 0.0023055836105098327\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00866891503168477\n",
      "Average test loss: 0.002336792173381481\n",
      "Epoch 112/300\n",
      "Average training loss: 0.00864853745740321\n",
      "Average test loss: 0.002354173839609656\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008661104214688142\n",
      "Average test loss: 0.002347287560813129\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008646210793819692\n",
      "Average test loss: 0.0023017138141310876\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00863945276538531\n",
      "Average test loss: 0.002308953368622396\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008611946565823423\n",
      "Average test loss: 0.002346706276345584\n",
      "Epoch 117/300\n",
      "Average training loss: 0.00857575268877877\n",
      "Average test loss: 0.0023518381760352187\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008554836216900084\n",
      "Average test loss: 0.0022796396397882037\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008547882438533836\n",
      "Average test loss: 0.0022717593593729866\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008538126731084453\n",
      "Average test loss: 0.0023092771453989878\n",
      "Epoch 121/300\n",
      "Average training loss: 0.008525311495694849\n",
      "Average test loss: 0.0023101505287405517\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008491648801498943\n",
      "Average test loss: 0.0022596761147595113\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008519394516117043\n",
      "Average test loss: 0.002285754067202409\n",
      "Epoch 124/300\n",
      "Average training loss: 0.00850146634876728\n",
      "Average test loss: 0.002284391221279899\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008464893024000856\n",
      "Average test loss: 0.002267623169761565\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008454981298082405\n",
      "Average test loss: 0.0023429551535389488\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00846234240465694\n",
      "Average test loss: 0.0023438447991179097\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008450931354529328\n",
      "Average test loss: 0.0023614084621787898\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008416447660989231\n",
      "Average test loss: 0.002346722506607572\n",
      "Epoch 130/300\n",
      "Average training loss: 0.008423045266419649\n",
      "Average test loss: 0.002350060928819908\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008386953812506464\n",
      "Average test loss: 0.002321059810411599\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008402745356990232\n",
      "Average test loss: 0.002275056132218904\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008360654772155815\n",
      "Average test loss: 0.002341732246801257\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008362464495003224\n",
      "Average test loss: 0.002395802626800206\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008347264354427656\n",
      "Average test loss: 0.002320781519636512\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008328222850130663\n",
      "Average test loss: 0.002366124098913537\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00833506289910939\n",
      "Average test loss: 0.002316956730869909\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008325704141623444\n",
      "Average test loss: 0.0023305789885214633\n",
      "Epoch 139/300\n",
      "Average training loss: 0.008319708024462065\n",
      "Average test loss: 0.0023968540003730192\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008306433674361971\n",
      "Average test loss: 0.002430312579808136\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0082987087600761\n",
      "Average test loss: 0.0025065182538496125\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008273072621888585\n",
      "Average test loss: 0.002401136429773437\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008250837061140272\n",
      "Average test loss: 0.002294661748947369\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008269436328775352\n",
      "Average test loss: 0.002256924712202615\n",
      "Epoch 145/300\n",
      "Average training loss: 0.00824267318389482\n",
      "Average test loss: 0.0023502188163499037\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008237220561338795\n",
      "Average test loss: 0.002388615900857581\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00824060184094641\n",
      "Average test loss: 0.002361422202653355\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00820890331765016\n",
      "Average test loss: 0.0023674820887131824\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008215918546335564\n",
      "Average test loss: 0.0025377575320502123\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008200771768473917\n",
      "Average test loss: 0.0024621989793247646\n",
      "Epoch 151/300\n",
      "Average training loss: 0.00817509285112222\n",
      "Average test loss: 0.002395899541883005\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008182355033854644\n",
      "Average test loss: 0.0025553329868449105\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008156144204652972\n",
      "Average test loss: 0.0024869957896363403\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008160345355669658\n",
      "Average test loss: 0.002375728903752234\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008142360131773684\n",
      "Average test loss: 0.0023831051621172163\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00813558902963996\n",
      "Average test loss: 0.0024660899781932433\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008148678602857722\n",
      "Average test loss: 0.002354347678108348\n",
      "Epoch 158/300\n",
      "Average training loss: 0.008126012214355998\n",
      "Average test loss: 0.0024872377792166337\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008102945112519794\n",
      "Average test loss: 0.0025257985709855953\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00809678058574597\n",
      "Average test loss: 0.002361752056827148\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0080995907170905\n",
      "Average test loss: 0.00237399429269135\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008111250884830952\n",
      "Average test loss: 0.002361258034904798\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008074081297549936\n",
      "Average test loss: 0.0024892491489234897\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008065905808574624\n",
      "Average test loss: 0.0024175025135692624\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008065597373578284\n",
      "Average test loss: 0.0024276818829692074\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008075296819623974\n",
      "Average test loss: 0.0025455223286731375\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008041085836374097\n",
      "Average test loss: 0.0023336527473810645\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008039925543384419\n",
      "Average test loss: 0.0023593101164119112\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008030311981009113\n",
      "Average test loss: 0.002374782159924507\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00803058596369293\n",
      "Average test loss: 0.00241583082307544\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008035105492091842\n",
      "Average test loss: 0.0024293964621093537\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00800678060327967\n",
      "Average test loss: 0.0024039779228882657\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008013562538143662\n",
      "Average test loss: 0.0023799342597938247\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008007573203908073\n",
      "Average test loss: 0.00239637473018633\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008013197247352865\n",
      "Average test loss: 0.0024151034149237807\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007986983790165849\n",
      "Average test loss: 0.002392635811327232\n",
      "Epoch 177/300\n",
      "Average training loss: 0.007961479078150457\n",
      "Average test loss: 0.002444852796072761\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007967678604854478\n",
      "Average test loss: 0.002389021171670821\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007963575930231148\n",
      "Average test loss: 0.0024243766396409936\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007959956150915888\n",
      "Average test loss: 0.0024143191560481985\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007949319527380996\n",
      "Average test loss: 0.0023608641574780144\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007948110461648967\n",
      "Average test loss: 0.002542699281229741\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007943918190896512\n",
      "Average test loss: 0.00239043426927593\n",
      "Epoch 184/300\n",
      "Average training loss: 0.00794922166566054\n",
      "Average test loss: 0.0024390180052982434\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007923894816802607\n",
      "Average test loss: 0.002513983788796597\n",
      "Epoch 186/300\n",
      "Average training loss: 0.007911787309580379\n",
      "Average test loss: 0.002496218666848209\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007906510541422499\n",
      "Average test loss: 0.0023347366586741474\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007910307182206047\n",
      "Average test loss: 0.0023968648018522394\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007900908450285593\n",
      "Average test loss: 0.002424541211583548\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007892747890204192\n",
      "Average test loss: 0.0023864095812249513\n",
      "Epoch 191/300\n",
      "Average training loss: 0.007888754736218188\n",
      "Average test loss: 0.0024013334386464623\n",
      "Epoch 192/300\n",
      "Average training loss: 0.007894778019852108\n",
      "Average test loss: 0.0024003846001707844\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007867743858860599\n",
      "Average test loss: 0.0024598340241031515\n",
      "Epoch 194/300\n",
      "Average training loss: 0.007893468736774392\n",
      "Average test loss: 0.002380457903775904\n",
      "Epoch 195/300\n",
      "Average training loss: 0.007893544167280197\n",
      "Average test loss: 0.002471681670182281\n",
      "Epoch 196/300\n",
      "Average training loss: 0.007845801575730244\n",
      "Average test loss: 0.0024320994792506097\n",
      "Epoch 197/300\n",
      "Average training loss: 0.007855625688201851\n",
      "Average test loss: 0.0025188095120506153\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007846069924533367\n",
      "Average test loss: 0.002423499426080121\n",
      "Epoch 199/300\n",
      "Average training loss: 0.007831966840558582\n",
      "Average test loss: 0.002413194943943785\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007848573337826464\n",
      "Average test loss: 0.0024152713391102022\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00783279170261489\n",
      "Average test loss: 0.002691183273887469\n",
      "Epoch 202/300\n",
      "Average training loss: 0.007819233189854356\n",
      "Average test loss: 0.002539232561985652\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007821674719452857\n",
      "Average test loss: 0.0025119161961807144\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007808605468107594\n",
      "Average test loss: 0.0024812422485815154\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007821063477132056\n",
      "Average test loss: 0.0023929011603403424\n",
      "Epoch 206/300\n",
      "Average training loss: 0.007804363350487418\n",
      "Average test loss: 0.002492561202806731\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0077896402490635714\n",
      "Average test loss: 0.002429432399363981\n",
      "Epoch 208/300\n",
      "Average training loss: 0.007802155225227277\n",
      "Average test loss: 0.0024752498363248177\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007780271342645089\n",
      "Average test loss: 0.002437953674958812\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007783633147676786\n",
      "Average test loss: 0.0025406587125940457\n",
      "Epoch 211/300\n",
      "Average training loss: 0.007790398407313559\n",
      "Average test loss: 0.00250296108590232\n",
      "Epoch 212/300\n",
      "Average training loss: 0.007757220362623532\n",
      "Average test loss: 0.0024793446641415357\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007758291860421498\n",
      "Average test loss: 0.002469425871554348\n",
      "Epoch 214/300\n",
      "Average training loss: 0.007757271961205536\n",
      "Average test loss: 0.0024617510363459587\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007745550571216477\n",
      "Average test loss: 0.0024909150645964677\n",
      "Epoch 216/300\n",
      "Average training loss: 0.007760932670285305\n",
      "Average test loss: 0.00250594708799488\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0077300394049121274\n",
      "Average test loss: 0.0024290431977974044\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0077356194489532045\n",
      "Average test loss: 0.002388030642643571\n",
      "Epoch 219/300\n",
      "Average training loss: 0.007734323736694124\n",
      "Average test loss: 0.0024580622462348806\n",
      "Epoch 220/300\n",
      "Average training loss: 0.007735998680194219\n",
      "Average test loss: 0.002422485171506802\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007733153660678201\n",
      "Average test loss: 0.0025230775233358147\n",
      "Epoch 222/300\n",
      "Average training loss: 0.007715931545529101\n",
      "Average test loss: 0.0024688210347667337\n",
      "Epoch 223/300\n",
      "Average training loss: 0.007714707978897625\n",
      "Average test loss: 0.0024554268336958356\n",
      "Epoch 224/300\n",
      "Average training loss: 0.007710350768433676\n",
      "Average test loss: 0.0024572873767465353\n",
      "Epoch 225/300\n",
      "Average training loss: 0.007748454585671425\n",
      "Average test loss: 0.002403115041760935\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0077056585492359265\n",
      "Average test loss: 0.0024722928872538937\n",
      "Epoch 227/300\n",
      "Average training loss: 0.007693377202169763\n",
      "Average test loss: 0.0024111673201744756\n",
      "Epoch 228/300\n",
      "Average training loss: 0.007695573787722323\n",
      "Average test loss: 0.0024823333296096987\n",
      "Epoch 229/300\n",
      "Average training loss: 0.007677747343977292\n",
      "Average test loss: 0.0024765899676001736\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007676421605878406\n",
      "Average test loss: 0.002488136475491855\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007703414538254341\n",
      "Average test loss: 0.0025684553501713605\n",
      "Epoch 232/300\n",
      "Average training loss: 0.007664936879442798\n",
      "Average test loss: 0.0024644393739290536\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007674328216248088\n",
      "Average test loss: 0.002613709362844626\n",
      "Epoch 234/300\n",
      "Average training loss: 0.007671668503847387\n",
      "Average test loss: 0.0024787022575943007\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007670483943488863\n",
      "Average test loss: 0.0024621888266669377\n",
      "Epoch 236/300\n",
      "Average training loss: 0.007653705715719197\n",
      "Average test loss: 0.0023443290523977745\n",
      "Epoch 237/300\n",
      "Average training loss: 0.007660009986410538\n",
      "Average test loss: 0.0024254082408216265\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00767133364867833\n",
      "Average test loss: 0.0024718415478451384\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007635647758013672\n",
      "Average test loss: 0.002432295218730966\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0076413258757028315\n",
      "Average test loss: 0.002378453788657983\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007634414241131809\n",
      "Average test loss: 0.0024342576515757374\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0076378678178621665\n",
      "Average test loss: 0.002546462696666519\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0076162704734338655\n",
      "Average test loss: 0.0023908528052787813\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007625934796200858\n",
      "Average test loss: 0.002351333464185397\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0076382995429966185\n",
      "Average test loss: 0.0024686313327401876\n",
      "Epoch 246/300\n",
      "Average training loss: 0.007600857487155332\n",
      "Average test loss: 0.0025115934202654495\n",
      "Epoch 247/300\n",
      "Average training loss: 0.007627224820769495\n",
      "Average test loss: 0.002466532541645898\n",
      "Epoch 248/300\n",
      "Average training loss: 0.007602455378406578\n",
      "Average test loss: 0.0025690647018038566\n",
      "Epoch 249/300\n",
      "Average training loss: 0.007606968466192484\n",
      "Average test loss: 0.002490152198821306\n",
      "Epoch 250/300\n",
      "Average training loss: 0.007608439234809743\n",
      "Average test loss: 0.002501290469740828\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007599944633328253\n",
      "Average test loss: 0.002496722638607025\n",
      "Epoch 252/300\n",
      "Average training loss: 0.007584448147979048\n",
      "Average test loss: 0.0024776721592578623\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007595268528908491\n",
      "Average test loss: 0.0024845123851878776\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0075881277310351535\n",
      "Average test loss: 0.0024856849859158196\n",
      "Epoch 255/300\n",
      "Average training loss: 0.007585181977185938\n",
      "Average test loss: 0.002513224231079221\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0075629030453662075\n",
      "Average test loss: 0.002468730518594384\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007585302928462625\n",
      "Average test loss: 0.002582683425810602\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007572696410533455\n",
      "Average test loss: 0.00269618346190287\n",
      "Epoch 259/300\n",
      "Average training loss: 0.007584330778155062\n",
      "Average test loss: 0.0024566484296487437\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007559517974654833\n",
      "Average test loss: 0.0025003414309273165\n",
      "Epoch 261/300\n",
      "Average training loss: 0.007559007117731703\n",
      "Average test loss: 0.0023673577130668695\n",
      "Epoch 262/300\n",
      "Average training loss: 0.007558531624161535\n",
      "Average test loss: 0.0024324045966689785\n",
      "Epoch 263/300\n",
      "Average training loss: 0.007539435709930128\n",
      "Average test loss: 0.002455268925883704\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007522531398054626\n",
      "Average test loss: 0.0025367576932120656\n",
      "Epoch 265/300\n",
      "Average training loss: 0.007558420529382096\n",
      "Average test loss: 0.002485099179463254\n",
      "Epoch 266/300\n",
      "Average training loss: 0.007550519582298067\n",
      "Average test loss: 0.00255711300815973\n",
      "Epoch 267/300\n",
      "Average training loss: 0.00751737561614977\n",
      "Average test loss: 0.0025164245853407517\n",
      "Epoch 268/300\n",
      "Average training loss: 0.007521006406181388\n",
      "Average test loss: 0.0024512983775801128\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007537153602474265\n",
      "Average test loss: 0.002530997359711263\n",
      "Epoch 270/300\n",
      "Average training loss: 0.007532179740154081\n",
      "Average test loss: 0.0025162229566938346\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007534787469026115\n",
      "Average test loss: 0.002494806950084037\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007521813469628493\n",
      "Average test loss: 0.0024108638674434687\n",
      "Epoch 273/300\n",
      "Average training loss: 0.007508779216971662\n",
      "Average test loss: 0.0024591659179164305\n",
      "Epoch 274/300\n",
      "Average training loss: 0.007518435113959842\n",
      "Average test loss: 0.002448436770381199\n",
      "Epoch 275/300\n",
      "Average training loss: 0.007516878670288457\n",
      "Average test loss: 0.002548098785388801\n",
      "Epoch 276/300\n",
      "Average training loss: 0.00750007922657662\n",
      "Average test loss: 0.002528403453528881\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007508058609647883\n",
      "Average test loss: 0.0024817649018433357\n",
      "Epoch 278/300\n",
      "Average training loss: 0.007501341669923729\n",
      "Average test loss: 0.0025094997317840657\n",
      "Epoch 279/300\n",
      "Average training loss: 0.007502328330443965\n",
      "Average test loss: 0.002480996647021837\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0074941837129493555\n",
      "Average test loss: 0.00247365419452803\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00747949287129773\n",
      "Average test loss: 0.0024466883254547914\n",
      "Epoch 282/300\n",
      "Average training loss: 0.007479491080260939\n",
      "Average test loss: 0.0025616092533907956\n",
      "Epoch 283/300\n",
      "Average training loss: 0.007488545820944839\n",
      "Average test loss: 0.002570943855990966\n",
      "Epoch 284/300\n",
      "Average training loss: 0.007492427442636755\n",
      "Average test loss: 0.002456925828009844\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0074786670245230195\n",
      "Average test loss: 0.0025889467948840726\n",
      "Epoch 286/300\n",
      "Average training loss: 0.007480565193626615\n",
      "Average test loss: 0.002611641289252374\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0074843147860633\n",
      "Average test loss: 0.0024062092700559232\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007448558249407345\n",
      "Average test loss: 0.002428701003702978\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0074483922173579535\n",
      "Average test loss: 0.0024878865447309283\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007451709801124202\n",
      "Average test loss: 0.0025872440738603474\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007462806259178453\n",
      "Average test loss: 0.002464606185340219\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0074460822211371526\n",
      "Average test loss: 0.0024907913849585585\n",
      "Epoch 293/300\n",
      "Average training loss: 0.007449338605834378\n",
      "Average test loss: 0.002454737696589695\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0074490138826270895\n",
      "Average test loss: 0.002692659757617447\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007439730393389861\n",
      "Average test loss: 0.0024976763955007\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007430259803103076\n",
      "Average test loss: 0.002535288001307183\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007430232687128915\n",
      "Average test loss: 0.002476768637282981\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00744466751979457\n",
      "Average test loss: 0.0026123188010727365\n",
      "Epoch 299/300\n",
      "Average training loss: 0.007436652050250106\n",
      "Average test loss: 0.0025031669032242565\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007424697385893928\n",
      "Average test loss: 0.0025168184600770473\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.25236489168802895\n",
      "Average test loss: 0.004490025280250443\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0682597586678134\n",
      "Average test loss: 0.0037730237276603776\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03699929915534125\n",
      "Average test loss: 0.003114796191453934\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02622777561015553\n",
      "Average test loss: 0.0029176026564091445\n",
      "Epoch 5/300\n",
      "Average training loss: 0.021537901389929982\n",
      "Average test loss: 0.002770157357470857\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018883215597934194\n",
      "Average test loss: 0.0026404493171721696\n",
      "Epoch 7/300\n",
      "Average training loss: 0.017258844289514755\n",
      "Average test loss: 0.00258770183670438\n",
      "Epoch 8/300\n",
      "Average training loss: 0.016158340038524732\n",
      "Average test loss: 0.0024299435298889877\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015381926346156332\n",
      "Average test loss: 0.0022982791802949377\n",
      "Epoch 10/300\n",
      "Average training loss: 0.014801234349608422\n",
      "Average test loss: 0.0022220856220357948\n",
      "Epoch 11/300\n",
      "Average training loss: 0.014339661858148046\n",
      "Average test loss: 0.002230982540382279\n",
      "Epoch 12/300\n",
      "Average training loss: 0.013977899149888092\n",
      "Average test loss: 0.0021076359136237037\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013662743701703018\n",
      "Average test loss: 0.002045805073550178\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013396659761667252\n",
      "Average test loss: 0.002015315061228143\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013146532574461566\n",
      "Average test loss: 0.0019691961299007138\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01292076306624545\n",
      "Average test loss: 0.002132653579633269\n",
      "Epoch 17/300\n",
      "Average training loss: 0.012698446615702576\n",
      "Average test loss: 0.0018881596912526423\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012493411011166043\n",
      "Average test loss: 0.0018441180497821834\n",
      "Epoch 19/300\n",
      "Average training loss: 0.012301255664063825\n",
      "Average test loss: 0.0017922755303896136\n",
      "Epoch 20/300\n",
      "Average training loss: 0.012105958470039898\n",
      "Average test loss: 0.0018261960244013204\n",
      "Epoch 21/300\n",
      "Average training loss: 0.011902566408945454\n",
      "Average test loss: 0.0017625566085593567\n",
      "Epoch 22/300\n",
      "Average training loss: 0.011721154783334997\n",
      "Average test loss: 0.0017249105979378024\n",
      "Epoch 23/300\n",
      "Average training loss: 0.011538957841694356\n",
      "Average test loss: 0.001732558216382232\n",
      "Epoch 24/300\n",
      "Average training loss: 0.011346211290193928\n",
      "Average test loss: 0.0017594594226943122\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01116141069183747\n",
      "Average test loss: 0.0016726535958134466\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010969507927281989\n",
      "Average test loss: 0.0016473470133625799\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010781248537202677\n",
      "Average test loss: 0.0019143816116783353\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010593218963593245\n",
      "Average test loss: 0.0017349912614251176\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01041494177116288\n",
      "Average test loss: 0.0016322772910611497\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010242258015606139\n",
      "Average test loss: 0.0015863250348096092\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01006641056223048\n",
      "Average test loss: 0.0015708667894618379\n",
      "Epoch 32/300\n",
      "Average training loss: 0.009922213964164256\n",
      "Average test loss: 0.0015554114589467645\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009764283518410391\n",
      "Average test loss: 0.0017250884451592962\n",
      "Epoch 34/300\n",
      "Average training loss: 0.009602301330202156\n",
      "Average test loss: 0.0015521336960502797\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009445951853361395\n",
      "Average test loss: 0.0015511472300729818\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009312232142521276\n",
      "Average test loss: 0.001598690627556708\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009199709217995405\n",
      "Average test loss: 0.0015492181920756895\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00908211441958944\n",
      "Average test loss: 0.0016011956028847232\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008982240193006065\n",
      "Average test loss: 0.0015536221763533023\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0088678289030989\n",
      "Average test loss: 0.001536710695674022\n",
      "Epoch 41/300\n",
      "Average training loss: 0.008784219757550293\n",
      "Average test loss: 0.0016744716909403601\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008683370038039154\n",
      "Average test loss: 0.0015156919250471725\n",
      "Epoch 43/300\n",
      "Average training loss: 0.008584102933605512\n",
      "Average test loss: 0.0015066205654293298\n",
      "Epoch 44/300\n",
      "Average training loss: 0.00850828179385927\n",
      "Average test loss: 0.0014982171696093346\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00844354500207636\n",
      "Average test loss: 0.0015004308598323\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00836372680299812\n",
      "Average test loss: 0.001490988665053414\n",
      "Epoch 47/300\n",
      "Average training loss: 0.008294708920021852\n",
      "Average test loss: 0.0015622374321230584\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008220836766064167\n",
      "Average test loss: 0.0016880074088565177\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008168032155682643\n",
      "Average test loss: 0.001550559837785032\n",
      "Epoch 50/300\n",
      "Average training loss: 0.008106464030841986\n",
      "Average test loss: 0.001497057756409049\n",
      "Epoch 51/300\n",
      "Average training loss: 0.008037505714222789\n",
      "Average test loss: 0.0015045139689205422\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007982016945464743\n",
      "Average test loss: 0.0014865085550376939\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007945137190736003\n",
      "Average test loss: 0.001566701256359617\n",
      "Epoch 54/300\n",
      "Average training loss: 0.007878049488282865\n",
      "Average test loss: 0.0015097510414198041\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007821425184193584\n",
      "Average test loss: 0.0015094207746701109\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007775555335813099\n",
      "Average test loss: 0.0014766294912745555\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0077159424126148225\n",
      "Average test loss: 0.001462978915207916\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007656478787047996\n",
      "Average test loss: 0.001482330829008586\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007632997912251287\n",
      "Average test loss: 0.0014889392907627755\n",
      "Epoch 60/300\n",
      "Average training loss: 0.007571562359730403\n",
      "Average test loss: 0.0015277658456729517\n",
      "Epoch 61/300\n",
      "Average training loss: 0.007545841803981199\n",
      "Average test loss: 0.001489776611742046\n",
      "Epoch 62/300\n",
      "Average training loss: 0.007488075014617708\n",
      "Average test loss: 0.0015142939436352915\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0074616245358354516\n",
      "Average test loss: 0.001522483029920194\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0074069010292490325\n",
      "Average test loss: 0.0016761207099383077\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007376502570592695\n",
      "Average test loss: 0.001528931831734048\n",
      "Epoch 66/300\n",
      "Average training loss: 0.007321477052652173\n",
      "Average test loss: 0.0015060746130636997\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0072922587742408115\n",
      "Average test loss: 0.0015010228644435604\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007254923937221368\n",
      "Average test loss: 0.0014891756617774565\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007225087159623702\n",
      "Average test loss: 0.0015346982603271803\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007197893963919746\n",
      "Average test loss: 0.0015567883022853898\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0071626258542140326\n",
      "Average test loss: 0.0015157907553431061\n",
      "Epoch 72/300\n",
      "Average training loss: 0.007112486590113905\n",
      "Average test loss: 0.001564352704005109\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007083011068403721\n",
      "Average test loss: 0.001517981187078274\n",
      "Epoch 74/300\n",
      "Average training loss: 0.007064098811811871\n",
      "Average test loss: 0.0015638512546817462\n",
      "Epoch 75/300\n",
      "Average training loss: 0.007027104736616214\n",
      "Average test loss: 0.0015459699605902036\n",
      "Epoch 76/300\n",
      "Average training loss: 0.006996971231367853\n",
      "Average test loss: 0.0015185594811207717\n",
      "Epoch 77/300\n",
      "Average training loss: 0.006960065397123496\n",
      "Average test loss: 0.001525045798263616\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006943278886791733\n",
      "Average test loss: 0.001607368055006696\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006913805962022808\n",
      "Average test loss: 0.001585540651033322\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006884522993531492\n",
      "Average test loss: 0.001558676541162034\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00689395864142312\n",
      "Average test loss: 0.001552695345806165\n",
      "Epoch 82/300\n",
      "Average training loss: 0.006847018608616458\n",
      "Average test loss: 0.0016325048876719343\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006811772831198242\n",
      "Average test loss: 0.001630175315671497\n",
      "Epoch 84/300\n",
      "Average training loss: 0.00679016145484315\n",
      "Average test loss: 0.0016130960246341097\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006788224549343189\n",
      "Average test loss: 0.0016045067666305437\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006772444676607847\n",
      "Average test loss: 0.001534641937352717\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006723293114867475\n",
      "Average test loss: 0.0015431257300078868\n",
      "Epoch 88/300\n",
      "Average training loss: 0.006700943370660146\n",
      "Average test loss: 0.0015384559352985686\n",
      "Epoch 89/300\n",
      "Average training loss: 0.006699273528738154\n",
      "Average test loss: 0.0016324029660059346\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006667442417807049\n",
      "Average test loss: 0.0015731342558024657\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006672626653065284\n",
      "Average test loss: 0.0015368701197827855\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006643695794045925\n",
      "Average test loss: 0.001575776744323472\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00662276726298862\n",
      "Average test loss: 0.001561941206558711\n",
      "Epoch 94/300\n",
      "Average training loss: 0.006619780403665371\n",
      "Average test loss: 0.0016025048815127875\n",
      "Epoch 95/300\n",
      "Average training loss: 0.006566691832823886\n",
      "Average test loss: 0.001585897539017929\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006564403510755963\n",
      "Average test loss: 0.001556736081942088\n",
      "Epoch 97/300\n",
      "Average training loss: 0.006559508451157146\n",
      "Average test loss: 0.0015891878031608132\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006528225026610825\n",
      "Average test loss: 0.0016203594642380872\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006525956392702129\n",
      "Average test loss: 0.00166805763139079\n",
      "Epoch 100/300\n",
      "Average training loss: 0.006493317378477918\n",
      "Average test loss: 0.001666031164738039\n",
      "Epoch 101/300\n",
      "Average training loss: 0.006492348824524217\n",
      "Average test loss: 0.0018339764455126391\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006480828111784326\n",
      "Average test loss: 0.0015983567801821563\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0064580126098460625\n",
      "Average test loss: 0.0015921820644806657\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006450390875339508\n",
      "Average test loss: 0.001768042273612486\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006428480017516348\n",
      "Average test loss: 0.0016664052793963087\n",
      "Epoch 106/300\n",
      "Average training loss: 0.006417834134565459\n",
      "Average test loss: 0.0016061611177606715\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006409914053976536\n",
      "Average test loss: 0.0018045709415649375\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006399848640792899\n",
      "Average test loss: 0.0016097931721661654\n",
      "Epoch 109/300\n",
      "Average training loss: 0.006388053081101842\n",
      "Average test loss: 0.0016496057015740208\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006371779994832145\n",
      "Average test loss: 0.0016376788220885726\n",
      "Epoch 111/300\n",
      "Average training loss: 0.006351975758456522\n",
      "Average test loss: 0.0015695475056353542\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0063497208476894435\n",
      "Average test loss: 0.001617736941151735\n",
      "Epoch 113/300\n",
      "Average training loss: 0.006324695973760552\n",
      "Average test loss: 0.0015873834925393263\n",
      "Epoch 114/300\n",
      "Average training loss: 0.006317212603986263\n",
      "Average test loss: 0.0016716891929714217\n",
      "Epoch 115/300\n",
      "Average training loss: 0.006305114221241739\n",
      "Average test loss: 0.001620721782040265\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006303739678114653\n",
      "Average test loss: 0.001654940599368678\n",
      "Epoch 117/300\n",
      "Average training loss: 0.006276071924302313\n",
      "Average test loss: 0.0015989224224661788\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006289673378898038\n",
      "Average test loss: 0.0016498799282643531\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0062663700862063305\n",
      "Average test loss: 0.001579963249154389\n",
      "Epoch 120/300\n",
      "Average training loss: 0.006267034576170974\n",
      "Average test loss: 0.0017067675046208831\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006257072005007002\n",
      "Average test loss: 0.0016281490243143506\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006241076508329974\n",
      "Average test loss: 0.0016284297317162985\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006225158417390453\n",
      "Average test loss: 0.0016759232487529516\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006227407859431372\n",
      "Average test loss: 0.001661108388668961\n",
      "Epoch 125/300\n",
      "Average training loss: 0.006206777939779891\n",
      "Average test loss: 0.002665505209006369\n",
      "Epoch 126/300\n",
      "Average training loss: 0.006210323100288709\n",
      "Average test loss: 0.0016593372080889012\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006185882536073526\n",
      "Average test loss: 0.0017071329123444028\n",
      "Epoch 128/300\n",
      "Average training loss: 0.006178643278363679\n",
      "Average test loss: 0.0016203598163815008\n",
      "Epoch 129/300\n",
      "Average training loss: 0.006164529597179757\n",
      "Average test loss: 0.0016650478641192117\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006162040941417217\n",
      "Average test loss: 0.0016747836504752438\n",
      "Epoch 131/300\n",
      "Average training loss: 0.006164802011102438\n",
      "Average test loss: 0.0016531902529920142\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006149848909427723\n",
      "Average test loss: 0.0016543027482305965\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006134438461313645\n",
      "Average test loss: 0.0018011176340902845\n",
      "Epoch 134/300\n",
      "Average training loss: 0.00612039079475734\n",
      "Average test loss: 0.0015999986545907126\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006121092987143331\n",
      "Average test loss: 0.0016280963136297132\n",
      "Epoch 136/300\n",
      "Average training loss: 0.006111859237982167\n",
      "Average test loss: 0.001751071316914426\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006114788702792592\n",
      "Average test loss: 0.001717520031819327\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006099385842680931\n",
      "Average test loss: 0.0016349145761794515\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0060831303033563825\n",
      "Average test loss: 0.0016762487050145864\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006075761488742298\n",
      "Average test loss: 0.001699647307395935\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006067418462493353\n",
      "Average test loss: 0.001710692077771657\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006066502306610346\n",
      "Average test loss: 0.0018042202553608352\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006076220239616102\n",
      "Average test loss: 0.0016439351731807822\n",
      "Epoch 144/300\n",
      "Average training loss: 0.006065464107112752\n",
      "Average test loss: 0.001711154211829934\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006033871411035458\n",
      "Average test loss: 0.0016993286772113707\n",
      "Epoch 146/300\n",
      "Average training loss: 0.00602041611696283\n",
      "Average test loss: 0.0017135137917680874\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00603146978136566\n",
      "Average test loss: 0.0016880741411199173\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0060279705164333185\n",
      "Average test loss: 0.0016792710083019403\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006016352730906672\n",
      "Average test loss: 0.0016283125698359476\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006017328541725874\n",
      "Average test loss: 0.0016673668097290728\n",
      "Epoch 151/300\n",
      "Average training loss: 0.005999870549887419\n",
      "Average test loss: 0.0017204671104231641\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0060006126422021125\n",
      "Average test loss: 0.0016324889092809625\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006000589237444931\n",
      "Average test loss: 0.0017444897050865823\n",
      "Epoch 154/300\n",
      "Average training loss: 0.005990971745716201\n",
      "Average test loss: 0.001715258782936467\n",
      "Epoch 155/300\n",
      "Average training loss: 0.005988727611800035\n",
      "Average test loss: 0.0016436987111551895\n",
      "Epoch 156/300\n",
      "Average training loss: 0.005968639009942611\n",
      "Average test loss: 0.0017303792642843392\n",
      "Epoch 157/300\n",
      "Average training loss: 0.005963602739903662\n",
      "Average test loss: 0.001653214882645342\n",
      "Epoch 158/300\n",
      "Average training loss: 0.005972020896772544\n",
      "Average test loss: 0.0016317118607047532\n",
      "Epoch 159/300\n",
      "Average training loss: 0.005957777506775327\n",
      "Average test loss: 0.0016805224193053112\n",
      "Epoch 160/300\n",
      "Average training loss: 0.005953616598000129\n",
      "Average test loss: 0.0016999600461373727\n",
      "Epoch 161/300\n",
      "Average training loss: 0.005954646687540743\n",
      "Average test loss: 0.0017135503881921372\n",
      "Epoch 162/300\n",
      "Average training loss: 0.005924132164153788\n",
      "Average test loss: 0.0017371287710136838\n",
      "Epoch 163/300\n",
      "Average training loss: 0.005935020002639956\n",
      "Average test loss: 0.0017511720188065536\n",
      "Epoch 164/300\n",
      "Average training loss: 0.005924229848302073\n",
      "Average test loss: 0.0019041240165630976\n",
      "Epoch 165/300\n",
      "Average training loss: 0.005922815218153927\n",
      "Average test loss: 0.0016773416553106572\n",
      "Epoch 166/300\n",
      "Average training loss: 0.005924712862405512\n",
      "Average test loss: 0.0016716178963995642\n",
      "Epoch 167/300\n",
      "Average training loss: 0.005915685727364487\n",
      "Average test loss: 0.0017980221009088887\n",
      "Epoch 168/300\n",
      "Average training loss: 0.005914851305799352\n",
      "Average test loss: 0.0017002737088542845\n",
      "Epoch 169/300\n",
      "Average training loss: 0.005909874624676174\n",
      "Average test loss: 0.0017850520927459002\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0058919739313423635\n",
      "Average test loss: 0.0017002993426803086\n",
      "Epoch 171/300\n",
      "Average training loss: 0.005885610760500034\n",
      "Average test loss: 0.0016419235059163637\n",
      "Epoch 172/300\n",
      "Average training loss: 0.005883090623137023\n",
      "Average test loss: 0.0017613119819822412\n",
      "Epoch 173/300\n",
      "Average training loss: 0.005871958513640695\n",
      "Average test loss: 0.0017172513673495914\n",
      "Epoch 174/300\n",
      "Average training loss: 0.005873482727756103\n",
      "Average test loss: 0.00173871764426844\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0058777235938857\n",
      "Average test loss: 0.0018943567623694738\n",
      "Epoch 176/300\n",
      "Average training loss: 0.005875938515696261\n",
      "Average test loss: 0.001739447542362743\n",
      "Epoch 177/300\n",
      "Average training loss: 0.005852148003876209\n",
      "Average test loss: 0.001657158096217447\n",
      "Epoch 178/300\n",
      "Average training loss: 0.005852215411762397\n",
      "Average test loss: 0.0016886556369976864\n",
      "Epoch 179/300\n",
      "Average training loss: 0.005850348865820302\n",
      "Average test loss: 0.0018707880636470186\n",
      "Epoch 180/300\n",
      "Average training loss: 0.005841766888896624\n",
      "Average test loss: 0.0017026234567165375\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0058576383122967346\n",
      "Average test loss: 0.001814380885515776\n",
      "Epoch 182/300\n",
      "Average training loss: 0.005827783405780792\n",
      "Average test loss: 0.0017154717793067297\n",
      "Epoch 183/300\n",
      "Average training loss: 0.005835000180535847\n",
      "Average test loss: 0.0017461430448003942\n",
      "Epoch 184/300\n",
      "Average training loss: 0.005833757379816638\n",
      "Average test loss: 0.0017411189809855487\n",
      "Epoch 185/300\n",
      "Average training loss: 0.005821755530933539\n",
      "Average test loss: 0.0017239459425003993\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0058190855818490185\n",
      "Average test loss: 0.0017442834695490699\n",
      "Epoch 187/300\n",
      "Average training loss: 0.005818716445316871\n",
      "Average test loss: 0.001689421758780049\n",
      "Epoch 188/300\n",
      "Average training loss: 0.005827846873137686\n",
      "Average test loss: 0.0017639822736899885\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0058098997138440606\n",
      "Average test loss: 0.0017180292037212185\n",
      "Epoch 190/300\n",
      "Average training loss: 0.005788240158723461\n",
      "Average test loss: 0.0016720246675734718\n",
      "Epoch 191/300\n",
      "Average training loss: 0.005815436941054132\n",
      "Average test loss: 0.0017171418691674868\n",
      "Epoch 192/300\n",
      "Average training loss: 0.005785232743455304\n",
      "Average test loss: 0.001751695467159152\n",
      "Epoch 193/300\n",
      "Average training loss: 0.005773074554900328\n",
      "Average test loss: 0.0017594594402859608\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0057950874856776665\n",
      "Average test loss: 0.0018348955290599\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005784067785160409\n",
      "Average test loss: 0.001716698545962572\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005780601094580359\n",
      "Average test loss: 0.0017376124314549897\n",
      "Epoch 197/300\n",
      "Average training loss: 0.005782530212981834\n",
      "Average test loss: 0.0017195726881424585\n",
      "Epoch 198/300\n",
      "Average training loss: 0.005763048770113124\n",
      "Average test loss: 0.0017104714217906197\n",
      "Epoch 199/300\n",
      "Average training loss: 0.005751492285894023\n",
      "Average test loss: 0.0017955968641779488\n",
      "Epoch 200/300\n",
      "Average training loss: 0.005758122893671194\n",
      "Average test loss: 0.0017138076013781959\n",
      "Epoch 201/300\n",
      "Average training loss: 0.005760216891765595\n",
      "Average test loss: 0.001667196205092801\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0057619651647077665\n",
      "Average test loss: 0.0016969489827752112\n",
      "Epoch 203/300\n",
      "Average training loss: 0.005766058667666382\n",
      "Average test loss: 0.0016908523750801881\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005742129844509893\n",
      "Average test loss: 0.0017211583972805075\n",
      "Epoch 205/300\n",
      "Average training loss: 0.005738343988027837\n",
      "Average test loss: 0.0017836982808593247\n",
      "Epoch 206/300\n",
      "Average training loss: 0.005735262162983417\n",
      "Average test loss: 0.0019048060356742806\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00574462018865678\n",
      "Average test loss: 0.0017484978857553667\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0057245466580821405\n",
      "Average test loss: 0.0017297213557693693\n",
      "Epoch 209/300\n",
      "Average training loss: 0.005727150194346905\n",
      "Average test loss: 0.0016742522289148636\n",
      "Epoch 210/300\n",
      "Average training loss: 0.005733713693916798\n",
      "Average test loss: 0.00175808590479816\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0057123773151801695\n",
      "Average test loss: 0.0018148136068549421\n",
      "Epoch 212/300\n",
      "Average training loss: 0.005721391480829981\n",
      "Average test loss: 0.0017800081671319075\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0057000721775823165\n",
      "Average test loss: 0.0017706910411102903\n",
      "Epoch 214/300\n",
      "Average training loss: 0.005714236958987183\n",
      "Average test loss: 0.001729434777982533\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00571876479850875\n",
      "Average test loss: 0.001753833847741286\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005700225363175074\n",
      "Average test loss: 0.0017786006869541275\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005704748016885585\n",
      "Average test loss: 0.0018539419832328955\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0056989107525183096\n",
      "Average test loss: 0.001719392828643322\n",
      "Epoch 219/300\n",
      "Average training loss: 0.005697516597393486\n",
      "Average test loss: 0.0016920064745677842\n",
      "Epoch 220/300\n",
      "Average training loss: 0.005678508756475316\n",
      "Average test loss: 0.001756748926722341\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0056835701821578875\n",
      "Average test loss: 0.0017126565618026588\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0056875457097258835\n",
      "Average test loss: 0.0017464426575849454\n",
      "Epoch 223/300\n",
      "Average training loss: 0.005664414416170782\n",
      "Average test loss: 0.001781251859644221\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0056805780509279835\n",
      "Average test loss: 0.0017226809025224711\n",
      "Epoch 225/300\n",
      "Average training loss: 0.005665891585664617\n",
      "Average test loss: 0.0018100191518250438\n",
      "Epoch 226/300\n",
      "Average training loss: 0.005666126218520933\n",
      "Average test loss: 0.0016641840081041057\n",
      "Epoch 227/300\n",
      "Average training loss: 0.005673103541135788\n",
      "Average test loss: 0.0018625552049941487\n",
      "Epoch 228/300\n",
      "Average training loss: 0.005665340762999323\n",
      "Average test loss: 0.0016988739785220887\n",
      "Epoch 229/300\n",
      "Average training loss: 0.005653009893993537\n",
      "Average test loss: 0.0018322078939527274\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00565015384554863\n",
      "Average test loss: 0.0017131317208210627\n",
      "Epoch 231/300\n",
      "Average training loss: 0.005642245049277942\n",
      "Average test loss: 0.0017680238759559062\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0056448075001438456\n",
      "Average test loss: 0.0017059208437179526\n",
      "Epoch 233/300\n",
      "Average training loss: 0.005651477719346682\n",
      "Average test loss: 0.00184780751215294\n",
      "Epoch 234/300\n",
      "Average training loss: 0.005641228274545736\n",
      "Average test loss: 0.001779470954193837\n",
      "Epoch 235/300\n",
      "Average training loss: 0.005645022796259986\n",
      "Average test loss: 0.0018166226340043876\n",
      "Epoch 236/300\n",
      "Average training loss: 0.005635565550790892\n",
      "Average test loss: 0.0018101986866030427\n",
      "Epoch 237/300\n",
      "Average training loss: 0.005651760942406125\n",
      "Average test loss: 0.001735665584810906\n",
      "Epoch 238/300\n",
      "Average training loss: 0.005626930076215003\n",
      "Average test loss: 0.0017447518672173222\n",
      "Epoch 239/300\n",
      "Average training loss: 0.005638461399409506\n",
      "Average test loss: 0.0018118211310356855\n",
      "Epoch 240/300\n",
      "Average training loss: 0.005644671646257242\n",
      "Average test loss: 0.00174663896072242\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0056191257565385766\n",
      "Average test loss: 0.0019598319330252707\n",
      "Epoch 242/300\n",
      "Average training loss: 0.005619995380027426\n",
      "Average test loss: 0.0017400993964531355\n",
      "Epoch 243/300\n",
      "Average training loss: 0.005606997571471665\n",
      "Average test loss: 0.0017375259863005744\n",
      "Epoch 244/300\n",
      "Average training loss: 0.005618051815778017\n",
      "Average test loss: 0.0017498055029039582\n",
      "Epoch 245/300\n",
      "Average training loss: 0.005602533825155761\n",
      "Average test loss: 0.0018277090314982665\n",
      "Epoch 246/300\n",
      "Average training loss: 0.005608721988482608\n",
      "Average test loss: 0.0016805699802935124\n",
      "Epoch 247/300\n",
      "Average training loss: 0.005616334233433008\n",
      "Average test loss: 0.0017365878795584043\n",
      "Epoch 248/300\n",
      "Average training loss: 0.005602933499962092\n",
      "Average test loss: 0.001778629685855574\n",
      "Epoch 249/300\n",
      "Average training loss: 0.005597820189678007\n",
      "Average test loss: 0.0017303221754522786\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005582516088667843\n",
      "Average test loss: 0.0018085930450922913\n",
      "Epoch 251/300\n",
      "Average training loss: 0.00559006510509385\n",
      "Average test loss: 0.0019557791503353253\n",
      "Epoch 252/300\n",
      "Average training loss: 0.005589824743568898\n",
      "Average test loss: 0.001784920822104646\n",
      "Epoch 253/300\n",
      "Average training loss: 0.005587248052987787\n",
      "Average test loss: 0.001729933417816129\n",
      "Epoch 254/300\n",
      "Average training loss: 0.005596099947475725\n",
      "Average test loss: 0.001874115759972483\n",
      "Epoch 255/300\n",
      "Average training loss: 0.005579207440631257\n",
      "Average test loss: 0.0017701070315928923\n",
      "Epoch 256/300\n",
      "Average training loss: 0.005582025931941138\n",
      "Average test loss: 0.0017889562516162792\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0055741163901984695\n",
      "Average test loss: 0.0017464621118787262\n",
      "Epoch 258/300\n",
      "Average training loss: 0.005578378226608038\n",
      "Average test loss: 0.0016704616798087954\n",
      "Epoch 259/300\n",
      "Average training loss: 0.005583660599672132\n",
      "Average test loss: 0.001755942513441874\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0055588594323231115\n",
      "Average test loss: 0.001793640624317858\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0055680594055189025\n",
      "Average test loss: 0.001680258713869585\n",
      "Epoch 262/300\n",
      "Average training loss: 0.005565081497447358\n",
      "Average test loss: 0.0017816448298593362\n",
      "Epoch 263/300\n",
      "Average training loss: 0.005563634144763152\n",
      "Average test loss: 0.0017682309640157554\n",
      "Epoch 264/300\n",
      "Average training loss: 0.005573326333529419\n",
      "Average test loss: 0.001792365745641291\n",
      "Epoch 265/300\n",
      "Average training loss: 0.00555243958450026\n",
      "Average test loss: 0.0018669453948322269\n",
      "Epoch 266/300\n",
      "Average training loss: 0.005553341222720014\n",
      "Average test loss: 0.0017426114910178714\n",
      "Epoch 267/300\n",
      "Average training loss: 0.00555313175626927\n",
      "Average test loss: 0.00173242907350262\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0055541192243496575\n",
      "Average test loss: 0.0017872231159773138\n",
      "Epoch 269/300\n",
      "Average training loss: 0.00554874278149671\n",
      "Average test loss: 0.0017014292733122905\n",
      "Epoch 270/300\n",
      "Average training loss: 0.005546720066004329\n",
      "Average test loss: 0.001764547523525026\n",
      "Epoch 271/300\n",
      "Average training loss: 0.005551602212091287\n",
      "Average test loss: 0.0018318252483796743\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005539962717228466\n",
      "Average test loss: 0.0017399406271676222\n",
      "Epoch 273/300\n",
      "Average training loss: 0.005528321635391977\n",
      "Average test loss: 0.0017514779547022448\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005541645534336567\n",
      "Average test loss: 0.0017723323781457213\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005539622856097089\n",
      "Average test loss: 0.0017129383722527159\n",
      "Epoch 276/300\n",
      "Average training loss: 0.005534550838172436\n",
      "Average test loss: 0.0018183957930240366\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005523953980041875\n",
      "Average test loss: 0.0018102005250338052\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0055271002592311965\n",
      "Average test loss: 0.0018146312543087535\n",
      "Epoch 279/300\n",
      "Average training loss: 0.005522798547314273\n",
      "Average test loss: 0.0017984109531260198\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005522640039937364\n",
      "Average test loss: 0.0017756052693972985\n",
      "Epoch 281/300\n",
      "Average training loss: 0.005515718298653762\n",
      "Average test loss: 0.001810193014331162\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005516616514159574\n",
      "Average test loss: 0.0019033743611847362\n",
      "Epoch 283/300\n",
      "Average training loss: 0.005514378284414609\n",
      "Average test loss: 0.001806385600939393\n",
      "Epoch 284/300\n",
      "Average training loss: 0.005517258179270559\n",
      "Average test loss: 0.0017927616940190394\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0055176772516634725\n",
      "Average test loss: 0.001826227166586452\n",
      "Epoch 286/300\n",
      "Average training loss: 0.005513991546713643\n",
      "Average test loss: 0.001744540634772016\n",
      "Epoch 287/300\n",
      "Average training loss: 0.005501953078226911\n",
      "Average test loss: 0.0017319190119807091\n",
      "Epoch 288/300\n",
      "Average training loss: 0.005509175553503964\n",
      "Average test loss: 0.0017390994981138243\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0054924004355238545\n",
      "Average test loss: 0.0017703761528763506\n",
      "Epoch 290/300\n",
      "Average training loss: 0.005505354226463371\n",
      "Average test loss: 0.0017882549966581993\n",
      "Epoch 291/300\n",
      "Average training loss: 0.005492539705087741\n",
      "Average test loss: 0.0017997405504186948\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0054880895734661156\n",
      "Average test loss: 0.0018048271271917556\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005500585478213098\n",
      "Average test loss: 0.001819722618493769\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00548860534073578\n",
      "Average test loss: 0.0018126515876501798\n",
      "Epoch 295/300\n",
      "Average training loss: 0.005485224378605684\n",
      "Average test loss: 0.00180249683962514\n",
      "Epoch 296/300\n",
      "Average training loss: 0.005489570414854421\n",
      "Average test loss: 0.0018375089064033496\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005485860654463371\n",
      "Average test loss: 0.0017217277186508807\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005473705172538757\n",
      "Average test loss: 0.0017689768055246936\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0054792189771930375\n",
      "Average test loss: 0.0018044901680615213\n",
      "Epoch 300/300\n",
      "Average training loss: 0.005477390609267685\n",
      "Average test loss: 0.0017995719164609909\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_50_Depth15/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.64\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.76\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.20\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.342203908284505\n",
      "Average test loss: 0.010165562235646778\n",
      "Epoch 2/300\n",
      "Average training loss: 3.618640337626139\n",
      "Average test loss: 0.005689266057478057\n",
      "Epoch 3/300\n",
      "Average training loss: 3.1229511335160995\n",
      "Average test loss: 0.004919944825271766\n",
      "Epoch 4/300\n",
      "Average training loss: 2.806181317223443\n",
      "Average test loss: 0.004698645453486178\n",
      "Epoch 5/300\n",
      "Average training loss: 2.5716672479841445\n",
      "Average test loss: 0.005236513333602084\n",
      "Epoch 6/300\n",
      "Average training loss: 2.3927683192359077\n",
      "Average test loss: 0.004444022496955262\n",
      "Epoch 7/300\n",
      "Average training loss: 2.24007226202223\n",
      "Average test loss: 0.0043875357885327605\n",
      "Epoch 8/300\n",
      "Average training loss: 2.1083439441257052\n",
      "Average test loss: 0.004360430430206987\n",
      "Epoch 9/300\n",
      "Average training loss: 1.9840836324691773\n",
      "Average test loss: 0.0043595412377682\n",
      "Epoch 10/300\n",
      "Average training loss: 1.8729809319178263\n",
      "Average test loss: 0.004272130894992086\n",
      "Epoch 11/300\n",
      "Average training loss: 1.7705748829311794\n",
      "Average test loss: 0.004226532450152768\n",
      "Epoch 12/300\n",
      "Average training loss: 1.6724814416037665\n",
      "Average test loss: 0.0042217118597278995\n",
      "Epoch 13/300\n",
      "Average training loss: 1.5839134163326687\n",
      "Average test loss: 0.004204925150507026\n",
      "Epoch 14/300\n",
      "Average training loss: 1.4990904748704699\n",
      "Average test loss: 0.004148733033074273\n",
      "Epoch 15/300\n",
      "Average training loss: 1.4228721570968628\n",
      "Average test loss: 0.0041463762890133595\n",
      "Epoch 16/300\n",
      "Average training loss: 1.3514823830922444\n",
      "Average test loss: 0.004138038399732775\n",
      "Epoch 17/300\n",
      "Average training loss: 1.2803022876315646\n",
      "Average test loss: 0.004107921343710687\n",
      "Epoch 18/300\n",
      "Average training loss: 1.2119081885019938\n",
      "Average test loss: 0.004074001683750087\n",
      "Epoch 19/300\n",
      "Average training loss: 1.1470805430942113\n",
      "Average test loss: 0.004082940972099701\n",
      "Epoch 20/300\n",
      "Average training loss: 1.0866373261345756\n",
      "Average test loss: 0.004060873959627416\n",
      "Epoch 21/300\n",
      "Average training loss: 1.0281626770231458\n",
      "Average test loss: 0.004060318942492207\n",
      "Epoch 22/300\n",
      "Average training loss: 0.9713615339597066\n",
      "Average test loss: 0.004032449818526705\n",
      "Epoch 23/300\n",
      "Average training loss: 0.9190307058228386\n",
      "Average test loss: 0.004006717583578494\n",
      "Epoch 24/300\n",
      "Average training loss: 0.8665030975341796\n",
      "Average test loss: 0.004000884749202265\n",
      "Epoch 25/300\n",
      "Average training loss: 0.8151713307168749\n",
      "Average test loss: 0.003992826602525181\n",
      "Epoch 26/300\n",
      "Average training loss: 0.7639365420871311\n",
      "Average test loss: 0.004190224324249559\n",
      "Epoch 27/300\n",
      "Average training loss: 0.7147534567515056\n",
      "Average test loss: 0.003985634181027611\n",
      "Epoch 28/300\n",
      "Average training loss: 0.667391907480028\n",
      "Average test loss: 0.003974962047404713\n",
      "Epoch 29/300\n",
      "Average training loss: 0.6211349330478244\n",
      "Average test loss: 0.0039443007626881205\n",
      "Epoch 30/300\n",
      "Average training loss: 0.5762968470785353\n",
      "Average test loss: 0.0039626456846793496\n",
      "Epoch 31/300\n",
      "Average training loss: 0.5327032754421234\n",
      "Average test loss: 0.003926952680779828\n",
      "Epoch 32/300\n",
      "Average training loss: 0.49102611751026576\n",
      "Average test loss: 0.003965748695760138\n",
      "Epoch 33/300\n",
      "Average training loss: 0.4501844473414951\n",
      "Average test loss: 0.0039434908616046115\n",
      "Epoch 34/300\n",
      "Average training loss: 0.4107808131906721\n",
      "Average test loss: 0.003906322013172838\n",
      "Epoch 35/300\n",
      "Average training loss: 0.37294232802920874\n",
      "Average test loss: 0.003925055462867022\n",
      "Epoch 36/300\n",
      "Average training loss: 0.338122196621365\n",
      "Average test loss: 0.0039388799669428\n",
      "Epoch 37/300\n",
      "Average training loss: 0.30546578754319087\n",
      "Average test loss: 0.003926757515097658\n",
      "Epoch 38/300\n",
      "Average training loss: 0.27531344363424515\n",
      "Average test loss: 0.003922103270888329\n",
      "Epoch 39/300\n",
      "Average training loss: 0.2478911269240909\n",
      "Average test loss: 0.003907173403021362\n",
      "Epoch 40/300\n",
      "Average training loss: 0.22268752075566187\n",
      "Average test loss: 0.003908413560854064\n",
      "Epoch 41/300\n",
      "Average training loss: 0.19971118381288316\n",
      "Average test loss: 0.0038891934475137127\n",
      "Epoch 42/300\n",
      "Average training loss: 0.17821969162093268\n",
      "Average test loss: 0.003934728188233243\n",
      "Epoch 43/300\n",
      "Average training loss: 0.15802170781294506\n",
      "Average test loss: 0.0039813847932964564\n",
      "Epoch 44/300\n",
      "Average training loss: 0.13918002835909526\n",
      "Average test loss: 0.003901066175558501\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12266636663013035\n",
      "Average test loss: 0.0038767307262039846\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10851827373107274\n",
      "Average test loss: 0.003908221580709021\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09729090687963697\n",
      "Average test loss: 0.0038837228669888445\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08821626343992021\n",
      "Average test loss: 0.0038822933153973686\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08116599114073647\n",
      "Average test loss: 0.0039054786389072734\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07586346772644255\n",
      "Average test loss: 0.0038807894225335784\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07189857230583827\n",
      "Average test loss: 0.003945138571576939\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06888551053073672\n",
      "Average test loss: 0.00388686676737335\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06666437711980608\n",
      "Average test loss: 0.003895669925957918\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06487785623470942\n",
      "Average test loss: 0.003936658498727613\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06349730056855414\n",
      "Average test loss: 0.0038858187635325723\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06250248879856533\n",
      "Average test loss: 0.003992859369557765\n",
      "Epoch 57/300\n",
      "Average training loss: 0.061683346596029066\n",
      "Average test loss: 0.003929301258590486\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06105223195751508\n",
      "Average test loss: 0.004088160093873739\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06040293754140536\n",
      "Average test loss: 0.003915360869011945\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05988493813739883\n",
      "Average test loss: 0.003939718556072977\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05950774551100201\n",
      "Average test loss: 0.003909913285738892\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05913828029897478\n",
      "Average test loss: 0.003888018921431568\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05891559811102019\n",
      "Average test loss: 0.0039866311268674\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05865248971184095\n",
      "Average test loss: 0.003919471988661422\n",
      "Epoch 65/300\n",
      "Average training loss: 0.058322058982319305\n",
      "Average test loss: 0.0039005250599649217\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05807389063636462\n",
      "Average test loss: 0.003925988830626011\n",
      "Epoch 67/300\n",
      "Average training loss: 0.057892475303676395\n",
      "Average test loss: 0.004037293185997341\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05765310576558113\n",
      "Average test loss: 0.003908693205565215\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05750040816598468\n",
      "Average test loss: 0.003953048957304822\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05719972208473417\n",
      "Average test loss: 0.003936280081255569\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05702066291040844\n",
      "Average test loss: 0.003945373248722818\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05679859283566475\n",
      "Average test loss: 0.003974079383744134\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05661657008197572\n",
      "Average test loss: 0.003945322180166841\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0564031243191825\n",
      "Average test loss: 0.003995784203625388\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05635553182827102\n",
      "Average test loss: 0.003961618942519029\n",
      "Epoch 76/300\n",
      "Average training loss: 0.056054904931121405\n",
      "Average test loss: 0.003961578630324867\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05579336854815483\n",
      "Average test loss: 0.003987605237298542\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05561336073610518\n",
      "Average test loss: 0.004143809221271012\n",
      "Epoch 79/300\n",
      "Average training loss: 0.055342830197678675\n",
      "Average test loss: 0.003976542090376218\n",
      "Epoch 80/300\n",
      "Average training loss: 0.055185444606675044\n",
      "Average test loss: 0.00396828943822119\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05503557524416182\n",
      "Average test loss: 0.003971762085747388\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05483295517166455\n",
      "Average test loss: 0.00393145796077119\n",
      "Epoch 83/300\n",
      "Average training loss: 0.054645897769265706\n",
      "Average test loss: 0.003961310068973237\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05444610843062401\n",
      "Average test loss: 0.003965959962043498\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05431629960073365\n",
      "Average test loss: 0.004135476183353199\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05417733174893591\n",
      "Average test loss: 0.00400848641867439\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05397781500882572\n",
      "Average test loss: 0.003980668159408702\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05381499601403872\n",
      "Average test loss: 0.004060093890668618\n",
      "Epoch 89/300\n",
      "Average training loss: 0.053686106652021406\n",
      "Average test loss: 0.0040272621864246\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05345338691936599\n",
      "Average test loss: 0.004041163584010469\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05339890437987116\n",
      "Average test loss: 0.0041321170387996565\n",
      "Epoch 92/300\n",
      "Average training loss: 0.053244196626875136\n",
      "Average test loss: 0.003988708866967095\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0530835436615679\n",
      "Average test loss: 0.004075909308675262\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05288155718313323\n",
      "Average test loss: 0.004095750541322761\n",
      "Epoch 95/300\n",
      "Average training loss: 0.052704592284229065\n",
      "Average test loss: 0.004075950139305658\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05258977637026045\n",
      "Average test loss: 0.00414164663809869\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05242304567495982\n",
      "Average test loss: 0.004013194382604625\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05227318669690026\n",
      "Average test loss: 0.004067189427299632\n",
      "Epoch 99/300\n",
      "Average training loss: 0.052232665659652816\n",
      "Average test loss: 0.004036291909093658\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05198196561137835\n",
      "Average test loss: 0.0040069218650460245\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05176991415023804\n",
      "Average test loss: 0.004179526605953773\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05161125365893046\n",
      "Average test loss: 0.004073065523886019\n",
      "Epoch 103/300\n",
      "Average training loss: 0.051496599187453584\n",
      "Average test loss: 0.004021148475714856\n",
      "Epoch 104/300\n",
      "Average training loss: 0.051409711052974066\n",
      "Average test loss: 0.004012852386467987\n",
      "Epoch 105/300\n",
      "Average training loss: 0.051225647469361625\n",
      "Average test loss: 0.004120962742302153\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05106715556979179\n",
      "Average test loss: 0.004038706377148628\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05090108221438196\n",
      "Average test loss: 0.00406751818002926\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05077032282948494\n",
      "Average test loss: 0.0041101233123077285\n",
      "Epoch 109/300\n",
      "Average training loss: 0.050545695155858994\n",
      "Average test loss: 0.004200151521298621\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05046502810385492\n",
      "Average test loss: 0.004149123548219601\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05032633636395137\n",
      "Average test loss: 0.004084961681316297\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05012315845820639\n",
      "Average test loss: 0.00409074843633506\n",
      "Epoch 113/300\n",
      "Average training loss: 0.050063322610325284\n",
      "Average test loss: 0.004127745743840933\n",
      "Epoch 114/300\n",
      "Average training loss: 0.049809667829010223\n",
      "Average test loss: 0.004064275638510783\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04972457842694389\n",
      "Average test loss: 0.0040538384738481705\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0495494805375735\n",
      "Average test loss: 0.0042256456733577785\n",
      "Epoch 117/300\n",
      "Average training loss: 0.049481157940295006\n",
      "Average test loss: 0.004270197265677982\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04931684532761574\n",
      "Average test loss: 0.004108889356462492\n",
      "Epoch 119/300\n",
      "Average training loss: 0.049112705919477674\n",
      "Average test loss: 0.004253582159471181\n",
      "Epoch 120/300\n",
      "Average training loss: 0.048998876955774096\n",
      "Average test loss: 0.004271037870811092\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04895112660692798\n",
      "Average test loss: 0.0040480405748304395\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04863338028060065\n",
      "Average test loss: 0.004160125327606996\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04852723808752166\n",
      "Average test loss: 0.004144745043168465\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04839898994896147\n",
      "Average test loss: 0.004097275375492043\n",
      "Epoch 125/300\n",
      "Average training loss: 0.048292082750134996\n",
      "Average test loss: 0.00399353536301189\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04827075169483821\n",
      "Average test loss: 0.004114617951007353\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04823459893796179\n",
      "Average test loss: 0.004054409035791954\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0480060844818751\n",
      "Average test loss: 0.004204067521211174\n",
      "Epoch 129/300\n",
      "Average training loss: 0.047750601139333514\n",
      "Average test loss: 0.004317443656424682\n",
      "Epoch 130/300\n",
      "Average training loss: 0.047566298368904324\n",
      "Average test loss: 0.004083658937985698\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04742776276667913\n",
      "Average test loss: 0.0041401170096877545\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04751361894607544\n",
      "Average test loss: 0.004160668203400241\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04730468580292331\n",
      "Average test loss: 0.004496407966439923\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04709725149141417\n",
      "Average test loss: 0.0041208058930933475\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04688661022649871\n",
      "Average test loss: 0.004134247593167755\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04683022394776344\n",
      "Average test loss: 0.004129356317843\n",
      "Epoch 137/300\n",
      "Average training loss: 0.046737731526295345\n",
      "Average test loss: 0.004096987082519465\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04659940441449483\n",
      "Average test loss: 0.004120577058444421\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04643390846252442\n",
      "Average test loss: 0.004236543289903138\n",
      "Epoch 140/300\n",
      "Average training loss: 0.046242372847265664\n",
      "Average test loss: 0.004140448569009701\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04618840060300297\n",
      "Average test loss: 0.004285616237256262\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04609838843345642\n",
      "Average test loss: 0.004246724761194653\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04608594485786226\n",
      "Average test loss: 0.004353926479609476\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04582368932829963\n",
      "Average test loss: 0.004334783374849293\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0456884737710158\n",
      "Average test loss: 0.00407238506194618\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04558987061182658\n",
      "Average test loss: 0.004138774099863238\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04551151328947809\n",
      "Average test loss: 0.004156375598162412\n",
      "Epoch 148/300\n",
      "Average training loss: 0.045407415439685184\n",
      "Average test loss: 0.004158520316084226\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04520475799838702\n",
      "Average test loss: 0.0041182937415109745\n",
      "Epoch 150/300\n",
      "Average training loss: 0.045054939091205594\n",
      "Average test loss: 0.0041858247878650825\n",
      "Epoch 151/300\n",
      "Average training loss: 0.044747609509362116\n",
      "Average test loss: 0.004303134097407261\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04463908953468005\n",
      "Average test loss: 0.004129722786032491\n",
      "Epoch 155/300\n",
      "Average training loss: 0.044575170450740394\n",
      "Average test loss: 0.004382842696375317\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04432718505130874\n",
      "Average test loss: 0.004246245823386643\n",
      "Epoch 158/300\n",
      "Average training loss: 0.044220014135042826\n",
      "Average test loss: 0.004154900768150886\n",
      "Epoch 159/300\n",
      "Average training loss: 0.044167514665259255\n",
      "Average test loss: 0.004132626566621993\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04406976199150085\n",
      "Average test loss: 0.004166191637929943\n",
      "Epoch 161/300\n",
      "Average training loss: 0.043925309489170714\n",
      "Average test loss: 0.004289164071074791\n",
      "Epoch 162/300\n",
      "Average training loss: 0.043796300563547344\n",
      "Average test loss: 0.004072216084433927\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04372083347704676\n",
      "Average test loss: 0.004262451426229543\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04364386781387859\n",
      "Average test loss: 0.004135912417537636\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04352221203843753\n",
      "Average test loss: 0.004459642074588272\n",
      "Epoch 166/300\n",
      "Average training loss: 0.043450594097375866\n",
      "Average test loss: 0.0042973136210607155\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04322234739032057\n",
      "Average test loss: 0.004359880050023397\n",
      "Epoch 169/300\n",
      "Average training loss: 0.043106127778689066\n",
      "Average test loss: 0.004274428197907077\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0430610372391012\n",
      "Average test loss: 0.0042380102260245215\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04309976069794761\n",
      "Average test loss: 0.00415047713327739\n",
      "Epoch 172/300\n",
      "Average training loss: 0.042848726716306476\n",
      "Average test loss: 0.0042479195230536995\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04275764356387986\n",
      "Average test loss: 0.004134670399957233\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04267122774322828\n",
      "Average test loss: 0.004325981540398465\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04272922992706299\n",
      "Average test loss: 0.0044061088156369\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04252044832706452\n",
      "Average test loss: 0.00435971700710555\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04245250750581423\n",
      "Average test loss: 0.004347054761317041\n",
      "Epoch 178/300\n",
      "Average training loss: 0.042381188684039645\n",
      "Average test loss: 0.0041422536596655846\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04224267749653922\n",
      "Average test loss: 0.004267830185592175\n",
      "Epoch 180/300\n",
      "Average training loss: 0.042245404806401994\n",
      "Average test loss: 0.004314196870765752\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0420071719719304\n",
      "Average test loss: 0.004210209452443652\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04200685538185967\n",
      "Average test loss: 0.004497007170485126\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0419208966659175\n",
      "Average test loss: 0.00420517432441314\n",
      "Epoch 184/300\n",
      "Average training loss: 0.041833078175783155\n",
      "Average test loss: 0.0041639137756493355\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04171107907427682\n",
      "Average test loss: 0.004405332288394371\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04166905599501398\n",
      "Average test loss: 0.004151001064727704\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04161506068375376\n",
      "Average test loss: 0.004355533955411778\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04158582906424999\n",
      "Average test loss: 0.004385556023981836\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04128648956782288\n",
      "Average test loss: 0.004273139907254113\n",
      "Epoch 192/300\n",
      "Average training loss: 0.041297490931219524\n",
      "Average test loss: 0.0041641450387736164\n",
      "Epoch 193/300\n",
      "Average training loss: 0.041128432429499096\n",
      "Average test loss: 0.004285685065098935\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04111297817031542\n",
      "Average test loss: 0.004351191569947534\n",
      "Epoch 195/300\n",
      "Average training loss: 0.041014214830266106\n",
      "Average test loss: 0.004151941362975372\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04085604918499788\n",
      "Average test loss: 0.004156427981124984\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04104169256157345\n",
      "Average test loss: 0.004324676748365164\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04083754415313403\n",
      "Average test loss: 0.004182960711419582\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04069319333798355\n",
      "Average test loss: 0.004168174535450008\n",
      "Epoch 200/300\n",
      "Average training loss: 0.040604119860463674\n",
      "Average test loss: 0.004380614335338275\n",
      "Epoch 202/300\n",
      "Average training loss: 0.040614736871586905\n",
      "Average test loss: 0.004200698146389591\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04048183632724815\n",
      "Average test loss: 0.00437989898191558\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04044106098678377\n",
      "Average test loss: 0.0042135232353789935\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04029848909378052\n",
      "Average test loss: 0.004173825720532073\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04025888034039073\n",
      "Average test loss: 0.004251572344245182\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04019823688599798\n",
      "Average test loss: 0.004371104506154855\n",
      "Epoch 208/300\n",
      "Average training loss: 0.040247536457247206\n",
      "Average test loss: 0.00436642442478074\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04014635073807504\n",
      "Average test loss: 0.0041955344368600185\n",
      "Epoch 210/300\n",
      "Average training loss: 0.040311099062363305\n",
      "Average test loss: 0.004191701588117414\n",
      "Epoch 211/300\n",
      "Average training loss: 0.039991849448945786\n",
      "Average test loss: 0.004236714923754334\n",
      "Epoch 212/300\n",
      "Average training loss: 0.039847251959972914\n",
      "Average test loss: 0.004151582580473688\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03979009761081802\n",
      "Average test loss: 0.004335968886812528\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03963867738842964\n",
      "Average test loss: 0.004183192683176862\n",
      "Epoch 216/300\n",
      "Average training loss: 0.039701952507098515\n",
      "Average test loss: 0.004290323241303365\n",
      "Epoch 217/300\n",
      "Average training loss: 0.039688996091485025\n",
      "Average test loss: 0.004227297441619966\n",
      "Epoch 218/300\n",
      "Average training loss: 0.039496472908390895\n",
      "Average test loss: 0.0042302322499454025\n",
      "Epoch 219/300\n",
      "Average training loss: 0.039459559705522326\n",
      "Average test loss: 0.0044671785752806396\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03947142172853152\n",
      "Average test loss: 0.00429633721543683\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0394256111184756\n",
      "Average test loss: 0.004230003119963739\n",
      "Epoch 222/300\n",
      "Average training loss: 0.040193989084826576\n",
      "Average test loss: 0.0044039947587168875\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03905872291326523\n",
      "Average test loss: 0.004222826855878035\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03906345245242119\n",
      "Average test loss: 0.004220691760898464\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03910189145472315\n",
      "Average test loss: 0.004258240360145767\n",
      "Epoch 227/300\n",
      "Average training loss: 0.039036179668373534\n",
      "Average test loss: 0.004323784292365114\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03907513317465782\n",
      "Average test loss: 0.004247921874953641\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03959084110127555\n",
      "Average test loss: 0.004257446029120021\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03885752484202385\n",
      "Average test loss: 0.004357281623615159\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03869794919424587\n",
      "Average test loss: 0.004160292206952969\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03874852764440907\n",
      "Average test loss: 0.004397169084598621\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03875466882189115\n",
      "Average test loss: 0.004189415925078922\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03870313315259086\n",
      "Average test loss: 0.004402318411196271\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03867848671641615\n",
      "Average test loss: 0.004571555198480686\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03864729362063938\n",
      "Average test loss: 0.004305575421700875\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03852775836322043\n",
      "Average test loss: 0.004382851293103562\n",
      "Epoch 238/300\n",
      "Average training loss: 0.038386555718051066\n",
      "Average test loss: 0.004291201964848571\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03841031467086739\n",
      "Average test loss: 0.004176833896794253\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03851564212971263\n",
      "Average test loss: 0.004377752563191784\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03839667781194051\n",
      "Average test loss: 0.004460145996676551\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03867335817052259\n",
      "Average test loss: 0.004418366839488348\n",
      "Epoch 243/300\n",
      "Average training loss: 0.038293306981523834\n",
      "Average test loss: 0.004543452338625987\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03808141438497437\n",
      "Average test loss: 0.004201936750362316\n",
      "Epoch 246/300\n",
      "Average training loss: 0.038093749241696465\n",
      "Average test loss: 0.00430825142790046\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03810698986384604\n",
      "Average test loss: 0.004361705533332295\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03800286884771453\n",
      "Average test loss: 0.0044740414269682435\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0379532597594791\n",
      "Average test loss: 0.0044134187224424545\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03818770328495238\n",
      "Average test loss: 0.0042567168341742624\n",
      "Epoch 251/300\n",
      "Average training loss: 0.038001481519805064\n",
      "Average test loss: 0.004320012208074331\n",
      "Epoch 252/300\n",
      "Average training loss: 0.037832397335105475\n",
      "Average test loss: 0.004530175768252876\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03775437826580472\n",
      "Average test loss: 0.004503664325094885\n",
      "Epoch 254/300\n",
      "Average training loss: 0.037747371253040105\n",
      "Average test loss: 0.00427818128052685\n",
      "Epoch 255/300\n",
      "Average training loss: 0.037736512230502235\n",
      "Average test loss: 0.0042324383594095705\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03770633034242524\n",
      "Average test loss: 0.004305610615346167\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03768369390898281\n",
      "Average test loss: 0.004342121794612871\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03774148244990243\n",
      "Average test loss: 0.00421563693922427\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03763674964176284\n",
      "Average test loss: 0.004229473685638772\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0375071228692929\n",
      "Average test loss: 0.004308856744319201\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03738390333784951\n",
      "Average test loss: 0.004191005500240459\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03759335042701827\n",
      "Average test loss: 0.004227265694282121\n",
      "Epoch 263/300\n",
      "Average training loss: 0.037479396727350026\n",
      "Average test loss: 0.004181582555174828\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03720949762397342\n",
      "Average test loss: 0.004541112795265184\n",
      "Epoch 266/300\n",
      "Average training loss: 0.037332778801520666\n",
      "Average test loss: 0.004435668463508288\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03726457545492384\n",
      "Average test loss: 0.004337658959130446\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03898977781997787\n",
      "Average test loss: 0.004332064042281774\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0370439418885443\n",
      "Average test loss: 0.004329455037911733\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03687117395798365\n",
      "Average test loss: 0.004443936129204101\n",
      "Epoch 271/300\n",
      "Average training loss: 0.037354646301931804\n",
      "Average test loss: 0.0042569979559630156\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03752979272603989\n",
      "Average test loss: 0.004365125835769706\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03703993673953745\n",
      "Average test loss: 0.004563055751224359\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03699248855312665\n",
      "Average test loss: 0.004382603198703793\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03689218001067639\n",
      "Average test loss: 0.004359089706093073\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0367965269751019\n",
      "Average test loss: 0.004504317269970973\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03706641653842396\n",
      "Average test loss: 0.004509184665770994\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03692977746989992\n",
      "Average test loss: 0.00425376844074991\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03669958394434717\n",
      "Average test loss: 0.004246453401736087\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03665432967742284\n",
      "Average test loss: 0.004298740226775408\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03670548771818479\n",
      "Average test loss: 0.004458058984536263\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03662427558998267\n",
      "Average test loss: 0.004312466953777605\n",
      "Epoch 284/300\n",
      "Average training loss: 0.036700303037961325\n",
      "Average test loss: 0.004207305216954814\n",
      "Epoch 285/300\n",
      "Average training loss: 0.036538058883614014\n",
      "Average test loss: 0.004368599153641198\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03649033729235331\n",
      "Average test loss: 0.004358595407464438\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03645834292140272\n",
      "Average test loss: 0.0042727560411310855\n",
      "Epoch 289/300\n",
      "Average training loss: 0.036680373768011726\n",
      "Average test loss: 0.004249096603236265\n",
      "Epoch 290/300\n",
      "Average training loss: 0.036331967221366036\n",
      "Average test loss: 0.004324098673545652\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03631359142727322\n",
      "Average test loss: 0.004264081611815426\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03640721453229586\n",
      "Average test loss: 0.0044335049407349695\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03626293638845285\n",
      "Average test loss: 0.004669518143973417\n",
      "Epoch 294/300\n",
      "Average training loss: 0.036446040304170715\n",
      "Average test loss: 0.004484495728587111\n",
      "Epoch 296/300\n",
      "Average training loss: 0.036421729963686734\n",
      "Average test loss: 0.004164009454763598\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03613977603117625\n",
      "Average test loss: 0.004416085499856207\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03615113687184122\n",
      "Average test loss: 0.004258035082990924\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03623071839412054\n",
      "Average test loss: 0.004325257351001104\n",
      "Epoch 300/300\n",
      "Average training loss: 0.035976709852615994\n",
      "Average test loss: 0.0042717151004407145\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.947888145870633\n",
      "Average test loss: 0.00716391312248177\n",
      "Epoch 2/300\n",
      "Average training loss: 3.3597957117292614\n",
      "Average test loss: 0.005164211742579937\n",
      "Epoch 3/300\n",
      "Average training loss: 2.9195722187889945\n",
      "Average test loss: 0.004718473085098796\n",
      "Epoch 4/300\n",
      "Average training loss: 2.44328639793396\n",
      "Average test loss: 0.0041284642538262735\n",
      "Epoch 6/300\n",
      "Average training loss: 2.28355569797092\n",
      "Average test loss: 0.003983515109039016\n",
      "Epoch 7/300\n",
      "Average training loss: 2.141627004623413\n",
      "Average test loss: 0.004069132683798671\n",
      "Epoch 8/300\n",
      "Average training loss: 2.0172262460920547\n",
      "Average test loss: 0.0038018357861373158\n",
      "Epoch 9/300\n",
      "Average training loss: 1.9051855665842692\n",
      "Average test loss: 0.00372423359627525\n",
      "Epoch 10/300\n",
      "Average training loss: 1.8023233173158433\n",
      "Average test loss: 0.0036604079744882055\n",
      "Epoch 11/300\n",
      "Average training loss: 1.707555473645528\n",
      "Average test loss: 0.0036074625439941883\n",
      "Epoch 12/300\n",
      "Average training loss: 1.6129522439108954\n",
      "Average test loss: 0.003540718526475959\n",
      "Epoch 13/300\n",
      "Average training loss: 1.5272408328586156\n",
      "Average test loss: 0.003518742064634959\n",
      "Epoch 14/300\n",
      "Average training loss: 1.4448879219690958\n",
      "Average test loss: 0.0034280270437399545\n",
      "Epoch 15/300\n",
      "Average training loss: 1.3656401069429185\n",
      "Average test loss: 0.0035777795534167023\n",
      "Epoch 16/300\n",
      "Average test loss: 0.00333500691730943\n",
      "Epoch 18/300\n",
      "Average training loss: 1.1540830370585125\n",
      "Average test loss: 0.003257046087334553\n",
      "Epoch 19/300\n",
      "Average training loss: 1.0887429783079359\n",
      "Average test loss: 0.0032278551155080397\n",
      "Epoch 20/300\n",
      "Average training loss: 1.0284630694919161\n",
      "Average test loss: 0.0032350053787231444\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9698824589517382\n",
      "Average test loss: 0.003164406390239795\n",
      "Epoch 22/300\n",
      "Average training loss: 0.9155709250768026\n",
      "Average test loss: 0.0031426084242347214\n",
      "Epoch 23/300\n",
      "Average training loss: 0.8625776966942681\n",
      "Average test loss: 0.003113816430585252\n",
      "Epoch 24/300\n",
      "Average training loss: 0.8120759036805895\n",
      "Average test loss: 0.003132849358850055\n",
      "Epoch 25/300\n",
      "Average training loss: 0.7621603689193726\n",
      "Average test loss: 0.003107067374409073\n",
      "Epoch 26/300\n",
      "Average training loss: 0.7105515533553229\n",
      "Average test loss: 0.0030603556076271667\n",
      "Epoch 27/300\n",
      "Average training loss: 0.6620883391168383\n",
      "Average test loss: 0.003083505760360923\n",
      "Epoch 28/300\n",
      "Average training loss: 0.5729954463640848\n",
      "Average test loss: 0.0031083811384936175\n",
      "Epoch 30/300\n",
      "Average training loss: 0.5313896858957079\n",
      "Average test loss: 0.0031301756908910143\n",
      "Epoch 31/300\n",
      "Average training loss: 0.49136621109644574\n",
      "Average test loss: 0.0030455300418867006\n",
      "Epoch 32/300\n",
      "Average training loss: 0.45246919994884066\n",
      "Average test loss: 0.0029739092890587117\n",
      "Epoch 33/300\n",
      "Average training loss: 0.37779476160473296\n",
      "Average test loss: 0.0029586568435447085\n",
      "Epoch 35/300\n",
      "Average training loss: 0.34340001678466797\n",
      "Average test loss: 0.0029360406330476204\n",
      "Epoch 36/300\n",
      "Average training loss: 0.31216205710834927\n",
      "Average test loss: 0.01653104337139262\n",
      "Epoch 37/300\n",
      "Average training loss: 0.2819222957558102\n",
      "Average test loss: 0.002915186853458484\n",
      "Epoch 38/300\n",
      "Average training loss: 0.25408635845449234\n",
      "Average test loss: 0.0030019210365911323\n",
      "Epoch 39/300\n",
      "Average training loss: 0.22846449168523153\n",
      "Average test loss: 0.0029701406765315266\n",
      "Epoch 40/300\n",
      "Average training loss: 0.20355869866742027\n",
      "Average test loss: 0.0029411810144989027\n",
      "Epoch 41/300\n",
      "Average training loss: 0.18110155385070376\n",
      "Average test loss: 0.0028945381759355464\n",
      "Epoch 42/300\n",
      "Average training loss: 0.15988261348671384\n",
      "Average test loss: 0.0029133240746127232\n",
      "Epoch 43/300\n",
      "Average training loss: 0.14040173635217879\n",
      "Average test loss: 0.0031213264947550163\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12291561012135611\n",
      "Average test loss: 0.0030280397248764832\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09483974934948815\n",
      "Average test loss: 0.0029260391315652267\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08427599655919604\n",
      "Average test loss: 0.002896426689086689\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07580991613202624\n",
      "Average test loss: 0.0029219926655706433\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06920468786027696\n",
      "Average test loss: 0.0031471543781873253\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06412858174244562\n",
      "Average test loss: 0.0029554362437791293\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06023578351736069\n",
      "Average test loss: 0.002905306361615658\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05731794152326054\n",
      "Average test loss: 0.003075814055899779\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05492677109109031\n",
      "Average test loss: 0.0028911170408957533\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05316182801458571\n",
      "Average test loss: 0.002850450876686308\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05175583255953259\n",
      "Average test loss: 0.0029597301081650787\n",
      "Epoch 56/300\n",
      "Average training loss: 0.050521688991122776\n",
      "Average test loss: 0.002942869206269582\n",
      "Epoch 57/300\n",
      "Average training loss: 0.049575446324215997\n",
      "Average test loss: 0.0028899184575097428\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04827150722344716\n",
      "Average test loss: 0.0028741585670246017\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0475137703385618\n",
      "Average test loss: 0.002845961744793587\n",
      "Epoch 61/300\n",
      "Average training loss: 0.047018132256137\n",
      "Average test loss: 0.0033238633738623723\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04660146268043253\n",
      "Average test loss: 0.0029098287284788156\n",
      "Epoch 63/300\n",
      "Average training loss: 0.046234475172228284\n",
      "Average test loss: 0.002903009808104899\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04582734928197331\n",
      "Average test loss: 0.0029598601166572836\n",
      "Epoch 65/300\n",
      "Average training loss: 0.045407041226824124\n",
      "Average test loss: 0.0028980381805449726\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04520472260978487\n",
      "Average test loss: 0.002943925525786148\n",
      "Epoch 67/300\n",
      "Average training loss: 0.044858498566680484\n",
      "Average test loss: 0.0034859080031100246\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04468090979258219\n",
      "Average test loss: 0.003076991147465176\n",
      "Epoch 69/300\n",
      "Average training loss: 0.044372622430324554\n",
      "Average test loss: 0.0028966168556362392\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04393397207392587\n",
      "Average test loss: 0.0029143444508728054\n",
      "Epoch 71/300\n",
      "Average training loss: 0.043422167950206335\n",
      "Average test loss: 0.003034673171531823\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0434357331958082\n",
      "Average test loss: 0.002953121763964494\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04259118587772052\n",
      "Average test loss: 0.0030931314135798148\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04243595623638895\n",
      "Average test loss: 0.0029140329862841303\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04222385691271888\n",
      "Average test loss: 0.00297388403945499\n",
      "Epoch 78/300\n",
      "Average training loss: 0.041806269937091405\n",
      "Average test loss: 0.002949994209119015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04157625689440304\n",
      "Average test loss: 0.002868487770151761\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04142962936229176\n",
      "Average test loss: 0.0029946936008830864\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04112050865093867\n",
      "Average test loss: 0.00300276787723932\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04082270637485716\n",
      "Average test loss: 0.002910674994190534\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0407587149573697\n",
      "Average test loss: 0.003041035011410713\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04028795534537898\n",
      "Average test loss: 0.003107642312016752\n",
      "Epoch 86/300\n",
      "Average training loss: 0.040152246554692586\n",
      "Average test loss: 0.002953803334592117\n",
      "Epoch 87/300\n",
      "Average training loss: 0.039942125621769166\n",
      "Average test loss: 0.002927816712194019\n",
      "Epoch 88/300\n",
      "Average training loss: 0.039615692363844976\n",
      "Average test loss: 0.0029742555818003083\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03943835729360581\n",
      "Average test loss: 0.003024999918002221\n",
      "Epoch 90/300\n",
      "Average training loss: 0.039243373102611966\n",
      "Average test loss: 0.00302965365205374\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03906137483649784\n",
      "Average test loss: 0.0030018047905630537\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03879209049211608\n",
      "Average test loss: 0.0029410975612699985\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03861782025959757\n",
      "Average test loss: 0.002931462133717206\n",
      "Epoch 94/300\n",
      "Average training loss: 0.038419742912054064\n",
      "Average test loss: 0.003046242746214072\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03825275836719407\n",
      "Average test loss: 0.0029523555448071823\n",
      "Epoch 96/300\n",
      "Average test loss: 0.0032218894565271008\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03787097035845121\n",
      "Average test loss: 0.0029908277680062586\n",
      "Epoch 99/300\n",
      "Average training loss: 0.037667545563644836\n",
      "Average test loss: 0.0030850819216834175\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03739963035782178\n",
      "Average test loss: 0.003250272525060508\n",
      "Epoch 101/300\n",
      "Average training loss: 0.037217621963885096\n",
      "Average test loss: 0.0030148922829992242\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03707677862379286\n",
      "Average test loss: 0.0029398546589331494\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03694813501007027\n",
      "Average test loss: 0.002961823100431098\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03692544458806515\n",
      "Average test loss: 0.0029930120760367977\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03663061503403717\n",
      "Average test loss: 0.003075802450792657\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0364588006734848\n",
      "Average test loss: 0.003054440177977085\n",
      "Epoch 107/300\n",
      "Average training loss: 0.036327042513423495\n",
      "Average test loss: 0.0029893719493928883\n",
      "Epoch 108/300\n",
      "Average training loss: 0.035906707972288135\n",
      "Average test loss: 0.00315777835311989\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03582487094402313\n",
      "Average test loss: 0.0030841702001376285\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03551816718611452\n",
      "Average test loss: 0.002957606833997286\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03539507755968306\n",
      "Average test loss: 0.002927193401174413\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03532552582356665\n",
      "Average test loss: 0.0028998679336574343\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03515110904309485\n",
      "Average test loss: 0.002940217237919569\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03501582502159807\n",
      "Average test loss: 0.0030718014967731303\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03485762111677064\n",
      "Average test loss: 0.0029135257539649804\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03490754879514376\n",
      "Average test loss: 0.0029896475569241576\n",
      "Epoch 120/300\n",
      "Average training loss: 0.034658145563469994\n",
      "Average test loss: 0.00291115706310504\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03451613693601555\n",
      "Average test loss: 0.0030376200135797263\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03450146252082454\n",
      "Average test loss: 0.00312165277885894\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03443642879029115\n",
      "Average test loss: 0.002986742558889091\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03423770176867644\n",
      "Average test loss: 0.003082964814785454\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03418534151795838\n",
      "Average test loss: 0.0031753175697392887\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03404933657579952\n",
      "Average test loss: 0.003114752050282227\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03390489305224684\n",
      "Average test loss: 0.0030443232332666714\n",
      "Epoch 128/300\n",
      "Average training loss: 0.033785480850272706\n",
      "Average test loss: 0.0029820024418748085\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03374044930272632\n",
      "Average test loss: 0.002982998167475065\n",
      "Epoch 130/300\n",
      "Average training loss: 0.033543722278541986\n",
      "Average test loss: 0.002952498801776932\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03338171876139111\n",
      "Average test loss: 0.003064884368951122\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03317935757504569\n",
      "Average test loss: 0.0029956525042653085\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03307030526134703\n",
      "Average test loss: 0.002994997870797912\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03298698162039121\n",
      "Average test loss: 0.002918349391987754\n",
      "Epoch 138/300\n",
      "Average training loss: 0.032864364580975636\n",
      "Average test loss: 0.003033074853113956\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03280329034063551\n",
      "Average test loss: 0.0031078821708344752\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03274803989960088\n",
      "Average test loss: 0.003081308847086297\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0327159806324376\n",
      "Average test loss: 0.002977068825935324\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03264402557578352\n",
      "Average test loss: 0.003025752619115843\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03246360826823447\n",
      "Average test loss: 0.003132641406108936\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03238313996129566\n",
      "Average test loss: 0.0031621118318289516\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0323292772869269\n",
      "Average test loss: 0.0031111119364698727\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03224718125992351\n",
      "Average test loss: 0.003116245261290007\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03214027836090989\n",
      "Average test loss: 0.0029601558193357454\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03200686418679025\n",
      "Average test loss: 0.0030884631977727015\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031968295435110726\n",
      "Average test loss: 0.003050852158210344\n",
      "Epoch 151/300\n",
      "Average training loss: 0.031925202295184134\n",
      "Average test loss: 0.003067299900369512\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03184139695266883\n",
      "Average test loss: 0.0030608788763897285\n",
      "Epoch 153/300\n",
      "Average training loss: 0.031749651514821585\n",
      "Average test loss: 0.0029611822366714478\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03167846031983693\n",
      "Average test loss: 0.0030395168587565424\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03159635221295887\n",
      "Average test loss: 0.0029862146944635445\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03152373969886038\n",
      "Average test loss: 0.0029813460394119222\n",
      "Epoch 157/300\n",
      "Average training loss: 0.031512302796045936\n",
      "Average test loss: 0.0032617372473080952\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03141410337388516\n",
      "Average test loss: 0.003030453596264124\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03130883184240924\n",
      "Average test loss: 0.003120304313281344\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03125852396090825\n",
      "Average test loss: 0.0030556174260046747\n",
      "Epoch 162/300\n",
      "Average training loss: 0.031077082440257073\n",
      "Average test loss: 0.0031942425130142105\n",
      "Epoch 163/300\n",
      "Average training loss: 0.031091742603315247\n",
      "Average test loss: 0.0030897573832836416\n",
      "Epoch 164/300\n",
      "Average training loss: 0.031052051794197823\n",
      "Average test loss: 0.003017663455257813\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031031023383140562\n",
      "Average test loss: 0.0030563641318844424\n",
      "Epoch 166/300\n",
      "Average training loss: 0.030850642550322743\n",
      "Average test loss: 0.0031526854071352215\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03090389941798316\n",
      "Average test loss: 0.003237624871234099\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030806239739060402\n",
      "Average test loss: 0.0031165493809514576\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030697869282629756\n",
      "Average test loss: 0.0030290495386968057\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03065116315583388\n",
      "Average test loss: 0.0032050890483789974\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03046102812886238\n",
      "Average test loss: 0.0030641761322816215\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03045013268954224\n",
      "Average test loss: 0.00324534556021293\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03038277430170112\n",
      "Average test loss: 0.0030050746933039693\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0303677792151769\n",
      "Average test loss: 0.0030755126681178808\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03033005196021663\n",
      "Average test loss: 0.0030913125119275517\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03017467014160421\n",
      "Average test loss: 0.003133449051529169\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03020505377981398\n",
      "Average test loss: 0.003077913980724083\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03011406374639935\n",
      "Average test loss: 0.0030705617266810602\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030104216519329284\n",
      "Average test loss: 0.0031073756656712958\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03012248625689083\n",
      "Average test loss: 0.0030409604095750385\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029976404406130315\n",
      "Average test loss: 0.0031919398065656424\n",
      "Epoch 185/300\n",
      "Average training loss: 0.029891054641869332\n",
      "Average test loss: 0.0030732834084580343\n",
      "Epoch 186/300\n",
      "Average training loss: 0.029858747329976825\n",
      "Average test loss: 0.0031215939881900946\n",
      "Epoch 187/300\n",
      "Average training loss: 0.029795435396333534\n",
      "Average test loss: 0.003241616491331822\n",
      "Epoch 188/300\n",
      "Average training loss: 0.029784700171815025\n",
      "Average test loss: 0.0031444977451529767\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02962120634648535\n",
      "Average test loss: 0.003156055200845003\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02966091630525059\n",
      "Average test loss: 0.0031402748032576507\n",
      "Epoch 191/300\n",
      "Average training loss: 0.029619100438223946\n",
      "Average test loss: 0.0030916489052275815\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0295588844170173\n",
      "Average test loss: 0.0032122227280504175\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029493593457672333\n",
      "Average test loss: 0.003113644545069999\n",
      "Epoch 194/300\n",
      "Average training loss: 0.029499936314092742\n",
      "Average test loss: 0.0030760501215441357\n",
      "Epoch 195/300\n",
      "Average training loss: 0.029398133835858768\n",
      "Average test loss: 0.00304730776614613\n",
      "Epoch 197/300\n",
      "Average training loss: 0.029305772811174392\n",
      "Average test loss: 0.0031228394611842103\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02930914890766144\n",
      "Average test loss: 0.0031586499539100464\n",
      "Epoch 200/300\n",
      "Average training loss: 0.029229602757427427\n",
      "Average test loss: 0.0030898604223297704\n",
      "Epoch 201/300\n",
      "Average training loss: 0.029111270247234238\n",
      "Average test loss: 0.003166086934092972\n",
      "Epoch 202/300\n",
      "Average training loss: 0.029164491304092936\n",
      "Average test loss: 0.003117262363847759\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02907914625108242\n",
      "Average test loss: 0.003174502309825685\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02903003847433461\n",
      "Average test loss: 0.003097098203168975\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029040125499169032\n",
      "Average test loss: 0.003245009833325942\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02895394383205308\n",
      "Average test loss: 0.0034964920518298945\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028898412353462643\n",
      "Average test loss: 0.0032636317436893782\n",
      "Epoch 208/300\n",
      "Average training loss: 0.028863992866542603\n",
      "Average test loss: 0.003094517658982012\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02887598380777571\n",
      "Average training loss: 0.028869308196836047\n",
      "Average test loss: 0.0031649769333501657\n",
      "Epoch 211/300\n",
      "Average training loss: 0.028758941885497834\n",
      "Average test loss: 0.0031606451815201176\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02872764804131455\n",
      "Average test loss: 0.003260614340710971\n",
      "Epoch 213/300\n",
      "Average training loss: 0.028662513938215044\n",
      "Average test loss: 0.003160026679850287\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02863161776545975\n",
      "Average test loss: 0.0031658226367500095\n",
      "Epoch 215/300\n",
      "Average training loss: 0.028644272158543267\n",
      "Average test loss: 0.003100572655805283\n",
      "Epoch 216/300\n",
      "Average training loss: 0.028596965809663137\n",
      "Average test loss: 0.003125478709323539\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028523718943198523\n",
      "Average test loss: 0.0031501850992855097\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028470717888739373\n",
      "Average test loss: 0.003234293162201842\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028496252016888723\n",
      "Average test loss: 0.003107387914011876\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028523321530885166\n",
      "Average test loss: 0.0032371604732341235\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028392515018582344\n",
      "Average test loss: 0.003226823120067517\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02847577442228794\n",
      "Average test loss: 0.0031420700109253325\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028295822236273024\n",
      "Average test loss: 0.0032733353016277155\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02829487480388747\n",
      "Average test loss: 0.003201525039763914\n",
      "Epoch 226/300\n",
      "Average training loss: 0.028213050065769088\n",
      "Average test loss: 0.0030907385150591533\n",
      "Epoch 227/300\n",
      "Average training loss: 0.028149736136198044\n",
      "Average test loss: 0.003224333894335561\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02816377534468969\n",
      "Average test loss: 0.003121348526535763\n",
      "Epoch 229/300\n",
      "Average training loss: 0.028114623737004067\n",
      "Average test loss: 0.003177994006002943\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02809214786108997\n",
      "Average test loss: 0.0031263404799004397\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02806229141685698\n",
      "Average test loss: 0.0031652423702180385\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02805294513205687\n",
      "Average test loss: 0.00313253819445769\n",
      "Epoch 233/300\n",
      "Average training loss: 0.027949779647919868\n",
      "Average test loss: 0.003166632493130035\n",
      "Epoch 234/300\n",
      "Average training loss: 0.027965636231833034\n",
      "Average test loss: 0.003122792981047597\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02787412949734264\n",
      "Average test loss: 0.0032245467410733303\n",
      "Epoch 237/300\n",
      "Average training loss: 0.027806527867913246\n",
      "Average test loss: 0.003157239700357119\n",
      "Epoch 238/300\n",
      "Average training loss: 0.027849888569778866\n",
      "Average test loss: 0.0032077582204300494\n",
      "Epoch 239/300\n",
      "Average training loss: 0.027767938697503672\n",
      "Average test loss: 0.0032703636499742668\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02800670804745621\n",
      "Average test loss: 0.003208606766122911\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02783045432633824\n",
      "Average test loss: 0.0031531057806892526\n",
      "Epoch 242/300\n",
      "Average training loss: 0.027661044834388627\n",
      "Average test loss: 0.003261137963583072\n",
      "Epoch 243/300\n",
      "Average training loss: 0.027689156098498237\n",
      "Average test loss: 0.0031523822034812635\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02758383466468917\n",
      "Average test loss: 0.003103395073364178\n",
      "Epoch 245/300\n",
      "Average training loss: 0.027611279924710592\n",
      "Average test loss: 0.0031702684122655125\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02751595388684008\n",
      "Average test loss: 0.003145328178587887\n",
      "Epoch 248/300\n",
      "Average training loss: 0.027517064326339297\n",
      "Average test loss: 0.003246511633611388\n",
      "Epoch 249/300\n",
      "Average training loss: 0.027486040600472026\n",
      "Average test loss: 0.003171190153600441\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027495492916968133\n",
      "Average test loss: 0.003155017582906617\n",
      "Epoch 251/300\n",
      "Average training loss: 0.027388490637143454\n",
      "Average test loss: 0.0031749281090580755\n",
      "Epoch 253/300\n",
      "Average training loss: 0.027385248402754466\n",
      "Average test loss: 0.003238545734849241\n",
      "Epoch 254/300\n",
      "Average training loss: 0.027343910644451776\n",
      "Average test loss: 0.0031225559167149994\n",
      "Epoch 255/300\n",
      "Average training loss: 0.027338889706465933\n",
      "Average test loss: 0.003177801728248596\n",
      "Epoch 256/300\n",
      "Average training loss: 0.027302655453483263\n",
      "Average test loss: 0.0032372757437535457\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0273189393563403\n",
      "Average test loss: 0.00311294108753403\n",
      "Epoch 258/300\n",
      "Average training loss: 0.027253946365581618\n",
      "Average test loss: 0.0031583394789033467\n",
      "Epoch 259/300\n",
      "Average training loss: 0.027170854431059627\n",
      "Average test loss: 0.003212688649280204\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02721702764845557\n",
      "Average test loss: 0.003225991162781914\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02708736740714974\n",
      "Average test loss: 0.003159918619112836\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02716867857840326\n",
      "Average test loss: 0.003177331411590179\n",
      "Epoch 263/300\n",
      "Average training loss: 0.027103489195307095\n",
      "Average test loss: 0.003131117331277993\n",
      "Epoch 264/300\n",
      "Average training loss: 0.027104289483692912\n",
      "Average test loss: 0.0032401319119251437\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026974801359905137\n",
      "Average test loss: 0.003177277587147223\n",
      "Epoch 268/300\n",
      "Average training loss: 0.026966089867883257\n",
      "Average test loss: 0.003108768435402049\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02693957167201572\n",
      "Average test loss: 0.0031977707220034466\n",
      "Epoch 270/300\n",
      "Average training loss: 0.026903247055080202\n",
      "Average test loss: 0.0031330290703723827\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02706182627545463\n",
      "Average test loss: 0.003147322055366304\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02686095248328315\n",
      "Average test loss: 0.003212805774166352\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02686948903567261\n",
      "Average test loss: 0.003154706832435396\n",
      "Epoch 274/300\n",
      "Average training loss: 0.026808543698655235\n",
      "Average test loss: 0.0032147461714016065\n",
      "Epoch 275/300\n",
      "Average training loss: 0.026841988484064737\n",
      "Average test loss: 0.0031872942934019696\n",
      "Epoch 276/300\n",
      "Average training loss: 0.026741419452759953\n",
      "Average test loss: 0.0032074706552343234\n",
      "Epoch 277/300\n",
      "Average training loss: 0.026731621710790528\n",
      "Average test loss: 0.003174074305014478\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02672240781121784\n",
      "Average test loss: 0.003187670356490546\n",
      "Epoch 279/300\n",
      "Average training loss: 0.026682683371835283\n",
      "Average test loss: 0.003162345298876365\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026661243927147655\n",
      "Average test loss: 0.003254199085964097\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02665377459095584\n",
      "Average test loss: 0.003181965512327022\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02674763734638691\n",
      "Average test loss: 0.003160515713195006\n",
      "Epoch 283/300\n",
      "Average training loss: 0.026625746801495553\n",
      "Average test loss: 0.0032191118233733705\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0265420020851824\n",
      "Average test loss: 0.003236480909710129\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026546429024802313\n",
      "Average test loss: 0.003245451679142813\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02656119206382169\n",
      "Average test loss: 0.0032757255392563012\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026497588659326235\n",
      "Average test loss: 0.003170775799908572\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02647195319665803\n",
      "Average test loss: 0.003215984488113059\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02649389877749814\n",
      "Average test loss: 0.0033210088436802228\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02652160084578726\n",
      "Average test loss: 0.0031133552866263524\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026589871464504138\n",
      "Average test loss: 0.0032067488903800645\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02637674609820048\n",
      "Average test loss: 0.003204752198110024\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02632451070182853\n",
      "Average test loss: 0.003138515300841795\n",
      "Epoch 294/300\n",
      "Average training loss: 0.026365788719720312\n",
      "Average test loss: 0.0031398040637787847\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02630037476784653\n",
      "Average test loss: 0.003227661318249173\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02646906760500537\n",
      "Average test loss: 0.003197331508828534\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02630935408588913\n",
      "Average test loss: 0.0032705620725949607\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02624122132940425\n",
      "Average test loss: 0.0031379866279247735\n",
      "Epoch 299/300\n",
      "Average training loss: 0.026263434504469236\n",
      "Average test loss: 0.003201798505667183\n",
      "Epoch 300/300\n",
      "Average training loss: 0.026196616050269867\n",
      "Average test loss: 0.0031578987991023395\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.6023900470733645\n",
      "Average test loss: 0.006955479823466804\n",
      "Epoch 2/300\n",
      "Average training loss: 3.168755576663547\n",
      "Average test loss: 0.019796773293779955\n",
      "Epoch 3/300\n",
      "Average training loss: 2.740356602138943\n",
      "Average test loss: 0.00418727870285511\n",
      "Epoch 4/300\n",
      "Average training loss: 2.474955118391249\n",
      "Average test loss: 0.0037621528237230246\n",
      "Epoch 5/300\n",
      "Average training loss: 2.280430123647054\n",
      "Average test loss: 0.003555579643075665\n",
      "Epoch 6/300\n",
      "Average training loss: 2.1238831581539577\n",
      "Average test loss: 0.0033776984179599416\n",
      "Epoch 7/300\n",
      "Average training loss: 1.993334713406033\n",
      "Average test loss: 0.0033133549835118984\n",
      "Epoch 8/300\n",
      "Average training loss: 1.880439243528578\n",
      "Average test loss: 0.0032821227227234177\n",
      "Epoch 9/300\n",
      "Average training loss: 1.7752160980436538\n",
      "Average test loss: 0.003072937013167474\n",
      "Epoch 10/300\n",
      "Average training loss: 1.680466118282742\n",
      "Average test loss: 0.003135698284953833\n",
      "Epoch 11/300\n",
      "Average training loss: 1.5876199488110012\n",
      "Average test loss: 0.002920030685659084\n",
      "Epoch 12/300\n",
      "Average training loss: 1.5019855303234524\n",
      "Average test loss: 0.002850209903385904\n",
      "Epoch 13/300\n",
      "Average training loss: 1.4195261697769166\n",
      "Average test loss: 0.0028300889852560227\n",
      "Epoch 14/300\n",
      "Average training loss: 1.3448395213021171\n",
      "Average test loss: 0.00270995014336788\n",
      "Epoch 15/300\n",
      "Average training loss: 1.2723409323162502\n",
      "Average test loss: 0.002658530566530923\n",
      "Epoch 16/300\n",
      "Average training loss: 1.2052147980795966\n",
      "Average test loss: 0.002591844109611379\n",
      "Epoch 17/300\n",
      "Average training loss: 1.1400095914204915\n",
      "Average test loss: 0.0026553692830105623\n",
      "Epoch 18/300\n",
      "Average training loss: 1.0787578068839179\n",
      "Average test loss: 0.002522415156993601\n",
      "Epoch 19/300\n",
      "Average training loss: 1.0194789477454291\n",
      "Average test loss: 0.002451482909835047\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9625024564531114\n",
      "Average test loss: 0.002399144133138988\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9088394192589654\n",
      "Average test loss: 0.0023910523601290254\n",
      "Epoch 22/300\n",
      "Average training loss: 0.8564149814711677\n",
      "Average test loss: 0.0023648761579145986\n",
      "Epoch 23/300\n",
      "Average training loss: 0.8060018051465352\n",
      "Average test loss: 0.0023052608816780977\n",
      "Epoch 24/300\n",
      "Average training loss: 0.7548513425191243\n",
      "Average test loss: 0.00229527814913955\n",
      "Epoch 25/300\n",
      "Average training loss: 0.7057041124237908\n",
      "Average test loss: 0.0022967533870703644\n",
      "Epoch 26/300\n",
      "Average training loss: 0.6585559121237861\n",
      "Average test loss: 0.0022562166831145685\n",
      "Epoch 27/300\n",
      "Average training loss: 0.6160133322079976\n",
      "Average test loss: 0.0022533720509252614\n",
      "Epoch 28/300\n",
      "Average training loss: 0.573703036096361\n",
      "Average test loss: 0.0022696039711849556\n",
      "Epoch 29/300\n",
      "Average training loss: 0.5349687926504347\n",
      "Average test loss: 0.0022173229596681066\n",
      "Epoch 30/300\n",
      "Average training loss: 0.4966736004617479\n",
      "Average test loss: 0.002187531634337372\n",
      "Epoch 31/300\n",
      "Average training loss: 0.4590706690682305\n",
      "Average test loss: 0.002181603069106738\n",
      "Epoch 32/300\n",
      "Average training loss: 0.42325319560368857\n",
      "Average test loss: 0.002295905036230882\n",
      "Epoch 33/300\n",
      "Average training loss: 0.3886559467845493\n",
      "Average test loss: 0.0021222320551880533\n",
      "Epoch 34/300\n",
      "Average training loss: 0.35539545525444877\n",
      "Average test loss: 0.002117722241828839\n",
      "Epoch 35/300\n",
      "Average training loss: 0.3234223781691657\n",
      "Average test loss: 0.002196300273450712\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2936369211938646\n",
      "Average test loss: 0.0021211126218549908\n",
      "Epoch 37/300\n",
      "Average training loss: 0.2658944660557641\n",
      "Average test loss: 0.002187542316607303\n",
      "Epoch 38/300\n",
      "Average training loss: 0.24079393557707468\n",
      "Average test loss: 0.002151009472500947\n",
      "Epoch 39/300\n",
      "Average training loss: 0.21673573033014934\n",
      "Average test loss: 0.002116024816615714\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1940118179453744\n",
      "Average test loss: 0.002079309264301426\n",
      "Epoch 41/300\n",
      "Average training loss: 0.17162584630648295\n",
      "Average test loss: 0.0020757128999878962\n",
      "Epoch 42/300\n",
      "Average training loss: 0.15001858551634684\n",
      "Average test loss: 0.002099056358759602\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1300054141945309\n",
      "Average test loss: 0.0020637518471727768\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1114771610432201\n",
      "Average test loss: 0.0022217820601330863\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09533220107025571\n",
      "Average test loss: 0.002141915883661972\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08154912491639456\n",
      "Average test loss: 0.002176663900208142\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07049462771746848\n",
      "Average test loss: 0.0020502028541846407\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06211812679966291\n",
      "Average test loss: 0.0021245257117682033\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0556454886926545\n",
      "Average test loss: 0.0020803388882842328\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05097133973240852\n",
      "Average test loss: 0.002151076256401009\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04746279328564803\n",
      "Average test loss: 0.002139843362900946\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04303212297293875\n",
      "Average test loss: 0.0021131624467670917\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04139048355817795\n",
      "Average test loss: 0.0020494309338844486\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04021626654598448\n",
      "Average test loss: 0.00203868703968409\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03925688759154743\n",
      "Average test loss: 0.002090285137709644\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03773729972044627\n",
      "Average test loss: 0.002045014638867643\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03712685128715303\n",
      "Average test loss: 0.0020585399692257247\n",
      "Epoch 60/300\n",
      "Average training loss: 0.036869797501299116\n",
      "Average test loss: 0.0020527978751601443\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03623067951699098\n",
      "Average test loss: 0.002140838338062167\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03574381659759415\n",
      "Average test loss: 0.00222428593536218\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03533481351534525\n",
      "Average test loss: 0.0020593319891227617\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03501967800325818\n",
      "Average test loss: 0.002045941295205719\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03471652791566319\n",
      "Average test loss: 0.0020626432314101192\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03434165562523736\n",
      "Average test loss: 0.002108016739082005\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03399170007639461\n",
      "Average test loss: 0.0020896177821689184\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03372947715719541\n",
      "Average test loss: 0.002046253867240416\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03307520630955696\n",
      "Average test loss: 0.002139915860671964\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03283295627931754\n",
      "Average test loss: 0.002026832825400763\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03252760499715805\n",
      "Average test loss: 0.0020809809161971013\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03222287098401123\n",
      "Average test loss: 0.0020553025376672546\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03204272442228264\n",
      "Average test loss: 0.0020524172000586987\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03162314238978757\n",
      "Average test loss: 0.0020628787591639493\n",
      "Epoch 76/300\n",
      "Average training loss: 0.031517908268504674\n",
      "Average test loss: 0.002118747455999255\n",
      "Epoch 77/300\n",
      "Average training loss: 0.031123959889014563\n",
      "Average test loss: 0.0020920153296449116\n",
      "Epoch 78/300\n",
      "Average training loss: 0.030865610662433835\n",
      "Average test loss: 0.002037764364646541\n",
      "Epoch 79/300\n",
      "Average training loss: 0.030560606428318555\n",
      "Average test loss: 0.002101319099776447\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03037217789226108\n",
      "Average test loss: 0.0021569931304289236\n",
      "Epoch 81/300\n",
      "Average training loss: 0.030079542628592915\n",
      "Average test loss: 0.002010286878483991\n",
      "Epoch 82/300\n",
      "Average training loss: 0.029806268486711713\n",
      "Average test loss: 0.0022829498801794318\n",
      "Epoch 83/300\n",
      "Average training loss: 0.029572530623939303\n",
      "Average test loss: 0.0021526132526083126\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02946942361858156\n",
      "Average test loss: 0.0020623553992869955\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02913315124809742\n",
      "Average test loss: 0.0020542696863412855\n",
      "Epoch 86/300\n",
      "Average training loss: 0.028943824753165245\n",
      "Average test loss: 0.0020924277572582166\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02867473114364677\n",
      "Average test loss: 0.0020724364697105356\n",
      "Epoch 88/300\n",
      "Average training loss: 0.028433322050505215\n",
      "Average test loss: 0.0020960404934982457\n",
      "Epoch 89/300\n",
      "Average training loss: 0.028200576048758294\n",
      "Average test loss: 0.0021790023773080773\n",
      "Epoch 90/300\n",
      "Average training loss: 0.028272319369845922\n",
      "Average test loss: 0.0020764387966030174\n",
      "Epoch 91/300\n",
      "Average training loss: 0.027950629957848125\n",
      "Average test loss: 0.0021591722725166217\n",
      "Epoch 92/300\n",
      "Average training loss: 0.027746445606152217\n",
      "Average test loss: 0.002067472677263949\n",
      "Epoch 93/300\n",
      "Average training loss: 0.027743785838286083\n",
      "Average test loss: 0.0020618077508277365\n",
      "Epoch 94/300\n",
      "Average training loss: 0.027420809159676235\n",
      "Average test loss: 0.0020456233186026416\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02723975234727065\n",
      "Average test loss: 0.0021317165311839845\n",
      "Epoch 96/300\n",
      "Average training loss: 0.026917660592330826\n",
      "Average test loss: 0.002175568442274299\n",
      "Epoch 97/300\n",
      "Average training loss: 0.026972591696514024\n",
      "Average test loss: 0.002146128062986665\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026781458949049312\n",
      "Average test loss: 0.0020875153312873507\n",
      "Epoch 99/300\n",
      "Average training loss: 0.026649689990613196\n",
      "Average test loss: 0.0021655752880291807\n",
      "Epoch 100/300\n",
      "Average training loss: 0.026392810927497016\n",
      "Average test loss: 0.0020697593107405635\n",
      "Epoch 101/300\n",
      "Average training loss: 0.026235096923179095\n",
      "Average test loss: 0.00256155646364722\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02616756583419111\n",
      "Average test loss: 0.0021991044156667258\n",
      "Epoch 103/300\n",
      "Average training loss: 0.026097588209642305\n",
      "Average test loss: 0.0021789139600263702\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02587553783092234\n",
      "Average test loss: 0.0021311689259277448\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02576959054834313\n",
      "Average test loss: 0.002182362603023648\n",
      "Epoch 106/300\n",
      "Average training loss: 0.025621199303203158\n",
      "Average test loss: 0.0023883566836723023\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02562409807741642\n",
      "Average test loss: 0.002114106739457283\n",
      "Epoch 108/300\n",
      "Average training loss: 0.025404327286614313\n",
      "Average test loss: 0.0020878930893830127\n",
      "Epoch 109/300\n",
      "Average training loss: 0.025188244667318133\n",
      "Average test loss: 0.002244156308679117\n",
      "Epoch 110/300\n",
      "Average training loss: 0.025160894180337588\n",
      "Average test loss: 0.0020862159724864696\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02504567112856441\n",
      "Average test loss: 0.0021765126412113507\n",
      "Epoch 112/300\n",
      "Average training loss: 0.024990572392940523\n",
      "Average test loss: 0.002175534747954872\n",
      "Epoch 113/300\n",
      "Average training loss: 0.024751004002160498\n",
      "Average test loss: 0.0022607699409127237\n",
      "Epoch 114/300\n",
      "Average training loss: 0.024692633466588128\n",
      "Average test loss: 0.0021492013226573664\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02461420471800698\n",
      "Average test loss: 0.0021656581902255614\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02464210524658362\n",
      "Average test loss: 0.0021429600537651115\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02441935960120625\n",
      "Average test loss: 0.0020901343872149784\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02424895567033026\n",
      "Average test loss: 0.0021101632584921187\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02418699496984482\n",
      "Average test loss: 0.0020908305073777836\n",
      "Epoch 120/300\n",
      "Average training loss: 0.024194973587989808\n",
      "Average test loss: 0.0020929744500252936\n",
      "Epoch 121/300\n",
      "Average training loss: 0.024008259440461796\n",
      "Average test loss: 0.0021390138517858254\n",
      "Epoch 122/300\n",
      "Average training loss: 0.023904283207323816\n",
      "Average test loss: 0.002228351596328947\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023852462745375103\n",
      "Average test loss: 0.002149472607506646\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023806880361504024\n",
      "Average test loss: 0.0021114312452781532\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023710204465521707\n",
      "Average test loss: 0.0020784888971183036\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02366732445690367\n",
      "Average test loss: 0.002873929326939914\n",
      "Epoch 127/300\n",
      "Average training loss: 0.023542126465174888\n",
      "Average test loss: 0.0022012601933545537\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023437469598319795\n",
      "Average test loss: 0.0022000787659651705\n",
      "Epoch 129/300\n",
      "Average training loss: 0.023391187998983595\n",
      "Average test loss: 0.0021527182122485504\n",
      "Epoch 130/300\n",
      "Average training loss: 0.023309225832422575\n",
      "Average test loss: 0.0021470319591462613\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023253944223125777\n",
      "Average test loss: 0.0021332161338585946\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02322755841082997\n",
      "Average test loss: 0.002207932743554314\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02306892974509133\n",
      "Average test loss: 0.0021507991298826204\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02305230125784874\n",
      "Average test loss: 0.0021604439568602375\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02291002835664484\n",
      "Average test loss: 0.0022449384294450283\n",
      "Epoch 136/300\n",
      "Average training loss: 0.022867074769404198\n",
      "Average test loss: 0.0022341780006471606\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02279978770017624\n",
      "Average test loss: 0.002145228087902069\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022785941797826025\n",
      "Average test loss: 0.002206894434471097\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022671219799253677\n",
      "Average test loss: 0.002230290895534886\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022625751530130703\n",
      "Average test loss: 0.0022185802488691275\n",
      "Epoch 141/300\n",
      "Average training loss: 0.022579963255259725\n",
      "Average test loss: 0.002125704238812129\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02254677710764938\n",
      "Average test loss: 0.0021678247653568786\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02246247103313605\n",
      "Average test loss: 0.002157382630225685\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022387482381529277\n",
      "Average test loss: 0.002125793242413137\n",
      "Epoch 145/300\n",
      "Average training loss: 0.022277806810206836\n",
      "Average test loss: 0.0021984092538348504\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02227750672896703\n",
      "Average test loss: 0.0022468571554765935\n",
      "Epoch 147/300\n",
      "Average training loss: 0.022164549238151973\n",
      "Average test loss: 0.002207643738637368\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022197690683934422\n",
      "Average test loss: 0.0021434199288487433\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022089833122160698\n",
      "Average test loss: 0.0022116550064335268\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02210010662343767\n",
      "Average test loss: 0.0022512641224182316\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021981093025869793\n",
      "Average test loss: 0.002167036489997473\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021872824036412768\n",
      "Average test loss: 0.0021581477456622655\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02187950685620308\n",
      "Average test loss: 0.002156111335588826\n",
      "Epoch 154/300\n",
      "Average training loss: 0.021863691217369502\n",
      "Average test loss: 0.00224113112129271\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02180141151862012\n",
      "Average test loss: 0.002179052476046814\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021732804474731287\n",
      "Average test loss: 0.0022163656164581576\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021688464219371477\n",
      "Average test loss: 0.0021632899375011523\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02163116484052605\n",
      "Average test loss: 0.002221267723788818\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021521127538548575\n",
      "Average test loss: 0.0022016634688609175\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0215322351720598\n",
      "Average test loss: 0.003129671809470488\n",
      "Epoch 161/300\n",
      "Average training loss: 0.021582860683401425\n",
      "Average test loss: 0.002241407618133558\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021482676102055443\n",
      "Average test loss: 0.002229991664075189\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021409036679400337\n",
      "Average test loss: 0.0023205688589562974\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02140685801539156\n",
      "Average test loss: 0.002232741954839892\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021298435881733895\n",
      "Average test loss: 0.0021734713880966106\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02130805143217246\n",
      "Average test loss: 0.0022468776470050216\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021230135099755392\n",
      "Average test loss: 0.0021480780007938544\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021194597686330478\n",
      "Average test loss: 0.00220392866515451\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021194049469298787\n",
      "Average test loss: 0.0021932088252570895\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021116356063220237\n",
      "Average test loss: 0.00217019432493382\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021051581621170042\n",
      "Average test loss: 0.00221249499441021\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0211939832849635\n",
      "Average test loss: 0.002264276271685958\n",
      "Epoch 173/300\n",
      "Average training loss: 0.020980945962998602\n",
      "Average test loss: 0.002330408638446695\n",
      "Epoch 174/300\n",
      "Average training loss: 0.020915967386629847\n",
      "Average test loss: 0.0022092829876475865\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02089555530084504\n",
      "Average test loss: 0.0021702018599543305\n",
      "Epoch 176/300\n",
      "Average training loss: 0.020859657486279805\n",
      "Average test loss: 0.002198867670674291\n",
      "Epoch 177/300\n",
      "Average training loss: 0.020799658332433964\n",
      "Average test loss: 0.0022851307207925452\n",
      "Epoch 178/300\n",
      "Average training loss: 0.020830973682304223\n",
      "Average test loss: 0.00237748761040469\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02077261631604698\n",
      "Average test loss: 0.0022747321153680482\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02075820353627205\n",
      "Average test loss: 0.0022642932160654\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02062840915388531\n",
      "Average test loss: 0.002185037011280656\n",
      "Epoch 182/300\n",
      "Average training loss: 0.020681878987285827\n",
      "Average test loss: 0.0022216599434614183\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02064799748195542\n",
      "Average test loss: 0.002173698768744038\n",
      "Epoch 184/300\n",
      "Average training loss: 0.020599262109233272\n",
      "Average test loss: 0.002314063402927584\n",
      "Epoch 185/300\n",
      "Average training loss: 0.020540637008017965\n",
      "Average test loss: 0.0022582735671765274\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02041951016916169\n",
      "Average test loss: 0.0022929377166243893\n",
      "Epoch 189/300\n",
      "Average training loss: 0.020411656676067248\n",
      "Average test loss: 0.0023912096017350754\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020380555717481506\n",
      "Average test loss: 0.0022385147454010114\n",
      "Epoch 191/300\n",
      "Average training loss: 0.020387610246737797\n",
      "Average test loss: 0.0022516111206884186\n",
      "Epoch 192/300\n",
      "Average training loss: 0.020336255100038315\n",
      "Average test loss: 0.002250376069193913\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02026666600174374\n",
      "Average test loss: 0.0022242812955131134\n",
      "Epoch 194/300\n",
      "Average training loss: 0.020250251842869653\n",
      "Average test loss: 0.002292500090888805\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020119396362039778\n",
      "Average test loss: 0.0022824062893374098\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020096422630879615\n",
      "Average test loss: 0.002224516997941666\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02016718051665359\n",
      "Average test loss: 0.002225326737285488\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020058408020271194\n",
      "Average test loss: 0.0022827163382122912\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0200501276138756\n",
      "Average test loss: 0.002221726161117355\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020018860780530504\n",
      "Average test loss: 0.002246772815369897\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020014830536312526\n",
      "Average test loss: 0.0023369871887067954\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01994878036942747\n",
      "Average test loss: 0.002215641437098384\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01994251815809144\n",
      "Average test loss: 0.002265458262628979\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01992035067660941\n",
      "Average test loss: 0.002297231052381297\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019901638164288468\n",
      "Average test loss: 0.0022527716366781128\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019864364003969563\n",
      "Average test loss: 0.0023200945291254257\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019861673376626438\n",
      "Average test loss: 0.0022616803410152593\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01989665294521385\n",
      "Average test loss: 0.00225496728780369\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019746543796526062\n",
      "Average test loss: 0.0022806038277016745\n",
      "Epoch 212/300\n",
      "Average training loss: 0.019684084940287803\n",
      "Average test loss: 0.0022538573728460407\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019741701929105654\n",
      "Average test loss: 0.002215152103971276\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019686785418126317\n",
      "Average test loss: 0.002251362885038058\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0196197574072414\n",
      "Average test loss: 0.002262971496209502\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0197140314363771\n",
      "Average test loss: 0.002260489777351419\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019548354831834633\n",
      "Average test loss: 0.002326856456696987\n",
      "Epoch 219/300\n",
      "Average training loss: 0.019505740569697488\n",
      "Average test loss: 0.002236075625547932\n",
      "Epoch 220/300\n",
      "Average training loss: 0.019537084294690026\n",
      "Average test loss: 0.002268310678915845\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019496815603640343\n",
      "Average test loss: 0.0022641716189682483\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019465121189753215\n",
      "Average test loss: 0.0022559984875842928\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019406402497655816\n",
      "Average test loss: 0.002252157557548748\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01950582447151343\n",
      "Average test loss: 0.0022485187022636332\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01940953971279992\n",
      "Average test loss: 0.0023495871619217926\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019357178987728225\n",
      "Average test loss: 0.002281155397598114\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01932685900065634\n",
      "Average test loss: 0.002282806043099198\n",
      "Epoch 229/300\n",
      "Average training loss: 0.019325184282329346\n",
      "Average test loss: 0.0023081720692829954\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019295229591429233\n",
      "Average test loss: 0.0024045846170435347\n",
      "Epoch 231/300\n",
      "Average training loss: 0.019293806930383045\n",
      "Average test loss: 0.0023857404463407065\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01921897449178828\n",
      "Average test loss: 0.0022689449783000683\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01922950321601497\n",
      "Average test loss: 0.0022777762868338163\n",
      "Epoch 235/300\n",
      "Average training loss: 0.019257279040084944\n",
      "Average test loss: 0.002253267963727315\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019148167274064487\n",
      "Average test loss: 0.0022266762018617655\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01914184775782956\n",
      "Average test loss: 0.0024729092615760036\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019119686488476065\n",
      "Average test loss: 0.0023607835579249592\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0190997539675898\n",
      "Average test loss: 0.002385231288149953\n",
      "Epoch 240/300\n",
      "Average training loss: 0.019084214549925593\n",
      "Average test loss: 0.002343307703733444\n",
      "Epoch 241/300\n",
      "Average training loss: 0.019049432105488248\n",
      "Average test loss: 0.002244579927995801\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019026269622147082\n",
      "Average test loss: 0.0022929911781102417\n",
      "Epoch 243/300\n",
      "Average training loss: 0.018983814084695447\n",
      "Average test loss: 0.0023767414945695137\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01900899781121148\n",
      "Average test loss: 0.0022353201580958236\n",
      "Epoch 245/300\n",
      "Average training loss: 0.019024522534675067\n",
      "Average test loss: 0.002297829129629665\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018969056304958133\n",
      "Average test loss: 0.0022251144788331457\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01892957351770666\n",
      "Average test loss: 0.0023154369694077304\n",
      "Epoch 248/300\n",
      "Average training loss: 0.018893959037131734\n",
      "Average test loss: 0.0022662562982489667\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01890698402788904\n",
      "Average test loss: 0.0024788566100307635\n",
      "Epoch 250/300\n",
      "Average training loss: 0.018896430132289727\n",
      "Average test loss: 0.002282685765168733\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018821115929219458\n",
      "Average test loss: 0.0023859956252078214\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018825549784633848\n",
      "Average test loss: 0.0022289632108165986\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018866654384467337\n",
      "Average test loss: 0.002414232247146881\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018802664313051436\n",
      "Average test loss: 0.00231331901707583\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01876995859377914\n",
      "Average test loss: 0.002330749508821302\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018836456151472197\n",
      "Average test loss: 0.002255739241010613\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018806047844390075\n",
      "Average test loss: 0.002330922498678168\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018702056237392954\n",
      "Average test loss: 0.0023075122728736866\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01870834198511309\n",
      "Average test loss: 0.0023826026774735913\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01868438994222217\n",
      "Average test loss: 0.002337014351867967\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01873154060708152\n",
      "Average test loss: 0.002271514147106144\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018603926233119433\n",
      "Average test loss: 0.002304090753818552\n",
      "Epoch 263/300\n",
      "Average training loss: 0.018616378544105423\n",
      "Average test loss: 0.002263893297769957\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01861152167287138\n",
      "Average test loss: 0.002312328499638372\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018595239410797756\n",
      "Average test loss: 0.0023411826036042636\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018640019862188235\n",
      "Average test loss: 0.0022771456990804935\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018528054111533694\n",
      "Average test loss: 0.0022967654023733405\n",
      "Epoch 268/300\n",
      "Average training loss: 0.018525814632574716\n",
      "Average test loss: 0.0023097075410187244\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0185360689808925\n",
      "Average test loss: 0.002205156666847567\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018510415405035018\n",
      "Average test loss: 0.00247217930915455\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01849593277606699\n",
      "Average test loss: 0.002365478653771182\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018514945816662576\n",
      "Average test loss: 0.002281857655901048\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01850417309999466\n",
      "Average test loss: 0.0023938182801422144\n",
      "Epoch 274/300\n",
      "Average training loss: 0.018447410298718346\n",
      "Average test loss: 0.002291799927958184\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018434132190214263\n",
      "Average test loss: 0.0023188201369096838\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01839613639977243\n",
      "Average test loss: 0.002340112187796169\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018397014876206715\n",
      "Average test loss: 0.0022763378268314733\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018373638631569014\n",
      "Average test loss: 0.0023069398823297687\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018354375773006016\n",
      "Average test loss: 0.0023449549351094497\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01832779595752557\n",
      "Average test loss: 0.0023985746338342625\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018387482535507944\n",
      "Average test loss: 0.002323179624457326\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01840983316799005\n",
      "Average test loss: 0.0023126687188115384\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01834181920190652\n",
      "Average test loss: 0.0023523887884285715\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018248164128926065\n",
      "Average test loss: 0.002282209215271804\n",
      "Epoch 285/300\n",
      "Average training loss: 0.018250860941078927\n",
      "Average test loss: 0.0022972103021004133\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01825423234866725\n",
      "Average test loss: 0.0022752108169305655\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01821410985291004\n",
      "Average test loss: 0.0023706914009526373\n",
      "Epoch 288/300\n",
      "Average training loss: 0.018301243210832277\n",
      "Average test loss: 0.0023706275451307497\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018189789856473605\n",
      "Average test loss: 0.002340116970551511\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01817232206794951\n",
      "Average test loss: 0.002350359617525505\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018205263156029914\n",
      "Average test loss: 0.002306250556682547\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01820195866955651\n",
      "Average test loss: 0.0023583096033997007\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01815538015299373\n",
      "Average test loss: 0.002361271943897009\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01814925990667608\n",
      "Average test loss: 0.0023151625339976616\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0181214791983366\n",
      "Average test loss: 0.0023289880255858105\n",
      "Epoch 296/300\n",
      "Average training loss: 0.018112193806303873\n",
      "Average test loss: 0.002299142799236708\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018078286627928417\n",
      "Average test loss: 0.0023455841531977058\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018076995541652045\n",
      "Average test loss: 0.0023218062373085153\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018045114450984532\n",
      "Average test loss: 0.0023749712480025157\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018071872108512455\n",
      "Average test loss: 0.0022853241372439597\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.1700175931718615\n",
      "Average test loss: 0.01328768077492714\n",
      "Epoch 2/300\n",
      "Average training loss: 2.872700948291355\n",
      "Average test loss: 0.00394314855730368\n",
      "Epoch 3/300\n",
      "Average training loss: 2.5053463219536676\n",
      "Average test loss: 0.0035455706947379643\n",
      "Epoch 4/300\n",
      "Average training loss: 2.268535615497165\n",
      "Average test loss: 0.007054270754257838\n",
      "Epoch 5/300\n",
      "Average training loss: 2.0988133131663003\n",
      "Average test loss: 0.0049057147616727486\n",
      "Epoch 6/300\n",
      "Average training loss: 1.9598353379567464\n",
      "Average test loss: 0.002772782196601232\n",
      "Epoch 7/300\n",
      "Average training loss: 1.8412794825236003\n",
      "Average test loss: 0.0026823580351968608\n",
      "Epoch 8/300\n",
      "Average training loss: 1.7307439842224122\n",
      "Average test loss: 0.0025597708760243323\n",
      "Epoch 9/300\n",
      "Average training loss: 1.6359183045493233\n",
      "Average test loss: 0.0024254059227597382\n",
      "Epoch 10/300\n",
      "Average training loss: 1.547662723965115\n",
      "Average test loss: 0.0023868751600384713\n",
      "Epoch 11/300\n",
      "Average training loss: 1.4681781674491035\n",
      "Average test loss: 0.0024228285513818266\n",
      "Epoch 12/300\n",
      "Average training loss: 1.3922359186808269\n",
      "Average test loss: 0.0021213779526038303\n",
      "Epoch 13/300\n",
      "Average training loss: 1.3188185953564113\n",
      "Average test loss: 0.0021346167863036196\n",
      "Epoch 14/300\n",
      "Average training loss: 1.2505219522052342\n",
      "Average test loss: 0.002052169115593036\n",
      "Epoch 15/300\n",
      "Average training loss: 1.1844087885750665\n",
      "Average test loss: 0.0020190238287775882\n",
      "Epoch 16/300\n",
      "Average training loss: 1.1211353533003066\n",
      "Average test loss: 0.0019049615094231234\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0601713466644287\n",
      "Average test loss: 0.0019237876975287994\n",
      "Epoch 18/300\n",
      "Average training loss: 1.0007204318576388\n",
      "Average test loss: 0.0018145739948894415\n",
      "Epoch 19/300\n",
      "Average training loss: 0.9452756110827129\n",
      "Average test loss: 0.001796467633710967\n",
      "Epoch 20/300\n",
      "Average training loss: 0.8917188217904832\n",
      "Average test loss: 0.0017803527451016837\n",
      "Epoch 21/300\n",
      "Average training loss: 0.8403983952734205\n",
      "Average test loss: 0.0017865026030275558\n",
      "Epoch 22/300\n",
      "Average training loss: 0.7931712055206299\n",
      "Average test loss: 0.0017206537303411297\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7484889931678772\n",
      "Average test loss: 0.0017246141804175244\n",
      "Epoch 24/300\n",
      "Average training loss: 0.704285649617513\n",
      "Average test loss: 0.0016811680945878228\n",
      "Epoch 25/300\n",
      "Average training loss: 0.6625140342712402\n",
      "Average test loss: 0.0016503848700473705\n",
      "Epoch 26/300\n",
      "Average training loss: 0.6226547079616123\n",
      "Average test loss: 0.0016153585758681098\n",
      "Epoch 27/300\n",
      "Average training loss: 0.5843810894224378\n",
      "Average test loss: 0.001641985763795674\n",
      "Epoch 28/300\n",
      "Average training loss: 0.5464143661393059\n",
      "Average test loss: 0.0015999340209075146\n",
      "Epoch 29/300\n",
      "Average training loss: 0.5113500133620368\n",
      "Average test loss: 0.001562674256766008\n",
      "Epoch 30/300\n",
      "Average training loss: 0.4775085164705912\n",
      "Average test loss: 0.0015444057462736965\n",
      "Epoch 31/300\n",
      "Average training loss: 0.44492208078172474\n",
      "Average test loss: 0.0015413203494002421\n",
      "Epoch 32/300\n",
      "Average training loss: 0.41341294309828014\n",
      "Average test loss: 0.0015405146968033578\n",
      "Epoch 33/300\n",
      "Average training loss: 0.38272026244799295\n",
      "Average test loss: 0.0015505987089127302\n",
      "Epoch 34/300\n",
      "Average training loss: 0.35313261172506544\n",
      "Average test loss: 0.0015629735299282604\n",
      "Epoch 35/300\n",
      "Average training loss: 0.3237309847142961\n",
      "Average test loss: 0.001507017576135695\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2954403569963243\n",
      "Average test loss: 0.0014608755855717592\n",
      "Epoch 37/300\n",
      "Average training loss: 0.2688223909272088\n",
      "Average test loss: 0.0014480181192565295\n",
      "Epoch 38/300\n",
      "Average training loss: 0.2425763292842441\n",
      "Average test loss: 0.0014497070027929212\n",
      "Epoch 39/300\n",
      "Average training loss: 0.2185180834001965\n",
      "Average test loss: 0.0015305305849760771\n",
      "Epoch 40/300\n",
      "Average training loss: 0.19605606849988302\n",
      "Average test loss: 0.0014597757320023246\n",
      "Epoch 41/300\n",
      "Average training loss: 0.17468558502197265\n",
      "Average test loss: 0.0014514380135159525\n",
      "Epoch 42/300\n",
      "Average training loss: 0.15454470058282216\n",
      "Average test loss: 0.0014357800346074833\n",
      "Epoch 43/300\n",
      "Average training loss: 0.13496364475621117\n",
      "Average test loss: 0.0014651331953290435\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11576666486263275\n",
      "Average test loss: 0.0014377176167132955\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09801539996597503\n",
      "Average test loss: 0.001436006603969468\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08171751315726174\n",
      "Average test loss: 0.0014313123629738887\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06905099756850136\n",
      "Average test loss: 0.0014190380150038335\n",
      "Epoch 48/300\n",
      "Average training loss: 0.058755414287249244\n",
      "Average test loss: 0.0014586614405529366\n",
      "Epoch 49/300\n",
      "Average training loss: 0.051115330004029805\n",
      "Average test loss: 0.0014528462580508657\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04532146365775002\n",
      "Average test loss: 0.0014143004075934489\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04111459274424447\n",
      "Average test loss: 0.0013950531801415814\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03783027684357431\n",
      "Average test loss: 0.001379348758711583\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03546350480450524\n",
      "Average test loss: 0.0013873237586683696\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03355300981137488\n",
      "Average test loss: 0.009773870135760969\n",
      "Epoch 55/300\n",
      "Average training loss: 0.032125526898437076\n",
      "Average test loss: 0.0013975530849355791\n",
      "Epoch 56/300\n",
      "Average training loss: 0.031038766978515517\n",
      "Average test loss: 0.00142956520906753\n",
      "Epoch 57/300\n",
      "Average training loss: 0.029998643186357286\n",
      "Average test loss: 0.0014218577407300473\n",
      "Epoch 58/300\n",
      "Average training loss: 0.029264061498973103\n",
      "Average test loss: 0.001399086187283198\n",
      "Epoch 59/300\n",
      "Average training loss: 0.028651174750592975\n",
      "Average test loss: 0.0014706020169994898\n",
      "Epoch 60/300\n",
      "Average training loss: 0.028097813218832014\n",
      "Average test loss: 0.0014142623217776418\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02765232427749369\n",
      "Average test loss: 0.001387310776549081\n",
      "Epoch 62/300\n",
      "Average training loss: 0.027227006227605873\n",
      "Average test loss: 0.0014436534517962073\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02677116581466463\n",
      "Average test loss: 0.001376965507451031\n",
      "Epoch 64/300\n",
      "Average training loss: 0.026406957172685198\n",
      "Average test loss: 0.001427260593407684\n",
      "Epoch 65/300\n",
      "Average training loss: 0.026091981717281873\n",
      "Average test loss: 0.001446619517583814\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02573963164124224\n",
      "Average test loss: 0.0015020211218959756\n",
      "Epoch 67/300\n",
      "Average training loss: 0.025548079811864428\n",
      "Average test loss: 0.0014079077373155289\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02518832610381974\n",
      "Average test loss: 0.001399534376234644\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024843100567658743\n",
      "Average test loss: 0.0014292052002209757\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024613378334376546\n",
      "Average test loss: 0.0014119949873743786\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02431511693365044\n",
      "Average test loss: 0.0013895988925877545\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02403224487023221\n",
      "Average test loss: 0.0014431756906625297\n",
      "Epoch 73/300\n",
      "Average training loss: 0.023834327114952935\n",
      "Average test loss: 0.0015273526513224674\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02354769582549731\n",
      "Average test loss: 0.001391410358560582\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023311092022392486\n",
      "Average test loss: 0.001445415671914816\n",
      "Epoch 76/300\n",
      "Average training loss: 0.023052519168290826\n",
      "Average test loss: 0.0014425693470984696\n",
      "Epoch 77/300\n",
      "Average training loss: 0.022730769948826897\n",
      "Average test loss: 0.0014092727314887776\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02243991862403022\n",
      "Average test loss: 0.0014090722480581866\n",
      "Epoch 79/300\n",
      "Average training loss: 0.022254912106527223\n",
      "Average test loss: 0.0015182505895694096\n",
      "Epoch 80/300\n",
      "Average training loss: 0.021982131169901952\n",
      "Average test loss: 0.0014400257469258375\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02179517900860972\n",
      "Average test loss: 0.0014894807191772593\n",
      "Epoch 82/300\n",
      "Average training loss: 0.021581727064318126\n",
      "Average test loss: 0.001425808236002922\n",
      "Epoch 83/300\n",
      "Average training loss: 0.021296978059742185\n",
      "Average test loss: 0.0014285944438953365\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02114202259729306\n",
      "Average test loss: 0.0014974899997727738\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020818340376019477\n",
      "Average test loss: 0.0014429743956360552\n",
      "Epoch 86/300\n",
      "Average training loss: 0.020668767304884064\n",
      "Average test loss: 0.0014118648905617494\n",
      "Epoch 87/300\n",
      "Average training loss: 0.020562663187583288\n",
      "Average test loss: 0.0014651217974929345\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02026907677286201\n",
      "Average test loss: 0.001451009023624162\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02010289296425051\n",
      "Average test loss: 0.0014277045818873579\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01996324422624376\n",
      "Average test loss: 0.0014338094216460983\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01973822368019157\n",
      "Average test loss: 0.0014779198316650258\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019616728961467742\n",
      "Average test loss: 0.0014358848825924927\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019437130063772203\n",
      "Average test loss: 0.0014106727845759855\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019262771958278285\n",
      "Average test loss: 0.0014155773363179632\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019097046364512708\n",
      "Average test loss: 0.0014558268222543927\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019028905810581313\n",
      "Average test loss: 0.001482481951928801\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018848932009604243\n",
      "Average test loss: 0.0014441936194068857\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018755647616253958\n",
      "Average test loss: 0.0014703095639124512\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01861965050631099\n",
      "Average test loss: 0.0015100439184655746\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018472021683222716\n",
      "Average test loss: 0.001498191559066375\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018353123072120878\n",
      "Average test loss: 0.0014892209344026116\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018243780282636485\n",
      "Average test loss: 0.0014716442688885662\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018181364706820913\n",
      "Average test loss: 0.001532573238429096\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018083987739351062\n",
      "Average test loss: 0.001487299686918656\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017903040086229643\n",
      "Average test loss: 0.0014829718669255574\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017789667235480415\n",
      "Average test loss: 0.001478439015025894\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017781572706997394\n",
      "Average test loss: 0.0014883883529239232\n",
      "Epoch 108/300\n",
      "Average training loss: 0.017642090565628477\n",
      "Average test loss: 0.0014321830177472696\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017530013263639475\n",
      "Average test loss: 0.0015127610856046278\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01740419206354353\n",
      "Average test loss: 0.0014789653514615364\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017631744298670027\n",
      "Average test loss: 0.0014579349944574965\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01741234132813083\n",
      "Average test loss: 0.0015099583826959134\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01716416191558043\n",
      "Average test loss: 0.001422930400301185\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017128625232312414\n",
      "Average test loss: 0.0014096654483841526\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017001530824436083\n",
      "Average test loss: 0.0014943509583051006\n",
      "Epoch 116/300\n",
      "Average training loss: 0.016965859784020317\n",
      "Average test loss: 0.0014411763778577249\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01689860954052872\n",
      "Average test loss: 0.0015298733560161458\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016784195457067753\n",
      "Average test loss: 0.0014929488148126338\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01673347812311517\n",
      "Average test loss: 0.001430003181927734\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016677621767752702\n",
      "Average test loss: 0.0014784015682008532\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016556007909278074\n",
      "Average test loss: 0.0014899285189393493\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01650019229037894\n",
      "Average test loss: 0.0014768128008581698\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016475092373788358\n",
      "Average test loss: 0.0014796727936611407\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01638114222884178\n",
      "Average test loss: 0.0015031534652743075\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016297997819052803\n",
      "Average test loss: 0.0014427389467859433\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016238904299835363\n",
      "Average test loss: 0.001549209039658308\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016255425324042637\n",
      "Average test loss: 0.0015158667353292307\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01620909586879942\n",
      "Average test loss: 0.001521555602343546\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016062214283479585\n",
      "Average test loss: 0.001547244102280173\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01598449673006932\n",
      "Average test loss: 0.0015226612763686313\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01593773540854454\n",
      "Average test loss: 0.0015047366264690127\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015890352259907457\n",
      "Average test loss: 0.0014885444591442743\n",
      "Epoch 133/300\n",
      "Average training loss: 0.015908247092531788\n",
      "Average test loss: 0.0015452679148875176\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015863584122724005\n",
      "Average test loss: 0.001481178710444106\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015756890800264146\n",
      "Average test loss: 0.0014312900417587824\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015677653841674327\n",
      "Average test loss: 0.0015064248147731026\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01565882635447714\n",
      "Average test loss: 0.001588064146745536\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015604981967144542\n",
      "Average test loss: 0.0014601462962519792\n",
      "Epoch 139/300\n",
      "Average training loss: 0.015528781038191584\n",
      "Average test loss: 0.0014674017927091983\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015571953516039584\n",
      "Average test loss: 0.0016316106639181573\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01548654325803121\n",
      "Average test loss: 0.0015225550800354944\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015458011298543878\n",
      "Average test loss: 0.0014746102520989048\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015385128443439802\n",
      "Average test loss: 0.0016133011697481076\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015317704489661587\n",
      "Average test loss: 0.0015676303880496159\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01526789579540491\n",
      "Average test loss: 0.0014956831828587586\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01528210262623098\n",
      "Average test loss: 0.0016571476589888334\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015189197063446045\n",
      "Average test loss: 0.0015770070490737757\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015171237238579327\n",
      "Average test loss: 0.0015051314109522436\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015119571315745513\n",
      "Average test loss: 0.0014932664734207922\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015123561845885383\n",
      "Average test loss: 0.001597297177132633\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01505138517336713\n",
      "Average test loss: 0.0016104290780300896\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01508134301751852\n",
      "Average test loss: 0.0015709160833309093\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01495742314805587\n",
      "Average test loss: 0.001451039179124766\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01498197870535983\n",
      "Average test loss: 0.001536985227269017\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014896133306953642\n",
      "Average test loss: 0.0015278710410412816\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014862586161328687\n",
      "Average test loss: 0.0015052749386264218\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014844165474176408\n",
      "Average test loss: 0.0015732701069985826\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01480658049798674\n",
      "Average test loss: 0.0015512560886434383\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01478055410583814\n",
      "Average test loss: 0.0015343984949092071\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014746507893006007\n",
      "Average test loss: 0.0015139293499911825\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014763102356758383\n",
      "Average test loss: 0.0015441693640831445\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014680666509601805\n",
      "Average test loss: 0.0015219456618651747\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01462513301273187\n",
      "Average test loss: 0.0015391221714930402\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014628066879179743\n",
      "Average test loss: 0.0015327108210056192\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014548619387878312\n",
      "Average test loss: 0.0015165763215886222\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014534959285623497\n",
      "Average test loss: 0.0017443534575609697\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014484000715116659\n",
      "Average test loss: 0.0015073985505021281\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014455749367674192\n",
      "Average test loss: 0.0015917884937177102\n",
      "Epoch 169/300\n",
      "Average training loss: 0.014477960317499108\n",
      "Average test loss: 0.0015642445805068646\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014395635141266717\n",
      "Average test loss: 0.0015971241048019793\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014448560665878985\n",
      "Average test loss: 0.00158971306681633\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014433468365006977\n",
      "Average test loss: 0.002065275218958656\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01434267902125915\n",
      "Average test loss: 0.001575745917339292\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014274832405977779\n",
      "Average test loss: 0.0015532190406488048\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01430044832163387\n",
      "Average test loss: 0.0016031235590991046\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014288334047628774\n",
      "Average test loss: 0.0016455917900635136\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014196179774072434\n",
      "Average test loss: 0.0015567463228685987\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014202422920200559\n",
      "Average test loss: 0.0015425046807568934\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014221380678315958\n",
      "Average test loss: 0.0015571859465498064\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01416013701342874\n",
      "Average test loss: 0.0016114624540011088\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014159629603226979\n",
      "Average test loss: 0.0015615082090306613\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014135833013388846\n",
      "Average test loss: 0.0015907179431782828\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014094096316231621\n",
      "Average test loss: 0.0015730160292651918\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014072611037227843\n",
      "Average test loss: 0.0015335791861224505\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014028357254134285\n",
      "Average test loss: 0.0016012487544988592\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01399051676193873\n",
      "Average test loss: 0.0015989563245740201\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014013958526982201\n",
      "Average test loss: 0.0015422329519771867\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013977251807020771\n",
      "Average test loss: 0.0015639093339236247\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013939447408748997\n",
      "Average test loss: 0.0015377649839760528\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013968192441595926\n",
      "Average test loss: 0.012039320168395836\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013911346924387747\n",
      "Average test loss: 0.001721977493725717\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013837022922933102\n",
      "Average test loss: 0.001527644318321513\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013900418154067463\n",
      "Average test loss: 0.0015752250299685532\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01381738185385863\n",
      "Average test loss: 0.0015772401692552699\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013855667813784547\n",
      "Average test loss: 0.0015617534029814932\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013773022220366532\n",
      "Average test loss: 0.0015718247777678901\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013743537715739674\n",
      "Average test loss: 0.0015570570220135981\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013742053320010503\n",
      "Average test loss: 0.0015496014308494827\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013712351272503535\n",
      "Average test loss: 0.001582396359079414\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013691577375762992\n",
      "Average test loss: 0.0015670575955882668\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013713258486655023\n",
      "Average test loss: 0.0015188019453651375\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01370582492235634\n",
      "Average test loss: 0.0016113296148056785\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0136183397124211\n",
      "Average test loss: 0.001582227194785244\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01367864206681649\n",
      "Average test loss: 0.001626567800839742\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013609368239012029\n",
      "Average test loss: 0.0016201173984963034\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01357602097094059\n",
      "Average test loss: 0.0016123212222009897\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013569743875000212\n",
      "Average test loss: 0.0015644110589184694\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013542996435529656\n",
      "Average test loss: 0.0015788217140361666\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013539200391206476\n",
      "Average test loss: 0.0015928062470629812\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013529003479414516\n",
      "Average test loss: 0.0016067443831513325\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013506920190321075\n",
      "Average test loss: 0.0016140092785159746\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013468648060328431\n",
      "Average test loss: 0.0016428887885995209\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013410754267540242\n",
      "Average test loss: 0.0015862702185081112\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013468825393252903\n",
      "Average test loss: 0.0016716127108989491\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013433243641422855\n",
      "Average test loss: 0.0016133239185954962\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013458469422327148\n",
      "Average test loss: 0.001791836176171071\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013364192527201441\n",
      "Average test loss: 0.0016411498894708025\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013376278921133942\n",
      "Average test loss: 0.0015486540829555855\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01336582453052203\n",
      "Average test loss: 0.0015489601919220553\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013370830819838577\n",
      "Average test loss: 0.0016147622976245151\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01336834526558717\n",
      "Average test loss: 0.0016168728736746642\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013283733163442876\n",
      "Average test loss: 0.0015793820351569189\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013301998254325655\n",
      "Average test loss: 0.0017981562017359667\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013288495565454165\n",
      "Average test loss: 0.0015936463648039433\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013259284671809939\n",
      "Average test loss: 0.0016761092553950019\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013215296912938356\n",
      "Average test loss: 0.0017222728606106506\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013269252055221134\n",
      "Average test loss: 0.0015650468764619695\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013220528816183408\n",
      "Average test loss: 0.002800525627616379\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013189202585981951\n",
      "Average test loss: 0.001617422160382072\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013187154362599054\n",
      "Average test loss: 0.0016421955429638424\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013128268404967254\n",
      "Average test loss: 0.001552587484009564\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013193926324446995\n",
      "Average test loss: 0.0016274023760731021\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013137710000077884\n",
      "Average test loss: 0.0016660206297205555\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013106784758468468\n",
      "Average test loss: 0.0016316121877688501\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013075956639316346\n",
      "Average test loss: 0.0018432942179756032\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013084313970473077\n",
      "Average test loss: 0.0015768112680978246\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013082988851600223\n",
      "Average test loss: 0.0016537072091466851\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01307658293346564\n",
      "Average test loss: 0.0015846932505567868\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013061281888021364\n",
      "Average test loss: 0.0016366249467763636\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013031512627171145\n",
      "Average test loss: 0.0015854374626134006\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01303083156628741\n",
      "Average test loss: 0.0016483985449497899\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013037256005737517\n",
      "Average test loss: 0.001631274133713709\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01299585919910007\n",
      "Average test loss: 0.0018722489825967286\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012986980059908495\n",
      "Average test loss: 0.0015723369732085202\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012979380982617536\n",
      "Average test loss: 0.00163238723617461\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012941233031451702\n",
      "Average test loss: 0.0016053753521086441\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012938940787480938\n",
      "Average test loss: 0.001585613401606679\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012936978132360511\n",
      "Average test loss: 0.0015826099190033144\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012888486489653588\n",
      "Average test loss: 0.0015945027863813771\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012867398137847583\n",
      "Average test loss: 0.0016363061393300693\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012888153372539414\n",
      "Average test loss: 0.001623265228130751\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012835463986628586\n",
      "Average test loss: 0.0015649437784320778\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01284439989593294\n",
      "Average test loss: 0.0016224245273818573\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01281463459548023\n",
      "Average test loss: 0.0015866851676255465\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012808936176614629\n",
      "Average test loss: 0.0017113778077893786\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012830910522490739\n",
      "Average test loss: 0.001660945718590584\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012796403982573085\n",
      "Average test loss: 0.001610596487712529\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012778542403545645\n",
      "Average test loss: 0.0015825019711628557\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012793754484918382\n",
      "Average test loss: 0.0016035071610887017\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012722146129442585\n",
      "Average test loss: 0.0016273086014100247\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012732895272473494\n",
      "Average test loss: 0.0015840931783119838\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012744729744891325\n",
      "Average test loss: 0.0015934841712522839\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01273899595025513\n",
      "Average test loss: 0.0016005572476941678\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012696932266983721\n",
      "Average test loss: 0.00166629589193811\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012666387505001491\n",
      "Average test loss: 0.0016777648936129278\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012652529990507496\n",
      "Average test loss: 0.0016105494875874784\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012668160652120909\n",
      "Average test loss: 0.0016568125658151175\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012670996443265014\n",
      "Average test loss: 0.0015873355838573642\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012622568473219872\n",
      "Average test loss: 0.001665878102183342\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012626421661840545\n",
      "Average test loss: 0.0015888110132267078\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012644080362386173\n",
      "Average test loss: 0.0016455770248754156\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012620928961369727\n",
      "Average test loss: 0.002041382811497897\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012575535419914458\n",
      "Average test loss: 0.001679845540266898\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012601148757669661\n",
      "Average test loss: 0.001614060533284727\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012581781808700827\n",
      "Average test loss: 0.0016340473817868365\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01256197908686267\n",
      "Average test loss: 0.0015937257587081855\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012511233776807785\n",
      "Average test loss: 0.0015799658454747664\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012520520750019285\n",
      "Average test loss: 0.0016188438939344551\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01254368011901776\n",
      "Average test loss: 0.001649696609419253\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012489539452724987\n",
      "Average test loss: 0.001618944501504302\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012546071728070577\n",
      "Average test loss: 0.0016276088198129502\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01248323174069325\n",
      "Average test loss: 0.0015954255723497934\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012484821113447349\n",
      "Average test loss: 0.0016176635599695146\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012468759444024828\n",
      "Average test loss: 0.0015832390335595442\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012506485278407732\n",
      "Average test loss: 0.0016408336581662298\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012479693352348274\n",
      "Average test loss: 0.0016237102982898553\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012405212309625414\n",
      "Average test loss: 0.0016085009833590853\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012439072868890233\n",
      "Average test loss: 0.0016870267408796483\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01243081237292952\n",
      "Average test loss: 0.001587645382206473\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012409630562696191\n",
      "Average test loss: 0.0016353748951935107\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012393480504552523\n",
      "Average test loss: 0.0015422557873858347\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01239428685274389\n",
      "Average test loss: 0.0016242077501697674\n",
      "Epoch 296/300\n",
      "Average training loss: 0.012395533935891257\n",
      "Average test loss: 0.0016496154787018894\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012367191578778955\n",
      "Average test loss: 0.0016605040110233757\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01234590337259902\n",
      "Average test loss: 0.001680433068631424\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_50_Depth15/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 16.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 17.42\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 19.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.03\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.99\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.28\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.19\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.76\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.98\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.40\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.77\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.90\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.10\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.15\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.55\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.92\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.39\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.54\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.79\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 26.498663160536026\n",
      "Average test loss: 0.03492959680822161\n",
      "Epoch 2/300\n",
      "Average training loss: 18.716067018296982\n",
      "Average test loss: 0.006467523323992888\n",
      "Epoch 3/300\n",
      "Average training loss: 16.683497717963323\n",
      "Average test loss: 0.005478435332162513\n",
      "Epoch 4/300\n",
      "Average training loss: 15.359078013949924\n",
      "Average test loss: 0.005779496670597129\n",
      "Epoch 5/300\n",
      "Average training loss: 13.530771505567763\n",
      "Average test loss: 0.13982970546682677\n",
      "Epoch 7/300\n",
      "Average training loss: 12.756943033854167\n",
      "Average test loss: 0.004531180586665869\n",
      "Epoch 8/300\n",
      "Average training loss: 12.127761216905382\n",
      "Average test loss: 0.004433102805995279\n",
      "Epoch 9/300\n",
      "Average training loss: 11.648339925977918\n",
      "Average test loss: 0.0044199223878482975\n",
      "Epoch 10/300\n",
      "Average training loss: 11.188149171617296\n",
      "Average test loss: 0.004344471220340994\n",
      "Epoch 11/300\n",
      "Average training loss: 10.702114129808214\n",
      "Average test loss: 0.004310408020599021\n",
      "Epoch 12/300\n",
      "Average training loss: 10.306843482123481\n",
      "Average test loss: 0.004329186408056154\n",
      "Epoch 13/300\n",
      "Average training loss: 9.934028417799208\n",
      "Average test loss: 0.004228413511067629\n",
      "Epoch 14/300\n",
      "Average training loss: 9.247509214613173\n",
      "Average test loss: 0.004177586869647106\n",
      "Epoch 16/300\n",
      "Average training loss: 8.936538746303983\n",
      "Average test loss: 0.00424765773365895\n",
      "Epoch 17/300\n",
      "Average training loss: 8.605747246636284\n",
      "Average test loss: 0.004135590068582031\n",
      "Epoch 18/300\n",
      "Average training loss: 8.307617628309462\n",
      "Average test loss: 0.0041090506774683795\n",
      "Epoch 19/300\n",
      "Average training loss: 8.007751074896918\n",
      "Average test loss: 0.004100464441503087\n",
      "Epoch 20/300\n",
      "Average training loss: 7.703294272104899\n",
      "Average test loss: 0.004080963225000434\n",
      "Epoch 21/300\n",
      "Average training loss: 7.128102425045437\n",
      "Average test loss: 0.004071688116424613\n",
      "Epoch 23/300\n",
      "Average training loss: 6.856965089162191\n",
      "Average test loss: 0.004032369245464603\n",
      "Epoch 24/300\n",
      "Average training loss: 6.5977978049384225\n",
      "Average test loss: 0.0040213334134055504\n",
      "Epoch 25/300\n",
      "Average training loss: 6.356898760053847\n",
      "Average test loss: 0.004056636294970909\n",
      "Epoch 26/300\n",
      "Average training loss: 6.113724440256755\n",
      "Average test loss: 0.004030191630125046\n",
      "Epoch 27/300\n",
      "Average training loss: 5.883803501976861\n",
      "Average training loss: 5.638197020636665\n",
      "Average test loss: 0.004004818489568101\n",
      "Epoch 29/300\n",
      "Average training loss: 5.425661828358968\n",
      "Average test loss: 0.003975363171969851\n",
      "Epoch 30/300\n",
      "Average training loss: 5.195142992231581\n",
      "Average test loss: 0.003956353795197275\n",
      "Epoch 31/300\n",
      "Average training loss: 4.9812532687717015\n",
      "Average test loss: 0.00394318287095262\n",
      "Epoch 32/300\n",
      "Average training loss: 4.768306307898627\n",
      "Average test loss: 0.003952272960088319\n",
      "Epoch 33/300\n",
      "Average training loss: 4.561842814127604\n",
      "Average test loss: 0.003945856529391474\n",
      "Epoch 34/300\n",
      "Average training loss: 4.1660018603007\n",
      "Average test loss: 0.003939265592230691\n",
      "Epoch 36/300\n",
      "Average training loss: 3.9835212841033933\n",
      "Average test loss: 0.003926832965471678\n",
      "Epoch 37/300\n",
      "Average training loss: 3.816212108823988\n",
      "Average test loss: 0.0039211492919259605\n",
      "Epoch 38/300\n",
      "Average training loss: 3.632296974182129\n",
      "Average test loss: 0.003983060540424453\n",
      "Epoch 39/300\n",
      "Average training loss: 3.4718387001885307\n",
      "Average test loss: 0.003900716477798091\n",
      "Epoch 40/300\n",
      "Average training loss: 3.1622981351216635\n",
      "Average test loss: 0.003947337513996495\n",
      "Epoch 42/300\n",
      "Average training loss: 3.018348445256551\n",
      "Average test loss: 0.003891383796516392\n",
      "Epoch 43/300\n",
      "Average training loss: 2.8820702101389566\n",
      "Average test loss: 0.004024978352917565\n",
      "Epoch 44/300\n",
      "Average training loss: 2.7439635147518584\n",
      "Average test loss: 0.00388047333020303\n",
      "Epoch 45/300\n",
      "Average training loss: 2.5997944208780925\n",
      "Average test loss: 0.003953345588511891\n",
      "Epoch 46/300\n",
      "Average training loss: 2.473109485414293\n",
      "Average test loss: 0.003896684033796191\n",
      "Epoch 47/300\n",
      "Average training loss: 2.3488211025661894\n",
      "Average test loss: 0.003867084085320433\n",
      "Epoch 48/300\n",
      "Average training loss: 2.2337293328179255\n",
      "Average test loss: 0.0039357857989768185\n",
      "Epoch 49/300\n",
      "Average training loss: 2.115723006778293\n",
      "Average test loss: 0.0038822372305310437\n",
      "Epoch 50/300\n",
      "Average training loss: 2.004114208433363\n",
      "Average test loss: 0.004220237841622697\n",
      "Epoch 51/300\n",
      "Average training loss: 1.901392691400316\n",
      "Average test loss: 0.003950263123959303\n",
      "Epoch 52/300\n",
      "Average training loss: 1.8032294212977091\n",
      "Average test loss: 0.003879765944969323\n",
      "Epoch 53/300\n",
      "Average training loss: 1.7112506362067328\n",
      "Average test loss: 0.0038656238367160162\n",
      "Epoch 54/300\n",
      "Average training loss: 1.6179897949430677\n",
      "Average test loss: 0.003872023909870121\n",
      "Epoch 55/300\n",
      "Average training loss: 1.5312572018305461\n",
      "Average test loss: 0.0039064892079267236\n",
      "Epoch 56/300\n",
      "Average training loss: 1.443574822001987\n",
      "Average test loss: 0.0038875005576345656\n",
      "Epoch 57/300\n",
      "Average training loss: 1.3600967962476942\n",
      "Average test loss: 0.003922422464936972\n",
      "Epoch 58/300\n",
      "Average training loss: 1.2801448187298246\n",
      "Average test loss: 0.003875701973421706\n",
      "Epoch 59/300\n",
      "Average training loss: 1.123755987379286\n",
      "Average test loss: 0.003985469690627522\n",
      "Epoch 61/300\n",
      "Average training loss: 1.0526770401530796\n",
      "Average test loss: 0.003994963612821367\n",
      "Epoch 62/300\n",
      "Average training loss: 0.9837309833102756\n",
      "Average test loss: 0.003915768174040648\n",
      "Epoch 63/300\n",
      "Average training loss: 0.916712734328376\n",
      "Average test loss: 0.0039394160287661685\n",
      "Epoch 64/300\n",
      "Average training loss: 0.8538340951601664\n",
      "Average test loss: 0.003867545663068692\n",
      "Epoch 65/300\n",
      "Average training loss: 0.7345823046366373\n",
      "Average test loss: 0.003888725725106067\n",
      "Epoch 67/300\n",
      "Average training loss: 0.6757718161476983\n",
      "Average test loss: 0.0039052680132703648\n",
      "Epoch 68/300\n",
      "Average training loss: 0.623160434934828\n",
      "Average test loss: 0.00392293520602915\n",
      "Epoch 69/300\n",
      "Average training loss: 0.5718911279042562\n",
      "Average test loss: 0.003939778450462553\n",
      "Epoch 70/300\n",
      "Average training loss: 0.5228010455237495\n",
      "Average test loss: 0.003968572482466698\n",
      "Epoch 71/300\n",
      "Average training loss: 0.4759501877360874\n",
      "Average test loss: 0.003962629409713878\n",
      "Epoch 72/300\n",
      "Average training loss: 0.391834737751219\n",
      "Average training loss: 0.35587573946846857\n",
      "Average test loss: 0.00403664723245634\n",
      "Epoch 75/300\n",
      "Average training loss: 0.32249737336900497\n",
      "Average test loss: 0.003918569112817447\n",
      "Epoch 76/300\n",
      "Average training loss: 0.29241880435413786\n",
      "Average test loss: 0.003932747940222422\n",
      "Epoch 77/300\n",
      "Average training loss: 0.26577123244603473\n",
      "Average test loss: 0.003927022210839722\n",
      "Epoch 78/300\n",
      "Average training loss: 0.23978000677956476\n",
      "Average test loss: 0.003942209190792508\n",
      "Epoch 79/300\n",
      "Average training loss: 0.19718019060293834\n",
      "Average test loss: 0.003969035360548231\n",
      "Epoch 81/300\n",
      "Average training loss: 0.17940121784475116\n",
      "Average test loss: 0.004003553337521023\n",
      "Epoch 82/300\n",
      "Average training loss: 0.16487851491239336\n",
      "Average test loss: 0.004210273388359282\n",
      "Epoch 83/300\n",
      "Average training loss: 0.15342768137984805\n",
      "Average test loss: 0.004036942128919893\n",
      "Epoch 84/300\n",
      "Average training loss: 0.14339676547712749\n",
      "Average test loss: 0.00403428116813302\n",
      "Epoch 85/300\n",
      "Average training loss: 0.13603192709551917\n",
      "Average test loss: 0.00398181810333497\n",
      "Epoch 86/300\n",
      "Average training loss: 0.13019629910257127\n",
      "Average test loss: 0.003982015865337517\n",
      "Epoch 87/300\n",
      "Average training loss: 0.12591997300253974\n",
      "Average test loss: 0.004036077074499594\n",
      "Epoch 88/300\n",
      "Average training loss: 0.12236781211031808\n",
      "Average test loss: 0.003966501545781891\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11982827963431676\n",
      "Average test loss: 0.003959017091120283\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11737085047033098\n",
      "Average test loss: 0.00406649108707077\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11549964625967873\n",
      "Average test loss: 0.0040325456069161495\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1138533399634891\n",
      "Average test loss: 0.004081203639093372\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11267811911635929\n",
      "Average test loss: 0.004012769487169054\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11168591150972579\n",
      "Average test loss: 0.003956401380606824\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11094578657547632\n",
      "Average test loss: 0.004144401736143563\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11004312468899621\n",
      "Average test loss: 0.004057364556524489\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1093823562529352\n",
      "Average test loss: 0.004082323896802134\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10891058164834976\n",
      "Average test loss: 0.004138994978947772\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10840205002493329\n",
      "Average test loss: 0.004072374655554692\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10770487606525421\n",
      "Average test loss: 0.004013808274848593\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10730568713612026\n",
      "Average test loss: 0.00398549024015665\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10681130847665998\n",
      "Average test loss: 0.004139356858200497\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10679212450318866\n",
      "Average test loss: 0.0040803359479953845\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10635656146870719\n",
      "Average test loss: 0.0041273889396753576\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10558057432042228\n",
      "Average test loss: 0.0039987474174963105\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10518191440237892\n",
      "Average test loss: 0.004084491940008269\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10496129339933395\n",
      "Average test loss: 0.004040118967700335\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10443221862448586\n",
      "Average test loss: 0.00419197574382027\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10403029067979919\n",
      "Average test loss: 0.004154866673880153\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10382310152716107\n",
      "Average test loss: 0.00416970936415924\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10369371840026644\n",
      "Average test loss: 0.004172579061653879\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10318999081850051\n",
      "Average test loss: 0.004037163664483361\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10260627741614978\n",
      "Average test loss: 0.004308046857826412\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10252715530660417\n",
      "Average test loss: 0.004019590470112033\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10217978964249293\n",
      "Average test loss: 0.004099905855953693\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10176368936565187\n",
      "Average test loss: 0.0040294704768392775\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10120858034822676\n",
      "Average test loss: 0.004074109241366386\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10082044568989013\n",
      "Average test loss: 0.0041575520142085026\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10082787905136745\n",
      "Average test loss: 0.004063869574417671\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10101046555572087\n",
      "Average test loss: 0.004183546943383084\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10143760041395823\n",
      "Average test loss: 0.0040745614307622115\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09949238361914953\n",
      "Average test loss: 0.0041251201873852145\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09904680267969768\n",
      "Average test loss: 0.004066069101707803\n",
      "Epoch 124/300\n",
      "Average training loss: 0.09887815063529544\n",
      "Average test loss: 0.0040951975261171655\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09855097059408824\n",
      "Average test loss: 0.004182870518623127\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09834991952445772\n",
      "Average test loss: 0.004035904159562455\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09797911026742723\n",
      "Average test loss: 0.004151950888335705\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09768472886747784\n",
      "Average test loss: 0.004123683210462332\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09745832647217645\n",
      "Average test loss: 0.0040947503240572085\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09701994988653395\n",
      "Average test loss: 0.004217900773510337\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09678716048929427\n",
      "Average test loss: 0.00412664100382891\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09667037587033378\n",
      "Average test loss: 0.004091872576210234\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09664950904581282\n",
      "Average test loss: 0.004121911469433043\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0958819635576672\n",
      "Average test loss: 0.004221081454720762\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09566736288203133\n",
      "Average test loss: 0.004188023075668348\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09515932779841953\n",
      "Average test loss: 0.004053049041165246\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09530453597837024\n",
      "Average test loss: 0.004079808656954103\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09468088982502619\n",
      "Average test loss: 0.004144435366822613\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09444594380590651\n",
      "Average test loss: 0.004116972032106585\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09425550746255451\n",
      "Average test loss: 0.004335417074461778\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09374118106895023\n",
      "Average test loss: 0.0040540018323808906\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09352842621670829\n",
      "Average test loss: 0.00429434826473395\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09331426709890366\n",
      "Average test loss: 0.004150376381973426\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09273599535226822\n",
      "Average test loss: 0.004132638613796896\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09269279172023137\n",
      "Average test loss: 0.004094426586189204\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09325605722268422\n",
      "Average test loss: 0.004180829936017593\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09205064007971021\n",
      "Average test loss: 0.004232416968378756\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09172431063652038\n",
      "Average test loss: 0.004177317839529779\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09147463285923005\n",
      "Average test loss: 0.004185735899748074\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09115646121237013\n",
      "Average test loss: 0.004205225877463818\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09096632566716936\n",
      "Average test loss: 0.004219259003798167\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09066750284698274\n",
      "Average test loss: 0.004121752591596709\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09040438128842247\n",
      "Average test loss: 0.00425982376270824\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0900998320778211\n",
      "Average test loss: 0.0041194039690825674\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08977298616038429\n",
      "Average test loss: 0.004209523012654649\n",
      "Epoch 156/300\n",
      "Average training loss: 0.089602903438939\n",
      "Average test loss: 0.004159062425295512\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08931986798180475\n",
      "Average test loss: 0.0041967609948996044\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08897536870506075\n",
      "Average test loss: 0.004192218964712487\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0885156694981787\n",
      "Average test loss: 0.004353541375448307\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08859139322572285\n",
      "Average test loss: 0.004218403295510345\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08834664816326565\n",
      "Average test loss: 0.00428960446971986\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08814656529823939\n",
      "Average test loss: 0.004123119958158996\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08738965921269523\n",
      "Average test loss: 0.004069312689205011\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08725517800781461\n",
      "Average test loss: 0.004277073218176762\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0873430707388454\n",
      "Average test loss: 0.004177954202724828\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08670834129386477\n",
      "Average test loss: 0.00430248054737846\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08653189990255568\n",
      "Average test loss: 0.004371217514905664\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08624284512466855\n",
      "Average test loss: 0.0040520894394980534\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08598487884468503\n",
      "Average test loss: 0.004135468920692801\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08572369258271323\n",
      "Average test loss: 0.004350891627371311\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08533682317866219\n",
      "Average test loss: 0.004114960270623366\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08519330369432768\n",
      "Average test loss: 0.004218465128706561\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08504360020822949\n",
      "Average test loss: 0.004160021183805334\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08485861245128844\n",
      "Average test loss: 0.004278596924203965\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08438840678665373\n",
      "Average test loss: 0.004191921548710929\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08411397239234712\n",
      "Average test loss: 0.004131773227204879\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08395565442906486\n",
      "Average test loss: 0.004194342016345925\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08365931097666422\n",
      "Average test loss: 0.004397675668199857\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0833115095893542\n",
      "Average test loss: 0.004189023303074969\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08331582122378879\n",
      "Average test loss: 0.004251556016711725\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08287365332576964\n",
      "Average test loss: 0.004215604270497958\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08252598157193926\n",
      "Average test loss: 0.004069462621377574\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0824440704551008\n",
      "Average test loss: 0.004251580776439772\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08210830248064464\n",
      "Average test loss: 0.004210828082015117\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08201086741685867\n",
      "Average test loss: 0.0042247258416480485\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08168801501724456\n",
      "Average test loss: 0.004168505863803957\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08142099044058058\n",
      "Average test loss: 0.004163367213267419\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08123421347141266\n",
      "Average test loss: 0.004248719558533695\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08082971911297904\n",
      "Average test loss: 0.004049061373497049\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0807244305147065\n",
      "Average test loss: 0.004279215320116944\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08054775861236785\n",
      "Average test loss: 0.004439810733207398\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08021825499998199\n",
      "Average test loss: 0.004413753763669067\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08002694487571717\n",
      "Average test loss: 0.004192425784551435\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07987867709000905\n",
      "Average test loss: 0.004222775581810209\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07948568717307515\n",
      "Average test loss: 0.004232097107503149\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07929507800605562\n",
      "Average test loss: 0.0042919488673408826\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07931568938493729\n",
      "Average test loss: 0.00429045676998794\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07911234145032035\n",
      "Average test loss: 0.004387244910415676\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07893871739837859\n",
      "Average test loss: 0.004162250372270743\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07869080924987792\n",
      "Average test loss: 0.004219544526189565\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07828076349364387\n",
      "Average test loss: 0.004154430907219648\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07797146185239157\n",
      "Average test loss: 0.004082606670239733\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07788354827960332\n",
      "Average test loss: 0.0045426322018934625\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07817071185509364\n",
      "Average test loss: 0.004212285298854113\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0773265624774827\n",
      "Average test loss: 0.004389539921449291\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07722532097498576\n",
      "Average test loss: 0.004331614222170578\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07684784659908878\n",
      "Average test loss: 0.0043471790953642795\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07679985854691929\n",
      "Average test loss: 0.0043152070716023444\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07682834094100528\n",
      "Average test loss: 0.004396932985219691\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07708637787236108\n",
      "Average test loss: 0.004168678487133649\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07610445494453112\n",
      "Average test loss: 0.004371021659837829\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07621993833118015\n",
      "Average test loss: 0.004270990567488803\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07579485345549053\n",
      "Average test loss: 0.004147157221411665\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07556044891807769\n",
      "Average test loss: 0.004149966731046637\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07535009104013443\n",
      "Average test loss: 0.0041995125179075535\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07552104562520981\n",
      "Average test loss: 0.004126843177196052\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07526681649684906\n",
      "Average test loss: 0.004188877477827999\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07519191644589106\n",
      "Average test loss: 0.004182084273960855\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07461848976214727\n",
      "Average test loss: 0.004496471627718873\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07448005643818113\n",
      "Average test loss: 0.0041969843219137855\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0743773538602723\n",
      "Average test loss: 0.004186897484378682\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07411675048536724\n",
      "Average test loss: 0.004319500141673618\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07423471126953761\n",
      "Average test loss: 0.004255352643421954\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07403743733962377\n",
      "Average test loss: 0.004362880838414033\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07351702788803313\n",
      "Average test loss: 0.004138548243169983\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07332100953658421\n",
      "Average test loss: 0.0042297250375979475\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07365606809986962\n",
      "Average test loss: 0.004195040273997519\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07320362070865101\n",
      "Average test loss: 0.004274357043620613\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07300881118906868\n",
      "Average test loss: 0.004498508208741744\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07271927608384027\n",
      "Average test loss: 0.00421546479066213\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07259074454175102\n",
      "Average test loss: 0.004232280362604393\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07260539401570956\n",
      "Average test loss: 0.004384784324715535\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07239483286937078\n",
      "Average test loss: 0.0043497780362764995\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07203584628634982\n",
      "Average test loss: 0.004194682943324248\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07202331784036424\n",
      "Average test loss: 0.004470173593196604\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07201644557714462\n",
      "Average test loss: 0.004205089634905259\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07167114312118954\n",
      "Average test loss: 0.004230507057574061\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07145670306682586\n",
      "Average test loss: 0.004183083548934923\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0713391000131766\n",
      "Average test loss: 0.004578506448202663\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07121594552199045\n",
      "Average test loss: 0.004246267968581782\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07498086174991396\n",
      "Average test loss: 0.004383335386713346\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0744760530392329\n",
      "Average test loss: 0.004208231441262695\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07103495523664687\n",
      "Average test loss: 0.004076002729849683\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0702170995970567\n",
      "Average test loss: 0.004320165532330672\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07013506442639562\n",
      "Average test loss: 0.0043545613044665915\n",
      "Epoch 246/300\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_50_Depth15/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
