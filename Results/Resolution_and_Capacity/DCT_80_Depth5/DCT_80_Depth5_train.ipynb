{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_80x80_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.054067917434705626\n",
      "Average test loss: 0.0045439425802065265\n",
      "Epoch 2/300\n",
      "Average training loss: 0.021808801776833003\n",
      "Average test loss: 0.004080914434459474\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020610824109779463\n",
      "Average test loss: 0.003973767609645923\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02021098852985435\n",
      "Average test loss: 0.003925836021701495\n",
      "Epoch 5/300\n",
      "Average training loss: 0.019963790057433976\n",
      "Average test loss: 0.003887150014233258\n",
      "Epoch 6/300\n",
      "Average training loss: 0.019762629348370763\n",
      "Average test loss: 0.0038505353164962597\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019614972210592694\n",
      "Average test loss: 0.0038321135395930875\n",
      "Epoch 8/300\n",
      "Average training loss: 0.019487916429837545\n",
      "Average test loss: 0.00380949465620021\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01898479586839676\n",
      "Average test loss: 0.0037156535159382554\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01892971564994918\n",
      "Average test loss: 0.00370875326047341\n",
      "Epoch 16/300\n",
      "Average training loss: 0.018857927434974248\n",
      "Average test loss: 0.0036923585169845157\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01881469248731931\n",
      "Average test loss: 0.003686655927449465\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018771241704622906\n",
      "Average test loss: 0.0036797951584060986\n",
      "Epoch 19/300\n",
      "Average training loss: 0.018703257005247807\n",
      "Average test loss: 0.0036678663088629644\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018655092462897302\n",
      "Average test loss: 0.003660798307715191\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01862244027853012\n",
      "Average test loss: 0.0036659909991754425\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018584751589430702\n",
      "Average test loss: 0.0036645674152920643\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018531237537662188\n",
      "Average test loss: 0.003630733264403211\n",
      "Epoch 24/300\n",
      "Average training loss: 0.018499054162038696\n",
      "Average test loss: 0.003626809357561999\n",
      "Epoch 25/300\n",
      "Average training loss: 0.018476919907662603\n",
      "Average test loss: 0.0036266894876543017\n",
      "Epoch 26/300\n",
      "Average training loss: 0.018437964538733163\n",
      "Average test loss: 0.0036234475982685884\n",
      "Epoch 27/300\n",
      "Average training loss: 0.018396985265115896\n",
      "Average test loss: 0.0036217954839683243\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0183762120720413\n",
      "Average test loss: 0.0036019060421321127\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01834380958477656\n",
      "Average test loss: 0.003605305658446418\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01832620740433534\n",
      "Average test loss: 0.0036139975560622083\n",
      "Epoch 31/300\n",
      "Average training loss: 0.018303165660964116\n",
      "Average test loss: 0.0035929018577767744\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01826977383179797\n",
      "Average test loss: 0.0035884691402316093\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01825463149117099\n",
      "Average test loss: 0.003579423911869526\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018236608636048104\n",
      "Average test loss: 0.0035964069991476005\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018215608363350233\n",
      "Average test loss: 0.00358023592001862\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018199466690421103\n",
      "Average test loss: 0.00357393224330412\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01817543840408325\n",
      "Average test loss: 0.003569456254856454\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0181604611840513\n",
      "Average test loss: 0.0035623265237857897\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018145262198315727\n",
      "Average test loss: 0.0035837898517234457\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01805564003934463\n",
      "Average test loss: 0.0035516967020101015\n",
      "Epoch 46/300\n",
      "Average training loss: 0.018043310071031254\n",
      "Average test loss: 0.003553708394782411\n",
      "Epoch 47/300\n",
      "Average training loss: 0.018028664679990876\n",
      "Average test loss: 0.0035549293607473375\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018017412640982205\n",
      "Average test loss: 0.0035563573729660777\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01800416827864117\n",
      "Average test loss: 0.0035549848104516665\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018000319172938664\n",
      "Average test loss: 0.003561662947345111\n",
      "Epoch 51/300\n",
      "Average training loss: 0.017979210159844824\n",
      "Average test loss: 0.003546681238338351\n",
      "Epoch 52/300\n",
      "Average training loss: 0.017966596298747594\n",
      "Average test loss: 0.003574671250871486\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017954410090214677\n",
      "Average test loss: 0.0035460031947327983\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01794624187797308\n",
      "Average test loss: 0.0035492970291525127\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017930421339141\n",
      "Average test loss: 0.0035456744184096654\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01787187503443824\n",
      "Average test loss: 0.00353697969019413\n",
      "Epoch 62/300\n",
      "Average training loss: 0.017862170426381958\n",
      "Average test loss: 0.003544319719903999\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017854578993386692\n",
      "Average test loss: 0.0035345019232481717\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017844993233680724\n",
      "Average test loss: 0.0035334540038473076\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017830461730559667\n",
      "Average test loss: 0.0035600634353856247\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017833118678794967\n",
      "Average test loss: 0.0035274527499245274\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01781766506532828\n",
      "Average test loss: 0.0035401560105383398\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01780189403063721\n",
      "Average test loss: 0.003536592685514026\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017795328836474153\n",
      "Average test loss: 0.003525505794833104\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017793483312759133\n",
      "Average test loss: 0.0035391515104307067\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01777912452071905\n",
      "Average test loss: 0.003536991361528635\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017728395274943777\n",
      "Average test loss: 0.003537422430391113\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017717767872744136\n",
      "Average test loss: 0.0035348155945539475\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017712110279334916\n",
      "Average test loss: 0.003554744879818625\n",
      "Epoch 80/300\n",
      "Average training loss: 0.017699268176323838\n",
      "Average test loss: 0.003574101278765334\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017693784842888513\n",
      "Average test loss: 0.0035355524625629185\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01768063562363386\n",
      "Average test loss: 0.003558947010172738\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01767675385955307\n",
      "Average test loss: 0.003532930990888013\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017666607805424266\n",
      "Average test loss: 0.0035384423265026675\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01765891428788503\n",
      "Average test loss: 0.00352568104200893\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017648193314671518\n",
      "Average test loss: 0.0035362396229886347\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017644137902392283\n",
      "Average test loss: 0.0035244532372388573\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017638112066520585\n",
      "Average test loss: 0.0035442706344442237\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01762517089645068\n",
      "Average test loss: 0.0035379222457607587\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0176210009008646\n",
      "Average test loss: 0.0035276262625637982\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017608502859870593\n",
      "Average test loss: 0.003535653954992692\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017607854975594413\n",
      "Average test loss: 0.0035212258932491142\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01758664477037059\n",
      "Average test loss: 0.003546150238149696\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01759837114314238\n",
      "Average test loss: 0.003532313459035423\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017583775462375748\n",
      "Average test loss: 0.003558192881031169\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017576467397312324\n",
      "Average test loss: 0.0035402318166775837\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017561168649130397\n",
      "Average test loss: 0.0035255108601931068\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01755482245898909\n",
      "Average test loss: 0.003531409593299031\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017552186294562285\n",
      "Average test loss: 0.0035357033794538844\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017552506786253717\n",
      "Average test loss: 0.003551857771558894\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017536957326862546\n",
      "Average test loss: 0.0035307935890224245\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017530023991233774\n",
      "Average test loss: 0.0035487853011323344\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017520524786578284\n",
      "Average test loss: 0.0035265562799241808\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017514816583858596\n",
      "Average test loss: 0.0035781643986701967\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017508208339413005\n",
      "Average test loss: 0.00354151941380567\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017500444876650968\n",
      "Average test loss: 0.0035713645050095186\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01748594719502661\n",
      "Average test loss: 0.003540181924899419\n",
      "Epoch 108/300\n",
      "Average training loss: 0.017483877482513586\n",
      "Average test loss: 0.0035562055800110104\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0174747233506706\n",
      "Average test loss: 0.003571931050883399\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01747234585881233\n",
      "Average test loss: 0.0035320651510523425\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017471952192485332\n",
      "Average test loss: 0.0035353556616852682\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01742980150381724\n",
      "Average test loss: 0.003554581422979633\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017418422736227512\n",
      "Average test loss: 0.003570650204602215\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017405588070551554\n",
      "Average test loss: 0.0035644884997357924\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01740446742044555\n",
      "Average test loss: 0.0035435535655253462\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017390880503588252\n",
      "Average test loss: 0.0035725877156688106\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0173867663393418\n",
      "Average test loss: 0.0035354761473006674\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017384119465947152\n",
      "Average test loss: 0.0035406022212571566\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017370214050014814\n",
      "Average test loss: 0.0035478888290623825\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01737210117859973\n",
      "Average test loss: 0.0035879989051156576\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017363834392693307\n",
      "Average test loss: 0.00355719853979018\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01735715539753437\n",
      "Average test loss: 0.0035565419697927104\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01734209457453754\n",
      "Average test loss: 0.003575661938637495\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017343145044313536\n",
      "Average test loss: 0.0035511222096780938\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01734237540927198\n",
      "Average test loss: 0.003544389760742585\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017327068035801252\n",
      "Average test loss: 0.0035515527725219725\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017318161903156173\n",
      "Average test loss: 0.0035384355197764106\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01731272556218836\n",
      "Average test loss: 0.003546928862730662\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017307692555917634\n",
      "Average test loss: 0.0035452058811982472\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01730945514722003\n",
      "Average test loss: 0.0035681648978756536\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017296779482728904\n",
      "Average test loss: 0.003579034660425451\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017291060558623738\n",
      "Average test loss: 0.0036702383094363742\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017284267514116235\n",
      "Average test loss: 0.0035553627314252986\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01728246566404899\n",
      "Average test loss: 0.003575089776681529\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017270919379260806\n",
      "Average test loss: 0.0035547387198441556\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017265519649618203\n",
      "Average test loss: 0.0036283990020553273\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017263610031869676\n",
      "Average test loss: 0.003577146136098438\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017253980187906158\n",
      "Average test loss: 0.0035816558462878068\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017250500587953462\n",
      "Average test loss: 0.0035577110739217865\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017239858109090064\n",
      "Average test loss: 0.0035702039796031183\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017240233961906697\n",
      "Average test loss: 0.0035738745842956833\n",
      "Epoch 147/300\n",
      "Average training loss: 0.017230447918176652\n",
      "Average test loss: 0.0035471958393851915\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017225964843398996\n",
      "Average test loss: 0.003601049673640066\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017211864325735304\n",
      "Average test loss: 0.0035539826274745994\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017210606457458602\n",
      "Average test loss: 0.003577943560977777\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017202442083093857\n",
      "Average test loss: 0.0036314535830169917\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017202148540152444\n",
      "Average test loss: 0.0035779188542316358\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017193851340148184\n",
      "Average test loss: 0.0036137985988623565\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017186910544832547\n",
      "Average test loss: 0.003577650420781639\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017177624452445243\n",
      "Average test loss: 0.003595345720234844\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017181948450704417\n",
      "Average test loss: 0.0035686090538899103\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017170819006032415\n",
      "Average test loss: 0.0035508784380637935\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017173295863800578\n",
      "Average test loss: 0.0035728716655737823\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017161111869745786\n",
      "Average test loss: 0.003583191516912646\n",
      "Epoch 160/300\n",
      "Average training loss: 0.017152107331487868\n",
      "Average test loss: 0.003574460305273533\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01714461271299256\n",
      "Average test loss: 0.0036055166154272027\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017139779636429417\n",
      "Average test loss: 0.0035710193185756606\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017140649078620805\n",
      "Average test loss: 0.0035666343035797277\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017138184500237307\n",
      "Average test loss: 0.0036009833953446813\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01712549510598183\n",
      "Average test loss: 0.003560283572309547\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01712055315905147\n",
      "Average test loss: 0.0036466762555970086\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017115100898676447\n",
      "Average test loss: 0.0035907685661481486\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017108433949450653\n",
      "Average test loss: 0.00359039139519963\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017111518669459556\n",
      "Average test loss: 0.0036109576786143914\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01709929973218176\n",
      "Average test loss: 0.0035927241953710716\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0170978586094247\n",
      "Average test loss: 0.0035584781728684903\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01709426570435365\n",
      "Average test loss: 0.003575488870135612\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01708000421275695\n",
      "Average test loss: 0.0036159746361275513\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017081714853644372\n",
      "Average test loss: 0.0035983832174291213\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017083000123500825\n",
      "Average test loss: 0.0036154149054653116\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017073150545358656\n",
      "Average test loss: 0.003622828574644195\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01706195928570297\n",
      "Average test loss: 0.003653059569083982\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017056460851596462\n",
      "Average test loss: 0.0035831699333050185\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017051978876193363\n",
      "Average test loss: 0.0035877570323646067\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017049559416042435\n",
      "Average test loss: 0.003613644813290901\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01704930746803681\n",
      "Average test loss: 0.003637527693063021\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01703790093296104\n",
      "Average test loss: 0.003593178859187497\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01703728645708826\n",
      "Average test loss: 0.003585019028435151\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017033657047483657\n",
      "Average test loss: 0.0036812437609252003\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017032549560070036\n",
      "Average test loss: 0.0036206480419884125\n",
      "Epoch 186/300\n",
      "Average training loss: 0.017024325151410367\n",
      "Average test loss: 0.003645962531367938\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01701118882248799\n",
      "Average test loss: 0.0036116024975975354\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017009260407752462\n",
      "Average test loss: 0.003590730818816357\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01701576097144021\n",
      "Average test loss: 0.0036156159434467556\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0169990668238865\n",
      "Average test loss: 0.0036352103888574573\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016997214294142193\n",
      "Average test loss: 0.003615608547296789\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017003972339961263\n",
      "Average test loss: 0.0036721524244381324\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016986005838546488\n",
      "Average test loss: 0.0036426795965267553\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016980693262484338\n",
      "Average test loss: 0.0036155319472567903\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016982621503372985\n",
      "Average test loss: 0.0036415936038311984\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01697807648115688\n",
      "Average test loss: 0.0036048978322909936\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016973975201447805\n",
      "Average test loss: 0.00358555655926466\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016965275034308433\n",
      "Average test loss: 0.0036433255076408384\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01696173044294119\n",
      "Average test loss: 0.003683996566467815\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016964684907760885\n",
      "Average test loss: 0.003636019999575284\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01695023734536436\n",
      "Average test loss: 0.0035856251621411905\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016943507574498654\n",
      "Average test loss: 0.0035854521958778303\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01695432985574007\n",
      "Average test loss: 0.0036322935877574816\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01693644968999757\n",
      "Average test loss: 0.0036165654903484714\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01693151457856099\n",
      "Average test loss: 0.0036000468623307016\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01693449120554659\n",
      "Average test loss: 0.0036254843543801043\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016928660654359395\n",
      "Average test loss: 0.0036437583077285024\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01689796892139647\n",
      "Average test loss: 0.0036322120094878806\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01688957658244504\n",
      "Average test loss: 0.0036545345100263754\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016892031883200008\n",
      "Average test loss: 0.0036604414880275727\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016890993879901037\n",
      "Average test loss: 0.0036344756152894763\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016882630774544344\n",
      "Average test loss: 0.0036229547177337937\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01688181164777941\n",
      "Average test loss: 0.0035936928569442698\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01688485185470846\n",
      "Average test loss: 0.0036838557333168055\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016865345851414734\n",
      "Average test loss: 0.0036238597207185297\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01687173418617911\n",
      "Average test loss: 0.0036708966282506784\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01686349727296167\n",
      "Average test loss: 0.003640110604878929\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016859868798818855\n",
      "Average test loss: 0.0037158682944460047\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01685982244213422\n",
      "Average test loss: 0.0036442514840099548\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01685946350793044\n",
      "Average test loss: 0.003623845295359691\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016847316432330343\n",
      "Average test loss: 0.0036279029831704167\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01684594087468253\n",
      "Average test loss: 0.0037764317612681126\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01684292798406548\n",
      "Average test loss: 0.003616778627038002\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016833388415475686\n",
      "Average test loss: 0.003632816956895921\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016841293322543305\n",
      "Average test loss: 0.0036882815145783954\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016844501800835132\n",
      "Average test loss: 0.0036352514525254567\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016835538542932935\n",
      "Average test loss: 0.0036348426159885194\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016817541354232364\n",
      "Average test loss: 0.00365039213332865\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01682087780535221\n",
      "Average test loss: 0.003610894029132194\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016817519871724976\n",
      "Average test loss: 0.0036098074060347345\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01681976889239417\n",
      "Average test loss: 0.003631527416821983\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016812831805811987\n",
      "Average test loss: 0.0037120501614279215\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01681876739197307\n",
      "Average test loss: 0.003635781464063459\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016807715019418135\n",
      "Average test loss: 0.0036958690509200096\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016796635380221737\n",
      "Average test loss: 0.003616145957261324\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01679247256037262\n",
      "Average test loss: 0.0036268242549979024\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016797275371021693\n",
      "Average test loss: 0.0036294412110000848\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01679181029399236\n",
      "Average test loss: 0.0036383206800868115\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016791807444559204\n",
      "Average test loss: 0.00363399987315966\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01679411270800564\n",
      "Average test loss: 0.00369705323005716\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01678041699859831\n",
      "Average test loss: 0.00362225918389029\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01678262945678499\n",
      "Average test loss: 0.003683332828183969\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016772533903519314\n",
      "Average test loss: 0.0036682840322868693\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01677509293705225\n",
      "Average test loss: 0.0036481021514369384\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016769469629973173\n",
      "Average test loss: 0.0036515664984989498\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016766517717805175\n",
      "Average test loss: 0.003698201064641277\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016762344712184536\n",
      "Average test loss: 0.0036372835307071608\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0167608766572343\n",
      "Average test loss: 0.003601351996469829\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016763015913466614\n",
      "Average test loss: 0.0036893345488028394\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016749064803123475\n",
      "Average test loss: 0.003654211641599735\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016747792192631297\n",
      "Average test loss: 0.003685255160762204\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01674652399950557\n",
      "Average test loss: 0.0036779413697206314\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016742742790944048\n",
      "Average test loss: 0.0036526002726621097\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016723833030296696\n",
      "Average test loss: 0.003645877714579304\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016724682645665276\n",
      "Average test loss: 0.003679056805661983\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01672236683799161\n",
      "Average test loss: 0.0037171083150638476\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016713141972819964\n",
      "Average test loss: 0.003655373069147269\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016725277081131935\n",
      "Average test loss: 0.00370305784791708\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01671160582370228\n",
      "Average test loss: 0.0037558935731649397\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016720760797460873\n",
      "Average test loss: 0.0037698611811631255\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01670468116303285\n",
      "Average test loss: 0.00369323609645168\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016707256481051445\n",
      "Average test loss: 0.003660842903993196\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016698791194293235\n",
      "Average test loss: 0.003650446622322003\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016689795292913912\n",
      "Average test loss: 0.0036070867013186215\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01669817498905791\n",
      "Average test loss: 0.003654529573602809\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016691986538469792\n",
      "Average test loss: 0.003608400821685791\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016689438712265758\n",
      "Average test loss: 0.0037553200154668755\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01669274761279424\n",
      "Average test loss: 0.003639346893876791\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016682991168565222\n",
      "Average test loss: 0.0036807262334558697\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0166813732691937\n",
      "Average test loss: 0.0036835710455973944\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01667388423697816\n",
      "Average test loss: 0.003604394483897421\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01668192018402947\n",
      "Average test loss: 0.003653323019130362\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01667480076021618\n",
      "Average test loss: 0.003626491333047549\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01667178663942549\n",
      "Average test loss: 0.003739150216182073\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016670650438302095\n",
      "Average test loss: 0.0037660531072566906\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01665724471501178\n",
      "Average test loss: 0.0036863409427718985\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016666567948129443\n",
      "Average test loss: 0.0036641359414077467\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016666575117243662\n",
      "Average test loss: 0.0036623456821673448\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01665114485555225\n",
      "Average test loss: 0.003659248137018747\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016663872565660213\n",
      "Average test loss: 0.0036805084902379248\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016661016165382334\n",
      "Average test loss: 0.0036753554948502115\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016648287094301648\n",
      "Average test loss: 0.0036817231201049356\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01665188266005781\n",
      "Average test loss: 0.003645649016731315\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01664822778519657\n",
      "Average test loss: 0.0037305910574893155\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016645559165212844\n",
      "Average test loss: 0.0036633585174050596\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016638098692728413\n",
      "Average test loss: 0.003646878873722421\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01663692329989539\n",
      "Average test loss: 0.003667080957442522\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01663593679252598\n",
      "Average test loss: 0.0037172622432311377\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.051329932138323785\n",
      "Average test loss: 0.0039836940790216125\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01910465033352375\n",
      "Average test loss: 0.0037711257719331317\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017921207833621236\n",
      "Average test loss: 0.0034458135087043046\n",
      "Epoch 4/300\n",
      "Average training loss: 0.017411943763494492\n",
      "Average test loss: 0.00332570023338\n",
      "Epoch 5/300\n",
      "Average training loss: 0.017033506952226163\n",
      "Average test loss: 0.0032971261193354925\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01671426631179121\n",
      "Average test loss: 0.0031964772248433697\n",
      "Epoch 7/300\n",
      "Average test loss: 0.002974137505102489\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015423815162645446\n",
      "Average test loss: 0.002986664191302326\n",
      "Epoch 13/300\n",
      "Average training loss: 0.015279083245330387\n",
      "Average test loss: 0.0029184128625525368\n",
      "Epoch 14/300\n",
      "Average training loss: 0.015142370769547091\n",
      "Average test loss: 0.002900727904091279\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01501345535284943\n",
      "Average test loss: 0.0028748900656484894\n",
      "Epoch 16/300\n",
      "Average training loss: 0.014903601515210337\n",
      "Average test loss: 0.002842858495087259\n",
      "Epoch 17/300\n",
      "Average training loss: 0.014801963689426581\n",
      "Average test loss: 0.0028162187807675865\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01470383168425825\n",
      "Average test loss: 0.002816786516457796\n",
      "Epoch 19/300\n",
      "Average training loss: 0.014614295158121321\n",
      "Average test loss: 0.00277115830923948\n",
      "Epoch 20/300\n",
      "Average training loss: 0.014530192380978002\n",
      "Average test loss: 0.002774423069217139\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01445985892911752\n",
      "Average test loss: 0.0027566452094664176\n",
      "Epoch 22/300\n",
      "Average training loss: 0.014398902191056145\n",
      "Average test loss: 0.00272640320741468\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014329098633593983\n",
      "Average test loss: 0.0027099909045630033\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014284819067352348\n",
      "Average test loss: 0.0027166070886370207\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014221108054949177\n",
      "Average test loss: 0.0026845526119901072\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014171383812195725\n",
      "Average test loss: 0.002688071452288164\n",
      "Epoch 27/300\n",
      "Average training loss: 0.014126669907735454\n",
      "Average test loss: 0.0026896413018306095\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01408895601414972\n",
      "Average test loss: 0.002723099415293998\n",
      "Epoch 29/300\n",
      "Average training loss: 0.014048052272862857\n",
      "Average test loss: 0.0026568699321813053\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01401435272809532\n",
      "Average test loss: 0.002666502704636918\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013967370595369073\n",
      "Average test loss: 0.0026492331618857053\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013931834450198544\n",
      "Average test loss: 0.0026275606329242388\n",
      "Epoch 33/300\n",
      "Average training loss: 0.013905288223591116\n",
      "Average test loss: 0.0026175569080644184\n",
      "Epoch 34/300\n",
      "Average training loss: 0.013877078388300207\n",
      "Average test loss: 0.0026143348852379453\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01384858784576257\n",
      "Average test loss: 0.0026105305778069627\n",
      "Epoch 36/300\n",
      "Average training loss: 0.013825391271048123\n",
      "Average test loss: 0.002633869228263696\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013791929360893037\n",
      "Average test loss: 0.0025899223426563873\n",
      "Epoch 38/300\n",
      "Average training loss: 0.013766942038304275\n",
      "Average test loss: 0.002595174682637056\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01375075864377949\n",
      "Average test loss: 0.002580520973023441\n",
      "Epoch 40/300\n",
      "Average training loss: 0.013716903751922978\n",
      "Average test loss: 0.0026008605830785303\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013701688291298019\n",
      "Average test loss: 0.002594412116540803\n",
      "Epoch 42/300\n",
      "Average training loss: 0.013682337292366557\n",
      "Average test loss: 0.0025840126251180967\n",
      "Epoch 43/300\n",
      "Average training loss: 0.013663138187593883\n",
      "Average test loss: 0.0025815558278312286\n",
      "Epoch 44/300\n",
      "Average training loss: 0.013645457265277704\n",
      "Average test loss: 0.0025804016158605617\n",
      "Epoch 45/300\n",
      "Average training loss: 0.013626030686000982\n",
      "Average test loss: 0.0025904925229648748\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013597447113030486\n",
      "Average test loss: 0.0025653631538152693\n",
      "Epoch 47/300\n",
      "Average training loss: 0.013588164147403506\n",
      "Average test loss: 0.0025623994503791133\n",
      "Epoch 48/300\n",
      "Average training loss: 0.013571789344151815\n",
      "Average test loss: 0.0025733372571153774\n",
      "Epoch 49/300\n",
      "Average training loss: 0.013563345667388704\n",
      "Average test loss: 0.002577709707741936\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013544096347358491\n",
      "Average test loss: 0.002576518756647905\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013525105560819308\n",
      "Average test loss: 0.002575690823296706\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013457641430199146\n",
      "Average test loss: 0.0025940126876036326\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013444344941940573\n",
      "Average test loss: 0.002564467079109616\n",
      "Epoch 58/300\n",
      "Average training loss: 0.013420597684052255\n",
      "Average test loss: 0.0025594834432833727\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013421975728538302\n",
      "Average test loss: 0.002542695263193713\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013402749420868026\n",
      "Average test loss: 0.0025532449084437556\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013394613184862666\n",
      "Average test loss: 0.0025686697997152805\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013378260120749474\n",
      "Average test loss: 0.002547693194821477\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013358845166034168\n",
      "Average test loss: 0.0025395062636170123\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013365559237698714\n",
      "Average test loss: 0.0025410146433860062\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0133416618042522\n",
      "Average test loss: 0.00253033435696529\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013323445394635201\n",
      "Average test loss: 0.0025539272487577463\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013340178064174122\n",
      "Average test loss: 0.002566700824846824\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013316602595150471\n",
      "Average test loss: 0.002528708614822891\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013324597485363483\n",
      "Average test loss: 0.0025420917349143163\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013281863644719123\n",
      "Average test loss: 0.0025436014818648496\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013278341583907604\n",
      "Average test loss: 0.0025353729335798156\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013265793613261646\n",
      "Average test loss: 0.0025523423020624453\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013268329245348771\n",
      "Average test loss: 0.002545787448477414\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013255101551612218\n",
      "Average test loss: 0.0026324305364655125\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013247893755634626\n",
      "Average test loss: 0.0025275273728701804\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013233106013801363\n",
      "Average test loss: 0.0025413992613967924\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013216462256179916\n",
      "Average test loss: 0.002529765689642065\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013248006919191943\n",
      "Average test loss: 0.0025366479824814533\n",
      "Epoch 79/300\n",
      "Average training loss: 0.013215402772443163\n",
      "Average test loss: 0.0025640370024161206\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013181636821892526\n",
      "Average test loss: 0.002538399574657281\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013243227921426296\n",
      "Average test loss: 0.002547043782348434\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013174191101557679\n",
      "Average test loss: 0.002564113590038485\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013156759024494225\n",
      "Average test loss: 0.0025359082525182102\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013155907194647523\n",
      "Average test loss: 0.002548541328559319\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013145786753959126\n",
      "Average test loss: 0.0025438065061138735\n",
      "Epoch 86/300\n",
      "Average training loss: 0.013160625053776635\n",
      "Average test loss: 0.002530720977112651\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01313709070864651\n",
      "Average test loss: 0.002535400571094619\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01313141008797619\n",
      "Average test loss: 0.0025443108315683074\n",
      "Epoch 89/300\n",
      "Average training loss: 0.013122525123258432\n",
      "Average test loss: 0.002549488177523017\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013106827622486486\n",
      "Average test loss: 0.0025295506479839484\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013138320675326718\n",
      "Average test loss: 0.0025569066477732524\n",
      "Epoch 92/300\n",
      "Average training loss: 0.013079291134244865\n",
      "Average test loss: 0.0025271393816090294\n",
      "Epoch 93/300\n",
      "Average training loss: 0.013080476860205333\n",
      "Average test loss: 0.002553989730361435\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013079547854761283\n",
      "Average test loss: 0.0025556429270654915\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013078299365109868\n",
      "Average test loss: 0.0025405981412364377\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013077086894876426\n",
      "Average test loss: 0.002562256205930478\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013052126934130987\n",
      "Average test loss: 0.0025371244651161964\n",
      "Epoch 98/300\n",
      "Average training loss: 0.013067175193793244\n",
      "Average test loss: 0.002538382878113124\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013049419577750895\n",
      "Average test loss: 0.002543288460622231\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013042500192920367\n",
      "Average test loss: 0.002562072833793031\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013033819269802835\n",
      "Average test loss: 0.0025295167977197304\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012990814913478163\n",
      "Average test loss: 0.0025636339885079197\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012987419834567441\n",
      "Average test loss: 0.0025391023314247528\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01298234325647354\n",
      "Average test loss: 0.0025708824863864316\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012977957319882182\n",
      "Average test loss: 0.0025608785454597736\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0129689933301674\n",
      "Average test loss: 0.002529077911646002\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012972844305137793\n",
      "Average test loss: 0.0025349224371214707\n",
      "Epoch 111/300\n",
      "Average training loss: 0.012977144982251856\n",
      "Average test loss: 0.002543650881594254\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012949009254574775\n",
      "Average test loss: 0.0027324445402870574\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012954571877088811\n",
      "Average test loss: 0.002588766910135746\n",
      "Epoch 114/300\n",
      "Average training loss: 0.012937129028141498\n",
      "Average test loss: 0.0025466206804331806\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012946893591847685\n",
      "Average test loss: 0.002537665336082379\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012907726892166668\n",
      "Average test loss: 0.002534244622828232\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012912646849950155\n",
      "Average test loss: 0.0026788683366030454\n",
      "Epoch 118/300\n",
      "Average training loss: 0.012910516598158412\n",
      "Average test loss: 0.002562660876247618\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012916563572982947\n",
      "Average test loss: 0.002603418318451279\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01291499838232994\n",
      "Average test loss: 0.002558768600018488\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012877105887565347\n",
      "Average test loss: 0.002566424599952168\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012901122591561742\n",
      "Average test loss: 0.002558702304545376\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012886422724359565\n",
      "Average test loss: 0.0025770197289271485\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012874582753413253\n",
      "Average test loss: 0.0025428209331714445\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012845401769710912\n",
      "Average test loss: 0.002556251008477476\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012860727367301782\n",
      "Average test loss: 0.002549543307473262\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012875214713315168\n",
      "Average test loss: 0.002696425420956479\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012853106329010593\n",
      "Average test loss: 0.002564393421428071\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012845904077920649\n",
      "Average test loss: 0.002545968166449004\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012838051413496336\n",
      "Average test loss: 0.0025702651960568296\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012825895199345218\n",
      "Average test loss: 0.002572130885389116\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012853186392121844\n",
      "Average test loss: 0.0025858853090968397\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012810050649775398\n",
      "Average test loss: 0.0025780186373740434\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012806652016937732\n",
      "Average test loss: 0.0025632592723187474\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01279894364790784\n",
      "Average test loss: 0.0025558651569816803\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01279422946439849\n",
      "Average test loss: 0.0025650512557476757\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012829832557174894\n",
      "Average test loss: 0.002530766344525748\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01283438450511959\n",
      "Average test loss: 0.0025734045722832282\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012765899325410526\n",
      "Average test loss: 0.0027610186454322602\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01277950913624631\n",
      "Average test loss: 0.0027998012929326957\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012800749111506674\n",
      "Average test loss: 0.0025671000621385045\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012768131078945266\n",
      "Average test loss: 0.002574647191291054\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012762560786472426\n",
      "Average test loss: 0.002567424081472887\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012774887087444464\n",
      "Average test loss: 0.0025482627192719115\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012753366545670563\n",
      "Average test loss: 0.0025742664264722004\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012768007017672061\n",
      "Average test loss: 0.0025480838660150765\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01273689989745617\n",
      "Average test loss: 0.002562458225422435\n",
      "Epoch 148/300\n",
      "Average training loss: 0.012742421183321211\n",
      "Average test loss: 0.002609286767948005\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012736906039218107\n",
      "Average test loss: 0.002570535952639249\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012726235202617115\n",
      "Average test loss: 0.00260380994156003\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012740135817892021\n",
      "Average test loss: 0.002547929865618547\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012702435528238614\n",
      "Average test loss: 0.002587842094608479\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012700701541370816\n",
      "Average test loss: 0.002688221462070942\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012696018637054496\n",
      "Average test loss: 0.0032604871849632924\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01269150966985358\n",
      "Average test loss: 0.002563731982269221\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012697008581625091\n",
      "Average test loss: 0.002550537991544439\n",
      "Epoch 160/300\n",
      "Average training loss: 0.012675478310220771\n",
      "Average test loss: 0.002564651661035087\n",
      "Epoch 161/300\n",
      "Average training loss: 0.012680001022087203\n",
      "Average test loss: 0.0026329981750912138\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01268987720956405\n",
      "Average test loss: 0.00257151511642668\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012655103630489772\n",
      "Average test loss: 0.0025622471132212216\n",
      "Epoch 164/300\n",
      "Average training loss: 0.012659696736269528\n",
      "Average test loss: 0.0025613428172138004\n",
      "Epoch 165/300\n",
      "Average training loss: 0.012649237708085114\n",
      "Average test loss: 0.0026137261357572343\n",
      "Epoch 166/300\n",
      "Average training loss: 0.012654380271004306\n",
      "Average test loss: 0.0025765758550001516\n",
      "Epoch 167/300\n",
      "Average training loss: 0.012648856459392442\n",
      "Average test loss: 0.0026113539459183812\n",
      "Epoch 168/300\n",
      "Average training loss: 0.012658534306618903\n",
      "Average test loss: 0.0025674131132869256\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012643341902229521\n",
      "Average test loss: 0.0025890392053665385\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01264102568063471\n",
      "Average test loss: 0.002552610070962045\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012610724570850532\n",
      "Average test loss: 0.0025785431392076944\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012642292139430841\n",
      "Average test loss: 0.0026431635440223745\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012631179498301613\n",
      "Average test loss: 0.0025598148080623814\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012610068754189544\n",
      "Average test loss: 0.0026321995771593516\n",
      "Epoch 175/300\n",
      "Average training loss: 0.012632690868857835\n",
      "Average test loss: 0.0026032850983966557\n",
      "Epoch 176/300\n",
      "Average training loss: 0.012609901788334052\n",
      "Average test loss: 0.0026186770414933563\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012595680678884188\n",
      "Average test loss: 0.0025538337944696348\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012610228973958228\n",
      "Average test loss: 0.002596656040598949\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012593497804469533\n",
      "Average test loss: 0.0025722226860622564\n",
      "Epoch 180/300\n",
      "Average training loss: 0.012597120592163669\n",
      "Average test loss: 0.0026377127977708975\n",
      "Epoch 181/300\n",
      "Average training loss: 0.012579223639435238\n",
      "Average test loss: 0.00261891836548845\n",
      "Epoch 182/300\n",
      "Average training loss: 0.012583008188340399\n",
      "Average test loss: 0.0026941578928381203\n",
      "Epoch 183/300\n",
      "Average training loss: 0.012566357179648347\n",
      "Average test loss: 0.002622409315572845\n",
      "Epoch 184/300\n",
      "Average training loss: 0.012595830157399178\n",
      "Average test loss: 0.002593118256578843\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01256214153021574\n",
      "Average test loss: 0.0025873452073170078\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012583067942824629\n",
      "Average test loss: 0.0026491425226752956\n",
      "Epoch 187/300\n",
      "Average training loss: 0.012560802268485228\n",
      "Average test loss: 0.002610334085300565\n",
      "Epoch 188/300\n",
      "Average training loss: 0.012564367334875795\n",
      "Average test loss: 0.0034329493625296485\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01255927159719997\n",
      "Average test loss: 0.0025882380912080408\n",
      "Epoch 190/300\n",
      "Average training loss: 0.012536958715154065\n",
      "Average test loss: 0.0026138771707192063\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012560281878544225\n",
      "Average test loss: 0.00259429199496905\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012542886164453294\n",
      "Average test loss: 0.0025631871241041356\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0125403013403217\n",
      "Average test loss: 0.0026510742691655954\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012534905907180574\n",
      "Average test loss: 0.0025701710865315463\n",
      "Epoch 195/300\n",
      "Average training loss: 0.012545509552790058\n",
      "Average test loss: 0.002578893605205748\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012541663407451577\n",
      "Average test loss: 0.002658162045189076\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012528059575292799\n",
      "Average test loss: 0.0026038723757697475\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012519784937302271\n",
      "Average test loss: 0.002672754658386111\n",
      "Epoch 199/300\n",
      "Average training loss: 0.012514807289673222\n",
      "Average test loss: 0.0026288952221059135\n",
      "Epoch 200/300\n",
      "Average training loss: 0.012526995408866141\n",
      "Average test loss: 0.0025896051919294728\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012517002428571383\n",
      "Average test loss: 0.0025924916964852147\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01252184058394697\n",
      "Average test loss: 0.0025850895496292246\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012502848719557126\n",
      "Average test loss: 0.0026249869434783854\n",
      "Epoch 204/300\n",
      "Average training loss: 0.012507789250877168\n",
      "Average test loss: 0.0025959167290065025\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012513837996456358\n",
      "Average test loss: 0.0026466357320961025\n",
      "Epoch 206/300\n",
      "Average training loss: 0.012507123332056735\n",
      "Average test loss: 0.0026426056824210616\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012499116784168615\n",
      "Average test loss: 0.0025661598276346923\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012473516427808338\n",
      "Average test loss: 0.002618650181012021\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012483682854308023\n",
      "Average test loss: 0.0026856228632645474\n",
      "Epoch 210/300\n",
      "Average training loss: 0.012478835944500234\n",
      "Average test loss: 0.002593263734959894\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012484078506628672\n",
      "Average test loss: 0.00255393412295315\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012493921846979195\n",
      "Average test loss: 0.002610663191104929\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012475475338598092\n",
      "Average test loss: 0.002610549748254319\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01246137561980221\n",
      "Average test loss: 0.002625775023880932\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0124751847618156\n",
      "Average test loss: 0.0026030896707541413\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012471036627060837\n",
      "Average test loss: 0.0026331359294967517\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01246347752130694\n",
      "Average test loss: 0.0027081142771575185\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012472956757578585\n",
      "Average test loss: 0.0025643489671250182\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012442366435296006\n",
      "Average test loss: 0.002611295889856087\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01244775058577458\n",
      "Average test loss: 0.0025602803795288006\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012462639100021785\n",
      "Average test loss: 0.0025923882528311676\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012434308181206385\n",
      "Average test loss: 0.002632387519089712\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01243841422183646\n",
      "Average test loss: 0.0026230931745635138\n",
      "Epoch 224/300\n",
      "Average training loss: 0.012453936696880394\n",
      "Average test loss: 0.0025617230818089512\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012432898451884588\n",
      "Average test loss: 0.0025831594744490254\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012426815790434679\n",
      "Average test loss: 0.002606655944966608\n",
      "Epoch 227/300\n",
      "Average training loss: 0.012427744422521855\n",
      "Average test loss: 0.002620027872423331\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012425652103291617\n",
      "Average test loss: 0.0025951562674923078\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012433630297581355\n",
      "Average test loss: 0.00262147306299044\n",
      "Epoch 230/300\n",
      "Average training loss: 0.012412028917835818\n",
      "Average test loss: 0.00258785845256514\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012422823105421331\n",
      "Average test loss: 0.002599847954387466\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012411925575799412\n",
      "Average test loss: 0.002601604446147879\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012396703869932227\n",
      "Average test loss: 0.00264563186508086\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012421413784225782\n",
      "Average test loss: 0.002651003464228577\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01240009130951431\n",
      "Average test loss: 0.0025979267011086147\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012413579508662224\n",
      "Average test loss: 0.002594676495736672\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012417564731505182\n",
      "Average test loss: 0.0026251419606722064\n",
      "Epoch 238/300\n",
      "Average training loss: 0.012397348543008169\n",
      "Average test loss: 0.002608860417890052\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012399248119857577\n",
      "Average test loss: 0.002587413705057568\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01239188720865382\n",
      "Average test loss: 0.0025916187674221067\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012378042225208547\n",
      "Average test loss: 0.00264300806965265\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01240919391810894\n",
      "Average test loss: 0.0025975994277331565\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012380698279374176\n",
      "Average test loss: 0.0025985645070258113\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012383933009372817\n",
      "Average test loss: 0.0025924876092208757\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01237152791602744\n",
      "Average test loss: 0.0026482414812263517\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012380193141599495\n",
      "Average test loss: 0.002565043554123905\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012373777784407139\n",
      "Average test loss: 0.0026032849937263464\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012365220257805454\n",
      "Average test loss: 0.002655703684935967\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012363899756222964\n",
      "Average test loss: 0.002673105839225981\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012371812897423904\n",
      "Average test loss: 0.0026944039254966708\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012372052184409565\n",
      "Average test loss: 0.0025694457010055584\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012351375238762962\n",
      "Average test loss: 0.002668313747478856\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012349803138938215\n",
      "Average test loss: 0.002611353782936931\n",
      "Epoch 254/300\n",
      "Average training loss: 0.012381644828452004\n",
      "Average test loss: 0.0025902631940941017\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012346159694095452\n",
      "Average test loss: 0.0026076561104920177\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01234971055885156\n",
      "Average test loss: 0.0026291756280180482\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012355298539830578\n",
      "Average test loss: 0.0025991344400164156\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012340077817440034\n",
      "Average test loss: 0.0026057484936383036\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012351881795459324\n",
      "Average test loss: 0.0026753390445891354\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012334579173061582\n",
      "Average test loss: 0.0026161351504011285\n",
      "Epoch 261/300\n",
      "Average test loss: 0.0026414587510128814\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012327871777117253\n",
      "Average test loss: 0.002678180334882604\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01232618727369441\n",
      "Average test loss: 0.0026239355767352715\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01232281481143501\n",
      "Average test loss: 0.002738897972429792\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012317169805367787\n",
      "Average test loss: 0.00266978933972617\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012307759621904956\n",
      "Average test loss: 0.002634094663792186\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012319565574742026\n",
      "Average test loss: 0.0026484351358893846\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012313154480523534\n",
      "Average test loss: 0.0026952314356135\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012318625008894338\n",
      "Average test loss: 0.0029181157547152705\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012318364725344711\n",
      "Average test loss: 0.002609792131206228\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012304728320075406\n",
      "Average test loss: 0.0026202402430483035\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012305392051736514\n",
      "Average test loss: 0.0026602204515495235\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012291388662325011\n",
      "Average test loss: 0.0026382531192567613\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012300044631792439\n",
      "Average test loss: 0.002635839612533649\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012302301227218575\n",
      "Average test loss: 0.0025813759888211885\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01229665318545368\n",
      "Average test loss: 0.0026633847165438863\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012298416333893935\n",
      "Average test loss: 0.0027291631908673377\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012298289545708233\n",
      "Average test loss: 0.0026053306052668227\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012282575756311417\n",
      "Average test loss: 0.0026082622410936486\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012295556558503045\n",
      "Average test loss: 0.002644027626141906\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012270120363268588\n",
      "Average test loss: 0.0026243222006079223\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012279051780700684\n",
      "Average test loss: 0.0027319092812637486\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012282311580247349\n",
      "Average test loss: 0.002639950095468925\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012284414772358206\n",
      "Average test loss: 0.002668249125695891\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01225651270399491\n",
      "Average test loss: 0.002615831980067823\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012276485405862332\n",
      "Average test loss: 0.0026848048919604884\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012272148040433724\n",
      "Average test loss: 0.002623351190860073\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012258735265996721\n",
      "Average test loss: 0.002664876357341806\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012270524344510503\n",
      "Average test loss: 0.0027123038013362223\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012287317788435352\n",
      "Average test loss: 0.0026646058183784284\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01225712334861358\n",
      "Average test loss: 0.0026499692670380074\n",
      "Epoch 296/300\n",
      "Average training loss: 0.012265186741948127\n",
      "Average test loss: 0.0026976709415515265\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012250629357165761\n",
      "Average test loss: 0.002616956401409374\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012256689018673366\n",
      "Average test loss: 0.0027092444350322086\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012252244743208091\n",
      "Average test loss: 0.002598828751179907\n",
      "Epoch 300/300\n",
      "Average training loss: 0.012242499258783129\n",
      "Average test loss: 0.0026083837379184036\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04373945563534896\n",
      "Average test loss: 0.003176986863215764\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015525161858234141\n",
      "Average test loss: 0.0029615457970649005\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014613526585201422\n",
      "Average test loss: 0.0028057883969611593\n",
      "Epoch 4/300\n",
      "Average training loss: 0.014015150835116704\n",
      "Average test loss: 0.002630823549297121\n",
      "Epoch 5/300\n",
      "Average training loss: 0.013550896956688829\n",
      "Average test loss: 0.0025729126524594095\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013164385880033175\n",
      "Average test loss: 0.0024479989643312163\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012832237022618452\n",
      "Average test loss: 0.0023907427236230837\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01255716349184513\n",
      "Average test loss: 0.002339584520811008\n",
      "Epoch 9/300\n",
      "Average training loss: 0.012291924933592479\n",
      "Average test loss: 0.002282720818598237\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01208234047724141\n",
      "Average test loss: 0.002282930955601235\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011871232209934128\n",
      "Average test loss: 0.002219980538305309\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01169976214526428\n",
      "Average test loss: 0.0021895376527681946\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011539978145311276\n",
      "Average test loss: 0.002104213574901223\n",
      "Epoch 14/300\n",
      "Average training loss: 0.011401256586942408\n",
      "Average test loss: 0.002105940362852481\n",
      "Epoch 15/300\n",
      "Average training loss: 0.011276169034341971\n",
      "Average test loss: 0.002097331415034003\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010738189385996924\n",
      "Average test loss: 0.0019358528386801482\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010666974601646265\n",
      "Average test loss: 0.0019350317544821235\n",
      "Epoch 23/300\n",
      "Average training loss: 0.010618388933026128\n",
      "Average test loss: 0.0019248493363459905\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01056357406742043\n",
      "Average test loss: 0.0019306900275664197\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010518260409434637\n",
      "Average test loss: 0.001888550295908418\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010458781319359938\n",
      "Average test loss: 0.001895377948673235\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0104224916746219\n",
      "Average test loss: 0.001884876620852285\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010377513397485019\n",
      "Average test loss: 0.0018631061874330045\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01033880714327097\n",
      "Average test loss: 0.0018663562841506468\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01030832070691718\n",
      "Average test loss: 0.0018538575409394172\n",
      "Epoch 31/300\n",
      "Average training loss: 0.010139417848653264\n",
      "Average test loss: 0.0018097061529341672\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010109597152719895\n",
      "Average test loss: 0.0018242497413108747\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010097002762887212\n",
      "Average test loss: 0.0018230787213477824\n",
      "Epoch 39/300\n",
      "Average training loss: 0.010059169632693132\n",
      "Average test loss: 0.0018211272110541662\n",
      "Epoch 40/300\n",
      "Average training loss: 0.010047572481135527\n",
      "Average test loss: 0.0018003041081958346\n",
      "Epoch 41/300\n",
      "Average training loss: 0.010025741761757268\n",
      "Average test loss: 0.0018017440054358708\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01001132290230857\n",
      "Average test loss: 0.0018062099054869678\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010002513168172704\n",
      "Average test loss: 0.0018181472551077604\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009975670725521114\n",
      "Average test loss: 0.0017944147618901398\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009966273379822572\n",
      "Average test loss: 0.001802199822333124\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009946299440330928\n",
      "Average test loss: 0.001779140280559659\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00992551251169708\n",
      "Average test loss: 0.0018054125522159869\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00991326198561324\n",
      "Average test loss: 0.0017875810445596775\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009895399044785235\n",
      "Average test loss: 0.0017770395767357614\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009888356814781825\n",
      "Average test loss: 0.0017805271677983304\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009873932970480786\n",
      "Average test loss: 0.001779603908614566\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00985884532166852\n",
      "Average test loss: 0.0017652231144408385\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009845533181395796\n",
      "Average test loss: 0.0017668636937936146\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009831136053635014\n",
      "Average test loss: 0.0017913384325802327\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009826306898146868\n",
      "Average test loss: 0.0017730197740925683\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009819045477857192\n",
      "Average test loss: 0.00177499236456222\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009801963260190354\n",
      "Average test loss: 0.0017726747244596482\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00978030310653978\n",
      "Average test loss: 0.0017535444759867258\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009781898817254437\n",
      "Average test loss: 0.0017596611322628127\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009778605024019877\n",
      "Average test loss: 0.0017612932650372386\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009756458942674929\n",
      "Average test loss: 0.001768220246045126\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009751647214094798\n",
      "Average test loss: 0.0017857027941693862\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009740125286082427\n",
      "Average test loss: 0.0017775393279476298\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009739806030359533\n",
      "Average test loss: 0.0018446966494537061\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009721444712744819\n",
      "Average test loss: 0.0017600762355658743\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009709406011634402\n",
      "Average test loss: 0.0017555222142901685\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00970361830583877\n",
      "Average test loss: 0.0017612367115086979\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009700455554657513\n",
      "Average test loss: 0.0017495577934508522\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009683958926962482\n",
      "Average test loss: 0.0017586645162146952\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009688463711904155\n",
      "Average test loss: 0.0017559578377339576\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009659159241037236\n",
      "Average test loss: 0.0017387327306593457\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009663107347157267\n",
      "Average test loss: 0.0017618335511328446\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009647659877108203\n",
      "Average test loss: 0.001762683239661985\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00963855878677633\n",
      "Average test loss: 0.001746957545272178\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009638448006163041\n",
      "Average test loss: 0.0017446491027043925\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009628637823379702\n",
      "Average test loss: 0.001784601421198911\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009623476858354277\n",
      "Average test loss: 0.0017813297065181865\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009614687983774477\n",
      "Average test loss: 0.001768265877953834\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009600891476704015\n",
      "Average test loss: 0.001751825828726093\n",
      "Epoch 80/300\n",
      "Average training loss: 0.009604786535931957\n",
      "Average test loss: 0.0017856497097139556\n",
      "Epoch 81/300\n",
      "Average training loss: 0.009592067433728112\n",
      "Average test loss: 0.0017575161680579185\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00957644143452247\n",
      "Average test loss: 0.0017464975346293714\n",
      "Epoch 83/300\n",
      "Average training loss: 0.009578573992268906\n",
      "Average test loss: 0.0017611042809051773\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009601668557359113\n",
      "Average test loss: 0.0017572473109596306\n",
      "Epoch 85/300\n",
      "Average training loss: 0.009575177983277373\n",
      "Average test loss: 0.0017418793383985759\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00955246395038234\n",
      "Average test loss: 0.0017685199797981316\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009547469422635105\n",
      "Average test loss: 0.0017503757896936603\n",
      "Epoch 88/300\n",
      "Average training loss: 0.009540404830541875\n",
      "Average test loss: 0.0017482355375670723\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009537427960998482\n",
      "Average test loss: 0.0017467380710360077\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009534631276296245\n",
      "Average test loss: 0.0017602886686929398\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009530957796093491\n",
      "Average test loss: 0.001799582549577786\n",
      "Epoch 92/300\n",
      "Average training loss: 0.009509783927765157\n",
      "Average test loss: 0.0017543527885443635\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009489624082214302\n",
      "Average test loss: 0.0017335574310272932\n",
      "Epoch 97/300\n",
      "Average training loss: 0.009496016724656026\n",
      "Average test loss: 0.0017392602637410163\n",
      "Epoch 98/300\n",
      "Average training loss: 0.00948155032015509\n",
      "Average test loss: 0.0017645285050902101\n",
      "Epoch 99/300\n",
      "Average training loss: 0.009506740880923138\n",
      "Average test loss: 0.0017365596253010962\n",
      "Epoch 100/300\n",
      "Average training loss: 0.009467600863840846\n",
      "Average test loss: 0.0017482886906299326\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009481987490422197\n",
      "Average test loss: 0.0017545807797772189\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009461370506220394\n",
      "Average test loss: 0.0017346833830492363\n",
      "Epoch 103/300\n",
      "Average training loss: 0.009481103938486841\n",
      "Average test loss: 0.001747001296426687\n",
      "Epoch 104/300\n",
      "Average training loss: 0.009470139146678977\n",
      "Average test loss: 0.001740784801542759\n",
      "Epoch 105/300\n",
      "Average training loss: 0.009440273801899619\n",
      "Average test loss: 0.001767599173511068\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009436930610487858\n",
      "Average test loss: 0.001880643455311656\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0094326282714804\n",
      "Average test loss: 0.0018375280954771572\n",
      "Epoch 108/300\n",
      "Average training loss: 0.009424782209098339\n",
      "Average test loss: 0.001753374516343077\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00947540540372332\n",
      "Average test loss: 0.0017520922070576085\n",
      "Epoch 110/300\n",
      "Average training loss: 0.009406073234975339\n",
      "Average test loss: 0.001755372243726419\n",
      "Epoch 111/300\n",
      "Average training loss: 0.009410146390398343\n",
      "Average test loss: 0.0017327388194907042\n",
      "Epoch 112/300\n",
      "Average training loss: 0.009413726496199766\n",
      "Average test loss: 0.0017459274284127686\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009409600182539887\n",
      "Average test loss: 0.0017484798862909276\n",
      "Epoch 114/300\n",
      "Average training loss: 0.009426004250430399\n",
      "Average test loss: 0.0017719602196787794\n",
      "Epoch 115/300\n",
      "Average training loss: 0.009390849034819338\n",
      "Average test loss: 0.0018874192794577944\n",
      "Epoch 116/300\n",
      "Average training loss: 0.009391408792800374\n",
      "Average test loss: 0.0017391846624927388\n",
      "Epoch 117/300\n",
      "Average training loss: 0.009381926431010167\n",
      "Average test loss: 0.001748430620568494\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00938585797448953\n",
      "Average test loss: 0.0017675434093301495\n",
      "Epoch 119/300\n",
      "Average training loss: 0.009399463115466966\n",
      "Average test loss: 0.0017666905098077323\n",
      "Epoch 120/300\n",
      "Average training loss: 0.009387343281673061\n",
      "Average test loss: 0.0017617464458776844\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009381245179308785\n",
      "Average test loss: 0.0017466826600333054\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009348651677783993\n",
      "Average test loss: 0.001863977090196891\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009355608081652058\n",
      "Average test loss: 0.0017502668204510377\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009356213436772426\n",
      "Average test loss: 0.001738599324391948\n",
      "Epoch 125/300\n",
      "Average training loss: 0.009359607893559668\n",
      "Average test loss: 0.0017320884403565693\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009361451569530698\n",
      "Average test loss: 0.0017349826297205356\n",
      "Epoch 127/300\n",
      "Average training loss: 0.009359074725045099\n",
      "Average test loss: 0.0017573893790443738\n",
      "Epoch 128/300\n",
      "Average training loss: 0.009333796630303065\n",
      "Average test loss: 0.001762745674389104\n",
      "Epoch 129/300\n",
      "Average training loss: 0.009333174594574504\n",
      "Average test loss: 0.0017694409036388\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009331396754417154\n",
      "Average test loss: 0.001761389863677323\n",
      "Epoch 131/300\n",
      "Average training loss: 0.009327188286516401\n",
      "Average test loss: 0.0017556745047784515\n",
      "Epoch 132/300\n",
      "Average training loss: 0.00931048838628663\n",
      "Average test loss: 0.0017583184941775268\n",
      "Epoch 133/300\n",
      "Average training loss: 0.00932372141132752\n",
      "Average test loss: 0.0017653180270766219\n",
      "Epoch 134/300\n",
      "Average training loss: 0.00931555995841821\n",
      "Average test loss: 0.0017469732658937573\n",
      "Epoch 135/300\n",
      "Average training loss: 0.009334579242600335\n",
      "Average test loss: 0.0017541076991086206\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009345597290330463\n",
      "Average test loss: 0.0017658242837836344\n",
      "Epoch 137/300\n",
      "Average training loss: 0.009285496212542058\n",
      "Average test loss: 0.0017404919431234398\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00927799938701921\n",
      "Average test loss: 0.0017445286850755414\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00928448919662171\n",
      "Average test loss: 0.0017539439822236696\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009287239695588747\n",
      "Average test loss: 0.001764472286009954\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009289278590844737\n",
      "Average test loss: 0.001744291851297021\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0092463216268354\n",
      "Average test loss: 0.00175070823646254\n",
      "Epoch 148/300\n",
      "Average training loss: 0.009258353381521173\n",
      "Average test loss: 0.0017714462425145838\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009252115429275566\n",
      "Average test loss: 0.0017473890646878215\n",
      "Epoch 150/300\n",
      "Average training loss: 0.009259721446368429\n",
      "Average test loss: 0.0017464867542601294\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009263222806155682\n",
      "Average test loss: 0.001754008227855795\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00923867998106612\n",
      "Average test loss: 0.0017442477617619766\n",
      "Epoch 153/300\n",
      "Average training loss: 0.00925388671292199\n",
      "Average test loss: 0.0017580446865823533\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009233836516737938\n",
      "Average test loss: 0.0017679467684485846\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009259339936077594\n",
      "Average test loss: 0.001757022340471546\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009225164108806186\n",
      "Average test loss: 0.0017498113120802575\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00924100450343556\n",
      "Average test loss: 0.0017666721230165825\n",
      "Epoch 158/300\n",
      "Average training loss: 0.009227380122575495\n",
      "Average test loss: 0.0017656227265381152\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00921825719707542\n",
      "Average test loss: 0.0017710460013606482\n",
      "Epoch 160/300\n",
      "Average training loss: 0.00923503349804216\n",
      "Average test loss: 0.0017580150675235522\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009221146722634634\n",
      "Average test loss: 0.0017892790224610104\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009227933473471138\n",
      "Average test loss: 0.0017430126585273279\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00920341413302554\n",
      "Average test loss: 0.0017676688368535704\n",
      "Epoch 164/300\n",
      "Average training loss: 0.009214649279084471\n",
      "Average test loss: 0.0017484576534479857\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009199471382631197\n",
      "Average test loss: 0.0017573810602641768\n",
      "Epoch 166/300\n",
      "Average training loss: 0.009203582110504309\n",
      "Average test loss: 0.0017330384502808253\n",
      "Epoch 167/300\n",
      "Average training loss: 0.009196552950888871\n",
      "Average test loss: 0.0017499351104100546\n",
      "Epoch 168/300\n",
      "Average training loss: 0.009290294504827924\n",
      "Average test loss: 0.001788916754639811\n",
      "Epoch 169/300\n",
      "Average training loss: 0.009175179788221915\n",
      "Average test loss: 0.0017713401423146327\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00917380068409774\n",
      "Average test loss: 0.002698217420735293\n",
      "Epoch 171/300\n",
      "Average training loss: 0.009172406874183152\n",
      "Average test loss: 0.0017418277085655266\n",
      "Epoch 172/300\n",
      "Average training loss: 0.009178075816896227\n",
      "Average test loss: 0.0017840247048685949\n",
      "Epoch 173/300\n",
      "Average training loss: 0.009177117912305726\n",
      "Average test loss: 0.0017931189356992642\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009193081277940008\n",
      "Average test loss: 0.00175745823348148\n",
      "Epoch 175/300\n",
      "Average training loss: 0.009183737232453294\n",
      "Average test loss: 0.0017569144724143876\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009171901947508256\n",
      "Average test loss: 0.0017497045081108808\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009167548900263178\n",
      "Average test loss: 0.0017419856211377514\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009154689132339425\n",
      "Average test loss: 0.0017647168127199013\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009154369973474078\n",
      "Average test loss: 0.0017835031433237924\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009170125521719455\n",
      "Average test loss: 0.0017651730665109223\n",
      "Epoch 181/300\n",
      "Average training loss: 0.009154992518325647\n",
      "Average test loss: 0.001775060629679097\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009162444940043821\n",
      "Average test loss: 0.0017849678364064959\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009136276243461503\n",
      "Average test loss: 0.0017414028537977072\n",
      "Epoch 184/300\n",
      "Average training loss: 0.009220942392531368\n",
      "Average test loss: 0.0017592716665110655\n",
      "Epoch 185/300\n",
      "Average training loss: 0.00913617867935035\n",
      "Average test loss: 0.0017639476955971785\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0091333626806736\n",
      "Average test loss: 0.0018458451357566648\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009130386812405454\n",
      "Average test loss: 0.001775473795624243\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009126164368457264\n",
      "Average test loss: 0.0017687125590940317\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009153048192461332\n",
      "Average test loss: 0.0017762316748913792\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00912622880977061\n",
      "Average test loss: 0.0018240113312171565\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009114877224796349\n",
      "Average test loss: 0.0017653770654772719\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009105716123763059\n",
      "Average test loss: 0.0017572992608572047\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00910967568308115\n",
      "Average test loss: 0.001768283841200173\n",
      "Epoch 199/300\n",
      "Average training loss: 0.00910319583904412\n",
      "Average test loss: 0.002283747575349278\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009118465115212733\n",
      "Average test loss: 0.0017561426040612988\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00910599540380968\n",
      "Average test loss: 0.0017666678592148754\n",
      "Epoch 202/300\n",
      "Average training loss: 0.009095068740348022\n",
      "Average test loss: 0.0017556934381524722\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009091682459745141\n",
      "Average test loss: 0.0017715966264820761\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009088112749159336\n",
      "Average test loss: 0.0017726923089681401\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009101103199852837\n",
      "Average test loss: 0.00175656069866899\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00908386007986135\n",
      "Average test loss: 0.0017603782048034998\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009096312609811624\n",
      "Average test loss: 0.0017754151973252495\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009083010523269574\n",
      "Average test loss: 0.0017664600893234214\n",
      "Epoch 209/300\n",
      "Average training loss: 0.009075666381667057\n",
      "Average test loss: 0.0017489077491271827\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009098317861143086\n",
      "Average test loss: 0.0017547737463480897\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009071519253568517\n",
      "Average test loss: 0.0017699803124285407\n",
      "Epoch 212/300\n",
      "Average training loss: 0.009072237388955223\n",
      "Average test loss: 0.0017691656013743744\n",
      "Epoch 213/300\n",
      "Average training loss: 0.009075722778836887\n",
      "Average test loss: 0.0017944959826353523\n",
      "Epoch 214/300\n",
      "Average training loss: 0.009087498881750637\n",
      "Average test loss: 0.0018861557847509782\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009060135795424381\n",
      "Average test loss: 0.001750034564278192\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009055294600625832\n",
      "Average test loss: 0.0017615074357017874\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009066572331306007\n",
      "Average test loss: 0.001766609684874614\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009067713239954577\n",
      "Average test loss: 0.001765967301506963\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009056234678874413\n",
      "Average test loss: 0.0018144004026220905\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009077176781992118\n",
      "Average test loss: 0.0017464779735439353\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009055732297400634\n",
      "Average test loss: 0.002392536353112923\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009059751398033566\n",
      "Average test loss: 0.0018068206128146914\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009053744651791122\n",
      "Average test loss: 0.0017621700101428561\n",
      "Epoch 224/300\n",
      "Average training loss: 0.009052664361894131\n",
      "Average test loss: 0.0018186704630239142\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009044912700437837\n",
      "Average test loss: 0.0017926433274729385\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009053804810262389\n",
      "Average test loss: 0.001769016455238064\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009046323721607526\n",
      "Average test loss: 0.001767014063998229\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00904070371058252\n",
      "Average test loss: 0.0018145187437120412\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009043844687441984\n",
      "Average test loss: 0.001787232828223043\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009047374313076338\n",
      "Average test loss: 0.0017624412906459637\n",
      "Epoch 231/300\n",
      "Average training loss: 0.00902753356554442\n",
      "Average test loss: 0.0017661572549906041\n",
      "Epoch 232/300\n",
      "Average training loss: 0.009047831890897619\n",
      "Average test loss: 0.0017729967585247424\n",
      "Epoch 233/300\n",
      "Average training loss: 0.009031473749627669\n",
      "Average test loss: 0.001788195354760521\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009024145568410556\n",
      "Average test loss: 0.001772560242563486\n",
      "Epoch 235/300\n",
      "Average training loss: 0.009024498749938275\n",
      "Average test loss: 0.001793338085524738\n",
      "Epoch 236/300\n",
      "Average training loss: 0.009030686108602418\n",
      "Average test loss: 0.0017612523852537077\n",
      "Epoch 237/300\n",
      "Average training loss: 0.009023112659653028\n",
      "Average test loss: 0.0017769263317394587\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009027087344891495\n",
      "Average test loss: 0.0017926332949557238\n",
      "Epoch 239/300\n",
      "Average training loss: 0.009019004884279437\n",
      "Average test loss: 0.0017958706895717317\n",
      "Epoch 240/300\n",
      "Average training loss: 0.00901423695931832\n",
      "Average test loss: 0.00177961594859759\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009016451223442952\n",
      "Average test loss: 0.0017590104720244806\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009013245224952698\n",
      "Average test loss: 0.0017666342663061288\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00901598847988579\n",
      "Average test loss: 0.0017591859999423227\n",
      "Epoch 244/300\n",
      "Average training loss: 0.00900368452899986\n",
      "Average test loss: 0.0017732227686792612\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009001344313224156\n",
      "Average test loss: 0.001795547587952266\n",
      "Epoch 246/300\n",
      "Average training loss: 0.009012635409004158\n",
      "Average test loss: 0.0017998120081093576\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009024348059462176\n",
      "Average test loss: 0.0038129156501963734\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008991514623165131\n",
      "Average test loss: 0.001817622461459703\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009015725334071452\n",
      "Average test loss: 0.0017646843742372261\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009001014032711585\n",
      "Average test loss: 0.001807832887603177\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008985758550879028\n",
      "Average test loss: 0.0018398936996236444\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008998186862717071\n",
      "Average test loss: 0.0017933039147820737\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008989649948146609\n",
      "Average test loss: 0.0017837518766108487\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008994067379169994\n",
      "Average test loss: 0.0017867848434382015\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008988541884554757\n",
      "Average test loss: 0.0017750378383530512\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008994699450002776\n",
      "Average test loss: 0.0017728428366697497\n",
      "Epoch 257/300\n",
      "Average training loss: 0.00897457655850384\n",
      "Average test loss: 0.0017645163611612385\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008983155531187852\n",
      "Average test loss: 0.001761020314310574\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008981027309265402\n",
      "Average test loss: 0.0017551788195139832\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009026966861552662\n",
      "Average test loss: 0.0017945062257349491\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008978495840397146\n",
      "Average test loss: 0.0017761710172312128\n",
      "Epoch 262/300\n",
      "Average training loss: 0.009004121061828402\n",
      "Average test loss: 0.001802028758658303\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00896839931110541\n",
      "Average test loss: 0.0017666417769570317\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008966362348033323\n",
      "Average test loss: 0.0017790860536818704\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008970938896553383\n",
      "Average test loss: 0.0017640695373217264\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008969859010229508\n",
      "Average test loss: 0.001835244251622094\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008979560049043761\n",
      "Average test loss: 0.0017864386810817652\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008971030611130927\n",
      "Average test loss: 0.0017875883407476876\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008983779710200097\n",
      "Average test loss: 0.001759152791566319\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008966696379499303\n",
      "Average test loss: 0.0018000753734053836\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008974622588190767\n",
      "Average test loss: 0.0017625911304106315\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008963278224484788\n",
      "Average test loss: 0.0018078598470116654\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008949959403938718\n",
      "Average test loss: 0.0018125229136397442\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008955750562664535\n",
      "Average test loss: 0.0018804394360631704\n",
      "Epoch 275/300\n",
      "Average training loss: 0.00896361772550477\n",
      "Average test loss: 0.0018168847206462588\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008951156933688455\n",
      "Average test loss: 0.0017912991396668884\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008952373978578383\n",
      "Average test loss: 0.0017740836078301073\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008981393278059031\n",
      "Average test loss: 0.001806249315953917\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008944345350894663\n",
      "Average test loss: 0.001778226245712075\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008951748041643037\n",
      "Average test loss: 0.0017821322411505712\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008977079471780194\n",
      "Average test loss: 0.0018107578334295086\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008939829358210166\n",
      "Average test loss: 0.0017742000302920738\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008950869857437082\n",
      "Average test loss: 0.001800416827822725\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008936021732373371\n",
      "Average test loss: 0.0017730936862321363\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008952274784859684\n",
      "Average test loss: 0.0017792432156081002\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008942756791081693\n",
      "Average test loss: 0.0017703954715074764\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008945567158361275\n",
      "Average test loss: 0.0017871419148933556\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008921225160360336\n",
      "Average test loss: 0.001780687759319941\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008938836336135864\n",
      "Average test loss: 0.0018734499141573905\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008946147824327151\n",
      "Average test loss: 0.001776411885395646\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008928218422664537\n",
      "Average test loss: 0.0018032782439970309\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008918939674066173\n",
      "Average test loss: 0.001801238757558167\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008920059027357234\n",
      "Average test loss: 0.0017908303011208773\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008947799166871441\n",
      "Average test loss: 0.0018032085506452455\n",
      "Epoch 295/300\n",
      "Average training loss: 0.00891554113684429\n",
      "Average test loss: 0.0018231597955649097\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008921686539219485\n",
      "Average test loss: 0.0018065444853984647\n",
      "Epoch 297/300\n",
      "Average training loss: 0.00892110368112723\n",
      "Average test loss: 0.0017640331091566218\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008930870757334761\n",
      "Average test loss: 0.0017635208278273542\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008949413361648718\n",
      "Average test loss: 0.006159543589585357\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008920304325305753\n",
      "Average test loss: 0.0017891701128747728\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.039740936463077865\n",
      "Average test loss: 0.0026704877372831105\n",
      "Epoch 2/300\n",
      "Average training loss: 0.013007335133022732\n",
      "Average test loss: 0.0024162702968137132\n",
      "Epoch 3/300\n",
      "Average training loss: 0.012023983058830102\n",
      "Average test loss: 0.0022557773700811797\n",
      "Epoch 4/300\n",
      "Average training loss: 0.011401002684401142\n",
      "Average test loss: 0.0020838752284439072\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01093249746494823\n",
      "Average test loss: 0.001982282343217068\n",
      "Epoch 6/300\n",
      "Average training loss: 0.010533124367396037\n",
      "Average test loss: 0.0019007360217057996\n",
      "Epoch 7/300\n",
      "Average training loss: 0.010213341849545638\n",
      "Average test loss: 0.001832664088656505\n",
      "Epoch 8/300\n",
      "Average training loss: 0.009926425279428561\n",
      "Average test loss: 0.0018083015955570672\n",
      "Epoch 9/300\n",
      "Average training loss: 0.009682543774445851\n",
      "Average test loss: 0.0017406887273407644\n",
      "Epoch 10/300\n",
      "Average training loss: 0.009448966788334979\n",
      "Average test loss: 0.0016735548747496473\n",
      "Epoch 11/300\n",
      "Average training loss: 0.009272486900289853\n",
      "Average test loss: 0.0016253079432580206\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009105185229745176\n",
      "Average test loss: 0.0016323417536914348\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0089646447027723\n",
      "Average test loss: 0.0015689406834749713\n",
      "Epoch 14/300\n",
      "Average training loss: 0.008834118266072537\n",
      "Average test loss: 0.001575972103824218\n",
      "Epoch 15/300\n",
      "Average training loss: 0.008718699266513188\n",
      "Average test loss: 0.0015367348316229052\n",
      "Epoch 16/300\n",
      "Average training loss: 0.008614747655060557\n",
      "Average test loss: 0.0014969326207000349\n",
      "Epoch 17/300\n",
      "Average training loss: 0.008524518602424197\n",
      "Average test loss: 0.0014857498706421918\n",
      "Epoch 18/300\n",
      "Average training loss: 0.008441828738070197\n",
      "Average test loss: 0.0014856167218854858\n",
      "Epoch 19/300\n",
      "Average training loss: 0.008366777401003573\n",
      "Average test loss: 0.0014536940180179146\n",
      "Epoch 20/300\n",
      "Average training loss: 0.00830718369036913\n",
      "Average test loss: 0.0014372779003137515\n",
      "Epoch 21/300\n",
      "Average training loss: 0.008248698739955823\n",
      "Average test loss: 0.001416251739900973\n",
      "Epoch 22/300\n",
      "Average training loss: 0.008177805416285992\n",
      "Average test loss: 0.0014651808299952084\n",
      "Epoch 23/300\n",
      "Average training loss: 0.008136110723432568\n",
      "Average test loss: 0.0013994608820519514\n",
      "Epoch 24/300\n",
      "Average training loss: 0.008092931816147434\n",
      "Average test loss: 0.0014172659336278836\n",
      "Epoch 25/300\n",
      "Average training loss: 0.008039940518637498\n",
      "Average test loss: 0.0014000270513610706\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00801033589326673\n",
      "Average test loss: 0.0013829038358396954\n",
      "Epoch 27/300\n",
      "Average training loss: 0.007965338432126575\n",
      "Average test loss: 0.0013869154372562966\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007934065887497531\n",
      "Average test loss: 0.001357826864760783\n",
      "Epoch 29/300\n",
      "Average training loss: 0.007902038974894417\n",
      "Average test loss: 0.0013458780266034107\n",
      "Epoch 30/300\n",
      "Average training loss: 0.007867939758631919\n",
      "Average test loss: 0.0013491356006513039\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007843467317521571\n",
      "Average test loss: 0.0013299648876612385\n",
      "Epoch 32/300\n",
      "Average training loss: 0.007816826177553998\n",
      "Average test loss: 0.001332086344766948\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0077879785609742\n",
      "Average test loss: 0.0013580161189246508\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00776475210653411\n",
      "Average test loss: 0.001365518962809195\n",
      "Epoch 35/300\n",
      "Average training loss: 0.007740647803578112\n",
      "Average test loss: 0.001376693615068992\n",
      "Epoch 36/300\n",
      "Average training loss: 0.007717457379731867\n",
      "Average test loss: 0.00132222622870985\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0077081661940448815\n",
      "Average test loss: 0.0013332604686212208\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00768722103536129\n",
      "Average test loss: 0.0013167703142907058\n",
      "Epoch 39/300\n",
      "Average training loss: 0.007665330942306254\n",
      "Average test loss: 0.0013177730998852188\n",
      "Epoch 40/300\n",
      "Average training loss: 0.007646985282500585\n",
      "Average test loss: 0.0013109762834178076\n",
      "Epoch 41/300\n",
      "Average training loss: 0.007629189947826995\n",
      "Average test loss: 0.0012937317999700704\n",
      "Epoch 42/300\n",
      "Average training loss: 0.007619173876527283\n",
      "Average test loss: 0.0012912748012070855\n",
      "Epoch 43/300\n",
      "Average training loss: 0.007602444001370006\n",
      "Average test loss: 0.0013047133193661768\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0075889872291849715\n",
      "Average test loss: 0.001283223952477177\n",
      "Epoch 45/300\n",
      "Average training loss: 0.007576473351154063\n",
      "Average test loss: 0.0013007705572785602\n",
      "Epoch 46/300\n",
      "Average training loss: 0.007559536471135086\n",
      "Average test loss: 0.0012959383919110728\n",
      "Epoch 47/300\n",
      "Average training loss: 0.007548519850605064\n",
      "Average test loss: 0.001281552356460856\n",
      "Epoch 48/300\n",
      "Average training loss: 0.007532272805356317\n",
      "Average test loss: 0.0012958691418170928\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0075258473211692435\n",
      "Average test loss: 0.001279450852320426\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007508053787466553\n",
      "Average test loss: 0.00127742595742974\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007502043649968174\n",
      "Average test loss: 0.001285839308363696\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007482735606945223\n",
      "Average test loss: 0.001274344208960732\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007476092761589421\n",
      "Average test loss: 0.0012605030449728172\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00747678427480989\n",
      "Average test loss: 0.001284290535789397\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00747899016758634\n",
      "Average test loss: 0.0012997499308031466\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007444574002590444\n",
      "Average test loss: 0.0012720067726655138\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007435976668778393\n",
      "Average test loss: 0.0012574051977652643\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00742499263294869\n",
      "Average test loss: 0.001269742892227239\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007422799779723088\n",
      "Average test loss: 0.0012708587475741902\n",
      "Epoch 60/300\n",
      "Average training loss: 0.007424908084587918\n",
      "Average test loss: 0.001312959918545352\n",
      "Epoch 61/300\n",
      "Average training loss: 0.007408204460723533\n",
      "Average test loss: 0.0012779054465807147\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0074027385633024904\n",
      "Average test loss: 0.0012612495108818014\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007387156630969709\n",
      "Average test loss: 0.0012740682204150492\n",
      "Epoch 64/300\n",
      "Average training loss: 0.007383698282970323\n",
      "Average test loss: 0.0013385771926906374\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007366866412262122\n",
      "Average test loss: 0.0012594362133079105\n",
      "Epoch 66/300\n",
      "Average training loss: 0.007367663336710797\n",
      "Average test loss: 0.0012632776129369934\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007353903594944212\n",
      "Average test loss: 0.0012657410027459264\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007351966725869311\n",
      "Average test loss: 0.0012595110381436016\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007344110333257252\n",
      "Average test loss: 0.0012844433074610101\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007338011338065068\n",
      "Average test loss: 0.001260040282116582\n",
      "Epoch 71/300\n",
      "Average training loss: 0.007324750344372458\n",
      "Average test loss: 0.0012571648702853256\n",
      "Epoch 72/300\n",
      "Average training loss: 0.007328108917093939\n",
      "Average test loss: 0.0012707217190311188\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007316196455309789\n",
      "Average test loss: 0.0012547964970322533\n",
      "Epoch 74/300\n",
      "Average training loss: 0.007308843286501037\n",
      "Average test loss: 0.001278069612156186\n",
      "Epoch 75/300\n",
      "Average training loss: 0.007298958569765091\n",
      "Average test loss: 0.0012601499353638954\n",
      "Epoch 76/300\n",
      "Average training loss: 0.007299411130448182\n",
      "Average test loss: 0.001266862031382819\n",
      "Epoch 77/300\n",
      "Average training loss: 0.007288598701357842\n",
      "Average test loss: 0.001249462829488847\n",
      "Epoch 78/300\n",
      "Average training loss: 0.007288892164412472\n",
      "Average test loss: 0.0012618948149805268\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00727908534101314\n",
      "Average test loss: 0.0013365080541827613\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007275179244991806\n",
      "Average test loss: 0.0012963163442909716\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007266417738050222\n",
      "Average test loss: 0.0012595414091936415\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00725581806898117\n",
      "Average test loss: 0.0012435609101214343\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007268225160323911\n",
      "Average test loss: 0.0012582044077830183\n",
      "Epoch 84/300\n",
      "Average training loss: 0.00726644329354167\n",
      "Average test loss: 0.001312113490473065\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0072455360934966144\n",
      "Average test loss: 0.0012399457423016428\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0072503296935723886\n",
      "Average test loss: 0.0012422589032082922\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007237837041831679\n",
      "Average test loss: 0.0012611385168404216\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007230725532190667\n",
      "Average test loss: 0.0012415874251681897\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007226921367976401\n",
      "Average test loss: 0.0012801866433065799\n",
      "Epoch 90/300\n",
      "Average training loss: 0.007223427372260226\n",
      "Average test loss: 0.0012461079681913058\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007220115267982085\n",
      "Average test loss: 0.0012695562448352576\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007222361964484056\n",
      "Average test loss: 0.001270906029889981\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007208603438817792\n",
      "Average test loss: 0.0012472098945743508\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007200617250468996\n",
      "Average test loss: 0.0012430319296610024\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00719322461490002\n",
      "Average test loss: 0.0012504424311013686\n",
      "Epoch 96/300\n",
      "Average training loss: 0.007195519167929888\n",
      "Average test loss: 0.0015103077688771818\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007190562377787299\n",
      "Average test loss: 0.0012474926824991902\n",
      "Epoch 98/300\n",
      "Average training loss: 0.007184992589470413\n",
      "Average test loss: 0.0012602578691310352\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0071866466970079475\n",
      "Average test loss: 0.0013092036534928613\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0071675129263765285\n",
      "Average test loss: 0.0012497912407335309\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0071689479197892874\n",
      "Average test loss: 0.0012745925225317479\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007173897202644083\n",
      "Average test loss: 0.0012554957525183757\n",
      "Epoch 103/300\n",
      "Average training loss: 0.007169629259241952\n",
      "Average test loss: 0.0012529302210443549\n",
      "Epoch 104/300\n",
      "Average training loss: 0.007173165860275428\n",
      "Average test loss: 0.0012476836352402138\n",
      "Epoch 105/300\n",
      "Average training loss: 0.007153325115227037\n",
      "Average test loss: 0.0012353785920681225\n",
      "Epoch 106/300\n",
      "Average training loss: 0.007150531561838256\n",
      "Average test loss: 0.0012566947810248369\n",
      "Epoch 107/300\n",
      "Average training loss: 0.007162765437530146\n",
      "Average test loss: 0.0012347067033147646\n",
      "Epoch 108/300\n",
      "Average training loss: 0.007146311688340372\n",
      "Average test loss: 0.0012760465012656317\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0071400138185256055\n",
      "Average test loss: 0.0012372143301698896\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0071328578570650685\n",
      "Average test loss: 0.001264424514801552\n",
      "Epoch 111/300\n",
      "Average training loss: 0.007140052527189255\n",
      "Average test loss: 0.0012595214878933297\n",
      "Epoch 112/300\n",
      "Average training loss: 0.007140189199397962\n",
      "Average test loss: 0.0012655946520260639\n",
      "Epoch 113/300\n",
      "Average training loss: 0.007121899037311474\n",
      "Average test loss: 0.0012356356845961676\n",
      "Epoch 114/300\n",
      "Average training loss: 0.007135590809914801\n",
      "Average test loss: 0.0012485704471150206\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00710700278605024\n",
      "Average test loss: 0.0012583362222131755\n",
      "Epoch 116/300\n",
      "Average training loss: 0.007111573733389377\n",
      "Average test loss: 0.0012605129221661224\n",
      "Epoch 117/300\n",
      "Average training loss: 0.007117803831481272\n",
      "Average test loss: 0.0012407029914773173\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007124406719373332\n",
      "Average test loss: 0.001260802205445038\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0071010836888518595\n",
      "Average test loss: 0.0012464057503061162\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0070966941979196336\n",
      "Average test loss: 0.0012365861085967885\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007102956457684438\n",
      "Average test loss: 0.0012375242092853619\n",
      "Epoch 122/300\n",
      "Average training loss: 0.007101752484010325\n",
      "Average test loss: 0.2937994709544712\n",
      "Epoch 123/300\n",
      "Average training loss: 0.007101226599266132\n",
      "Average test loss: 0.0012594776943119035\n",
      "Epoch 124/300\n",
      "Average training loss: 0.007082749748395549\n",
      "Average test loss: 0.0012493367079231474\n",
      "Epoch 125/300\n",
      "Average training loss: 0.007088187441643742\n",
      "Average test loss: 0.001280804881412122\n",
      "Epoch 126/300\n",
      "Average training loss: 0.007082705409576496\n",
      "Average test loss: 0.0012452191703228486\n",
      "Epoch 127/300\n",
      "Average training loss: 0.007080523328234752\n",
      "Average test loss: 0.0012402770546161466\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007085697648425897\n",
      "Average test loss: 0.001252323945104662\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0070775436866614555\n",
      "Average test loss: 0.0012340051278264986\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00707164709104432\n",
      "Average test loss: 0.0012444800053619677\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007054407607764005\n",
      "Average test loss: 0.0012451447256737285\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007075571274591816\n",
      "Average test loss: 0.001246504145881368\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007066937616715828\n",
      "Average test loss: 0.0012408610889170734\n",
      "Epoch 134/300\n",
      "Average training loss: 0.007053941613684098\n",
      "Average test loss: 0.0012482675452613167\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007050708583866557\n",
      "Average test loss: 0.0012714866203152471\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007051131197975741\n",
      "Average test loss: 0.0012448452490174935\n",
      "Epoch 137/300\n",
      "Average training loss: 0.007054243378755119\n",
      "Average test loss: 0.0012466102144163516\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00703835297210349\n",
      "Average test loss: 0.0012492845823160476\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007050918673061663\n",
      "Average test loss: 0.0012705213348898623\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007058724270926581\n",
      "Average test loss: 0.0012945931433286104\n",
      "Epoch 141/300\n",
      "Average training loss: 0.007027471071730057\n",
      "Average test loss: 0.0012574747677685486\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007022555238670773\n",
      "Average test loss: 0.0012452809908944699\n",
      "Epoch 143/300\n",
      "Average training loss: 0.007039857907841603\n",
      "Average test loss: 0.0012429127720081144\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007033887972434362\n",
      "Average test loss: 0.0012328714753190677\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0070164770500527485\n",
      "Average test loss: 0.0012823137287050485\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007028160692503055\n",
      "Average test loss: 0.0012773913433775305\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0070262173319028485\n",
      "Average test loss: 0.0012420908010875185\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0070175194177362656\n",
      "Average test loss: 0.0012402261563887198\n",
      "Epoch 149/300\n",
      "Average training loss: 0.007015476173410813\n",
      "Average test loss: 0.0012606125983099143\n",
      "Epoch 150/300\n",
      "Average training loss: 0.007020442829777797\n",
      "Average test loss: 0.0012545178476721047\n",
      "Epoch 151/300\n",
      "Average training loss: 0.006999272880454858\n",
      "Average test loss: 0.0012531212938742505\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007011240536967913\n",
      "Average test loss: 0.0012504190515933765\n",
      "Epoch 153/300\n",
      "Average training loss: 0.007005078771875964\n",
      "Average test loss: 0.0012463141301025946\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007003075103792879\n",
      "Average test loss: 0.0012391468231669732\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007000319188253747\n",
      "Average test loss: 0.004115418790943093\n",
      "Epoch 156/300\n",
      "Average training loss: 0.006991748856587542\n",
      "Average test loss: 0.0012474909922004574\n",
      "Epoch 157/300\n",
      "Average training loss: 0.006997517603966925\n",
      "Average test loss: 0.001260625921914147\n",
      "Epoch 158/300\n",
      "Average training loss: 0.006983740628593498\n",
      "Average test loss: 0.001252799005454613\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0069841589422689546\n",
      "Average test loss: 0.0012821089023103316\n",
      "Epoch 160/300\n",
      "Average training loss: 0.006989011516173681\n",
      "Average test loss: 0.0012570074849451581\n",
      "Epoch 161/300\n",
      "Average training loss: 0.007119817701064878\n",
      "Average test loss: 0.0012906621645929085\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006972120138092173\n",
      "Average test loss: 0.0012358642817578381\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006959469228155083\n",
      "Average test loss: 0.00125129179077016\n",
      "Epoch 164/300\n",
      "Average training loss: 0.006970610633078548\n",
      "Average test loss: 0.001230857006325904\n",
      "Epoch 165/300\n",
      "Average training loss: 0.006964850603913267\n",
      "Average test loss: 0.0012769635561853648\n",
      "Epoch 166/300\n",
      "Average training loss: 0.006967022937619024\n",
      "Average test loss: 0.0012581152524799109\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0069615400404565864\n",
      "Average test loss: 0.0012851581274428302\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006997974174718062\n",
      "Average test loss: 0.0012398471193802026\n",
      "Epoch 169/300\n",
      "Average training loss: 0.006958677951660421\n",
      "Average test loss: 0.0012456220522419445\n",
      "Epoch 170/300\n",
      "Average training loss: 0.006968013748940494\n",
      "Average test loss: 0.001277530505735841\n",
      "Epoch 171/300\n",
      "Average training loss: 0.006967548629062043\n",
      "Average test loss: 0.001244524507628133\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006990620361434089\n",
      "Average test loss: 0.0012710050156133042\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0069470270292626484\n",
      "Average test loss: 0.001255939251329336\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006955135169128577\n",
      "Average test loss: 0.0012541201170533896\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0069444249495863915\n",
      "Average test loss: 0.0012554390272384302\n",
      "Epoch 181/300\n",
      "Average training loss: 0.006934025927136342\n",
      "Average test loss: 0.001245912971906364\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00694506007929643\n",
      "Average test loss: 0.0012550409032652775\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006936869279791911\n",
      "Average test loss: 0.0012356148072414927\n",
      "Epoch 184/300\n",
      "Average training loss: 0.006936327920191818\n",
      "Average test loss: 0.0012520902537637288\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0069229371738102705\n",
      "Average test loss: 0.0028270323771155542\n",
      "Epoch 186/300\n",
      "Average training loss: 0.006934411724408468\n",
      "Average test loss: 0.0012541333826051819\n",
      "Epoch 187/300\n",
      "Average training loss: 0.006947215790135993\n",
      "Average test loss: 0.0012491374995766415\n",
      "Epoch 188/300\n",
      "Average training loss: 0.006916396445698208\n",
      "Average test loss: 0.001256786889810529\n",
      "Epoch 189/300\n",
      "Average training loss: 0.006916795415596829\n",
      "Average test loss: 0.001251156778799163\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0069238688705696\n",
      "Average test loss: 0.0012638450365306602\n",
      "Epoch 191/300\n",
      "Average training loss: 0.006934325487663349\n",
      "Average test loss: 0.001248596928185887\n",
      "Epoch 192/300\n",
      "Average training loss: 0.006920355408555932\n",
      "Average test loss: 0.0012626252442391383\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0069114215907951195\n",
      "Average test loss: 0.0012486885820205012\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006913984382318126\n",
      "Average test loss: 0.0012690613429165549\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0069160025471614464\n",
      "Average test loss: 0.0012553805446562669\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0069069113905231155\n",
      "Average test loss: 0.0012629030004868078\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00691650624656015\n",
      "Average test loss: 0.0012357299388903711\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0069030789993703365\n",
      "Average test loss: 0.0012853993598578705\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0069111070123811565\n",
      "Average test loss: 0.0013177978310009671\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00689619469208022\n",
      "Average test loss: 0.0012772324548309876\n",
      "Epoch 201/300\n",
      "Average training loss: 0.006900960077842077\n",
      "Average test loss: 0.0012529105797616973\n",
      "Epoch 202/300\n",
      "Average training loss: 0.006897954396489593\n",
      "Average test loss: 0.0012548989858478308\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0069057768256300025\n",
      "Average test loss: 0.0013044033916667103\n",
      "Epoch 204/300\n",
      "Average training loss: 0.006888771021945609\n",
      "Average test loss: 0.0012731518949278527\n",
      "Epoch 205/300\n",
      "Average training loss: 0.006894159991294145\n",
      "Average test loss: 0.0012401813499422537\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0068908561108012995\n",
      "Average test loss: 0.0012487912281519836\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0068926956028574045\n",
      "Average test loss: 0.0012720071430214577\n",
      "Epoch 208/300\n",
      "Average training loss: 0.006885478186110655\n",
      "Average test loss: 0.0012537435575181411\n",
      "Epoch 209/300\n",
      "Average training loss: 0.006887239606844054\n",
      "Average test loss: 0.0012340172646153305\n",
      "Epoch 210/300\n",
      "Average training loss: 0.006883836393968927\n",
      "Average test loss: 0.0012537487565229335\n",
      "Epoch 211/300\n",
      "Average training loss: 0.006876211705721087\n",
      "Average test loss: 0.0012536023422661754\n",
      "Epoch 212/300\n",
      "Average training loss: 0.006888302341517475\n",
      "Average test loss: 0.001266010907685591\n",
      "Epoch 213/300\n",
      "Average training loss: 0.006882560513085789\n",
      "Average test loss: 0.0012618411924793487\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006873384817606873\n",
      "Average test loss: 0.0012456294147608182\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0069265769124031065\n",
      "Average test loss: 0.0012575455319343341\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00685419920914703\n",
      "Average test loss: 0.001264198304257459\n",
      "Epoch 217/300\n",
      "Average training loss: 0.006865463259319464\n",
      "Average test loss: 0.0012754042340546018\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006870094701233838\n",
      "Average test loss: 0.0012509202523570922\n",
      "Epoch 219/300\n",
      "Average training loss: 0.006869416678531302\n",
      "Average test loss: 0.0013162002110232909\n",
      "Epoch 220/300\n",
      "Average training loss: 0.006873537627359231\n",
      "Average test loss: 0.0012771271641055743\n",
      "Epoch 221/300\n",
      "Average training loss: 0.006860130186296172\n",
      "Average test loss: 0.0012536236150190235\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006864400916215446\n",
      "Average test loss: 0.0012695442537984086\n",
      "Epoch 223/300\n",
      "Average training loss: 0.006870979497416152\n",
      "Average test loss: 0.010850637137889863\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006878037113696337\n",
      "Average test loss: 0.0012621883689943286\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006855203109069003\n",
      "Average test loss: 0.0013368371447755232\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0068501251116395\n",
      "Average test loss: 0.0017549074458786183\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0068591286962231\n",
      "Average test loss: 0.001471300101114644\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006844624598821005\n",
      "Average test loss: 0.001250963907274935\n",
      "Epoch 234/300\n",
      "Average training loss: 0.006856106803649001\n",
      "Average test loss: 0.001260759081484543\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006846896200958226\n",
      "Average test loss: 0.0012737549206035005\n",
      "Epoch 236/300\n",
      "Average training loss: 0.006834660731256008\n",
      "Average test loss: 0.0012551246542069647\n",
      "Epoch 237/300\n",
      "Average training loss: 0.006848868409792582\n",
      "Average test loss: 0.001290733986430698\n",
      "Epoch 238/300\n",
      "Average training loss: 0.006838663652125332\n",
      "Average test loss: 0.001261352913868096\n",
      "Epoch 239/300\n",
      "Average training loss: 0.006838059654252397\n",
      "Average test loss: 0.0012545766122639178\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0068378052682512335\n",
      "Average test loss: 0.001248497407986886\n",
      "Epoch 241/300\n",
      "Average training loss: 0.006835858495699035\n",
      "Average test loss: 0.001330243529855377\n",
      "Epoch 242/300\n",
      "Average training loss: 0.006834158969008261\n",
      "Average test loss: 0.0012607503417465422\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006842399034649133\n",
      "Average test loss: 0.0012461251055614815\n",
      "Epoch 244/300\n",
      "Average training loss: 0.006822184606765707\n",
      "Average test loss: 0.0012731158333934016\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0068268141365713545\n",
      "Average test loss: 0.0012605955890483327\n",
      "Epoch 246/300\n",
      "Average training loss: 0.006838387431783809\n",
      "Average test loss: 0.0012749216416850686\n",
      "Epoch 247/300\n",
      "Average training loss: 0.006825736104614205\n",
      "Average test loss: 0.0012629905165069634\n",
      "Epoch 254/300\n",
      "Average training loss: 0.006813470171143611\n",
      "Average test loss: 0.0012770664252133833\n",
      "Epoch 255/300\n",
      "Average training loss: 0.006824746225857072\n",
      "Average test loss: 0.001261825183354732\n",
      "Epoch 256/300\n",
      "Average training loss: 0.006810110667927398\n",
      "Average test loss: 0.001274865264693896\n",
      "Epoch 257/300\n",
      "Average training loss: 0.006809623651206493\n",
      "Average test loss: 0.0013070602607395915\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006816705943395694\n",
      "Average test loss: 0.00125908029348486\n",
      "Epoch 259/300\n",
      "Average training loss: 0.006805993790013923\n",
      "Average test loss: 0.0012633029945815603\n",
      "Epoch 260/300\n",
      "Average training loss: 0.006877921174797747\n",
      "Average test loss: 0.0012583323868198527\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00679968059145742\n",
      "Average test loss: 0.0012962814994777242\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00679868197772238\n",
      "Average test loss: 0.0012775808687632282\n",
      "Epoch 263/300\n",
      "Average training loss: 0.006797840237203571\n",
      "Average test loss: 0.001258812425347666\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0068045990392565725\n",
      "Average test loss: 0.0012744740973123245\n",
      "Epoch 265/300\n",
      "Average training loss: 0.006803059065921439\n",
      "Average test loss: 0.0012741538195663856\n",
      "Epoch 266/300\n",
      "Average training loss: 0.006799542122830947\n",
      "Average test loss: 0.0012625349778681994\n",
      "Epoch 267/300\n",
      "Average training loss: 0.006799669698294666\n",
      "Average test loss: 0.0012760238097980618\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006802154684232341\n",
      "Average test loss: 0.001293610846934219\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006806118784679307\n",
      "Average test loss: 0.0012647674776510233\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0067881870418787\n",
      "Average test loss: 0.0012480371371946402\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006799521969838275\n",
      "Average test loss: 0.0012628251165151597\n",
      "Epoch 272/300\n",
      "Average training loss: 0.006792144273718198\n",
      "Average test loss: 0.0012767870583468013\n",
      "Epoch 273/300\n",
      "Average training loss: 0.006792757347226143\n",
      "Average test loss: 0.001256531935961296\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006793293095297284\n",
      "Average test loss: 0.001267690594204598\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006790509370052152\n",
      "Average test loss: 0.0012516202273675137\n",
      "Epoch 276/300\n",
      "Average training loss: 0.006801755747861332\n",
      "Average test loss: 0.0012754974626004697\n",
      "Epoch 277/300\n",
      "Average training loss: 0.006782989840954542\n",
      "Average test loss: 0.0012637057083969316\n",
      "Epoch 278/300\n",
      "Average training loss: 0.006785564632465442\n",
      "Average test loss: 0.001263061436979721\n",
      "Epoch 279/300\n",
      "Average training loss: 0.006791110073526701\n",
      "Average test loss: 0.0012644137881903184\n",
      "Epoch 280/300\n",
      "Average training loss: 0.006780605553752846\n",
      "Average test loss: 0.0012769066927333674\n",
      "Epoch 281/300\n",
      "Average training loss: 0.006785034370919069\n",
      "Average test loss: 0.0012968134103963772\n",
      "Epoch 282/300\n",
      "Average training loss: 0.006781157448059983\n",
      "Average test loss: 0.00127091093827039\n",
      "Epoch 283/300\n",
      "Average training loss: 0.006778490133583546\n",
      "Average test loss: 0.0012889424163020319\n",
      "Epoch 284/300\n",
      "Average training loss: 0.006779759612762266\n",
      "Average test loss: 0.0013009310129564256\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00678210019485818\n",
      "Average test loss: 0.0012607054637951984\n",
      "Epoch 286/300\n",
      "Average training loss: 0.006781692244526413\n",
      "Average test loss: 0.0012687419369402859\n",
      "Epoch 287/300\n",
      "Average training loss: 0.006766626110507382\n",
      "Average test loss: 0.0012686472851783036\n",
      "Epoch 288/300\n",
      "Average training loss: 0.006778730926828252\n",
      "Average test loss: 0.0012639353178027603\n",
      "Epoch 289/300\n",
      "Average training loss: 0.006775814087854492\n",
      "Average test loss: 0.001317529885408779\n",
      "Epoch 290/300\n",
      "Average training loss: 0.006769083335788713\n",
      "Average test loss: 0.0012946849528493152\n",
      "Epoch 291/300\n",
      "Average training loss: 0.006769546876351039\n",
      "Average test loss: 0.001256698389744593\n",
      "Epoch 292/300\n",
      "Average training loss: 0.006780044007011586\n",
      "Average test loss: 0.0013040897319507268\n",
      "Epoch 293/300\n",
      "Average training loss: 0.006774223646356\n",
      "Average test loss: 0.0012936553646706872\n",
      "Epoch 294/300\n",
      "Average training loss: 0.006769805074979862\n",
      "Average test loss: 0.0012728137242504292\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006764778480761581\n",
      "Average test loss: 0.0012708620871934625\n",
      "Epoch 296/300\n",
      "Average training loss: 0.006769584524962637\n",
      "Average test loss: 0.001274514688592818\n",
      "Epoch 297/300\n",
      "Average training loss: 0.006765262811134259\n",
      "Average test loss: 0.0018055536481034425\n",
      "Epoch 298/300\n",
      "Average training loss: 0.006779737065649695\n",
      "Average test loss: 0.001318436070241862\n",
      "Epoch 299/300\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_80_Depth5/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.8163252019749747\n",
      "Average test loss: 0.004859172720462084\n",
      "Epoch 2/300\n",
      "Average training loss: 0.16617054817411633\n",
      "Average test loss: 0.0042945958802269564\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11384887694650227\n",
      "Average test loss: 0.004116662138452133\n",
      "Epoch 4/300\n",
      "Average training loss: 0.09300006428692076\n",
      "Average test loss: 0.012958646061933704\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08147523899210823\n",
      "Average test loss: 0.003980668223152558\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07450364821487003\n",
      "Average test loss: 0.0039467997132904\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06978477074371443\n",
      "Average test loss: 0.004085940466572841\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06634252260128658\n",
      "Average test loss: 0.0038801412400272157\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06378620469239023\n",
      "Average test loss: 0.0038536936189565393\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0622007663514879\n",
      "Average test loss: 0.0038134805355221034\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06099716566337479\n",
      "Average test loss: 0.003806192878426777\n",
      "Epoch 12/300\n",
      "Average training loss: 0.060194482045041194\n",
      "Average test loss: 0.0037614881346623103\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05955755097667376\n",
      "Average test loss: 0.0037582964733656908\n",
      "Epoch 14/300\n",
      "Average training loss: 0.058243999815649455\n",
      "Average test loss: 0.003736061617318127\n",
      "Epoch 17/300\n",
      "Average training loss: 0.057922438227468065\n",
      "Average test loss: 0.0037202506829053165\n",
      "Epoch 18/300\n",
      "Average training loss: 0.057611207101080156\n",
      "Average test loss: 0.003741818175133732\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05733021806014909\n",
      "Average test loss: 0.0036971982665773896\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05708014739221997\n",
      "Average test loss: 0.0036587927283512223\n",
      "Epoch 21/300\n",
      "Average training loss: 0.056820187244150375\n",
      "Average test loss: 0.0036445241036514443\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05660200073652797\n",
      "Average test loss: 0.0036429767778350247\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05639548466602961\n",
      "Average test loss: 0.003634439870715141\n",
      "Epoch 24/300\n",
      "Average training loss: 0.056218226396375234\n",
      "Average test loss: 0.003619654324112667\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05597342743807369\n",
      "Average test loss: 0.0036109237261116507\n",
      "Epoch 26/300\n",
      "Average training loss: 0.055800566219621234\n",
      "Average test loss: 0.003608342562077774\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0556469710568587\n",
      "Average test loss: 0.0036012657727632256\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05550795952147908\n",
      "Average test loss: 0.003599184016800589\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05534979881842931\n",
      "Average test loss: 0.0035812307805236842\n",
      "Epoch 30/300\n",
      "Average training loss: 0.055207440863053\n",
      "Average test loss: 0.003601803977456358\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05506701175371806\n",
      "Average test loss: 0.003566470893099904\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05477561005949974\n",
      "Average test loss: 0.003551847130474117\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0546438349518511\n",
      "Average test loss: 0.0035489162409471143\n",
      "Epoch 35/300\n",
      "Average training loss: 0.054561044964525435\n",
      "Average test loss: 0.0035988769318080613\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05439692304200596\n",
      "Average test loss: 0.0035446165868391593\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05430095866322517\n",
      "Average test loss: 0.0035280637440996037\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05420422488451004\n",
      "Average test loss: 0.003539110649170147\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05410404614607493\n",
      "Average test loss: 0.0035216897336973084\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05402515399456024\n",
      "Average test loss: 0.0035194458994600507\n",
      "Epoch 41/300\n",
      "Average training loss: 0.053909152739577824\n",
      "Average test loss: 0.0035151481038580338\n",
      "Epoch 42/300\n",
      "Average training loss: 0.053837129205465316\n",
      "Average test loss: 0.003510441579338577\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05358948353264067\n",
      "Average test loss: 0.0035201439306967786\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05349998946322335\n",
      "Average test loss: 0.0035085490170038407\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05343110696805848\n",
      "Average test loss: 0.003514596550828881\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05339522622028987\n",
      "Average test loss: 0.0035062580381830535\n",
      "Epoch 49/300\n",
      "Average training loss: 0.053278308040565915\n",
      "Average test loss: 0.0034944184987495343\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05324601507849164\n",
      "Average test loss: 0.003503161845314834\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05319338752494918\n",
      "Average test loss: 0.0034890589287711513\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05311241756214036\n",
      "Average test loss: 0.003497632175270054\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05302781306372748\n",
      "Average test loss: 0.0035207679422779217\n",
      "Epoch 54/300\n",
      "Average training loss: 0.053028673314385946\n",
      "Average test loss: 0.0034903725849257574\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05294280271728834\n",
      "Average test loss: 0.003486189000101553\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05288796887463994\n",
      "Average test loss: 0.003487905649261342\n",
      "Epoch 57/300\n",
      "Average training loss: 0.052811394847101636\n",
      "Average test loss: 0.003488711477567752\n",
      "Epoch 58/300\n",
      "Average training loss: 0.052760709795686936\n",
      "Average test loss: 0.003482359290950828\n",
      "Epoch 59/300\n",
      "Average training loss: 0.052672035329871705\n",
      "Average test loss: 0.0035025398892660937\n",
      "Epoch 60/300\n",
      "Average training loss: 0.052652897116210726\n",
      "Average test loss: 0.003522600883514517\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05255374939242999\n",
      "Average test loss: 0.0034879706234981615\n",
      "Epoch 63/300\n",
      "Average training loss: 0.052519126060936186\n",
      "Average test loss: 0.003481698094142808\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05246009336246384\n",
      "Average test loss: 0.0034752810498078664\n",
      "Epoch 65/300\n",
      "Average training loss: 0.052399327980147466\n",
      "Average test loss: 0.003485691415145993\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05233406023515595\n",
      "Average test loss: 0.0035111539318329757\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05230947021643321\n",
      "Average test loss: 0.0035873650598029294\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0522575260731909\n",
      "Average test loss: 0.003474922886118293\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05219836344321569\n",
      "Average test loss: 0.0034785162769258022\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05213788668645753\n",
      "Average test loss: 0.0034781580074793764\n",
      "Epoch 71/300\n",
      "Average training loss: 0.052109749495983126\n",
      "Average test loss: 0.0034764110433558623\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05206608932548099\n",
      "Average test loss: 0.0034781080757578216\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05199486382140053\n",
      "Average test loss: 0.0034856864687883194\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05198731400238143\n",
      "Average test loss: 0.0034762426556812394\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05188426611158583\n",
      "Average test loss: 0.0034817528176224895\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05184195920493868\n",
      "Average test loss: 0.0034789235504964987\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05181545118159718\n",
      "Average test loss: 0.003519714393135574\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05175947400596407\n",
      "Average test loss: 0.0034773679001049865\n",
      "Epoch 79/300\n",
      "Average training loss: 0.051726910708679096\n",
      "Average test loss: 0.0034797264917029273\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05166138158573045\n",
      "Average test loss: 0.0035108263393243155\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05162896423538526\n",
      "Average test loss: 0.0035047905262973575\n",
      "Epoch 82/300\n",
      "Average training loss: 0.051568412522474925\n",
      "Average test loss: 0.0034903985255708295\n",
      "Epoch 83/300\n",
      "Average test loss: 0.0034902163644631706\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05143798429105017\n",
      "Average test loss: 0.003477427206105656\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0513695371846358\n",
      "Average test loss: 0.0034905263570447764\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05134256982141071\n",
      "Average test loss: 0.0035472758321298493\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05128575321038564\n",
      "Average test loss: 0.0034894129013021786\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0512431611104144\n",
      "Average test loss: 0.0035254649702045654\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05118275358610683\n",
      "Average test loss: 0.0034763911329209803\n",
      "Epoch 91/300\n",
      "Average training loss: 0.051128459556235205\n",
      "Average test loss: 0.0034926267142097157\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05109708170096079\n",
      "Average test loss: 0.0035300645157694817\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05101962513724963\n",
      "Average test loss: 0.0034783904945684804\n",
      "Epoch 94/300\n",
      "Average training loss: 0.050973413477341335\n",
      "Average test loss: 0.0034866374557216964\n",
      "Epoch 95/300\n",
      "Average training loss: 0.050944821698798076\n",
      "Average test loss: 0.0034898002987934483\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05091291272640228\n",
      "Average test loss: 0.003505114014363951\n",
      "Epoch 97/300\n",
      "Average training loss: 0.050841423054536185\n",
      "Average test loss: 0.003491740753253301\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05077655956149101\n",
      "Average test loss: 0.0035028998768991894\n",
      "Epoch 99/300\n",
      "Average training loss: 0.050768604497114816\n",
      "Average test loss: 0.0035069159921258686\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05073722759220335\n",
      "Average test loss: 0.0035235638490153684\n",
      "Epoch 101/300\n",
      "Average training loss: 0.050675987786716885\n",
      "Average test loss: 0.003531516766382588\n",
      "Epoch 102/300\n",
      "Average training loss: 0.050612351169188816\n",
      "Average test loss: 0.003571058541536331\n",
      "Epoch 103/300\n",
      "Average training loss: 0.050564915392133924\n",
      "Average test loss: 0.0035237826568384964\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05051857421795527\n",
      "Average test loss: 0.0035564325843006374\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05049649811784426\n",
      "Average test loss: 0.0035108702956802313\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05046205601758427\n",
      "Average test loss: 0.0035077291809850267\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05039904757340749\n",
      "Average test loss: 0.003524356026616361\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05035013881160153\n",
      "Average test loss: 0.0035572261152168115\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05029670406712426\n",
      "Average test loss: 0.003539558228519228\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05026629691653781\n",
      "Average test loss: 0.003534642638431655\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05021065041919549\n",
      "Average test loss: 0.0035692498311400414\n",
      "Epoch 112/300\n",
      "Average training loss: 0.050197896003723146\n",
      "Average test loss: 0.00351675565706359\n",
      "Epoch 113/300\n",
      "Average training loss: 0.050122861832380294\n",
      "Average test loss: 0.0035701514850887987\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05011106147699886\n",
      "Average test loss: 0.003562751789059904\n",
      "Epoch 115/300\n",
      "Average training loss: 0.050035459329684576\n",
      "Average test loss: 0.00357457242326604\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05000627573662334\n",
      "Average test loss: 0.0035520244447721375\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04995236562358008\n",
      "Average test loss: 0.003528815388265583\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0499566906425688\n",
      "Average test loss: 0.0035325667377975253\n",
      "Epoch 119/300\n",
      "Average training loss: 0.049929500503672494\n",
      "Average test loss: 0.003529142830727829\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04987938140498267\n",
      "Average test loss: 0.0035275799501687287\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04978502248393165\n",
      "Average test loss: 0.0035797652231736314\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0498401883078946\n",
      "Average test loss: 0.0035594333716564707\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04971573832299974\n",
      "Average test loss: 0.003550344847970539\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04968628000219663\n",
      "Average test loss: 0.0035441354318625396\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04965572151210573\n",
      "Average test loss: 0.003540781116320027\n",
      "Epoch 126/300\n",
      "Average training loss: 0.049605024086104496\n",
      "Average test loss: 0.003576189956938227\n",
      "Epoch 127/300\n",
      "Average training loss: 0.049594092107481426\n",
      "Average test loss: 0.003550417116118802\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04955049783653683\n",
      "Average test loss: 0.0036013176256997717\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04952288583914439\n",
      "Average test loss: 0.003547112912353542\n",
      "Epoch 130/300\n",
      "Average training loss: 0.049449798934989506\n",
      "Average test loss: 0.0035572989587154655\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04944115859270096\n",
      "Average test loss: 0.0035416652440196936\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04940471284919315\n",
      "Average test loss: 0.003599061967080666\n",
      "Epoch 133/300\n",
      "Average training loss: 0.049374245347248186\n",
      "Average test loss: 0.0035456453044381405\n",
      "Epoch 134/300\n",
      "Average training loss: 0.049344656735658646\n",
      "Average test loss: 0.0035602337527606224\n",
      "Epoch 135/300\n",
      "Average training loss: 0.049286167856719756\n",
      "Average test loss: 0.003578301528468728\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04924083755413691\n",
      "Average test loss: 0.003593816145012776\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04919965410232544\n",
      "Average test loss: 0.003540685828982128\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0492045526670085\n",
      "Average test loss: 0.0035368138959424363\n",
      "Epoch 139/300\n",
      "Average training loss: 0.049157456682788\n",
      "Average test loss: 0.003581766921819912\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04919929671618674\n",
      "Average test loss: 0.003560894972127345\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04909241784281201\n",
      "Average test loss: 0.0035441823825240137\n",
      "Epoch 142/300\n",
      "Average training loss: 0.049071439445018766\n",
      "Average test loss: 0.0036866164153648746\n",
      "Epoch 143/300\n",
      "Average training loss: 0.049045250720447964\n",
      "Average test loss: 0.0035787637736648323\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04899301957090696\n",
      "Average test loss: 0.0035803617466655044\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04896266888247596\n",
      "Average test loss: 0.003689595125201676\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04893150827619765\n",
      "Average test loss: 0.003547841500283943\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04891827559139993\n",
      "Average test loss: 0.0035556123281518617\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04888959854510096\n",
      "Average test loss: 0.0035577459269099765\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0488737632797824\n",
      "Average test loss: 0.003577726800615589\n",
      "Epoch 150/300\n",
      "Average training loss: 0.048833262039555445\n",
      "Average test loss: 0.0036209335626206465\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04877050154407819\n",
      "Average test loss: 0.0036429290649377635\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04874789353542858\n",
      "Average test loss: 0.0036381134012093146\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0487796435157458\n",
      "Average test loss: 0.003583367605176237\n",
      "Epoch 154/300\n",
      "Average training loss: 0.048750938329431746\n",
      "Average test loss: 0.0035996115644358925\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0486668746901883\n",
      "Average test loss: 0.003552502331013481\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04864963096049097\n",
      "Average test loss: 0.003624424963982569\n",
      "Epoch 157/300\n",
      "Average training loss: 0.048647249072790145\n",
      "Average test loss: 0.003648841459097134\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04857316622800297\n",
      "Average test loss: 0.0036341006958650218\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04853368902040853\n",
      "Average test loss: 0.003624756147257156\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04854658741752307\n",
      "Average test loss: 0.0036119609338541823\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04854611798127492\n",
      "Average test loss: 0.003570519141231974\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04850187145007981\n",
      "Average test loss: 0.0036281084906723765\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04847076903449164\n",
      "Average test loss: 0.003604770372932156\n",
      "Epoch 164/300\n",
      "Average training loss: 0.048428098138835696\n",
      "Average test loss: 0.0036018540871640046\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04838602164387703\n",
      "Average test loss: 0.0036487694953878723\n",
      "Epoch 166/300\n",
      "Average training loss: 0.048377366380559075\n",
      "Average test loss: 0.0035489208118783104\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04837085158957376\n",
      "Average test loss: 0.0036205836834592954\n",
      "Epoch 168/300\n",
      "Average training loss: 0.048354999426338405\n",
      "Average test loss: 0.0036247060240970717\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04830821601549784\n",
      "Average test loss: 0.003579362716525793\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04826548357142343\n",
      "Average test loss: 0.003616310249807106\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04824268872539202\n",
      "Average test loss: 0.003630506919903888\n",
      "Epoch 172/300\n",
      "Average training loss: 0.048264853285418616\n",
      "Average test loss: 0.003577830277590288\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04825107142329216\n",
      "Average test loss: 0.003557758206501603\n",
      "Epoch 174/300\n",
      "Average training loss: 0.048173746367295586\n",
      "Average test loss: 0.0036073683489941887\n",
      "Epoch 175/300\n",
      "Average training loss: 0.048191520896222854\n",
      "Average test loss: 0.003647867767761151\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04812491587135527\n",
      "Average test loss: 0.0036153887684146563\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04811515063378546\n",
      "Average test loss: 0.0036128595353414616\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04809697980682055\n",
      "Average test loss: 0.003635640234169033\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04807504307892588\n",
      "Average test loss: 0.0036050080959167743\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04806899012128512\n",
      "Average test loss: 0.003638292450250851\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04801535332865185\n",
      "Average test loss: 0.0035970543794747855\n",
      "Epoch 182/300\n",
      "Average training loss: 0.048010541442367764\n",
      "Average test loss: 0.003579573745528857\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04798093783193164\n",
      "Average test loss: 0.003606693016779092\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04797429292069541\n",
      "Average test loss: 0.0036535258415258593\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04791691529750824\n",
      "Average test loss: 0.0036420363570666974\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04790133125252194\n",
      "Average test loss: 0.0036066604550513957\n",
      "Epoch 187/300\n",
      "Average training loss: 0.047942441327704324\n",
      "Average test loss: 0.0037069499062167273\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04789730773369471\n",
      "Average test loss: 0.003647008591435022\n",
      "Epoch 189/300\n",
      "Average training loss: 0.047872946682903504\n",
      "Average test loss: 0.0035949932038784028\n",
      "Epoch 190/300\n",
      "Average training loss: 0.047859001222583984\n",
      "Average test loss: 0.0036053741607401106\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04783061022228665\n",
      "Average test loss: 0.0036213452578004865\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0477737752728992\n",
      "Average test loss: 0.003710331617958016\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04776573893427849\n",
      "Average test loss: 0.003667320943954918\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04777722540166643\n",
      "Average test loss: 0.0036610330939292907\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04776557626989153\n",
      "Average test loss: 0.0035997827293144332\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0477370654957162\n",
      "Average test loss: 0.003636221041282018\n",
      "Epoch 197/300\n",
      "Average training loss: 0.047726510408851834\n",
      "Average test loss: 0.0036632439349260594\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04768017854293188\n",
      "Average test loss: 0.003635038343982564\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04763150682051977\n",
      "Average test loss: 0.0035925128443373574\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04764512535267406\n",
      "Average test loss: 0.0036017686881952816\n",
      "Epoch 201/300\n",
      "Average training loss: 0.047614615423811804\n",
      "Average test loss: 0.0036766163661248155\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04760898566246033\n",
      "Average test loss: 0.003649212641434537\n",
      "Epoch 203/300\n",
      "Average training loss: 0.047598778173327444\n",
      "Average test loss: 0.003640114634194308\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04756797018150489\n",
      "Average test loss: 0.003661751142806477\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04753740638163355\n",
      "Average test loss: 0.0036182123093555373\n",
      "Epoch 206/300\n",
      "Average training loss: 0.047522093964947595\n",
      "Average test loss: 0.0037153339677800734\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04749493204222785\n",
      "Average test loss: 0.0036367872417387035\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04749940974182553\n",
      "Average test loss: 0.0036273530665785073\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04747796168261104\n",
      "Average test loss: 0.003651152122351858\n",
      "Epoch 210/300\n",
      "Average training loss: 0.047469328502813976\n",
      "Average test loss: 0.003635228136761321\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04745769363310602\n",
      "Average test loss: 0.00363417632629474\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04744770759012964\n",
      "Average test loss: 0.0036152339306556517\n",
      "Epoch 213/300\n",
      "Average training loss: 0.047378897173537146\n",
      "Average test loss: 0.0038111520525481966\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04738437525762452\n",
      "Average test loss: 0.0036237106054193445\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04735310053163105\n",
      "Average test loss: 0.0036981819596969417\n",
      "Epoch 216/300\n",
      "Average training loss: 0.047385846336682635\n",
      "Average test loss: 0.003660596228101187\n",
      "Epoch 217/300\n",
      "Average training loss: 0.047334223121404645\n",
      "Average test loss: 0.0037137161567807196\n",
      "Epoch 218/300\n",
      "Average training loss: 0.047309452891349794\n",
      "Average test loss: 0.003635213577498992\n",
      "Epoch 219/300\n",
      "Average training loss: 0.047329989655150305\n",
      "Average test loss: 0.0036186578526265093\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0472818545864688\n",
      "Average test loss: 0.003618032597212328\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04730208138955964\n",
      "Average test loss: 0.0036672132892741098\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0472842612862587\n",
      "Average test loss: 0.0037038809019658303\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0472764214111699\n",
      "Average test loss: 0.003684057651087642\n",
      "Epoch 224/300\n",
      "Average training loss: 0.047243691543738045\n",
      "Average test loss: 0.003621616239556008\n",
      "Epoch 225/300\n",
      "Average training loss: 0.047205464525355234\n",
      "Average test loss: 0.0036232414613995285\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04718498847219679\n",
      "Average test loss: 0.00367018745901684\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04717813198102845\n",
      "Average test loss: 0.0036700174059304926\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04714730138911141\n",
      "Average test loss: 0.0036424839136501154\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04717016953892178\n",
      "Average test loss: 0.003694827243478762\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04713487350278431\n",
      "Average test loss: 0.0036281485071198807\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04712565073039797\n",
      "Average test loss: 0.003699519283655617\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04713864546848668\n",
      "Average test loss: 0.0036662712662170333\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04708559171027608\n",
      "Average test loss: 0.003680623922497034\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04707017638285955\n",
      "Average test loss: 0.00368282927489943\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04707593173450894\n",
      "Average test loss: 0.003680943902168009\n",
      "Epoch 236/300\n",
      "Average training loss: 0.047068948497374856\n",
      "Average test loss: 0.0036912370270325077\n",
      "Epoch 237/300\n",
      "Average training loss: 0.046999039368497\n",
      "Average test loss: 0.003638843837297625\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04699387186600102\n",
      "Average test loss: 0.0036544390606383483\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04703110498355494\n",
      "Average test loss: 0.0036892176754772664\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04695352462596363\n",
      "Average test loss: 0.0037088865484628413\n",
      "Epoch 241/300\n",
      "Average training loss: 0.047002651790777845\n",
      "Average test loss: 0.0036944877381126086\n",
      "Epoch 242/300\n",
      "Average training loss: 0.046962580018573334\n",
      "Average test loss: 0.0036756644786025087\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04693441485696369\n",
      "Average test loss: 0.0036975158258444733\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04692339544826084\n",
      "Average test loss: 0.0037022933303895926\n",
      "Epoch 245/300\n",
      "Average training loss: 0.046956190990077126\n",
      "Average test loss: 0.003691930826753378\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04691859863201777\n",
      "Average test loss: 0.003660111377843552\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04688126297791799\n",
      "Average test loss: 0.004554705639680226\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04684515546427833\n",
      "Average test loss: 0.003670751622774535\n",
      "Epoch 249/300\n",
      "Average training loss: 0.046898010141319696\n",
      "Average test loss: 0.0036389162445233927\n",
      "Epoch 250/300\n",
      "Average training loss: 0.046828779909345836\n",
      "Average test loss: 0.0036503293733629915\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04681394426690207\n",
      "Average test loss: 0.0036361221025387447\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04684532805946138\n",
      "Average test loss: 0.003743661834961838\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04686902833647198\n",
      "Average test loss: 0.0036700995409240326\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04681926157739427\n",
      "Average test loss: 0.003817300140754216\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04679464684592353\n",
      "Average test loss: 0.003744044298099147\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04676861816975805\n",
      "Average test loss: 0.00364510661487778\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04678364372584555\n",
      "Average test loss: 0.0037198590801821815\n",
      "Epoch 258/300\n",
      "Average training loss: 0.046741813590129216\n",
      "Average test loss: 0.0037403187155723573\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04673659843537543\n",
      "Average test loss: 0.0037109341559310753\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04671354012356864\n",
      "Average test loss: 0.0036953747258004215\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04670591841803657\n",
      "Average test loss: 0.003673679005147682\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04671610295110279\n",
      "Average test loss: 0.0037447791037460167\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04668220762080617\n",
      "Average test loss: 0.003727971673425701\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04663910006483396\n",
      "Average test loss: 0.0036522203269932006\n",
      "Epoch 265/300\n",
      "Average training loss: 0.046726365298032764\n",
      "Average test loss: 0.003720247610575623\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04664435343941053\n",
      "Average test loss: 0.0036232288125902412\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0466428307890892\n",
      "Average test loss: 0.0037078826098392408\n",
      "Epoch 268/300\n",
      "Average training loss: 0.046646770732270346\n",
      "Average test loss: 0.0037247854951355194\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04660382595327165\n",
      "Average test loss: 0.0037152331098914146\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04655939041409227\n",
      "Average test loss: 0.0037187235268453757\n",
      "Epoch 271/300\n",
      "Average training loss: 0.046637152913543915\n",
      "Average test loss: 0.003603032910575469\n",
      "Epoch 272/300\n",
      "Average training loss: 0.046559877577755183\n",
      "Average test loss: 0.0036588741524351967\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04660232626067268\n",
      "Average test loss: 0.0037105571130911508\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0465679087208377\n",
      "Average test loss: 0.003704451801048385\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04655973049667146\n",
      "Average test loss: 0.0036803832681228717\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04655054234630532\n",
      "Average test loss: 0.00377054483940204\n",
      "Epoch 277/300\n",
      "Average training loss: 0.046517173846562704\n",
      "Average test loss: 0.003687075710131062\n",
      "Epoch 278/300\n",
      "Average training loss: 0.046519425299432544\n",
      "Average test loss: 0.003681739425700572\n",
      "Epoch 279/300\n",
      "Average training loss: 0.046580957947505844\n",
      "Average test loss: 0.0037364617927620808\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04646328727735413\n",
      "Average test loss: 0.003704480836788813\n",
      "Epoch 281/300\n",
      "Average training loss: 0.046432410114341315\n",
      "Average test loss: 0.003665482932080825\n",
      "Epoch 282/300\n",
      "Average training loss: 0.046487060613102385\n",
      "Average test loss: 0.0036844189734094672\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04648242027892007\n",
      "Average test loss: 0.003724222072917554\n",
      "Epoch 284/300\n",
      "Average training loss: 0.046462178867724205\n",
      "Average test loss: 0.0038168453729401034\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04643206677006351\n",
      "Average test loss: 0.0037447648954888183\n",
      "Epoch 286/300\n",
      "Average training loss: 0.046394370956553355\n",
      "Average test loss: 0.0037566831952167883\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04640718746185303\n",
      "Average test loss: 0.0036653874648941887\n",
      "Epoch 288/300\n",
      "Average training loss: 0.046399311592181525\n",
      "Average test loss: 0.0037387916702363226\n",
      "Epoch 289/300\n",
      "Average training loss: 0.046418610039684505\n",
      "Average test loss: 0.0036980910015602907\n",
      "Epoch 290/300\n",
      "Average training loss: 0.046362479802634984\n",
      "Average test loss: 0.003715230987717708\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04635225898358557\n",
      "Average test loss: 0.0036678357859038645\n",
      "Epoch 292/300\n",
      "Average training loss: 0.046377904699908365\n",
      "Average test loss: 0.003706201752440797\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04634519094891018\n",
      "Average test loss: 0.0037242539249774482\n",
      "Epoch 294/300\n",
      "Average training loss: 0.046320337414741515\n",
      "Average test loss: 0.0036855373502605493\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04635037184755007\n",
      "Average test loss: 0.0037150294677250916\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04630026193459829\n",
      "Average test loss: 0.0037220524065196513\n",
      "Epoch 297/300\n",
      "Average training loss: 0.046328163703282674\n",
      "Average test loss: 0.00377372928812272\n",
      "Epoch 298/300\n",
      "Average training loss: 0.046260578754875395\n",
      "Average test loss: 0.003680943749845028\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04628684312105179\n",
      "Average test loss: 0.003678088110354212\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04629024115701517\n",
      "Average test loss: 0.0036702386914855903\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.6873815112643772\n",
      "Average test loss: 0.004343132494638364\n",
      "Epoch 2/300\n",
      "Average training loss: 0.14678616954220666\n",
      "Average test loss: 0.1474591976172394\n",
      "Epoch 3/300\n",
      "Average training loss: 0.10330645024114185\n",
      "Average test loss: 0.003963157458437814\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08512947597106298\n",
      "Average test loss: 0.003556409231076638\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07474838670094808\n",
      "Average test loss: 0.0034982262133724158\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0683991464442677\n",
      "Average test loss: 0.003386176681352986\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06304297406474749\n",
      "Average test loss: 0.0033090949054393505\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05910574100414912\n",
      "Average test loss: 0.003231550248339772\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05623656891451942\n",
      "Average test loss: 0.003223063431473242\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05391948172118929\n",
      "Average test loss: 0.0031370992639826404\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05215220226513015\n",
      "Average test loss: 0.0030892111503829558\n",
      "Epoch 12/300\n",
      "Average training loss: 0.050895611981550855\n",
      "Average test loss: 0.003298917340942555\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04991585035125415\n",
      "Average test loss: 0.002976066868131359\n",
      "Epoch 14/300\n",
      "Average training loss: 0.049068564666642084\n",
      "Average test loss: 0.0030349861656626064\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04831545426779323\n",
      "Average test loss: 0.0028767664109667144\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04761884370115068\n",
      "Average test loss: 0.002883953250116772\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04698611462447378\n",
      "Average test loss: 0.002786914440078868\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0464313759373294\n",
      "Average test loss: 0.002777525230207377\n",
      "Epoch 19/300\n",
      "Average training loss: 0.045826166740722124\n",
      "Average test loss: 0.002740519516997867\n",
      "Epoch 20/300\n",
      "Average training loss: 0.045281979388660853\n",
      "Average test loss: 0.002725078402294053\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04479292246368196\n",
      "Average test loss: 0.002690894260381659\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04427552677856551\n",
      "Average test loss: 0.0026997892597897186\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0438760272761186\n",
      "Average test loss: 0.002646969460778766\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04344144184390704\n",
      "Average test loss: 0.002669887782798873\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0430794080429607\n",
      "Average test loss: 0.002630095593838228\n",
      "Epoch 26/300\n",
      "Average training loss: 0.042634710090027915\n",
      "Average test loss: 0.0026226093957407606\n",
      "Epoch 27/300\n",
      "Average training loss: 0.042374367611275776\n",
      "Average test loss: 0.002595657205933498\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04202691070569886\n",
      "Average test loss: 0.0025928636665145557\n",
      "Epoch 29/300\n",
      "Average training loss: 0.041709184656540554\n",
      "Average test loss: 0.0025761597444199855\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04143960861033864\n",
      "Average test loss: 0.0025513462242152955\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04116703231136004\n",
      "Average test loss: 0.0025446227521946033\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04098761062986321\n",
      "Average test loss: 0.00252868472205268\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04068186512258318\n",
      "Average test loss: 0.0025414171945303678\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04050301632285118\n",
      "Average test loss: 0.0025187540226098565\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04027235500680076\n",
      "Average test loss: 0.002524198412274321\n",
      "Epoch 36/300\n",
      "Average training loss: 0.040119232889678745\n",
      "Average test loss: 0.002505433650273416\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03992909987436401\n",
      "Average test loss: 0.0024952295499129427\n",
      "Epoch 38/300\n",
      "Average training loss: 0.039765262994501324\n",
      "Average test loss: 0.0024891679785731765\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03960114455554221\n",
      "Average test loss: 0.0025311992559581995\n",
      "Epoch 40/300\n",
      "Average training loss: 0.039485555724965204\n",
      "Average test loss: 0.002484035594595803\n",
      "Epoch 41/300\n",
      "Average training loss: 0.039294304761621685\n",
      "Average test loss: 0.0024722514951394663\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03914348585572508\n",
      "Average test loss: 0.0024598478770090472\n",
      "Epoch 43/300\n",
      "Average training loss: 0.039048637388481036\n",
      "Average test loss: 0.0024713528044521807\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03898173834052351\n",
      "Average test loss: 0.0024592498203532564\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03884485957357618\n",
      "Average test loss: 0.0024504898879677056\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03871851374374496\n",
      "Average test loss: 0.0024492137980543904\n",
      "Epoch 47/300\n",
      "Average training loss: 0.038607517970932856\n",
      "Average test loss: 0.002454151447655426\n",
      "Epoch 48/300\n",
      "Average training loss: 0.038492915845579574\n",
      "Average test loss: 0.002456402859960993\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03837728847066561\n",
      "Average test loss: 0.002443374800702764\n",
      "Epoch 50/300\n",
      "Average training loss: 0.038294072267082\n",
      "Average test loss: 0.0024789186258696847\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03822236560781797\n",
      "Average test loss: 0.0024459762355933585\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03813112408253882\n",
      "Average test loss: 0.002450045608397987\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0380564188675748\n",
      "Average test loss: 0.002435634868219495\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03792909617887603\n",
      "Average test loss: 0.0024339863753153218\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03783285577429665\n",
      "Average test loss: 0.00242421749047935\n",
      "Epoch 56/300\n",
      "Average training loss: 0.037793481649623976\n",
      "Average test loss: 0.0024317902009934186\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03769077494906055\n",
      "Average test loss: 0.00247272951900959\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03769701996280087\n",
      "Average test loss: 0.0024746751284433735\n",
      "Epoch 59/300\n",
      "Average training loss: 0.037553350523114204\n",
      "Average test loss: 0.0024306726187674535\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03745944656762812\n",
      "Average test loss: 0.002434441240090463\n",
      "Epoch 61/300\n",
      "Average training loss: 0.037389805104997426\n",
      "Average test loss: 0.0024277184307575227\n",
      "Epoch 62/300\n",
      "Average training loss: 0.037322871974772874\n",
      "Average test loss: 0.002429788907782899\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03724490667051739\n",
      "Average test loss: 0.002435170759136478\n",
      "Epoch 64/300\n",
      "Average training loss: 0.037168511069483226\n",
      "Average test loss: 0.0024211708460417057\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0371189993388123\n",
      "Average test loss: 0.0024509330554347898\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03707435530424118\n",
      "Average test loss: 0.00245568777124087\n",
      "Epoch 67/300\n",
      "Average training loss: 0.036987071371740766\n",
      "Average test loss: 0.0024196257104890212\n",
      "Epoch 68/300\n",
      "Average training loss: 0.036926885704199476\n",
      "Average test loss: 0.002440032020004259\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03682924641503228\n",
      "Average test loss: 0.002422226395457983\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03677607138786051\n",
      "Average test loss: 0.002467316916005479\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03670896365741889\n",
      "Average test loss: 0.002457034173111121\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03671916600896252\n",
      "Average test loss: 0.0024273996998866398\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0365923939579063\n",
      "Average test loss: 0.0024561446114546724\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03655021068950494\n",
      "Average test loss: 0.0024350190586927864\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03646571650935544\n",
      "Average test loss: 0.0024191689445740646\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03638230124447081\n",
      "Average test loss: 0.0024492667756146856\n",
      "Epoch 77/300\n",
      "Average training loss: 0.036309582130776515\n",
      "Average test loss: 0.00246489016753104\n",
      "Epoch 78/300\n",
      "Average training loss: 0.036350547277265124\n",
      "Average test loss: 0.002433624199281136\n",
      "Epoch 79/300\n",
      "Average training loss: 0.036180071752932336\n",
      "Average test loss: 0.0024407722384979327\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03616603168845177\n",
      "Average test loss: 0.0024523894499159523\n",
      "Epoch 81/300\n",
      "Average training loss: 0.036128811409076055\n",
      "Average test loss: 0.002460860589105222\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03601105005045732\n",
      "Average test loss: 0.002431424490072661\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03610565653112199\n",
      "Average test loss: 0.0024502177257090807\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03594238064686457\n",
      "Average test loss: 0.0024379761521187092\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03586926340560118\n",
      "Average test loss: 0.0024758014157414436\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03583240755067931\n",
      "Average test loss: 0.0024243107558124593\n",
      "Epoch 87/300\n",
      "Average training loss: 0.035768976269496815\n",
      "Average test loss: 0.0024510184135288\n",
      "Epoch 88/300\n",
      "Average training loss: 0.035701678996284805\n",
      "Average test loss: 0.0024472390885154406\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03573346190154553\n",
      "Average test loss: 0.0024412355785154633\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03568260324001312\n",
      "Average test loss: 0.0024550746594452196\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03559487474792533\n",
      "Average test loss: 0.0024794556317437027\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03550191462205516\n",
      "Average test loss: 0.0024475880621208086\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03548633299271266\n",
      "Average test loss: 0.002457673302334216\n",
      "Epoch 94/300\n",
      "Average training loss: 0.035509798957241906\n",
      "Average test loss: 0.0024592683650553225\n",
      "Epoch 95/300\n",
      "Average training loss: 0.035352026283741\n",
      "Average test loss: 0.002437202285354336\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03538233725892173\n",
      "Average test loss: 0.002444305341690779\n",
      "Epoch 97/300\n",
      "Average training loss: 0.035292830811606515\n",
      "Average test loss: 0.002447020009160042\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03526405374374655\n",
      "Average test loss: 0.00248666133141766\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03524643006092972\n",
      "Average test loss: 0.002452417706760267\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0351698155850172\n",
      "Average test loss: 0.0024595392605082857\n",
      "Epoch 101/300\n",
      "Average training loss: 0.035119686846103934\n",
      "Average test loss: 0.00251217626221478\n",
      "Epoch 102/300\n",
      "Average training loss: 0.035033736136224534\n",
      "Average test loss: 0.0024780285205278133\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03503890861405266\n",
      "Average test loss: 0.002447067893938058\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03500882576902707\n",
      "Average test loss: 0.0024829611885878776\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03492431892785761\n",
      "Average test loss: 0.002475115817454126\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03493912908600436\n",
      "Average test loss: 0.0024788132287147974\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034868444515599145\n",
      "Average test loss: 0.0024469887196189827\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03481139089167118\n",
      "Average test loss: 0.0024756308837483327\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03476306310296059\n",
      "Average test loss: 0.0024737547191066873\n",
      "Epoch 110/300\n",
      "Average training loss: 0.034729886442422864\n",
      "Average test loss: 0.0024725646084795398\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03480787905885114\n",
      "Average test loss: 0.0024528410182231004\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03463150069448683\n",
      "Average test loss: 0.0025003208698083958\n",
      "Epoch 113/300\n",
      "Average training loss: 0.034652218265665904\n",
      "Average test loss: 0.002443478664590253\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03461876940727234\n",
      "Average test loss: 0.0024953468816561832\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03460693126254612\n",
      "Average test loss: 0.002469306563751565\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03449330680072307\n",
      "Average test loss: 0.0025001728172517486\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03453520319362481\n",
      "Average test loss: 0.002461745184328821\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03443176011078888\n",
      "Average test loss: 0.0024774804354334872\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03448765152196089\n",
      "Average test loss: 0.002467188220471144\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03440372955799103\n",
      "Average test loss: 0.002593522152553002\n",
      "Epoch 121/300\n",
      "Average training loss: 0.034310367272959816\n",
      "Average test loss: 0.002516019424009654\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03430728486842579\n",
      "Average test loss: 0.0024698480324198803\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03425083051290777\n",
      "Average test loss: 0.0024942977411879434\n",
      "Epoch 124/300\n",
      "Average training loss: 0.034229352929525905\n",
      "Average test loss: 0.002561341820491685\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03425864868031608\n",
      "Average test loss: 0.0024867453378521736\n",
      "Epoch 126/300\n",
      "Average training loss: 0.034188519800702734\n",
      "Average test loss: 0.0025128661286499764\n",
      "Epoch 127/300\n",
      "Average training loss: 0.034141773906018996\n",
      "Average test loss: 0.002479447836470273\n",
      "Epoch 128/300\n",
      "Average training loss: 0.034113193134466806\n",
      "Average test loss: 0.002479889040398929\n",
      "Epoch 129/300\n",
      "Average training loss: 0.034131590904461016\n",
      "Average test loss: 0.0024874967286984124\n",
      "Epoch 130/300\n",
      "Average training loss: 0.034051025066110824\n",
      "Average test loss: 0.002484412373560998\n",
      "Epoch 131/300\n",
      "Average training loss: 0.034057691050900354\n",
      "Average test loss: 0.00248266145059218\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03399616318609979\n",
      "Average test loss: 0.0024686923132588466\n",
      "Epoch 133/300\n",
      "Average training loss: 0.033998040184378624\n",
      "Average test loss: 0.0024976375048152274\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03395730023417208\n",
      "Average test loss: 0.00255873396475282\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03395150202843878\n",
      "Average test loss: 0.002507787239841289\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03390024009678099\n",
      "Average test loss: 0.0025282753691491153\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03387829975618256\n",
      "Average test loss: 0.0025224393484079176\n",
      "Epoch 138/300\n",
      "Average training loss: 0.033832662532726925\n",
      "Average test loss: 0.0026308650870083107\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03381584842171934\n",
      "Average test loss: 0.002567014543960492\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03373627382020156\n",
      "Average test loss: 0.0025404414747738175\n",
      "Epoch 141/300\n",
      "Average training loss: 0.033788471382525234\n",
      "Average test loss: 0.0025012946404102776\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03370667388869657\n",
      "Average test loss: 0.0025162566484262547\n",
      "Epoch 143/300\n",
      "Average training loss: 0.033701725885272024\n",
      "Average test loss: 0.0025056916652247308\n",
      "Epoch 144/300\n",
      "Average training loss: 0.033755639867650136\n",
      "Average test loss: 0.002502104837447405\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03362471531993813\n",
      "Average test loss: 0.0025204744268622664\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03365580038891898\n",
      "Average test loss: 0.0025034919858185783\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03361823747555415\n",
      "Average test loss: 0.0025642253487474388\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03356630503965749\n",
      "Average test loss: 0.0025080102373742394\n",
      "Epoch 149/300\n",
      "Average training loss: 0.033593884670072134\n",
      "Average test loss: 0.002539253077780207\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0335704096108675\n",
      "Average test loss: 0.0024904176220297814\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03352589221133126\n",
      "Average test loss: 0.002563077472564247\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03349888125558694\n",
      "Average test loss: 0.0025038421930124364\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03346580969293912\n",
      "Average test loss: 0.002508975139301684\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03353097310827838\n",
      "Average test loss: 0.002486507558160358\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03340530770354801\n",
      "Average test loss: 0.002519134089971582\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0334082956628667\n",
      "Average test loss: 0.002546830546317829\n",
      "Epoch 157/300\n",
      "Average training loss: 0.033354839144481556\n",
      "Average test loss: 0.002551514210593369\n",
      "Epoch 158/300\n",
      "Average training loss: 0.033447109245591695\n",
      "Average test loss: 0.0025612113508913254\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03336660907003615\n",
      "Average test loss: 0.0027808141373097898\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03330040957364771\n",
      "Average test loss: 0.0025019592403744658\n",
      "Epoch 161/300\n",
      "Average training loss: 0.033240618179241815\n",
      "Average test loss: 0.002532771510899895\n",
      "Epoch 162/300\n",
      "Average training loss: 0.033342953441871534\n",
      "Average test loss: 0.002502747778677278\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03327398621704843\n",
      "Average test loss: 0.0025682047131574816\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03324884051415655\n",
      "Average test loss: 0.0025262278768544396\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03318991517523925\n",
      "Average test loss: 0.002541185653458039\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03325457389156024\n",
      "Average test loss: 0.002558267121617165\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03317709683709674\n",
      "Average test loss: 0.0026884854816728167\n",
      "Epoch 168/300\n",
      "Average training loss: 0.033157651531199615\n",
      "Average test loss: 0.0025771930497139692\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03312501625385549\n",
      "Average test loss: 0.002532990917356478\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03311529622640875\n",
      "Average test loss: 0.002557441947153873\n",
      "Epoch 171/300\n",
      "Average training loss: 0.033112620072232354\n",
      "Average test loss: 0.002847373887689577\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03305127195848359\n",
      "Average test loss: 0.0025570557347188395\n",
      "Epoch 173/300\n",
      "Average training loss: 0.033061830364995534\n",
      "Average test loss: 0.0025872058518644834\n",
      "Epoch 174/300\n",
      "Average training loss: 0.033060175390707124\n",
      "Average test loss: 0.0025748337631424268\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03304338164130847\n",
      "Average test loss: 0.0025919815353635284\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03310667780207263\n",
      "Average test loss: 0.002516558677992887\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03302780215938886\n",
      "Average test loss: 0.002502067914750013\n",
      "Epoch 178/300\n",
      "Average training loss: 0.032999706382552785\n",
      "Average test loss: 0.002581843222387963\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03296137407422066\n",
      "Average test loss: 0.002542681173111002\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03302830697430505\n",
      "Average test loss: 0.002547868182675706\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03292069600853655\n",
      "Average test loss: 0.0025739520572953755\n",
      "Epoch 182/300\n",
      "Average training loss: 0.032878791453109846\n",
      "Average test loss: 0.00252835267306202\n",
      "Epoch 183/300\n",
      "Average training loss: 0.032891779275404084\n",
      "Average test loss: 0.0025651567933253116\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03287480258444945\n",
      "Average test loss: 0.00255638001465963\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03285885426070955\n",
      "Average test loss: 0.0025591863300651312\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0328926061226262\n",
      "Average test loss: 0.002536895983024604\n",
      "Epoch 187/300\n",
      "Average training loss: 0.032925971403717995\n",
      "Average test loss: 0.002556964143179357\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03280910003185272\n",
      "Average test loss: 0.0025274959502534734\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03281579714682367\n",
      "Average test loss: 0.002533716342308455\n",
      "Epoch 190/300\n",
      "Average training loss: 0.032809574762980145\n",
      "Average test loss: 0.002533920960707797\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03280530391467942\n",
      "Average test loss: 0.0025839008328815303\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03273903078834216\n",
      "Average test loss: 0.002566431727260351\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03276125577588876\n",
      "Average test loss: 0.002544906706135306\n",
      "Epoch 194/300\n",
      "Average training loss: 0.032702089842822815\n",
      "Average test loss: 0.0025204878132790327\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0327573801494307\n",
      "Average test loss: 0.0026208206617997754\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03274358073539204\n",
      "Average test loss: 0.0025692409274892677\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03270231869485643\n",
      "Average test loss: 0.0025373279441975884\n",
      "Epoch 198/300\n",
      "Average training loss: 0.032726129531860354\n",
      "Average test loss: 0.0025975534964559808\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03263488617208269\n",
      "Average test loss: 0.002592200155887339\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03266110558145576\n",
      "Average test loss: 0.002575014462487565\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03265672322279877\n",
      "Average test loss: 0.0025775169430093635\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03261303718056944\n",
      "Average test loss: 0.0025468725578652487\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03267408164342245\n",
      "Average test loss: 0.0025814001076958247\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0326297771996922\n",
      "Average test loss: 0.0025966522158640953\n",
      "Epoch 205/300\n",
      "Average training loss: 0.032575073689222335\n",
      "Average test loss: 0.0025505623382826646\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03255771252181795\n",
      "Average test loss: 0.0025987928766343328\n",
      "Epoch 207/300\n",
      "Average training loss: 0.032521009693543115\n",
      "Average test loss: 0.002589011043517126\n",
      "Epoch 208/300\n",
      "Average training loss: 0.032585068851709365\n",
      "Average test loss: 0.0025768560138013626\n",
      "Epoch 209/300\n",
      "Average training loss: 0.032561627172761495\n",
      "Average test loss: 0.002564986421002282\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0325607424494293\n",
      "Average test loss: 0.0025843703692985907\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03247495051721732\n",
      "Average test loss: 0.0025879521804551284\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03247028868728214\n",
      "Average test loss: 0.002570872330831157\n",
      "Epoch 213/300\n",
      "Average training loss: 0.032481129330065514\n",
      "Average test loss: 0.0026337880523254475\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03249257831441032\n",
      "Average test loss: 0.0025796513938241535\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03242578221029706\n",
      "Average test loss: 0.0025812057236002553\n",
      "Epoch 216/300\n",
      "Average training loss: 0.032430762542618646\n",
      "Average test loss: 0.0026101250358753735\n",
      "Epoch 217/300\n",
      "Average training loss: 0.032451603238781296\n",
      "Average test loss: 0.0026053215737144152\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03241706033216583\n",
      "Average test loss: 0.002566916964948177\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03248588900930352\n",
      "Average test loss: 0.002625695771848162\n",
      "Epoch 220/300\n",
      "Average training loss: 0.032378448704878486\n",
      "Average test loss: 0.0025903210325373544\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0323837155368593\n",
      "Average test loss: 0.002589539906423953\n",
      "Epoch 222/300\n",
      "Average training loss: 0.032355986485878625\n",
      "Average test loss: 0.002549435603328877\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03236255620254411\n",
      "Average test loss: 0.002530867957510054\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03230906687842475\n",
      "Average test loss: 0.002559141025981969\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03232747350798713\n",
      "Average test loss: 0.0026112423224581615\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03239353785249922\n",
      "Average test loss: 0.002657102572835154\n",
      "Epoch 227/300\n",
      "Average training loss: 0.032362481165263385\n",
      "Average test loss: 0.0025303495929886897\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03229728419250912\n",
      "Average test loss: 0.0026488382313400506\n",
      "Epoch 229/300\n",
      "Average training loss: 0.032324757983287176\n",
      "Average test loss: 0.002600321729357044\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03228285272088316\n",
      "Average test loss: 0.0026354701231337254\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03223649304111799\n",
      "Average test loss: 0.002560491364656223\n",
      "Epoch 232/300\n",
      "Average training loss: 0.032258507053057356\n",
      "Average test loss: 0.002591912467757033\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03231174198786418\n",
      "Average test loss: 0.0025745833249141772\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03225404317180316\n",
      "Average test loss: 0.0025869636672238507\n",
      "Epoch 235/300\n",
      "Average training loss: 0.032228033143613076\n",
      "Average test loss: 0.00262400134652853\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03220497926738527\n",
      "Average test loss: 0.0025628421081023084\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03218065156373713\n",
      "Average test loss: 0.0025863331938162444\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03220335961381594\n",
      "Average test loss: 0.002549556859044565\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03219685632321569\n",
      "Average test loss: 0.0026316893454641104\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03218547291888131\n",
      "Average test loss: 0.002574260306234161\n",
      "Epoch 241/300\n",
      "Average training loss: 0.032178804404205744\n",
      "Average test loss: 0.0026345914452233247\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03219054777920246\n",
      "Average test loss: 0.0025778909793330563\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0322212714801232\n",
      "Average test loss: 0.002596341241771976\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03215593074096574\n",
      "Average test loss: 0.0068954896007974945\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03225944200489256\n",
      "Average test loss: 0.002544669196837478\n",
      "Epoch 246/300\n",
      "Average training loss: 0.032081598043441774\n",
      "Average test loss: 0.0026027589801491963\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03212245760195785\n",
      "Average test loss: 0.0025845433146589334\n",
      "Epoch 248/300\n",
      "Average training loss: 0.032109760504629874\n",
      "Average test loss: 0.002595286106897725\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03214414482977655\n",
      "Average test loss: 0.002594714913310276\n",
      "Epoch 250/300\n",
      "Average training loss: 0.032077261835336685\n",
      "Average test loss: 0.002655509905372229\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03206733973489867\n",
      "Average test loss: 0.0026109222682813805\n",
      "Epoch 252/300\n",
      "Average training loss: 0.032073346922794975\n",
      "Average test loss: 0.0025886656896521648\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03200922073258294\n",
      "Average test loss: 0.0026144979801028968\n",
      "Epoch 254/300\n",
      "Average training loss: 0.032082313901848264\n",
      "Average test loss: 0.0025892033634914292\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03207819721764989\n",
      "Average test loss: 0.0025828561760071252\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03204567992356088\n",
      "Average test loss: 0.0025816421293550066\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03203296742339929\n",
      "Average test loss: 0.0026124106022632782\n",
      "Epoch 258/300\n",
      "Average training loss: 0.032065199883447755\n",
      "Average test loss: 0.0025835283673885795\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03205555716488096\n",
      "Average test loss: 0.002593534233048558\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03205095580054654\n",
      "Average test loss: 0.002636786743791567\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03197618068754673\n",
      "Average test loss: 0.002617561356474956\n",
      "Epoch 262/300\n",
      "Average training loss: 0.032001992409427964\n",
      "Average test loss: 0.002668792222109106\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03198852268192503\n",
      "Average test loss: 0.0025845637191087008\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03195099563151598\n",
      "Average test loss: 0.0025842738865564266\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03195968520806895\n",
      "Average test loss: 0.0026168099401725664\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03194138935042752\n",
      "Average test loss: 0.002690120605751872\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0319728650401036\n",
      "Average test loss: 0.0026830148257108197\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03195052273240354\n",
      "Average test loss: 0.0026119029819965363\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03194052816430728\n",
      "Average test loss: 0.002614817216578457\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03190242762698067\n",
      "Average test loss: 0.0026041832125435274\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03195465029776096\n",
      "Average test loss: 0.0025595526030908027\n",
      "Epoch 272/300\n",
      "Average training loss: 0.031892381891608236\n",
      "Average test loss: 0.0025574254010700516\n",
      "Epoch 273/300\n",
      "Average training loss: 0.031878412197033565\n",
      "Average test loss: 0.002651293359696865\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03191128787895044\n",
      "Average test loss: 0.002614583981119924\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03193387704756525\n",
      "Average test loss: 0.0026412973283893534\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0318599030127128\n",
      "Average test loss: 0.002584226772396101\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03188960111306773\n",
      "Average test loss: 0.0025848332453105183\n",
      "Epoch 278/300\n",
      "Average training loss: 0.031846866321232584\n",
      "Average test loss: 0.0025920500366224185\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03181436009870635\n",
      "Average test loss: 0.0025730057545006275\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03188888253023227\n",
      "Average test loss: 0.002637008417604698\n",
      "Epoch 281/300\n",
      "Average training loss: 0.031800205263826584\n",
      "Average test loss: 0.0026243956786476904\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03180827109018962\n",
      "Average test loss: 0.0026004042346030472\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03188024069699976\n",
      "Average test loss: 0.0025926746336950197\n",
      "Epoch 284/300\n",
      "Average training loss: 0.031807044799129165\n",
      "Average test loss: 0.0026047839673442974\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03178800372282664\n",
      "Average test loss: 0.0026176292719319464\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03178911777337392\n",
      "Average test loss: 0.0025899903799096744\n",
      "Epoch 287/300\n",
      "Average training loss: 0.031795027792453766\n",
      "Average test loss: 0.002657047133944515\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03179737245705393\n",
      "Average test loss: 0.0026212943438440562\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0317933225300577\n",
      "Average test loss: 0.002638083346084588\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0317144376469983\n",
      "Average test loss: 0.00262562394535376\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03175039498342408\n",
      "Average test loss: 0.002615244907326996\n",
      "Epoch 292/300\n",
      "Average training loss: 0.031753112468454574\n",
      "Average test loss: 0.0026481317010604672\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03177011279761791\n",
      "Average test loss: 0.0026013604050709144\n",
      "Epoch 294/300\n",
      "Average training loss: 0.031693140771653916\n",
      "Average test loss: 0.002582838249289327\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03173362497654226\n",
      "Average test loss: 0.0026288348243882257\n",
      "Epoch 296/300\n",
      "Average training loss: 0.031731597887972994\n",
      "Average test loss: 0.002610995290180047\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03167807551721732\n",
      "Average test loss: 0.0026406001280993223\n",
      "Epoch 298/300\n",
      "Average training loss: 0.031687297337585026\n",
      "Average test loss: 0.002578492201657759\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0317232029520803\n",
      "Average test loss: 0.002630685549022423\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03165226701398691\n",
      "Average test loss: 0.002620559581141505\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7184292443328434\n",
      "Average test loss: 0.003761609617206785\n",
      "Epoch 2/300\n",
      "Average training loss: 0.17285766562488344\n",
      "Average test loss: 0.0032327043345818916\n",
      "Epoch 3/300\n",
      "Average training loss: 0.10708310811387169\n",
      "Average test loss: 0.002988453786406252\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0815217431916131\n",
      "Average test loss: 0.002921496825499667\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06751551838053597\n",
      "Average test loss: 0.0027427657650162776\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05881887564063072\n",
      "Average test loss: 0.0026352127695249187\n",
      "Epoch 7/300\n",
      "Average training loss: 0.053122886813349195\n",
      "Average test loss: 0.0025573832475476793\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04922408711910248\n",
      "Average test loss: 0.0024600032953959374\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04644852181937959\n",
      "Average test loss: 0.002472600802779198\n",
      "Epoch 10/300\n",
      "Average training loss: 0.044324790686368944\n",
      "Average test loss: 0.002315593491411871\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04267878458731704\n",
      "Average test loss: 0.0022556645127220286\n",
      "Epoch 12/300\n",
      "Average training loss: 0.041275658428668975\n",
      "Average test loss: 0.0022405725127706924\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04010003818737136\n",
      "Average test loss: 0.0021380327922395533\n",
      "Epoch 14/300\n",
      "Average training loss: 0.039075731115208734\n",
      "Average test loss: 0.0021599277016810243\n",
      "Epoch 15/300\n",
      "Average training loss: 0.038134405983818905\n",
      "Average test loss: 0.0021737520071781345\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03733492904570367\n",
      "Average test loss: 0.002003976445024212\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03657040410654412\n",
      "Average test loss: 0.0020352243561711576\n",
      "Epoch 18/300\n",
      "Average training loss: 0.035899444394641454\n",
      "Average test loss: 0.0019659120589494705\n",
      "Epoch 19/300\n",
      "Average training loss: 0.035251821405357785\n",
      "Average test loss: 0.0019755776611467203\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03461696472598447\n",
      "Average test loss: 0.001906342128912608\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03406993960340818\n",
      "Average test loss: 0.0019122725770705277\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03303212952779399\n",
      "Average test loss: 0.0018655849790407551\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0325360845145252\n",
      "Average test loss: 0.0018925957859804233\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03208393490314484\n",
      "Average test loss: 0.001910606664294998\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03134791652858257\n",
      "Average test loss: 0.001820518070107533\n",
      "Epoch 28/300\n",
      "Average training loss: 0.030912620059318014\n",
      "Average test loss: 0.0017843293543491098\n",
      "Epoch 29/300\n",
      "Average training loss: 0.030556560380591288\n",
      "Average test loss: 0.0017682117997772165\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03027547139591641\n",
      "Average test loss: 0.0017926523685455322\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0299573290348053\n",
      "Average test loss: 0.0017510097319674161\n",
      "Epoch 32/300\n",
      "Average training loss: 0.029657116485966577\n",
      "Average test loss: 0.0017549447810484304\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02942996990183989\n",
      "Average test loss: 0.0017561826035380364\n",
      "Epoch 34/300\n",
      "Average training loss: 0.029200646145476235\n",
      "Average test loss: 0.0017330061032747228\n",
      "Epoch 35/300\n",
      "Average training loss: 0.028988878122634357\n",
      "Average test loss: 0.0017491159757806196\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02878630225194825\n",
      "Average test loss: 0.0017250683886102505\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02859827342463864\n",
      "Average test loss: 0.0017274232964134878\n",
      "Epoch 38/300\n",
      "Average training loss: 0.028472513978679975\n",
      "Average test loss: 0.0017258096759517987\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02826501006219122\n",
      "Average test loss: 0.0017685584646339218\n",
      "Epoch 40/300\n",
      "Average training loss: 0.028071450849374135\n",
      "Average test loss: 0.001707328657619655\n",
      "Epoch 41/300\n",
      "Average training loss: 0.027825936042600207\n",
      "Average test loss: 0.0016823304545962147\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02772732878724734\n",
      "Average test loss: 0.0017066211212012504\n",
      "Epoch 44/300\n",
      "Average training loss: 0.027563605924447378\n",
      "Average test loss: 0.0016766433040094045\n",
      "Epoch 45/300\n",
      "Average training loss: 0.027428588607245022\n",
      "Average test loss: 0.0016841272314389547\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0273468183759186\n",
      "Average test loss: 0.0016676052211680346\n",
      "Epoch 47/300\n",
      "Average training loss: 0.027238994841774306\n",
      "Average test loss: 0.0016797181993412475\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0271422158645259\n",
      "Average test loss: 0.0016713539395067428\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02711931862185399\n",
      "Average test loss: 0.0017159971487190988\n",
      "Epoch 50/300\n",
      "Average training loss: 0.026990854990151192\n",
      "Average test loss: 0.0016601131653620137\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02689601919386122\n",
      "Average test loss: 0.0016617896779336863\n",
      "Epoch 52/300\n",
      "Average training loss: 0.026821801531645987\n",
      "Average test loss: 0.001655168389280637\n",
      "Epoch 53/300\n",
      "Average training loss: 0.026767010652356676\n",
      "Average test loss: 0.0016516813337802887\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02664917004936271\n",
      "Average test loss: 0.0016577681988063785\n",
      "Epoch 55/300\n",
      "Average training loss: 0.026611704010102485\n",
      "Average test loss: 0.0016562486539284389\n",
      "Epoch 56/300\n",
      "Average training loss: 0.026522396978404788\n",
      "Average test loss: 0.0016801891191345123\n",
      "Epoch 57/300\n",
      "Average training loss: 0.026446015344725717\n",
      "Average test loss: 0.0016540667462266155\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02642311469051573\n",
      "Average test loss: 0.001648102438180811\n",
      "Epoch 59/300\n",
      "Average training loss: 0.026317014772031043\n",
      "Average test loss: 0.0016391244758334426\n",
      "Epoch 60/300\n",
      "Average training loss: 0.026367145738667913\n",
      "Average test loss: 0.0016385097667160962\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02623366565340095\n",
      "Average test loss: 0.0016542542554024192\n",
      "Epoch 62/300\n",
      "Average training loss: 0.026124583140843444\n",
      "Average test loss: 0.0016531190770781702\n",
      "Epoch 63/300\n",
      "Average training loss: 0.026142330212725533\n",
      "Average test loss: 0.0016477006582750215\n",
      "Epoch 64/300\n",
      "Average training loss: 0.026038492808739343\n",
      "Average test loss: 0.0016391328501825532\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02600495590103997\n",
      "Average test loss: 0.001642882731018795\n",
      "Epoch 66/300\n",
      "Average training loss: 0.025904708926876387\n",
      "Average test loss: 0.0017010867363876766\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02596577203273773\n",
      "Average test loss: 0.0016634936400999624\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02582114556928476\n",
      "Average test loss: 0.001644723533342282\n",
      "Epoch 69/300\n",
      "Average training loss: 0.025785034495923256\n",
      "Average test loss: 0.0017066573013241092\n",
      "Epoch 70/300\n",
      "Average training loss: 0.025741810725794897\n",
      "Average test loss: 0.0016436851696214742\n",
      "Epoch 71/300\n",
      "Average training loss: 0.025726609191960757\n",
      "Average test loss: 0.001630869137847589\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02558616077568796\n",
      "Average test loss: 0.001659393545343644\n",
      "Epoch 73/300\n",
      "Average training loss: 0.025559246192375817\n",
      "Average test loss: 0.0017007923308346006\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02550397419101662\n",
      "Average test loss: 0.0016381525276228785\n",
      "Epoch 75/300\n",
      "Average training loss: 0.025453141185972427\n",
      "Average test loss: 0.0016492555207676357\n",
      "Epoch 76/300\n",
      "Average training loss: 0.025480078417393897\n",
      "Average test loss: 0.0016392910633650092\n",
      "Epoch 77/300\n",
      "Average training loss: 0.025389621870385275\n",
      "Average test loss: 0.0016533641238800354\n",
      "Epoch 78/300\n",
      "Average training loss: 0.025379996966984536\n",
      "Average test loss: 0.0016935668688060508\n",
      "Epoch 79/300\n",
      "Average training loss: 0.025298755907350118\n",
      "Average test loss: 0.0016439468888565898\n",
      "Epoch 80/300\n",
      "Average training loss: 0.025281567192739912\n",
      "Average test loss: 0.0016417693989868793\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02519089582065741\n",
      "Average test loss: 0.001681687785933415\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025148363088568052\n",
      "Average test loss: 0.0016417852836764521\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025123584913710754\n",
      "Average test loss: 0.0016438986966386437\n",
      "Epoch 84/300\n",
      "Average training loss: 0.025111802169018323\n",
      "Average test loss: 0.0016523065491475992\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02507260997262266\n",
      "Average test loss: 0.0016370035227802064\n",
      "Epoch 86/300\n",
      "Average training loss: 0.024964792667163744\n",
      "Average test loss: 0.00163520841114223\n",
      "Epoch 87/300\n",
      "Average training loss: 0.024930458133419355\n",
      "Average test loss: 0.0016467429752358132\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02494237185849084\n",
      "Average test loss: 0.0016865554864828785\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0248527283138699\n",
      "Average test loss: 0.0016424235953018068\n",
      "Epoch 90/300\n",
      "Average training loss: 0.024846803996298048\n",
      "Average test loss: 0.0017026378470990393\n",
      "Epoch 91/300\n",
      "Average training loss: 0.024786882508132192\n",
      "Average test loss: 0.001658531170959274\n",
      "Epoch 92/300\n",
      "Average training loss: 0.024794888065920936\n",
      "Average test loss: 0.0016489533905146851\n",
      "Epoch 93/300\n",
      "Average training loss: 0.024728796104590098\n",
      "Average test loss: 0.0016438713763943977\n",
      "Epoch 94/300\n",
      "Average training loss: 0.024684502366516325\n",
      "Average test loss: 0.0016400378030828303\n",
      "Epoch 95/300\n",
      "Average training loss: 0.024623324281639523\n",
      "Average test loss: 0.0016760795470327139\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02470741737882296\n",
      "Average test loss: 0.0016479447687872582\n",
      "Epoch 97/300\n",
      "Average training loss: 0.024566296396983996\n",
      "Average test loss: 0.0016468031559553412\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024555442997150952\n",
      "Average test loss: 0.0016870544357225298\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02449363020228015\n",
      "Average test loss: 0.001637332343806823\n",
      "Epoch 100/300\n",
      "Average training loss: 0.024489373076293203\n",
      "Average test loss: 0.001642643749093016\n",
      "Epoch 101/300\n",
      "Average training loss: 0.024398548225561777\n",
      "Average test loss: 0.001665344567865961\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02439180124302705\n",
      "Average test loss: 0.0016740942851950725\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02437822706335121\n",
      "Average test loss: 0.0016614134605042637\n",
      "Epoch 104/300\n",
      "Average training loss: 0.024343627229332924\n",
      "Average test loss: 0.0016672651852325847\n",
      "Epoch 105/300\n",
      "Average training loss: 0.024326670529113876\n",
      "Average test loss: 0.001650429997489684\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02428972864813275\n",
      "Average test loss: 0.001661705624208682\n",
      "Epoch 107/300\n",
      "Average training loss: 0.024206026213036642\n",
      "Average test loss: 0.00167753234008948\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024168611634108755\n",
      "Average test loss: 0.0016602403930284912\n",
      "Epoch 109/300\n",
      "Average training loss: 0.024171236117680867\n",
      "Average test loss: 0.0016692134681054287\n",
      "Epoch 110/300\n",
      "Average training loss: 0.024141587942838667\n",
      "Average test loss: 0.0016942549270267289\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02413219443625874\n",
      "Average test loss: 0.0016631742233617437\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02408700992498133\n",
      "Average test loss: 0.0016518058116651243\n",
      "Epoch 113/300\n",
      "Average training loss: 0.024108868330717088\n",
      "Average test loss: 0.0016781838161663877\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02403970446023676\n",
      "Average test loss: 0.0016586621622037556\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023955166939232084\n",
      "Average test loss: 0.0016949432113518318\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02395724869767825\n",
      "Average test loss: 0.0016534286801599793\n",
      "Epoch 117/300\n",
      "Average training loss: 0.023979198725687132\n",
      "Average test loss: 0.0016623903944984907\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023913478208912743\n",
      "Average test loss: 0.0016703275872601404\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02389420808851719\n",
      "Average test loss: 0.0016799348550331262\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023842447895142768\n",
      "Average test loss: 0.0016690665003326205\n",
      "Epoch 121/300\n",
      "Average training loss: 0.023970141597919995\n",
      "Average test loss: 0.0016538195682482587\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02384241155948904\n",
      "Average test loss: 0.0017052632404698266\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02373934432864189\n",
      "Average test loss: 0.001657504509927498\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023754185350404845\n",
      "Average test loss: 0.0016836770231732064\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023763371871577368\n",
      "Average test loss: 0.0016618042412317462\n",
      "Epoch 126/300\n",
      "Average training loss: 0.023705516788694595\n",
      "Average test loss: 0.0016728655306829346\n",
      "Epoch 127/300\n",
      "Average training loss: 0.023683304280042648\n",
      "Average test loss: 0.001686430675184561\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023678289535972806\n",
      "Average test loss: 0.0016956696711066697\n",
      "Epoch 129/300\n",
      "Average training loss: 0.023651703577902583\n",
      "Average test loss: 0.0016654933552361197\n",
      "Epoch 130/300\n",
      "Average training loss: 0.023625690158870484\n",
      "Average test loss: 0.0017089563773738012\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023585779709948435\n",
      "Average test loss: 0.0016730658454923995\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02354691912399398\n",
      "Average test loss: 0.0017766792531021768\n",
      "Epoch 133/300\n",
      "Average training loss: 0.023603893452220494\n",
      "Average test loss: 0.0016893593474394746\n",
      "Epoch 134/300\n",
      "Average training loss: 0.023563563596871163\n",
      "Average test loss: 0.0016816909101067317\n",
      "Epoch 135/300\n",
      "Average training loss: 0.023494616051514945\n",
      "Average test loss: 0.0017746551694969336\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02348429766628477\n",
      "Average test loss: 0.0016968468938850694\n",
      "Epoch 137/300\n",
      "Average training loss: 0.023427580995692147\n",
      "Average test loss: 0.0018024380903484093\n",
      "Epoch 138/300\n",
      "Average training loss: 0.023458806923694082\n",
      "Average test loss: 0.0016889907988823123\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0234194094075097\n",
      "Average test loss: 0.0016909404148658116\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0233452753259076\n",
      "Average test loss: 0.0017073244971947538\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0233892243206501\n",
      "Average test loss: 0.0055600227862596515\n",
      "Epoch 142/300\n",
      "Average training loss: 0.023414428899685542\n",
      "Average test loss: 0.0016743567200998466\n",
      "Epoch 143/300\n",
      "Average training loss: 0.023329056590795517\n",
      "Average test loss: 0.0016828277957522206\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02330670033643643\n",
      "Average test loss: 0.0016799831930547954\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023332605795727837\n",
      "Average test loss: 0.0017102869391027425\n",
      "Epoch 146/300\n",
      "Average training loss: 0.023285990024606386\n",
      "Average test loss: 0.0017101230303653413\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02323404798573918\n",
      "Average test loss: 0.0017163781424363454\n",
      "Epoch 148/300\n",
      "Average training loss: 0.023240286942985324\n",
      "Average test loss: 0.0016871131864479847\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0232321714543634\n",
      "Average test loss: 0.0017013418595823978\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02322316682504283\n",
      "Average test loss: 0.0016897169659949012\n",
      "Epoch 151/300\n",
      "Average training loss: 0.023266924525300662\n",
      "Average test loss: 0.0016926492566449775\n",
      "Epoch 152/300\n",
      "Average training loss: 0.023134633291098806\n",
      "Average test loss: 0.0016958597475248906\n",
      "Epoch 153/300\n",
      "Average training loss: 0.023120949185556836\n",
      "Average test loss: 0.0016788438508907953\n",
      "Epoch 154/300\n",
      "Average training loss: 0.023154195643133587\n",
      "Average test loss: 0.00168854552321136\n",
      "Epoch 155/300\n",
      "Average training loss: 0.023086793538596894\n",
      "Average test loss: 0.0017235950489218035\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02309443465868632\n",
      "Average test loss: 0.0017089557072354688\n",
      "Epoch 157/300\n",
      "Average training loss: 0.023071889988250202\n",
      "Average test loss: 0.001706165325207015\n",
      "Epoch 158/300\n",
      "Average training loss: 0.023053688837422266\n",
      "Average test loss: 0.0016880757289214267\n",
      "Epoch 159/300\n",
      "Average training loss: 0.023052494447264406\n",
      "Average test loss: 0.001860496663591928\n",
      "Epoch 160/300\n",
      "Average training loss: 0.023038630470633505\n",
      "Average test loss: 0.0016880441390805774\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02300558255943987\n",
      "Average test loss: 0.0017188413211454948\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02301156597998407\n",
      "Average test loss: 0.0016961385710164904\n",
      "Epoch 163/300\n",
      "Average training loss: 0.022974274466435116\n",
      "Average test loss: 0.0016962403651947777\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02302224938985374\n",
      "Average test loss: 0.0017072852802359395\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02296398513101869\n",
      "Average test loss: 0.0017357147154915663\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02293237725728088\n",
      "Average test loss: 0.0017072767499420377\n",
      "Epoch 167/300\n",
      "Average training loss: 0.022929183299342792\n",
      "Average test loss: 0.0017098423798258106\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022917132704622215\n",
      "Average test loss: 0.0017075660988274547\n",
      "Epoch 169/300\n",
      "Average training loss: 0.022875297121703626\n",
      "Average test loss: 0.001716856364471217\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022877673515015177\n",
      "Average test loss: 0.00171641444394158\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022955666927827727\n",
      "Average test loss: 0.0016968564964416953\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02286656847761737\n",
      "Average test loss: 0.0016961351374371184\n",
      "Epoch 173/300\n",
      "Average training loss: 0.022814576006597944\n",
      "Average test loss: 0.0017266270257532597\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0228197514017423\n",
      "Average test loss: 0.0017025290957858993\n",
      "Epoch 175/300\n",
      "Average training loss: 0.022826413000623384\n",
      "Average test loss: 0.001767062579178148\n",
      "Epoch 176/300\n",
      "Average training loss: 0.022800077339013416\n",
      "Average test loss: 0.0017427409885244238\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022774859736363093\n",
      "Average test loss: 0.0016845553812260428\n",
      "Epoch 178/300\n",
      "Average training loss: 0.022758171929253473\n",
      "Average test loss: 0.0017422028605101837\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022787734735343193\n",
      "Average test loss: 0.0017192847117160758\n",
      "Epoch 180/300\n",
      "Average training loss: 0.022721534432636366\n",
      "Average test loss: 0.001782261396654778\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02273313094013267\n",
      "Average test loss: 0.0017408466684735483\n",
      "Epoch 182/300\n",
      "Average training loss: 0.022722978782322673\n",
      "Average test loss: 0.0017046443795164425\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02271533284087976\n",
      "Average test loss: 0.0016937032140170534\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022694511021176974\n",
      "Average test loss: 0.0017044777504892812\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02268013063073158\n",
      "Average test loss: 0.001725132151817282\n",
      "Epoch 186/300\n",
      "Average training loss: 0.022647282307346663\n",
      "Average test loss: 0.001719739021629923\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02267417812347412\n",
      "Average test loss: 0.0017087395556478037\n",
      "Epoch 188/300\n",
      "Average training loss: 0.022657418782512345\n",
      "Average test loss: 0.001731339541160398\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02262803310735358\n",
      "Average test loss: 0.0018919404951027697\n",
      "Epoch 190/300\n",
      "Average training loss: 0.022606900402241285\n",
      "Average test loss: 0.001731658884168913\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02263556443982654\n",
      "Average test loss: 0.001762609756551683\n",
      "Epoch 192/300\n",
      "Average training loss: 0.022607558298442097\n",
      "Average test loss: 0.0017531009962161383\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02260683092309369\n",
      "Average test loss: 0.0017186499196622106\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022562733388609358\n",
      "Average test loss: 0.0017577146811203824\n",
      "Epoch 195/300\n",
      "Average training loss: 0.022564737500415908\n",
      "Average test loss: 0.0017165623106476333\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02257473322418001\n",
      "Average test loss: 0.0017582693689813216\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02254757738775677\n",
      "Average test loss: 0.0017231077210356793\n",
      "Epoch 198/300\n",
      "Average training loss: 0.022530150315827793\n",
      "Average test loss: 0.0017538570443996124\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02254079641898473\n",
      "Average test loss: 0.0017463121125474571\n",
      "Epoch 200/300\n",
      "Average training loss: 0.022505499912632836\n",
      "Average test loss: 0.0017763298254253135\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022516484128104315\n",
      "Average test loss: 0.001743198356591165\n",
      "Epoch 202/300\n",
      "Average training loss: 0.022485690934790505\n",
      "Average test loss: 0.001715517532494333\n",
      "Epoch 203/300\n",
      "Average training loss: 0.022493067500491938\n",
      "Average test loss: 0.0017404696964141396\n",
      "Epoch 204/300\n",
      "Average training loss: 0.022448365771108203\n",
      "Average test loss: 0.0017694440049429736\n",
      "Epoch 205/300\n",
      "Average training loss: 0.022508187560571564\n",
      "Average test loss: 0.0017410100623965264\n",
      "Epoch 206/300\n",
      "Average training loss: 0.022493391616476906\n",
      "Average test loss: 0.0017452287641871306\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022453400742676523\n",
      "Average test loss: 0.001747720584532039\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022392929502659375\n",
      "Average test loss: 0.0017382160512109598\n",
      "Epoch 209/300\n",
      "Average training loss: 0.022449657842516898\n",
      "Average test loss: 0.0017307695297317373\n",
      "Epoch 210/300\n",
      "Average training loss: 0.022402394870917\n",
      "Average test loss: 0.0017470615246436662\n",
      "Epoch 211/300\n",
      "Average training loss: 0.022410371629728212\n",
      "Average test loss: 0.0017125588297430012\n",
      "Epoch 212/300\n",
      "Average training loss: 0.022405396479699347\n",
      "Average test loss: 0.0017404418190320333\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02243937774333689\n",
      "Average test loss: 0.001742063511783878\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0223559144337972\n",
      "Average test loss: 0.0017584883720717496\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02234231291131841\n",
      "Average test loss: 0.0017445612479415206\n",
      "Epoch 216/300\n",
      "Average training loss: 0.022332962075869243\n",
      "Average test loss: 0.001736616021229161\n",
      "Epoch 217/300\n",
      "Average training loss: 0.022385979096094768\n",
      "Average test loss: 0.0017341211079102423\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02236556384546889\n",
      "Average test loss: 0.0017737906942557958\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0223328397952848\n",
      "Average test loss: 0.0017292549562537007\n",
      "Epoch 220/300\n",
      "Average training loss: 0.022324351944857174\n",
      "Average test loss: 0.0017415767117506928\n",
      "Epoch 221/300\n",
      "Average training loss: 0.022325401445229848\n",
      "Average test loss: 0.0017151877181604506\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02230214881234699\n",
      "Average test loss: 0.0020364999034338527\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022298784661624167\n",
      "Average test loss: 0.0017540628173285061\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022252969021598498\n",
      "Average test loss: 0.0017233170819365316\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022285230446192954\n",
      "Average test loss: 0.0018369419309828016\n",
      "Epoch 226/300\n",
      "Average training loss: 0.022273808527323934\n",
      "Average test loss: 0.0017158259022980928\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022263095983200604\n",
      "Average test loss: 0.0017345494274049998\n",
      "Epoch 228/300\n",
      "Average training loss: 0.022222249744666946\n",
      "Average test loss: 0.0017899055099114775\n",
      "Epoch 229/300\n",
      "Average training loss: 0.022259574867784977\n",
      "Average test loss: 0.0017446110660417212\n",
      "Epoch 230/300\n",
      "Average training loss: 0.022195816583103604\n",
      "Average test loss: 0.0017343709841370584\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022236255829532942\n",
      "Average test loss: 0.001824980421198739\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022209321659472255\n",
      "Average test loss: 0.001758861439095603\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022240225961638823\n",
      "Average test loss: 0.0017758557362895873\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022233003608054584\n",
      "Average test loss: 0.0017374712895188067\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02215928292605612\n",
      "Average test loss: 0.0017711403732084566\n",
      "Epoch 236/300\n",
      "Average training loss: 0.022192859657936628\n",
      "Average test loss: 0.00174594076241677\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02224367929332786\n",
      "Average test loss: 0.0017443259263204205\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022139841821458603\n",
      "Average test loss: 0.0017790987129426665\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02216532104048464\n",
      "Average test loss: 0.0017492555965565972\n",
      "Epoch 240/300\n",
      "Average training loss: 0.022174333992931578\n",
      "Average test loss: 0.0017262577234456937\n",
      "Epoch 241/300\n",
      "Average training loss: 0.022164369550844033\n",
      "Average test loss: 0.001748400264005694\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02210079880969392\n",
      "Average test loss: 0.0017277582490609752\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02212703522046407\n",
      "Average test loss: 0.0017633819499363501\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02211523739248514\n",
      "Average test loss: 0.0018267321814265517\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022108907560507457\n",
      "Average test loss: 0.0017433218204726776\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02211020852211449\n",
      "Average test loss: 0.0017731672242904703\n",
      "Epoch 247/300\n",
      "Average training loss: 0.022094613879919053\n",
      "Average test loss: 0.001782049354372753\n",
      "Epoch 248/300\n",
      "Average training loss: 0.022130037569337423\n",
      "Average test loss: 0.0017544635347504583\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02212107505235407\n",
      "Average test loss: 0.001754014728176925\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022065257997976408\n",
      "Average test loss: 0.0017288680099364784\n",
      "Epoch 251/300\n",
      "Average training loss: 0.022086961903505856\n",
      "Average test loss: 0.0017350302782530587\n",
      "Epoch 252/300\n",
      "Average training loss: 0.022105420287284586\n",
      "Average test loss: 0.0018080444919566314\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022158049455947345\n",
      "Average test loss: 0.0017520321593619884\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02202017294201586\n",
      "Average test loss: 0.0017732119892413419\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02200997108883328\n",
      "Average test loss: 0.0017593895953355563\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02206447495188978\n",
      "Average test loss: 0.0017665082887849874\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02203791113032235\n",
      "Average test loss: 0.0017766717239800426\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021998659905460145\n",
      "Average test loss: 0.0017524729292425845\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021999263174004025\n",
      "Average test loss: 0.001744701847847965\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021990910253591008\n",
      "Average test loss: 0.0018086117669526074\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021997647278838687\n",
      "Average test loss: 0.0017450477400173744\n",
      "Epoch 262/300\n",
      "Average training loss: 0.021984721990095244\n",
      "Average test loss: 0.0018534009024086925\n",
      "Epoch 263/300\n",
      "Average training loss: 0.021994164776470926\n",
      "Average test loss: 0.0017222753641092114\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022017443695002133\n",
      "Average test loss: 0.001776833573770192\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022010858030782805\n",
      "Average test loss: 0.0017467519536407457\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02196928576131662\n",
      "Average test loss: 0.0017458976973365578\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021960590647326576\n",
      "Average test loss: 0.0018152630492630933\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021977141075664095\n",
      "Average test loss: 0.0017394550066027377\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021950300388866\n",
      "Average test loss: 0.0017337130861770776\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021937024487389458\n",
      "Average test loss: 0.0017657541862895918\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021928170354829895\n",
      "Average test loss: 0.0017120990030881432\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02192878081401189\n",
      "Average test loss: 0.001775122965582543\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021935998706354036\n",
      "Average test loss: 0.0018034908719774749\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021949691865179272\n",
      "Average test loss: 0.0017565216231677267\n",
      "Epoch 275/300\n",
      "Average training loss: 0.021914586428138944\n",
      "Average test loss: 0.001743981513608661\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021956647369596693\n",
      "Average test loss: 0.001757013312437468\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021904879725641674\n",
      "Average test loss: 0.0017797418219140835\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021876688157518705\n",
      "Average test loss: 0.0019705381157497565\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02189516409403748\n",
      "Average test loss: 0.0017639168864116072\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021885400280356408\n",
      "Average test loss: 0.0017932017058548\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02186336153248946\n",
      "Average test loss: 0.0017560754575663142\n",
      "Epoch 282/300\n",
      "Average training loss: 0.021887384840183786\n",
      "Average test loss: 0.0017822631523013114\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021872421739829912\n",
      "Average test loss: 0.0017657291384206878\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021868123890625105\n",
      "Average test loss: 0.0017615252236525217\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021867072587211926\n",
      "Average test loss: 0.0018031990045888557\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021835622984502052\n",
      "Average test loss: 0.0017329990574055248\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0218476356814305\n",
      "Average test loss: 0.001736499518000831\n",
      "Epoch 288/300\n",
      "Average training loss: 0.021821229270762868\n",
      "Average test loss: 0.0017931208117968507\n",
      "Epoch 289/300\n",
      "Average training loss: 0.021850931147734325\n",
      "Average test loss: 0.0017635410481856928\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0218407666310668\n",
      "Average test loss: 0.0017940583155076537\n",
      "Epoch 291/300\n",
      "Average training loss: 0.021806992588771713\n",
      "Average test loss: 0.001753106478601694\n",
      "Epoch 292/300\n",
      "Average training loss: 0.021822606383098495\n",
      "Average test loss: 0.0017503734902582235\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02182906364897887\n",
      "Average test loss: 0.001773817768941323\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02182540576822228\n",
      "Average test loss: 0.0017495344183925125\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021797624094618693\n",
      "Average test loss: 0.001746253804821107\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021799732797675664\n",
      "Average test loss: 0.0017623404545916452\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02181485109527906\n",
      "Average test loss: 0.001805327776612507\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021828967922263676\n",
      "Average test loss: 0.001757047070397271\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021762233487433856\n",
      "Average test loss: 0.001766478453659349\n",
      "Epoch 300/300\n",
      "Average training loss: 0.021802595247824987\n",
      "Average test loss: 0.0017966625945021709\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.6698677890963024\n",
      "Average test loss: 0.0032422995865345\n",
      "Epoch 2/300\n",
      "Average training loss: 0.14564751062790554\n",
      "Average test loss: 0.0028319058772176505\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08314341659347216\n",
      "Average test loss: 0.002494625607712401\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06161598229408264\n",
      "Average test loss: 0.002370079037629896\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05185363718867302\n",
      "Average test loss: 0.0022596511361706586\n",
      "Epoch 6/300\n",
      "Average training loss: 0.046022233327229815\n",
      "Average test loss: 0.0021117017726517387\n",
      "Epoch 7/300\n",
      "Average training loss: 0.042134547617700364\n",
      "Average test loss: 0.002036621743088795\n",
      "Epoch 8/300\n",
      "Average training loss: 0.039293557114071315\n",
      "Average test loss: 0.0019111686170929008\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03709493950009346\n",
      "Average test loss: 0.001887198918292092\n",
      "Epoch 10/300\n",
      "Average training loss: 0.035335846788353394\n",
      "Average test loss: 0.0018443569988012313\n",
      "Epoch 11/300\n",
      "Average training loss: 0.033881330145729915\n",
      "Average test loss: 0.00166316510633462\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03264529109332297\n",
      "Average test loss: 0.0016327797177558144\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0316406182795763\n",
      "Average test loss: 0.0016273510020433201\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03073234128455321\n",
      "Average test loss: 0.0015468467301171687\n",
      "Epoch 15/300\n",
      "Average training loss: 0.029918611291382048\n",
      "Average test loss: 0.001527607793505821\n",
      "Epoch 16/300\n",
      "Average training loss: 0.029188371408316822\n",
      "Average test loss: 0.0015023087754638658\n",
      "Epoch 17/300\n",
      "Average training loss: 0.028488529395725992\n",
      "Average test loss: 0.001471526789582438\n",
      "Epoch 18/300\n",
      "Average training loss: 0.027869057340754403\n",
      "Average test loss: 0.0014907472065339485\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02723405952089363\n",
      "Average test loss: 0.0014252741447546417\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02666002132826381\n",
      "Average test loss: 0.0014334246506914497\n",
      "Epoch 21/300\n",
      "Average training loss: 0.026106934098733797\n",
      "Average test loss: 0.001440436403370566\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02558765308227804\n",
      "Average test loss: 0.0013588736834418442\n",
      "Epoch 23/300\n",
      "Average training loss: 0.025106243184871143\n",
      "Average test loss: 0.0013501072953869071\n",
      "Epoch 24/300\n",
      "Average training loss: 0.024670022721091905\n",
      "Average test loss: 0.0013359354279107519\n",
      "Epoch 25/300\n",
      "Average training loss: 0.024250985488295556\n",
      "Average test loss: 0.0013165558441542088\n",
      "Epoch 26/300\n",
      "Average training loss: 0.023825721520516607\n",
      "Average test loss: 0.0013128289309226803\n",
      "Epoch 27/300\n",
      "Average training loss: 0.023498905951778094\n",
      "Average test loss: 0.0013118250262406138\n",
      "Epoch 28/300\n",
      "Average training loss: 0.023116328770915667\n",
      "Average test loss: 0.0012856565815293126\n",
      "Epoch 29/300\n",
      "Average training loss: 0.022825950698720083\n",
      "Average test loss: 0.0012803655996815198\n",
      "Epoch 30/300\n",
      "Average training loss: 0.022501977034740977\n",
      "Average test loss: 0.001274237220072084\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02227202185160584\n",
      "Average test loss: 0.001261485497156779\n",
      "Epoch 32/300\n",
      "Average training loss: 0.022024686129556763\n",
      "Average test loss: 0.0012503056234369676\n",
      "Epoch 33/300\n",
      "Average training loss: 0.021798080672820408\n",
      "Average test loss: 0.0012376643752472268\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021653181371589503\n",
      "Average test loss: 0.0012350071693460146\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02142384290860759\n",
      "Average test loss: 0.0012697870573028921\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02131188388168812\n",
      "Average test loss: 0.0012303269379254844\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02107018603053358\n",
      "Average test loss: 0.0012252445003638666\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020956105200780763\n",
      "Average test loss: 0.00121415508331524\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020830789987411762\n",
      "Average test loss: 0.0012065861249963442\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020714824504322476\n",
      "Average test loss: 0.0012038016449660064\n",
      "Epoch 41/300\n",
      "Average training loss: 0.020543965706394777\n",
      "Average test loss: 0.0012170937871560455\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02047088205648793\n",
      "Average test loss: 0.0012214659995709856\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02041219574213028\n",
      "Average test loss: 0.001193159015228351\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020274608429935243\n",
      "Average test loss: 0.0011957129965432815\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020172537134753334\n",
      "Average test loss: 0.0011894549751240346\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020091805678274895\n",
      "Average test loss: 0.0011977898754282957\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019982102548910512\n",
      "Average test loss: 0.0012023756379882494\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0199382618094484\n",
      "Average test loss: 0.0011942478793466257\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019911018038789433\n",
      "Average test loss: 0.0012453293094618454\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019834091127746634\n",
      "Average test loss: 0.001199674935080111\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019709437635209826\n",
      "Average test loss: 0.001173136414339145\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01964157563365168\n",
      "Average test loss: 0.0011929754768498241\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019602873870068125\n",
      "Average test loss: 0.0011629443875410491\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019541634865932994\n",
      "Average test loss: 0.0011970422932257255\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019506912608941395\n",
      "Average test loss: 0.001169574932816128\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01942393929594093\n",
      "Average test loss: 0.0011688505817825596\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019373522991935413\n",
      "Average test loss: 0.001178195291923152\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01933164843999677\n",
      "Average test loss: 0.0011706346451408334\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019282220001022022\n",
      "Average test loss: 0.0011664353320375086\n",
      "Epoch 60/300\n",
      "Average training loss: 0.019217303903566466\n",
      "Average test loss: 0.0011601290079868503\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01915880980922116\n",
      "Average test loss: 0.0011852974969272813\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01910563061386347\n",
      "Average test loss: 0.0011652397373173799\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019042346637282107\n",
      "Average test loss: 0.0011567836029248105\n",
      "Epoch 64/300\n",
      "Average training loss: 0.019032353241410522\n",
      "Average test loss: 0.0011611144652383194\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01900561655726698\n",
      "Average test loss: 0.0011599416411999198\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01893606043027507\n",
      "Average test loss: 0.0011469559142779973\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018839785838292704\n",
      "Average test loss: 0.0011506828321143985\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01880255780948533\n",
      "Average test loss: 0.001164294173837536\n",
      "Epoch 70/300\n",
      "Average training loss: 0.018781944377554788\n",
      "Average test loss: 0.0011738659538742568\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01877407280769613\n",
      "Average test loss: 0.0011540497292040124\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01854340975648827\n",
      "Average test loss: 0.0011533385650772188\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01852183104223675\n",
      "Average test loss: 0.0011635898919776081\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018484305866890482\n",
      "Average test loss: 0.0011642512798102367\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018414247413476307\n",
      "Average test loss: 0.0011581305489720157\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018399577874276374\n",
      "Average test loss: 0.0012894742281900511\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018372017498645518\n",
      "Average test loss: 0.0011513306272940504\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018310192436807687\n",
      "Average test loss: 0.0011527341285513506\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01827238313191467\n",
      "Average test loss: 0.001163921475617422\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01827573590560092\n",
      "Average test loss: 0.0011520552644506098\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01826397491329246\n",
      "Average test loss: 0.001190741641033027\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018211437675688003\n",
      "Average test loss: 0.0011811722602902187\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018164381883210606\n",
      "Average test loss: 0.0011686474387533963\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018131161035762892\n",
      "Average test loss: 0.0011625310967986783\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01810362744745281\n",
      "Average test loss: 0.0011566255023806459\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018102065598799122\n",
      "Average test loss: 0.0011599543002537556\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018041282628973324\n",
      "Average test loss: 0.0011624609120190143\n",
      "Epoch 92/300\n",
      "Average test loss: 0.0011625864712728395\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017974807422194216\n",
      "Average test loss: 0.0011730194324627518\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017922835713459387\n",
      "Average test loss: 0.0011625024246362347\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017849077270262772\n",
      "Average test loss: 0.0011580903995782137\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017833852047721546\n",
      "Average test loss: 0.0011660127033893432\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017802574330733883\n",
      "Average test loss: 0.0011726001839463909\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017759630657732485\n",
      "Average test loss: 0.0011649474767554137\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01772994437896543\n",
      "Average test loss: 0.001179965723823342\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017703076134125393\n",
      "Average test loss: 0.0011515875021513137\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01769516798853874\n",
      "Average test loss: 0.0011753470932857858\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01767873414274719\n",
      "Average test loss: 0.0011727817789134053\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017638772282335492\n",
      "Average test loss: 0.001201956859893269\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017598971641725965\n",
      "Average test loss: 0.0012312128888443113\n",
      "Epoch 108/300\n",
      "Average training loss: 0.017612916162444484\n",
      "Average test loss: 0.0012470279956857363\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01756930684298277\n",
      "Average test loss: 0.0011836797043474185\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01755562280325426\n",
      "Average test loss: 0.0011965808756649494\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01749967572175794\n",
      "Average test loss: 0.0012076982592956888\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01747225130846103\n",
      "Average test loss: 0.0011952267808632718\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017401287082996632\n",
      "Average test loss: 0.0011822545430105592\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017402593890825907\n",
      "Average test loss: 0.00116742053690056\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01738558734042777\n",
      "Average test loss: 0.0011972050316010913\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017356337899963062\n",
      "Average test loss: 0.0011790126846689317\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017338478543692165\n",
      "Average test loss: 0.0012679281700402499\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01732491123759084\n",
      "Average test loss: 0.0011993372877024942\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01752365562899245\n",
      "Average test loss: 0.0012043327322850625\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017361233892540136\n",
      "Average test loss: 0.001181054460712605\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01724310061832269\n",
      "Average test loss: 0.00121122214746558\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017213484421372414\n",
      "Average test loss: 0.001175664729749163\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01717973888748222\n",
      "Average test loss: 0.0012085010984188153\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017158647074467605\n",
      "Average test loss: 0.0011860099493836364\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01717054683052831\n",
      "Average test loss: 0.0011825148582251534\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01713544080323643\n",
      "Average test loss: 0.00118503373902705\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017119902728332415\n",
      "Average test loss: 0.001194245986536973\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017100588949190247\n",
      "Average test loss: 0.0012135485561771526\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017149845451944406\n",
      "Average test loss: 0.0011813176579049064\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01708068393750323\n",
      "Average test loss: 0.0012283931319705314\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017062411016060246\n",
      "Average test loss: 0.0012142155137326981\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017031831140319507\n",
      "Average test loss: 0.0011738192422005038\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01703640635559956\n",
      "Average test loss: 0.001205761986836377\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017018912590212293\n",
      "Average test loss: 0.001200108922759278\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017004034142527314\n",
      "Average test loss: 0.001191135399767922\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016971703350543977\n",
      "Average test loss: 0.0011919582651721107\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017008375417855052\n",
      "Average test loss: 0.0011899776940958368\n",
      "Epoch 142/300\n",
      "Average training loss: 0.016941211187177233\n",
      "Average test loss: 0.0011986930212005973\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016920560939444437\n",
      "Average test loss: 0.00119224073111804\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016915832742220825\n",
      "Average test loss: 0.0011762845885629454\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016923248067498207\n",
      "Average test loss: 0.0011973799593850143\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01690930858751138\n",
      "Average test loss: 0.0011872060478975376\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016842543203797605\n",
      "Average test loss: 0.0012116266505585776\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016837632010380427\n",
      "Average test loss: 0.0012313043986861077\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01681780015842782\n",
      "Average test loss: 0.0012076494906925492\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016799844952093232\n",
      "Average test loss: 0.0011973833001943097\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016799586962494586\n",
      "Average test loss: 0.0012826720998725958\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016782088499102326\n",
      "Average test loss: 0.001199071215465665\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016770945571362973\n",
      "Average test loss: 0.0012350702532049683\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016742675848305227\n",
      "Average test loss: 0.0012084251676375668\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016737063522140185\n",
      "Average test loss: 0.0012108865703663065\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01674867306235764\n",
      "Average test loss: 0.001215572978887293\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016708665274911456\n",
      "Average test loss: 0.001214345203091701\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016695429182714886\n",
      "Average test loss: 0.0012150988954429825\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016697521711389225\n",
      "Average test loss: 0.0012096064452909762\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016680661583112346\n",
      "Average test loss: 0.0012058401954256827\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01663669422351652\n",
      "Average test loss: 0.0012054563148154152\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016634542451964485\n",
      "Average test loss: 0.0012381126017620167\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016622501636544863\n",
      "Average test loss: 0.0013454243177548051\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016584027364850044\n",
      "Average test loss: 0.0012292111754003499\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016600295915371842\n",
      "Average test loss: 0.0012239341102540493\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0165847413217028\n",
      "Average test loss: 0.0012580981089526581\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01657880911976099\n",
      "Average test loss: 0.0012571386494156388\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016616087429225445\n",
      "Average test loss: 0.001228805748331878\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016561977316935858\n",
      "Average test loss: 0.0012176699314473405\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01657341971827878\n",
      "Average test loss: 0.0012167104054759774\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01652958504938417\n",
      "Average test loss: 0.0012003193908474512\n",
      "Epoch 176/300\n",
      "Average training loss: 0.016502818593548404\n",
      "Average test loss: 0.0012187668461766507\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016510474438468616\n",
      "Average test loss: 0.0012194883204065263\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016555920573572316\n",
      "Average test loss: 0.0012528446118554308\n",
      "Epoch 179/300\n",
      "Average training loss: 0.016506181851029395\n",
      "Average test loss: 0.0012108755140151414\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016483286064532067\n",
      "Average test loss: 0.001225923212348587\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016481025466488466\n",
      "Average test loss: 0.0012292037644009623\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016454720217320654\n",
      "Average test loss: 0.0012240771834428111\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01640729718820916\n",
      "Average test loss: 0.0012096534645598795\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016432160279817053\n",
      "Average test loss: 0.0012325364412843352\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016404048067000177\n",
      "Average test loss: 0.0012768479488376114\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01640164303034544\n",
      "Average test loss: 0.001195400168084436\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016422784491545623\n",
      "Average test loss: 0.001229246632920371\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016391808844274946\n",
      "Average test loss: 0.001227542778942734\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016377830541796156\n",
      "Average test loss: 0.001225637921753029\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01634645956920253\n",
      "Average test loss: 0.001265454331619872\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01634265433996916\n",
      "Average test loss: 0.001212718603718612\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01633697701079978\n",
      "Average test loss: 0.0012487158872083657\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01635130139854219\n",
      "Average test loss: 0.0012454704837873579\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01635059280693531\n",
      "Average test loss: 0.0012494560151050489\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01633905341145065\n",
      "Average test loss: 0.0012178934913956456\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016327608012490804\n",
      "Average test loss: 0.0012240154935926613\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01630998908976714\n",
      "Average test loss: 0.0012278211845291986\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01626837148765723\n",
      "Average test loss: 0.0012225997347591652\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01628598847405778\n",
      "Average test loss: 0.0012289149972299735\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016302658198608294\n",
      "Average test loss: 0.0012251566683666574\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01630569154024124\n",
      "Average test loss: 0.0012568210972886947\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016257575401829347\n",
      "Average test loss: 0.0012154746887067126\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0162610146742728\n",
      "Average test loss: 0.001253544291584856\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016253385576109093\n",
      "Average test loss: 0.001238923527352098\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0163010736240281\n",
      "Average test loss: 0.001247040543291304\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016228072120083703\n",
      "Average test loss: 0.0012677716503385454\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016236468449234962\n",
      "Average test loss: 0.0012486075380713575\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01622762133429448\n",
      "Average test loss: 0.0012297898220519224\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016203011908464963\n",
      "Average test loss: 0.0012532395098565354\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016229339542488258\n",
      "Average test loss: 0.001220818904073288\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016202658377587796\n",
      "Average test loss: 0.0012224465443028344\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01616449728111426\n",
      "Average test loss: 0.0012215044863211613\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016174103851119677\n",
      "Average test loss: 0.00124423913926714\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0161778965898686\n",
      "Average test loss: 0.0012480346913345986\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016172860563629203\n",
      "Average test loss: 0.0012592165259023508\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016159454265402422\n",
      "Average test loss: 0.0012164596232275167\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016167172014713287\n",
      "Average test loss: 0.00126400879646341\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01614243040068282\n",
      "Average test loss: 0.001212639058422711\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016113661681612334\n",
      "Average test loss: 0.0012333174482401874\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016108297533045213\n",
      "Average test loss: 0.0012486511484409372\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01613182924605078\n",
      "Average test loss: 0.0012299673372051783\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016110487843553224\n",
      "Average test loss: 0.001253353206647767\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01612708731740713\n",
      "Average test loss: 0.00122482725377712\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016106778312060567\n",
      "Average test loss: 0.001236051041736371\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016123682448433506\n",
      "Average test loss: 0.0012435570714167423\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01609588275435898\n",
      "Average test loss: 0.0012411316212059722\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01607510761833853\n",
      "Average test loss: 0.0012386250448309713\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016086587017609014\n",
      "Average test loss: 0.0012557608425203296\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01609615776522292\n",
      "Average test loss: 0.0012418454071092936\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016040990782280762\n",
      "Average test loss: 0.0012440908493267166\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01604681375126044\n",
      "Average test loss: 0.0012543416297477153\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016092599044243493\n",
      "Average test loss: 0.0012702189671496551\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016035905236171353\n",
      "Average test loss: 0.0012642328292648826\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016020827571551006\n",
      "Average test loss: 0.0012458835490461852\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016030123707320956\n",
      "Average test loss: 0.0012319068539266786\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01602835918383466\n",
      "Average test loss: 0.0012471326222746736\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016014563703702557\n",
      "Average test loss: 0.0012336114044818613\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016012520637777118\n",
      "Average test loss: 0.0012905770712532104\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01600865886774328\n",
      "Average test loss: 0.001255858588187645\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01601179682628976\n",
      "Average test loss: 0.0012350179499739575\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016010789189073775\n",
      "Average test loss: 0.0012316209748387337\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01600924933867322\n",
      "Average test loss: 0.0012240875693969428\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015984881662660176\n",
      "Average test loss: 0.0012280020743815436\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015979251847498946\n",
      "Average test loss: 0.0012629838914920886\n",
      "Epoch 251/300\n",
      "Average training loss: 0.015970533701280754\n",
      "Average test loss: 0.0012899167786041895\n",
      "Epoch 252/300\n",
      "Average training loss: 0.015990206114947794\n",
      "Average test loss: 0.001225692213823398\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01596483835660749\n",
      "Average test loss: 0.0012319714079300562\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015945094597008495\n",
      "Average test loss: 0.0012269725416683488\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01596853654169374\n",
      "Average test loss: 0.001256969492468569\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01591981650226646\n",
      "Average test loss: 0.0012635775236412882\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015940658926963808\n",
      "Average test loss: 0.0012352242852664657\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015916733420557445\n",
      "Average test loss: 0.0012581095097896954\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016683801717228358\n",
      "Average test loss: 0.0012501627915642328\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01591134395202001\n",
      "Average test loss: 0.0012346372739929293\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015884634998109607\n",
      "Average test loss: 0.0012987511646416453\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015904551529222065\n",
      "Average test loss: 0.0012643089919454522\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015924108510216077\n",
      "Average test loss: 0.001272132297926065\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01590244112080998\n",
      "Average test loss: 0.0012633290244783793\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01588398513197899\n",
      "Average test loss: 0.0012451386131967107\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015927459489140247\n",
      "Average test loss: 0.0012843288347745936\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015881369380487335\n",
      "Average test loss: 0.0012519751225287716\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015858362142410543\n",
      "Average test loss: 0.001251474123965535\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015860896341502666\n",
      "Average test loss: 0.0012480123567705354\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015875494466059736\n",
      "Average test loss: 0.0012589236297127274\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01585432337721189\n",
      "Average test loss: 0.0012423327792332405\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015858001479672063\n",
      "Average test loss: 0.00126614242605865\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015860229284399084\n",
      "Average test loss: 0.0012311505953677827\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015842936014963522\n",
      "Average test loss: 0.0012630172530189157\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015839279260900285\n",
      "Average test loss: 0.0012584364448880983\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01582656101220184\n",
      "Average test loss: 0.0013066480002469486\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01584979953037368\n",
      "Average test loss: 0.0012311754542299442\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015807594276964664\n",
      "Average test loss: 0.0012307527100460396\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015835391586853396\n",
      "Average test loss: 0.0012749593451412188\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015834360437260733\n",
      "Average test loss: 0.0012954077566456465\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015809144948091772\n",
      "Average test loss: 0.0012701262143544024\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015804905801183648\n",
      "Average test loss: 0.0012593287868011328\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015792170461681153\n",
      "Average test loss: 0.0012525216650424733\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015803760224746333\n",
      "Average test loss: 0.0012350526962222324\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015798113178875712\n",
      "Average test loss: 0.0012397722819199165\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015783647193676895\n",
      "Average test loss: 0.001251639220242699\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015791690548261007\n",
      "Average test loss: 0.0012488665507278509\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015777024582028388\n",
      "Average test loss: 0.0012544360585096809\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01578273765163289\n",
      "Average test loss: 0.0012633694517426193\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015804671453932922\n",
      "Average test loss: 0.001274169974649946\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015766171070436638\n",
      "Average test loss: 0.0012697731534846955\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015758556070427098\n",
      "Average test loss: 0.0012613074335580071\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01577901275207599\n",
      "Average test loss: 0.001250178722354273\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015767806254327298\n",
      "Average test loss: 0.0012455618957885437\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01575463092409902\n",
      "Average test loss: 0.0012457877833189236\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015742539366914166\n",
      "Average test loss: 0.0012900770966791444\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_80_Depth5/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 9.178822778489854\n",
      "Average test loss: 36.016617354167835\n",
      "Epoch 2/300\n",
      "Average training loss: 4.416801657782661\n",
      "Average test loss: 0.3074418547174169\n",
      "Epoch 3/300\n",
      "Average training loss: 3.0718085604773626\n",
      "Average test loss: 15.234313211581773\n",
      "Epoch 4/300\n",
      "Average training loss: 2.5854611810048422\n",
      "Average test loss: 0.0043590621372891795\n",
      "Epoch 5/300\n",
      "Average training loss: 2.0204520301818847\n",
      "Average test loss: 0.004249170659316911\n",
      "Epoch 6/300\n",
      "Average training loss: 1.5955271824730768\n",
      "Average test loss: 0.004079945445474651\n",
      "Epoch 7/300\n",
      "Average training loss: 1.3319784115685358\n",
      "Average test loss: 0.006449781628118621\n",
      "Epoch 8/300\n",
      "Average training loss: 1.1103455737431844\n",
      "Average test loss: 0.0043299313361446065\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8310090502632989\n",
      "Average test loss: 0.003945938070408172\n",
      "Epoch 11/300\n",
      "Average training loss: 0.7167082468138801\n",
      "Average test loss: 0.0038482487162368167\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5993577028910319\n",
      "Average test loss: 0.003816964165824983\n",
      "Epoch 13/300\n",
      "Average training loss: 0.42740039947297837\n",
      "Average test loss: 0.0038583764802250595\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3656066688961453\n",
      "Average test loss: 0.003754078237960736\n",
      "Epoch 16/300\n",
      "Average training loss: 0.3143628860844506\n",
      "Average test loss: 0.003779323178033034\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2715782633887397\n",
      "Average test loss: 0.003749517498537898\n",
      "Epoch 18/300\n",
      "Average training loss: 0.23785916226440007\n",
      "Average test loss: 0.0037098913461797766\n",
      "Epoch 19/300\n",
      "Average training loss: 0.21159437080224355\n",
      "Average test loss: 0.003754499846862422\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1904192276398341\n",
      "Average test loss: 0.0036890741439743174\n",
      "Epoch 21/300\n",
      "Average training loss: 0.17389070835378434\n",
      "Average training loss: 0.16091708764764998\n",
      "Average test loss: 0.003661445382568571\n",
      "Epoch 23/300\n",
      "Average training loss: 0.14995141535335116\n",
      "Average test loss: 0.0037150640349007313\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1404493612713284\n",
      "Average test loss: 0.0036682110964838003\n",
      "Epoch 25/300\n",
      "Average training loss: 0.13417120883199904\n",
      "Average test loss: 0.0036353941216237017\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1296052384906345\n",
      "Average test loss: 0.003651948764713274\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12595581187142266\n",
      "Average test loss: 0.0036112874702860912\n",
      "Epoch 28/300\n",
      "Average training loss: 0.12304276339213054\n",
      "Average test loss: 0.003617417360138562\n",
      "Epoch 29/300\n",
      "Average training loss: 0.12090473409493764\n",
      "Average test loss: 0.0036015644965486393\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11787684036625756\n",
      "Average test loss: 0.0035873808314402897\n",
      "Epoch 32/300\n",
      "Average training loss: 0.11668668487999174\n",
      "Average test loss: 0.003589908586608039\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11576145019133886\n",
      "Average test loss: 0.003583582183967034\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11493877893024021\n",
      "Average test loss: 0.0035767139018409782\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1141763667066892\n",
      "Average test loss: 0.0035571122941457562\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11357625878519483\n",
      "Average test loss: 0.003546951402392652\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1129909281598197\n",
      "Average test loss: 0.003564250624428193\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11256754155953726\n",
      "Average test loss: 0.003531663261147009\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11161593504084481\n",
      "Average test loss: 0.0035306558929797676\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11120942351553176\n",
      "Average test loss: 0.0035454244433591763\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11051886605554156\n",
      "Average test loss: 0.003523937638435099\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11023569351434707\n",
      "Average test loss: 0.003559233583096001\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10996419075462553\n",
      "Average test loss: 0.003521674145013094\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10960795180002848\n",
      "Average test loss: 0.003536140785449081\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10951129953066507\n",
      "Average test loss: 0.003502634034388595\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10911668631765578\n",
      "Average test loss: 0.003506983512391647\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10885608489645852\n",
      "Average test loss: 0.0035192937103824483\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10858757506145371\n",
      "Average test loss: 0.0035007565816243488\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10837787895070182\n",
      "Average test loss: 0.0035059504879431593\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10798589830928379\n",
      "Average test loss: 0.0035321325150628885\n",
      "Epoch 54/300\n",
      "Average training loss: 0.107682169119517\n",
      "Average test loss: 0.003507774224711789\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10753653384579552\n",
      "Average test loss: 0.0034871435517238245\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10728912108474307\n",
      "Average test loss: 0.0035139570416261754\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10710964770449533\n",
      "Average test loss: 0.0034957678191777732\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10691207055913078\n",
      "Average test loss: 0.0034805049273288913\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1067010998527209\n",
      "Average test loss: 0.0034850927489913173\n",
      "Epoch 60/300\n",
      "Average training loss: 0.10679114022519853\n",
      "Average test loss: 0.0034976867429084247\n",
      "Epoch 61/300\n",
      "Average training loss: 0.10623700120713976\n",
      "Average test loss: 0.0034720380792601243\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1059819177488486\n",
      "Average test loss: 0.0035116030532452796\n",
      "Epoch 64/300\n",
      "Average training loss: 0.10586023366451264\n",
      "Average test loss: 0.0034894035901460382\n",
      "Epoch 65/300\n",
      "Average training loss: 0.10581520091825061\n",
      "Average test loss: 0.0034757083809624115\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10560254105594423\n",
      "Average test loss: 0.003463809212255809\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10547008196512858\n",
      "Average test loss: 0.003477640046324167\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10537570459312863\n",
      "Average test loss: 0.003467080461482207\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10511934187677172\n",
      "Average test loss: 0.003455453098234203\n",
      "Epoch 70/300\n",
      "Average training loss: 0.10499492906861835\n",
      "Average test loss: 0.0034805591832846403\n",
      "Epoch 71/300\n",
      "Average training loss: 0.1049133251508077\n",
      "Average test loss: 0.003465264801349905\n",
      "Epoch 72/300\n",
      "Average training loss: 0.10482063799434238\n",
      "Average test loss: 0.0034705085336334176\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10454051166110569\n",
      "Average test loss: 0.00346428749917282\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10440280096398459\n",
      "Average test loss: 0.0034591168355610634\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10425127047962612\n",
      "Average test loss: 0.0034579795855614872\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10413655321134462\n",
      "Average test loss: 0.003468992629398902\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1040537973244985\n",
      "Average test loss: 0.003461390941714247\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10385916550291908\n",
      "Average test loss: 0.003460162012734347\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10365832093689177\n",
      "Average test loss: 0.0035507185920659037\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10347237616115146\n",
      "Average test loss: 0.0034584014415740966\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10342287437121074\n",
      "Average test loss: 0.003478385093311469\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10332148996988932\n",
      "Average test loss: 0.00347754720867508\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10316556952397028\n",
      "Average test loss: 0.003487282674759626\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10306790334648556\n",
      "Average test loss: 0.0034526215857929653\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10290610084931055\n",
      "Average test loss: 0.0034853326299538216\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10274668246507644\n",
      "Average test loss: 0.0034837326693038144\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10273035408390893\n",
      "Average test loss: 0.0034469843310200505\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10244568904240926\n",
      "Average test loss: 0.0034796276638905206\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10224249458312988\n",
      "Average test loss: 0.003474902712429563\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10208795666032368\n",
      "Average test loss: 0.003466101139783859\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10203121781349182\n",
      "Average test loss: 0.0034615717137025464\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10186373606324196\n",
      "Average test loss: 0.003586469320373403\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10172640693849988\n",
      "Average test loss: 0.0035059670954942705\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10172987292210262\n",
      "Average test loss: 0.003466445413314634\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1015436336795489\n",
      "Average test loss: 0.0035017242700689367\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10117695250113805\n",
      "Average test loss: 0.0034681356342302428\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10109351277351379\n",
      "Average test loss: 0.0034542747881884375\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10094553979237875\n",
      "Average test loss: 0.0034806295805093314\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10080351252357166\n",
      "Average test loss: 0.003489294040741192\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10071384697490268\n",
      "Average test loss: 0.003524290440811051\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10059067608912786\n",
      "Average test loss: 0.0035126542767716777\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10054691892200046\n",
      "Average test loss: 0.0035149924928943315\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10034052901797824\n",
      "Average test loss: 0.0034913814610077277\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10006930892997318\n",
      "Average test loss: 0.003497462832679351\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09997257914808061\n",
      "Average test loss: 0.0035048000620057185\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0998959999481837\n",
      "Average test loss: 0.0035068407629927\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09965513577726152\n",
      "Average test loss: 0.0035062581561505794\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0995363597869873\n",
      "Average test loss: 0.0035393010932538243\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09940602085987726\n",
      "Average test loss: 0.003972633027782043\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09931435138649411\n",
      "Average test loss: 0.003495552329760459\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09924945601489808\n",
      "Average test loss: 0.003480084010503358\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09905980345275667\n",
      "Average test loss: 0.0035241750029640067\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09887297548850378\n",
      "Average test loss: 0.0035197091274377372\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09875206036700143\n",
      "Average test loss: 0.0035117687775443\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09879836481809616\n",
      "Average test loss: 0.003538227622707685\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0983409491578738\n",
      "Average test loss: 0.003544094471053945\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0983726859887441\n",
      "Average test loss: 0.0035200134780671863\n",
      "Epoch 124/300\n",
      "Average training loss: 0.09816552643643485\n",
      "Average test loss: 0.003549741974307431\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09790712038675944\n",
      "Average test loss: 0.0035153061925537058\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09789396272102992\n",
      "Average test loss: 0.003531425329960055\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09770804297261768\n",
      "Average test loss: 0.0035395399532798266\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09751898743046655\n",
      "Average test loss: 0.00364074888618456\n",
      "Epoch 131/300\n",
      "Average training loss: 0.097452913430002\n",
      "Average test loss: 0.003539299963869982\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09727565590540568\n",
      "Average test loss: 0.0035311509327342113\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09722892022132873\n",
      "Average test loss: 0.0035283013673292267\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09712296334240171\n",
      "Average test loss: 0.0035409831661317084\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09702207109000947\n",
      "Average test loss: 0.0037002197814484435\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09687622090842989\n",
      "Average test loss: 0.0036300570426715745\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09692070215940475\n",
      "Average test loss: 0.003568472840719753\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09668578416109085\n",
      "Average test loss: 0.0035240357886585924\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09660016763210297\n",
      "Average test loss: 0.0035992518762747445\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09644268477625317\n",
      "Average test loss: 0.0035784430857747794\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09633149170213276\n",
      "Average test loss: 0.003564676163511144\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09625781435436673\n",
      "Average test loss: 0.003597946715851625\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09621046218607161\n",
      "Average test loss: 0.0035876528140571383\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09605274750789007\n",
      "Average test loss: 0.0035942896171990367\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09592844808763928\n",
      "Average test loss: 0.003589724241859383\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09589455018440883\n",
      "Average test loss: 0.0036402240668733914\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09572330337762833\n",
      "Average test loss: 0.0035763161737057897\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0957488638692432\n",
      "Average test loss: 0.003550056915730238\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0955909126665857\n",
      "Average test loss: 0.003584573284205463\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09554833963844511\n",
      "Average test loss: 0.003565391291139854\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09546682892243068\n",
      "Average test loss: 0.0036356825075215763\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09531066852145725\n",
      "Average test loss: 0.003632188048420681\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09539524012141758\n",
      "Average test loss: 0.0036502256997757486\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09517222408453624\n",
      "Average test loss: 0.0036016610279265377\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09509145492315292\n",
      "Average test loss: 0.003681373858410451\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09500357540448506\n",
      "Average test loss: 0.0035726331025362013\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09488459692398707\n",
      "Average test loss: 0.003624617973756459\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09473121606641345\n",
      "Average test loss: 0.0036424104786581465\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0947797494398223\n",
      "Average test loss: 0.003580717393093639\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09475896792941624\n",
      "Average test loss: 0.003554881846325265\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09468784619039959\n",
      "Average test loss: 0.0036782837866081134\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0943839535050922\n",
      "Average test loss: 0.0036290685907006265\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09444905146625307\n",
      "Average test loss: 0.0036255107753806644\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09445410495996476\n",
      "Average test loss: 0.0035827247877087856\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0942330788506402\n",
      "Average test loss: 0.0036813676446262335\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09419402445024914\n",
      "Average test loss: 0.003590306545711226\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09425060039096408\n",
      "Average test loss: 0.003576681438833475\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09402273762888379\n",
      "Average test loss: 0.0036667207315978076\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09391235256857342\n",
      "Average test loss: 0.0036721248243831925\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0938925744758712\n",
      "Average test loss: 0.0036281999706601104\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09378508658541573\n",
      "Average test loss: 0.0036560234133568075\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09380028544531928\n",
      "Average test loss: 0.003629556464859181\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09367085948917601\n",
      "Average test loss: 0.0036135786494447125\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09367387144433127\n",
      "Average test loss: 0.0035969814786480534\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09350734355052312\n",
      "Average test loss: 0.0036487289383593533\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09346668966611227\n",
      "Average test loss: 0.003600424943284856\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09336796940697564\n",
      "Average test loss: 0.003616786189791229\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09347108756502469\n",
      "Average test loss: 0.003609628875222471\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09325602064530054\n",
      "Average test loss: 0.003607942990958691\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09340110625161065\n",
      "Average test loss: 0.003714303123454253\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09323870180050532\n",
      "Average test loss: 0.0036962165176454517\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09307986430989372\n",
      "Average test loss: 0.0036229702739251986\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09291949475473828\n",
      "Average test loss: 0.003601635096801652\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09289846589830186\n",
      "Average test loss: 0.0036909608880264892\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0928755732708507\n",
      "Average test loss: 0.003715483261893193\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0927978397210439\n",
      "Average test loss: 0.003637518346723583\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09270118296146393\n",
      "Average test loss: 0.0036792492436038124\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09278545014063518\n",
      "Average test loss: 0.003627335351788335\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09266299923923281\n",
      "Average test loss: 0.0036776465347243678\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09263011068767972\n",
      "Average test loss: 0.003607266742736101\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09247786750396093\n",
      "Average test loss: 0.003643158268597391\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09250824972656038\n",
      "Average test loss: 0.0036217792131420638\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0923578556113773\n",
      "Average test loss: 0.003678772781872087\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09233441903856066\n",
      "Average test loss: 0.0036904859244823456\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0922845992313491\n",
      "Average test loss: 0.003605532286067804\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09217162854803933\n",
      "Average test loss: 0.0036913175280723306\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09223046705457899\n",
      "Average test loss: 0.0036504010727836027\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09217211240530014\n",
      "Average test loss: 0.0036444688838803106\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09202759110265309\n",
      "Average test loss: 0.0037140410935713186\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09199591163794199\n",
      "Average test loss: 0.0036782512911078\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09199017496903737\n",
      "Average test loss: 0.003676332658363713\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09198007417056296\n",
      "Average test loss: 0.003649456517150005\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0919784852663676\n",
      "Average test loss: 0.00366220483618478\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09174756504429711\n",
      "Average test loss: 0.0036845171619206665\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09176121686564552\n",
      "Average test loss: 0.003713113175912036\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09178675526380539\n",
      "Average test loss: 0.003723022065849768\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09170843610829778\n",
      "Average test loss: 0.0039010222686661615\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09158486479520797\n",
      "Average test loss: 0.003653545528443323\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09150737012757196\n",
      "Average test loss: 0.003668316996966799\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0914614178472095\n",
      "Average test loss: 0.003672512357433637\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09155695719851388\n",
      "Average test loss: 0.0036996891430268685\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0914130860765775\n",
      "Average test loss: 0.003726730172832807\n",
      "Epoch 213/300\n",
      "Average training loss: 0.091369677901268\n",
      "Average test loss: 0.0036730544275293746\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09131839240921868\n",
      "Average test loss: 0.0037226937452538144\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09122394531965256\n",
      "Average test loss: 0.003739197131039368\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09113280161884096\n",
      "Average test loss: 0.0036362085969497762\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09114992552995682\n",
      "Average test loss: 0.003673909614069594\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09117639142274857\n",
      "Average test loss: 0.00363396778019766\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09114960510863199\n",
      "Average test loss: 0.00363612108429273\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09099275761180455\n",
      "Average test loss: 0.003686428279305498\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09090710463788775\n",
      "Average test loss: 0.0036558385505858393\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09096169116099675\n",
      "Average test loss: 0.0036844282779428692\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09083241930272844\n",
      "Average test loss: 0.003783414331368274\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09084684411684672\n",
      "Average test loss: 0.003667182356946998\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09079640964004729\n",
      "Average test loss: 0.0036750956827567684\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09070983236365848\n",
      "Average test loss: 0.0036274609861688483\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09063438903623157\n",
      "Average test loss: 0.003622811484254069\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09060452259249158\n",
      "Average test loss: 0.003736619218770001\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09053030861086316\n",
      "Average test loss: 0.003679656703232063\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09053928142123752\n",
      "Average test loss: 0.0037038964765767255\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09044165013896094\n",
      "Average test loss: 0.0037443107432789274\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0904782767560747\n",
      "Average test loss: 0.00363037966688474\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09038317336638768\n",
      "Average test loss: 0.0036978653708679807\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0903392494585779\n",
      "Average test loss: 0.0036668320920111405\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09034103610780504\n",
      "Average test loss: 0.0038271957079155576\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09029474041859309\n",
      "Average test loss: 0.0036784948465517827\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09025135645601484\n",
      "Average test loss: 0.0036631943637298213\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09022793206241396\n",
      "Average test loss: 0.003675181700537602\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09016543545325598\n",
      "Average test loss: 0.0037237920169201164\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09015367055601543\n",
      "Average test loss: 0.003605764339160588\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09002978273232778\n",
      "Average test loss: 0.0036521186435388192\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09003032871749667\n",
      "Average test loss: 0.0037463395800441503\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08999179156621297\n",
      "Average test loss: 0.0036852843155049616\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08997283322943582\n",
      "Average test loss: 0.0036851403247565033\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08994181081983778\n",
      "Average test loss: 0.0037359293471607895\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08984890946414735\n",
      "Average test loss: 0.0037098090352697503\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08979981308513217\n",
      "Average test loss: 0.0036758764158520436\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08980156164036857\n",
      "Average test loss: 0.003726872386617793\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08978680307335324\n",
      "Average test loss: 0.0036915469554563364\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08972823084725275\n",
      "Average test loss: 0.003787251121468014\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08967834381262461\n",
      "Average test loss: 0.003709273538034823\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08959447713692983\n",
      "Average test loss: 0.003649086724759804\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08966973451773326\n",
      "Average test loss: 0.0037564360631836785\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08963212300671472\n",
      "Average test loss: 0.0037743495961444244\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08952704751491547\n",
      "Average test loss: 0.003729577557080322\n",
      "Epoch 256/300\n",
      "Average training loss: 0.089468870513969\n",
      "Average test loss: 0.0036658589974459675\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08948878361119164\n",
      "Average test loss: 0.0036863128354565964\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08935986541377174\n",
      "Average test loss: 0.003756854324705071\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08934418087535434\n",
      "Average test loss: 0.0036675304513838556\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08932858163118362\n",
      "Average test loss: 0.003739841789007187\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0892521200577418\n",
      "Average test loss: 0.0037371889617707995\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08925596202744378\n",
      "Average test loss: 0.0037180245344837506\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08921601219971975\n",
      "Average test loss: 0.003675497488015228\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08925990469588174\n",
      "Average test loss: 0.0036447634098844394\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08914320488108529\n",
      "Average test loss: 0.0037264523050851294\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0891342986424764\n",
      "Average test loss: 0.003698532522138622\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08905521006054348\n",
      "Average test loss: 0.0037627362658580145\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08910380523072349\n",
      "Average test loss: 0.0036880368068814276\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08908210811350081\n",
      "Average test loss: 0.003769427194363541\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0890645169681973\n",
      "Average test loss: 0.003748124500322673\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08889884122875001\n",
      "Average test loss: 0.003721202630549669\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08894508876403173\n",
      "Average test loss: 0.0036836225577733584\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08894830459025171\n",
      "Average test loss: 0.0037720210512893065\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08883624219232135\n",
      "Average test loss: 0.003772896583709452\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08889383257759942\n",
      "Average test loss: 0.003668108254671097\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08883350063694848\n",
      "Average test loss: 0.003699810969746775\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08873516288730833\n",
      "Average test loss: 0.0036985625643283127\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0888161573542489\n",
      "Average test loss: 0.0037604518201616076\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08863150384028752\n",
      "Average test loss: 0.0037577027773691546\n",
      "Epoch 280/300\n",
      "Average test loss: 0.0037773845055037074\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08861671449740728\n",
      "Average test loss: 0.0037352260324276156\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0885968784491221\n",
      "Average test loss: 0.003758300858653254\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08848867311742571\n",
      "Average test loss: 0.0037791125083135235\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08860623192124896\n",
      "Average test loss: 0.0037295583554853994\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08844749588436551\n",
      "Average test loss: 0.0036526484514276185\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08826671418216493\n",
      "Average test loss: 0.0037615808215406207\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0883858781059583\n",
      "Average test loss: 0.0037625476237800387\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08836510105927785\n",
      "Average test loss: 0.0036923102032807137\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08826131163040797\n",
      "Average test loss: 0.0036908237474660077\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08825661478439967\n",
      "Average test loss: 0.0037737901515016953\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08815565976169375\n",
      "Average test loss: 0.003772802327035202\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08817115585340395\n",
      "Average test loss: 0.0037153253596689966\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0881351756784651\n",
      "Average test loss: 0.00367101997592383\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08806916405757269\n",
      "Average test loss: 0.003751983115242587\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08805656569533878\n",
      "Average test loss: 0.003739363396540284\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.241179812749227\n",
      "Average test loss: 0.5197179160018762\n",
      "Epoch 2/300\n",
      "Average training loss: 3.1731922647688124\n",
      "Average test loss: 0.004313792661246326\n",
      "Epoch 3/300\n",
      "Average training loss: 2.2005283837848237\n",
      "Average test loss: 0.003815848103000058\n",
      "Epoch 4/300\n",
      "Average training loss: 1.6667775215572782\n",
      "Average test loss: 0.003737218337961369\n",
      "Epoch 5/300\n",
      "Average training loss: 1.1149070173899334\n",
      "Average test loss: 0.0036878006394124693\n",
      "Epoch 7/300\n",
      "Average training loss: 0.9527339359389411\n",
      "Average test loss: 0.003393359125074413\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7817022391955057\n",
      "Average test loss: 0.0033166385350955857\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6407630291514926\n",
      "Average test loss: 0.0032673288142929476\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5202097429699368\n",
      "Average test loss: 0.0032257598704761928\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4275125936932034\n",
      "Average test loss: 0.003244565051669876\n",
      "Epoch 12/300\n",
      "Average training loss: 0.35547317690319485\n",
      "Average test loss: 0.0030993838181926146\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3008585010634528\n",
      "Average test loss: 0.0030522555944820246\n",
      "Epoch 14/300\n",
      "Average training loss: 0.22740910749965243\n",
      "Average test loss: 0.0029413866359326575\n",
      "Epoch 16/300\n",
      "Average training loss: 0.20194888495074378\n",
      "Average test loss: 0.0031247670805702606\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1819882372352812\n",
      "Average test loss: 0.002916511235965623\n",
      "Epoch 18/300\n",
      "Average training loss: 0.152211109413041\n",
      "Average test loss: 0.002870318292536669\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1412181854115592\n",
      "Average test loss: 0.0028416555689440835\n",
      "Epoch 21/300\n",
      "Average training loss: 0.13217374789714814\n",
      "Average test loss: 0.002745809947864877\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12441074909104241\n",
      "Average test loss: 0.0027350331817029253\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11800448020299276\n",
      "Average test loss: 0.0027504155271583135\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11291647718350092\n",
      "Average test loss: 0.0026817700400327644\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10860825063784917\n",
      "Average test loss: 0.002653714039466447\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10513799374302228\n",
      "Average test loss: 0.0026445031404081316\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09949972632196215\n",
      "Average test loss: 0.002637630983773205\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09705923797686895\n",
      "Average test loss: 0.0025794449403054183\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09495735591650009\n",
      "Average test loss: 0.002581255758181214\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09327785944276386\n",
      "Average test loss: 0.0026052904409459895\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09188236030936241\n",
      "Average test loss: 0.0025597750023007393\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09062431759635607\n",
      "Average test loss: 0.0025378641840070487\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08947287373410331\n",
      "Average test loss: 0.0025538071851349538\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08768373219834434\n",
      "Average test loss: 0.0025068648720367086\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08671967515680525\n",
      "Average test loss: 0.0025199776649889017\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08601554512646463\n",
      "Average test loss: 0.0024922756349874866\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08519191763136122\n",
      "Average test loss: 0.0024962839960224097\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08452025671137704\n",
      "Average test loss: 0.002493541744434171\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08367689902583758\n",
      "Average test loss: 0.00248416907650729\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08316132840183046\n",
      "Average test loss: 0.002502799924669994\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08240951592061255\n",
      "Average test loss: 0.002494766640994284\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08183996366792255\n",
      "Average test loss: 0.0024927241337589093\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08083154250515832\n",
      "Average test loss: 0.002456315184219016\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0802281000216802\n",
      "Average test loss: 0.002448220543252925\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07995871822039287\n",
      "Average test loss: 0.0024556766293115087\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07923461561070548\n",
      "Average test loss: 0.0024355860116581124\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07890111042062442\n",
      "Average test loss: 0.0024296216369710036\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07856924997435676\n",
      "Average test loss: 0.0024215162553721005\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07764543099866973\n",
      "Average test loss: 0.002410855153368579\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0773225633766916\n",
      "Average test loss: 0.0024213484896139967\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07712365208731757\n",
      "Average test loss: 0.0024294307571318414\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07665901947352621\n",
      "Average test loss: 0.0024243198103374906\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07645499596993129\n",
      "Average test loss: 0.0024193517418785227\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07604523991213905\n",
      "Average test loss: 0.0024082782047076356\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07586277459727393\n",
      "Average test loss: 0.002430096149651541\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07547804569204648\n",
      "Average test loss: 0.0024203981669205757\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0753231511414051\n",
      "Average test loss: 0.002397370020962424\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07494034097923173\n",
      "Average test loss: 0.0024035804800482258\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07477678932746251\n",
      "Average test loss: 0.002405230136588216\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07437008912695779\n",
      "Average test loss: 0.0023930994808259937\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07416078385048443\n",
      "Average test loss: 0.0024324778294604684\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0739181280930837\n",
      "Average test loss: 0.0024884405580038824\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07385812315013673\n",
      "Average test loss: 0.0024007742887155875\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07365213098459773\n",
      "Average test loss: 0.0024070151918050317\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07331180779801474\n",
      "Average test loss: 0.002397611264967256\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07288313220938047\n",
      "Average test loss: 0.002394759901695781\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07269291645288467\n",
      "Average test loss: 0.002378798221755359\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07255765058596929\n",
      "Average test loss: 0.002411926389568382\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07223097047540876\n",
      "Average test loss: 0.002391343233382536\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07206928136613634\n",
      "Average test loss: 0.0024113754007137485\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07196389150619507\n",
      "Average test loss: 0.0024146460129155054\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0717137862543265\n",
      "Average test loss: 0.0023949684008128112\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07136359894275665\n",
      "Average test loss: 0.002408363090000219\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07123032904995813\n",
      "Average test loss: 0.0024020825202266376\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07114289632439613\n",
      "Average test loss: 0.002438744914614492\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0708513520028856\n",
      "Average test loss: 0.002391101318101088\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07062171037329568\n",
      "Average test loss: 0.002378901301158799\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07047138170401256\n",
      "Average test loss: 0.002429456691361136\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07031841240326564\n",
      "Average test loss: 0.002401440466133257\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07022146051294274\n",
      "Average test loss: 0.0024534191460245185\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06990796676609251\n",
      "Average test loss: 0.0024488948087932334\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06977280725042025\n",
      "Average test loss: 0.0024055440209599004\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06953226665655772\n",
      "Average test loss: 0.0024081546813249587\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06941471677025159\n",
      "Average test loss: 0.002510886946072181\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0692260374294387\n",
      "Average test loss: 0.0024199040652149255\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06895607067147891\n",
      "Average test loss: 0.0024256459664967326\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06887849323948224\n",
      "Average test loss: 0.0024499534647911786\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06877184024122027\n",
      "Average test loss: 0.0024129705845067897\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06863568178812662\n",
      "Average test loss: 0.0024205374266538355\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06836181116104126\n",
      "Average test loss: 0.0024509273434264794\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06817490547564295\n",
      "Average test loss: 0.0024586692662495707\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06800734169284503\n",
      "Average test loss: 0.0024404439248351586\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06799371525976393\n",
      "Average test loss: 0.0024280373601035937\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06781640017694897\n",
      "Average test loss: 0.0024581035032040545\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06762160184648301\n",
      "Average test loss: 0.002449304604281982\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0674719293680456\n",
      "Average test loss: 0.002434444679361251\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06731549699107806\n",
      "Average test loss: 0.002496847429002325\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06726407351758745\n",
      "Average test loss: 0.002431085108261969\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06710396764013503\n",
      "Average test loss: 0.0024688611032648218\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06689709437555737\n",
      "Average test loss: 0.0024493360217246745\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06681200465228823\n",
      "Average test loss: 0.002434824269471897\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06678721513681941\n",
      "Average test loss: 0.0024504554602834913\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0665833606157038\n",
      "Average test loss: 0.0024770192987699476\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06667336808972889\n",
      "Average test loss: 0.0024475144700457654\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06628123294976022\n",
      "Average test loss: 0.0024690749082300397\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06632306747635205\n",
      "Average test loss: 0.0024801987057758703\n",
      "Epoch 112/300\n",
      "Average training loss: 0.066002266228199\n",
      "Average test loss: 0.002489579881851872\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0659965607424577\n",
      "Average test loss: 0.002468565348535776\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06574775762028165\n",
      "Average test loss: 0.0024813839186810785\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06570889656742414\n",
      "Average test loss: 0.002487146878304581\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06554399172133869\n",
      "Average test loss: 0.0024648884636246497\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06551174283027648\n",
      "Average test loss: 0.0024805651439560785\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06552986351648966\n",
      "Average test loss: 0.002481631070996324\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06541881119211515\n",
      "Average test loss: 0.0024926773904719287\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06524274798565441\n",
      "Average test loss: 0.00246791543211374\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06511012462774912\n",
      "Average test loss: 0.002477177670639422\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06495342011584176\n",
      "Average test loss: 0.002510577340506845\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06483813895119561\n",
      "Average test loss: 0.0024579975207646688\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06474078095952669\n",
      "Average test loss: 0.0024437620234158304\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06473900887701246\n",
      "Average test loss: 0.0024732232238683437\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0647757240401374\n",
      "Average test loss: 0.002491902611528834\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06458581651250521\n",
      "Average test loss: 0.002507411137637165\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06440972150365512\n",
      "Average test loss: 0.00264287359909051\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06428228981296222\n",
      "Average test loss: 0.0024826177095787394\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06420095459289021\n",
      "Average test loss: 0.0025061161058644453\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06425838880406486\n",
      "Average test loss: 0.0024710499704298046\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06407704070541594\n",
      "Average test loss: 0.0025140128951105806\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06399669892258114\n",
      "Average test loss: 0.0024908308924900162\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06399164928661452\n",
      "Average test loss: 0.0025577862499695683\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06381263524625036\n",
      "Average test loss: 0.0024781341850757597\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06430047212706672\n",
      "Average test loss: 0.002562435760887133\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06375887186990845\n",
      "Average test loss: 0.0025074269434230195\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0637025832898087\n",
      "Average test loss: 0.0025171493246323533\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06341824991504351\n",
      "Average test loss: 0.0025973895421872537\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06342776702509986\n",
      "Average test loss: 0.0025412228169540566\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06336455216672686\n",
      "Average test loss: 0.002496141207921836\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06333720009691185\n",
      "Average test loss: 0.002520921171332399\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06332995287577312\n",
      "Average test loss: 0.0025895961191919115\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06336439647608333\n",
      "Average test loss: 0.002531454378221598\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06321349335047934\n",
      "Average test loss: 0.0025329623706638812\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06299060892065367\n",
      "Average test loss: 0.0024849321662137906\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06287405784924825\n",
      "Average test loss: 0.002516080951111184\n",
      "Epoch 148/300\n",
      "Average training loss: 0.062919797539711\n",
      "Average test loss: 0.002675893990529908\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06305245963732402\n",
      "Average test loss: 0.0025849560635785264\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06292977556586266\n",
      "Average test loss: 0.0025790239982306958\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06275539749860763\n",
      "Average test loss: 0.0025290887916667596\n",
      "Epoch 152/300\n",
      "Average training loss: 0.062695329848263\n",
      "Average test loss: 0.0025491665419605045\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06247886178890864\n",
      "Average test loss: 0.0025068719589875802\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06253621035483148\n",
      "Average test loss: 0.00252818263704992\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06256151463588079\n",
      "Average test loss: 0.002526749651879072\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06260160347157054\n",
      "Average test loss: 0.002545771087416344\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06226909515592787\n",
      "Average test loss: 0.0027443668248338833\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06227789750032955\n",
      "Average test loss: 0.0025295236081712778\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06223298927148183\n",
      "Average test loss: 0.002513421795227461\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06222676230139203\n",
      "Average test loss: 0.0025587530196126964\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06210621173845397\n",
      "Average test loss: 0.0025310536291864184\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06216702767544323\n",
      "Average test loss: 0.002807473184333907\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06200734700759252\n",
      "Average test loss: 0.0025391161491473517\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06188396871421072\n",
      "Average test loss: 0.0025409181906531253\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06206152984831068\n",
      "Average test loss: 0.002505580929107964\n",
      "Epoch 166/300\n",
      "Average training loss: 0.061783726377619634\n",
      "Average test loss: 0.0025415195166650748\n",
      "Epoch 168/300\n",
      "Average training loss: 0.061757401009400685\n",
      "Average test loss: 0.0025135662452214296\n",
      "Epoch 169/300\n",
      "Average training loss: 0.061755654089980655\n",
      "Average test loss: 0.002548568894051843\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06179356239570512\n",
      "Average test loss: 0.002535950197527806\n",
      "Epoch 171/300\n",
      "Average training loss: 0.061651656852828134\n",
      "Average test loss: 0.0026090634227212933\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06159428261054887\n",
      "Average test loss: 0.0025694842429624665\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06153135333789719\n",
      "Average test loss: 0.0025468744637651575\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06150559714767668\n",
      "Average test loss: 0.0025552740841069156\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06145955381790797\n",
      "Average test loss: 0.0025431293629937703\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06135924452874396\n",
      "Average test loss: 0.0025311715885375937\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06127698509560691\n",
      "Average test loss: 0.0025521618156797354\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06114245571858353\n",
      "Average test loss: 0.002577040185737941\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06118142108453645\n",
      "Average test loss: 0.002559802112686965\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06119685560795996\n",
      "Average test loss: 0.0025779751737912498\n",
      "Epoch 182/300\n",
      "Average training loss: 0.061109059628513124\n",
      "Average test loss: 0.0025452589098778036\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06101146918535232\n",
      "Average test loss: 0.0026001191660761833\n",
      "Epoch 184/300\n",
      "Average training loss: 0.061023046559757654\n",
      "Average test loss: 0.002565010054036975\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06096438174777561\n",
      "Average test loss: 0.002541576283880406\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06097028395202425\n",
      "Average test loss: 0.0026019525188538764\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06082338733143277\n",
      "Average test loss: 0.002581631727930572\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06082984095480707\n",
      "Average test loss: 0.002516220971321066\n",
      "Epoch 190/300\n",
      "Average training loss: 0.060725885497199164\n",
      "Average test loss: 0.002679507609663738\n",
      "Epoch 191/300\n",
      "Average training loss: 0.060670639445384346\n",
      "Average test loss: 0.002575003505167034\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06072627698381742\n",
      "Average test loss: 0.0025540118931482235\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06068887335393164\n",
      "Average test loss: 0.0025763724334537984\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06054328365789519\n",
      "Average test loss: 0.0026018678202397293\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06061364762650596\n",
      "Average test loss: 0.0026005313124300706\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06056301773919\n",
      "Average test loss: 0.0027778589791721767\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06040290294090907\n",
      "Average test loss: 0.002599250141117308\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06044134161207411\n",
      "Average test loss: 0.0025942988513658442\n",
      "Epoch 200/300\n",
      "Average training loss: 0.060356685943073696\n",
      "Average test loss: 0.0025353944862468373\n",
      "Epoch 201/300\n",
      "Average training loss: 0.060422179576423436\n",
      "Average test loss: 0.0026117754214339786\n",
      "Epoch 202/300\n",
      "Average training loss: 0.060773523141940435\n",
      "Average test loss: 0.003976558609555165\n",
      "Epoch 203/300\n",
      "Average training loss: 0.060229650497436525\n",
      "Average test loss: 0.0026557066400224962\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06021481797099113\n",
      "Average test loss: 0.002605477738401128\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06025552700956662\n",
      "Average test loss: 0.0025554054660929574\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06008829720152749\n",
      "Average test loss: 0.0025512869927204317\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06006543720099661\n",
      "Average test loss: 0.002577622212469578\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06011703040202459\n",
      "Average test loss: 0.002545239829768737\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06005840847227308\n",
      "Average test loss: 0.0025648860281540287\n",
      "Epoch 211/300\n",
      "Average training loss: 0.059947225948174795\n",
      "Average test loss: 0.0025623931681944263\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05990728883610831\n",
      "Average test loss: 0.004049089346184499\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05989885033501519\n",
      "Average test loss: 0.0026978879938316016\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05993445960515075\n",
      "Average test loss: 0.002595146469357941\n",
      "Epoch 216/300\n",
      "Average training loss: 0.059889043622546725\n",
      "Average test loss: 0.002600779519519872\n",
      "Epoch 217/300\n",
      "Average training loss: 0.059841169791089166\n",
      "Average test loss: 0.0025720395747986105\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05975549293061098\n",
      "Average test loss: 0.0026104540212286845\n",
      "Epoch 219/300\n",
      "Average training loss: 0.059788487179411785\n",
      "Average test loss: 0.00257246926964985\n",
      "Epoch 220/300\n",
      "Average test loss: 0.002608354065981176\n",
      "Epoch 221/300\n",
      "Average training loss: 0.059790813359949326\n",
      "Average test loss: 0.005625740555011564\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0597131428056293\n",
      "Average test loss: 0.0025980049442085955\n",
      "Epoch 223/300\n",
      "Average training loss: 0.059641604565911825\n",
      "Average training loss: 0.05962647410233816\n",
      "Average test loss: 0.0026492289991842375\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05957329471906026\n",
      "Average test loss: 0.0025836314210254284\n",
      "Epoch 226/300\n",
      "Average training loss: 0.059690794342094\n",
      "Average test loss: 0.0025774125115325054\n",
      "Epoch 227/300\n",
      "Average training loss: 0.059583945555819404\n",
      "Average test loss: 0.002632962871135937\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05944109206729465\n",
      "Average test loss: 0.0025573871098458767\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05947760163413154\n",
      "Average test loss: 0.00278468787132038\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05942974170049031\n",
      "Average test loss: 0.0026940297341595094\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05941315305233002\n",
      "Average test loss: 0.0027079937062743638\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05954416224029329\n",
      "Average test loss: 0.002622145253130131\n",
      "Epoch 233/300\n",
      "Average training loss: 0.059520049267345006\n",
      "Average test loss: 0.0026165787323067587\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05934567787912157\n",
      "Average test loss: 0.002594664124978913\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05924034809403949\n",
      "Average test loss: 0.002581755199366146\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05922555585371123\n",
      "Average test loss: 0.0025932820679412947\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05927133324080043\n",
      "Average test loss: 0.003029617778129048\n",
      "Epoch 238/300\n",
      "Average training loss: 0.059148891362879014\n",
      "Average test loss: 0.00257057420267827\n",
      "Epoch 239/300\n",
      "Average training loss: 0.059218503637446296\n",
      "Average test loss: 0.0025800876496359705\n",
      "Epoch 240/300\n",
      "Average training loss: 0.059190753681792156\n",
      "Average test loss: 0.002547554659139779\n",
      "Epoch 241/300\n",
      "Average training loss: 0.059142445700036156\n",
      "Average test loss: 0.0026237569908714956\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05907940097981029\n",
      "Average test loss: 0.0025974369707206886\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05907359408007728\n",
      "Average test loss: 0.0026012579353733196\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05898543650905291\n",
      "Average test loss: 0.0025943428667055237\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05905769859088792\n",
      "Average test loss: 0.00265072127762768\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05905690759420395\n",
      "Average test loss: 0.0025998459536996153\n",
      "Epoch 247/300\n",
      "Average training loss: 0.058958612746662566\n",
      "Average test loss: 0.002650829671157731\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05908482594291369\n",
      "Average test loss: 0.0025747584833039176\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05900270940694544\n",
      "Average test loss: 0.002658504240628746\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05894408585959011\n",
      "Average test loss: 0.002699035782366991\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05891359903415044\n",
      "Average test loss: 0.002653535982283453\n",
      "Epoch 252/300\n",
      "Average training loss: 0.058797086162699595\n",
      "Average test loss: 0.002593755984885825\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0588748424715466\n",
      "Average test loss: 0.0026412172116753127\n",
      "Epoch 254/300\n",
      "Average training loss: 0.058859311064084374\n",
      "Average test loss: 0.0026389862592849467\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0588840295208825\n",
      "Average test loss: 0.0026826318870815965\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05874394527408812\n",
      "Average test loss: 0.002598450407385826\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05866054370668199\n",
      "Average test loss: 0.0026297809845871396\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05874646733866797\n",
      "Average test loss: 0.0026692635325921907\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05867079711291525\n",
      "Average test loss: 0.002614039636579239\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05869339665108257\n",
      "Average test loss: 0.002585086558841997\n",
      "Epoch 261/300\n",
      "Average training loss: 0.058674926658471425\n",
      "Average test loss: 0.004707371562098463\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0588486205637455\n",
      "Average test loss: 0.0026104491514464217\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05859827596611447\n",
      "Average test loss: 0.0026514326747920777\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05851420795420806\n",
      "Average test loss: 0.0026261715019742647\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05854452661673228\n",
      "Average test loss: 0.002622191396438413\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05848408916923735\n",
      "Average test loss: 0.002660603538155556\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05843580984075864\n",
      "Average test loss: 0.0026014925641939042\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05853913931383027\n",
      "Average test loss: 0.002639475873981913\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0584453354411655\n",
      "Average test loss: 0.0026291445274319912\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05845162324110667\n",
      "Average test loss: 0.0025789832632160847\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05843386548757553\n",
      "Average test loss: 0.002626940132636163\n",
      "Epoch 272/300\n",
      "Average training loss: 0.058352421955929865\n",
      "Average test loss: 0.0026204816709376045\n",
      "Epoch 273/300\n",
      "Average training loss: 0.058344692677259447\n",
      "Average test loss: 0.0027369618113670083\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05831652553876241\n",
      "Average test loss: 0.002622123489673767\n",
      "Epoch 275/300\n",
      "Average training loss: 0.058444603453079856\n",
      "Average test loss: 0.002606418621622854\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05831844091415405\n",
      "Average test loss: 0.0026278763893577786\n",
      "Epoch 277/300\n",
      "Average training loss: 0.058330109457174935\n",
      "Average test loss: 0.002587497798105081\n",
      "Epoch 279/300\n",
      "Average training loss: 0.058304619014263155\n",
      "Average test loss: 0.002600696304399106\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05823700562119484\n",
      "Average test loss: 0.002711110956345995\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05827009015944269\n",
      "Average test loss: 0.0025983700371450848\n",
      "Epoch 282/300\n",
      "Average training loss: 0.058218519204192694\n",
      "Average test loss: 0.0025995711114050616\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05822830296225018\n",
      "Average test loss: 0.0026110122398369843\n",
      "Epoch 284/300\n",
      "Average training loss: 0.058100947611861756\n",
      "Average test loss: 0.002645482548409038\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05808251319660081\n",
      "Average test loss: 0.0026457805319999657\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05805554356177648\n",
      "Average test loss: 0.002591832391348564\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05809027820825577\n",
      "Average test loss: 0.0026320634343557888\n",
      "Epoch 288/300\n",
      "Average training loss: 0.058193520814180374\n",
      "Average test loss: 0.0026231554384446806\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05798749209774865\n",
      "Average test loss: 0.0025721472849448523\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05802604005734126\n",
      "Average test loss: 0.0026489502001139855\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05809236310919126\n",
      "Average test loss: 0.0027023390987887977\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0580164210067855\n",
      "Average test loss: 0.002637654196574456\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05805971906582515\n",
      "Average test loss: 0.002623119825290309\n",
      "Epoch 296/300\n",
      "Average training loss: 0.057831551750501\n",
      "Average test loss: 0.0026107424861854978\n",
      "Epoch 297/300\n",
      "Average training loss: 0.057938790616061955\n",
      "Average test loss: 0.002633056388753984\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0578235153456529\n",
      "Average test loss: 0.002631947833009892\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05782973570956124\n",
      "Average test loss: 0.002639177827992373\n",
      "Epoch 300/300\n",
      "Average training loss: 0.057827660179800455\n",
      "Average test loss: 0.0025920307998441986\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.507570007748074\n",
      "Average test loss: 0.004049890895270639\n",
      "Epoch 2/300\n",
      "Average training loss: 2.867938937081231\n",
      "Average test loss: 0.003262099041293065\n",
      "Epoch 4/300\n",
      "Average training loss: 2.3535171434614393\n",
      "Average test loss: 0.0030870485168157354\n",
      "Epoch 5/300\n",
      "Average training loss: 1.9806751754548815\n",
      "Average test loss: 0.002940764328671826\n",
      "Epoch 6/300\n",
      "Average training loss: 1.7036567197375827\n",
      "Average test loss: 0.0029707156115521987\n",
      "Epoch 7/300\n",
      "Average training loss: 1.4780403459337021\n",
      "Average test loss: 0.0027457902510133053\n",
      "Epoch 8/300\n",
      "Average training loss: 1.2810786714553832\n",
      "Average test loss: 0.0025878879823204542\n",
      "Epoch 9/300\n",
      "Average training loss: 1.1240924748314751\n",
      "Average test loss: 0.002603073853171534\n",
      "Epoch 10/300\n",
      "Average training loss: 0.9861444703208075\n",
      "Average test loss: 0.002444774680253532\n",
      "Epoch 11/300\n",
      "Average training loss: 0.7490692411528693\n",
      "Average test loss: 0.0023236034050997762\n",
      "Epoch 13/300\n",
      "Average training loss: 0.6500316816965739\n",
      "Average test loss: 0.00225095596909523\n",
      "Epoch 14/300\n",
      "Average training loss: 0.5635514877902137\n",
      "Average test loss: 0.002198151798504922\n",
      "Epoch 15/300\n",
      "Average training loss: 0.48709049444728425\n",
      "Average test loss: 0.002178465967790948\n",
      "Epoch 16/300\n",
      "Average training loss: 0.41941193747520444\n",
      "Average test loss: 0.002080707570330964\n",
      "Epoch 17/300\n",
      "Average training loss: 0.35860373322168987\n",
      "Average test loss: 0.0020971881560981273\n",
      "Epoch 18/300\n",
      "Average training loss: 0.30533853761355084\n",
      "Average test loss: 0.002019350054156449\n",
      "Epoch 19/300\n",
      "Average training loss: 0.259149288309945\n",
      "Average test loss: 0.0020048489694794018\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2208031573957867\n",
      "Average test loss: 0.0019465091948707898\n",
      "Epoch 21/300\n",
      "Average training loss: 0.18891089895036486\n",
      "Average test loss: 0.0019560804735455246\n",
      "Epoch 22/300\n",
      "Average training loss: 0.16382990041044024\n",
      "Average test loss: 0.0018998462210098903\n",
      "Epoch 23/300\n",
      "Average training loss: 0.14381332212024264\n",
      "Average test loss: 0.0018946186560723517\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1285701189968321\n",
      "Average test loss: 0.0019022004881149364\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11644189412064022\n",
      "Average test loss: 0.0018397562538997995\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10694542872243458\n",
      "Average test loss: 0.0018243833567119308\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0992076168457667\n",
      "Average test loss: 0.0018418105831369757\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09306759158770243\n",
      "Average test loss: 0.0018080584686249494\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08801167292727365\n",
      "Average test loss: 0.0017951800574858984\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08365618088841438\n",
      "Average test loss: 0.0017860066530605157\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08010493300358454\n",
      "Average test loss: 0.0017787484106504255\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07698993980222278\n",
      "Average test loss: 0.0017613133350387216\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07446897288825777\n",
      "Average test loss: 0.0017629099478945135\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07244771776596705\n",
      "Average test loss: 0.0017517294037259288\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07076426919301351\n",
      "Average test loss: 0.0017358312935878834\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0692943196826511\n",
      "Average test loss: 0.0017258274213721354\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06791151559021738\n",
      "Average test loss: 0.0017109254710893664\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06683438895808326\n",
      "Average test loss: 0.0017078143120225933\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06568475757042568\n",
      "Average test loss: 0.0017262107676102056\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06467202682958709\n",
      "Average test loss: 0.0017072382069503267\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06381715432802836\n",
      "Average test loss: 0.0017008692347961994\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06282450836896897\n",
      "Average test loss: 0.0016901288020114104\n",
      "Epoch 43/300\n",
      "Average training loss: 0.062023798866404425\n",
      "Average test loss: 0.0016802190620866086\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0612551427450445\n",
      "Average test loss: 0.001704077577425374\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05973373002476162\n",
      "Average test loss: 0.0016601008474826813\n",
      "Epoch 47/300\n",
      "Average training loss: 0.059057589997847874\n",
      "Average test loss: 0.0016559352268361383\n",
      "Epoch 48/300\n",
      "Average training loss: 0.058494699276155894\n",
      "Average test loss: 0.0016691322382539511\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05795961104830106\n",
      "Average test loss: 0.0016508224817613762\n",
      "Epoch 50/300\n",
      "Average training loss: 0.057386465701791976\n",
      "Average test loss: 0.001685435325321224\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05673473213116328\n",
      "Average test loss: 0.001685872410837975\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05631358594695727\n",
      "Average test loss: 0.001650918612153166\n",
      "Epoch 53/300\n",
      "Average training loss: 0.055853939841190976\n",
      "Average test loss: 0.0016469603731400437\n",
      "Epoch 54/300\n",
      "Average training loss: 0.055431187929378616\n",
      "Average test loss: 0.0016827225172892212\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05521124033464326\n",
      "Average test loss: 0.001660131064347095\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05431029854880439\n",
      "Average test loss: 0.001648220723701848\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05406499782204628\n",
      "Average test loss: 0.0016335005614285668\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05367170767320527\n",
      "Average test loss: 0.001623108584433794\n",
      "Epoch 60/300\n",
      "Average training loss: 0.053385989397764205\n",
      "Average test loss: 0.0016230642089827194\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05304778635501862\n",
      "Average test loss: 0.0016434592242456144\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05279397357172436\n",
      "Average test loss: 0.0016182273359348377\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05245802798536089\n",
      "Average test loss: 0.0016407942418009043\n",
      "Epoch 64/300\n",
      "Average training loss: 0.051975399358405006\n",
      "Average test loss: 0.0016106164776202704\n",
      "Epoch 66/300\n",
      "Average training loss: 0.051840948720773064\n",
      "Average test loss: 0.001626532004111343\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05155956743160883\n",
      "Average test loss: 0.0016165089100185369\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05134244797627131\n",
      "Average test loss: 0.0016243560084452232\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05106659918030103\n",
      "Average test loss: 0.0016322649690426057\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05101608890957302\n",
      "Average test loss: 0.0016435484985510508\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05070994538068772\n",
      "Average test loss: 0.0016076053661397763\n",
      "Epoch 72/300\n",
      "Average training loss: 0.050457364686661295\n",
      "Average test loss: 0.0016271690984980929\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05022073455982738\n",
      "Average test loss: 0.0016229550604605013\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05013289797306061\n",
      "Average test loss: 0.001622267367111312\n",
      "Epoch 76/300\n",
      "Average training loss: 0.049626837458875445\n",
      "Average test loss: 0.0016084890022046035\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04940945986244413\n",
      "Average test loss: 0.0016100091220190129\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04927538803219795\n",
      "Average test loss: 0.0016765156273419659\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04929059362742636\n",
      "Average test loss: 0.001607623729751342\n",
      "Epoch 81/300\n",
      "Average training loss: 0.048992998239066866\n",
      "Average test loss: 0.0016259038070630695\n",
      "Epoch 82/300\n",
      "Average training loss: 0.048796629746754965\n",
      "Average test loss: 0.0016459306416412194\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04857756553093592\n",
      "Average test loss: 0.0016151625137362216\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04842878053254551\n",
      "Average test loss: 0.0016226189164444803\n",
      "Epoch 85/300\n",
      "Average training loss: 0.048499859849611915\n",
      "Average test loss: 0.0016264093014308149\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04836619644694858\n",
      "Average test loss: 0.0016477507167599267\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04812195830543836\n",
      "Average test loss: 0.0016293355088887944\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04783776223328379\n",
      "Average test loss: 0.00161817858606163\n",
      "Epoch 89/300\n",
      "Average training loss: 0.047686236371596656\n",
      "Average test loss: 0.0016278380814732777\n",
      "Epoch 90/300\n",
      "Average training loss: 0.047561177823278636\n",
      "Average test loss: 0.001641779176166488\n",
      "Epoch 91/300\n",
      "Average training loss: 0.047463315556446714\n",
      "Average test loss: 0.0016239184114254183\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04739364476998647\n",
      "Average test loss: 0.0016195015974549783\n",
      "Epoch 93/300\n",
      "Average training loss: 0.047249829724431036\n",
      "Average test loss: 0.001679740294193228\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04703812729815642\n",
      "Average test loss: 0.0016567928130841918\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04695622656080458\n",
      "Average test loss: 0.0016378105411099064\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04682234077486727\n",
      "Average test loss: 0.001639224083783726\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04681600266032749\n",
      "Average test loss: 0.001627808704144425\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0465506392982271\n",
      "Average test loss: 0.001632096256646845\n",
      "Epoch 99/300\n",
      "Average training loss: 0.046566040876838896\n",
      "Average test loss: 0.0016280263248417113\n",
      "Epoch 100/300\n",
      "Average training loss: 0.046226281099849276\n",
      "Average test loss: 0.001641866346821189\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04606269324156973\n",
      "Average test loss: 0.0016308966601888339\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04590837882955869\n",
      "Average test loss: 0.0016610813925249709\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04592468148801062\n",
      "Average test loss: 0.0016351469231562481\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04570203821195497\n",
      "Average test loss: 0.0016690730163827537\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04548242431547907\n",
      "Average test loss: 0.0016411323936449157\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04533590236968464\n",
      "Average test loss: 0.0016326484740194348\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04527061587406529\n",
      "Average test loss: 0.0016737732462481492\n",
      "Epoch 110/300\n",
      "Average training loss: 0.045269414292441475\n",
      "Average test loss: 0.0016836327904214462\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04525517921646436\n",
      "Average test loss: 0.0017150312362031804\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04493198935190837\n",
      "Average test loss: 0.0016481823145101469\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04476899704005983\n",
      "Average test loss: 0.0016419636674432291\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04469848918914795\n",
      "Average test loss: 0.0016565161626785993\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04464769940243827\n",
      "Average test loss: 0.0016526361513468954\n",
      "Epoch 117/300\n",
      "Average training loss: 0.044546632728642885\n",
      "Average test loss: 0.0016613670881423686\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04435836807555622\n",
      "Average test loss: 0.0016802389874226518\n",
      "Epoch 119/300\n",
      "Average training loss: 0.044525529202487736\n",
      "Average test loss: 0.00164757283590734\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04442355050643285\n",
      "Average test loss: 0.0016633454855117533\n",
      "Epoch 121/300\n",
      "Average training loss: 0.044173373169369166\n",
      "Average test loss: 0.0016923361010849475\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04396966760357221\n",
      "Average test loss: 0.001657795106060803\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04390698969860871\n",
      "Average test loss: 0.0016837190951531132\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04387514866391818\n",
      "Average test loss: 0.0016604791695459021\n",
      "Epoch 126/300\n",
      "Average training loss: 0.043816405206918714\n",
      "Average test loss: 0.00173640773155623\n",
      "Epoch 127/300\n",
      "Average training loss: 0.043730621402462325\n",
      "Average test loss: 0.0016868636226281524\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04386620105637445\n",
      "Average test loss: 0.0016765331772880421\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04351254706581434\n",
      "Average test loss: 0.0016736856368887755\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0435098061396016\n",
      "Average test loss: 0.0017336804633960128\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04334106781250901\n",
      "Average test loss: 0.001698799095633957\n",
      "Epoch 132/300\n",
      "Average training loss: 0.043253175700704256\n",
      "Average test loss: 0.001680925428453419\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04319409071405729\n",
      "Average test loss: 0.0016798340272572306\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04300642920533816\n",
      "Average test loss: 0.0017224431484937667\n",
      "Epoch 137/300\n",
      "Average training loss: 0.043076149175564446\n",
      "Average test loss: 0.0016865517423074279\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04285015928082996\n",
      "Average test loss: 0.0016921145981177688\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04281495173772176\n",
      "Average test loss: 0.001685353406291041\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04277444695432981\n",
      "Average test loss: 0.0017154304887064629\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04269451224969493\n",
      "Average test loss: 0.0016829491972716317\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04264586795038647\n",
      "Average test loss: 0.0016777822177132798\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04258607801132732\n",
      "Average test loss: 0.0016647031485723953\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04260045369466146\n",
      "Average test loss: 0.001691509242893921\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04241023242308034\n",
      "Average test loss: 0.0016919792159977886\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04227297876940833\n",
      "Average test loss: 0.0017299753392322195\n",
      "Epoch 148/300\n",
      "Average training loss: 0.042409091240829894\n",
      "Average test loss: 0.0016721142126868169\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04230018202463786\n",
      "Average test loss: 0.0017233597248171766\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0421826279229588\n",
      "Average test loss: 0.0017122726406281193\n",
      "Epoch 151/300\n",
      "Average training loss: 0.042067723787493176\n",
      "Average test loss: 0.0016864863905227847\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04207761592004034\n",
      "Average test loss: 0.0016930534349133572\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04206812883416811\n",
      "Average test loss: 0.0017118517381863462\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04195563943849669\n",
      "Average test loss: 0.00172412082972005\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04187594292064508\n",
      "Average test loss: 0.0016943325898610055\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04187199263440238\n",
      "Average test loss: 0.0016995701109473076\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04184315271178881\n",
      "Average test loss: 0.0017416524160653352\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04181914495428403\n",
      "Average test loss: 0.0016854815830787022\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04178460793362723\n",
      "Average test loss: 0.0016960261161956523\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04160253986053997\n",
      "Average test loss: 0.0017667008282409775\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04159715329276191\n",
      "Average test loss: 0.0017373926350846888\n",
      "Epoch 163/300\n",
      "Average test loss: 0.0017129498801918494\n",
      "Epoch 164/300\n",
      "Average training loss: 0.041595645697580445\n",
      "Average test loss: 0.0017186409511292975\n",
      "Epoch 165/300\n",
      "Average training loss: 0.041628556152184804\n",
      "Average test loss: 0.0017489250383546783\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04141562606228723\n",
      "Average test loss: 0.0017441299885718358\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04143541582756572\n",
      "Average test loss: 0.0017025762520109613\n",
      "Epoch 168/300\n",
      "Average training loss: 0.041336912711461386\n",
      "Average test loss: 0.0017086480353027582\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04137806712256537\n",
      "Average test loss: 0.0016902202172204852\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04124610348045826\n",
      "Average test loss: 0.00172369830056818\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04123782285716798\n",
      "Average test loss: 0.0017082408347891436\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04128768031464683\n",
      "Average test loss: 0.0017220591528134214\n",
      "Epoch 174/300\n",
      "Average training loss: 0.041182442883650465\n",
      "Average test loss: 0.0016931130372815663\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04105773192975256\n",
      "Average test loss: 0.0017368023802215855\n",
      "Epoch 176/300\n",
      "Average training loss: 0.041112245422270564\n",
      "Average test loss: 0.0017643926391998927\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04100577820340792\n",
      "Average test loss: 0.0017026639848740565\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04095355747805701\n",
      "Average test loss: 0.0017410166329807706\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0409375382281012\n",
      "Average test loss: 0.0017332500790556273\n",
      "Epoch 180/300\n",
      "Average training loss: 0.040993976828124785\n",
      "Average test loss: 0.001748248283751309\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0408272060751915\n",
      "Average test loss: 0.0017468432679565416\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04087805508242713\n",
      "Average test loss: 0.0017233086516045863\n",
      "Epoch 184/300\n",
      "Average training loss: 0.040785011238522\n",
      "Average test loss: 0.0017469867463741039\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0407535096473164\n",
      "Average test loss: 0.0016967427334230807\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04072238218122058\n",
      "Average test loss: 0.0017606190535653797\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04066060396201081\n",
      "Average test loss: 0.0017463032607403066\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04066945616735353\n",
      "Average test loss: 0.001724102484062314\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04056197538309627\n",
      "Average test loss: 0.001729675295141836\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04057487936152352\n",
      "Average test loss: 0.0017095335312187672\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04052737753589948\n",
      "Average test loss: 0.0017515562902101212\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04060317137506273\n",
      "Average test loss: 0.001758095003457533\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04047452906933096\n",
      "Average test loss: 0.0017532746815122663\n",
      "Epoch 195/300\n",
      "Average training loss: 0.040519744591580496\n",
      "Average test loss: 0.0018952455177075334\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04042103280954891\n",
      "Average test loss: 0.0017100049489074283\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04034520068102413\n",
      "Average test loss: 0.0017464699668602811\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04033274531364441\n",
      "Average test loss: 0.0025688656754791735\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0404265517824226\n",
      "Average test loss: 0.0026683322122941413\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0402643568582005\n",
      "Average test loss: 0.0017263804396821392\n",
      "Epoch 202/300\n",
      "Average training loss: 0.040327790866295496\n",
      "Average test loss: 0.0017235333863645792\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04021370826827155\n",
      "Average test loss: 0.0017529679576141967\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04031156165070004\n",
      "Average test loss: 0.0017329236027888125\n",
      "Epoch 205/300\n",
      "Average training loss: 0.040080517835087245\n",
      "Average test loss: 0.001781053683306608\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04023274142212338\n",
      "Average test loss: 0.0018615998909291293\n",
      "Epoch 207/300\n",
      "Average training loss: 0.040065732853280175\n",
      "Average test loss: 0.001769206830403871\n",
      "Epoch 208/300\n",
      "Average training loss: 0.040078402284118866\n",
      "Average test loss: 0.0017448643961300453\n",
      "Epoch 209/300\n",
      "Average training loss: 0.040009961310360165\n",
      "Average test loss: 0.0017293477615134584\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04002194287214014\n",
      "Average test loss: 0.0017308399085369376\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03993956804275513\n",
      "Average test loss: 0.0017478279231323136\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03998314319385422\n",
      "Average test loss: 0.0017295076724969678\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0399230991817183\n",
      "Average test loss: 0.0017867154789467652\n",
      "Epoch 214/300\n",
      "Average training loss: 0.039884900962313015\n",
      "Average test loss: 0.0017470050770789385\n",
      "Epoch 216/300\n",
      "Average training loss: 0.039917324693666566\n",
      "Average test loss: 0.0017706957212131884\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03988174309498734\n",
      "Average test loss: 0.0017292186665452188\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03992273144258393\n",
      "Average test loss: 0.0019163908478286532\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03987478576434983\n",
      "Average test loss: 0.001727309404561917\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0397221122201946\n",
      "Average test loss: 0.0019122998619245158\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03968072091374132\n",
      "Average test loss: 0.0017521499357082777\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03978137392964628\n",
      "Average test loss: 0.0017388124863306682\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03966276308894157\n",
      "Average test loss: 0.0017343714522818725\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03970014682743284\n",
      "Average test loss: 0.001742861735324065\n",
      "Epoch 225/300\n",
      "Average training loss: 0.039685607708162735\n",
      "Average test loss: 0.013679775989924868\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03968338551123937\n",
      "Average test loss: 0.0017644802410569456\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03970864283045133\n",
      "Average test loss: 0.0017601053608167503\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03959901853402455\n",
      "Average test loss: 0.0017782173063606024\n",
      "Epoch 229/300\n",
      "Average training loss: 0.039620908035172354\n",
      "Average test loss: 0.0017513573493601548\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03958511224720213\n",
      "Average test loss: 0.0017285999372187587\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03956514964501063\n",
      "Average test loss: 0.0017389741509945857\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03946317478352123\n",
      "Average test loss: 0.0018299740929570462\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03947381918629011\n",
      "Average test loss: 0.0017694604752792253\n",
      "Epoch 234/300\n",
      "Average training loss: 0.039690281771951254\n",
      "Average test loss: 0.001849770188331604\n",
      "Epoch 235/300\n",
      "Average training loss: 0.039439839747216966\n",
      "Average test loss: 0.001762913800879485\n",
      "Epoch 236/300\n",
      "Average training loss: 0.039498774058289\n",
      "Average test loss: 0.0018444627630524338\n",
      "Epoch 237/300\n",
      "Average training loss: 0.039347149208188055\n",
      "Average test loss: 0.0017430423591285945\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03938360450002882\n",
      "Average test loss: 0.00173486545888914\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03941415892706977\n",
      "Average test loss: 0.001729795014899638\n",
      "Epoch 240/300\n",
      "Average training loss: 0.039355921824773155\n",
      "Average test loss: 0.0017661606790497898\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03928051713936859\n",
      "Average test loss: 0.001748471142620676\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03928736379328701\n",
      "Average test loss: 0.0017784054558724165\n",
      "Epoch 243/300\n",
      "Average training loss: 0.039263544748226804\n",
      "Average test loss: 0.0027215855254067315\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03939943693081538\n",
      "Average test loss: 0.0017483185039729708\n",
      "Epoch 245/300\n",
      "Average training loss: 0.039311274690760505\n",
      "Average test loss: 0.0018170047733518813\n",
      "Epoch 246/300\n",
      "Average training loss: 0.039229909933275645\n",
      "Average test loss: 0.0017360809515747759\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03924505544702212\n",
      "Average test loss: 0.0017912442846637633\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03922947131925159\n",
      "Average test loss: 0.001787763947620988\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03915877761443456\n",
      "Average test loss: 0.0017634234444962608\n",
      "Epoch 250/300\n",
      "Average training loss: 0.039141990817255445\n",
      "Average test loss: 0.0017566950174255503\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03913424036569065\n",
      "Average test loss: 0.0017644461166734496\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03917198239101304\n",
      "Average test loss: 0.0017625725993162227\n",
      "Epoch 253/300\n",
      "Average training loss: 0.039070187966028846\n",
      "Average test loss: 0.00176560968429678\n",
      "Epoch 254/300\n",
      "Average training loss: 0.039127865274747216\n",
      "Average test loss: 0.0017790222350094053\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03908930531475279\n",
      "Average test loss: 0.0017743830221394698\n",
      "Epoch 256/300\n",
      "Average training loss: 0.039008102575937904\n",
      "Average test loss: 0.0017642397818466028\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03908363076382213\n",
      "Average test loss: 0.0017649659027035038\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03905492892861366\n",
      "Average test loss: 0.001756831291752557\n",
      "Epoch 259/300\n",
      "Average training loss: 0.039011913200219475\n",
      "Average test loss: 0.0018273021095535822\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03901486422121525\n",
      "Average test loss: 0.001755897600410713\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03894675897227393\n",
      "Average test loss: 0.0018308426765207616\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03894757577776909\n",
      "Average test loss: 0.0017536833685719304\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03893487839731905\n",
      "Average test loss: 0.0017607450127187702\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0389058488673634\n",
      "Average test loss: 0.0017832874063816336\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03900710331400235\n",
      "Average test loss: 0.0017791556910508209\n",
      "Epoch 266/300\n",
      "Average training loss: 0.038846718453698685\n",
      "Average test loss: 0.001764652414040433\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03886170544226964\n",
      "Average test loss: 0.0017633177217923934\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03889868943227662\n",
      "Average test loss: 0.0017774133040673202\n",
      "Epoch 269/300\n",
      "Average training loss: 0.038862477390302554\n",
      "Average test loss: 0.0018388107876396841\n",
      "Epoch 270/300\n",
      "Average training loss: 0.038811686154868866\n",
      "Average test loss: 0.0017444136747055583\n",
      "Epoch 271/300\n",
      "Average training loss: 0.038809705505768455\n",
      "Average test loss: 0.001840509600006044\n",
      "Epoch 272/300\n",
      "Average training loss: 0.038824405584070416\n",
      "Average test loss: 0.0017806672897810738\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03871754336853822\n",
      "Average test loss: 0.0017800918276318245\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03874239596227805\n",
      "Average test loss: 0.0017662778387053145\n",
      "Epoch 275/300\n",
      "Average training loss: 0.038745816830131743\n",
      "Average test loss: 0.0017938143205311564\n",
      "Epoch 276/300\n",
      "Average training loss: 0.038783625377549064\n",
      "Average test loss: 0.0017697028091384305\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0388355060186651\n",
      "Average test loss: 0.001745312638166878\n",
      "Epoch 278/300\n",
      "Average training loss: 0.038740948809517756\n",
      "Average test loss: 0.0017717370794465145\n",
      "Epoch 279/300\n",
      "Average training loss: 0.038682360531555285\n",
      "Average test loss: 0.0017948339720153146\n",
      "Epoch 280/300\n",
      "Average training loss: 0.038623430798451104\n",
      "Average test loss: 0.0017434922094560332\n",
      "Epoch 281/300\n",
      "Average training loss: 0.038836868729856276\n",
      "Average test loss: 0.0017757603196013306\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03867088230782085\n",
      "Average test loss: 0.001780170068455239\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0386071264627907\n",
      "Average test loss: 0.0017766479717360602\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03859897333714697\n",
      "Average test loss: 0.0017432166184816096\n",
      "Epoch 285/300\n",
      "Average training loss: 0.038663202691409326\n",
      "Average test loss: 0.0017538733465803993\n",
      "Epoch 286/300\n",
      "Average training loss: 0.038611237757735783\n",
      "Average test loss: 0.0017936861664056777\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03854017516639498\n",
      "Average test loss: 0.002067318252184325\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03852742701768875\n",
      "Average test loss: 0.0017814684464699693\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03852180642882983\n",
      "Average test loss: 0.001799964751634333\n",
      "Epoch 290/300\n",
      "Average training loss: 0.038600358532534705\n",
      "Average test loss: 0.0017793811524493828\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03855419411427445\n",
      "Average test loss: 0.0018560774779568115\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03850078839063644\n",
      "Average test loss: 0.0017776986985570855\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03849112065302001\n",
      "Average test loss: 0.0018314338357498247\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0384672999812497\n",
      "Average test loss: 0.0017874339287065797\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03852842375636101\n",
      "Average test loss: 0.0017849151158912315\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03838976440495915\n",
      "Average test loss: 0.0017828400471351213\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03841490044362015\n",
      "Average test loss: 0.0017986644895540344\n",
      "Epoch 298/300\n",
      "Average training loss: 0.038470329201883736\n",
      "Average test loss: 0.0017865235622351368\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03842080609831545\n",
      "Average test loss: 0.0017871539567907652\n",
      "Epoch 300/300\n",
      "Average training loss: 0.038377863206797175\n",
      "Average test loss: 0.0017837000532179243\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.895355690638224\n",
      "Average test loss: 0.004154694232261843\n",
      "Epoch 2/300\n",
      "Average training loss: 2.5721246780819365\n",
      "Average test loss: 0.003023254058841202\n",
      "Epoch 3/300\n",
      "Average training loss: 1.869165381219652\n",
      "Average test loss: 0.003985683863465156\n",
      "Epoch 4/300\n",
      "Average training loss: 1.4704352931976319\n",
      "Average test loss: 0.0024831075167490377\n",
      "Epoch 5/300\n",
      "Average training loss: 1.222939104821947\n",
      "Average test loss: 0.002416348353545699\n",
      "Epoch 6/300\n",
      "Average training loss: 1.0394293236732484\n",
      "Average test loss: 0.0022341086867575842\n",
      "Epoch 7/300\n",
      "Average training loss: 0.8863858568403455\n",
      "Average test loss: 0.002120577456843522\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7581400423579746\n",
      "Average test loss: 0.0020664871922797625\n",
      "Epoch 9/300\n",
      "Average training loss: 0.647419906563229\n",
      "Average test loss: 0.0019471268362055221\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5496197734938727\n",
      "Average test loss: 0.0018626748538679548\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4645186956723531\n",
      "Average test loss: 0.0018941727737999624\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3898751827345954\n",
      "Average test loss: 0.001723125214378039\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3248706797758738\n",
      "Average test loss: 0.0016544128502201704\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2692713137467702\n",
      "Average test loss: 0.0016299233390018343\n",
      "Epoch 15/300\n",
      "Average training loss: 0.2219124202463362\n",
      "Average test loss: 0.001575194314432641\n",
      "Epoch 16/300\n",
      "Average training loss: 0.18448393556806778\n",
      "Average test loss: 0.0015755711575556133\n",
      "Epoch 17/300\n",
      "Average training loss: 0.15598134844170677\n",
      "Average test loss: 0.001491984400794738\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13386262398958207\n",
      "Average test loss: 0.0014769992583120862\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11617444759607315\n",
      "Average test loss: 0.0014885705803624459\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10217903945181105\n",
      "Average test loss: 0.001427994679297424\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09186959212356144\n",
      "Average test loss: 0.0013962437467028698\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08418035723103418\n",
      "Average test loss: 0.0013859642003145483\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07834271595875422\n",
      "Average test loss: 0.0013677912423283689\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07336628783080312\n",
      "Average test loss: 0.0013761189109645785\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06944705563783646\n",
      "Average test loss: 0.0013440152512242396\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06604696377780703\n",
      "Average test loss: 0.0013435879465089075\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06340197411841816\n",
      "Average test loss: 0.0013114077209288048\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06097033255298932\n",
      "Average test loss: 0.0012824628178237213\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05888820709453689\n",
      "Average test loss: 0.0012797099964486228\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05705828039513694\n",
      "Average test loss: 0.001280078224113418\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05546338258518113\n",
      "Average test loss: 0.0013093374169742067\n",
      "Epoch 32/300\n",
      "Average training loss: 0.053954784164826075\n",
      "Average test loss: 0.0012777258657539884\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05274720473753081\n",
      "Average test loss: 0.0012475500762876538\n",
      "Epoch 34/300\n",
      "Average training loss: 0.051537670897112954\n",
      "Average test loss: 0.0012325674777126146\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05041039875480864\n",
      "Average test loss: 0.001236957415834897\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04936673113041454\n",
      "Average test loss: 0.0012327911042504841\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04754188840256797\n",
      "Average test loss: 0.0012136240455632409\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04592037730084525\n",
      "Average test loss: 0.001215776635075195\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0453000073250797\n",
      "Average test loss: 0.0012037783464830783\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0446332618445158\n",
      "Average test loss: 0.001211723091494706\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0440112778213289\n",
      "Average test loss: 0.0011948381528620504\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0434859720799658\n",
      "Average test loss: 0.001190010918304324\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04294830341140429\n",
      "Average test loss: 0.0011851763794612554\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04247593436804083\n",
      "Average test loss: 0.0011997437346726657\n",
      "Epoch 47/300\n",
      "Average training loss: 0.041993606805801394\n",
      "Average test loss: 0.001176622132356796\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0415286258475648\n",
      "Average test loss: 0.0011785116122207707\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04108524829480383\n",
      "Average test loss: 0.0011763402599220475\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04082264796561665\n",
      "Average test loss: 0.0011675126968572536\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04041322879658805\n",
      "Average test loss: 0.0011689597953421374\n",
      "Epoch 52/300\n",
      "Average training loss: 0.040078384516967665\n",
      "Average test loss: 0.001205695623687158\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03986421882443958\n",
      "Average test loss: 0.0011657998748123646\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03953453985187742\n",
      "Average test loss: 0.0011588172807047764\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03922773908244239\n",
      "Average test loss: 0.0011590764365262455\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03886927795410156\n",
      "Average test loss: 0.0011505853016343381\n",
      "Epoch 57/300\n",
      "Average training loss: 0.038687004021472404\n",
      "Average test loss: 0.0011892900845656793\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03858371475835641\n",
      "Average test loss: 0.0011553141354686684\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03840039586524169\n",
      "Average test loss: 0.0011527418350904353\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03804075586133533\n",
      "Average test loss: 0.0011468173826320303\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03787081008652846\n",
      "Average test loss: 0.0011528181962461935\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03764003245366944\n",
      "Average test loss: 0.0011447212955810956\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03753466386265225\n",
      "Average test loss: 0.0011659808734224902\n",
      "Epoch 64/300\n",
      "Average training loss: 0.037519544498787985\n",
      "Average test loss: 0.0011331454399559234\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03710518051518334\n",
      "Average test loss: 0.0011353607481966415\n",
      "Epoch 66/300\n",
      "Average training loss: 0.036834758940670224\n",
      "Average test loss: 0.0011460349186737503\n",
      "Epoch 67/300\n",
      "Average training loss: 0.036743431457214884\n",
      "Average test loss: 0.0011384341050353316\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03659827214645015\n",
      "Average test loss: 0.0012173223006539046\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03646851777202553\n",
      "Average test loss: 0.0011408897263204885\n",
      "Epoch 70/300\n",
      "Average training loss: 0.036293428745534685\n",
      "Average test loss: 0.0011450073628479407\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03610196776688099\n",
      "Average test loss: 0.0011320454026055005\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0360159635461039\n",
      "Average test loss: 0.0011293074722505277\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03580053789913654\n",
      "Average test loss: 0.001138869915364517\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03569005623956521\n",
      "Average test loss: 0.0011236339342883892\n",
      "Epoch 75/300\n",
      "Average training loss: 0.035543889133466615\n",
      "Average test loss: 0.0011479663780579964\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03541444614695178\n",
      "Average test loss: 0.001136578178136713\n",
      "Epoch 77/300\n",
      "Average training loss: 0.035669574447804026\n",
      "Average test loss: 0.0011310056483166085\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03512693847384718\n",
      "Average test loss: 0.001153764017754131\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03494583977262179\n",
      "Average test loss: 0.00114822687757098\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03484360275831488\n",
      "Average test loss: 0.001164014243417316\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03476115989685059\n",
      "Average test loss: 0.0011313881121782794\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03481293818851312\n",
      "Average test loss: 0.0011471881959069935\n",
      "Epoch 83/300\n",
      "Average training loss: 0.034642796663774386\n",
      "Average test loss: 0.00116946579164101\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03435072450670931\n",
      "Average test loss: 0.0012864823391040166\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03421254750423961\n",
      "Average test loss: 0.0011313704198433294\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03415224738915761\n",
      "Average test loss: 0.0011408279604899387\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03413570533527268\n",
      "Average test loss: 0.001136315280261139\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03388585355629523\n",
      "Average test loss: 0.0018700961145675845\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03369210721386803\n",
      "Average test loss: 0.0011600376789768537\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03366400308907032\n",
      "Average test loss: 0.0011342537755974465\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03359004069699181\n",
      "Average test loss: 0.0011684768564171261\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03351943211588595\n",
      "Average test loss: 0.0011679133626425432\n",
      "Epoch 93/300\n",
      "Average training loss: 0.033418587686287034\n",
      "Average test loss: 0.0011690207570273844\n",
      "Epoch 94/300\n",
      "Average training loss: 0.033289639961388375\n",
      "Average test loss: 0.0011675981875095102\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03312153019507726\n",
      "Average test loss: 0.0011526354162229432\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03304645417796241\n",
      "Average test loss: 0.001169105137180951\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03290605632132954\n",
      "Average test loss: 0.001201398511003289\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03285263157718712\n",
      "Average test loss: 0.0011568431355473067\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03278405616680781\n",
      "Average test loss: 0.00116197300174584\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03269551128976875\n",
      "Average test loss: 0.0011443633146377074\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03253075881633494\n",
      "Average test loss: 0.0011772854221053421\n",
      "Epoch 102/300\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_80_Depth5/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
