{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2767426262166765\n",
      "Average test loss: 0.014018040837513075\n",
      "Epoch 2/300\n",
      "Average training loss: 0.1040878919031885\n",
      "Average test loss: 0.011644459908207257\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08474999412563113\n",
      "Average test loss: 0.015503065846032567\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07622158760494656\n",
      "Average test loss: 0.00912888856149382\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06985881433884303\n",
      "Average test loss: 0.015216456765102016\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06446373532878028\n",
      "Average test loss: 0.009479823517302671\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06267012344466315\n",
      "Average test loss: 0.008498453937470913\n",
      "Epoch 8/300\n",
      "Average training loss: 0.058704585863484274\n",
      "Average test loss: 0.0091702588217126\n",
      "Epoch 9/300\n",
      "Average training loss: 0.056953025331099826\n",
      "Average test loss: 0.008171152039534516\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05463389962249332\n",
      "Average test loss: 0.009117535936335722\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05224322571025954\n",
      "Average test loss: 0.008180764659411378\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05050421986314985\n",
      "Average test loss: 0.009986428456174003\n",
      "Epoch 13/300\n",
      "Average training loss: 0.048774613605605234\n",
      "Average test loss: 0.0073225918461879095\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04738382941153314\n",
      "Average test loss: 0.008700481326215797\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04576927684081925\n",
      "Average test loss: 0.008853915108988683\n",
      "Epoch 16/300\n",
      "Average training loss: 0.044240200989776185\n",
      "Average test loss: 0.007956029070748223\n",
      "Epoch 17/300\n",
      "Average training loss: 0.043002366774612\n",
      "Average test loss: 0.007378137631548776\n",
      "Epoch 18/300\n",
      "Average training loss: 0.041904495785633725\n",
      "Average test loss: 0.006807373018728362\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04142743604878585\n",
      "Average test loss: 0.008049049083557394\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04053301133380996\n",
      "Average test loss: 0.00726223826325602\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03963700327277184\n",
      "Average test loss: 0.006575723771833711\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03907475310564041\n",
      "Average test loss: 0.010955213444928329\n",
      "Epoch 23/300\n",
      "Average training loss: 0.038390820138984254\n",
      "Average test loss: 0.00654994488424725\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03776505254043473\n",
      "Average test loss: 0.006375828350583712\n",
      "Epoch 25/300\n",
      "Average training loss: 0.037225595977571274\n",
      "Average test loss: 0.006350537055896388\n",
      "Epoch 26/300\n",
      "Average training loss: 0.037058049359255364\n",
      "Average test loss: 0.006545467792906695\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03654080869754155\n",
      "Average test loss: 0.006734889306128025\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03610253282388051\n",
      "Average test loss: 0.0069836409452060854\n",
      "Epoch 29/300\n",
      "Average training loss: 0.035806070019801454\n",
      "Average test loss: 0.23415940481093195\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0357344223856926\n",
      "Average test loss: 0.00908958420323001\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03502225590414471\n",
      "Average test loss: 0.006218797218882376\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03471069569720162\n",
      "Average test loss: 0.006367471294270621\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03472456696298387\n",
      "Average test loss: 0.00617563131161862\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03415836175779502\n",
      "Average test loss: 0.006293888919055462\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03385110289686256\n",
      "Average test loss: 0.0060944873951375485\n",
      "Epoch 36/300\n",
      "Average training loss: 0.033694766223430636\n",
      "Average test loss: 0.012884974535968569\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033538997201455964\n",
      "Average test loss: 0.00607752568854226\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03343427601787779\n",
      "Average test loss: 0.0070145539873176154\n",
      "Epoch 39/300\n",
      "Average training loss: 0.033034368680583105\n",
      "Average test loss: 0.005991827833983633\n",
      "Epoch 40/300\n",
      "Average training loss: 0.032829269028372236\n",
      "Average test loss: 0.006050951488729981\n",
      "Epoch 41/300\n",
      "Average training loss: 0.032577065729432635\n",
      "Average test loss: 0.006242910224116511\n",
      "Epoch 42/300\n",
      "Average training loss: 0.032446436842282614\n",
      "Average test loss: 0.03302179995179176\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03230265187389321\n",
      "Average test loss: 0.005962412018742826\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03210553604695532\n",
      "Average test loss: 0.006918553298546209\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03189394893248876\n",
      "Average test loss: 0.00597363973243369\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03172944224874179\n",
      "Average test loss: 0.0071453114226460454\n",
      "Epoch 47/300\n",
      "Average training loss: 0.031627709643708334\n",
      "Average test loss: 0.006327816764513651\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0315123867276642\n",
      "Average test loss: 0.006524805817546116\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03147422773639361\n",
      "Average test loss: 0.00662685635768705\n",
      "Epoch 50/300\n",
      "Average training loss: 0.031182937184969586\n",
      "Average test loss: 0.006522703209271034\n",
      "Epoch 51/300\n",
      "Average training loss: 0.031149089250299664\n",
      "Average test loss: 0.007076965488493443\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03098340051372846\n",
      "Average test loss: 0.00593836705179678\n",
      "Epoch 53/300\n",
      "Average training loss: 0.030800287403994136\n",
      "Average test loss: 0.006382243813739883\n",
      "Epoch 54/300\n",
      "Average training loss: 0.030753251635366016\n",
      "Average test loss: 0.01701866384098927\n",
      "Epoch 55/300\n",
      "Average training loss: 0.030610584893160396\n",
      "Average test loss: 0.006282330679396788\n",
      "Epoch 56/300\n",
      "Average training loss: 0.030529227044847278\n",
      "Average test loss: 0.00601780180964205\n",
      "Epoch 57/300\n",
      "Average training loss: 0.030592174814807045\n",
      "Average test loss: 0.006122196961194277\n",
      "Epoch 58/300\n",
      "Average training loss: 0.030339692417118286\n",
      "Average test loss: 0.0059217002975444\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030206780599223243\n",
      "Average test loss: 0.005933542772299714\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030131957029302915\n",
      "Average test loss: 0.006612123041103283\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030088603306147786\n",
      "Average test loss: 0.006304985337787204\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02992791266904937\n",
      "Average test loss: 0.007902478429592318\n",
      "Epoch 63/300\n",
      "Average training loss: 0.029828308350510067\n",
      "Average test loss: 0.0061819038126203745\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029805574893951418\n",
      "Average test loss: 0.006267041364891661\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029645497683021757\n",
      "Average test loss: 0.0063026457350287175\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02961754364437527\n",
      "Average test loss: 0.00612750217732456\n",
      "Epoch 67/300\n",
      "Average training loss: 0.029505621782607502\n",
      "Average test loss: 0.008040455815692743\n",
      "Epoch 68/300\n",
      "Average training loss: 0.029481993161969713\n",
      "Average test loss: 0.006062362920078966\n",
      "Epoch 69/300\n",
      "Average training loss: 0.029387373364633984\n",
      "Average test loss: 0.00632048422553473\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0292902595102787\n",
      "Average test loss: 0.006036390233370993\n",
      "Epoch 71/300\n",
      "Average training loss: 0.029254063798321617\n",
      "Average test loss: 0.00906462961435318\n",
      "Epoch 72/300\n",
      "Average training loss: 0.029126340243551468\n",
      "Average test loss: 0.006321933763722578\n",
      "Epoch 73/300\n",
      "Average training loss: 0.029084675300452445\n",
      "Average test loss: 0.008630396352459987\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02907588158051173\n",
      "Average test loss: 0.006149384817315472\n",
      "Epoch 75/300\n",
      "Average training loss: 0.029009508907794954\n",
      "Average test loss: 0.005891683364080059\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02891726362374094\n",
      "Average test loss: 0.00634992355066869\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02885637862980366\n",
      "Average test loss: 0.006285859395232465\n",
      "Epoch 78/300\n",
      "Average training loss: 0.028720027827554277\n",
      "Average test loss: 0.007351883214381006\n",
      "Epoch 79/300\n",
      "Average training loss: 0.028683270078566338\n",
      "Average test loss: 0.006419950184722741\n",
      "Epoch 80/300\n",
      "Average training loss: 0.028650690090325143\n",
      "Average test loss: 0.006094553078835209\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02857285981045829\n",
      "Average test loss: 0.006115426728294956\n",
      "Epoch 82/300\n",
      "Average training loss: 0.028554099190566274\n",
      "Average test loss: 0.0061436127382848\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02850104458630085\n",
      "Average test loss: 0.006073023709985945\n",
      "Epoch 84/300\n",
      "Average training loss: 0.028431386220786305\n",
      "Average test loss: 0.006373827749656306\n",
      "Epoch 85/300\n",
      "Average training loss: 0.028466840570171674\n",
      "Average test loss: 0.007210516388631529\n",
      "Epoch 86/300\n",
      "Average training loss: 0.028321662445863087\n",
      "Average test loss: 0.006044338893559244\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02825410398675336\n",
      "Average test loss: 0.006734249665505356\n",
      "Epoch 88/300\n",
      "Average training loss: 0.028258461430668833\n",
      "Average test loss: 0.006443412007557021\n",
      "Epoch 89/300\n",
      "Average training loss: 0.028234364413552815\n",
      "Average test loss: 0.005986213143914938\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02810696779522631\n",
      "Average test loss: 0.006212924956447549\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02810404585633013\n",
      "Average test loss: 0.006014877164115509\n",
      "Epoch 92/300\n",
      "Average training loss: 0.028039368414216573\n",
      "Average test loss: 0.006732599240210321\n",
      "Epoch 93/300\n",
      "Average training loss: 0.027928710255357956\n",
      "Average test loss: 0.006033890143864685\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0279105220850971\n",
      "Average test loss: 0.0060759810203065475\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02793301067749659\n",
      "Average test loss: 0.01885872843530443\n",
      "Epoch 96/300\n",
      "Average training loss: 0.027982519879937173\n",
      "Average test loss: 0.006151533050255643\n",
      "Epoch 97/300\n",
      "Average training loss: 0.027780079614784983\n",
      "Average test loss: 0.006419743484507004\n",
      "Epoch 98/300\n",
      "Average training loss: 0.027729386329650878\n",
      "Average test loss: 0.012155338510043091\n",
      "Epoch 99/300\n",
      "Average training loss: 0.027746753426061737\n",
      "Average test loss: 0.006166163747923242\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02777694709599018\n",
      "Average test loss: 0.0066337312128808765\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02762618086569839\n",
      "Average test loss: 0.005907365423937639\n",
      "Epoch 102/300\n",
      "Average training loss: 0.027711343304978476\n",
      "Average test loss: 0.006715844201130999\n",
      "Epoch 103/300\n",
      "Average training loss: 0.027569766655564307\n",
      "Average test loss: 0.005962088225616349\n",
      "Epoch 104/300\n",
      "Average training loss: 0.027610856582721076\n",
      "Average test loss: 0.006307074710726738\n",
      "Epoch 105/300\n",
      "Average training loss: 0.027513976121942202\n",
      "Average test loss: 0.006006324843813976\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02744111550350984\n",
      "Average test loss: 0.006569191212455432\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02750655856397417\n",
      "Average test loss: 0.006186794580684767\n",
      "Epoch 108/300\n",
      "Average training loss: 0.027430835232138635\n",
      "Average test loss: 0.006714955671793884\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02737080177002483\n",
      "Average test loss: 0.006246711389472087\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027293381162815624\n",
      "Average test loss: 0.006352317845241891\n",
      "Epoch 111/300\n",
      "Average training loss: 0.027289411617649927\n",
      "Average test loss: 0.008237282881720199\n",
      "Epoch 112/300\n",
      "Average training loss: 0.027286247493492233\n",
      "Average test loss: 0.007784082270330853\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027210672578877874\n",
      "Average test loss: 0.006470839355554846\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02718287662168344\n",
      "Average test loss: 0.006079339491824309\n",
      "Epoch 115/300\n",
      "Average training loss: 0.027185593247413636\n",
      "Average test loss: 0.0062420168622500365\n",
      "Epoch 116/300\n",
      "Average training loss: 0.027213244396779274\n",
      "Average test loss: 0.006120535468061765\n",
      "Epoch 117/300\n",
      "Average training loss: 0.027155170700616307\n",
      "Average test loss: 0.006153291082216634\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02709056680566735\n",
      "Average test loss: 0.007608534378310045\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02705830029812124\n",
      "Average test loss: 0.006431678309208817\n",
      "Epoch 120/300\n",
      "Average training loss: 0.027026848749981985\n",
      "Average test loss: 0.0060609062578943045\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027048781923121875\n",
      "Average test loss: 0.011918719184895357\n",
      "Epoch 122/300\n",
      "Average training loss: 0.026901142593887116\n",
      "Average test loss: 0.006604870043694973\n",
      "Epoch 123/300\n",
      "Average training loss: 0.026994421399301954\n",
      "Average test loss: 0.006117230945163303\n",
      "Epoch 124/300\n",
      "Average training loss: 0.026890895295474265\n",
      "Average test loss: 0.006152330927136872\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02693432525628143\n",
      "Average test loss: 0.006338555545856555\n",
      "Epoch 126/300\n",
      "Average training loss: 0.026819216796093517\n",
      "Average test loss: 0.006709686832709445\n",
      "Epoch 127/300\n",
      "Average training loss: 0.026773838667405975\n",
      "Average test loss: 0.006041721109300852\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02680687142908573\n",
      "Average test loss: 0.006247383500966761\n",
      "Epoch 129/300\n",
      "Average training loss: 0.026797534313466813\n",
      "Average test loss: 0.010349736757576465\n",
      "Epoch 130/300\n",
      "Average training loss: 0.026800431537959312\n",
      "Average test loss: 0.006015254182120164\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02670436631143093\n",
      "Average test loss: 0.0076424123417172165\n",
      "Epoch 132/300\n",
      "Average training loss: 0.026713395742906463\n",
      "Average test loss: 0.006022759261644549\n",
      "Epoch 133/300\n",
      "Average training loss: 0.026716985556814406\n",
      "Average test loss: 0.006040442892660697\n",
      "Epoch 134/300\n",
      "Average training loss: 0.026700227656298214\n",
      "Average test loss: 0.0061227657294107805\n",
      "Epoch 135/300\n",
      "Average training loss: 0.026663506294290224\n",
      "Average test loss: 0.0065009832233190535\n",
      "Epoch 136/300\n",
      "Average training loss: 0.026531153849429553\n",
      "Average test loss: 0.0060766814599434536\n",
      "Epoch 137/300\n",
      "Average training loss: 0.026633618066708248\n",
      "Average test loss: 0.00605843203721775\n",
      "Epoch 138/300\n",
      "Average training loss: 0.026575654241773817\n",
      "Average test loss: 0.006058753406835927\n",
      "Epoch 139/300\n",
      "Average training loss: 0.026472619363003307\n",
      "Average test loss: 0.006244570312607619\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02652652260661125\n",
      "Average test loss: 0.006114867189692126\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02650125952727265\n",
      "Average test loss: 0.006693072088476685\n",
      "Epoch 142/300\n",
      "Average training loss: 0.026434346675872803\n",
      "Average test loss: 0.006039284765720367\n",
      "Epoch 143/300\n",
      "Average training loss: 0.026368994404872257\n",
      "Average test loss: 0.006046750221401453\n",
      "Epoch 144/300\n",
      "Average training loss: 0.026431382336550287\n",
      "Average test loss: 0.006601010226127174\n",
      "Epoch 145/300\n",
      "Average training loss: 0.026374984797504213\n",
      "Average test loss: 0.006156855717301369\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02632574272155762\n",
      "Average test loss: 0.006433277603652742\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0263915419495768\n",
      "Average test loss: 0.006280066719071733\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02635183215638002\n",
      "Average test loss: 0.006158205529467927\n",
      "Epoch 149/300\n",
      "Average training loss: 0.026302005203233825\n",
      "Average test loss: 0.006417269026239713\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02627844031320678\n",
      "Average test loss: 0.0063087497825423874\n",
      "Epoch 151/300\n",
      "Average training loss: 0.026218237120244237\n",
      "Average test loss: 0.0062103012514611085\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02629556849102179\n",
      "Average test loss: 0.006342910827861892\n",
      "Epoch 153/300\n",
      "Average training loss: 0.026221458514531452\n",
      "Average test loss: 0.006059300198737118\n",
      "Epoch 154/300\n",
      "Average training loss: 0.026212825124462446\n",
      "Average test loss: 0.006328156010972129\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02615843206644058\n",
      "Average test loss: 0.006270446567899651\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02612957902252674\n",
      "Average test loss: 0.006423462197598484\n",
      "Epoch 157/300\n",
      "Average training loss: 0.026185130625963213\n",
      "Average test loss: 0.006367359979906016\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02607979759077231\n",
      "Average test loss: 0.007356172412633896\n",
      "Epoch 159/300\n",
      "Average training loss: 0.026163280088040565\n",
      "Average test loss: 0.006439052421185705\n",
      "Epoch 160/300\n",
      "Average training loss: 0.026067275524139403\n",
      "Average test loss: 0.005998267256137398\n",
      "Epoch 161/300\n",
      "Average training loss: 0.026084638729691505\n",
      "Average test loss: 0.00614928142974774\n",
      "Epoch 162/300\n",
      "Average training loss: 0.025998005467984413\n",
      "Average test loss: 0.00635951608295242\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026041935778326458\n",
      "Average test loss: 0.006415081784543064\n",
      "Epoch 164/300\n",
      "Average training loss: 0.025982046261429786\n",
      "Average test loss: 0.006266988393333223\n",
      "Epoch 165/300\n",
      "Average training loss: 0.025955656045013004\n",
      "Average test loss: 0.0063745697091023125\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026002465966675017\n",
      "Average test loss: 0.006298541259434488\n",
      "Epoch 167/300\n",
      "Average training loss: 0.025961940586566926\n",
      "Average test loss: 0.00628345783924063\n",
      "Epoch 168/300\n",
      "Average training loss: 0.025998031198978425\n",
      "Average test loss: 0.006417505349136061\n",
      "Epoch 169/300\n",
      "Average training loss: 0.025908929765224457\n",
      "Average test loss: 0.00633834836507837\n",
      "Epoch 170/300\n",
      "Average training loss: 0.025954189750883313\n",
      "Average test loss: 0.0068803770608372155\n",
      "Epoch 171/300\n",
      "Average training loss: 0.025894254288739628\n",
      "Average test loss: 0.006409518188900417\n",
      "Epoch 172/300\n",
      "Average training loss: 0.025817836930354436\n",
      "Average test loss: 0.006392161861889892\n",
      "Epoch 173/300\n",
      "Average training loss: 0.025873773900998962\n",
      "Average test loss: 0.006903148924724923\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02589997873041365\n",
      "Average test loss: 0.006267561021778318\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02582203814884027\n",
      "Average test loss: 0.006153564831862847\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0258638421818614\n",
      "Average test loss: 0.006571971209926738\n",
      "Epoch 177/300\n",
      "Average training loss: 0.025796688831514782\n",
      "Average test loss: 0.006403620401190387\n",
      "Epoch 178/300\n",
      "Average training loss: 0.025803027479185\n",
      "Average test loss: 0.006489091615916954\n",
      "Epoch 179/300\n",
      "Average training loss: 0.025746207733949027\n",
      "Average test loss: 0.0061474625352356165\n",
      "Epoch 180/300\n",
      "Average training loss: 0.025762952231698565\n",
      "Average test loss: 0.006546918869846397\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02571310910085837\n",
      "Average test loss: 0.00615027455613017\n",
      "Epoch 182/300\n",
      "Average training loss: 0.025723412682612738\n",
      "Average test loss: 0.006245062540802691\n",
      "Epoch 183/300\n",
      "Average training loss: 0.025700537007715966\n",
      "Average test loss: 0.006943666896058453\n",
      "Epoch 184/300\n",
      "Average training loss: 0.025759505122900008\n",
      "Average test loss: 0.006397132469134198\n",
      "Epoch 185/300\n",
      "Average training loss: 0.025700765828291575\n",
      "Average test loss: 0.006579806481177608\n",
      "Epoch 186/300\n",
      "Average training loss: 0.025676175587707097\n",
      "Average test loss: 0.006241852466017008\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02560731771422757\n",
      "Average test loss: 0.0061825014096167355\n",
      "Epoch 188/300\n",
      "Average training loss: 0.025694866753286784\n",
      "Average test loss: 0.0074340116249190435\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02568402757578426\n",
      "Average test loss: 0.006296995665464137\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02559142869214217\n",
      "Average test loss: 0.006096494828247361\n",
      "Epoch 191/300\n",
      "Average training loss: 0.025550236754947237\n",
      "Average test loss: 0.006364992791579829\n",
      "Epoch 192/300\n",
      "Average training loss: 0.025633201718330383\n",
      "Average test loss: 0.006241613278372421\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02554860549337334\n",
      "Average test loss: 0.006159177647696601\n",
      "Epoch 194/300\n",
      "Average training loss: 0.025508641764521597\n",
      "Average test loss: 0.006333697917560737\n",
      "Epoch 195/300\n",
      "Average training loss: 0.025582700577047136\n",
      "Average test loss: 0.006736813469479482\n",
      "Epoch 196/300\n",
      "Average training loss: 0.025518656639589205\n",
      "Average test loss: 0.006159961503412989\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02552334195872148\n",
      "Average test loss: 0.006060317787445254\n",
      "Epoch 198/300\n",
      "Average training loss: 0.025492188564605184\n",
      "Average test loss: 0.0062543305831236976\n",
      "Epoch 199/300\n",
      "Average training loss: 0.025480949603848987\n",
      "Average test loss: 0.006262025507373942\n",
      "Epoch 200/300\n",
      "Average training loss: 0.025559202791916\n",
      "Average test loss: 0.006205581405096584\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02543443236582809\n",
      "Average test loss: 0.006419416238864263\n",
      "Epoch 202/300\n",
      "Average training loss: 0.025470062335332236\n",
      "Average test loss: 0.006140664526157909\n",
      "Epoch 203/300\n",
      "Average training loss: 0.025460574688182937\n",
      "Average test loss: 0.006493760890430874\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02548083482682705\n",
      "Average test loss: 0.006193118568509817\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02539451463520527\n",
      "Average test loss: 0.0065172093680335415\n",
      "Epoch 206/300\n",
      "Average training loss: 0.025433489365710153\n",
      "Average test loss: 0.008696553862343233\n",
      "Epoch 207/300\n",
      "Average training loss: 0.025392069266902077\n",
      "Average test loss: 0.006267162885102961\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0253629437453217\n",
      "Average test loss: 0.006163936668386062\n",
      "Epoch 209/300\n",
      "Average training loss: 0.025325142156746652\n",
      "Average test loss: 0.006659453803466426\n",
      "Epoch 210/300\n",
      "Average training loss: 0.025304274318946732\n",
      "Average test loss: 0.0063548797091676126\n",
      "Epoch 211/300\n",
      "Average training loss: 0.025343409748540983\n",
      "Average test loss: 0.006514497420026196\n",
      "Epoch 212/300\n",
      "Average training loss: 0.025329875138070848\n",
      "Average test loss: 0.006406806089811855\n",
      "Epoch 213/300\n",
      "Average training loss: 0.025287630998426014\n",
      "Average test loss: 0.00661158266911904\n",
      "Epoch 214/300\n",
      "Average training loss: 0.025332797358433405\n",
      "Average test loss: 0.00625486353577839\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02531858722700013\n",
      "Average test loss: 0.006485724560502503\n",
      "Epoch 216/300\n",
      "Average training loss: 0.025233477277888192\n",
      "Average test loss: 0.006214871142059565\n",
      "Epoch 217/300\n",
      "Average training loss: 0.025249300165308845\n",
      "Average test loss: 0.006673025500857168\n",
      "Epoch 218/300\n",
      "Average training loss: 0.025244871836569573\n",
      "Average test loss: 0.006335479129519727\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02522706416911549\n",
      "Average test loss: 0.0069337438253892795\n",
      "Epoch 220/300\n",
      "Average training loss: 0.025240886772672336\n",
      "Average test loss: 0.006673340752513872\n",
      "Epoch 221/300\n",
      "Average training loss: 0.025236674730976422\n",
      "Average test loss: 0.006413662400510576\n",
      "Epoch 222/300\n",
      "Average training loss: 0.025211516216397285\n",
      "Average test loss: 0.0064458383205864165\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02520747539732191\n",
      "Average test loss: 0.006294480314685239\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02516876577006446\n",
      "Average test loss: 0.006142937085487776\n",
      "Epoch 225/300\n",
      "Average training loss: 0.025147464594907232\n",
      "Average test loss: 0.006490153545306789\n",
      "Epoch 226/300\n",
      "Average training loss: 0.025111052357488207\n",
      "Average test loss: 0.0063079739117787945\n",
      "Epoch 227/300\n",
      "Average training loss: 0.025123109536038506\n",
      "Average test loss: 0.006413354494919379\n",
      "Epoch 228/300\n",
      "Average training loss: 0.025124753412273194\n",
      "Average test loss: 0.006252534415158961\n",
      "Epoch 229/300\n",
      "Average training loss: 0.025138740138875112\n",
      "Average test loss: 0.006179398304058446\n",
      "Epoch 230/300\n",
      "Average training loss: 0.025100234314799307\n",
      "Average test loss: 0.006320042957034376\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02514881077574359\n",
      "Average test loss: 0.006856812259389294\n",
      "Epoch 232/300\n",
      "Average training loss: 0.025115504817830193\n",
      "Average test loss: 0.00639885073983007\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02506307130886449\n",
      "Average test loss: 0.006482246660316984\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02507741045455138\n",
      "Average test loss: 0.006328951404740413\n",
      "Epoch 235/300\n",
      "Average training loss: 0.025030253160330983\n",
      "Average test loss: 0.006615864787664678\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02509147990743319\n",
      "Average test loss: 0.0064802149915032916\n",
      "Epoch 237/300\n",
      "Average training loss: 0.025049932080838416\n",
      "Average test loss: 0.007058616609209114\n",
      "Epoch 238/300\n",
      "Average training loss: 0.025055042897661528\n",
      "Average test loss: 0.006465630628168583\n",
      "Epoch 239/300\n",
      "Average training loss: 0.025014678276247447\n",
      "Average test loss: 0.006339057330456045\n",
      "Epoch 240/300\n",
      "Average training loss: 0.024971771500176855\n",
      "Average test loss: 0.007254938568092055\n",
      "Epoch 241/300\n",
      "Average training loss: 0.025005518400006824\n",
      "Average test loss: 0.006422306980937719\n",
      "Epoch 242/300\n",
      "Average training loss: 0.024984531136022673\n",
      "Average test loss: 0.006401609992815389\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02496912374595801\n",
      "Average test loss: 0.006221500565608343\n",
      "Epoch 244/300\n",
      "Average training loss: 0.024929102515180906\n",
      "Average test loss: 0.006397382337599993\n",
      "Epoch 245/300\n",
      "Average training loss: 0.024974703164564238\n",
      "Average test loss: 0.006156557210203674\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02496417409181595\n",
      "Average test loss: 0.006229863631642527\n",
      "Epoch 247/300\n",
      "Average training loss: 0.025024418491456243\n",
      "Average test loss: 0.007063200463023451\n",
      "Epoch 248/300\n",
      "Average training loss: 0.024880255114701058\n",
      "Average test loss: 0.006162813131180074\n",
      "Epoch 249/300\n",
      "Average training loss: 0.024969263540373907\n",
      "Average test loss: 0.006102695042888323\n",
      "Epoch 250/300\n",
      "Average training loss: 0.024889571112063195\n",
      "Average test loss: 0.006765076246526506\n",
      "Epoch 251/300\n",
      "Average training loss: 0.024882976604832543\n",
      "Average test loss: 0.006827883321377966\n",
      "Epoch 252/300\n",
      "Average training loss: 0.024906260155969195\n",
      "Average test loss: 0.00621171454878317\n",
      "Epoch 253/300\n",
      "Average training loss: 0.024942457081543076\n",
      "Average test loss: 0.006341815429015292\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02492568705810441\n",
      "Average test loss: 0.0065918077702323595\n",
      "Epoch 255/300\n",
      "Average training loss: 0.024873097894920242\n",
      "Average test loss: 0.006352483413078719\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02479228182302581\n",
      "Average test loss: 0.0068903070133593345\n",
      "Epoch 257/300\n",
      "Average training loss: 0.024862542173928685\n",
      "Average test loss: 0.006347172576934099\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02482152398924033\n",
      "Average test loss: 0.0065573272386358844\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02483571473757426\n",
      "Average test loss: 0.0064769308815399804\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02482450539039241\n",
      "Average test loss: 0.006381180211901665\n",
      "Epoch 261/300\n",
      "Average training loss: 0.024819960375626882\n",
      "Average test loss: 0.00666101677591602\n",
      "Epoch 262/300\n",
      "Average training loss: 0.024831603828403683\n",
      "Average test loss: 0.00662602118237151\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02483231104579237\n",
      "Average test loss: 0.00639107831981447\n",
      "Epoch 264/300\n",
      "Average training loss: 0.024810012666715515\n",
      "Average test loss: 0.006623531252973609\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02478423878467745\n",
      "Average test loss: 0.00659765714738104\n",
      "Epoch 266/300\n",
      "Average training loss: 0.024778372489743763\n",
      "Average test loss: 0.006482295689897405\n",
      "Epoch 267/300\n",
      "Average training loss: 0.024860975032051404\n",
      "Average test loss: 0.006303676096929444\n",
      "Epoch 268/300\n",
      "Average training loss: 0.024711828839447763\n",
      "Average test loss: 0.006729199649973048\n",
      "Epoch 269/300\n",
      "Average training loss: 0.024707766183548502\n",
      "Average test loss: 0.006188101644317309\n",
      "Epoch 270/300\n",
      "Average training loss: 0.024737628370523453\n",
      "Average test loss: 0.006258330793844329\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02474653940068351\n",
      "Average test loss: 0.006202656151519882\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02470931492249171\n",
      "Average test loss: 0.006468438102139367\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02470873212483194\n",
      "Average test loss: 0.006524727081259092\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02476145668824514\n",
      "Average test loss: 0.0065488563093046345\n",
      "Epoch 275/300\n",
      "Average training loss: 0.024701844240228334\n",
      "Average test loss: 0.006321669643123945\n",
      "Epoch 276/300\n",
      "Average training loss: 0.024620778171552553\n",
      "Average test loss: 0.00653768878761265\n",
      "Epoch 277/300\n",
      "Average training loss: 0.024719940824641122\n",
      "Average test loss: 0.006459697847151094\n",
      "Epoch 278/300\n",
      "Average training loss: 0.024649416185087626\n",
      "Average test loss: 0.006431515434549915\n",
      "Epoch 279/300\n",
      "Average training loss: 0.024623683823479545\n",
      "Average test loss: 0.006282315481454134\n",
      "Epoch 280/300\n",
      "Average training loss: 0.024635176387925943\n",
      "Average test loss: 0.00645935469865799\n",
      "Epoch 281/300\n",
      "Average training loss: 0.024634241575996082\n",
      "Average test loss: 0.0063677176063259445\n",
      "Epoch 282/300\n",
      "Average training loss: 0.024627305428187054\n",
      "Average test loss: 0.006357442982494831\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02460943810807334\n",
      "Average test loss: 0.006386312676386701\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02464001284042994\n",
      "Average test loss: 0.0063473719205293395\n",
      "Epoch 285/300\n",
      "Average training loss: 0.024668291297223834\n",
      "Average test loss: 0.006286692077087031\n",
      "Epoch 286/300\n",
      "Average training loss: 0.024561124288373523\n",
      "Average test loss: 0.006355612371944719\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02459269516997867\n",
      "Average test loss: 0.006449056439515617\n",
      "Epoch 288/300\n",
      "Average training loss: 0.024534917722145718\n",
      "Average test loss: 0.006552585332343976\n",
      "Epoch 289/300\n",
      "Average training loss: 0.024565430238842965\n",
      "Average test loss: 0.00628999755365981\n",
      "Epoch 290/300\n",
      "Average training loss: 0.024580412545137934\n",
      "Average test loss: 0.006456365630030632\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02453768743574619\n",
      "Average test loss: 0.006851246648364597\n",
      "Epoch 292/300\n",
      "Average training loss: 0.024568300495545068\n",
      "Average test loss: 0.0067102714089883695\n",
      "Epoch 293/300\n",
      "Average training loss: 0.024578474821315872\n",
      "Average test loss: 0.006270454036692778\n",
      "Epoch 294/300\n",
      "Average training loss: 0.024484797878397836\n",
      "Average test loss: 0.006465443616939915\n",
      "Epoch 295/300\n",
      "Average training loss: 0.024523833956983355\n",
      "Average test loss: 0.006626237397392591\n",
      "Epoch 296/300\n",
      "Average training loss: 0.024501798484060498\n",
      "Average test loss: 0.0064090392303963505\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02459621853629748\n",
      "Average test loss: 0.0063753800830907295\n",
      "Epoch 298/300\n",
      "Average training loss: 0.024514917643533814\n",
      "Average test loss: 0.007713265592025386\n",
      "Epoch 299/300\n",
      "Average training loss: 0.024523506205942895\n",
      "Average test loss: 0.006642992592520184\n",
      "Epoch 300/300\n",
      "Average training loss: 0.024504678896731802\n",
      "Average test loss: 0.006374911015232404\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.27195840317673153\n",
      "Average test loss: 0.015743176649841997\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09777175591389338\n",
      "Average test loss: 0.00986614976078272\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07488750950495401\n",
      "Average test loss: 0.009199921807481182\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06414306236637964\n",
      "Average test loss: 0.006889425157672829\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05752035377091832\n",
      "Average test loss: 0.00755118096454276\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05250652131769392\n",
      "Average test loss: 0.005726826991471979\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04907897769080268\n",
      "Average test loss: 0.005508607128014167\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04646261994043986\n",
      "Average test loss: 0.00546088208258152\n",
      "Epoch 9/300\n",
      "Average training loss: 0.043510262270768485\n",
      "Average test loss: 0.013047753492163288\n",
      "Epoch 10/300\n",
      "Average training loss: 0.041275400545861984\n",
      "Average test loss: 0.00526959528028965\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03898616581492954\n",
      "Average test loss: 0.005505546124445067\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03765705011619462\n",
      "Average test loss: 0.004776649521870745\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03594002071022987\n",
      "Average test loss: 0.005113983333938652\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03467290458579858\n",
      "Average test loss: 0.004539058590928713\n",
      "Epoch 15/300\n",
      "Average training loss: 0.033285471664534674\n",
      "Average test loss: 0.0049087104830476975\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03213553868399726\n",
      "Average test loss: 0.004219135731044743\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030919370553559727\n",
      "Average test loss: 0.009523399850146637\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030110342708726725\n",
      "Average test loss: 0.0043248847449819244\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02926468266877863\n",
      "Average test loss: 0.0043196681915885875\n",
      "Epoch 20/300\n",
      "Average training loss: 0.028451191602481735\n",
      "Average test loss: 0.003944803005498316\n",
      "Epoch 21/300\n",
      "Average training loss: 0.027894792480601206\n",
      "Average test loss: 0.00413639639918175\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027449374632702934\n",
      "Average test loss: 0.004073548581865098\n",
      "Epoch 23/300\n",
      "Average training loss: 0.026869517212112743\n",
      "Average test loss: 0.0039012066072060003\n",
      "Epoch 24/300\n",
      "Average training loss: 0.026723337231410873\n",
      "Average test loss: 0.003979980824722184\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02601835091908773\n",
      "Average test loss: 0.0040044757862471875\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025749403248230617\n",
      "Average test loss: 0.003680970339311494\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02543859529164102\n",
      "Average test loss: 0.0036410014033317565\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02502461210058795\n",
      "Average test loss: 0.0036456302512023185\n",
      "Epoch 29/300\n",
      "Average training loss: 0.024843024467428525\n",
      "Average test loss: 0.0038676371177037558\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024658201401432354\n",
      "Average test loss: 0.0037083773190776507\n",
      "Epoch 31/300\n",
      "Average training loss: 0.024222033442722425\n",
      "Average test loss: 0.0036583266742527486\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024156295562783876\n",
      "Average test loss: 0.003611534884406461\n",
      "Epoch 33/300\n",
      "Average training loss: 0.023908449485898018\n",
      "Average test loss: 0.0037299674215416113\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0237275722341405\n",
      "Average test loss: 0.003789942680961556\n",
      "Epoch 35/300\n",
      "Average training loss: 0.023623775515291427\n",
      "Average test loss: 0.0035600900403741334\n",
      "Epoch 36/300\n",
      "Average training loss: 0.023366008510192236\n",
      "Average test loss: 0.0035933022076884906\n",
      "Epoch 37/300\n",
      "Average training loss: 0.023253104441695743\n",
      "Average test loss: 0.004898547168407175\n",
      "Epoch 38/300\n",
      "Average training loss: 0.023073711479703586\n",
      "Average test loss: 0.003656475270373954\n",
      "Epoch 39/300\n",
      "Average training loss: 0.022993497591879634\n",
      "Average test loss: 0.003920977199243175\n",
      "Epoch 40/300\n",
      "Average training loss: 0.022777197039789622\n",
      "Average test loss: 0.004228257334480683\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02274032122062312\n",
      "Average test loss: 0.0034732406495345965\n",
      "Epoch 42/300\n",
      "Average training loss: 0.022660469030340514\n",
      "Average test loss: 0.0034226444624364375\n",
      "Epoch 43/300\n",
      "Average training loss: 0.022438916479547818\n",
      "Average test loss: 0.0045686076302081345\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02233944653140174\n",
      "Average test loss: 0.0036400389903121525\n",
      "Epoch 45/300\n",
      "Average training loss: 0.022274029307895238\n",
      "Average test loss: 0.003466654579879509\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02219103183514542\n",
      "Average test loss: 0.0036454206763042343\n",
      "Epoch 47/300\n",
      "Average training loss: 0.022104462873604562\n",
      "Average test loss: 0.003543933116313484\n",
      "Epoch 48/300\n",
      "Average training loss: 0.021984940990805626\n",
      "Average test loss: 0.0036283046247230634\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02190699314739969\n",
      "Average test loss: 0.0035398619808256625\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02180284918927484\n",
      "Average test loss: 0.003649432614652647\n",
      "Epoch 51/300\n",
      "Average training loss: 0.021752808180120257\n",
      "Average test loss: 0.0034240165379726224\n",
      "Epoch 52/300\n",
      "Average training loss: 0.021686961556474368\n",
      "Average test loss: 0.0036607170189834302\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02158415762748983\n",
      "Average test loss: 0.0039003469049930573\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02156739400823911\n",
      "Average test loss: 0.0033604911658912896\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02147481783893373\n",
      "Average test loss: 0.00344595832315584\n",
      "Epoch 56/300\n",
      "Average training loss: 0.021366250990165606\n",
      "Average test loss: 0.0036220999918878077\n",
      "Epoch 57/300\n",
      "Average training loss: 0.021280798261364302\n",
      "Average test loss: 0.0036208159248862\n",
      "Epoch 58/300\n",
      "Average training loss: 0.021231150640381707\n",
      "Average test loss: 0.003539711847073502\n",
      "Epoch 59/300\n",
      "Average training loss: 0.021257900670170783\n",
      "Average test loss: 0.0036545896163831153\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021115548885530896\n",
      "Average test loss: 0.003430420693837934\n",
      "Epoch 61/300\n",
      "Average training loss: 0.021087237626314162\n",
      "Average test loss: 0.003606962701926629\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020987296577956942\n",
      "Average test loss: 0.0035716411895636057\n",
      "Epoch 63/300\n",
      "Average training loss: 0.020971568807959556\n",
      "Average test loss: 0.003530855481616325\n",
      "Epoch 64/300\n",
      "Average training loss: 0.020894315340452725\n",
      "Average test loss: 0.003546429182092349\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020862067504061593\n",
      "Average test loss: 0.003357053601907359\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020815892827179695\n",
      "Average test loss: 0.003348322070721123\n",
      "Epoch 67/300\n",
      "Average training loss: 0.020704914157589276\n",
      "Average test loss: 0.0036996182921446032\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020724016959468523\n",
      "Average test loss: 0.003631551030609343\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020644687397612466\n",
      "Average test loss: 0.003534782545020183\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020601285515560043\n",
      "Average test loss: 0.003445262166361014\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02056642973754141\n",
      "Average test loss: 0.0036914133317768574\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020518471515840953\n",
      "Average test loss: 0.003469595224907001\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02045147318806913\n",
      "Average test loss: 0.0033560884036123754\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020463965612981055\n",
      "Average test loss: 0.0035004850396265588\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02040982359730535\n",
      "Average test loss: 0.00356165701813168\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02036136328180631\n",
      "Average test loss: 0.0037653453047904704\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020327137279841635\n",
      "Average test loss: 0.0044184024102158015\n",
      "Epoch 78/300\n",
      "Average training loss: 0.020291768550872804\n",
      "Average test loss: 0.003607538998954826\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02025164008471701\n",
      "Average test loss: 0.0036744920242991713\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02017344115343359\n",
      "Average test loss: 0.003475770517355866\n",
      "Epoch 81/300\n",
      "Average training loss: 0.020146841375364197\n",
      "Average test loss: 0.0033727468537787597\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02011019311679734\n",
      "Average test loss: 0.0034362581686841116\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020092555539475546\n",
      "Average test loss: 0.003321502328539888\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020090540770855213\n",
      "Average test loss: 0.0036961190328001978\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02002633179558648\n",
      "Average test loss: 0.0034905292911248074\n",
      "Epoch 86/300\n",
      "Average training loss: 0.020047483041882516\n",
      "Average test loss: 0.0033138521917992166\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019985139353407753\n",
      "Average test loss: 0.0038487549624923205\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01989042733112971\n",
      "Average test loss: 0.0035424101108478176\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019896208399699793\n",
      "Average test loss: 0.0035280319878624546\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019848547375864452\n",
      "Average test loss: 0.003669477451592684\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01986503042611811\n",
      "Average test loss: 0.0034262185340954198\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019808806071678797\n",
      "Average test loss: 0.004399433623585436\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019789668041798804\n",
      "Average test loss: 0.003831381989022096\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019739135675960116\n",
      "Average test loss: 0.003486153643578291\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019696673441264363\n",
      "Average test loss: 0.0035469994441502625\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019711349636316298\n",
      "Average test loss: 0.0036571359588868087\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019656542770564556\n",
      "Average test loss: 0.0035065627760357326\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01960606486433082\n",
      "Average test loss: 0.003309604077289502\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019650528952479363\n",
      "Average test loss: 0.0035238829830454457\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019563319019145434\n",
      "Average test loss: 0.0035209137621439166\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019555534470412465\n",
      "Average test loss: 0.0033786552800900405\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019498445606893962\n",
      "Average test loss: 0.003397009102006753\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019478041264745925\n",
      "Average test loss: 0.003564412545412779\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019477120712399484\n",
      "Average test loss: 0.0038684318189819654\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019458766965402496\n",
      "Average test loss: 0.003589001993338267\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019457312083906598\n",
      "Average test loss: 0.003611640027620726\n",
      "Epoch 107/300\n",
      "Average training loss: 0.019409036980734932\n",
      "Average test loss: 0.004774466230223576\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019388427262504896\n",
      "Average test loss: 0.0033805826985173755\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01941385605600145\n",
      "Average test loss: 0.003453261921596196\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019293445916639434\n",
      "Average test loss: 0.004211903683841229\n",
      "Epoch 111/300\n",
      "Average training loss: 0.019371856505672136\n",
      "Average test loss: 0.0037633711799151368\n",
      "Epoch 112/300\n",
      "Average training loss: 0.019267205738359026\n",
      "Average test loss: 0.0035237136495610078\n",
      "Epoch 113/300\n",
      "Average training loss: 0.019242862141794628\n",
      "Average test loss: 0.003373644640462266\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019252367797825073\n",
      "Average test loss: 0.004035161652912696\n",
      "Epoch 115/300\n",
      "Average training loss: 0.019251042787399558\n",
      "Average test loss: 0.0038977685409287612\n",
      "Epoch 116/300\n",
      "Average training loss: 0.019234089543422064\n",
      "Average test loss: 0.003475514911943012\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019165964977608786\n",
      "Average test loss: 0.0034776305579062965\n",
      "Epoch 118/300\n",
      "Average training loss: 0.019216969387398825\n",
      "Average test loss: 0.0036783912719951734\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01921334613362948\n",
      "Average test loss: 0.00355972374872201\n",
      "Epoch 120/300\n",
      "Average training loss: 0.019112959929638438\n",
      "Average test loss: 0.0036830225413044294\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01911778017381827\n",
      "Average test loss: 0.004862442565461\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01908066654039754\n",
      "Average test loss: 0.003588679759038819\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01907930606438054\n",
      "Average test loss: 0.003621623539262348\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019040921058091853\n",
      "Average test loss: 0.00337676298018131\n",
      "Epoch 125/300\n",
      "Average training loss: 0.019015850954585606\n",
      "Average test loss: 0.003737341120011277\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01897647106481923\n",
      "Average test loss: 0.0034095122578243413\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019009617017375097\n",
      "Average test loss: 0.0034276893863247502\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018932318574852412\n",
      "Average test loss: 0.0036861443236056303\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01893178803390927\n",
      "Average test loss: 0.0035013344950146144\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018929753803544575\n",
      "Average test loss: 0.0035557371537304587\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018928022290269533\n",
      "Average test loss: 0.003490092562511563\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018860545312364897\n",
      "Average test loss: 0.003549953016762932\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01893397942185402\n",
      "Average test loss: 0.003388311262656417\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018880377400252554\n",
      "Average test loss: 0.0034187337344305383\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018834810804989603\n",
      "Average test loss: 0.0034599234093394544\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018851341030663913\n",
      "Average test loss: 0.003416134241968393\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018888958773679205\n",
      "Average test loss: 0.003528243345518907\n",
      "Epoch 138/300\n",
      "Average training loss: 0.018826630213194424\n",
      "Average test loss: 0.0033499615627030532\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01877552074359523\n",
      "Average test loss: 0.0035052561379141276\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01873739916914039\n",
      "Average test loss: 0.0037065049049754936\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018748670898377895\n",
      "Average test loss: 0.003567148803008927\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01876009348862701\n",
      "Average test loss: 0.0034310559013651476\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018684330500662327\n",
      "Average test loss: 0.0034929695495714745\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018681727414329847\n",
      "Average test loss: 0.0034151723720133305\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01867429068022304\n",
      "Average test loss: 0.003500890388463934\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01868723903099696\n",
      "Average test loss: 0.003490721817645762\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018638820520705646\n",
      "Average test loss: 0.0034077960732910367\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018687170346577964\n",
      "Average test loss: 0.003931081730458472\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018612690881722502\n",
      "Average test loss: 0.0034795462290445963\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018638801864451833\n",
      "Average test loss: 0.003673330310318205\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01862635761582189\n",
      "Average test loss: 0.003703526582982805\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01858186961048179\n",
      "Average test loss: 0.0034567831359389755\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018531477419866455\n",
      "Average test loss: 0.004187131484142608\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01859592437495788\n",
      "Average test loss: 0.0035653235130012036\n",
      "Epoch 155/300\n",
      "Average training loss: 0.018525578770372603\n",
      "Average test loss: 0.003479426797893312\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018568657283981643\n",
      "Average test loss: 0.003461566822189424\n",
      "Epoch 157/300\n",
      "Average training loss: 0.018513775423169135\n",
      "Average test loss: 0.0033910438434945215\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018482580714755587\n",
      "Average test loss: 0.004312898916088872\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018475753085480797\n",
      "Average test loss: 0.003537974499579933\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018516526603036456\n",
      "Average test loss: 0.0036473373555474813\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018446229295598138\n",
      "Average test loss: 0.003438485306998094\n",
      "Epoch 162/300\n",
      "Average training loss: 0.018469506760438282\n",
      "Average test loss: 0.0037280554121567142\n",
      "Epoch 163/300\n",
      "Average training loss: 0.018509672597050667\n",
      "Average test loss: 0.0035428983693321548\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018447384908795356\n",
      "Average test loss: 0.0037791484350131617\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01844793804983298\n",
      "Average test loss: 0.0035093228763176335\n",
      "Epoch 166/300\n",
      "Average training loss: 0.018345070087247426\n",
      "Average test loss: 0.0034470939193334844\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01840927333964242\n",
      "Average test loss: 0.004301854081865814\n",
      "Epoch 168/300\n",
      "Average training loss: 0.018431488709317313\n",
      "Average test loss: 0.004469059963607126\n",
      "Epoch 169/300\n",
      "Average training loss: 0.018352076705131264\n",
      "Average test loss: 0.003599615718962418\n",
      "Epoch 170/300\n",
      "Average training loss: 0.018365635487768386\n",
      "Average test loss: 0.003628998695562283\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01837658888846636\n",
      "Average test loss: 0.003579240729411443\n",
      "Epoch 172/300\n",
      "Average training loss: 0.018361326595975294\n",
      "Average test loss: 0.00363680692244735\n",
      "Epoch 173/300\n",
      "Average training loss: 0.018338197499513625\n",
      "Average test loss: 0.0036724840555753973\n",
      "Epoch 174/300\n",
      "Average training loss: 0.018330789387226105\n",
      "Average test loss: 0.0034999262976149717\n",
      "Epoch 175/300\n",
      "Average training loss: 0.018325066725413005\n",
      "Average test loss: 0.0035123807589213054\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01826435130669011\n",
      "Average test loss: 0.0036276778998888203\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018245981920096608\n",
      "Average test loss: 0.0035263633434143332\n",
      "Epoch 178/300\n",
      "Average training loss: 0.018258265636033482\n",
      "Average test loss: 0.0034428281758187547\n",
      "Epoch 179/300\n",
      "Average training loss: 0.018310194825132686\n",
      "Average test loss: 0.003660649199452665\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01822353221144941\n",
      "Average test loss: 0.003689542407169938\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01823486382431454\n",
      "Average test loss: 0.0035009194066127143\n",
      "Epoch 182/300\n",
      "Average training loss: 0.018200313037468326\n",
      "Average test loss: 0.004434048995375633\n",
      "Epoch 183/300\n",
      "Average training loss: 0.018288038078281613\n",
      "Average test loss: 0.003531272707300054\n",
      "Epoch 184/300\n",
      "Average training loss: 0.018199161685175366\n",
      "Average test loss: 0.0037339513699213662\n",
      "Epoch 185/300\n",
      "Average training loss: 0.018191651024752193\n",
      "Average test loss: 0.003548728713972701\n",
      "Epoch 186/300\n",
      "Average training loss: 0.018228555493884618\n",
      "Average test loss: 0.0035779341264731354\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01817114788459407\n",
      "Average test loss: 0.0035203921780404116\n",
      "Epoch 188/300\n",
      "Average training loss: 0.018157613908251127\n",
      "Average test loss: 0.00358860484800405\n",
      "Epoch 189/300\n",
      "Average training loss: 0.018152611005637382\n",
      "Average test loss: 0.0035542254127148127\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018178569352461232\n",
      "Average test loss: 0.003698644341280063\n",
      "Epoch 191/300\n",
      "Average training loss: 0.018117165432208114\n",
      "Average test loss: 0.0038570270612835883\n",
      "Epoch 192/300\n",
      "Average training loss: 0.018119697277744613\n",
      "Average test loss: 0.0036445251812951434\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018094875782728195\n",
      "Average test loss: 0.0035543638478136726\n",
      "Epoch 194/300\n",
      "Average training loss: 0.018087052275737127\n",
      "Average test loss: 0.003695275108019511\n",
      "Epoch 195/300\n",
      "Average training loss: 0.018129234772589473\n",
      "Average test loss: 0.003513670345147451\n",
      "Epoch 196/300\n",
      "Average training loss: 0.018084441372089914\n",
      "Average test loss: 0.0035068439708815682\n",
      "Epoch 197/300\n",
      "Average training loss: 0.018117645409372118\n",
      "Average test loss: 0.003548602511278457\n",
      "Epoch 198/300\n",
      "Average training loss: 0.018072666433122424\n",
      "Average test loss: 0.003486725273438626\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01807322189791335\n",
      "Average test loss: 0.003505258835852146\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018034912953774133\n",
      "Average test loss: 0.003934219517020715\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01801854833298259\n",
      "Average test loss: 0.0035590667805323996\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01804663863364193\n",
      "Average test loss: 0.0039566051293578415\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01803893654545148\n",
      "Average test loss: 0.0035543396644708183\n",
      "Epoch 204/300\n",
      "Average training loss: 0.017984106194641854\n",
      "Average test loss: 0.003525958542815513\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01803346726960606\n",
      "Average test loss: 0.0034621545618606937\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017987713193727863\n",
      "Average test loss: 0.0038256365160147347\n",
      "Epoch 207/300\n",
      "Average training loss: 0.018013637617230415\n",
      "Average test loss: 0.0035812239723487034\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01797261916928821\n",
      "Average test loss: 0.003710656378625168\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01791982337584098\n",
      "Average test loss: 0.0036943555358383393\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017935956633753247\n",
      "Average test loss: 0.0036383939219845666\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017909553558462195\n",
      "Average test loss: 0.0036725099306139683\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01793620405760076\n",
      "Average test loss: 0.003703481183283859\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01791207924981912\n",
      "Average test loss: 0.003926577720584141\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017928597137331963\n",
      "Average test loss: 0.0035566342630320123\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017909557349152037\n",
      "Average test loss: 0.0035237771305772995\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01787751117017534\n",
      "Average test loss: 0.0037766236290335655\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01791268734716707\n",
      "Average test loss: 0.003758127819539772\n",
      "Epoch 218/300\n",
      "Average training loss: 0.017898223895165655\n",
      "Average test loss: 0.003998481864316596\n",
      "Epoch 219/300\n",
      "Average training loss: 0.017879605740308762\n",
      "Average test loss: 0.003608160888362262\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01785468850367599\n",
      "Average test loss: 0.0034582470988647807\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017876488433943856\n",
      "Average test loss: 0.004096083116614156\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017832252433730496\n",
      "Average test loss: 0.0036103924084454776\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017923564164174927\n",
      "Average test loss: 0.0035491504126952756\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01779949605630504\n",
      "Average test loss: 0.00394673510765036\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017824728647867837\n",
      "Average test loss: 0.0035895196737514602\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01780872316658497\n",
      "Average test loss: 0.003635251082893875\n",
      "Epoch 227/300\n",
      "Average training loss: 0.017869757884078557\n",
      "Average test loss: 0.00349760470373763\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017795442306333117\n",
      "Average test loss: 0.004486339604068134\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017845012853542962\n",
      "Average test loss: 0.0035409970002041923\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017808347154822615\n",
      "Average test loss: 0.0035643639891511864\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017821821317076682\n",
      "Average test loss: 0.003831228529827462\n",
      "Epoch 232/300\n",
      "Average training loss: 0.017792778266800775\n",
      "Average test loss: 0.0035362678210561476\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01774844410518805\n",
      "Average test loss: 0.004011774345404572\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017775507014658715\n",
      "Average test loss: 0.0035357740129240687\n",
      "Epoch 235/300\n",
      "Average training loss: 0.017729038978616395\n",
      "Average test loss: 0.0035935194260544247\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017804791680640645\n",
      "Average test loss: 0.0037635198839836652\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017730794366863038\n",
      "Average test loss: 0.0036835972285932965\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017727266730533705\n",
      "Average test loss: 0.0036858209137701327\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01769148396452268\n",
      "Average test loss: 0.003555968165811565\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017727180934614607\n",
      "Average test loss: 0.003566275064729982\n",
      "Epoch 241/300\n",
      "Average training loss: 0.017713267074690925\n",
      "Average test loss: 0.003644583564458622\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017739346886674563\n",
      "Average test loss: 0.003609056956652138\n",
      "Epoch 243/300\n",
      "Average training loss: 0.017737073812219832\n",
      "Average test loss: 0.0035680563863780762\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01770116699486971\n",
      "Average test loss: 0.003676105573773384\n",
      "Epoch 245/300\n",
      "Average training loss: 0.017724223665065236\n",
      "Average test loss: 0.0035487445572184194\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017683469068672922\n",
      "Average test loss: 0.003529280794577466\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01771513850490252\n",
      "Average test loss: 0.003516829636775785\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01765994999309381\n",
      "Average test loss: 0.0035575212981137964\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017649181659022968\n",
      "Average test loss: 0.003798966774923934\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017672933745715352\n",
      "Average test loss: 0.0037959475181996824\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01768753645486302\n",
      "Average test loss: 0.00376638852722115\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01759329552286201\n",
      "Average test loss: 0.0036635809544887806\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01763110461499956\n",
      "Average test loss: 0.003654354120915135\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01764707780381044\n",
      "Average test loss: 0.0036016332544386385\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017619014549586507\n",
      "Average test loss: 0.0037083008339007695\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017668479196727274\n",
      "Average test loss: 0.003667867390024993\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01762676255653302\n",
      "Average test loss: 0.0036097914901458555\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017609189440806708\n",
      "Average test loss: 0.003666593398277958\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017634217744072277\n",
      "Average test loss: 0.0036111106926368344\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01760255398849646\n",
      "Average test loss: 0.0037075133711720505\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017561386444502405\n",
      "Average test loss: 0.0036675631776452063\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017626450556847785\n",
      "Average test loss: 0.0037899674624204635\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01753889948460791\n",
      "Average test loss: 0.003564064462772674\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017580991978446644\n",
      "Average test loss: 0.003611215983207027\n",
      "Epoch 265/300\n",
      "Average training loss: 0.017576152365240787\n",
      "Average test loss: 0.0036853065140959288\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01758269964158535\n",
      "Average test loss: 0.0036813079688905012\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017549457702371807\n",
      "Average test loss: 0.0038453839015629558\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017587610070904095\n",
      "Average test loss: 0.0035147433028452925\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017546488637725514\n",
      "Average test loss: 0.00379452459845278\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01750177280770408\n",
      "Average test loss: 0.0038934123052491083\n",
      "Epoch 271/300\n",
      "Average training loss: 0.017538785950177247\n",
      "Average test loss: 0.0035732818218982883\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01751205074538787\n",
      "Average test loss: 0.003703192230107056\n",
      "Epoch 273/300\n",
      "Average training loss: 0.017521587368514804\n",
      "Average test loss: 0.0034836443008648024\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0174886855011185\n",
      "Average test loss: 0.0038362681818091206\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017487665100230112\n",
      "Average test loss: 0.003782865472551849\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01753857564760579\n",
      "Average test loss: 0.0036218852549791338\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017452063656515544\n",
      "Average test loss: 0.0038980501666665075\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017552974195943937\n",
      "Average test loss: 0.003525241274179684\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017508060933815107\n",
      "Average test loss: 0.0036909586396068335\n",
      "Epoch 280/300\n",
      "Average training loss: 0.017424570004145306\n",
      "Average test loss: 0.003549805542661084\n",
      "Epoch 281/300\n",
      "Average training loss: 0.017512293823063373\n",
      "Average test loss: 0.0035386272689534558\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01743025688992606\n",
      "Average test loss: 0.003645619235932827\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01746143803662724\n",
      "Average test loss: 0.0035710560280001824\n",
      "Epoch 284/300\n",
      "Average training loss: 0.017485533388952413\n",
      "Average test loss: 0.0035991380566524137\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01750400366385778\n",
      "Average test loss: 0.0036452278184394044\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017429370487729707\n",
      "Average test loss: 0.0035389193064636654\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017441128980782296\n",
      "Average test loss: 0.003611870683522688\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017453871603641244\n",
      "Average test loss: 0.0035727780753125747\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017425981983542442\n",
      "Average test loss: 0.003732181662176218\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017410093620419502\n",
      "Average test loss: 0.0036268910980886884\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01744024388326539\n",
      "Average test loss: 0.0037603979013446304\n",
      "Epoch 292/300\n",
      "Average training loss: 0.017445661008358003\n",
      "Average test loss: 0.0036002028348545234\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017382671577235064\n",
      "Average test loss: 0.0036098807342350484\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017381146528654627\n",
      "Average test loss: 0.003641148460408052\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017435721604360476\n",
      "Average test loss: 0.0037116600357823904\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01740023694601324\n",
      "Average test loss: 0.0037114567262017065\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017417776323854925\n",
      "Average test loss: 0.003602721493277285\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017358386054635046\n",
      "Average test loss: 0.0036427510815362136\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017389938477012847\n",
      "Average test loss: 0.003925038520660665\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01739412265188164\n",
      "Average test loss: 0.0036295890129274793\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2521060640083419\n",
      "Average test loss: 0.007567714985460043\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08817889940738678\n",
      "Average test loss: 0.00578735255698363\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06665960929128859\n",
      "Average test loss: 0.008876294990380606\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05562956910663181\n",
      "Average test loss: 0.0047843487200637655\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04880267650882403\n",
      "Average test loss: 0.004591038039161099\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04458608800172806\n",
      "Average test loss: 0.008580067752963967\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04068669977784157\n",
      "Average test loss: 0.0039645270856304305\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03833275339669651\n",
      "Average test loss: 0.0041643425905042225\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03616198842061891\n",
      "Average test loss: 0.0038699468970298766\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0341903427210119\n",
      "Average test loss: 0.004613171237831315\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03198275124529998\n",
      "Average test loss: 0.0034964713296956485\n",
      "Epoch 12/300\n",
      "Average training loss: 0.030873362864057223\n",
      "Average test loss: 0.008250476964645915\n",
      "Epoch 13/300\n",
      "Average training loss: 0.029277801937527128\n",
      "Average test loss: 0.0034027087483555077\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02817390827006764\n",
      "Average test loss: 0.0034058131385180687\n",
      "Epoch 15/300\n",
      "Average training loss: 0.026826843471990693\n",
      "Average test loss: 0.003252643833764725\n",
      "Epoch 16/300\n",
      "Average training loss: 0.025849829879071976\n",
      "Average test loss: 0.0029487059163964455\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02486561275025209\n",
      "Average test loss: 0.0032033118131673997\n",
      "Epoch 18/300\n",
      "Average training loss: 0.023950977570480772\n",
      "Average test loss: 0.002715644824835989\n",
      "Epoch 19/300\n",
      "Average training loss: 0.023305332478549746\n",
      "Average test loss: 0.0029744043151537576\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022494513202044698\n",
      "Average test loss: 0.002896057146290938\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02210108307003975\n",
      "Average test loss: 0.0028678625000433788\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0216290871385071\n",
      "Average test loss: 0.003062452830270761\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021213541626930236\n",
      "Average test loss: 0.005924778312858608\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02098415734867255\n",
      "Average test loss: 0.002781215983132521\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020556165337562562\n",
      "Average test loss: 0.0024876647653679054\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02021466501388285\n",
      "Average test loss: 0.0025625425392968785\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01998385056356589\n",
      "Average test loss: 0.002454998429140283\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01968138370083438\n",
      "Average test loss: 0.002621615077058474\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019536106298367183\n",
      "Average test loss: 0.0024065904507620467\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019238265479604402\n",
      "Average test loss: 0.002363369958682193\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019133686244487764\n",
      "Average test loss: 0.0025072895327789917\n",
      "Epoch 32/300\n",
      "Average training loss: 0.018975630203882852\n",
      "Average test loss: 0.00277551375515759\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018689461338851188\n",
      "Average test loss: 0.002603181851096451\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018562719386484888\n",
      "Average test loss: 0.0024727451987564563\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018460476123624378\n",
      "Average test loss: 0.0024906838398633733\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018339373572005165\n",
      "Average test loss: 0.002425069816203581\n",
      "Epoch 37/300\n",
      "Average training loss: 0.018260578806201615\n",
      "Average test loss: 0.002379630166830288\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018097104157010713\n",
      "Average test loss: 0.002355774549767375\n",
      "Epoch 39/300\n",
      "Average training loss: 0.017995771918031905\n",
      "Average test loss: 0.002401813075049884\n",
      "Epoch 40/300\n",
      "Average training loss: 0.017917756019367112\n",
      "Average test loss: 0.002592246703596579\n",
      "Epoch 41/300\n",
      "Average training loss: 0.017869324284295243\n",
      "Average test loss: 0.0023735499432724384\n",
      "Epoch 42/300\n",
      "Average training loss: 0.017701313799454107\n",
      "Average test loss: 0.0026875160994629067\n",
      "Epoch 43/300\n",
      "Average training loss: 0.017576360682646435\n",
      "Average test loss: 0.003189492342165775\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017560090394483672\n",
      "Average test loss: 0.0023052682551658816\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01743958419561386\n",
      "Average test loss: 0.002411978472645084\n",
      "Epoch 46/300\n",
      "Average training loss: 0.017426697584489982\n",
      "Average test loss: 0.0023046050666727953\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01726419773366716\n",
      "Average test loss: 0.0022466572038829327\n",
      "Epoch 48/300\n",
      "Average training loss: 0.017232631537649368\n",
      "Average test loss: 0.002558973023460971\n",
      "Epoch 49/300\n",
      "Average training loss: 0.017170978277921675\n",
      "Average test loss: 0.0022617710628029374\n",
      "Epoch 50/300\n",
      "Average training loss: 0.017105345089402462\n",
      "Average test loss: 0.002267436043255859\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01704686019404067\n",
      "Average test loss: 0.002330778776564532\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01698110531270504\n",
      "Average test loss: 0.0022833788136227265\n",
      "Epoch 53/300\n",
      "Average training loss: 0.016920825966530377\n",
      "Average test loss: 0.002235787993400461\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01680885568757852\n",
      "Average test loss: 0.0023856945490050648\n",
      "Epoch 55/300\n",
      "Average training loss: 0.016825824600127007\n",
      "Average test loss: 0.0029737208789835374\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01675852682524257\n",
      "Average test loss: 0.0023009125048087703\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0166851271862785\n",
      "Average test loss: 0.002361861046610607\n",
      "Epoch 58/300\n",
      "Average training loss: 0.016669595571027863\n",
      "Average test loss: 0.0026107068020436498\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0165442420343558\n",
      "Average test loss: 0.002243619113229215\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016565386929445796\n",
      "Average test loss: 0.0023568575026260484\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01656488274037838\n",
      "Average test loss: 0.0038812893198596106\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016491846727000343\n",
      "Average test loss: 0.00224232045478291\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016406439325875707\n",
      "Average test loss: 0.0022286765488485497\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01636376809908284\n",
      "Average test loss: 0.002796969403823217\n",
      "Epoch 65/300\n",
      "Average training loss: 0.016326185114681722\n",
      "Average test loss: 0.0022125327467090554\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016352074165311124\n",
      "Average test loss: 0.0024532326619244285\n",
      "Epoch 67/300\n",
      "Average training loss: 0.016253261983394624\n",
      "Average test loss: 0.0022513584587723016\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016188958699504534\n",
      "Average test loss: 0.002526636650785804\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016208850779467158\n",
      "Average test loss: 0.0022247033005373344\n",
      "Epoch 70/300\n",
      "Average training loss: 0.016085518040590815\n",
      "Average test loss: 0.0024510221117072634\n",
      "Epoch 71/300\n",
      "Average training loss: 0.016101711804668108\n",
      "Average test loss: 0.002255884742571248\n",
      "Epoch 72/300\n",
      "Average training loss: 0.016031205910775396\n",
      "Average test loss: 0.00220844493382093\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016016512687835428\n",
      "Average test loss: 0.002281092688855198\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016007126554846762\n",
      "Average test loss: 0.0023345650074382625\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01602397721178002\n",
      "Average test loss: 0.002321692730093168\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01590991220043765\n",
      "Average test loss: 0.0025331467130324907\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01589271055161953\n",
      "Average test loss: 0.0022469244173003566\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01587359800686439\n",
      "Average test loss: 0.0023321618181135918\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01584028218852149\n",
      "Average test loss: 0.002295618397080236\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015768653022746246\n",
      "Average test loss: 0.002256399127551251\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015778972402215003\n",
      "Average test loss: 0.002326330225707756\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01575390273001459\n",
      "Average test loss: 0.002319762506729199\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015687952269282605\n",
      "Average test loss: 0.002296364411297772\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01562761828634474\n",
      "Average test loss: 0.002205193435359332\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015628404612342515\n",
      "Average test loss: 0.0022506204201943342\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015629515796071955\n",
      "Average test loss: 0.0022766480938427977\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015607003781530591\n",
      "Average test loss: 0.002239335564068622\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015578009902603097\n",
      "Average test loss: 0.00219827926510738\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015562271384729279\n",
      "Average test loss: 0.002202745230247577\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015508925082782904\n",
      "Average test loss: 0.0022430770635190936\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015440730690956115\n",
      "Average test loss: 0.0022748973610707455\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015481910894314449\n",
      "Average test loss: 0.0024627051804628638\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015455759853124619\n",
      "Average test loss: 0.002348154965167244\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015432468011147445\n",
      "Average test loss: 0.0028010997047854796\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015329956183830897\n",
      "Average test loss: 0.0022419037653340235\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01536779945757654\n",
      "Average test loss: 0.0023143242249886193\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015323234594530529\n",
      "Average test loss: 0.002230854853573773\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015355430744588375\n",
      "Average test loss: 0.002709047181117866\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01530846570763323\n",
      "Average test loss: 0.0022997766433076727\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01523879425227642\n",
      "Average test loss: 0.0023980104668686786\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015260910605390867\n",
      "Average test loss: 0.002239199029488696\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015214484786821737\n",
      "Average test loss: 0.0022204878299186626\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015239721979531978\n",
      "Average test loss: 0.002299605213209159\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015174231014317936\n",
      "Average test loss: 0.002220946870330307\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015129309405883154\n",
      "Average test loss: 0.0022115249401993223\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01515667222854164\n",
      "Average test loss: 0.0022108410658935704\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01512733970499701\n",
      "Average test loss: 0.0022921763537451624\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015114766129189067\n",
      "Average test loss: 0.002281460132035944\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015074136936002307\n",
      "Average test loss: 0.002204306427596344\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015043620108730263\n",
      "Average test loss: 0.0022570695193070505\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0150264450179206\n",
      "Average test loss: 0.0024427881665113897\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015032018951243824\n",
      "Average test loss: 0.0022353684294761883\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014985798776977591\n",
      "Average test loss: 0.002464046232816246\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014998521318038305\n",
      "Average test loss: 0.002400089379400015\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014982029111021094\n",
      "Average test loss: 0.002274672935716808\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014908588350647026\n",
      "Average test loss: 0.0022518197627117236\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014911418928040399\n",
      "Average test loss: 0.0023398224338889123\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01490166455010573\n",
      "Average test loss: 0.002234649270359013\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014885243346293767\n",
      "Average test loss: 0.0022957573886960743\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014887617663376861\n",
      "Average test loss: 0.002307164755132463\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014863203297886584\n",
      "Average test loss: 0.0022577048918853205\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014803361955616209\n",
      "Average test loss: 0.0023250346989888283\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014842431413630645\n",
      "Average test loss: 0.002299108716762728\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014826269573635526\n",
      "Average test loss: 0.0022416123130048313\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014750448846982586\n",
      "Average test loss: 0.002330308220245772\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014786622852087022\n",
      "Average test loss: 0.0023149533717789585\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014744344939788183\n",
      "Average test loss: 0.002230440672073099\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014721494793891907\n",
      "Average test loss: 0.0023960710875689985\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01472567668557167\n",
      "Average test loss: 0.00235495367522041\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014722519409325388\n",
      "Average test loss: 0.002276877539542814\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014692950263619423\n",
      "Average test loss: 0.002235294254600174\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014645288737283813\n",
      "Average test loss: 0.002289370134472847\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014643532605634795\n",
      "Average test loss: 0.0022987974969049295\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014647272566954295\n",
      "Average test loss: 0.002748874533093638\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014680915632181697\n",
      "Average test loss: 0.0023376604087857735\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014623921489550008\n",
      "Average test loss: 0.002316804097137517\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014606158322758144\n",
      "Average test loss: 0.0023380715707316995\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014602151261435615\n",
      "Average test loss: 0.002529073411391841\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014562826672361957\n",
      "Average test loss: 0.0022980098033116925\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014576043270942238\n",
      "Average test loss: 0.0025844834900150696\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014573106918070051\n",
      "Average test loss: 0.0023252406832244663\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014556467182106441\n",
      "Average test loss: 0.002280034107259578\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014557841262883611\n",
      "Average test loss: 0.0022071053538885383\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014502700651685397\n",
      "Average test loss: 0.0023328034484552014\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014498283925983641\n",
      "Average test loss: 0.0023095933836367395\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014500421322882175\n",
      "Average test loss: 0.0022660694861163697\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01448217896454864\n",
      "Average test loss: 0.0025034163975053364\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014450943815211455\n",
      "Average test loss: 0.0023855655580345128\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014487328063282701\n",
      "Average test loss: 0.0028785455282777546\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014418889658318626\n",
      "Average test loss: 0.002311634089073373\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014441743893755807\n",
      "Average test loss: 0.0025720882866945532\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014419900231891209\n",
      "Average test loss: 0.004881333315331075\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015099936404161983\n",
      "Average test loss: 0.0023471448154499135\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014363503790564008\n",
      "Average test loss: 0.0024528802845420108\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01437531170497338\n",
      "Average test loss: 0.0022752633260356057\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0143435627453857\n",
      "Average test loss: 0.0023023061064175432\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014303071748051378\n",
      "Average test loss: 0.002382602755808168\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014352300329340829\n",
      "Average test loss: 0.002392913695424795\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014346250535713301\n",
      "Average test loss: 0.002263940511478318\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014328910223311848\n",
      "Average test loss: 0.002483877792850965\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01432032390559713\n",
      "Average test loss: 0.0024019659149150056\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014323495767182774\n",
      "Average test loss: 0.0022789903587351243\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014307043571439054\n",
      "Average test loss: 0.002331093035534852\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01428390224195189\n",
      "Average test loss: 0.002712045912113455\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014287070165077844\n",
      "Average test loss: 0.0023054743334650993\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01426246794892682\n",
      "Average test loss: 0.002297763960953388\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01429572287450234\n",
      "Average test loss: 0.002363778212107718\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014271940141916274\n",
      "Average test loss: 0.014363262120220396\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03648719821208053\n",
      "Average test loss: 0.0029943050266140036\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022231813247005145\n",
      "Average test loss: 0.002396207186910841\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01939249812397692\n",
      "Average test loss: 0.0023123149344076712\n",
      "Epoch 172/300\n",
      "Average training loss: 0.018015957221388816\n",
      "Average test loss: 0.002306723318166203\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01710293712715308\n",
      "Average test loss: 0.002274879546629058\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01671851432075103\n",
      "Average test loss: 0.00227993936650455\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016026627898216246\n",
      "Average test loss: 0.0022560959669450918\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015852634514371553\n",
      "Average test loss: 0.00231226740963757\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016076702546742226\n",
      "Average test loss: 0.002239745091336469\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015505795181625419\n",
      "Average test loss: 0.002399000785003106\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015329493545823627\n",
      "Average test loss: 0.0023010158966191943\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015308237429294321\n",
      "Average test loss: 0.0023413310910885532\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015103907605012257\n",
      "Average test loss: 0.0023060512505471706\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015221982268823517\n",
      "Average test loss: 0.002471873987466097\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015004497718479897\n",
      "Average test loss: 0.002546361556276679\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01498769370549255\n",
      "Average test loss: 0.0024023187164631155\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014994863883488708\n",
      "Average test loss: 0.0024599027110056744\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014947500515315269\n",
      "Average test loss: 0.0022818046870330968\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014839516336719195\n",
      "Average test loss: 0.0024590485021471977\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014754962682723999\n",
      "Average test loss: 0.002831636172408859\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01499813558657964\n",
      "Average test loss: 0.0024341203032268418\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014808838388158215\n",
      "Average test loss: 0.0022758580915008984\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014719246851073372\n",
      "Average test loss: 0.0023885217145499255\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014777081448170875\n",
      "Average test loss: 0.0023392043174761865\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014647662171887028\n",
      "Average test loss: 0.002299676679695646\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01467033909757932\n",
      "Average test loss: 0.002287963702239924\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01461359587477313\n",
      "Average test loss: 0.002381664258427918\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014627258583903313\n",
      "Average test loss: 0.002368984170154565\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014648943801720937\n",
      "Average test loss: 0.002355046377827724\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014555456105205748\n",
      "Average test loss: 0.002331022059441441\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014546604798071914\n",
      "Average test loss: 0.0023138012360367512\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014495074431101481\n",
      "Average test loss: 0.0023063060282212163\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01459865378588438\n",
      "Average test loss: 0.0023160183392465115\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014475704583028952\n",
      "Average test loss: 0.002316692249228557\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014496552858087751\n",
      "Average test loss: 0.0024360354942166143\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014465559311211108\n",
      "Average test loss: 0.0024034447421630223\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014455797696279155\n",
      "Average test loss: 0.0024324705687661964\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014471862788001695\n",
      "Average test loss: 0.0022576779855622187\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014375803833206495\n",
      "Average test loss: 0.0024487578318350845\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014412808927396933\n",
      "Average test loss: 0.0022575887439565527\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014454159100022582\n",
      "Average test loss: 0.0023673840067866777\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014310474657350117\n",
      "Average test loss: 0.002318556769233611\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014332607405053245\n",
      "Average test loss: 0.002347040287529429\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01438673709332943\n",
      "Average test loss: 0.0022604935094714166\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014304820025132762\n",
      "Average test loss: 0.0023295130818668337\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014322527810931206\n",
      "Average test loss: 0.0024653460522078806\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014359516762197018\n",
      "Average test loss: 0.0023773944514493147\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014223139469822248\n",
      "Average test loss: 0.0026235547955665325\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014220529345174631\n",
      "Average test loss: 0.0023064074232760404\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014319199377463924\n",
      "Average test loss: 0.0023144829931358497\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01429158291220665\n",
      "Average test loss: 0.0029943361293731465\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014175537835392688\n",
      "Average test loss: 0.00244242225587368\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01420754597004917\n",
      "Average test loss: 0.002352132856224974\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01413345323006312\n",
      "Average test loss: 0.0023886998715913956\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014220941028661198\n",
      "Average test loss: 0.002336741245455212\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01419870579491059\n",
      "Average test loss: 0.002365300158245696\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01417457406471173\n",
      "Average test loss: 0.0024006312451221876\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014180611641870604\n",
      "Average test loss: 0.0023496956744541724\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014417021721601486\n",
      "Average test loss: 0.0023995450718535317\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014094656979044278\n",
      "Average test loss: 0.002433758838619623\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014046795680291123\n",
      "Average test loss: 0.002339632268374165\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0141907582498259\n",
      "Average test loss: 0.002468460327014327\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014124240246084002\n",
      "Average test loss: 0.002459499433222744\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014051910124719142\n",
      "Average test loss: 0.0022786660009167263\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014096519239246845\n",
      "Average test loss: 0.002352661464259856\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014039818753798802\n",
      "Average test loss: 0.0023708253771894507\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014156586173507903\n",
      "Average test loss: 0.0023436487830347484\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014128073044949108\n",
      "Average test loss: 0.0022903822970887027\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013973021468354597\n",
      "Average test loss: 0.0023762088717064925\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014060451449619399\n",
      "Average test loss: 0.0023668687597124113\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014004831982983483\n",
      "Average test loss: 0.002565542052189509\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014023873182634513\n",
      "Average test loss: 0.002354018966356913\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013999444055888387\n",
      "Average test loss: 0.0023039157516840433\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014031271119084624\n",
      "Average test loss: 0.0023404224618441528\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013900460842582915\n",
      "Average test loss: 0.002326600204480605\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014032089233398437\n",
      "Average test loss: 0.0023884023690803184\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01402634834084246\n",
      "Average test loss: 0.002554098654952314\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013938064255648188\n",
      "Average test loss: 0.0024502702253974148\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013930789411895805\n",
      "Average test loss: 0.0023331223575191367\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014003946233954694\n",
      "Average test loss: 0.002357271932168967\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013910082077814473\n",
      "Average test loss: 0.002323876516169144\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013953912456830343\n",
      "Average test loss: 0.002750142542852296\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013884963115056355\n",
      "Average test loss: 0.0027428498706883854\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013845774355861876\n",
      "Average test loss: 0.002365766642615199\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013891904352439774\n",
      "Average test loss: 0.0023214623406529428\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013883664973080159\n",
      "Average test loss: 0.002405285521513886\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013980345447858174\n",
      "Average test loss: 0.002336836855444643\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013945091120070882\n",
      "Average test loss: 0.0024391845233945383\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013859123441908094\n",
      "Average test loss: 0.0023944850450174676\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013816898342635896\n",
      "Average test loss: 0.0024512074351724653\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013828260384500028\n",
      "Average test loss: 0.0024558462352595395\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013836212825444009\n",
      "Average test loss: 0.0024185252264142037\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013869518977072504\n",
      "Average test loss: 0.002499457931042545\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013803880945675903\n",
      "Average test loss: 0.0024076083809551267\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013835989312993156\n",
      "Average test loss: 0.0023566710783375633\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013881418177651034\n",
      "Average test loss: 0.002380418419010109\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013764264221820567\n",
      "Average test loss: 0.0023807199402815764\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013865562650064627\n",
      "Average test loss: 0.0023599137475507127\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0238296191294988\n",
      "Average test loss: 0.0026511506349262263\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020084946321116553\n",
      "Average test loss: 0.0023134630988869403\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01728115333451165\n",
      "Average test loss: 0.0022566344315807026\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01608989090886381\n",
      "Average test loss: 0.002396709837226404\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01526446583122015\n",
      "Average test loss: 0.0022398602646879024\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014692472831242614\n",
      "Average test loss: 0.002303055121252934\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014345630338622463\n",
      "Average test loss: 0.0023654860384348365\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01409365540328953\n",
      "Average test loss: 0.002368754378002551\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01398409186800321\n",
      "Average test loss: 0.002349699878237314\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013853281916015679\n",
      "Average test loss: 0.0023742433472846943\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013830107908282015\n",
      "Average test loss: 0.0023243654875291717\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013857838467591338\n",
      "Average test loss: 0.0023215747355586954\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013794554115169578\n",
      "Average test loss: 0.0027561421518524486\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013771327769590749\n",
      "Average test loss: 0.0023700685083038278\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013747697511480915\n",
      "Average test loss: 0.0023759650370726985\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01375701633012957\n",
      "Average test loss: 0.002416594923370414\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013749891066716776\n",
      "Average test loss: 0.002341679649324053\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013709201220836905\n",
      "Average test loss: 0.0027876361277368333\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013698557604518201\n",
      "Average test loss: 0.002427431638869974\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013689019422564242\n",
      "Average test loss: 0.0024944304238177007\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013690057796736559\n",
      "Average test loss: 0.0024671807636817296\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013627086609601975\n",
      "Average test loss: 0.002366532144861089\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013686560944550567\n",
      "Average test loss: 0.0023538234283526738\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013673694168527921\n",
      "Average test loss: 0.0023557006671196886\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013633761850496133\n",
      "Average test loss: 0.0023964860528293583\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013600054453644487\n",
      "Average test loss: 0.002363668565120962\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013615104123950004\n",
      "Average test loss: 0.0023439930573933656\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013611591070062583\n",
      "Average test loss: 0.0024559686570945712\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013625251589549912\n",
      "Average test loss: 0.0023607885556088553\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013600074741575452\n",
      "Average test loss: 0.0024210441267738738\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013573143849770227\n",
      "Average test loss: 0.0024175877860850756\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01355907175110446\n",
      "Average test loss: 0.0025002347172962294\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013590125765237543\n",
      "Average test loss: 0.0024364215381857423\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013561987689799732\n",
      "Average test loss: 0.00248873373410768\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.25070357193549475\n",
      "Average test loss: 0.008922951858904626\n",
      "Epoch 2/300\n",
      "Average training loss: 0.083208947552575\n",
      "Average test loss: 0.012479608164065414\n",
      "Epoch 3/300\n",
      "Average training loss: 0.061225747343566686\n",
      "Average test loss: 0.0039366623059742985\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05055354885591401\n",
      "Average test loss: 0.003529650838010841\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04448993808031082\n",
      "Average test loss: 0.0037639965038332676\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03947529521915648\n",
      "Average test loss: 0.00428097069532507\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03654849620329009\n",
      "Average test loss: 0.0031609348613354893\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03332172723280059\n",
      "Average test loss: 0.010451202451354927\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03093507534596655\n",
      "Average test loss: 0.0028222540101657313\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02964774244858159\n",
      "Average test loss: 0.004868446297943592\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02775372857517666\n",
      "Average test loss: 0.0028817661073472765\n",
      "Epoch 12/300\n",
      "Average training loss: 0.026442772587140402\n",
      "Average test loss: 0.002568249630431334\n",
      "Epoch 13/300\n",
      "Average training loss: 0.025291457013951406\n",
      "Average test loss: 0.009744592557350794\n",
      "Epoch 14/300\n",
      "Average training loss: 0.023830033691393006\n",
      "Average test loss: 0.0024145067052708732\n",
      "Epoch 15/300\n",
      "Average training loss: 0.022944095358252526\n",
      "Average test loss: 0.002284292810700006\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021819866918855242\n",
      "Average test loss: 0.002539661996273531\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021148151559962167\n",
      "Average test loss: 0.004857656065581573\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020164039974411328\n",
      "Average test loss: 0.0022680093343887065\n",
      "Epoch 19/300\n",
      "Average training loss: 0.019563759828607242\n",
      "Average test loss: 0.002037416417048209\n",
      "Epoch 20/300\n",
      "Average training loss: 0.019178155216905805\n",
      "Average test loss: 0.002013395906322532\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018540773236917125\n",
      "Average test loss: 0.0021413388413687546\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018247340836458736\n",
      "Average test loss: 0.0021874910994536347\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017888462408549254\n",
      "Average test loss: 0.0029310781387612223\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017367249305049577\n",
      "Average test loss: 0.00209095272111396\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01714025001724561\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_50_Depth10/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_50_Depth10/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_50_Depth10/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
