{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 15)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.6316093946562873\n",
      "Average test loss: 0.032739553891950185\n",
      "Epoch 2/300\n",
      "Average training loss: 0.25292832107014124\n",
      "Average test loss: 0.01897468139562342\n",
      "Epoch 3/300\n",
      "Average training loss: 0.17799060742060344\n",
      "Average test loss: 0.014897091077433693\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1432752480374442\n",
      "Average test loss: 0.014187427227695783\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1225808853705724\n",
      "Average test loss: 0.01555120323267248\n",
      "Epoch 6/300\n",
      "Average training loss: 0.11023527493079503\n",
      "Average test loss: 0.011236514143645763\n",
      "Epoch 7/300\n",
      "Average training loss: 0.10113757424884372\n",
      "Average test loss: 0.013637902566128308\n",
      "Epoch 8/300\n",
      "Average training loss: 0.09521947863366868\n",
      "Average test loss: 0.0129820703069369\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08892338584529029\n",
      "Average test loss: 0.009628057513799932\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08511175553003947\n",
      "Average test loss: 0.008849266403251224\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08077307615677516\n",
      "Average test loss: 0.023306039315130974\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07776896323098077\n",
      "Average test loss: 0.009634701018532118\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07433347130484051\n",
      "Average test loss: 0.008403338093724516\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07096634296576182\n",
      "Average test loss: 0.04300710973888636\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06784802140792211\n",
      "Average test loss: 0.009466782133612368\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06442928388383654\n",
      "Average test loss: 0.014141707113219631\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06154901279343499\n",
      "Average test loss: 0.008595321635405223\n",
      "Epoch 18/300\n",
      "Average training loss: 0.058754994776513844\n",
      "Average test loss: 0.007818776591370502\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05671675823132197\n",
      "Average test loss: 0.015524842142230934\n",
      "Epoch 20/300\n",
      "Average training loss: 0.054019424650404185\n",
      "Average test loss: 0.009919886883762148\n",
      "Epoch 21/300\n",
      "Average training loss: 0.052002605236238904\n",
      "Average test loss: 0.009411962387048537\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0497970862587293\n",
      "Average test loss: 0.007738267293406857\n",
      "Epoch 23/300\n",
      "Average training loss: 0.048605339258909225\n",
      "Average test loss: 0.008568891999622186\n",
      "Epoch 24/300\n",
      "Average training loss: 0.046949730942646664\n",
      "Average test loss: 0.007978792795704471\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04562130320734448\n",
      "Average test loss: 0.02345235465466976\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04424839249915547\n",
      "Average test loss: 0.0076686906152301365\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04313052424622907\n",
      "Average test loss: 0.006729185215300984\n",
      "Epoch 28/300\n",
      "Average training loss: 0.041851844446526634\n",
      "Average test loss: 0.0065404408342308465\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0410270288321707\n",
      "Average test loss: 0.00657530967808432\n",
      "Epoch 30/300\n",
      "Average training loss: 0.040424405955606035\n",
      "Average test loss: 0.011963051377485196\n",
      "Epoch 31/300\n",
      "Average training loss: 0.039586041745212346\n",
      "Average test loss: 0.007040352169010374\n",
      "Epoch 32/300\n",
      "Average training loss: 0.038800339887539545\n",
      "Average test loss: 0.006704824530416065\n",
      "Epoch 33/300\n",
      "Average training loss: 0.038296363232864276\n",
      "Average test loss: 0.006881959627072017\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03780645704600546\n",
      "Average test loss: 0.006265657812356949\n",
      "Epoch 35/300\n",
      "Average training loss: 0.036928143108884495\n",
      "Average test loss: 0.006353266506559319\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03664062593049473\n",
      "Average test loss: 0.009702775841785801\n",
      "Epoch 37/300\n",
      "Average training loss: 0.036160328520668875\n",
      "Average test loss: 0.08038208648893569\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03559938627150323\n",
      "Average test loss: 0.006441037410249313\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03521552630596691\n",
      "Average test loss: 0.006276005800399515\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03491360472970539\n",
      "Average test loss: 0.027111342013710075\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03445024097793632\n",
      "Average test loss: 0.006121235407061047\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03401880809995863\n",
      "Average test loss: 0.011490053743124009\n",
      "Epoch 43/300\n",
      "Average training loss: 0.033688812133338714\n",
      "Average test loss: 0.006055641152378586\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03351174953248766\n",
      "Average test loss: 0.006391278742088212\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03313393680585755\n",
      "Average test loss: 0.0063907021172344685\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0331928959356414\n",
      "Average test loss: 0.006218622922069496\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03251874506142404\n",
      "Average test loss: 0.0063059182142217955\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03238511537677712\n",
      "Average test loss: 0.00592629162222147\n",
      "Epoch 49/300\n",
      "Average training loss: 0.032163046634859506\n",
      "Average test loss: 0.006047801269011365\n",
      "Epoch 50/300\n",
      "Average training loss: 0.031798713139361806\n",
      "Average test loss: 0.0062388119242257544\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03158921138445536\n",
      "Average test loss: 0.006441905266915758\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03150741185579035\n",
      "Average test loss: 0.005965803627752595\n",
      "Epoch 53/300\n",
      "Average training loss: 0.031285127967596055\n",
      "Average test loss: 0.006229481151534451\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03116685473256641\n",
      "Average test loss: 0.0066126819054285685\n",
      "Epoch 55/300\n",
      "Average training loss: 0.030871727416912714\n",
      "Average test loss: 0.00606520295681225\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030211516754494773\n",
      "Average test loss: 0.0063187419287860395\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030109890242417652\n",
      "Average test loss: 0.006592763777822256\n",
      "Epoch 61/300\n",
      "Average training loss: 0.029998563705219163\n",
      "Average test loss: 0.006196299353407489\n",
      "Epoch 62/300\n",
      "Average training loss: 0.029765326200260057\n",
      "Average test loss: 0.006080090074903435\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02967273810505867\n",
      "Average test loss: 0.006125264738996824\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029674899250268935\n",
      "Average test loss: 0.006832869918396075\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029535742627249825\n",
      "Average test loss: 0.006201631740977367\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02927472505470117\n",
      "Average test loss: 0.006416066978954606\n",
      "Epoch 67/300\n",
      "Average training loss: 0.029293594398432307\n",
      "Average test loss: 0.006460037325405412\n",
      "Epoch 68/300\n",
      "Average training loss: 0.029028521029485596\n",
      "Average test loss: 0.006129244536575344\n",
      "Epoch 69/300\n",
      "Average training loss: 0.028976221652494537\n",
      "Average test loss: 0.006164634406980541\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02890801428920693\n",
      "Average test loss: 0.006246752623054717\n",
      "Epoch 71/300\n",
      "Average training loss: 0.028826956459217602\n",
      "Average test loss: 0.006173164391062326\n",
      "Epoch 72/300\n",
      "Average training loss: 0.028697290567888153\n",
      "Average test loss: 0.007361788519968589\n",
      "Epoch 73/300\n",
      "Average training loss: 0.028654393202728695\n",
      "Average test loss: 0.006455378524959087\n",
      "Epoch 74/300\n",
      "Average training loss: 0.028655748105711407\n",
      "Average test loss: 0.006146058581769466\n",
      "Epoch 75/300\n",
      "Average training loss: 0.028390951512588396\n",
      "Average test loss: 0.006318175346487098\n",
      "Epoch 76/300\n",
      "Average training loss: 0.028372304449478785\n",
      "Average test loss: 0.006045850735571649\n",
      "Epoch 77/300\n",
      "Average training loss: 0.028253520354628563\n",
      "Average test loss: 0.006205483915905158\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02834153454005718\n",
      "Average test loss: 0.006638013454775015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.028129229431351026\n",
      "Average test loss: 0.00660533849356903\n",
      "Epoch 80/300\n",
      "Average training loss: 0.027928323535455598\n",
      "Average test loss: 0.005971902213990688\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02795445042848587\n",
      "Average test loss: 0.006214110363688734\n",
      "Epoch 82/300\n",
      "Average training loss: 0.027830209566487206\n",
      "Average test loss: 0.006052727691001362\n",
      "Epoch 83/300\n",
      "Average training loss: 0.027932980845371883\n",
      "Average test loss: 0.006207768023427989\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02764828829632865\n",
      "Average test loss: 0.006085133846021361\n",
      "Epoch 85/300\n",
      "Average training loss: 0.027448137561480204\n",
      "Average test loss: 0.006309200037270785\n",
      "Epoch 89/300\n",
      "Average training loss: 0.027451708863178888\n",
      "Average test loss: 0.006051792700671487\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02728367594546742\n",
      "Average test loss: 0.007270289033651352\n",
      "Epoch 91/300\n",
      "Average training loss: 0.027230121064517235\n",
      "Average test loss: 0.006485708768169085\n",
      "Epoch 92/300\n",
      "Average training loss: 0.027166431195206113\n",
      "Average test loss: 0.006579068097389407\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02720326393842697\n",
      "Average test loss: 0.006314501221395201\n",
      "Epoch 94/300\n",
      "Average training loss: 0.027045761939552094\n",
      "Average test loss: 0.006013474287258254\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02699816787408458\n",
      "Average test loss: 0.0061314643190966715\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02688995704220401\n",
      "Average test loss: 0.006139777438094219\n",
      "Epoch 97/300\n",
      "Average training loss: 0.026852596590916315\n",
      "Average test loss: 0.006261803630946411\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026907959434721206\n",
      "Average test loss: 0.00610452772759729\n",
      "Epoch 99/300\n",
      "Average training loss: 0.026855374213722017\n",
      "Average test loss: 0.006201115844150384\n",
      "Epoch 100/300\n",
      "Average training loss: 0.026804842768443957\n",
      "Average test loss: 0.00614920967651738\n",
      "Epoch 101/300\n",
      "Average training loss: 0.026669740789466434\n",
      "Average test loss: 0.006100235788358582\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02679888027906418\n",
      "Average test loss: 0.00820587566577726\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02667048723830117\n",
      "Average test loss: 0.006492756560030911\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02655634063647853\n",
      "Average test loss: 0.006137355801545911\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026451183026035628\n",
      "Average test loss: 0.006245643119017283\n",
      "Epoch 106/300\n",
      "Average training loss: 0.026536052635974354\n",
      "Average test loss: 0.0062620213482942845\n",
      "Epoch 107/300\n",
      "Average training loss: 0.026454219079679913\n",
      "Average test loss: 0.006298027041885588\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02638835060265329\n",
      "Average test loss: 0.0060244432364900905\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02625079346034262\n",
      "Average test loss: 0.006397069716619121\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0263002456045813\n",
      "Average test loss: 0.006541072664989366\n",
      "Epoch 111/300\n",
      "Average training loss: 0.026391210238138833\n",
      "Average test loss: 0.006133174136694935\n",
      "Epoch 112/300\n",
      "Average training loss: 0.026146710904108153\n",
      "Average test loss: 0.006240647137578991\n",
      "Epoch 113/300\n",
      "Average training loss: 0.026130556146303812\n",
      "Average test loss: 0.006269329627354939\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02608227779302332\n",
      "Average test loss: 0.006495333025438918\n",
      "Epoch 115/300\n",
      "Average training loss: 0.026066716243823368\n",
      "Average test loss: 0.006588272511959076\n",
      "Epoch 116/300\n",
      "Average training loss: 0.026005468539065785\n",
      "Average test loss: 0.006290488760090537\n",
      "Epoch 117/300\n",
      "Average training loss: 0.026058629494574336\n",
      "Average test loss: 0.006318463253478209\n",
      "Epoch 118/300\n",
      "Average training loss: 0.025866126908196344\n",
      "Average test loss: 0.006073714156531625\n",
      "Epoch 119/300\n",
      "Average training loss: 0.025952254715893002\n",
      "Average test loss: 0.006315819293260574\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02588680181072818\n",
      "Average test loss: 0.00635490793403652\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02578977486656772\n",
      "Average test loss: 0.00608783034566376\n",
      "Epoch 122/300\n",
      "Average training loss: 0.025809898729125657\n",
      "Average test loss: 0.00644038986083534\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02582448036637571\n",
      "Average test loss: 0.006128967236313555\n",
      "Epoch 124/300\n",
      "Average training loss: 0.025700987488031388\n",
      "Average test loss: 0.006082070053451591\n",
      "Epoch 125/300\n",
      "Average training loss: 0.025697335314419534\n",
      "Average test loss: 0.006243650443438027\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02565119136373202\n",
      "Average test loss: 0.006242659579548571\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02561894226902061\n",
      "Average test loss: 0.006251284571571482\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025633663752012783\n",
      "Average test loss: 0.00624366547829575\n",
      "Epoch 129/300\n",
      "Average training loss: 0.025581073997749223\n",
      "Average test loss: 0.006282261205216248\n",
      "Epoch 130/300\n",
      "Average training loss: 0.025568079572584893\n",
      "Average test loss: 0.006099624059266514\n",
      "Epoch 131/300\n",
      "Average training loss: 0.025507297191354965\n",
      "Average test loss: 0.006266819284607966\n",
      "Epoch 132/300\n",
      "Average training loss: 0.025436308052804734\n",
      "Average test loss: 0.006154396246290869\n",
      "Epoch 133/300\n",
      "Average training loss: 0.025361865719159445\n",
      "Average test loss: 0.006378484406405025\n",
      "Epoch 134/300\n",
      "Average training loss: 0.025423112615942954\n",
      "Average test loss: 0.006288395347901516\n",
      "Epoch 135/300\n",
      "Average training loss: 0.025370752551489406\n",
      "Average test loss: 0.0062095077195101316\n",
      "Epoch 136/300\n",
      "Average training loss: 0.025323833598030938\n",
      "Average test loss: 0.006161476218865977\n",
      "Epoch 137/300\n",
      "Average training loss: 0.025191903384195434\n",
      "Average test loss: 0.006338419637746281\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02527775639295578\n",
      "Average test loss: 0.006419492718453209\n",
      "Epoch 139/300\n",
      "Average training loss: 0.025242751341727043\n",
      "Average test loss: 0.007028247218579054\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02524456061753962\n",
      "Average test loss: 0.006207268906964196\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025151745617389677\n",
      "Average test loss: 0.006738451068600019\n",
      "Epoch 142/300\n",
      "Average training loss: 0.025153881311416625\n",
      "Average test loss: 0.006387266272885932\n",
      "Epoch 143/300\n",
      "Average training loss: 0.025061784045563803\n",
      "Average test loss: 0.006275477377490865\n",
      "Epoch 144/300\n",
      "Average training loss: 0.025066953412360614\n",
      "Average test loss: 0.00634962135222223\n",
      "Epoch 145/300\n",
      "Average training loss: 0.025015902901689212\n",
      "Average test loss: 0.006423779389096631\n",
      "Epoch 146/300\n",
      "Average training loss: 0.025026414882805614\n",
      "Average test loss: 0.006356614944835504\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024987691212031576\n",
      "Average test loss: 0.0061884456516967876\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024988697994086477\n",
      "Average test loss: 0.006391026530000898\n",
      "Epoch 149/300\n",
      "Average training loss: 0.024960841697123315\n",
      "Average test loss: 0.006112612384061019\n",
      "Epoch 150/300\n",
      "Average training loss: 0.025002734336588117\n",
      "Average test loss: 0.00710244194087055\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024850583699014453\n",
      "Average test loss: 0.007107311974383063\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02489324281281895\n",
      "Average test loss: 0.006179361694388919\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02479933315018813\n",
      "Average test loss: 0.006049841813743115\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024819281074735855\n",
      "Average test loss: 0.006190705412377913\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024831821648610964\n",
      "Average test loss: 0.00623103775911861\n",
      "Epoch 156/300\n",
      "Average training loss: 0.024706198134356074\n",
      "Average test loss: 0.00647469906301962\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024772173020574783\n",
      "Average test loss: 0.006076978179315726\n",
      "Epoch 158/300\n",
      "Average training loss: 0.024654366945226987\n",
      "Average test loss: 0.006510538929038578\n",
      "Epoch 159/300\n",
      "Average training loss: 0.024716045142875776\n",
      "Average test loss: 0.006085617372559177\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02466261625952191\n",
      "Average test loss: 0.006322884665181239\n",
      "Epoch 161/300\n",
      "Average training loss: 0.024778188101119467\n",
      "Average test loss: 0.006444177843630314\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024494612233506307\n",
      "Average test loss: 0.006648602164040009\n",
      "Epoch 163/300\n",
      "Average training loss: 0.024580349947015443\n",
      "Average test loss: 0.00638471621585389\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02464161141547892\n",
      "Average test loss: 0.0064953319645590256\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024438135251402857\n",
      "Average test loss: 0.0062186895981431005\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02457572414808803\n",
      "Average test loss: 0.006603893724580606\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024537645210822422\n",
      "Average test loss: 0.00627147544009818\n",
      "Epoch 168/300\n",
      "Average training loss: 0.024456057025326623\n",
      "Average test loss: 0.006210971860008107\n",
      "Epoch 169/300\n",
      "Average training loss: 0.024494896055923567\n",
      "Average test loss: 0.006119795177545812\n",
      "Epoch 170/300\n",
      "Average training loss: 0.024430564703212844\n",
      "Average test loss: 0.006210868029958672\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02435893757475747\n",
      "Average test loss: 0.006325587809913688\n",
      "Epoch 172/300\n",
      "Average training loss: 0.024395068226589097\n",
      "Average test loss: 0.006235913401676549\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024374086068736183\n",
      "Average test loss: 0.00674246237716741\n",
      "Epoch 174/300\n",
      "Average training loss: 0.024297722078031964\n",
      "Average test loss: 0.006361066613346338\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0242975035690599\n",
      "Average test loss: 0.00655202685503496\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024337512572606405\n",
      "Average test loss: 0.006284584073556794\n",
      "Epoch 177/300\n",
      "Average training loss: 0.024201687552862696\n",
      "Average test loss: 0.00618288537984093\n",
      "Epoch 178/300\n",
      "Average training loss: 0.024359736329979367\n",
      "Average test loss: 0.006704340302695831\n",
      "Epoch 179/300\n",
      "Average training loss: 0.024301101272304854\n",
      "Average test loss: 0.006382816345741352\n",
      "Epoch 180/300\n",
      "Average training loss: 0.024299436410268146\n",
      "Average test loss: 0.006119967538449499\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02422627963953548\n",
      "Average test loss: 0.006257367356369893\n",
      "Epoch 182/300\n",
      "Average training loss: 0.024167446815305287\n",
      "Average test loss: 0.006365537451787128\n",
      "Epoch 183/300\n",
      "Average training loss: 0.024172290795379216\n",
      "Average test loss: 0.006533937918643156\n",
      "Epoch 184/300\n",
      "Average training loss: 0.024190152575572332\n",
      "Average test loss: 0.00627080083390077\n",
      "Epoch 185/300\n",
      "Average training loss: 0.024071599072880215\n",
      "Average test loss: 0.006471665841423803\n",
      "Epoch 186/300\n",
      "Average training loss: 0.024164660077955986\n",
      "Average test loss: 0.007729279046257337\n",
      "Epoch 187/300\n",
      "Average training loss: 0.024027687127391496\n",
      "Average test loss: 0.006530811566445562\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02405646498501301\n",
      "Average test loss: 0.006306833059837421\n",
      "Epoch 189/300\n",
      "Average training loss: 0.024029057264328003\n",
      "Average test loss: 0.0063016331738068\n",
      "Epoch 190/300\n",
      "Average training loss: 0.024042364550961388\n",
      "Average test loss: 0.006682801672154003\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023937125850054954\n",
      "Average test loss: 0.007098188614679707\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02395949308739768\n",
      "Average test loss: 0.0061709004292885465\n",
      "Epoch 193/300\n",
      "Average training loss: 0.024009221668044724\n",
      "Average test loss: 0.02192237784134017\n",
      "Epoch 194/300\n",
      "Average training loss: 0.024028504078586897\n",
      "Average test loss: 0.0066009760821859045\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023966914274626307\n",
      "Average test loss: 0.006727667999764283\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02399364659852452\n",
      "Average test loss: 0.006818800689859523\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02386706796122922\n",
      "Average test loss: 0.006239766544765896\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023850886105663247\n",
      "Average test loss: 0.006476111714210775\n",
      "Epoch 199/300\n",
      "Average training loss: 0.023908363996280563\n",
      "Average test loss: 0.006229818233185344\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023911497063934802\n",
      "Average test loss: 0.006396805126633909\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02378415037194888\n",
      "Average test loss: 0.007173029555214776\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023854024794366626\n",
      "Average test loss: 0.006446531461758746\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023763843966854943\n",
      "Average test loss: 0.00625648721266124\n",
      "Epoch 204/300\n",
      "Average training loss: 0.023685121999846563\n",
      "Average test loss: 0.006298476024634308\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02377068322400252\n",
      "Average test loss: 0.0063609982894526585\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023850014468034107\n",
      "Average test loss: 0.006621854616536035\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023815928844941988\n",
      "Average test loss: 0.0065741024331914056\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023781951111223963\n",
      "Average test loss: 0.006316673967987299\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023679482266306878\n",
      "Average test loss: 0.006408733316179779\n",
      "Epoch 210/300\n",
      "Average training loss: 0.023669486289223034\n",
      "Average test loss: 0.006441570775376426\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023583441361784935\n",
      "Average test loss: 0.006226754183570544\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023703668736749225\n",
      "Average test loss: 0.006389986385487848\n",
      "Epoch 213/300\n",
      "Average training loss: 0.023689894179503124\n",
      "Average test loss: 0.006304609659231371\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02351565570798185\n",
      "Average test loss: 0.006541849469972981\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0235675692939096\n",
      "Average test loss: 0.0065884503581457666\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023547612342569562\n",
      "Average test loss: 0.0067052935962047845\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0236491736471653\n",
      "Average test loss: 0.00634744547907677\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02355623589290513\n",
      "Average test loss: 0.006184660502605968\n",
      "Epoch 219/300\n",
      "Average training loss: 0.023557825482553908\n",
      "Average test loss: 0.006161960533095731\n",
      "Epoch 220/300\n",
      "Average training loss: 0.023504510455661348\n",
      "Average test loss: 0.006571692580150233\n",
      "Epoch 221/300\n",
      "Average training loss: 0.023513971015810965\n",
      "Average test loss: 0.006604942606555091\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02347841745449437\n",
      "Average test loss: 0.006256429052808219\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02344677980078591\n",
      "Average test loss: 0.006324719873981343\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02354678779674901\n",
      "Average test loss: 0.006485021727987462\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023476129886176852\n",
      "Average test loss: 0.006723996486514807\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023460166845056744\n",
      "Average test loss: 0.006290706055031883\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023462031963798737\n",
      "Average test loss: 0.006376246132370499\n",
      "Epoch 228/300\n",
      "Average training loss: 0.023475102994177075\n",
      "Average test loss: 0.006232709561785062\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023425943147805002\n",
      "Average test loss: 0.006417584187454648\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02338515738149484\n",
      "Average test loss: 0.006282623518672255\n",
      "Epoch 231/300\n",
      "Average training loss: 0.023363015625211928\n",
      "Average test loss: 0.0063588605523109435\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02345431295533975\n",
      "Average test loss: 0.006355937422149711\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02326253385676278\n",
      "Average test loss: 0.006358680863347318\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023303752811418638\n",
      "Average test loss: 0.00655270157671637\n",
      "Epoch 235/300\n",
      "Average training loss: 0.023283242179287806\n",
      "Average test loss: 0.006474885136302975\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023388642928666537\n",
      "Average test loss: 0.006536563630319304\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023265830382704734\n",
      "Average test loss: 0.00635389920282695\n",
      "Epoch 238/300\n",
      "Average training loss: 0.023340295700563323\n",
      "Average test loss: 0.00650327030569315\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023262771841552524\n",
      "Average test loss: 0.006362944898506005\n",
      "Epoch 240/300\n",
      "Average training loss: 0.023269382155603834\n",
      "Average test loss: 0.006621668165756597\n",
      "Epoch 241/300\n",
      "Average training loss: 0.023192682327495683\n",
      "Average test loss: 0.006477007833619913\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02319689653813839\n",
      "Average test loss: 0.006608367414938079\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023247929962144958\n",
      "Average test loss: 0.0063621120535665085\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023241397299700314\n",
      "Average test loss: 0.006441172661880652\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0231021443605423\n",
      "Average test loss: 0.006389904155913327\n",
      "Epoch 246/300\n",
      "Average training loss: 0.023132615557975238\n",
      "Average test loss: 0.006317355766064591\n",
      "Epoch 247/300\n",
      "Average training loss: 0.023194167360663414\n",
      "Average test loss: 0.00697289960210522\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023120245350731743\n",
      "Average test loss: 0.006425643787615829\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023172174571288957\n",
      "Average test loss: 0.030622616877158483\n",
      "Epoch 250/300\n",
      "Average training loss: 0.023092871313293774\n",
      "Average test loss: 0.006296388886455033\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02310463234119945\n",
      "Average test loss: 0.006665233167923159\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02322940398918258\n",
      "Average test loss: 0.00664041748849882\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023083142636550796\n",
      "Average test loss: 0.006540781640344196\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022993344916237723\n",
      "Average test loss: 0.006381563894864586\n",
      "Epoch 255/300\n",
      "Average training loss: 0.023081119906571176\n",
      "Average test loss: 0.006341378902809488\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02304378745290968\n",
      "Average test loss: 0.00638062135833833\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023076517979303997\n",
      "Average test loss: 0.006704008015493552\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02301886604560746\n",
      "Average test loss: 0.006538743845704529\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02300179994602998\n",
      "Average test loss: 0.006643366247415542\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0229683397743437\n",
      "Average test loss: 0.006357411546425687\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02306840336157216\n",
      "Average test loss: 0.006395470557941331\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02292247458630138\n",
      "Average test loss: 0.006555705966634883\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02287407269908322\n",
      "Average test loss: 0.006882146396570736\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02295920065873199\n",
      "Average test loss: 0.00704551519619094\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022991895395848488\n",
      "Average test loss: 0.006643767032358382\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022953403040766717\n",
      "Average test loss: 0.006594651723901431\n",
      "Epoch 267/300\n",
      "Average training loss: 0.022890961724850868\n",
      "Average test loss: 0.006656875575168265\n",
      "Epoch 268/300\n",
      "Average training loss: 0.022903756237692304\n",
      "Average test loss: 0.006426562714493937\n",
      "Epoch 269/300\n",
      "Average training loss: 0.022905013958613076\n",
      "Average test loss: 0.006533083680603239\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022915576598710484\n",
      "Average test loss: 0.006634199899931748\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022899671809540854\n",
      "Average test loss: 0.006303492469506131\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02286575105620755\n",
      "Average test loss: 0.006505006053795418\n",
      "Epoch 273/300\n",
      "Average training loss: 0.022868980977270338\n",
      "Average test loss: 0.006326465300801727\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02284159471425745\n",
      "Average test loss: 0.007210715281466643\n",
      "Epoch 275/300\n",
      "Average training loss: 0.022881672428713903\n",
      "Average test loss: 0.006691345086528196\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02289199402762784\n",
      "Average test loss: 0.0067249298546877175\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022835367381572724\n",
      "Average test loss: 0.006469476772679223\n",
      "Epoch 278/300\n",
      "Average training loss: 0.022798594981431962\n",
      "Average test loss: 0.006364259094413784\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02273745478855239\n",
      "Average test loss: 0.006466141629550192\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022773957532313135\n",
      "Average test loss: 0.0063327702842652794\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02278567848437362\n",
      "Average test loss: 0.006607871576315827\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022822940927412775\n",
      "Average test loss: 0.006648096453812387\n",
      "Epoch 283/300\n",
      "Average training loss: 0.022699620640940138\n",
      "Average test loss: 0.006310113054596715\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022733169111940597\n",
      "Average test loss: 0.006321807005339198\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022714738662044206\n",
      "Average test loss: 0.006357612868563996\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022772196839253106\n",
      "Average test loss: 0.006615142486161656\n",
      "Epoch 287/300\n",
      "Average training loss: 0.022694949169953665\n",
      "Average test loss: 0.006359138362937503\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02271974749863148\n",
      "Average test loss: 0.00639535073356496\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02271794397301144\n",
      "Average test loss: 0.006460627231332991\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022737274959683418\n",
      "Average test loss: 0.006644999778519074\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022679401420884662\n",
      "Average test loss: 0.006307277378936609\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022647370407978692\n",
      "Average test loss: 0.006436862169040574\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02259563043548001\n",
      "Average test loss: 0.006452099283536275\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022642028134730128\n",
      "Average test loss: 0.006359776989453368\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022684212687942715\n",
      "Average test loss: 0.006498808277149995\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02254376030796104\n",
      "Average test loss: 0.006330297984596756\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02254704101383686\n",
      "Average test loss: 0.006855600566085842\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02254691936737961\n",
      "Average test loss: 0.006781938169565465\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02260180158747567\n",
      "Average test loss: 0.006342706140958601\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022544983403550253\n",
      "Average test loss: 0.006416248387760586\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.5811787376138899\n",
      "Average test loss: 0.017823576500018437\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2469573465453254\n",
      "Average test loss: 0.01126618318259716\n",
      "Epoch 3/300\n",
      "Average training loss: 0.18209144539303249\n",
      "Average test loss: 0.010010646083288723\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1480903893576728\n",
      "Average test loss: 0.016628737412393093\n",
      "Epoch 5/300\n",
      "Average training loss: 0.12884977656602858\n",
      "Average test loss: 0.007511975410911772\n",
      "Epoch 6/300\n",
      "Average training loss: 0.11587001167403327\n",
      "Average test loss: 0.009621145704554187\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1069443596204122\n",
      "Average test loss: 0.006917595767726501\n",
      "Epoch 8/300\n",
      "Average training loss: 0.09931489643785689\n",
      "Average test loss: 0.006196374081903034\n",
      "Epoch 9/300\n",
      "Average training loss: 0.09285823358429802\n",
      "Average test loss: 0.007907565496034092\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0869415224260754\n",
      "Average test loss: 0.0069743906868000825\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08014947301149368\n",
      "Average test loss: 0.0073625658609800864\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07424132734537124\n",
      "Average test loss: 0.008412811113728417\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06861099217335383\n",
      "Average test loss: 0.005533921264525917\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06266504467195935\n",
      "Average test loss: 0.007180306833651331\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05772892173462444\n",
      "Average test loss: 0.0073396578679482145\n",
      "Epoch 16/300\n",
      "Average training loss: 0.053413990169763566\n",
      "Average test loss: 0.0051415341008040644\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04975448360708025\n",
      "Average test loss: 0.007885152560141351\n",
      "Epoch 18/300\n",
      "Average training loss: 0.046927646583980985\n",
      "Average test loss: 0.005877486003355847\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04419297995832231\n",
      "Average test loss: 0.00797827221867111\n",
      "Epoch 20/300\n",
      "Average training loss: 0.041808603468868465\n",
      "Average test loss: 0.0052648118916485045\n",
      "Epoch 21/300\n",
      "Average training loss: 0.040067404528458916\n",
      "Average test loss: 0.008051244450112184\n",
      "Epoch 22/300\n",
      "Average training loss: 0.038090210378170015\n",
      "Average test loss: 0.005391137580076854\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03643475737174352\n",
      "Average test loss: 0.006807143850458993\n",
      "Epoch 24/300\n",
      "Average training loss: 0.034431861261526746\n",
      "Average test loss: 0.004159636019004716\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03308101636502478\n",
      "Average test loss: 0.003957188702705834\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03188141426940759\n",
      "Average test loss: 0.005002191301642193\n",
      "Epoch 27/300\n",
      "Average training loss: 0.030772568406330216\n",
      "Average test loss: 0.004075774822591079\n",
      "Epoch 28/300\n",
      "Average training loss: 0.029939815600713095\n",
      "Average test loss: 0.0038832218965722453\n",
      "Epoch 29/300\n",
      "Average training loss: 0.028991667676303123\n",
      "Average test loss: 0.004058688400520219\n",
      "Epoch 30/300\n",
      "Average training loss: 0.028346799357069862\n",
      "Average test loss: 0.003770506739202473\n",
      "Epoch 31/300\n",
      "Average training loss: 0.027637105766269897\n",
      "Average test loss: 0.003997210416528914\n",
      "Epoch 32/300\n",
      "Average training loss: 0.027016598259409268\n",
      "Average test loss: 0.004962831005040142\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02664855163461632\n",
      "Average test loss: 0.003559437006298039\n",
      "Epoch 34/300\n",
      "Average training loss: 0.026090461997522248\n",
      "Average test loss: 0.003764585730102327\n",
      "Epoch 35/300\n",
      "Average training loss: 0.025845070825682746\n",
      "Average test loss: 0.003624116471865111\n",
      "Epoch 36/300\n",
      "Average training loss: 0.025353661300407514\n",
      "Average test loss: 0.0037289724602467484\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024986880370312266\n",
      "Average test loss: 0.004064019426496492\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024735513978534275\n",
      "Average test loss: 0.008743269400878086\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02448167101542155\n",
      "Average test loss: 0.004350845522764656\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02415049522452884\n",
      "Average test loss: 0.003450741897440619\n",
      "Epoch 41/300\n",
      "Average training loss: 0.023837313450045056\n",
      "Average test loss: 0.003649113132721848\n",
      "Epoch 42/300\n",
      "Average training loss: 0.023685524839493963\n",
      "Average test loss: 0.004697477834920088\n",
      "Epoch 43/300\n",
      "Average training loss: 0.023415266599920062\n",
      "Average test loss: 0.009574274553192987\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02326040048400561\n",
      "Average test loss: 0.003487072131699986\n",
      "Epoch 45/300\n",
      "Average training loss: 0.023116598289873867\n",
      "Average test loss: 0.0035179092234207524\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023075676414701672\n",
      "Average test loss: 0.003715762571535177\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02273347732424736\n",
      "Average test loss: 0.00916882698569033\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02265301016304228\n",
      "Average test loss: 0.00363820830732584\n",
      "Epoch 49/300\n",
      "Average training loss: 0.022272512990567418\n",
      "Average test loss: 0.003388636446040538\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02223346192472511\n",
      "Average test loss: 0.003454843134101894\n",
      "Epoch 51/300\n",
      "Average training loss: 0.022132495476139918\n",
      "Average test loss: 0.0035508341925839584\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02201055460671584\n",
      "Average test loss: 0.004042135354545381\n",
      "Epoch 53/300\n",
      "Average training loss: 0.021873356049259503\n",
      "Average test loss: 0.003543382674248682\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0217190835972627\n",
      "Average test loss: 0.003775619180045194\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021601993024349214\n",
      "Average test loss: 0.003663152929188477\n",
      "Epoch 56/300\n",
      "Average training loss: 0.021508156932062573\n",
      "Average test loss: 0.0034884862305803433\n",
      "Epoch 57/300\n",
      "Average training loss: 0.021389205523663097\n",
      "Average test loss: 0.005119333448095454\n",
      "Epoch 58/300\n",
      "Average training loss: 0.021353155604667134\n",
      "Average test loss: 0.0035124980898367034\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02120042423572805\n",
      "Average test loss: 0.0037366538527939053\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021071590710017418\n",
      "Average test loss: 0.003535384228039119\n",
      "Epoch 61/300\n",
      "Average training loss: 0.020958639214436213\n",
      "Average test loss: 0.0034190808503578108\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020973117580016454\n",
      "Average test loss: 0.0034858206233216657\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02088225562704934\n",
      "Average test loss: 0.003409675133311086\n",
      "Epoch 64/300\n",
      "Average training loss: 0.020692944020032883\n",
      "Average test loss: 0.0036248545203771856\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020651367709040643\n",
      "Average test loss: 0.003976636414105694\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020608387074536747\n",
      "Average test loss: 0.0034208678863942624\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02049277847674158\n",
      "Average test loss: 0.0035983750648382637\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020367284336851702\n",
      "Average test loss: 0.0035555072348150943\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020410131990909578\n",
      "Average test loss: 0.0035047628972679377\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02026804462654723\n",
      "Average test loss: 0.0034464236533062326\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020139868466390504\n",
      "Average test loss: 0.0033876563550697433\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020154367635647454\n",
      "Average test loss: 0.009033285751110978\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02004706921842363\n",
      "Average test loss: 0.003470318531617522\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020013442531228064\n",
      "Average test loss: 0.003997041664189763\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020008468803432253\n",
      "Average test loss: 0.0034029884388049442\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01988781488769584\n",
      "Average test loss: 0.0034478217458559406\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019821023384730022\n",
      "Average test loss: 0.004180532825489839\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01972179443968667\n",
      "Average test loss: 0.0035869645186596445\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01973001278936863\n",
      "Average test loss: 0.0036610725161929925\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019680417938364878\n",
      "Average test loss: 0.0034885379489925173\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019609008974499173\n",
      "Average test loss: 0.0034545158731440705\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01954920220706198\n",
      "Average test loss: 0.003522551594509019\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019493183553218843\n",
      "Average test loss: 0.004346298489098748\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019413411802715726\n",
      "Average test loss: 0.0033942872241346372\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01943204178329971\n",
      "Average test loss: 0.0035160077729572852\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019398160090049108\n",
      "Average test loss: 0.0033416046173208288\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019320525899529456\n",
      "Average test loss: 0.0034637690728737246\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019194826900959014\n",
      "Average test loss: 0.003426124640016092\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01918383349147108\n",
      "Average test loss: 0.004177689352797137\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019129860483937795\n",
      "Average test loss: 0.0033987475120358995\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01912982187834051\n",
      "Average test loss: 0.0035204546321183443\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019161048168937364\n",
      "Average test loss: 0.0034649666965835624\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01898222132027149\n",
      "Average test loss: 0.003792541063701113\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019020103378428355\n",
      "Average test loss: 0.0035790803604241876\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01898272859884633\n",
      "Average test loss: 0.003444145360754596\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018896925166249277\n",
      "Average test loss: 0.0036400240663852956\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018833871482147112\n",
      "Average test loss: 0.0034250622470345764\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01886270713061094\n",
      "Average test loss: 0.0035486304715482723\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01879298417435752\n",
      "Average test loss: 0.005698290722237693\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018772159680724144\n",
      "Average test loss: 0.003514742926694453\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01866275128059917\n",
      "Average test loss: 0.0038680104627791378\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018786444650755987\n",
      "Average test loss: 0.003546790063795116\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018625291174484623\n",
      "Average test loss: 0.0034087178599503305\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01862178936269548\n",
      "Average test loss: 0.0033970368906027742\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018593308155735334\n",
      "Average test loss: 0.003542521029089888\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018491184798379738\n",
      "Average test loss: 0.005148872188809845\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01853003535336918\n",
      "Average test loss: 0.0035308392124457494\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018431080693999926\n",
      "Average test loss: 0.003810280261768235\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018423782096968756\n",
      "Average test loss: 0.0034639618620276452\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018438592948847348\n",
      "Average test loss: 0.0036017484731144375\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018381360868612924\n",
      "Average test loss: 0.0034175127434233825\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01838303111659156\n",
      "Average test loss: 0.0036376681116720043\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018314783840543695\n",
      "Average test loss: 0.0035586192020111615\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01829129828181532\n",
      "Average test loss: 0.003579951579786009\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018299684441751903\n",
      "Average test loss: 0.0036623235311773086\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018204270783397886\n",
      "Average test loss: 0.0033999029253092075\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01826050961845451\n",
      "Average test loss: 0.003444041446161767\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01819780853556262\n",
      "Average test loss: 0.0043705291358961\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01816578791042169\n",
      "Average test loss: 0.0037597259773562352\n",
      "Epoch 120/300\n",
      "Average training loss: 0.018122478380799292\n",
      "Average test loss: 0.0038968237990306483\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01805274082058006\n",
      "Average test loss: 0.00340920255974763\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018048065896663402\n",
      "Average test loss: 0.0035548786900730595\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018011519466837246\n",
      "Average test loss: 0.003878257700552543\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018022540360689163\n",
      "Average test loss: 0.0034802153147757055\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01794906975908412\n",
      "Average test loss: 0.003659060666958491\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017970268782642154\n",
      "Average test loss: 0.0036056068140185543\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017924313482311036\n",
      "Average test loss: 0.003377795373607013\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017924045537908873\n",
      "Average test loss: 0.003479103794321418\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017946360621187422\n",
      "Average test loss: 0.00378951778759559\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017855852686696583\n",
      "Average test loss: 0.003676714613205857\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0178185825281673\n",
      "Average test loss: 0.003570319342530436\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01779251849816905\n",
      "Average test loss: 0.003438988136541512\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017816447714136707\n",
      "Average test loss: 0.003777522970818811\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01778992669035991\n",
      "Average test loss: 0.0034807325365642707\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017764287571112314\n",
      "Average test loss: 0.003511343048264583\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01774151991473304\n",
      "Average test loss: 0.0033731282345122763\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017655010215110248\n",
      "Average test loss: 0.0035587395949082242\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01764690079788367\n",
      "Average test loss: 0.0035510001700992384\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01763954704089297\n",
      "Average test loss: 0.0034886168491923145\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017679084319207405\n",
      "Average test loss: 0.003682141486969259\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017646649402048853\n",
      "Average test loss: 0.0034732890342258744\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017541332090894382\n",
      "Average test loss: 0.003606039161276486\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017580678891804483\n",
      "Average test loss: 0.0034933804228074023\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01750214868783951\n",
      "Average test loss: 0.0034551509376615284\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017542040056652494\n",
      "Average test loss: 0.0034553170177257725\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017546428034702938\n",
      "Average test loss: 0.0036177285926209555\n",
      "Epoch 147/300\n",
      "Average training loss: 0.017546710525949798\n",
      "Average test loss: 0.003642162850954466\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017498425682385764\n",
      "Average test loss: 0.0034793433985776372\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01744688250952297\n",
      "Average test loss: 0.003508539378643036\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017435802045795652\n",
      "Average test loss: 0.003630016179341409\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01741390694346693\n",
      "Average test loss: 0.003964513870577018\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017450584924883314\n",
      "Average test loss: 0.0034871534092558755\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01739433561017116\n",
      "Average test loss: 0.0036434128348612122\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017326847321457332\n",
      "Average test loss: 0.0035078634369290537\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017386899460521008\n",
      "Average test loss: 0.0037417524666008024\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017298751844300166\n",
      "Average test loss: 0.003751512955253323\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017318188996778595\n",
      "Average test loss: 0.003685000450660785\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01724646150569121\n",
      "Average test loss: 0.003478638016929229\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017329259554545083\n",
      "Average test loss: 0.003577734018986424\n",
      "Epoch 160/300\n",
      "Average training loss: 0.017246212424503433\n",
      "Average test loss: 0.0036435173654721844\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01722290606300036\n",
      "Average test loss: 0.003551514574326575\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017232658356428145\n",
      "Average test loss: 0.0037435513687216573\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017248361395465005\n",
      "Average test loss: 0.0035684563397533364\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017208097255892223\n",
      "Average test loss: 0.0035736070641626916\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017153930644194286\n",
      "Average test loss: 0.004517295992622773\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017152685127324527\n",
      "Average test loss: 0.0038900722505317794\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01713948133422269\n",
      "Average test loss: 0.0036784594700568253\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01719602114127742\n",
      "Average test loss: 0.003727154389023781\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017127023956841892\n",
      "Average test loss: 0.0036271343452648984\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017087130733662182\n",
      "Average test loss: 0.0035578849307364886\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017099918820791773\n",
      "Average test loss: 0.00359491267034577\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017074187654587958\n",
      "Average test loss: 0.0036470349648346505\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01703537255194452\n",
      "Average test loss: 0.0036307928562164305\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01708149813943439\n",
      "Average test loss: 0.0036186786281565825\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017022898957961136\n",
      "Average test loss: 0.003593614581144518\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017008227015535036\n",
      "Average test loss: 0.0038986531814767253\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01695383945769734\n",
      "Average test loss: 0.0036483951873249477\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016986596835984125\n",
      "Average test loss: 0.0039010162928866015\n",
      "Epoch 179/300\n",
      "Average training loss: 0.016958684942788547\n",
      "Average test loss: 0.0035565157160162924\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01699205236302482\n",
      "Average test loss: 0.0034746175909207925\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016950024515390395\n",
      "Average test loss: 0.003858999981234471\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016964968644082545\n",
      "Average test loss: 0.003772211816161871\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0168725407090452\n",
      "Average test loss: 0.0034575079068955446\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016965631023049356\n",
      "Average test loss: 0.0034905263802243605\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016931880977418688\n",
      "Average test loss: 0.003543372833169997\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016843311920762064\n",
      "Average test loss: 0.003619945781926314\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016848253208729957\n",
      "Average test loss: 0.0035106116159715588\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01689297472106086\n",
      "Average test loss: 0.0035612052099572287\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016837164445883696\n",
      "Average test loss: 0.0036661520683103138\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016793874652849303\n",
      "Average test loss: 0.003531268216255638\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016830395150515768\n",
      "Average test loss: 0.0035536943876908885\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01680732162296772\n",
      "Average test loss: 0.004429581567231152\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016766292730967202\n",
      "Average test loss: 0.0036810103336142167\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016807979822158814\n",
      "Average test loss: 0.003637339767275585\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016747575683726205\n",
      "Average test loss: 0.003586423406170474\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016845972802076076\n",
      "Average test loss: 0.0037728135076661906\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016725376094381014\n",
      "Average test loss: 0.0035722264183892143\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01668740507049693\n",
      "Average test loss: 0.0037163316042472918\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016676058166556887\n",
      "Average test loss: 0.0036091179077823955\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016732113546795314\n",
      "Average test loss: 0.0035522453780803417\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016674787206782234\n",
      "Average test loss: 0.0036063086493975586\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016703296431236796\n",
      "Average test loss: 0.0038099481707645788\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016664836396773658\n",
      "Average test loss: 0.00371868025801248\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016618455891807875\n",
      "Average test loss: 0.0036108919630448025\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016647960773772664\n",
      "Average test loss: 0.0035959415942844416\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016611404731041855\n",
      "Average test loss: 0.0035701867395804988\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016599836080438562\n",
      "Average test loss: 0.003588066922293769\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016641611129045487\n",
      "Average test loss: 0.0035692221698247722\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016637655335995885\n",
      "Average test loss: 0.003610736713020338\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016565315438641443\n",
      "Average test loss: 0.0036627593624095124\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01663300275637044\n",
      "Average test loss: 0.003706692939831151\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016525244386659727\n",
      "Average test loss: 0.003644098239226474\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016519912762774362\n",
      "Average test loss: 0.003914114805766278\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016572133623891407\n",
      "Average test loss: 0.0035896491596682204\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016530389027463066\n",
      "Average test loss: 0.0037933402611977523\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01653593910071585\n",
      "Average test loss: 0.0035644405958139233\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01650062316821681\n",
      "Average test loss: 0.007407503774182664\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016514571378628412\n",
      "Average test loss: 0.003683489239257243\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016540287108057074\n",
      "Average test loss: 0.0035818641084349816\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01647924358314938\n",
      "Average test loss: 0.0036003311646895277\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01644530011547936\n",
      "Average test loss: 0.0035425240916924345\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016421102902127636\n",
      "Average test loss: 0.00362795354736348\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016477477533949746\n",
      "Average test loss: 0.003536939062592056\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016398707608381906\n",
      "Average test loss: 0.0038462963145640162\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0164156025548776\n",
      "Average test loss: 0.003717132437146372\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016430661547515126\n",
      "Average test loss: 0.0037377230597452986\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016405921477410528\n",
      "Average test loss: 0.003628501018302308\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016457106726037132\n",
      "Average test loss: 0.0035452968945933715\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016392639168434673\n",
      "Average test loss: 0.003794883093279269\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016403555182947054\n",
      "Average test loss: 0.0036186041434605916\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01638444429387649\n",
      "Average test loss: 0.0038001040439638828\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016341663976510365\n",
      "Average test loss: 0.0037392498167852562\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01636722588042418\n",
      "Average test loss: 0.003716005226183269\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016342097747657033\n",
      "Average test loss: 0.003647165844009982\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01631374430656433\n",
      "Average test loss: 0.003502819280036622\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016273274724682173\n",
      "Average test loss: 0.0038141272939327692\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016298697554402882\n",
      "Average test loss: 0.003691862397723728\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016323905640178257\n",
      "Average test loss: 0.003570259127351973\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016254101829396354\n",
      "Average test loss: 0.004333865306029717\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016376681559615663\n",
      "Average test loss: 0.0036320252532346382\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016315070773164433\n",
      "Average test loss: 0.003706768616620037\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016269845702581935\n",
      "Average test loss: 0.003786904657466544\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016220091382662456\n",
      "Average test loss: 0.0036784787302215896\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016245144011245834\n",
      "Average test loss: 0.003868592677017053\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0162079673566752\n",
      "Average test loss: 0.0037234799236887032\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01618234035041597\n",
      "Average test loss: 0.0037392580397427083\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016205950619445908\n",
      "Average test loss: 0.003643845069118672\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016232428088784216\n",
      "Average test loss: 0.003690237277083927\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016219929264651406\n",
      "Average test loss: 0.003622983363767465\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01618456491496828\n",
      "Average test loss: 0.003706149648461077\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01622149464653598\n",
      "Average test loss: 0.0036419374752375815\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01619257542822096\n",
      "Average test loss: 0.0036053193329523008\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01617711997942792\n",
      "Average test loss: 0.003928030764891042\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016164337158203125\n",
      "Average test loss: 0.003654598929815822\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016115736422439415\n",
      "Average test loss: 0.003791119390477737\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016185704711410735\n",
      "Average test loss: 0.0036404969077557326\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016156665489905412\n",
      "Average test loss: 0.0036329411011603143\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01614236217074924\n",
      "Average test loss: 0.00363013564919432\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016078509313364825\n",
      "Average test loss: 0.003718559825585948\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01616238443719016\n",
      "Average test loss: 0.0036340575739741325\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016068966468175253\n",
      "Average test loss: 0.0036656267423596646\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016083025344543988\n",
      "Average test loss: 0.003618112667153279\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01611864582863119\n",
      "Average test loss: 0.0036721920199278327\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016035317921804057\n",
      "Average test loss: 0.0036307388403349454\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01606788025713629\n",
      "Average test loss: 0.0036951557671030364\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01609102002862427\n",
      "Average test loss: 0.00376056953974896\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016108571524421373\n",
      "Average test loss: 0.003646280032893022\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01602057275838322\n",
      "Average test loss: 0.003564996511985858\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016054033672644033\n",
      "Average test loss: 0.0036701198714888758\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016031853028469614\n",
      "Average test loss: 0.004756492600672775\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01601752658188343\n",
      "Average test loss: 0.003629885780935486\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016033364304237897\n",
      "Average test loss: 0.0037092593415743776\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01607077916711569\n",
      "Average test loss: 0.0037007209298511347\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015961684380968413\n",
      "Average test loss: 0.0036235527541074487\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016002596479323176\n",
      "Average test loss: 0.0038064318402773805\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015981318237053024\n",
      "Average test loss: 0.0036618733995904527\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015993600267502996\n",
      "Average test loss: 0.003748434749328428\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016000828799274232\n",
      "Average test loss: 0.003760119539582067\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01593102502491739\n",
      "Average test loss: 0.004451583362287946\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0160063962009218\n",
      "Average test loss: 0.0035851892282565434\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015906541582610872\n",
      "Average test loss: 0.003666262746685081\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01592809694343143\n",
      "Average test loss: 0.0036356143313977453\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01596936018599404\n",
      "Average test loss: 0.0036812456647555033\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015938417727748552\n",
      "Average test loss: 0.003664827875379059\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015940361713369686\n",
      "Average test loss: 0.003592005031804244\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01598328504545821\n",
      "Average test loss: 0.003752122295813428\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01589954588148329\n",
      "Average test loss: 0.003663150039811929\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015944615023003685\n",
      "Average test loss: 0.003813853964416517\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01596869601143731\n",
      "Average test loss: 0.0036847569888664618\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015894423430164655\n",
      "Average test loss: 0.0037107199664331144\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015847093593743113\n",
      "Average test loss: 0.0036310985572636126\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015899131649070315\n",
      "Average test loss: 0.00362834802766641\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015896049578156737\n",
      "Average test loss: 0.0037186625299768317\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015919576860136456\n",
      "Average test loss: 0.003768690603888697\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01587211798876524\n",
      "Average test loss: 0.0036505696225083537\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01585647794438733\n",
      "Average test loss: 0.0036278322250064875\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015861189808282588\n",
      "Average test loss: 0.0037980791963636877\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015861480300625164\n",
      "Average test loss: 0.0037499465462234286\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015834898764888444\n",
      "Average test loss: 0.004537398934364319\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01586240247388681\n",
      "Average test loss: 0.0037396461769110627\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.5865620164076487\n",
      "Average test loss: 0.01385508083883259\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2527399977313148\n",
      "Average test loss: 0.009257271855655645\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1870304452445772\n",
      "Average test loss: 0.008935687298576037\n",
      "Epoch 4/300\n",
      "Average training loss: 0.15547255063056944\n",
      "Average test loss: 0.006041141229785151\n",
      "Epoch 5/300\n",
      "Average training loss: 0.136586906141705\n",
      "Average test loss: 0.012799990903172229\n",
      "Epoch 6/300\n",
      "Average training loss: 0.12390679718388452\n",
      "Average test loss: 0.005190878674801853\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11328712404436535\n",
      "Average test loss: 0.0052900476501219805\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10389718801445431\n",
      "Average test loss: 0.0066703338676856625\n",
      "Epoch 9/300\n",
      "Average training loss: 0.09390946023331748\n",
      "Average test loss: 0.004485810873409112\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08451676167382134\n",
      "Average test loss: 0.006025417732695738\n",
      "Epoch 11/300\n",
      "Average training loss: 0.07572783287366232\n",
      "Average test loss: 0.004638805596364869\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06744799623886744\n",
      "Average test loss: 0.0039629270535790255\n",
      "Epoch 13/300\n",
      "Average training loss: 0.061190638422966\n",
      "Average test loss: 0.003928798706167274\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05485014855861664\n",
      "Average test loss: 0.003815174014411039\n",
      "Epoch 15/300\n",
      "Average training loss: 0.049944138881233\n",
      "Average test loss: 0.0036586030224959056\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0458562394645479\n",
      "Average test loss: 0.0033478931453492906\n",
      "Epoch 17/300\n",
      "Average training loss: 0.042349848051865896\n",
      "Average test loss: 0.004992710738960239\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03917549742261569\n",
      "Average test loss: 0.0036317303102049565\n",
      "Epoch 19/300\n",
      "Average training loss: 0.036691360720329816\n",
      "Average test loss: 0.0031665509212762117\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03472237850560082\n",
      "Average test loss: 0.005040717296302319\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03284036745296584\n",
      "Average test loss: 0.0032405260701974235\n",
      "Epoch 22/300\n",
      "Average training loss: 0.031067811528841655\n",
      "Average test loss: 0.00436187874360217\n",
      "Epoch 23/300\n",
      "Average training loss: 0.029313476777738996\n",
      "Average test loss: 0.002947574313936962\n",
      "Epoch 24/300\n",
      "Average training loss: 0.028427717621127765\n",
      "Average test loss: 0.0031522718001570967\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02709336164759265\n",
      "Average test loss: 0.0027724452347805103\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025706599502099886\n",
      "Average test loss: 0.0026472160956925814\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0248724973599116\n",
      "Average test loss: 0.0027247347593721415\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02401543969247076\n",
      "Average test loss: 0.0026783989378147653\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02314189925127559\n",
      "Average test loss: 0.0038514020656132035\n",
      "Epoch 30/300\n",
      "Average training loss: 0.022503507847587267\n",
      "Average test loss: 0.0035847641283439265\n",
      "Epoch 31/300\n",
      "Average training loss: 0.021909609221749837\n",
      "Average test loss: 0.002554851202915112\n",
      "Epoch 32/300\n",
      "Average training loss: 0.021683615149723157\n",
      "Average test loss: 0.002505408627808922\n",
      "Epoch 33/300\n",
      "Average training loss: 0.021244452685945565\n",
      "Average test loss: 0.002598898247919149\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020706703116496403\n",
      "Average test loss: 0.0025243822135445145\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020340422771043247\n",
      "Average test loss: 0.0025452090625961623\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02013129755689038\n",
      "Average test loss: 0.0023539254932353894\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019799238752987648\n",
      "Average test loss: 0.0024234319844593606\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019486504359377755\n",
      "Average test loss: 0.002480671283685499\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019198302876618174\n",
      "Average test loss: 0.0026317259696208768\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019050931619273292\n",
      "Average test loss: 0.006165409399403466\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01886670135292742\n",
      "Average test loss: 0.0023792709292223056\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018737517781555652\n",
      "Average test loss: 0.003878293218712012\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01844644783768389\n",
      "Average test loss: 0.0023945398084405396\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018325401736630335\n",
      "Average test loss: 0.0023293029685608215\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018170420239369073\n",
      "Average test loss: 0.0023684959944544567\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01804624445405271\n",
      "Average test loss: 0.0025607432048353885\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01779325702620877\n",
      "Average test loss: 0.0023416685795204506\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01776510094271766\n",
      "Average test loss: 0.002640769020654261\n",
      "Epoch 49/300\n",
      "Average training loss: 0.017585061545173326\n",
      "Average test loss: 0.0024531992988453973\n",
      "Epoch 50/300\n",
      "Average training loss: 0.017544295221567155\n",
      "Average test loss: 0.0023657159267200362\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01734357813331816\n",
      "Average test loss: 0.00262728828129669\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01731162372065915\n",
      "Average test loss: 0.0023908717720251943\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01723135502802001\n",
      "Average test loss: 0.0023321688121391666\n",
      "Epoch 54/300\n",
      "Average training loss: 0.017156342907912202\n",
      "Average test loss: 0.0023205134628547564\n",
      "Epoch 55/300\n",
      "Average training loss: 0.016958721857931878\n",
      "Average test loss: 0.0023484214471860064\n",
      "Epoch 56/300\n",
      "Average training loss: 0.016889965469638505\n",
      "Average test loss: 0.002408797962591052\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01680981472465727\n",
      "Average test loss: 0.002449057929010855\n",
      "Epoch 58/300\n",
      "Average training loss: 0.016712832493914496\n",
      "Average test loss: 0.0022942572457508906\n",
      "Epoch 59/300\n",
      "Average training loss: 0.016645567374096976\n",
      "Average test loss: 0.00260094264936116\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016654953834083344\n",
      "Average test loss: 0.0023888473943289783\n",
      "Epoch 61/300\n",
      "Average training loss: 0.016545775330728953\n",
      "Average test loss: 0.002382135585571329\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016474589738580916\n",
      "Average test loss: 0.002274388023962577\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016414238553908136\n",
      "Average test loss: 0.0022732093394216566\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01635512100160122\n",
      "Average test loss: 0.002470035381201241\n",
      "Epoch 65/300\n",
      "Average training loss: 0.016255702434314623\n",
      "Average test loss: 0.002276301187566585\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016149144569204913\n",
      "Average test loss: 0.002201925605121586\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01606729974846045\n",
      "Average test loss: 0.002215336209576991\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016061226040952736\n",
      "Average test loss: 0.002265653816362222\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015984248195257452\n",
      "Average test loss: 0.002259228174471193\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015888131191333137\n",
      "Average test loss: 0.002289038125011656\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015921019550826813\n",
      "Average test loss: 0.0022284797868794867\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015773057711621127\n",
      "Average test loss: 0.0022512586561756003\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015738121671809092\n",
      "Average test loss: 0.002190138030797243\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01569457297027111\n",
      "Average test loss: 0.0023781737316813734\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015672716658562423\n",
      "Average test loss: 0.00245797381322417\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015633695546123715\n",
      "Average test loss: 0.0024702426973316404\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015510846174425549\n",
      "Average test loss: 0.002240956993566619\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015506038806504674\n",
      "Average test loss: 0.0022984923489598763\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015486969924635358\n",
      "Average test loss: 0.0022091542842487496\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015441458301411734\n",
      "Average test loss: 0.0024979218328371644\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015395451350344551\n",
      "Average test loss: 0.0023058519104702606\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01529269310997592\n",
      "Average test loss: 0.002219744635331962\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015329331174492837\n",
      "Average test loss: 0.0026802222349991403\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015253156878881985\n",
      "Average test loss: 0.002260069736176067\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015160426252418095\n",
      "Average test loss: 0.0023226996739912366\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015180831137630675\n",
      "Average test loss: 0.0024057902249818046\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015144574760562845\n",
      "Average test loss: 0.0022294919525997505\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015099502606524361\n",
      "Average test loss: 0.0025433621381719907\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015091365839872096\n",
      "Average test loss: 0.0030306246649059983\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015037186665667428\n",
      "Average test loss: 0.002454601578724881\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014977205305463738\n",
      "Average test loss: 0.002507871861466103\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014939511518511507\n",
      "Average test loss: 0.002469463158191906\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014928217677606477\n",
      "Average test loss: 0.0022299417933035227\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014863091755244468\n",
      "Average test loss: 0.002890015325198571\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014835367737544907\n",
      "Average test loss: 0.002296684607863426\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01487659489777353\n",
      "Average test loss: 0.0024375614203098746\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01477481297403574\n",
      "Average test loss: 0.002445110900224083\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014714671487609545\n",
      "Average test loss: 0.0021965034526462354\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014680471770465374\n",
      "Average test loss: 0.002304187353493439\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014699729902876747\n",
      "Average test loss: 0.0023040676884767083\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014629702154960898\n",
      "Average test loss: 0.002293453810115655\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014636341809398597\n",
      "Average test loss: 0.002501415453954703\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01457123702019453\n",
      "Average test loss: 0.0023157134279608725\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014551056216160456\n",
      "Average test loss: 0.004765562273354994\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014548146532641517\n",
      "Average test loss: 0.002210668771631188\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0145228504654434\n",
      "Average test loss: 0.002386392183808817\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014490300355686082\n",
      "Average test loss: 0.002316167449992564\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014448950936810838\n",
      "Average test loss: 0.002362029270475937\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014410789970722463\n",
      "Average test loss: 0.0023618186799188455\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014376884975367122\n",
      "Average test loss: 0.0024425043698607218\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014391301626132594\n",
      "Average test loss: 0.0025418588053435086\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014353658660418458\n",
      "Average test loss: 0.0022772286743339564\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014346983378132184\n",
      "Average test loss: 0.0022993128071021702\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01430808230737845\n",
      "Average test loss: 0.0023272952681614293\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014242645295957724\n",
      "Average test loss: 0.0022413639707697763\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014250912565324042\n",
      "Average test loss: 0.0023691117842164306\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014237795650959015\n",
      "Average test loss: 0.0022339598159823153\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014203470160563787\n",
      "Average test loss: 0.002506058375661572\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014182077581683794\n",
      "Average test loss: 0.00226779577581005\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014176817013157738\n",
      "Average test loss: 0.0023101905641249483\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014147214405238629\n",
      "Average test loss: 0.0022593647957675988\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01410561953485012\n",
      "Average test loss: 0.0023468325342983006\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014130170678098996\n",
      "Average test loss: 0.0023514061650882164\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014062997341156005\n",
      "Average test loss: 0.002233582771072785\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014082575678825378\n",
      "Average test loss: 0.0022257836253071826\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014050488969518079\n",
      "Average test loss: 0.002250056696227855\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0139978818313943\n",
      "Average test loss: 0.0022668026876118447\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014026592147019174\n",
      "Average test loss: 0.0022592162011812133\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013962404617004924\n",
      "Average test loss: 0.002236193968190087\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013966441456642416\n",
      "Average test loss: 0.0022905801832675933\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013917414097322358\n",
      "Average test loss: 0.0023420126349147823\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013954321680797471\n",
      "Average test loss: 0.00230442587327626\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013911708775493834\n",
      "Average test loss: 0.0032295515237169134\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013848287735548285\n",
      "Average test loss: 0.002370240605953667\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013862324150900046\n",
      "Average test loss: 0.0023169820414235195\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013831729478306241\n",
      "Average test loss: 0.0026180655776212613\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013790877247022259\n",
      "Average test loss: 0.0022520708525553346\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013845959614548419\n",
      "Average test loss: 0.0022726421654224397\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0137971873200602\n",
      "Average test loss: 0.0025029076722760993\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013787625348402395\n",
      "Average test loss: 0.002432760005299416\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013729774718483289\n",
      "Average test loss: 0.0023663572263386513\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013742621144486798\n",
      "Average test loss: 0.0023075371504657797\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013690517143242889\n",
      "Average test loss: 0.0023045699966864454\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013742996643814776\n",
      "Average test loss: 0.002265417077475124\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01368158412228028\n",
      "Average test loss: 0.002264811401979791\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01366463172601329\n",
      "Average test loss: 0.00240697536803782\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013662438850435946\n",
      "Average test loss: 0.0024974174259437456\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013633075546059343\n",
      "Average test loss: 0.0023466506556918225\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013690809187789758\n",
      "Average test loss: 0.002356423327078422\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013633861436198155\n",
      "Average test loss: 0.002466314166991247\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013590809550550248\n",
      "Average test loss: 0.0023533930437018473\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013578924352096186\n",
      "Average test loss: 0.002402820956789785\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013538769416511059\n",
      "Average test loss: 0.0023456547621948025\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013567307444496288\n",
      "Average test loss: 0.0024556983204351533\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013542566335035694\n",
      "Average test loss: 0.002383318922792872\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013496042577756776\n",
      "Average test loss: 0.002583881350234151\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013505122390886148\n",
      "Average test loss: 0.0025521089266985655\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013501184316145049\n",
      "Average test loss: 0.0023489157704429495\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013497840631339285\n",
      "Average test loss: 0.0023704428060187235\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013461680188775062\n",
      "Average test loss: 0.0023775123750997915\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013490394008656342\n",
      "Average test loss: 0.002327886458693279\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013436440084543493\n",
      "Average test loss: 0.002513172593174709\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013444084333048927\n",
      "Average test loss: 0.002377850609934992\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013436672573288282\n",
      "Average test loss: 0.002322000897386008\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01340906581034263\n",
      "Average test loss: 0.0023929847826560336\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013380498703155252\n",
      "Average test loss: 0.0023454742102573317\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013390171161956257\n",
      "Average test loss: 0.0023593813402371273\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013389698126249842\n",
      "Average test loss: 0.0022972188556773794\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013372178194423517\n",
      "Average test loss: 0.0025495067516134846\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013334713715646002\n",
      "Average test loss: 0.0024373567087782756\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013317941691312524\n",
      "Average test loss: 0.002339842629929384\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013341755581398804\n",
      "Average test loss: 0.002380496664179696\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01334197869979673\n",
      "Average test loss: 0.002315620164697369\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013315571771727668\n",
      "Average test loss: 0.0023269143876516155\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01327415727161699\n",
      "Average test loss: 0.0025102245927684837\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013283517354064517\n",
      "Average test loss: 0.002481506912658612\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013263049369884861\n",
      "Average test loss: 0.0025685025772286788\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013225510658489334\n",
      "Average test loss: 0.002455770572854413\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013244743435747094\n",
      "Average test loss: 0.002431530955351061\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01323609140680896\n",
      "Average test loss: 0.0023024497983149357\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013235277732213339\n",
      "Average test loss: 0.0022812649942934513\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01324138801296552\n",
      "Average test loss: 0.0024945153398439287\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013186319525871012\n",
      "Average test loss: 0.20970710478888618\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013274371223317252\n",
      "Average test loss: 0.002356907883245084\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013214462567534712\n",
      "Average test loss: 0.0028018471066736514\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013172913895712958\n",
      "Average test loss: 0.002434915628284216\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013167618481649292\n",
      "Average test loss: 0.0023673744975692694\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013119701706700855\n",
      "Average test loss: 0.002409090619120333\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013131524885694187\n",
      "Average test loss: 0.002630707566315929\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01315496923112207\n",
      "Average test loss: 0.002348247614585691\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013122201559444269\n",
      "Average test loss: 0.0023729340984589525\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013085993981195821\n",
      "Average test loss: 0.002416449533568488\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013127419980035887\n",
      "Average test loss: 0.0024737308621406557\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013103038678566615\n",
      "Average test loss: 0.002441972064268258\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013165598198771477\n",
      "Average test loss: 0.0029282066958645978\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013079681154754427\n",
      "Average test loss: 0.0023522600575039783\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013047712459332414\n",
      "Average test loss: 0.0024614798304521375\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013073637903564505\n",
      "Average test loss: 0.0024290125506619613\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013012850647999182\n",
      "Average test loss: 0.002373075413517654\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013022507432434294\n",
      "Average test loss: 0.002370227536703977\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013035866362353167\n",
      "Average test loss: 0.0025234968765742247\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013019774362444878\n",
      "Average test loss: 0.002471860878997379\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013031310741272237\n",
      "Average test loss: 0.0023782199908875755\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013005748118791315\n",
      "Average test loss: 0.00249924736449288\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01298481756862667\n",
      "Average test loss: 0.0023727581227819127\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013032569715546238\n",
      "Average test loss: 0.002447453970917397\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01297012394418319\n",
      "Average test loss: 0.002503639794472191\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012943592252830665\n",
      "Average test loss: 0.002376652173077067\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012931672755214903\n",
      "Average test loss: 0.0024275059370944896\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01298542911062638\n",
      "Average test loss: 0.0026804326364977494\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012976129062473774\n",
      "Average test loss: 0.002398730343828599\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012921828125086096\n",
      "Average test loss: 0.002377609572787252\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012914845187630919\n",
      "Average test loss: 0.002450552954855892\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012943763182395034\n",
      "Average test loss: 0.0026424950543377135\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012881825785670015\n",
      "Average test loss: 0.002519330985637175\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012969806116074324\n",
      "Average test loss: 0.0024694788857466646\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01284903992464145\n",
      "Average test loss: 0.0025180695119003454\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01286189230531454\n",
      "Average test loss: 0.002692659898764557\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01289809237420559\n",
      "Average test loss: 0.0024297435579614507\n",
      "Epoch 220/300\n",
      "Average training loss: 0.012846252291566796\n",
      "Average test loss: 0.002616273628754748\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012852370153698656\n",
      "Average test loss: 0.002419283272491561\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012854434201286898\n",
      "Average test loss: 0.0024253447202758657\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01291233994315068\n",
      "Average test loss: 0.0023653462585061786\n",
      "Epoch 224/300\n",
      "Average training loss: 0.012824818429847559\n",
      "Average test loss: 0.002409244743693206\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012848927513592774\n",
      "Average test loss: 0.0023530720089458756\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012827095155914625\n",
      "Average test loss: 0.002467157686750094\n",
      "Epoch 227/300\n",
      "Average training loss: 0.012836811596320736\n",
      "Average test loss: 0.0025010705078020693\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012840426681770219\n",
      "Average test loss: 0.00244314919018911\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012788306363754802\n",
      "Average test loss: 0.0024510649585475523\n",
      "Epoch 230/300\n",
      "Average training loss: 0.012776135893331634\n",
      "Average test loss: 0.0024115493980546793\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012828282400965691\n",
      "Average test loss: 0.002447841322152979\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012767982255253527\n",
      "Average test loss: 0.0025438725190858047\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012749075282779005\n",
      "Average test loss: 0.0024849576252616115\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012793507638904784\n",
      "Average test loss: 0.002443337834647132\n",
      "Epoch 235/300\n",
      "Average training loss: 0.012762088598476516\n",
      "Average test loss: 0.002532986904597945\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012761633659402529\n",
      "Average test loss: 0.002402872783649299\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012755906853410933\n",
      "Average test loss: 0.0024193096465120714\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01272961570819219\n",
      "Average test loss: 0.0025153731416083045\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012722428146335814\n",
      "Average test loss: 0.002395513711704148\n",
      "Epoch 240/300\n",
      "Average training loss: 0.012764667316443391\n",
      "Average test loss: 0.0026545826043519708\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012697015667127238\n",
      "Average test loss: 0.002368420600787633\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01271779004484415\n",
      "Average test loss: 0.002431955102003283\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01268254371566905\n",
      "Average test loss: 0.0024618030382423765\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012729818512168195\n",
      "Average test loss: 0.0025221637667467195\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012716598318682777\n",
      "Average test loss: 0.002461474077569114\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012681353950666057\n",
      "Average test loss: 0.0025123296435922383\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012705331264270677\n",
      "Average test loss: 0.002403764634910557\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012651675552129746\n",
      "Average test loss: 0.0025680531319230795\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012681352599627441\n",
      "Average test loss: 0.0025044603979008065\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012669236293269529\n",
      "Average test loss: 0.002445563613333636\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012669489812519814\n",
      "Average test loss: 0.0024387058621893327\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012642592837413152\n",
      "Average test loss: 0.0024762973570161394\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012650941330525611\n",
      "Average test loss: 0.002429014216073685\n",
      "Epoch 254/300\n",
      "Average training loss: 0.012653037495083279\n",
      "Average test loss: 0.0024063323713425133\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012621846966445445\n",
      "Average test loss: 0.0024072315254145197\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012611109321316084\n",
      "Average test loss: 0.0024499488135592807\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012654199404435025\n",
      "Average test loss: 0.002541551414773696\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012581250150998433\n",
      "Average test loss: 0.002761607026991745\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012609448912243049\n",
      "Average test loss: 0.002480313757434487\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01262924093918668\n",
      "Average test loss: 0.0023624864582800203\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012604275222453807\n",
      "Average test loss: 0.002441463229039477\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012591434641016854\n",
      "Average test loss: 0.0024585406684006256\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012593118828203943\n",
      "Average test loss: 0.0024147953848457997\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012537272677653366\n",
      "Average test loss: 0.0025075431285012098\n",
      "Epoch 265/300\n",
      "Average training loss: 0.012600787087447114\n",
      "Average test loss: 0.002675704911765125\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012551855106320646\n",
      "Average test loss: 0.0026634979266673326\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012523173737857076\n",
      "Average test loss: 0.002457890099949307\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01254053895506594\n",
      "Average test loss: 0.0027341559562418195\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012561101001169946\n",
      "Average test loss: 0.002553756784233782\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012566676616668701\n",
      "Average test loss: 0.0024798467858797974\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012545452934172419\n",
      "Average test loss: 0.002589903917370571\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012523202082349194\n",
      "Average test loss: 0.0023988028820604084\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012542212751176621\n",
      "Average test loss: 0.0025232632915592854\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01254019230686956\n",
      "Average test loss: 0.002390996176128586\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012492576135529412\n",
      "Average test loss: 0.002588168416586187\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012519394575721687\n",
      "Average test loss: 0.002410355701421698\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012496504223181142\n",
      "Average test loss: 0.0024385522653659186\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012508125411967436\n",
      "Average test loss: 0.0024738499851276476\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012475653693079949\n",
      "Average test loss: 0.0024992852767722474\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012519961292545\n",
      "Average test loss: 0.0024459004021353194\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012462480071518156\n",
      "Average test loss: 0.002427745886767904\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012491434858077102\n",
      "Average test loss: 0.0024966392130073576\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012461952900720967\n",
      "Average test loss: 0.002612723394607504\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012475666574305959\n",
      "Average test loss: 0.002500946233049035\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01246887985865275\n",
      "Average test loss: 0.002480312500562933\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012499410400787989\n",
      "Average test loss: 0.0026150651057768198\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012495782490405772\n",
      "Average test loss: 0.002424561513794793\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012446656127770742\n",
      "Average test loss: 0.002475045977677736\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012416511582831542\n",
      "Average test loss: 0.002429867987624473\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01244596028741863\n",
      "Average test loss: 0.002503165249298844\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012476802262167136\n",
      "Average test loss: 0.0024789210402717194\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012391675178375508\n",
      "Average test loss: 0.00260133651116242\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012407137108345826\n",
      "Average test loss: 0.0024672051419814428\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012419705501861042\n",
      "Average test loss: 0.0024813167922612695\n",
      "Epoch 295/300\n",
      "Average training loss: 0.012422101204593977\n",
      "Average test loss: 0.0027159568884720406\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01244035262780057\n",
      "Average test loss: 0.0025056432138921485\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012362200588815741\n",
      "Average test loss: 0.002483582008216116\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012392186757591036\n",
      "Average test loss: 0.0025189632748564084\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012382421544028653\n",
      "Average test loss: 0.0025919574573636055\n",
      "Epoch 300/300\n",
      "Average training loss: 0.012395815675457318\n",
      "Average test loss: 0.002515421281879147\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.5383111644585927\n",
      "Average test loss: 0.008453646117614375\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2373818555408054\n",
      "Average test loss: 0.007152032581468423\n",
      "Epoch 3/300\n",
      "Average training loss: 0.18387792739603254\n",
      "Average test loss: 0.005652628997547759\n",
      "Epoch 4/300\n",
      "Average training loss: 0.15750853436523013\n",
      "Average test loss: 0.004677250932488177\n",
      "Epoch 5/300\n",
      "Average training loss: 0.139918398976326\n",
      "Average test loss: 0.005197260698510541\n",
      "Epoch 6/300\n",
      "Average training loss: 0.12558659162786273\n",
      "Average test loss: 0.004303378536883328\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11226399560107125\n",
      "Average test loss: 0.005403821488014526\n",
      "Epoch 8/300\n",
      "Average training loss: 0.09914057487249374\n",
      "Average test loss: 0.003982079989794228\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08669600956307517\n",
      "Average test loss: 0.003818509040400386\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07629900031619602\n",
      "Average test loss: 0.003706535652693775\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06653160849544737\n",
      "Average test loss: 0.0036054294094857242\n",
      "Epoch 12/300\n",
      "Average training loss: 0.059109963019688926\n",
      "Average test loss: 0.004333170276963049\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05287585540612539\n",
      "Average test loss: 0.0029501806698325607\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04747972746358978\n",
      "Average test loss: 0.003844270569996701\n",
      "Epoch 15/300\n",
      "Average training loss: 0.042793449802531136\n",
      "Average test loss: 0.002980770035336415\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03927531178130044\n",
      "Average test loss: 0.00376350493894683\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03586362959610091\n",
      "Average test loss: 0.004365793878833453\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03356913384464052\n",
      "Average test loss: 0.002565386950555775\n",
      "Epoch 19/300\n",
      "Average training loss: 0.031113467984729342\n",
      "Average test loss: 0.04035023256474071\n",
      "Epoch 20/300\n",
      "Average training loss: 0.029640878326363033\n",
      "Average test loss: 0.005179992088013225\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02766040487256315\n",
      "Average test loss: 0.007251571817530526\n",
      "Epoch 22/300\n",
      "Average training loss: 0.025896053794357513\n",
      "Average test loss: 0.0021668132450431586\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02497766724228859\n",
      "Average test loss: 0.0033436180580821303\n",
      "Epoch 24/300\n",
      "Average training loss: 0.023644071362084813\n",
      "Average test loss: 0.0027151204912612835\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0225440041952663\n",
      "Average test loss: 0.0023137386184599665\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021460149890846675\n",
      "Average test loss: 0.00407269021247824\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020568053202496633\n",
      "Average test loss: 0.0027383608573840726\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019831442784931926\n",
      "Average test loss: 0.0019424902422146666\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01913227575023969\n",
      "Average test loss: 0.0018387691138519182\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01856883771220843\n",
      "Average test loss: 0.0019103804941599568\n",
      "Epoch 31/300\n",
      "Average training loss: 0.018206040122442774\n",
      "Average test loss: 0.0017520144326198432\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017875904424322978\n",
      "Average test loss: 0.0027173692161838216\n",
      "Epoch 33/300\n",
      "Average training loss: 0.017490078001386588\n",
      "Average test loss: 0.004031251327859031\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017254779403408367\n",
      "Average test loss: 0.002446594229588906\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01688730952805943\n",
      "Average test loss: 0.001819862496935659\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01665445543328921\n",
      "Average test loss: 0.0017166071907720633\n",
      "Epoch 37/300\n",
      "Average training loss: 0.016392188254329892\n",
      "Average test loss: 0.006426520575251844\n",
      "Epoch 38/300\n",
      "Average training loss: 0.016182422106464703\n",
      "Average test loss: 0.002027825930673215\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016020851858788068\n",
      "Average test loss: 0.002660525026006831\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015776357140805986\n",
      "Average test loss: 0.0017121334130772287\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015565597040785684\n",
      "Average test loss: 0.0018088813845275177\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01562497289147642\n",
      "Average test loss: 0.0016596042074056136\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015327725622389052\n",
      "Average test loss: 0.0016782913516379065\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015104562669992447\n",
      "Average test loss: 0.001632743085320625\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014980216258101994\n",
      "Average test loss: 0.0016338248503290945\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014964595483409034\n",
      "Average test loss: 0.0017204363497181072\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014758050214913157\n",
      "Average test loss: 0.1707235296037462\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014576142788761192\n",
      "Average test loss: 0.0016923270300030708\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014504469011392858\n",
      "Average test loss: 0.001665488080225057\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01445814053548707\n",
      "Average test loss: 0.0016640987516277365\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014304922132028474\n",
      "Average test loss: 0.0016491148338342706\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01427602889471584\n",
      "Average test loss: 0.0017352086434451242\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014199449147615168\n",
      "Average test loss: 0.0019374062056756681\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014041971598234441\n",
      "Average test loss: 0.002326724385842681\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013960848573181365\n",
      "Average test loss: 0.001700325714941654\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013933568161394861\n",
      "Average test loss: 0.001604590018486811\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013879360780947739\n",
      "Average test loss: 0.0015890753921121359\n",
      "Epoch 58/300\n",
      "Average training loss: 0.013741633716556761\n",
      "Average test loss: 0.0016159672626397676\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013740891340706083\n",
      "Average test loss: 0.0016033145679781834\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013616385227276219\n",
      "Average test loss: 0.001805550635481874\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013588904860946866\n",
      "Average test loss: 0.0015633763678164945\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013493451510038642\n",
      "Average test loss: 0.0019404042416976558\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013512632447812293\n",
      "Average test loss: 0.001634095088682241\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013323823808795876\n",
      "Average test loss: 0.00160276159328512\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013347545459866524\n",
      "Average test loss: 0.0019036502171721724\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013258010547194216\n",
      "Average test loss: 0.001553266041705178\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013217749165991942\n",
      "Average test loss: 0.0016512687419437699\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013164732977747918\n",
      "Average test loss: 0.001567763318048997\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013104338075551721\n",
      "Average test loss: 0.001978874444961548\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013045994354618921\n",
      "Average test loss: 0.0015832514675437575\n",
      "Epoch 71/300\n",
      "Average training loss: 0.012987307138741017\n",
      "Average test loss: 0.0015683560681839783\n",
      "Epoch 72/300\n",
      "Average training loss: 0.012971367028852303\n",
      "Average test loss: 0.0016765881611241235\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012899277744193872\n",
      "Average test loss: 0.0019899481849537954\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0129093536304103\n",
      "Average test loss: 0.0018982028868049384\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01281166233205133\n",
      "Average test loss: 0.0016184977075705926\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012779262042707866\n",
      "Average test loss: 0.0015343785891309381\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012704786763423018\n",
      "Average test loss: 0.001641304156432549\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012693683858546946\n",
      "Average test loss: 0.0015940173716905216\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012684696166051758\n",
      "Average test loss: 0.0016031625545066264\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012651903819706704\n",
      "Average test loss: 0.0015460235670001971\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01258263857331541\n",
      "Average test loss: 0.0015713031395441957\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012561754003167152\n",
      "Average test loss: 0.0015989025564243397\n",
      "Epoch 83/300\n",
      "Average training loss: 0.012476019336117639\n",
      "Average test loss: 0.0017479692629228036\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012470991960002316\n",
      "Average test loss: 0.0015912316975494225\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012429048022462263\n",
      "Average test loss: 0.0016006638111753595\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01240125430126985\n",
      "Average test loss: 0.00167064689223965\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012385182208485073\n",
      "Average test loss: 0.002089340567174885\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012390110426478916\n",
      "Average test loss: 0.00156885011235459\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012292296353313657\n",
      "Average test loss: 0.0016414315754340754\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0122712627193994\n",
      "Average test loss: 0.0017151264757331874\n",
      "Epoch 91/300\n",
      "Average training loss: 0.012284711736771795\n",
      "Average test loss: 0.0015578375865912273\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012184824474155902\n",
      "Average test loss: 0.0016467987386923697\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012203142578403155\n",
      "Average test loss: 0.0016034341733902693\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012171288531687524\n",
      "Average test loss: 0.0016066101473859616\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012127211289687289\n",
      "Average test loss: 0.0016482411267028914\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012143205817374918\n",
      "Average test loss: 0.0016829471353234516\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012066491132809056\n",
      "Average test loss: 0.0016019369444499414\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012043722257018089\n",
      "Average test loss: 0.0016172088651607434\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011986851120160685\n",
      "Average test loss: 0.00181710730969078\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01199807418965631\n",
      "Average test loss: 0.001643044433866938\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011965948815147082\n",
      "Average test loss: 0.0016975065402479634\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011973071704308192\n",
      "Average test loss: 0.0018281177054676745\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011943469213942686\n",
      "Average test loss: 0.0015771479163215391\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011913814855118592\n",
      "Average test loss: 0.0019882326419982644\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011851745002799565\n",
      "Average test loss: 0.0015637795398425725\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011864781535334058\n",
      "Average test loss: 0.0018483336764491267\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011790018463300335\n",
      "Average test loss: 0.0016668500584653683\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011815602338976331\n",
      "Average test loss: 0.0016052608273716437\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011786304256982273\n",
      "Average test loss: 0.0016778424353235297\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011765580933127138\n",
      "Average test loss: 0.003955784336146381\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01173757913791471\n",
      "Average test loss: 0.00160004810606026\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011704387844022777\n",
      "Average test loss: 0.001637915596469409\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011703878654374016\n",
      "Average test loss: 0.0034982910392185053\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011655333849291006\n",
      "Average test loss: 0.0017854065251433186\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011652453809148736\n",
      "Average test loss: 0.0017664232791091005\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01162681353257762\n",
      "Average test loss: 0.0015906954229705863\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011589801715479957\n",
      "Average test loss: 0.0016164039619680909\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011621485595073965\n",
      "Average test loss: 0.0016606992555575238\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011590505943530135\n",
      "Average test loss: 0.001564601537047161\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011558359816256497\n",
      "Average test loss: 0.0017205311432480811\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0115750457652741\n",
      "Average test loss: 0.0017530066364755232\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011492840360436175\n",
      "Average test loss: 0.001633220671624359\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011504889926976628\n",
      "Average test loss: 0.0022353933786766395\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011548407141533162\n",
      "Average test loss: 0.0016019440348156624\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011499843233161503\n",
      "Average test loss: 0.0017313249418511986\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011416451593240102\n",
      "Average test loss: 0.0017294296400828493\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011427598155207105\n",
      "Average test loss: 0.0016560382712632418\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011419370000561079\n",
      "Average test loss: 0.001585100677692228\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011365300049384436\n",
      "Average test loss: 0.001862149466243055\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011433703486290242\n",
      "Average test loss: 0.0017126728810576929\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011393369500835736\n",
      "Average test loss: 0.0016172165051102638\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011350877370271418\n",
      "Average test loss: 0.0015956342491424745\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011392727182143264\n",
      "Average test loss: 0.0017145451845394242\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011304205213156011\n",
      "Average test loss: 0.002057147248958548\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011361862118873332\n",
      "Average test loss: 0.0016665422585275438\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011289872554441293\n",
      "Average test loss: 0.001627171573953496\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011277901443342368\n",
      "Average test loss: 0.0017254550551167793\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011282594987915623\n",
      "Average test loss: 0.0016505615643949971\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011270787350005574\n",
      "Average test loss: 0.0016454794522788789\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011213490801552931\n",
      "Average test loss: 0.0017805210790700382\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011205870233476162\n",
      "Average test loss: 0.0018294500208770235\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011224589188065794\n",
      "Average test loss: 0.0018811084118982157\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011196013764374785\n",
      "Average test loss: 0.0017532696379348636\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011193437150783008\n",
      "Average test loss: 0.001693965134314365\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01114972686684794\n",
      "Average test loss: 0.0017000970580718583\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011124495659437444\n",
      "Average test loss: 0.0019179986065460575\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011150883012761672\n",
      "Average test loss: 0.001676344895321462\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011142188831335968\n",
      "Average test loss: 0.0019067245878072248\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01111230788545476\n",
      "Average test loss: 0.0016067432359688813\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011087778363790777\n",
      "Average test loss: 0.0016863861224717565\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011106576330959798\n",
      "Average test loss: 0.001717023071832955\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011088795176810688\n",
      "Average test loss: 0.001725809336743421\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011060631344715754\n",
      "Average test loss: 0.0018430777147619261\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01105186321751939\n",
      "Average test loss: 0.0016704688341253333\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011060986195173528\n",
      "Average test loss: 0.0018081831450884542\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011013391933093468\n",
      "Average test loss: 0.0017787675973441866\n",
      "Epoch 157/300\n",
      "Average training loss: 0.010984005735152298\n",
      "Average test loss: 0.0017947787398265467\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011018798035879929\n",
      "Average test loss: 0.0016417970729784832\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01102374933908383\n",
      "Average test loss: 0.0021005478385421965\n",
      "Epoch 160/300\n",
      "Average training loss: 0.010991638787090778\n",
      "Average test loss: 0.0016393497987753816\n",
      "Epoch 161/300\n",
      "Average training loss: 0.010950331557956007\n",
      "Average test loss: 0.0016942703005754286\n",
      "Epoch 162/300\n",
      "Average training loss: 0.010980824654301008\n",
      "Average test loss: 0.0017627299463169443\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011011619923015435\n",
      "Average test loss: 0.0016628432172454066\n",
      "Epoch 164/300\n",
      "Average training loss: 0.010891054198973708\n",
      "Average test loss: 0.0017748496470352015\n",
      "Epoch 165/300\n",
      "Average training loss: 0.010969681425227059\n",
      "Average test loss: 0.001688828266536196\n",
      "Epoch 166/300\n",
      "Average training loss: 0.010915757459070948\n",
      "Average test loss: 0.001791467560765644\n",
      "Epoch 167/300\n",
      "Average training loss: 0.010965685502522521\n",
      "Average test loss: 0.00381924225224389\n",
      "Epoch 168/300\n",
      "Average training loss: 0.010908239492111735\n",
      "Average test loss: 0.001696980953630474\n",
      "Epoch 169/300\n",
      "Average training loss: 0.010857707486384444\n",
      "Average test loss: 0.0016833470457543929\n",
      "Epoch 170/300\n",
      "Average training loss: 0.010882630330820878\n",
      "Average test loss: 0.0017554871638615927\n",
      "Epoch 171/300\n",
      "Average training loss: 0.010880474472211467\n",
      "Average test loss: 0.001714316522392134\n",
      "Epoch 172/300\n",
      "Average training loss: 0.010868671328657204\n",
      "Average test loss: 0.0016683997958381143\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01082890526453654\n",
      "Average test loss: 0.0017812651481686366\n",
      "Epoch 174/300\n",
      "Average training loss: 0.010851285547845894\n",
      "Average test loss: 0.001714868301836153\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01080799712654617\n",
      "Average test loss: 0.0019130926873120997\n",
      "Epoch 176/300\n",
      "Average training loss: 0.010818318077259593\n",
      "Average test loss: 0.0016785878787438074\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01080988578001658\n",
      "Average test loss: 0.0016338387473175922\n",
      "Epoch 178/300\n",
      "Average training loss: 0.010785314780142573\n",
      "Average test loss: 0.0016815290178896652\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01079707522276375\n",
      "Average test loss: 0.0017361554749723937\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01078538818905751\n",
      "Average test loss: 0.0017351602599438693\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010769748006429937\n",
      "Average test loss: 0.0017157627918447057\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010801291634639104\n",
      "Average test loss: 0.0016882061527834999\n",
      "Epoch 183/300\n",
      "Average training loss: 0.010796078821851147\n",
      "Average test loss: 0.0018651656650213733\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010741857938468456\n",
      "Average test loss: 0.0018836480029341247\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010707925147480434\n",
      "Average test loss: 0.0020093583985128336\n",
      "Epoch 186/300\n",
      "Average training loss: 0.010727171936796771\n",
      "Average test loss: 0.0017720939671206806\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010779037633703815\n",
      "Average test loss: 0.0017722433204245237\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010707266261180242\n",
      "Average test loss: 0.0016524448675724367\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010715989041659567\n",
      "Average test loss: 0.0017588659681172835\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010682307086057134\n",
      "Average test loss: 0.0016482601932560404\n",
      "Epoch 191/300\n",
      "Average training loss: 0.010729331051309904\n",
      "Average test loss: 0.0017418183791968558\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010693403883940644\n",
      "Average test loss: 0.0019007558924042517\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010624964958263768\n",
      "Average test loss: 0.0016851897606005271\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010702093963821728\n",
      "Average test loss: 0.001746730558367239\n",
      "Epoch 195/300\n",
      "Average training loss: 0.010692394812901814\n",
      "Average test loss: 0.0017445897221979169\n",
      "Epoch 196/300\n",
      "Average training loss: 0.010617943363885085\n",
      "Average test loss: 0.0018572460685132278\n",
      "Epoch 197/300\n",
      "Average training loss: 0.010648604633907478\n",
      "Average test loss: 0.0018368007176452212\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010612274474567838\n",
      "Average test loss: 0.0016672836043354537\n",
      "Epoch 199/300\n",
      "Average training loss: 0.010619171609481176\n",
      "Average test loss: 0.002218202454658846\n",
      "Epoch 200/300\n",
      "Average training loss: 0.010619295649230481\n",
      "Average test loss: 0.0016561491197595994\n",
      "Epoch 201/300\n",
      "Average training loss: 0.010606725939446025\n",
      "Average test loss: 0.0017108422250797352\n",
      "Epoch 202/300\n",
      "Average training loss: 0.010585142771402994\n",
      "Average test loss: 0.0016609662891262107\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010608332879841328\n",
      "Average test loss: 0.0016976916539586252\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010649154739247428\n",
      "Average test loss: 0.0016998301330539915\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01057314827458726\n",
      "Average test loss: 0.0017794152843869395\n",
      "Epoch 206/300\n",
      "Average training loss: 0.010571633901033137\n",
      "Average test loss: 0.0017362097653870782\n",
      "Epoch 207/300\n",
      "Average training loss: 0.010570474556750722\n",
      "Average test loss: 0.0017017635258121623\n",
      "Epoch 208/300\n",
      "Average training loss: 0.010560443378157086\n",
      "Average test loss: 0.0018344898857176304\n",
      "Epoch 209/300\n",
      "Average training loss: 0.010539984789159563\n",
      "Average test loss: 0.0016581614127175676\n",
      "Epoch 210/300\n",
      "Average training loss: 0.010547383337385124\n",
      "Average test loss: 0.0018618709710426628\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01058367007639673\n",
      "Average test loss: 0.0018223162312060595\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01053467234886355\n",
      "Average test loss: 0.0017076361656395925\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01052915211353037\n",
      "Average test loss: 0.0017210258626275592\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01052886074450281\n",
      "Average test loss: 0.0017359899037724567\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010519437308940623\n",
      "Average test loss: 0.002049750074537264\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010498690591918098\n",
      "Average test loss: 0.0017821898423135282\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010517022813359896\n",
      "Average test loss: 0.0017907592723560002\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010464945154057608\n",
      "Average test loss: 0.0016550974971097376\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010504543369015058\n",
      "Average test loss: 0.001821755881110827\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01046258742113908\n",
      "Average test loss: 0.0019732941993408732\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010521362793942293\n",
      "Average test loss: 0.0016653345713598861\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010447004404332903\n",
      "Average test loss: 0.0018079239041027095\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010455846400724517\n",
      "Average test loss: 0.001758524935071667\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010465257382227314\n",
      "Average test loss: 0.0017258300836094551\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010437744532194402\n",
      "Average test loss: 0.0018442897911493978\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010413514918751187\n",
      "Average test loss: 0.0016896529552630252\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01046665377335416\n",
      "Average test loss: 0.0020419646890627013\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010455649219453335\n",
      "Average test loss: 0.0017170455485789311\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010389107229395044\n",
      "Average test loss: 0.001795282955177956\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010414513972898325\n",
      "Average test loss: 0.0019002957555154958\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010408582185705503\n",
      "Average test loss: 0.0018876008777361777\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010418952828480137\n",
      "Average test loss: 0.001692491100066238\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010437274128198623\n",
      "Average test loss: 0.0018129496475060782\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010387628276315\n",
      "Average test loss: 0.0017147646811273363\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010401493844058779\n",
      "Average test loss: 0.0017460889139523108\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010340266869299942\n",
      "Average test loss: 0.0018232473477514254\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010375023651454185\n",
      "Average test loss: 0.0017335360654526287\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010377751044101186\n",
      "Average test loss: 0.001926749427181979\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010372380122542381\n",
      "Average test loss: 0.0019413879062566493\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010354685054471095\n",
      "Average test loss: 0.001700258757505152\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01035176363380419\n",
      "Average test loss: 0.0017149563416217763\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010375310106823842\n",
      "Average test loss: 0.0017586307825727596\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010327513009309768\n",
      "Average test loss: 0.0017421820565230316\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01035070971896251\n",
      "Average test loss: 0.0017168094316083525\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010302602581679822\n",
      "Average test loss: 0.001811232953125404\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010357580543392234\n",
      "Average test loss: 0.0017151180029743248\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010331156623032358\n",
      "Average test loss: 0.00171125800597171\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01032305014712943\n",
      "Average test loss: 0.0017606660225946043\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010273248348799016\n",
      "Average test loss: 0.0018077265055229266\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0102619609153933\n",
      "Average test loss: 0.0017565777608089977\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010286879325906436\n",
      "Average test loss: 0.001808393976547652\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01030969650298357\n",
      "Average test loss: 0.0018426826608677704\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010302917819884088\n",
      "Average test loss: 0.0017545033833011984\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010283411290082667\n",
      "Average test loss: 0.0017796877372699478\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010261768242551221\n",
      "Average test loss: 0.0017365551322905553\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010263745377461116\n",
      "Average test loss: 0.001827357465504772\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010278160116738743\n",
      "Average test loss: 0.0018949385432319508\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010267704331212574\n",
      "Average test loss: 0.0017748952941555117\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010262100613779492\n",
      "Average test loss: 0.0018647210283411874\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01024308273361789\n",
      "Average test loss: 0.0017350273744927511\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010242941694955031\n",
      "Average test loss: 0.0017172334359751807\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010287624795403745\n",
      "Average test loss: 0.0017317844467858474\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01022736593708396\n",
      "Average test loss: 0.001785635991435912\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010245626529885662\n",
      "Average test loss: 0.0017795585304912593\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010234731000330713\n",
      "Average test loss: 0.0018164585811189478\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010229369466503461\n",
      "Average test loss: 0.0017529387320909236\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010234994879199398\n",
      "Average test loss: 0.0017514463145699767\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010217619800733196\n",
      "Average test loss: 0.001795439791555206\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01021704473429256\n",
      "Average test loss: 0.002028406632443269\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01019055184804731\n",
      "Average test loss: 0.0018358996105897758\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010190863757497734\n",
      "Average test loss: 0.0017834439772284694\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01024069991790586\n",
      "Average test loss: 0.0017716461600114902\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01017005786465274\n",
      "Average test loss: 0.0017090399594356616\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010192148490912385\n",
      "Average test loss: 0.001760542681440711\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010170019132395585\n",
      "Average test loss: 0.001712675883124272\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01020982435759571\n",
      "Average test loss: 0.001881766082925929\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010164845017923248\n",
      "Average test loss: 0.0017864403240382672\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010177173623608218\n",
      "Average test loss: 0.0018109649144526984\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010168789003458288\n",
      "Average test loss: 0.0019823274511016078\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010154292749861876\n",
      "Average test loss: 0.0017391405618853038\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01014102562268575\n",
      "Average test loss: 0.00181627047724194\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010166259141017993\n",
      "Average test loss: 0.0018483739628766974\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010131267902751763\n",
      "Average test loss: 0.0018255380143721899\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01017796199520429\n",
      "Average test loss: 0.001739447268553906\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010145376875168748\n",
      "Average test loss: 0.0018523154897201392\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010158688531153731\n",
      "Average test loss: 0.0017459793300885294\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01009634339147144\n",
      "Average test loss: 0.0017922042635165982\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01012174858070082\n",
      "Average test loss: 0.0017611939477630788\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01010746263464292\n",
      "Average test loss: 0.001715995921028985\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010128204211592674\n",
      "Average test loss: 0.001780551190695001\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010126729789707396\n",
      "Average test loss: 0.0018677758423404562\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010119922793573804\n",
      "Average test loss: 0.0018270834370826682\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010100824797319042\n",
      "Average test loss: 0.001811871666047308\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010127859288205704\n",
      "Average test loss: 0.001848090780278047\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01010496779365672\n",
      "Average test loss: 0.001827179504972365\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010090455307728714\n",
      "Average test loss: 0.0017480658813276225\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010196939871542984\n",
      "Average test loss: 0.0019430905959258477\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010055226931969325\n",
      "Average test loss: 0.0017841895119183593\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010047716850207911\n",
      "Average test loss: 0.0018646431795010963\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010103215384814474\n",
      "Average test loss: 0.0017952558288557663\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_50_Depth15/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.22\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.84\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.99\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.38\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.14\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.47\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.914157444424099\n",
      "Average test loss: 0.026315841678116056\n",
      "Epoch 2/300\n",
      "Average training loss: 7.2947554859585235\n",
      "Average test loss: 0.022015089452266694\n",
      "Epoch 3/300\n",
      "Average training loss: 6.319058312733968\n",
      "Average test loss: 0.02265446564058463\n",
      "Epoch 4/300\n",
      "Average training loss: 5.725391022152371\n",
      "Average test loss: 0.01625961933202214\n",
      "Epoch 5/300\n",
      "Average training loss: 5.2914383515252\n",
      "Average test loss: 61.86460879834493\n",
      "Epoch 6/300\n",
      "Average training loss: 4.8817454490661625\n",
      "Average test loss: 0.26701856321758694\n",
      "Epoch 7/300\n",
      "Average training loss: 4.5366599265204535\n",
      "Average test loss: 0.0137095759883523\n",
      "Epoch 8/300\n",
      "Average training loss: 4.2150373395284015\n",
      "Average test loss: 1.1386487545304829\n",
      "Epoch 9/300\n",
      "Average training loss: 3.9170149879455565\n",
      "Average test loss: 0.028046112295654086\n",
      "Epoch 10/300\n",
      "Average training loss: 3.6474458476172553\n",
      "Average test loss: 0.38623620563497146\n",
      "Epoch 11/300\n",
      "Average training loss: 3.393631697760688\n",
      "Average test loss: 0.009122134392046266\n",
      "Epoch 12/300\n",
      "Average training loss: 3.163683287302653\n",
      "Average test loss: 0.008770039459897412\n",
      "Epoch 13/300\n",
      "Average training loss: 2.958871809217665\n",
      "Average test loss: 0.02400809507899814\n",
      "Epoch 14/300\n",
      "Average training loss: 2.762866482840644\n",
      "Average test loss: 0.013042565927737289\n",
      "Epoch 15/300\n",
      "Average training loss: 2.5796299347347684\n",
      "Average test loss: 0.009888706107934317\n",
      "Epoch 16/300\n",
      "Average training loss: 2.4042644738091363\n",
      "Average test loss: 0.009806345712807443\n",
      "Epoch 17/300\n",
      "Average training loss: 2.2443532644907633\n",
      "Average test loss: 0.025121726222750215\n",
      "Epoch 18/300\n",
      "Average training loss: 2.099549394183689\n",
      "Average test loss: 0.06370221135558353\n",
      "Epoch 19/300\n",
      "Average training loss: 1.9719963393741184\n",
      "Average test loss: 0.017821776098675197\n",
      "Epoch 20/300\n",
      "Average training loss: 1.848208638827006\n",
      "Average test loss: 0.007829406079318788\n",
      "Epoch 21/300\n",
      "Average training loss: 1.7276630634731716\n",
      "Average test loss: 0.0073551592425339755\n",
      "Epoch 22/300\n",
      "Average training loss: 1.6066784868240356\n",
      "Average test loss: 0.008103841570930349\n",
      "Epoch 23/300\n",
      "Average training loss: 1.499896500799391\n",
      "Average test loss: 0.013290895442167917\n",
      "Epoch 24/300\n",
      "Average training loss: 1.4008421108457778\n",
      "Average test loss: 0.030041861853665777\n",
      "Epoch 25/300\n",
      "Average training loss: 1.3043650302886962\n",
      "Average test loss: 0.0073696838000582325\n",
      "Epoch 26/300\n",
      "Average training loss: 1.2060503323872884\n",
      "Average test loss: 0.015599575224849912\n",
      "Epoch 27/300\n",
      "Average training loss: 1.1214741259680854\n",
      "Average test loss: 0.017506131110919847\n",
      "Epoch 28/300\n",
      "Average training loss: 1.0370527568923102\n",
      "Average test loss: 0.006235198091715574\n",
      "Epoch 29/300\n",
      "Average training loss: 0.9566886788474189\n",
      "Average test loss: 0.006984054412278864\n",
      "Epoch 30/300\n",
      "Average training loss: 0.8832505484157138\n",
      "Average test loss: 0.03924203582149413\n",
      "Epoch 31/300\n",
      "Average training loss: 0.8136160580317179\n",
      "Average test loss: 0.0063310508314106196\n",
      "Epoch 32/300\n",
      "Average training loss: 0.747591719309489\n",
      "Average test loss: 0.008100980362751418\n",
      "Epoch 33/300\n",
      "Average training loss: 0.6869846482806735\n",
      "Average test loss: 0.006516423214640883\n",
      "Epoch 34/300\n",
      "Average training loss: 0.6276831403838263\n",
      "Average test loss: 0.0056749407787703805\n",
      "Epoch 35/300\n",
      "Average training loss: 0.5762903304100037\n",
      "Average test loss: 0.011365965181133813\n",
      "Epoch 36/300\n",
      "Average training loss: 0.5277054635153876\n",
      "Average test loss: 0.006943652893933985\n",
      "Epoch 37/300\n",
      "Average training loss: 0.4845426414542728\n",
      "Average test loss: 1.0602085649834738\n",
      "Epoch 38/300\n",
      "Average training loss: 0.44724938554233973\n",
      "Average test loss: 0.005665802544189824\n",
      "Epoch 39/300\n",
      "Average training loss: 0.4108183545006646\n",
      "Average test loss: 0.03464031917187903\n",
      "Epoch 40/300\n",
      "Average training loss: 0.37877866305245295\n",
      "Average test loss: 0.006648199707269669\n",
      "Epoch 41/300\n",
      "Average training loss: 0.34880147102144027\n",
      "Average test loss: 0.006067080174055365\n",
      "Epoch 42/300\n",
      "Average training loss: 0.3220123709572686\n",
      "Average test loss: 0.005798329878184531\n",
      "Epoch 43/300\n",
      "Average training loss: 0.2966845698356628\n",
      "Average test loss: 0.4370071817437808\n",
      "Epoch 44/300\n",
      "Average training loss: 0.2706464728381899\n",
      "Average test loss: 0.005373202207394772\n",
      "Epoch 45/300\n",
      "Average training loss: 0.24773788412412007\n",
      "Average test loss: 0.014350912276655435\n",
      "Epoch 46/300\n",
      "Average training loss: 0.22722566368844774\n",
      "Average test loss: 0.005579946686824163\n",
      "Epoch 47/300\n",
      "Average training loss: 0.21004025273852878\n",
      "Average test loss: 0.007937318147884474\n",
      "Epoch 48/300\n",
      "Average training loss: 0.19612999783621893\n",
      "Average test loss: 0.005519170128222969\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1830534918838077\n",
      "Average test loss: 0.005271848089993\n",
      "Epoch 50/300\n",
      "Average training loss: 0.17320225948757595\n",
      "Average test loss: 0.005512726860741775\n",
      "Epoch 51/300\n",
      "Average training loss: 0.16541612270143297\n",
      "Average test loss: 0.00907033450777332\n",
      "Epoch 52/300\n",
      "Average training loss: 0.15968928666909535\n",
      "Average test loss: 0.17166040707503755\n",
      "Epoch 53/300\n",
      "Average training loss: 0.15346513044834137\n",
      "Average test loss: 0.006990978605217404\n",
      "Epoch 54/300\n",
      "Average training loss: 0.14739693621794384\n",
      "Average test loss: 0.005788308013644483\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1436402795712153\n",
      "Average test loss: 0.005886255854947699\n",
      "Epoch 56/300\n",
      "Average training loss: 0.13904275212022993\n",
      "Average test loss: 0.012222525351577335\n",
      "Epoch 57/300\n",
      "Average training loss: 0.13701914840274387\n",
      "Average test loss: 0.005523472759872675\n",
      "Epoch 58/300\n",
      "Average training loss: 0.13187804544634288\n",
      "Average test loss: 0.005436062042911847\n",
      "Epoch 59/300\n",
      "Average training loss: 0.12960027656290266\n",
      "Average test loss: 0.00519594170153141\n",
      "Epoch 60/300\n",
      "Average training loss: 0.12567536736859217\n",
      "Average test loss: 0.005477091546273894\n",
      "Epoch 61/300\n",
      "Average training loss: 0.12262343638473087\n",
      "Average test loss: 0.0301410182317098\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11921274072594112\n",
      "Average test loss: 0.005122571184817288\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1160380549563302\n",
      "Average test loss: 0.005228415612959199\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11270969240532981\n",
      "Average test loss: 0.005144761620296373\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11032798749870724\n",
      "Average test loss: 0.005143244858417246\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10875621238019731\n",
      "Average test loss: 0.005210785244074132\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10556426089339786\n",
      "Average test loss: 0.005229719468288952\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10383946135309008\n",
      "Average test loss: 0.005387172489530511\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10193596767716938\n",
      "Average test loss: 0.005655345281793012\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09998237596617805\n",
      "Average test loss: 0.008949889127579\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0984476043979327\n",
      "Average test loss: 0.0050057902915610205\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09687906418906318\n",
      "Average test loss: 0.00514359287213948\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09533762558963564\n",
      "Average test loss: 0.13818301980487174\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0939144348303477\n",
      "Average test loss: 0.007966398380696774\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09303449834717645\n",
      "Average test loss: 0.005758839867181248\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09144360566801495\n",
      "Average test loss: 0.005194475480251842\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09097823117838966\n",
      "Average test loss: 51.08531415430705\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09035458101828893\n",
      "Average test loss: 0.010639113177441888\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08891382624705632\n",
      "Average test loss: 0.005273654678422544\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08821554433637195\n",
      "Average test loss: 0.005134696654561493\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08716627412372165\n",
      "Average test loss: 0.005599793160955111\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08666342017385695\n",
      "Average test loss: 0.005057467854271332\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08578720067607032\n",
      "Average test loss: 0.005040953156641788\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08540538183848063\n",
      "Average test loss: 0.00512391599929995\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08438885191082954\n",
      "Average test loss: 0.005162406517813603\n",
      "Epoch 86/300\n",
      "Average training loss: 1876.8216459343168\n",
      "Average test loss: 42.8199176987542\n",
      "Epoch 87/300\n",
      "Average training loss: 23.65671380360921\n",
      "Average test loss: 0.5746001067691379\n",
      "Epoch 88/300\n",
      "Average training loss: 14.69600606367323\n",
      "Average test loss: 2006.6779896982246\n",
      "Epoch 89/300\n",
      "Average training loss: 12.96292182498508\n",
      "Average test loss: 0.02491226581070158\n",
      "Epoch 90/300\n",
      "Average training loss: 11.654377170138888\n",
      "Average test loss: 3.5997533121092453\n",
      "Epoch 91/300\n",
      "Average training loss: 10.455249687194824\n",
      "Average test loss: 0.043779483397801716\n",
      "Epoch 92/300\n",
      "Average training loss: 9.326953689575195\n",
      "Average test loss: 0.1086907584534751\n",
      "Epoch 93/300\n",
      "Average training loss: 8.287534202575683\n",
      "Average test loss: 0.012465018722746107\n",
      "Epoch 94/300\n",
      "Average training loss: 7.334064312405056\n",
      "Average test loss: 0.012227164769338238\n",
      "Epoch 95/300\n",
      "Average training loss: 6.4342317093743215\n",
      "Average test loss: 0.029140429087811046\n",
      "Epoch 96/300\n",
      "Average training loss: 5.606477596706815\n",
      "Average test loss: 0.010406678744488293\n",
      "Epoch 97/300\n",
      "Average training loss: 4.854798154619005\n",
      "Average test loss: 0.00943224237445328\n",
      "Epoch 98/300\n",
      "Average training loss: 4.10836375257704\n",
      "Average test loss: 0.009034794900152418\n",
      "Epoch 99/300\n",
      "Average training loss: 3.467470649931166\n",
      "Average test loss: 0.008481429516441293\n",
      "Epoch 100/300\n",
      "Average training loss: 2.92080776807997\n",
      "Average test loss: 0.010258545378843943\n",
      "Epoch 101/300\n",
      "Average training loss: 2.4630467211405436\n",
      "Average test loss: 6.7367225613594055\n",
      "Epoch 102/300\n",
      "Average training loss: 2.1052388145658707\n",
      "Average test loss: 0.007684930995934539\n",
      "Epoch 103/300\n",
      "Average training loss: 1.8117039521535239\n",
      "Average test loss: 0.03589426390247213\n",
      "Epoch 104/300\n",
      "Average training loss: 1.5519708569844564\n",
      "Average test loss: 0.007469498169091013\n",
      "Epoch 105/300\n",
      "Average training loss: 1.3138734331130981\n",
      "Average test loss: 0.0073114560089177555\n",
      "Epoch 106/300\n",
      "Average training loss: 1.092501167456309\n",
      "Average test loss: 0.006680774238788419\n",
      "Epoch 107/300\n",
      "Average training loss: 0.8959503967497083\n",
      "Average test loss: 0.006710128751065996\n",
      "Epoch 108/300\n",
      "Average training loss: 0.7308305039405822\n",
      "Average test loss: 0.008722110314501657\n",
      "Epoch 109/300\n",
      "Average training loss: 0.5924088499281142\n",
      "Average test loss: 0.006328061398532656\n",
      "Epoch 110/300\n",
      "Average training loss: 0.4644909009138743\n",
      "Average test loss: 0.006076887662212054\n",
      "Epoch 111/300\n",
      "Average training loss: 0.3496526433361901\n",
      "Average test loss: 0.010048816883729564\n",
      "Epoch 112/300\n",
      "Average training loss: 0.27658465809292265\n",
      "Average test loss: 1.083684632708629\n",
      "Epoch 113/300\n",
      "Average training loss: 0.23311025758584342\n",
      "Average test loss: 0.0201084796604183\n",
      "Epoch 114/300\n",
      "Average training loss: 0.20756219209565058\n",
      "Average test loss: 80.98980876275732\n",
      "Epoch 115/300\n",
      "Average training loss: 0.19010895217789545\n",
      "Average test loss: 0.09757445416516727\n",
      "Epoch 116/300\n",
      "Average training loss: 0.17817449245187972\n",
      "Average test loss: 12942.03098590766\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1683900663057963\n",
      "Average test loss: 0.8758685645047162\n",
      "Epoch 118/300\n",
      "Average training loss: 0.16023122696081799\n",
      "Average test loss: 0.007460776864240567\n",
      "Epoch 119/300\n",
      "Average training loss: 0.15260024889972476\n",
      "Average test loss: 0.30766056370238465\n",
      "Epoch 120/300\n",
      "Average training loss: 0.14643854439258575\n",
      "Average test loss: 0.0822818504696091\n",
      "Epoch 121/300\n",
      "Average training loss: 0.13301189960373772\n",
      "Average test loss: 0.005282364245090219\n",
      "Epoch 123/300\n",
      "Average training loss: 0.12740640110439724\n",
      "Average test loss: 0.007627450849446986\n",
      "Epoch 124/300\n",
      "Average training loss: 0.12224409276909298\n",
      "Average test loss: 0.005287061592563987\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11810977103975084\n",
      "Average test loss: 0.006372969188210037\n",
      "Epoch 126/300\n",
      "Average training loss: 0.11367378576596578\n",
      "Average test loss: 0.005174678898106019\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11066865593857235\n",
      "Average test loss: 0.005768819170693556\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10781079944637087\n",
      "Average test loss: 0.005229942811032136\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10493715920713213\n",
      "Average test loss: 0.006296698176198536\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10255115396446651\n",
      "Average test loss: 0.005333507903127207\n",
      "Epoch 131/300\n",
      "Average training loss: 0.1006419127980868\n",
      "Average test loss: 0.006020333311210076\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09883757699198192\n",
      "Average test loss: 0.008236275872422589\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09784752265943421\n",
      "Average test loss: 0.0052752149659726355\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09581261294417912\n",
      "Average test loss: 0.005618403918627236\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09449514714214537\n",
      "Average test loss: 0.005470830213278532\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09369614415036308\n",
      "Average test loss: 0.007761973685688443\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09157853437132306\n",
      "Average test loss: 0.016841025426983833\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0914508449236552\n",
      "Average test loss: 0.013709439989593293\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08904134462277094\n",
      "Average test loss: 0.005445775335447656\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08971391083796819\n",
      "Average test loss: 0.028381287129388914\n",
      "Epoch 142/300\n",
      "Average training loss: 0.08926590032709969\n",
      "Average test loss: 0.005221874263137579\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08699630104170905\n",
      "Average test loss: 0.005758505813777447\n",
      "Epoch 144/300\n",
      "Average training loss: 0.085905262960328\n",
      "Average test loss: 0.31270694964047935\n",
      "Epoch 145/300\n",
      "Average training loss: 0.08679481173886193\n",
      "Average test loss: 0.007366612585054504\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08537071990304523\n",
      "Average test loss: 0.005601207072950072\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08486093576086892\n",
      "Average test loss: 0.00515070327412751\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08661739785141415\n",
      "Average test loss: 0.00761404764196939\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08953937916623221\n",
      "Average test loss: 0.005429390321589178\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08386938482522964\n",
      "Average test loss: 0.005183968949649069\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08208209780852\n",
      "Average test loss: 0.005547060613003042\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08179934795035257\n",
      "Average test loss: 0.005315402793801493\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08138638381825553\n",
      "Average test loss: 0.01987822024193075\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08069848291741477\n",
      "Average test loss: 0.006167127308332258\n",
      "Epoch 155/300\n",
      "Average training loss: 133.65400950330496\n",
      "Average test loss: 27327.70354398401\n",
      "Epoch 156/300\n",
      "Average training loss: 17.94064741007487\n",
      "Average test loss: 518.0523686746757\n",
      "Epoch 157/300\n",
      "Average training loss: 14.691044611612956\n",
      "Average test loss: 16985.568399638054\n",
      "Epoch 158/300\n",
      "Average training loss: 12.14414372253418\n",
      "Average test loss: 580851.6555621069\n",
      "Epoch 159/300\n",
      "Average training loss: 7.038106930626763\n",
      "Average test loss: 0.011753751408722665\n",
      "Epoch 162/300\n",
      "Average training loss: 5.866860434638129\n",
      "Average test loss: 0.010673133739994631\n",
      "Epoch 163/300\n",
      "Average training loss: 4.88710882780287\n",
      "Average test loss: 0.009338430542084906\n",
      "Epoch 164/300\n",
      "Average training loss: 4.048939893510607\n",
      "Average test loss: 0.009108383071919282\n",
      "Epoch 165/300\n",
      "Average training loss: 3.304828846613566\n",
      "Average test loss: 0.009013542732430829\n",
      "Epoch 166/300\n",
      "Average training loss: 2.6518082538180883\n",
      "Average test loss: 0.008292420686119132\n",
      "Epoch 167/300\n",
      "Average training loss: 2.1200456097920735\n",
      "Average test loss: 0.00817344140758117\n",
      "Epoch 168/300\n",
      "Average training loss: 1.6785042213863797\n",
      "Average test loss: 0.026575474174900186\n",
      "Epoch 169/300\n",
      "Average training loss: 1.3160711320241292\n",
      "Average test loss: 0.007809701121515698\n",
      "Epoch 170/300\n",
      "Average training loss: 1.0299208388328551\n",
      "Average test loss: 0.007210436738199658\n",
      "Epoch 171/300\n",
      "Average training loss: 0.6609277113278706\n",
      "Average test loss: 0.006778802531047\n",
      "Epoch 173/300\n",
      "Average training loss: 0.5484399455388387\n",
      "Average test loss: 0.008286133631236023\n",
      "Epoch 174/300\n",
      "Average training loss: 0.46486246588495045\n",
      "Average test loss: 0.0065619757423798246\n",
      "Epoch 175/300\n",
      "Average training loss: 0.3984679591390822\n",
      "Average test loss: 0.006368625205424097\n",
      "Epoch 176/300\n",
      "Average training loss: 0.3447351293034024\n",
      "Average test loss: 0.006338776095459859\n",
      "Epoch 177/300\n",
      "Average training loss: 0.30088821795251636\n",
      "Average test loss: 0.006063801115999619\n",
      "Epoch 178/300\n",
      "Average training loss: 0.2640415356689029\n",
      "Average test loss: 0.0066716429516673085\n",
      "Epoch 179/300\n",
      "Average training loss: 0.23453181301222908\n",
      "Average test loss: 0.0061390542077521485\n",
      "Epoch 180/300\n",
      "Average training loss: 0.2095590905745824\n",
      "Average test loss: 0.005854124078734054\n",
      "Epoch 181/300\n",
      "Average training loss: 0.18553898032506308\n",
      "Average test loss: 0.005856683987710211\n",
      "Epoch 182/300\n",
      "Average training loss: 0.17056883499357436\n",
      "Average test loss: 0.005784661252465513\n",
      "Epoch 183/300\n",
      "Average training loss: 0.14123429645432367\n",
      "Average test loss: 0.005529639252771934\n",
      "Epoch 186/300\n",
      "Average training loss: 0.13513112245665657\n",
      "Average test loss: 0.005359041729321083\n",
      "Epoch 187/300\n",
      "Average training loss: 0.1292599709563785\n",
      "Average test loss: 0.0057245509214699265\n",
      "Epoch 188/300\n",
      "Average training loss: 0.12461762228276994\n",
      "Average test loss: 0.0057577781329552335\n",
      "Epoch 189/300\n",
      "Average training loss: 0.11994513624244266\n",
      "Average test loss: 0.006006566841155291\n",
      "Epoch 190/300\n",
      "Average training loss: 0.11567073506116866\n",
      "Average test loss: 0.006091797119213475\n",
      "Epoch 191/300\n",
      "Average training loss: 0.11237332363261117\n",
      "Average test loss: 0.005278226200077269\n",
      "Epoch 192/300\n",
      "Average training loss: 0.10911768645048142\n",
      "Average test loss: 0.005329060163969795\n",
      "Epoch 193/300\n",
      "Average training loss: 0.10522112921873729\n",
      "Average test loss: 0.006441373085810079\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0997369286749098\n",
      "Average test loss: 0.005099962249398232\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09753540217876434\n",
      "Average test loss: 0.006180967352456517\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09534391035636267\n",
      "Average test loss: 0.005297907910413212\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0936262301272816\n",
      "Average test loss: 0.00512233391073015\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09242597554127376\n",
      "Average test loss: 0.005215008966624737\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09072303516997231\n",
      "Average test loss: 0.005421491172164679\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08964148985015022\n",
      "Average test loss: 0.005592210760133134\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0884283275604248\n",
      "Average test loss: 0.0051184559427201745\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08803129242195024\n",
      "Average test loss: 0.005376741970578829\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08683628224001991\n",
      "Average test loss: 0.0057335531984766324\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08576069390111499\n",
      "Average test loss: 0.0053517195449935065\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0847458308007982\n",
      "Average test loss: 1.214380171991057\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08481359001662996\n",
      "Average test loss: 0.0052585730184283525\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08385252822107739\n",
      "Average test loss: 0.005391427098462979\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08272015155686273\n",
      "Average test loss: 0.00521496294811368\n",
      "Epoch 210/300\n",
      "Average training loss: 0.082835372461213\n",
      "Average test loss: 0.005547275349911716\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08171366860469183\n",
      "Average test loss: 0.005208297317537169\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08072326486640506\n",
      "Average test loss: 0.005251491010189057\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08021027022600175\n",
      "Average test loss: 0.0201618580793341\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07972806531853147\n",
      "Average test loss: 0.005513874813086457\n",
      "Epoch 215/300\n",
      "Average training loss: 18.853508824666342\n",
      "Average test loss: 0.06979404121968481\n",
      "Epoch 218/300\n",
      "Average training loss: 15.79210225168864\n",
      "Average test loss: 0.041056296918127275\n",
      "Epoch 219/300\n",
      "Average training loss: 13.440778077867297\n",
      "Average test loss: 0.033998742140001724\n",
      "Epoch 220/300\n",
      "Average training loss: 11.59060240342882\n",
      "Average test loss: 0.025701362853248914\n",
      "Epoch 221/300\n",
      "Average training loss: 10.14059033033583\n",
      "Average test loss: 0.020937532580561108\n",
      "Epoch 222/300\n",
      "Average training loss: 9.025713274637859\n",
      "Average test loss: 0.029368735863102808\n",
      "Epoch 223/300\n",
      "Average training loss: 8.030000270843505\n",
      "Average test loss: 0.30511288244856727\n",
      "Epoch 224/300\n",
      "Average training loss: 7.148695458306206\n",
      "Average test loss: 0.014870423773096667\n",
      "Epoch 225/300\n",
      "Average training loss: 6.337322148640951\n",
      "Average test loss: 0.013433121715982755\n",
      "Epoch 226/300\n",
      "Average training loss: 5.571183309343126\n",
      "Average test loss: 0.013512405666212242\n",
      "Epoch 227/300\n",
      "Average training loss: 4.851659606085883\n",
      "Average test loss: 0.011045142975946267\n",
      "Epoch 228/300\n",
      "Average training loss: 4.164676068200006\n",
      "Average test loss: 0.011378517414960597\n",
      "Epoch 229/300\n",
      "Average training loss: 3.5128811384836833\n",
      "Average test loss: 4.479417827622758\n",
      "Epoch 230/300\n",
      "Average training loss: 2.9098973195817734\n",
      "Average test loss: 0.013726617753505707\n",
      "Epoch 231/300\n",
      "Average training loss: 2.3913926673465307\n",
      "Average test loss: 0.3476459879643387\n",
      "Epoch 232/300\n",
      "Average training loss: 1.9896137095557318\n",
      "Average test loss: 0.666682388074696\n",
      "Epoch 233/300\n",
      "Average training loss: 1.660285161442227\n",
      "Average test loss: 0.42320311012864115\n",
      "Epoch 234/300\n",
      "Average training loss: 335.29190539190506\n",
      "Average test loss: 12.137239705456627\n",
      "Epoch 235/300\n",
      "Average training loss: 14.62523193105062\n",
      "Average test loss: 0.11939715473850568\n",
      "Epoch 237/300\n",
      "Average training loss: 11.643063635932075\n",
      "Average test loss: 0.1620058852210641\n",
      "Epoch 238/300\n",
      "Average training loss: 9.474619670444065\n",
      "Average test loss: 0.013352210861941178\n",
      "Epoch 239/300\n",
      "Average training loss: 7.8164174465603296\n",
      "Average test loss: 0.012166192424380117\n",
      "Epoch 240/300\n",
      "Average training loss: 45.597725405375165\n",
      "Average test loss: 0.0785262429912885\n",
      "Epoch 241/300\n",
      "Average training loss: 13.216605269538032\n",
      "Average test loss: 0.016015258178942733\n",
      "Epoch 242/300\n",
      "Average training loss: 9.367533874511718\n",
      "Average test loss: 0.19320938448028432\n",
      "Epoch 243/300\n",
      "Average training loss: 7.000118986341688\n",
      "Average test loss: 21.705834424323506\n",
      "Epoch 244/300\n",
      "Average training loss: 5.215922030131022\n",
      "Average test loss: 0.010573534612854322\n",
      "Epoch 245/300\n",
      "Average training loss: 2.978270370059543\n",
      "Average test loss: 0.009113837767806318\n",
      "Epoch 247/300\n",
      "Average training loss: 2.332970301310221\n",
      "Average test loss: 0.00942551005217764\n",
      "Epoch 248/300\n",
      "Average training loss: 1.8879589545991686\n",
      "Average test loss: 0.009796804238938623\n",
      "Epoch 249/300\n",
      "Average training loss: 1.5629507126278348\n",
      "Average test loss: 0.04702283612887065\n",
      "Epoch 250/300\n",
      "Average training loss: 1.3237615127563476\n",
      "Average test loss: 0.01215886276298099\n",
      "Epoch 251/300\n",
      "Average training loss: 1.1326138992309571\n",
      "Average test loss: 202.35976145447626\n",
      "Epoch 252/300\n",
      "Average training loss: 0.9667775409486559\n",
      "Average test loss: 0.012457919663853116\n",
      "Epoch 253/300\n",
      "Average training loss: 0.8237501273685032\n",
      "Average test loss: 241.4761321588026\n",
      "Epoch 254/300\n",
      "Average training loss: 0.6997669727537367\n",
      "Average test loss: 0.08280769306586848\n",
      "Epoch 255/300\n",
      "Average training loss: 0.5891994032329984\n",
      "Average test loss: 4.052147900856204\n",
      "Epoch 256/300\n",
      "Average training loss: 27.658467403835722\n",
      "Average test loss: 0.9296703893740972\n",
      "Epoch 258/300\n",
      "Average training loss: 18.37541417948405\n",
      "Average test loss: 0.19295792194538647\n",
      "Epoch 259/300\n",
      "Average training loss: 14.581646681891547\n",
      "Average test loss: 2.6622570057710013\n",
      "Epoch 260/300\n",
      "Average training loss: 11.615132954067654\n",
      "Average test loss: 11.678183314270443\n",
      "Epoch 261/300\n",
      "Average training loss: 9.683469131469726\n",
      "Average test loss: 0.893283261720505\n",
      "Epoch 262/300\n",
      "Average training loss: 8.396341204325358\n",
      "Average test loss: 58.25696243659655\n",
      "Epoch 263/300\n",
      "Average training loss: 7.427190678066678\n",
      "Average test loss: 0.1085994388129976\n",
      "Epoch 264/300\n",
      "Average training loss: 6.538246618058946\n",
      "Average test loss: 0.5606295528055893\n",
      "Epoch 265/300\n",
      "Average training loss: 5.09082123904758\n",
      "Average test loss: 0.025968106079432698\n",
      "Epoch 267/300\n",
      "Average training loss: 4.464549822065566\n",
      "Average test loss: 0.12497082970705298\n",
      "Epoch 268/300\n",
      "Average training loss: 3.868018480088976\n",
      "Average test loss: 0.03847822850280338\n",
      "Epoch 269/300\n",
      "Average training loss: 3.3031775641971164\n",
      "Average test loss: 0.04420726005691621\n",
      "Epoch 270/300\n",
      "Average training loss: 2.7360734776390925\n",
      "Average test loss: 244.38017480723065\n",
      "Epoch 271/300\n",
      "Average training loss: 2.199718091117011\n",
      "Average test loss: 0.019095364734116528\n",
      "Epoch 272/300\n",
      "Average training loss: 1.7169844617843628\n",
      "Average test loss: 0.1220361636976401\n",
      "Epoch 273/300\n",
      "Average training loss: 1.3320046274397108\n",
      "Average test loss: 2.3070830755010245\n",
      "Epoch 274/300\n",
      "Average training loss: 1.0414118611547682\n",
      "Average test loss: 0.08030135775357485\n",
      "Epoch 275/300\n",
      "Average training loss: 0.831009948571523\n",
      "Average test loss: 0.08400423926528958\n",
      "Epoch 276/300\n",
      "Average training loss: 0.6809511071311103\n",
      "Average test loss: 323.96824886559114\n",
      "Epoch 277/300\n",
      "Average training loss: 0.5650154389805264\n",
      "Average test loss: 0.07644800030274523\n",
      "Epoch 278/300\n",
      "Average training loss: 0.4841648136774699\n",
      "Average test loss: 232.75860003352827\n",
      "Epoch 279/300\n",
      "Average training loss: 0.4260110615624322\n",
      "Average test loss: 10.797667522652281\n",
      "Epoch 280/300\n",
      "Average training loss: 0.3807321783701579\n",
      "Average test loss: 0.09205803322253955\n",
      "Epoch 281/300\n",
      "Average training loss: 0.33772187836964923\n",
      "Average test loss: 697.7744853895406\n",
      "Epoch 282/300\n",
      "Average training loss: 0.28677832582261825\n",
      "Average test loss: 95886.3818425668\n",
      "Epoch 283/300\n",
      "Average training loss: 0.2365192118220859\n",
      "Average test loss: 30286.25763709834\n",
      "Epoch 284/300\n",
      "Average training loss: 0.21379960278669993\n",
      "Average test loss: 3261.095169887917\n",
      "Epoch 285/300\n",
      "Average training loss: 0.19442511550585428\n",
      "Average test loss: 1.3366147678912514\n",
      "Epoch 286/300\n",
      "Average training loss: 0.17746839688883886\n",
      "Average test loss: 116.91893937365214\n",
      "Epoch 287/300\n",
      "Average training loss: 0.16343519532680512\n",
      "Average test loss: 0.09998527105483744\n",
      "Epoch 288/300\n",
      "Average training loss: 0.14759977520836723\n",
      "Average test loss: 0.625510340432326\n",
      "Epoch 290/300\n",
      "Average training loss: 0.13944398295879365\n",
      "Average test loss: 39.55371488602625\n",
      "Epoch 291/300\n",
      "Average training loss: 0.1336381387313207\n",
      "Average test loss: 0.013469318004945914\n",
      "Epoch 292/300\n",
      "Average training loss: 0.12802668197949726\n",
      "Average test loss: 0.005589570593916708\n",
      "Epoch 293/300\n",
      "Average training loss: 0.12351210612720913\n",
      "Average test loss: 0.005315358218840427\n",
      "Epoch 294/300\n",
      "Average training loss: 0.1195425976978408\n",
      "Average test loss: 0.005591015599254105\n",
      "Epoch 295/300\n",
      "Average training loss: 0.11624052747090657\n",
      "Average test loss: 0.005390685281612807\n",
      "Epoch 296/300\n",
      "Average training loss: 0.11373434142271678\n",
      "Average test loss: 0.005397462063365512\n",
      "Epoch 297/300\n",
      "Average training loss: 0.10998833953009711\n",
      "Average test loss: 0.0052454113918874\n",
      "Epoch 298/300\n",
      "Average training loss: 0.10769413016239802\n",
      "Average test loss: 0.005178350307461288\n",
      "Epoch 299/300\n",
      "Average training loss: 673.0961521697177\n",
      "Average test loss: 0.04436621708008978\n",
      "Epoch 300/300\n",
      "Average training loss: 10.340279621971979\n",
      "Average test loss: 0.04410470749934514\n",
      "Epoch 2/300\n",
      "Average training loss: 6.711506138271756\n",
      "Average test loss: 0.011818816874590184\n",
      "Epoch 3/300\n",
      "Average training loss: 5.768314232720269\n",
      "Average test loss: 0.021298205660449133\n",
      "Epoch 4/300\n",
      "Average training loss: 5.187975362989637\n",
      "Average test loss: 26.9034562157823\n",
      "Epoch 5/300\n",
      "Average training loss: 4.411677821689182\n",
      "Average test loss: 0.2834833938413196\n",
      "Epoch 7/300\n",
      "Average training loss: 4.112695694817437\n",
      "Average test loss: 0.007404366825189856\n",
      "Epoch 8/300\n",
      "Average training loss: 3.8551778661939835\n",
      "Average test loss: 15.065574337492386\n",
      "Epoch 9/300\n",
      "Average training loss: 3.6152411573198107\n",
      "Average test loss: 0.005968868859112263\n",
      "Epoch 10/300\n",
      "Average training loss: 3.3862753045823837\n",
      "Average test loss: 0.005675862441046371\n",
      "Epoch 11/300\n",
      "Average training loss: 3.1880529874165853\n",
      "Average test loss: 0.0053429625402722095\n",
      "Epoch 12/300\n",
      "Average training loss: 2.9874862721761066\n",
      "Average test loss: 0.00638936259266403\n",
      "Epoch 13/300\n",
      "Average training loss: 2.8049852612813315\n",
      "Average test loss: 0.00558746119754182\n",
      "Epoch 14/300\n",
      "Average training loss: 2.640192270702786\n",
      "Average test loss: 0.004859413422230217\n",
      "Epoch 15/300\n",
      "Average training loss: 2.491372500737508\n",
      "Average test loss: 4.342550507969326\n",
      "Epoch 16/300\n",
      "Average training loss: 2.3439827393425836\n",
      "Average test loss: 0.005399930702729358\n",
      "Epoch 17/300\n",
      "Average training loss: 2.0700968844095864\n",
      "Average test loss: 0.00500209976070457\n",
      "Epoch 19/300\n",
      "Average training loss: 1.9475440218183728\n",
      "Average test loss: 0.004390398037516409\n",
      "Epoch 20/300\n",
      "Average training loss: 1.8316540128919814\n",
      "Average test loss: 0.003972668353882101\n",
      "Epoch 21/300\n",
      "Average training loss: 1.709876642121209\n",
      "Average test loss: 0.004050467295779122\n",
      "Epoch 22/300\n",
      "Average training loss: 1.5957290064493814\n",
      "Average test loss: 0.0037635535771648088\n",
      "Epoch 23/300\n",
      "Average training loss: 1.4860108323627048\n",
      "Average test loss: 0.0037616369451085726\n",
      "Epoch 24/300\n",
      "Average training loss: 1.3770825959311592\n",
      "Average test loss: 0.011564716166920131\n",
      "Epoch 25/300\n",
      "Average training loss: 1.2713752518759833\n",
      "Average test loss: 0.030176901607049837\n",
      "Epoch 26/300\n",
      "Average training loss: 1.1677008894814385\n",
      "Average test loss: 0.0035141776411069764\n",
      "Epoch 27/300\n",
      "Average training loss: 1.071248084810045\n",
      "Average test loss: 0.4705439286927382\n",
      "Epoch 28/300\n",
      "Average training loss: 0.9792823437584771\n",
      "Average test loss: 0.0036733978432085777\n",
      "Epoch 29/300\n",
      "Average training loss: 0.7423572551409403\n",
      "Average test loss: 0.005638570501572556\n",
      "Epoch 32/300\n",
      "Average training loss: 0.6798557200961642\n",
      "Average test loss: 0.0033941853259586625\n",
      "Epoch 33/300\n",
      "Average training loss: 0.6198221832381354\n",
      "Average test loss: 0.003246097136495842\n",
      "Epoch 34/300\n",
      "Average training loss: 0.5675941585964627\n",
      "Average test loss: 0.0035103885947416224\n",
      "Epoch 35/300\n",
      "Average training loss: 0.5171914137999216\n",
      "Average test loss: 0.004323283724486828\n",
      "Epoch 36/300\n",
      "Average training loss: 0.4717991851435767\n",
      "Average test loss: 0.06142884145842658\n",
      "Epoch 37/300\n",
      "Average training loss: 0.42937393607033625\n",
      "Average test loss: 0.003213167478640874\n",
      "Epoch 38/300\n",
      "Average training loss: 0.3929088649219937\n",
      "Average test loss: 0.0030690269339829685\n",
      "Epoch 39/300\n",
      "Average training loss: 0.3601545687251621\n",
      "Average test loss: 0.003071713475096557\n",
      "Epoch 40/300\n",
      "Average training loss: 0.3303607468869951\n",
      "Average test loss: 0.0031326533453539013\n",
      "Epoch 41/300\n",
      "Average training loss: 0.3037015325758192\n",
      "Average test loss: 0.022980714141494697\n",
      "Epoch 42/300\n",
      "Average training loss: 0.27833381843566896\n",
      "Average test loss: 0.0040471158714758025\n",
      "Epoch 43/300\n",
      "Average training loss: 0.23502128390471141\n",
      "Average test loss: 0.0034255717667854495\n",
      "Epoch 45/300\n",
      "Average training loss: 0.21517673038111793\n",
      "Average test loss: 0.00306824920202295\n",
      "Epoch 46/300\n",
      "Average training loss: 0.18279125991132525\n",
      "Average test loss: 0.002991606532699532\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1702017962800132\n",
      "Average test loss: 0.002942590889003542\n",
      "Epoch 49/300\n",
      "Average training loss: 0.16062826693058013\n",
      "Average test loss: 0.003973642450653844\n",
      "Epoch 50/300\n",
      "Average training loss: 0.15131077953179678\n",
      "Average test loss: 0.003313795442796416\n",
      "Epoch 51/300\n",
      "Average training loss: 0.14497202108965979\n",
      "Average test loss: 0.007268586683397492\n",
      "Epoch 52/300\n",
      "Average training loss: 0.16213532242510054\n",
      "Average test loss: 0.00394190705443422\n",
      "Epoch 53/300\n",
      "Average training loss: 0.15329354895485772\n",
      "Average test loss: 0.003536718531201283\n",
      "Epoch 54/300\n",
      "Average training loss: 0.13760102319717407\n",
      "Average test loss: 0.0032378955468949345\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12700189725557964\n",
      "Average test loss: 0.0031445771294335524\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11853816722499\n",
      "Average training loss: 0.10500389397144318\n",
      "Average test loss: 0.003413940423892604\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09940752147303687\n",
      "Average test loss: 0.003081565099250939\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09327512017885844\n",
      "Average test loss: 0.0029530473086569046\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08930017661054929\n",
      "Average test loss: 0.003123763342905376\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09422831743955612\n",
      "Average test loss: 0.0032885620914813547\n",
      "Epoch 63/300\n",
      "Average training loss: 0.6484787509375148\n",
      "Average test loss: 3022183.893608079\n",
      "Epoch 64/300\n",
      "Average training loss: 0.20195312129126655\n",
      "Average test loss: 19398600.981205486\n",
      "Epoch 65/300\n",
      "Average training loss: 0.15276689194308388\n",
      "Average test loss: 0.033339558496450386\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1362120816840066\n",
      "Average test loss: 0.13236756889687645\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12521911333666907\n",
      "Average test loss: 600.0550650682846\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11437434409062068\n",
      "Average test loss: 0.7084377558181684\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10299651230043835\n",
      "Average test loss: 236.7083331185546\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09659024942583508\n",
      "Average test loss: 0.0040939342386813625\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0926295879483223\n",
      "Average test loss: 0.22601636877117887\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08849834097094006\n",
      "Average test loss: 0.0030294795022863482\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08520313749048444\n",
      "Average test loss: 0.0036721764722218117\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08229503111044566\n",
      "Average test loss: 236.90672930917805\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07998006214035881\n",
      "Average test loss: 0.003063148260116577\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07722582659787602\n",
      "Average test loss: 0.0031828524195071724\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07562062347597547\n",
      "Average test loss: 0.022434454989929995\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07316339695453644\n",
      "Average test loss: 0.0029391524785508712\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07846934675508076\n",
      "Average test loss: 0.0035677028745412826\n",
      "Epoch 81/300\n",
      "Average test loss: 0.0030215779836806986\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06765038443273969\n",
      "Average test loss: 0.015027293802963363\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06616007195578681\n",
      "Average test loss: 0.0029521965908093583\n",
      "Epoch 85/300\n",
      "Average training loss: 0.24824574072493447\n",
      "Average test loss: 0.4691896843312101\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10149606679545509\n",
      "Average test loss: 0.006261885347465674\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08816822246048185\n",
      "Average test loss: 0.003076922927569184\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08020441259278191\n",
      "Average test loss: 0.004171863073069188\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07519016989072164\n",
      "Average test loss: 0.003109487662091851\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07187632977962494\n",
      "Average test loss: 0.0029700917767153845\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06982693607608477\n",
      "Average test loss: 0.002959036047880848\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06813536347945531\n",
      "Average test loss: 0.002940800674673584\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06710292641652955\n",
      "Average test loss: 0.0029702174560063415\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06526886411507925\n",
      "Average test loss: 0.0029137424218157928\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06450145683685939\n",
      "Average test loss: 0.003092335742794805\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0677039677898089\n",
      "Average test loss: 0.0031418568260139887\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1366685837507248\n",
      "Average test loss: 0.0034765106313344504\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08734675956765811\n",
      "Average test loss: 0.0031843746627370517\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07387030417720476\n",
      "Average test loss: 0.003079373467299673\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06951081748803456\n",
      "Average test loss: 0.00303808185706536\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06678235296739472\n",
      "Average test loss: 0.002982828514857425\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06472920436660448\n",
      "Average training loss: 0.061565946367051864\n",
      "Average test loss: 0.002909595185269912\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06171207721365823\n",
      "Average test loss: 0.0028888619351718162\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0596072419418229\n",
      "Average test loss: 0.01220727828807301\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05871196394165357\n",
      "Average test loss: 0.3648738863567511\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06168208798766136\n",
      "Average test loss: 0.003085960895443956\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10285620452629195\n",
      "Average test loss: 0.008131348387131261\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0812319067981508\n",
      "Average test loss: 0.003002694254482372\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06741352112425698\n",
      "Average test loss: 0.0030069688165353403\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06390742662217883\n",
      "Average test loss: 0.0030769960230423343\n",
      "Epoch 116/300\n",
      "Average training loss: 0.062104644470744666\n",
      "Average test loss: 0.002933477139307393\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06027804362442758\n",
      "Average test loss: 0.0038181512620713977\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05937514500485526\n",
      "Average test loss: 0.0029716804586350917\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05909684138827854\n",
      "Average test loss: 0.002952393631554312\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05856035073929363\n",
      "Average training loss: 0.07794533831212255\n",
      "Average test loss: 0.0032282962082988685\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06297975870966911\n",
      "Average test loss: 0.003083786378407644\n",
      "Epoch 124/300\n",
      "Average training loss: 0.060228426347176234\n",
      "Average test loss: 0.003119296751088566\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05958590426544348\n",
      "Average test loss: 0.17969198889616464\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05994672841164801\n",
      "Average test loss: 0.002936249395004577\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05821013096637196\n",
      "Average test loss: 0.005922283324930403\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05684392193290922\n",
      "Average test loss: 0.002901090878682832\n",
      "Epoch 129/300\n",
      "Average training loss: 0.056147440701723096\n",
      "Average test loss: 0.002880950033250782\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05557130648692449\n",
      "Average test loss: 0.003908680381460322\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05532539215021663\n",
      "Average test loss: 0.026964836556050514\n",
      "Epoch 132/300\n",
      "Average training loss: 0.054803013821442925\n",
      "Average test loss: 0.00390778218416704\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05370431816246774\n",
      "Average test loss: 0.002941812866056959\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0536510487265057\n",
      "Average test loss: 0.09708319032854504\n",
      "Epoch 136/300\n",
      "Average training loss: 0.053095038678910994\n",
      "Average test loss: 0.003041643023904827\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05253861431280772\n",
      "Average test loss: 0.0030114044411521817\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05216134008102947\n",
      "Average test loss: 29.38037285281221\n",
      "Epoch 139/300\n",
      "Average training loss: 0.14785384371214444\n",
      "Average test loss: 47190.42067948644\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10624059492349625\n",
      "Average test loss: 455643738847.4597\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08408909718195598\n",
      "Average test loss: 56802.90672311655\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07604167074627347\n",
      "Average test loss: 69.89276611537714\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07146105703380373\n",
      "Average test loss: 1298.2081347098763\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06618966792689429\n",
      "Average test loss: 0.0033475817462636365\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06382501525349087\n",
      "Average test loss: 0.0030238887204064265\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06070312796036403\n",
      "Average test loss: 0.002964778959751129\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05880257207817501\n",
      "Average test loss: 0.0028670747108343574\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05741657394170761\n",
      "Average test loss: 2146.0487553768094\n",
      "Epoch 150/300\n",
      "Average training loss: 0.056236813320053945\n",
      "Average test loss: 0.003074759754455752\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05512106485499276\n",
      "Average test loss: 0.0029041855997509426\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05669879155688816\n",
      "Average test loss: 0.009052108124519388\n",
      "Epoch 153/300\n",
      "Average training loss: 0.053833447231186764\n",
      "Average test loss: 0.003127200488001108\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05309955146577623\n",
      "Average test loss: 114.59404134496053\n",
      "Epoch 155/300\n",
      "Average training loss: 0.052997612575689954\n",
      "Average test loss: 0.0028636664766818285\n",
      "Epoch 156/300\n",
      "Average training loss: 0.051790561434295446\n",
      "Average test loss: 0.0028573878585464426\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05150113004114893\n",
      "Average test loss: 43.30774462774065\n",
      "Epoch 159/300\n",
      "Average training loss: 0.050855967717038264\n",
      "Average test loss: 0.0028527783639729024\n",
      "Epoch 160/300\n",
      "Average training loss: 0.061621544155809616\n",
      "Average test loss: 0.013921741288776199\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05228747484750218\n",
      "Average test loss: 0.003282669432668222\n",
      "Epoch 162/300\n",
      "Average training loss: 0.050635926998323866\n",
      "Average test loss: 0.0029856385574158696\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05025938799315029\n",
      "Average test loss: 0.0034305961800532208\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04982386134068171\n",
      "Average test loss: 0.00347058374952111\n",
      "Epoch 165/300\n",
      "Average training loss: 0.050355718364318215\n",
      "Average test loss: 0.002939250529019369\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09249192153745227\n",
      "Average test loss: 0.0048761394355032175\n",
      "Epoch 167/300\n",
      "Average training loss: 0.061426538169384\n",
      "Average test loss: 0.0028459945689472887\n",
      "Epoch 168/300\n",
      "Average training loss: 0.056308887541294095\n",
      "Average test loss: 0.0028462965287682084\n",
      "Epoch 169/300\n",
      "Average training loss: 0.052325580534007814\n",
      "Average test loss: 0.003195505725012885\n",
      "Epoch 171/300\n",
      "Average training loss: 0.051256727344459956\n",
      "Average test loss: 0.0029294995034320486\n",
      "Epoch 172/300\n",
      "Average training loss: 0.050191735492812264\n",
      "Average test loss: 0.003286045090191894\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0495372980899281\n",
      "Average test loss: 0.0029310772276173037\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0492630884150664\n",
      "Average test loss: 0.01043126292857859\n",
      "Epoch 176/300\n",
      "Average training loss: 0.049086175057623124\n",
      "Average test loss: 0.0029464860448820723\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04909500723414951\n",
      "Average test loss: 0.2312801689590431\n",
      "Epoch 178/300\n",
      "Average training loss: 0.048479047540161344\n",
      "Average test loss: 0.00605061759841111\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04795353320572111\n",
      "Average test loss: 0.005289714488511284\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04797512177626292\n",
      "Average test loss: 3.0192157113701104\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04766866487264633\n",
      "Average test loss: 0.003038394762410058\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04769656338294347\n",
      "Average test loss: 0.0035228515764077505\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04869103322757615\n",
      "Average test loss: 0.003074676287877891\n",
      "Epoch 186/300\n",
      "Average training loss: 0.046701243162155154\n",
      "Average test loss: 0.0029259928282764224\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04651525255375438\n",
      "Average test loss: 0.008849784303042623\n",
      "Epoch 188/300\n",
      "Average training loss: 0.047666307363245224\n",
      "Average test loss: 0.4711491912570265\n",
      "Epoch 189/300\n",
      "Average training loss: 0.11454390685425864\n",
      "Average test loss: 29.365640274026326\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07414435571432114\n",
      "Average test loss: 0.0029952521460751692\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0664914139509201\n",
      "Average test loss: 0.002859853993480404\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06220514030588998\n",
      "Average test loss: 0.002851555643396245\n",
      "Epoch 193/300\n",
      "Average training loss: 0.059262333578533596\n",
      "Average test loss: 0.007360821055041419\n",
      "Epoch 194/300\n",
      "Average training loss: 0.057119510143995286\n",
      "Average test loss: 0.0028323192987591028\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0547579499648677\n",
      "Average test loss: 0.0028374771479931144\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05166039702627394\n",
      "Average test loss: 0.0033196977269318367\n",
      "Epoch 198/300\n",
      "Average training loss: 0.050547527571519216\n",
      "Average test loss: 0.010735168282356527\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04952235165900654\n",
      "Average test loss: 1.8919883459541533\n",
      "Epoch 200/300\n",
      "Average training loss: 0.049361854215463005\n",
      "Average test loss: 0.0028978874151491456\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04758411427338918\n",
      "Average test loss: 0.002944080179143283\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04731231903367572\n",
      "Average test loss: 0.00971949650388625\n",
      "Epoch 203/300\n",
      "Average training loss: 0.046655439684788386\n",
      "Average test loss: 0.0033745343271229\n",
      "Epoch 204/300\n",
      "Average training loss: 0.045961669862270356\n",
      "Average test loss: 0.0029337583982075254\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04611320933037334\n",
      "Average test loss: 311593687.10297394\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0455366715921296\n",
      "Average test loss: 0.0030656822632170386\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04555644864506192\n",
      "Average test loss: 0.0031228433913654753\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04511032829682032\n",
      "Average test loss: 0.011994145029534897\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05871767588787609\n",
      "Average test loss: 0.00295602778552307\n",
      "Epoch 212/300\n",
      "Average training loss: 0.051611357728640235\n",
      "Average test loss: 0.002908072662850221\n",
      "Epoch 213/300\n",
      "Average training loss: 0.12492707091901037\n",
      "Average test loss: 17947.326186387563\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06557283708784316\n",
      "Average test loss: 0.0030184861284991106\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05855390711625417\n",
      "Average test loss: 37.78533324935494\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05432434680726793\n",
      "Average test loss: 139.1778267413557\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05171817906366454\n",
      "Average test loss: 0.0029427843489166763\n",
      "Epoch 218/300\n",
      "Average training loss: 0.049353658997350267\n",
      "Average test loss: 0.0029350331689541537\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04781979873445299\n",
      "Average test loss: 0.018484038153249355\n",
      "Epoch 220/300\n",
      "Average test loss: 0.002894009940326214\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04488606245319048\n",
      "Average test loss: 0.006591483339046439\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04446911533342467\n",
      "Average test loss: 0.003179786852457457\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04427892328964339\n",
      "Average test loss: 0.0031199427087687786\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04382345744305187\n",
      "Average test loss: 0.0029222757884611685\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04352672643462817\n",
      "Average test loss: 0.0029643399506393405\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04366378506024678\n",
      "Average test loss: 0.0030264440425154236\n",
      "Epoch 229/300\n",
      "Average training loss: 0.043124332388242086\n",
      "Average test loss: 0.0030644360958702035\n",
      "Epoch 230/300\n",
      "Average training loss: 0.042884174759189285\n",
      "Average test loss: 0.0031996270151187977\n",
      "Epoch 231/300\n",
      "Average training loss: 0.042945831146505145\n",
      "Average test loss: 0.0032779380997849834\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04325683714946111\n",
      "Average test loss: 16.15024728685038\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04244820076227188\n",
      "Average test loss: 0.0030189601340227658\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04237017693453365\n",
      "Average test loss: 0.0036608011801209715\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04250245428416464\n",
      "Average test loss: 0.002979864217547907\n",
      "Epoch 236/300\n",
      "Average training loss: 0.041981503976715934\n",
      "Average test loss: 0.005463051630804936\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04178706230719884\n",
      "Average test loss: 0.0038861790605717234\n",
      "Epoch 238/300\n",
      "Average training loss: 0.041758011013269425\n",
      "Average test loss: 0.012230918452868032\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05078132490151458\n",
      "Average test loss: 0.0031456067450344564\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04144924388329188\n",
      "Average test loss: 0.003241167708196574\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04151098695728514\n",
      "Average test loss: 0.0031984822704560225\n",
      "Epoch 243/300\n",
      "Average training loss: 0.16294322453273669\n",
      "Average test loss: 14791.502322786779\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0701680807073911\n",
      "Average test loss: 7237.18533690424\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06266883751418856\n",
      "Average test loss: 0.0029368507254661784\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05837925281127294\n",
      "Average test loss: 0.0028975410664247143\n",
      "Epoch 247/300\n",
      "Average training loss: 0.055349616017606525\n",
      "Average test loss: 0.0028431770969182253\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05839298226104842\n",
      "Average test loss: 2945599054377.896\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05624613235725297\n",
      "Average test loss: 0.0029832828322218525\n",
      "Epoch 250/300\n",
      "Average training loss: 0.050157175328996446\n",
      "Average test loss: 0.0029317036188311048\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04755843323800299\n",
      "Average test loss: 0.0030265147687039444\n",
      "Epoch 252/300\n",
      "Average training loss: 0.045886131339603\n",
      "Average test loss: 0.003512329685398274\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04778040834599071\n",
      "Average test loss: 0.03975340417193042\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04387635642621252\n",
      "Average test loss: 0.003089724236064487\n",
      "Epoch 255/300\n",
      "Average training loss: 0.043446910442577466\n",
      "Average test loss: 0.0030097656043039426\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04246983826160431\n",
      "Average test loss: 0.003090804830607441\n",
      "Epoch 257/300\n",
      "Average training loss: 0.045322340647379555\n",
      "Average test loss: 0.0030163980598251023\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04220006138417456\n",
      "Average test loss: 0.003003046488803294\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04177691125869751\n",
      "Average test loss: 0.00373732449962861\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0426337387462457\n",
      "Average test loss: 0.003443885222905212\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04146578427155813\n",
      "Average test loss: 0.0030831918915112813\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04140197818477948\n",
      "Average test loss: 0.0033442439726657338\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04182765575912264\n",
      "Average test loss: 0.04725579128497177\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04174701974127028\n",
      "Average test loss: 21.0441242837641\n",
      "Epoch 265/300\n",
      "Average training loss: 0.040811681379874544\n",
      "Average test loss: 0.004745115042560631\n",
      "Epoch 266/300\n",
      "Average training loss: 0.040855273713668185\n",
      "Average test loss: 0.003995635717693302\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04297298148936696\n",
      "Average test loss: 0.0033321471717208625\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04627937008606063\n",
      "Average test loss: 0.12149546151463356\n",
      "Epoch 269/300\n",
      "Average training loss: 0.040362271865208944\n",
      "Average test loss: 2.4327974294192263\n",
      "Epoch 270/300\n",
      "Average training loss: 0.039972926831907694\n",
      "Average test loss: 0.003162104663128654\n",
      "Epoch 271/300\n",
      "Average training loss: 0.040754875722858644\n",
      "Average test loss: 0.0030800739750266076\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0405767658551534\n",
      "Average test loss: 0.021627165765501558\n",
      "Epoch 273/300\n",
      "Average training loss: 0.040948370357354484\n",
      "Average test loss: 1.217226768417806e+17\n",
      "Epoch 274/300\n",
      "Average training loss: 0.039926974828044574\n",
      "Average test loss: 0.006917227088991139\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03995456345213784\n",
      "Average test loss: 0.006746996833218469\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04015653430918852\n",
      "Average test loss: 273.12963458143673\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04003369989328914\n",
      "Average test loss: 0.0030992816674212617\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0402086327638891\n",
      "Average test loss: 0.0032841860949993132\n",
      "Epoch 279/300\n",
      "Average training loss: 0.041534788688023884\n",
      "Average test loss: 0.0030251188911497593\n",
      "Epoch 280/300\n",
      "Average training loss: 0.11318930805722872\n",
      "Average test loss: 0.0077711497942606605\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06057538268301222\n",
      "Average test loss: 0.011496809713956382\n",
      "Epoch 282/300\n",
      "Average training loss: 0.054668492847018774\n",
      "Average test loss: 0.0030639737380875483\n",
      "Epoch 283/300\n",
      "Average training loss: 0.050864746024211246\n",
      "Average test loss: 0.05184847132033772\n",
      "Epoch 284/300\n",
      "Average training loss: 0.047916478839185504\n",
      "Average test loss: 0.007385245849688848\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04535983793934186\n",
      "Average test loss: 0.006061003086467584\n",
      "Epoch 286/300\n",
      "Average training loss: 0.043214024369915324\n",
      "Average test loss: 0.003369622388854623\n",
      "Epoch 287/300\n",
      "Average training loss: 0.041701037655274074\n",
      "Average test loss: 6.198488134882516\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04146448495984077\n",
      "Average test loss: 0.0043791701876454885\n",
      "Epoch 289/300\n",
      "Average training loss: 0.039962770230240294\n",
      "Average test loss: 0.003164681087765429\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03957667589849896\n",
      "Average test loss: 0.0030306522332959706\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03951032426291042\n",
      "Average test loss: 0.009112148975125618\n",
      "Epoch 292/300\n",
      "Average training loss: 0.039631387064854307\n",
      "Average test loss: 0.003032260743694173\n",
      "Epoch 293/300\n",
      "Average training loss: 0.039355686648024454\n",
      "Average test loss: 0.003370824424136016\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03909414551986588\n",
      "Average test loss: 0.0030356710447619357\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03936877645055453\n",
      "Average test loss: 0.0033715753398007816\n",
      "Epoch 296/300\n",
      "Average training loss: 0.051506385922431945\n",
      "Average test loss: 0.0031210522796544763\n",
      "Epoch 297/300\n",
      "Average training loss: 0.046216115122040116\n",
      "Average test loss: 0.003078667176473472\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04164105700121985\n",
      "Average test loss: 0.005021200090232823\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04081495839357376\n",
      "Average test loss: 0.0030656636212435034\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03943615012367566\n",
      "Average test loss: 0.00463950349121458\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.892711471557616\n",
      "Average test loss: 3.6190795344842805\n",
      "Epoch 2/300\n",
      "Average training loss: 5.828441799587674\n",
      "Average test loss: 0.04368698076986604\n",
      "Epoch 3/300\n",
      "Average training loss: 5.054634094662137\n",
      "Average test loss: 0.012656852302451928\n",
      "Epoch 4/300\n",
      "Average training loss: 4.517024151696099\n",
      "Average test loss: 0.008442905973229144\n",
      "Epoch 5/300\n",
      "Average training loss: 4.117699363708496\n",
      "Average test loss: 0.005620310559454891\n",
      "Epoch 6/300\n",
      "Average training loss: 3.795580255508423\n",
      "Average test loss: 0.005690757676131196\n",
      "Epoch 7/300\n",
      "Average training loss: 3.519984759012858\n",
      "Average test loss: 0.09697712224018243\n",
      "Epoch 8/300\n",
      "Average training loss: 3.266057971318563\n",
      "Average test loss: 0.14969056433853176\n",
      "Epoch 9/300\n",
      "Average training loss: 3.0429295410580104\n",
      "Average test loss: 1.829749343837301\n",
      "Epoch 10/300\n",
      "Average training loss: 2.8442462955051\n",
      "Average test loss: 0.018414485266225206\n",
      "Epoch 11/300\n",
      "Average training loss: 2.663882685131497\n",
      "Average test loss: 0.005495530684789022\n",
      "Epoch 12/300\n",
      "Average training loss: 2.4930724601745604\n",
      "Average test loss: 0.013313789349463251\n",
      "Epoch 13/300\n",
      "Average training loss: 2.3353004716237384\n",
      "Average test loss: 0.005668456131385433\n",
      "Epoch 14/300\n",
      "Average training loss: 2.1840489370558\n",
      "Average test loss: 0.008240319516923693\n",
      "Epoch 15/300\n",
      "Average training loss: 2.0487550297843087\n",
      "Average test loss: 0.005518823179519839\n",
      "Epoch 16/300\n",
      "Average training loss: 1.923159493552314\n",
      "Average test loss: 0.011397763500610987\n",
      "Epoch 17/300\n",
      "Average training loss: 1.8046865380605062\n",
      "Average test loss: 0.005017198657823934\n",
      "Epoch 18/300\n",
      "Average training loss: 1.6901283084021674\n",
      "Average test loss: 0.13477826442031396\n",
      "Epoch 19/300\n",
      "Average training loss: 1.57770027340783\n",
      "Average test loss: 0.003781436899056037\n",
      "Epoch 20/300\n",
      "Average training loss: 1.468532414648268\n",
      "Average test loss: 0.002924156587984827\n",
      "Epoch 21/300\n",
      "Average training loss: 1.358619238641527\n",
      "Average test loss: 0.0047742874353296225\n",
      "Epoch 22/300\n",
      "Average training loss: 1.253794347551134\n",
      "Average test loss: 0.0037985622553775708\n",
      "Epoch 23/300\n",
      "Average training loss: 1.1508382842805651\n",
      "Average test loss: 0.0025153552450032698\n",
      "Epoch 24/300\n",
      "Average training loss: 1.0489325450261433\n",
      "Average test loss: 0.0044244591699292265\n",
      "Epoch 25/300\n",
      "Average training loss: 0.9534010392294989\n",
      "Average test loss: 0.002473147876146767\n",
      "Epoch 26/300\n",
      "Average training loss: 0.8637407728830974\n",
      "Average test loss: 0.0023756254652722015\n",
      "Epoch 27/300\n",
      "Average training loss: 0.7825117384592692\n",
      "Average test loss: 0.002527396039312912\n",
      "Epoch 28/300\n",
      "Average training loss: 0.640744211991628\n",
      "Average test loss: 0.0024413240276690987\n",
      "Epoch 30/300\n",
      "Average training loss: 0.5814400340716044\n",
      "Average test loss: 0.0029608896339519157\n",
      "Epoch 31/300\n",
      "Average training loss: 0.5274349400732252\n",
      "Average test loss: 0.012502239127126004\n",
      "Epoch 32/300\n",
      "Average training loss: 0.4812072241041395\n",
      "Average test loss: 0.002462748429427544\n",
      "Epoch 33/300\n",
      "Average test loss: 0.002068221547123459\n",
      "Epoch 35/300\n",
      "Average training loss: 0.3705879107316335\n",
      "Average test loss: 0.0023914370872080326\n",
      "Epoch 36/300\n",
      "Average training loss: 0.34000306158595617\n",
      "Average test loss: 0.002697126713891824\n",
      "Epoch 37/300\n",
      "Average training loss: 0.3106563557518853\n",
      "Average test loss: 0.0023415210935183698\n",
      "Epoch 38/300\n",
      "Average training loss: 0.284061935265859\n",
      "Average test loss: 0.0023380782444857888\n",
      "Epoch 39/300\n",
      "Average training loss: 0.2587603678173489\n",
      "Average test loss: 0.002027757303375337\n",
      "Epoch 40/300\n",
      "Average training loss: 0.23699757537576888\n",
      "Average test loss: 0.0019758582810560864\n",
      "Epoch 41/300\n",
      "Average training loss: 0.22116204949220022\n",
      "Average test loss: 0.002135109397065308\n",
      "Epoch 42/300\n",
      "Average training loss: 0.20093786741627587\n",
      "Average test loss: 0.0028866873661884\n",
      "Epoch 43/300\n",
      "Average training loss: 0.18496936090787253\n",
      "Average test loss: 0.0020634160570593345\n",
      "Epoch 44/300\n",
      "Average training loss: 0.173276895682017\n",
      "Average test loss: 0.41789137623376316\n",
      "Epoch 45/300\n",
      "Average training loss: 0.15231838519043392\n",
      "Average test loss: 0.0020559129013369482\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1433799161248737\n",
      "Average test loss: 0.0023388109114021063\n",
      "Epoch 48/300\n",
      "Average training loss: 0.13565265862147013\n",
      "Average test loss: 0.002058500894034902\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12744772230254278\n",
      "Average test loss: 0.30711749975259106\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11964606212907367\n",
      "Average test loss: 0.0021893375555260314\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11154381591743893\n",
      "Average test loss: 0.0021996093406859373\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10623167340623008\n",
      "Average test loss: 0.0021497386581160955\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09493022355106141\n",
      "Average test loss: 0.0025764482468366625\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0865551302962833\n",
      "Average test loss: 0.003015609211805794\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07951348574956259\n",
      "Average test loss: 0.002534684909507632\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07397570994827482\n",
      "Average test loss: 0.002002365775923762\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06904523409075207\n",
      "Average test loss: 0.0019832875160500406\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06528393846750259\n",
      "Average test loss: 0.0018426701063290237\n",
      "Epoch 59/300\n",
      "Average training loss: 0.062388272802035016\n",
      "Average test loss: 0.001871686290535662\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05900259349743525\n",
      "Average test loss: 0.0022495070696911877\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05772573284970389\n",
      "Average test loss: 0.002071228349995282\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0981250035961469\n",
      "Average test loss: 0.0022197276608397565\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06332510871357389\n",
      "Average test loss: 0.001973391805258062\n",
      "Epoch 64/300\n",
      "Average training loss: 0.057529159284300274\n",
      "Average test loss: 0.0019425948671996593\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05436791595154338\n",
      "Average test loss: 0.0020156505066487524\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05165769747230742\n",
      "Average test loss: 0.0018597571839474969\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05200199844439824\n",
      "Average test loss: 0.0022019517690771155\n",
      "Epoch 68/300\n",
      "Average training loss: 0.052542632695701384\n",
      "Average test loss: 0.035989439477523165\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05439714013205634\n",
      "Average test loss: 0.0018806276021318302\n",
      "Epoch 70/300\n",
      "Average training loss: 0.052074804815981125\n",
      "Average test loss: 0.00188528389390558\n",
      "Epoch 71/300\n",
      "Average training loss: 0.047700705157385934\n",
      "Average test loss: 0.001925977813700835\n",
      "Epoch 72/300\n",
      "Average training loss: 0.045846631632910834\n",
      "Average test loss: 0.0018618965543185672\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04441206633051237\n",
      "Average test loss: 0.0018389555442457398\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04362095093395975\n",
      "Average test loss: 0.0033719472367730405\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04290532348553339\n",
      "Average test loss: 0.001942191834251086\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04438932146959835\n",
      "Average test loss: 0.00187308751564059\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05669166854686207\n",
      "Average test loss: 0.0022066552649355597\n",
      "Epoch 78/300\n",
      "Average training loss: 0.051139570901791255\n",
      "Average test loss: 0.002015003953749935\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04506002721852726\n",
      "Average test loss: 0.0018758752843778994\n",
      "Epoch 80/300\n",
      "Average training loss: 0.043227438158459135\n",
      "Average test loss: 0.001975809011918803\n",
      "Epoch 81/300\n",
      "Average training loss: 0.042049909604920284\n",
      "Average test loss: 0.0018867596426150866\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04138055774072806\n",
      "Average test loss: 0.0020464004418916173\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04098254952828089\n",
      "Average test loss: 0.005762457605451346\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04099966881341404\n",
      "Average test loss: 0.001953208627851887\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0413781535923481\n",
      "Average test loss: 0.001856670199169053\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04004041205512153\n",
      "Average test loss: 0.0025874085745049846\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03926340741415819\n",
      "Average test loss: 0.0018316170256584884\n",
      "Epoch 88/300\n",
      "Average training loss: 0.038518341607517666\n",
      "Average test loss: 0.001942616016500526\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03857056378490395\n",
      "Average test loss: 0.0018125305399298668\n",
      "Epoch 90/300\n",
      "Average training loss: 0.037943603793780006\n",
      "Average test loss: 0.001997299472077025\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0376857375005881\n",
      "Average test loss: 0.001845389139941997\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03747261778844727\n",
      "Average test loss: 0.0018522495787797702\n",
      "Epoch 93/300\n",
      "Average training loss: 0.048700240241156684\n",
      "Average test loss: 0.0018642628492994441\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04030475705862045\n",
      "Average test loss: 0.0018978360011759731\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03899630136291186\n",
      "Average test loss: 0.017444410858054957\n",
      "Epoch 96/300\n",
      "Average training loss: 0.037716924919022454\n",
      "Average test loss: 0.0018233113029143876\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03730169818467564\n",
      "Average test loss: 0.001957875717120866\n",
      "Epoch 98/300\n",
      "Average training loss: 0.036697139690319694\n",
      "Average test loss: 0.0020836189814532795\n",
      "Epoch 99/300\n",
      "Average training loss: 0.036505597114562985\n",
      "Average test loss: 0.0019158787859810723\n",
      "Epoch 100/300\n",
      "Average training loss: 0.036282901422844994\n",
      "Average test loss: 0.002340322806603379\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03592342716952165\n",
      "Average test loss: 0.060862326636910435\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03565518966979451\n",
      "Average test loss: 0.019661948426730105\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03532533328400718\n",
      "Average test loss: 0.0018754726849082443\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03626142729487684\n",
      "Average test loss: 0.002136104399545325\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04146861629353629\n",
      "Average test loss: 0.003566354043781757\n",
      "Epoch 106/300\n",
      "Average training loss: 0.038096841322051156\n",
      "Average test loss: 0.0022777559474731483\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03570400520828035\n",
      "Average test loss: 0.0018662718068808318\n",
      "Epoch 108/300\n",
      "Average training loss: 0.034977170758777196\n",
      "Average test loss: 0.001855004614012109\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03457744055655267\n",
      "Average test loss: 86.15278751587867\n",
      "Epoch 110/300\n",
      "Average training loss: 0.034427722384532296\n",
      "Average test loss: 0.0019405764763553938\n",
      "Epoch 111/300\n",
      "Average training loss: 0.034020799689822724\n",
      "Average test loss: 0.002031493951876958\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03401524354351891\n",
      "Average test loss: 0.005552581049915817\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03390040575961272\n",
      "Average test loss: 0.0021063718922022315\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03342043036222458\n",
      "Average test loss: 0.0018361175197900998\n",
      "Epoch 115/300\n",
      "Average training loss: 0.033278966036107804\n",
      "Average test loss: 0.002614748042490747\n",
      "Epoch 116/300\n",
      "Average training loss: 0.033193655287226044\n",
      "Average test loss: 0.0029960608883864348\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03296824269162284\n",
      "Average test loss: 0.0019556151148345737\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03271502369973395\n",
      "Average test loss: 0.0019006609564854039\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03283982294632329\n",
      "Average test loss: 0.002131299843183822\n",
      "Epoch 120/300\n",
      "Average training loss: 0.046427978914644986\n",
      "Average test loss: 0.0019264257464350926\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04062676058212916\n",
      "Average test loss: 0.0019571648640558125\n",
      "Epoch 122/300\n",
      "Average training loss: 0.037094427183270454\n",
      "Average test loss: 115.22248596883813\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03527713058723344\n",
      "Average test loss: 0.0019034544974565505\n",
      "Epoch 124/300\n",
      "Average training loss: 0.034256989027063055\n",
      "Average test loss: 0.0019707831468743584\n",
      "Epoch 125/300\n",
      "Average training loss: 0.033386946919891566\n",
      "Average test loss: 0.002069922341240777\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03297854874696996\n",
      "Average test loss: 0.0019153513365114728\n",
      "Epoch 127/300\n",
      "Average training loss: 0.032471682705812986\n",
      "Average test loss: 1.8043797538412942\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03239528418249554\n",
      "Average test loss: 0.001879609703603718\n",
      "Epoch 129/300\n",
      "Average training loss: 0.032280868381261825\n",
      "Average test loss: 0.0022589604392026863\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03196862811512417\n",
      "Average test loss: 0.0024459523080537717\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03175758591625426\n",
      "Average test loss: 0.005532704593406783\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03178452371723122\n",
      "Average test loss: 0.004619629535410139\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03164634959234132\n",
      "Average test loss: 0.001986144219732119\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03152070450120502\n",
      "Average test loss: 0.0018554078424349427\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031306516074472004\n",
      "Average test loss: 10.376779317886259\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031792511958214975\n",
      "Average test loss: 0.0025546691198315885\n",
      "Epoch 137/300\n",
      "Average training loss: 0.032536918603711655\n",
      "Average test loss: 0.0019230508048915201\n",
      "Epoch 138/300\n",
      "Average training loss: 0.031112462752395206\n",
      "Average test loss: 0.003173151362480389\n",
      "Epoch 139/300\n",
      "Average training loss: 0.030746665166483986\n",
      "Average test loss: 0.003378723266068846\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0310399792459276\n",
      "Average test loss: 0.0029294081528981526\n",
      "Epoch 141/300\n",
      "Average training loss: 0.030572284751468233\n",
      "Average test loss: 0.004155506152245733\n",
      "Epoch 142/300\n",
      "Average training loss: 0.030458056426710553\n",
      "Average test loss: 0.0019189808428701426\n",
      "Epoch 143/300\n",
      "Average training loss: 0.030295559828480086\n",
      "Average test loss: 0.0019960357077005836\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03052460022105111\n",
      "Average test loss: 0.003866072831882371\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07618285693393813\n",
      "Average test loss: 0.0019682263150397274\n",
      "Epoch 146/300\n",
      "Average training loss: 0.044448052760627535\n",
      "Average test loss: 0.0019021949357767073\n",
      "Epoch 147/300\n",
      "Average training loss: 0.039261055883434086\n",
      "Average test loss: 0.0018829477103427053\n",
      "Epoch 148/300\n",
      "Average training loss: 0.036255445500214895\n",
      "Average test loss: 0.0018485140359650056\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03414839580655098\n",
      "Average test loss: 0.0018879310629434055\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03245107313825024\n",
      "Average test loss: 0.0022104826163914467\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03142039492726326\n",
      "Average test loss: 0.0018803018356362978\n",
      "Epoch 152/300\n",
      "Average training loss: 0.030663780387904908\n",
      "Average test loss: 0.0023006130850149525\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030442370653152466\n",
      "Average test loss: 0.00603231573301471\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03006461219655143\n",
      "Average test loss: 2.131324918212162\n",
      "Epoch 155/300\n",
      "Average training loss: 0.029972005693448916\n",
      "Average test loss: 1.6582682997973428\n",
      "Epoch 156/300\n",
      "Average training loss: 0.029987262003951604\n",
      "Average test loss: 0.0019915067861891456\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030068462404939864\n",
      "Average test loss: 0.0033095525596290826\n",
      "Epoch 158/300\n",
      "Average training loss: 0.029691550243231984\n",
      "Average test loss: 0.002740434892061684\n",
      "Epoch 159/300\n",
      "Average training loss: 0.029641993249456086\n",
      "Average test loss: 0.012233777166240745\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02971290928622087\n",
      "Average test loss: 0.0019778014652224053\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02953350383043289\n",
      "Average test loss: 0.002345515046682623\n",
      "Epoch 162/300\n",
      "Average training loss: 0.029316720565160114\n",
      "Average test loss: 0.00204471380263567\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029339775977863206\n",
      "Average training loss: 0.029241176502572165\n",
      "Average test loss: 0.0019214889990786712\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02910622090101242\n",
      "Average test loss: 0.0038080695864434046\n",
      "Epoch 167/300\n",
      "Average training loss: 0.028833408074246513\n",
      "Average test loss: 0.0025020800760636726\n",
      "Epoch 168/300\n",
      "Average training loss: 0.029381389968925052\n",
      "Average test loss: 0.0029297029783742295\n",
      "Epoch 169/300\n",
      "Average training loss: 0.028851539239287377\n",
      "Average test loss: 0.005453625484266215\n",
      "Epoch 170/300\n",
      "Average training loss: 0.028717754390504627\n",
      "Average test loss: 0.003307065308507946\n",
      "Epoch 171/300\n",
      "Average training loss: 0.028795779845780797\n",
      "Average test loss: 0.0019980198273228276\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02843763445980019\n",
      "Average test loss: 0.001975708183315065\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03235818335413933\n",
      "Average test loss: 0.002099900150878562\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02813566867344909\n",
      "Average test loss: 0.0022342209961886206\n",
      "Epoch 178/300\n",
      "Average training loss: 0.028093212784992324\n",
      "Average test loss: 0.0031842040750715466\n",
      "Epoch 179/300\n",
      "Average training loss: 0.028106264442205427\n",
      "Average test loss: 0.0020007454146527583\n",
      "Epoch 180/300\n",
      "Average training loss: 0.028008626133203506\n",
      "Average test loss: 0.023515798460278247\n",
      "Epoch 181/300\n",
      "Average training loss: 0.028081988082991706\n",
      "Average test loss: 0.0019779398108108175\n",
      "Epoch 182/300\n",
      "Average training loss: 0.027851970901091892\n",
      "Average test loss: 0.23732821069657803\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02803355723619461\n",
      "Average test loss: 0.002122239011960725\n",
      "Epoch 184/300\n",
      "Average training loss: 0.027732462817596063\n",
      "Average test loss: 0.00219420539173815\n",
      "Epoch 185/300\n",
      "Average training loss: 0.027746906452708775\n",
      "Average test loss: 0.00205392431622992\n",
      "Epoch 186/300\n",
      "Average training loss: 0.027902665752503608\n",
      "Average test loss: 0.002238017397829228\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02895627798140049\n",
      "Average test loss: 0.00312523780659669\n",
      "Epoch 188/300\n",
      "Average training loss: 0.027734898312224282\n",
      "Average test loss: 0.004107192148351007\n",
      "Epoch 189/300\n",
      "Average training loss: 0.027524615305993292\n",
      "Average test loss: 0.0020577862585584323\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02742579331828488\n",
      "Average test loss: 0.0021062359359736243\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02747563149697251\n",
      "Average test loss: 0.0024163485747865504\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02719135331776407\n",
      "Average test loss: 0.0021406323875093627\n",
      "Epoch 194/300\n",
      "Average training loss: 0.027519356874956026\n",
      "Average test loss: 0.002903901188323895\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02773172684179412\n",
      "Average test loss: 0.002174230150671469\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02760897637738122\n",
      "Average test loss: 0.0025141192215184373\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026984717920422552\n",
      "Average test loss: 0.003185763443302777\n",
      "Epoch 198/300\n",
      "Average training loss: 0.026922515642311837\n",
      "Average test loss: 0.0021680887571225564\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02697507147490978\n",
      "Average test loss: 0.0024149978363679513\n",
      "Epoch 201/300\n",
      "Average training loss: 0.027009714696142408\n",
      "Average test loss: 0.0026092982832342385\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02808560744424661\n",
      "Average test loss: 48.22668224557324\n",
      "Epoch 203/300\n",
      "Average training loss: 0.027614286238948505\n",
      "Average test loss: 0.0032874912216017643\n",
      "Epoch 204/300\n",
      "Average training loss: 0.026909019048015275\n",
      "Average test loss: 0.00399874350728674\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02703942691617542\n",
      "Average test loss: 0.1421296722880668\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04541076368590196\n",
      "Average test loss: 0.0020264044574772318\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030619070707095995\n",
      "Average test loss: 0.04363503489353591\n",
      "Epoch 208/300\n",
      "Average training loss: 0.027756735240419706\n",
      "Average test loss: 0.002076388268214133\n",
      "Epoch 209/300\n",
      "Average training loss: 0.026827020862036283\n",
      "Average test loss: 0.007481474909103579\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026435819615920383\n",
      "Average test loss: 0.0020371081216467753\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026455922186374665\n",
      "Average test loss: 0.0033000375341830984\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026666072274247804\n",
      "Average test loss: 0.0036513290293514727\n",
      "Epoch 214/300\n",
      "Average training loss: 0.034134985168774924\n",
      "Average test loss: 1498202035.3444955\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0409819048874908\n",
      "Average test loss: 1.2719120998407403\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030802311688661575\n",
      "Average test loss: 0.0019638097705319524\n",
      "Epoch 217/300\n",
      "Average training loss: 0.027785834794243177\n",
      "Average test loss: 0.11349298637463814\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02673340742621157\n",
      "Average test loss: 0.016441554940823052\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026859085988667278\n",
      "Average test loss: 0.0023073548489871126\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026314000606536866\n",
      "Average test loss: 0.004162301431099574\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02638081210023827\n",
      "Average test loss: 0.0028139716154999203\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02749831025633547\n",
      "Average test loss: 0.002067567072291341\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05322180702620082\n",
      "Average test loss: 0.0019644935369077655\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03478980075485177\n",
      "Average test loss: 0.002801745711722308\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03043702264957958\n",
      "Average test loss: 0.0021701370217940873\n",
      "Epoch 226/300\n",
      "Average training loss: 0.027967789363529946\n",
      "Average test loss: 0.002120558370836079\n",
      "Epoch 227/300\n",
      "Average training loss: 0.026805189814832476\n",
      "Average test loss: 0.0021273492112134896\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02635455706384447\n",
      "Average test loss: 0.0022490425089167223\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02621358702911271\n",
      "Average test loss: 0.002069538502436545\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026197040875752767\n",
      "Average test loss: 0.00278676759906941\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026130333392156493\n",
      "Average test loss: 0.0026066904759241474\n",
      "Epoch 232/300\n",
      "Average training loss: 0.026213422152731154\n",
      "Average test loss: 0.0020110284701610605\n",
      "Epoch 233/300\n",
      "Average training loss: 0.026394693979786502\n",
      "Average test loss: 0.00401360376437919\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02653714490764671\n",
      "Average test loss: 0.003455038929358125\n",
      "Epoch 235/300\n",
      "Average training loss: 0.026150735432902972\n",
      "Average test loss: 0.002122459177548687\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02609019453326861\n",
      "Average test loss: 0.0022606178079214363\n",
      "Epoch 237/300\n",
      "Average training loss: 0.026239789990915193\n",
      "Average test loss: 0.4718654125953714\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02675829827785492\n",
      "Average test loss: 0.00212272869857649\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026123657350738842\n",
      "Average test loss: 0.0022216504292769564\n",
      "Epoch 240/300\n",
      "Average training loss: 0.025981645449995993\n",
      "Average test loss: 0.003551496836046378\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026217354946666292\n",
      "Average test loss: 0.002209421473244826\n",
      "Epoch 242/300\n",
      "Average training loss: 0.025827416713866923\n",
      "Average test loss: 0.0021949916993164354\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02605022964047061\n",
      "Average test loss: 0.0022755541977369123\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02593216482963827\n",
      "Average test loss: 0.002042084117109577\n",
      "Epoch 245/300\n",
      "Average training loss: 0.025764126300811766\n",
      "Average test loss: 0.0020311544676207835\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0260494173872802\n",
      "Average test loss: 0.0020509567451145916\n",
      "Epoch 247/300\n",
      "Average training loss: 0.025784520374404058\n",
      "Average test loss: 0.002065430085071259\n",
      "Epoch 248/300\n",
      "Average training loss: 0.025768596223658985\n",
      "Average test loss: 0.0020608816925022336\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02589019473062621\n",
      "Average test loss: 0.0022902988974625865\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026045901868906287\n",
      "Average test loss: 0.002764369891335567\n",
      "Epoch 251/300\n",
      "Average training loss: 0.025998667433857918\n",
      "Average test loss: 0.0022800697597364587\n",
      "Epoch 252/300\n",
      "Average training loss: 0.026104749711023438\n",
      "Average test loss: 0.002279115432666408\n",
      "Epoch 253/300\n",
      "Average training loss: 0.025575140317281086\n",
      "Average test loss: 0.00939476386209329\n",
      "Epoch 254/300\n",
      "Average training loss: 0.025458009276125165\n",
      "Average test loss: 0.002107251307616631\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025775042624937163\n",
      "Average test loss: 0.004292331578209996\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02557626015279028\n",
      "Average test loss: 0.002065817172742552\n",
      "Epoch 257/300\n",
      "Average training loss: 0.025676061246130203\n",
      "Average test loss: 0.0021928199254390266\n",
      "Epoch 258/300\n",
      "Average training loss: 0.025477976895040937\n",
      "Average test loss: 0.002505553530322181\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02540093265308274\n",
      "Average test loss: 0.002377230515289638\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02537639281484816\n",
      "Average test loss: 0.017791803975900015\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026334518414404658\n",
      "Average test loss: 0.004062839068058465\n",
      "Epoch 262/300\n",
      "Average training loss: 0.025377888136439854\n",
      "Average test loss: 0.002124888075929549\n",
      "Epoch 263/300\n",
      "Average training loss: 0.025333331988917455\n",
      "Average test loss: 0.00203363847380711\n",
      "Epoch 264/300\n",
      "Average training loss: 0.025413606885406705\n",
      "Average test loss: 0.0021332799192104075\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026995583621992007\n",
      "Average test loss: 0.0030096256461822324\n",
      "Epoch 266/300\n",
      "Average training loss: 0.025363743942644863\n",
      "Average test loss: 0.0027036440475947327\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0254878757910596\n",
      "Average test loss: 0.002778271573078301\n",
      "Epoch 268/300\n",
      "Average training loss: 0.025022610142827036\n",
      "Average test loss: 0.004498928682671653\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025212327268388537\n",
      "Average test loss: 0.002335913867379228\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02535698587861326\n",
      "Average test loss: 0.0022619031634595658\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025239412118991215\n",
      "Average test loss: 0.0027869460880756377\n",
      "Epoch 272/300\n",
      "Average training loss: 0.025341569556130303\n",
      "Average test loss: 0.0029685776895946926\n",
      "Epoch 273/300\n",
      "Average training loss: 0.025163489974207347\n",
      "Average test loss: 0.0022505068599970803\n",
      "Epoch 274/300\n",
      "Average training loss: 0.025423859003517364\n",
      "Average test loss: 0.0024945035164968834\n",
      "Epoch 275/300\n",
      "Average training loss: 0.025005225977963872\n",
      "Average test loss: 0.002081874314488636\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02587921699715985\n",
      "Average test loss: 0.0037713699156625403\n",
      "Epoch 277/300\n",
      "Average training loss: 0.025121839824650022\n",
      "Average test loss: 0.002763202199091514\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02496554161277082\n",
      "Average test loss: 0.006703692632209923\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025052309234937033\n",
      "Average test loss: 0.002105354667899923\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02505040852063232\n",
      "Average test loss: 0.0025445814162699714\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02580294031732612\n",
      "Average test loss: 0.002153706061032911\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02507832277648979\n",
      "Average test loss: 0.0035544763112233743\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02512723987466759\n",
      "Average test loss: 0.00257222281396389\n",
      "Epoch 284/300\n",
      "Average training loss: 0.024994270296560393\n",
      "Average test loss: 0.002069015978732043\n",
      "Epoch 285/300\n",
      "Average training loss: 0.024927985237704384\n",
      "Average test loss: 0.0020654773209244012\n",
      "Epoch 286/300\n",
      "Average training loss: 0.025441516952382195\n",
      "Average test loss: 0.0024006323997552196\n",
      "Epoch 287/300\n",
      "Average training loss: 0.025016558289527892\n",
      "Average test loss: 0.1229544906400972\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02495041115085284\n",
      "Average test loss: 0.0022478594791351092\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02477575376795398\n",
      "Average test loss: 0.0021038885908201336\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02482617853747474\n",
      "Average test loss: 0.0021749413885797063\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0253223468263944\n",
      "Average test loss: 0.0022737280326998897\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02510224688384268\n",
      "Average test loss: 0.01209831819186608\n",
      "Epoch 293/300\n",
      "Average training loss: 0.024577968471580083\n",
      "Average test loss: 0.0021863178176184496\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025083759523100324\n",
      "Average test loss: 0.002473520370717678\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025025122692187627\n",
      "Average test loss: 0.0023060546765724817\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05841513382891814\n",
      "Average test loss: 0.0019773112525128655\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04667311502827538\n",
      "Average test loss: 0.0019051264730385608\n",
      "Epoch 298/300\n",
      "Average training loss: 0.039602417684263654\n",
      "Average test loss: 0.0018476641743133465\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03588033304280705\n",
      "Average test loss: 0.0019223942255808246\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03333726394176483\n",
      "Average test loss: 0.002554077609959576\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.382207987467448\n",
      "Average test loss: 0.01040843043062422\n",
      "Epoch 2/300\n",
      "Average training loss: 4.676897820790609\n",
      "Average test loss: 0.00823057636866967\n",
      "Epoch 3/300\n",
      "Average training loss: 3.8147783442603216\n",
      "Average test loss: 0.006363263949751854\n",
      "Epoch 4/300\n",
      "Average training loss: 3.3085246098836265\n",
      "Average test loss: 0.00981481250623862\n",
      "Epoch 5/300\n",
      "Average training loss: 2.9226565905676947\n",
      "Average test loss: 0.0050879501977728475\n",
      "Epoch 6/300\n",
      "Average training loss: 2.623798885345459\n",
      "Average test loss: 0.004216228330921796\n",
      "Epoch 7/300\n",
      "Average training loss: 2.3861703571743433\n",
      "Average test loss: 0.014465640819734997\n",
      "Epoch 8/300\n",
      "Average training loss: 2.1883068033854167\n",
      "Average test loss: 0.0044773908172630605\n",
      "Epoch 9/300\n",
      "Average training loss: 2.024226875729031\n",
      "Average test loss: 0.0038343508535375196\n",
      "Epoch 10/300\n",
      "Average training loss: 1.8873575457466973\n",
      "Average test loss: 0.0039362922486745645\n",
      "Epoch 11/300\n",
      "Average training loss: 1.7683862105475532\n",
      "Average test loss: 0.00407314336196416\n",
      "Epoch 12/300\n",
      "Average training loss: 1.6599336711035835\n",
      "Average test loss: 0.0031384314373135566\n",
      "Epoch 13/300\n",
      "Average training loss: 1.5544081490834554\n",
      "Average test loss: 0.0032511039326588312\n",
      "Epoch 14/300\n",
      "Average training loss: 1.4516547551684909\n",
      "Average test loss: 0.0030300882591141596\n",
      "Epoch 15/300\n",
      "Average training loss: 1.3520934811698067\n",
      "Average test loss: 0.0181487724069092\n",
      "Epoch 16/300\n",
      "Average training loss: 1.2573892318937514\n",
      "Average test loss: 0.00256478008069098\n",
      "Epoch 17/300\n",
      "Average training loss: 1.1643416324191622\n",
      "Average test loss: 0.0028004099921219877\n",
      "Epoch 18/300\n",
      "Average training loss: 1.0748933910793728\n",
      "Average test loss: 0.00231390787071238\n",
      "Epoch 19/300\n",
      "Average training loss: 0.9872290884653727\n",
      "Average test loss: 0.0022995580304414035\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9077186779446073\n",
      "Average test loss: 0.002511073854751885\n",
      "Epoch 21/300\n",
      "Average training loss: 0.8342543987698026\n",
      "Average test loss: 0.008137033205065463\n",
      "Epoch 22/300\n",
      "Average training loss: 0.7692195483313666\n",
      "Average test loss: 0.30288463751475014\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7132778503629896\n",
      "Average test loss: 0.00260904398570872\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6617483939594693\n",
      "Average test loss: 0.004075724730889002\n",
      "Epoch 25/300\n",
      "Average training loss: 0.6139213729964362\n",
      "Average test loss: 0.002039882530354791\n",
      "Epoch 26/300\n",
      "Average training loss: 0.5673346202108596\n",
      "Average test loss: 0.0019028788290710913\n",
      "Epoch 27/300\n",
      "Average training loss: 0.5248523214128282\n",
      "Average test loss: 0.0018918556130180755\n",
      "Epoch 28/300\n",
      "Average training loss: 0.4827781414190928\n",
      "Average test loss: 0.003235182201075885\n",
      "Epoch 29/300\n",
      "Average training loss: 0.44355035532845394\n",
      "Average test loss: 0.004357882337748176\n",
      "Epoch 30/300\n",
      "Average training loss: 0.4075366974936591\n",
      "Average test loss: 0.023253094196733503\n",
      "Epoch 31/300\n",
      "Average training loss: 0.3748023596074846\n",
      "Average test loss: 0.003937022858195835\n",
      "Epoch 32/300\n",
      "Average training loss: 0.3431960527367062\n",
      "Average test loss: 0.3223535620768865\n",
      "Epoch 33/300\n",
      "Average training loss: 0.31535784647199844\n",
      "Average test loss: 0.004717771173765262\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2888737338516447\n",
      "Average test loss: 0.9283418773276111\n",
      "Epoch 35/300\n",
      "Average training loss: 0.264352581249343\n",
      "Average test loss: 0.001543952505569905\n",
      "Epoch 36/300\n",
      "Average training loss: 0.24250924221674602\n",
      "Average test loss: 0.01181097025041365\n",
      "Epoch 37/300\n",
      "Average training loss: 0.22327076847023433\n",
      "Average test loss: 0.0027903536710267266\n",
      "Epoch 38/300\n",
      "Average training loss: 0.2050228363672892\n",
      "Average test loss: 0.01667897386766142\n",
      "Epoch 39/300\n",
      "Average training loss: 0.19006705213917627\n",
      "Average test loss: 0.0017191261505294178\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1773807050784429\n",
      "Average test loss: 0.001688422971405089\n",
      "Epoch 41/300\n",
      "Average training loss: 0.16527143564489152\n",
      "Average test loss: 0.001457951485696766\n",
      "Epoch 42/300\n",
      "Average training loss: 0.15428223278125128\n",
      "Average test loss: 0.033282872075421945\n",
      "Epoch 43/300\n",
      "Average training loss: 0.14433464592695236\n",
      "Average test loss: 0.0017204178979413377\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1339271505408817\n",
      "Average test loss: 0.0035660642640044293\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1231972835196389\n",
      "Average test loss: 0.0014637268565905592\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1116458824608061\n",
      "Average test loss: 0.001608053587584032\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10127753827969233\n",
      "Average test loss: 0.034759081737034854\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09063020475705465\n",
      "Average test loss: 0.0020093883254254856\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08198891674147712\n",
      "Average test loss: 0.0018703775291020672\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07407644923528035\n",
      "Average test loss: 0.00127978368403597\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06775707609785928\n",
      "Average test loss: 0.001602199395135459\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06159529825051625\n",
      "Average test loss: 0.00138212544300283\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05721826465262307\n",
      "Average test loss: 0.0015964242762161625\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0534951847228739\n",
      "Average test loss: 0.0012747774076544577\n",
      "Epoch 55/300\n",
      "Average training loss: 0.049472810135947336\n",
      "Average test loss: 0.0012782952450215816\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06620076808333397\n",
      "Average test loss: 0.0012785667959186765\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04765039596623845\n",
      "Average test loss: 0.0016078125043875642\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04424276447958416\n",
      "Average test loss: 0.003428311554508077\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04223962875869539\n",
      "Average test loss: 0.6041941351774666\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04035099074575636\n",
      "Average test loss: 0.0012558014564112657\n",
      "Epoch 61/300\n",
      "Average training loss: 0.038902187178532284\n",
      "Average test loss: 0.0015113331452012062\n",
      "Epoch 62/300\n",
      "Average training loss: 0.037479547773798305\n",
      "Average test loss: 0.7173276049296061\n",
      "Epoch 63/300\n",
      "Average training loss: 0.036957280649079216\n",
      "Average test loss: 0.0034952218476682903\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04684785696532991\n",
      "Average test loss: 0.0012664312315690848\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0408120837741428\n",
      "Average test loss: 0.0014547345974586076\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03831158850424819\n",
      "Average test loss: 0.0013984819114622143\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03496218132972717\n",
      "Average test loss: 0.004208347310415573\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03380200677116712\n",
      "Average test loss: 0.002151731196894414\n",
      "Epoch 69/300\n",
      "Average training loss: 0.032843118530180716\n",
      "Average test loss: 0.0016434745012472074\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03227813453806771\n",
      "Average test loss: 0.0013923672304178279\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03170659430159463\n",
      "Average test loss: 0.001313192994851205\n",
      "Epoch 72/300\n",
      "Average training loss: 0.031125869072145887\n",
      "Average test loss: 0.001368648098471264\n",
      "Epoch 73/300\n",
      "Average training loss: 0.030737274285819797\n",
      "Average test loss: 0.0021231608263009953\n",
      "Epoch 74/300\n",
      "Average training loss: 0.030713343317310016\n",
      "Average test loss: 0.001381928106252518\n",
      "Epoch 75/300\n",
      "Average training loss: 0.032848620663086574\n",
      "Average test loss: 0.0013687545073529084\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03263438476787673\n",
      "Average test loss: 0.001543399731007715\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0304755604200893\n",
      "Average test loss: 0.0012321995033158196\n",
      "Epoch 78/300\n",
      "Average training loss: 0.029570960642562972\n",
      "Average test loss: 0.0012346106407543024\n",
      "Epoch 79/300\n",
      "Average training loss: 0.029081003349688318\n",
      "Average test loss: 0.0014713186603039503\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02867691554956966\n",
      "Average test loss: 0.0025840761101701194\n",
      "Epoch 81/300\n",
      "Average training loss: 0.028448690420223608\n",
      "Average test loss: 0.0012951226020231842\n",
      "Epoch 82/300\n",
      "Average training loss: 0.028072725547684564\n",
      "Average test loss: 0.0012115949810379081\n",
      "Epoch 83/300\n",
      "Average training loss: 0.027792566211687193\n",
      "Average test loss: 0.0014188966102277238\n",
      "Epoch 84/300\n",
      "Average training loss: 0.027932533169786136\n",
      "Average test loss: 0.47046731713083056\n",
      "Epoch 85/300\n",
      "Average training loss: 0.028133790988061163\n",
      "Average test loss: 0.0012332195185331834\n",
      "Epoch 86/300\n",
      "Average training loss: 0.027508309710356924\n",
      "Average test loss: 0.0013468803041097191\n",
      "Epoch 87/300\n",
      "Average training loss: 0.026744965083069273\n",
      "Average test loss: 0.002271612096267442\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02670180301864942\n",
      "Average test loss: 0.0012631694412686759\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02656858423186673\n",
      "Average test loss: 0.001265256354274849\n",
      "Epoch 90/300\n",
      "Average training loss: 0.026400540211134485\n",
      "Average test loss: 0.0012714750397329528\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02600574588444498\n",
      "Average test loss: 0.0012937633730471135\n",
      "Epoch 92/300\n",
      "Average training loss: 0.025947179423438178\n",
      "Average test loss: 0.0013898935132763452\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02606968835161792\n",
      "Average test loss: 0.001313367985913323\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025689702775743273\n",
      "Average test loss: 0.00185167197841737\n",
      "Epoch 95/300\n",
      "Average training loss: 0.025336383369233873\n",
      "Average test loss: 0.0032837568687068093\n",
      "Epoch 96/300\n",
      "Average training loss: 0.025220540346370803\n",
      "Average test loss: 0.0021661558672785757\n",
      "Epoch 97/300\n",
      "Average training loss: 0.024990962887803714\n",
      "Average test loss: 0.0012955083428985543\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024874550097518495\n",
      "Average test loss: 0.0032828720112641654\n",
      "Epoch 99/300\n",
      "Average training loss: 0.025020588916209008\n",
      "Average test loss: 0.0038443781051577795\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02609267507162359\n",
      "Average test loss: 0.06738432005420328\n",
      "Epoch 101/300\n",
      "Average training loss: 0.025195595877038107\n",
      "Average test loss: 0.0013249314655032422\n",
      "Epoch 102/300\n",
      "Average training loss: 0.024626120979587236\n",
      "Average test loss: 0.001412202607219418\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02424228888253371\n",
      "Average test loss: 0.0014065621664954555\n",
      "Epoch 104/300\n",
      "Average training loss: 0.024075708489451144\n",
      "Average test loss: 0.0012797484706259436\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02406135455932882\n",
      "Average test loss: 0.0023141839067555136\n",
      "Epoch 106/300\n",
      "Average training loss: 0.024014108343256846\n",
      "Average test loss: 0.001362215482733316\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023870247157083618\n",
      "Average test loss: 0.0013667005867593817\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024220711620317564\n",
      "Average test loss: 0.0013140510881526602\n",
      "Epoch 109/300\n",
      "Average training loss: 0.023823280847734874\n",
      "Average test loss: 0.0013204803470936086\n",
      "Epoch 110/300\n",
      "Average training loss: 0.023579018362694316\n",
      "Average test loss: 0.001290887598051793\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0234546009550492\n",
      "Average test loss: 0.0034209793479078347\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02342324780093299\n",
      "Average test loss: 0.0013825023581998217\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023843141620357833\n",
      "Average test loss: 0.002886248556897044\n",
      "Epoch 114/300\n",
      "Average training loss: 0.025016823975576294\n",
      "Average test loss: 0.0012313332857771051\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023174699029988713\n",
      "Average test loss: 0.0012647539177495573\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02298036556939284\n",
      "Average test loss: 0.0013107405733317136\n",
      "Epoch 117/300\n",
      "Average training loss: 0.023631153320272765\n",
      "Average test loss: 0.0020755953803244563\n",
      "Epoch 118/300\n",
      "Average training loss: 0.024131484657526018\n",
      "Average test loss: 0.001398548311036494\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023115805312991143\n",
      "Average test loss: 0.0013086123936292197\n",
      "Epoch 120/300\n",
      "Average training loss: 0.022873815789818765\n",
      "Average test loss: 0.0012590987566444608\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02271563174492783\n",
      "Average test loss: 0.0012802200548143851\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02307308926847246\n",
      "Average test loss: 0.0012799443912485408\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02248360316786501\n",
      "Average test loss: 0.0013256476169659032\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0223930959602197\n",
      "Average test loss: 0.0017521300809457897\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02249898799426026\n",
      "Average test loss: 0.0015113169140078956\n",
      "Epoch 126/300\n",
      "Average training loss: 0.022432489257719783\n",
      "Average test loss: 0.0013184326415260632\n",
      "Epoch 127/300\n",
      "Average training loss: 0.022283400272329647\n",
      "Average test loss: 0.0013305435277935532\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0224395259635316\n",
      "Average test loss: 0.0013261756998383337\n",
      "Epoch 129/300\n",
      "Average training loss: 0.022238195776939394\n",
      "Average test loss: 0.0015929292691871525\n",
      "Epoch 130/300\n",
      "Average training loss: 0.022166085829337438\n",
      "Average test loss: 0.0013050510924723414\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0225816001875533\n",
      "Average test loss: 0.0012743285144679248\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021928487550881174\n",
      "Average test loss: 0.001332852810828222\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02207553036345376\n",
      "Average test loss: 0.001638996127165026\n",
      "Epoch 134/300\n",
      "Average training loss: 0.021815306363834275\n",
      "Average test loss: 0.0014753602717278733\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021760547649529245\n",
      "Average test loss: 0.0013360098977055815\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02171983654052019\n",
      "Average test loss: 0.0013704504137858748\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021609825210438836\n",
      "Average test loss: 0.0014150935779325665\n",
      "Epoch 138/300\n",
      "Average training loss: 0.021959642622205944\n",
      "Average test loss: 0.0013702856757574612\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021689455972777473\n",
      "Average test loss: 0.0019740872301368248\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021567911808689437\n",
      "Average test loss: 0.0014002653894325098\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021401658371090888\n",
      "Average test loss: 0.0013611492721570862\n",
      "Epoch 142/300\n",
      "Average training loss: 0.021365337156587175\n",
      "Average test loss: 0.0012918595735811524\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021313007666005027\n",
      "Average test loss: 0.0014009798552013105\n",
      "Epoch 144/300\n",
      "Average training loss: 0.021466294560167524\n",
      "Average test loss: 0.001348066776421749\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02113197578655349\n",
      "Average test loss: 0.001311265297441019\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02125015882982148\n",
      "Average test loss: 0.0013712034866006839\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02113910094400247\n",
      "Average test loss: 0.001328435487113893\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02125777213772138\n",
      "Average test loss: 0.001480828077532351\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02137232269677851\n",
      "Average test loss: 0.0013246704913261863\n",
      "Epoch 150/300\n",
      "Average training loss: 0.020976342999272875\n",
      "Average test loss: 0.0016555988933477136\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021144422651992904\n",
      "Average test loss: 0.0034200682213736907\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021074374162488512\n",
      "Average test loss: 0.0013712051323511534\n",
      "Epoch 153/300\n",
      "Average training loss: 0.020891538553767734\n",
      "Average test loss: 0.0013053629841241572\n",
      "Epoch 154/300\n",
      "Average training loss: 0.020826873833934466\n",
      "Average test loss: 0.001400002418189413\n",
      "Epoch 155/300\n",
      "Average training loss: 0.020830431542462773\n",
      "Average test loss: 0.001353234045414461\n",
      "Epoch 156/300\n",
      "Average training loss: 0.020737179840604466\n",
      "Average test loss: 0.0014046214410207338\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02419528706702921\n",
      "Average test loss: 0.0013394021785093678\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0214305080903901\n",
      "Average test loss: 0.0014285032646730541\n",
      "Epoch 159/300\n",
      "Average training loss: 0.020736278070343866\n",
      "Average test loss: 0.0013712421147566703\n",
      "Epoch 160/300\n",
      "Average training loss: 0.020659888792369102\n",
      "Average test loss: 0.00131782805842037\n",
      "Epoch 161/300\n",
      "Average training loss: 0.020607905194163324\n",
      "Average test loss: 0.0021865470024446648\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02050058753291766\n",
      "Average test loss: 0.0014439939364997876\n",
      "Epoch 163/300\n",
      "Average training loss: 0.020612873759534623\n",
      "Average test loss: 0.001366098915433718\n",
      "Epoch 164/300\n",
      "Average training loss: 0.020526284452941683\n",
      "Average test loss: 0.0013205143240176969\n",
      "Epoch 165/300\n",
      "Average training loss: 0.020558816007441943\n",
      "Average test loss: 0.0013613144707762532\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02048353491226832\n",
      "Average test loss: 0.0014591803283741077\n",
      "Epoch 167/300\n",
      "Average training loss: 0.020408392931024234\n",
      "Average test loss: 0.001425357647240162\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02038610577583313\n",
      "Average test loss: 0.001412690316637357\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02058482027053833\n",
      "Average test loss: 0.0013655144705747564\n",
      "Epoch 170/300\n",
      "Average training loss: 0.020302290021545356\n",
      "Average test loss: 0.0013970574614488415\n",
      "Epoch 171/300\n",
      "Average training loss: 0.020237450853817993\n",
      "Average test loss: 0.0014384802569531732\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02031840304699209\n",
      "Average test loss: 0.002082817791443732\n",
      "Epoch 173/300\n",
      "Average training loss: 0.020447129877077207\n",
      "Average test loss: 0.0014030876871612336\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02019023603697618\n",
      "Average test loss: 0.0016014870456937287\n",
      "Epoch 175/300\n",
      "Average training loss: 0.020138785385423238\n",
      "Average test loss: 0.0014656410900255044\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02021438865032461\n",
      "Average test loss: 0.001437451986492508\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01998994846145312\n",
      "Average test loss: 0.0015363195323281817\n",
      "Epoch 178/300\n",
      "Average training loss: 0.020461499429411358\n",
      "Average test loss: 0.0013322610299413404\n",
      "Epoch 179/300\n",
      "Average training loss: 0.020687579404976633\n",
      "Average test loss: 0.0022190927426434227\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02015805981391006\n",
      "Average test loss: 0.0013625948576049673\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019984052727619807\n",
      "Average test loss: 0.0016714105982747344\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019880142526494133\n",
      "Average test loss: 0.001428569904011157\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019885904749234518\n",
      "Average test loss: 0.0013691177539941338\n",
      "Epoch 184/300\n",
      "Average training loss: 0.020253832634952333\n",
      "Average test loss: 0.0013407140876063042\n",
      "Epoch 185/300\n",
      "Average training loss: 0.019871324916680654\n",
      "Average test loss: 0.0014855739666769902\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019833009677628678\n",
      "Average test loss: 0.00251810635595272\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02074331480761369\n",
      "Average test loss: 0.0013409208067589335\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0198221153534121\n",
      "Average test loss: 0.001619201787540482\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019836889477239716\n",
      "Average test loss: 0.0018623721576813195\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019767025116417144\n",
      "Average test loss: 0.0013849964423312082\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02121416616274251\n",
      "Average test loss: 0.001373161522982021\n",
      "Epoch 192/300\n",
      "Average training loss: 0.020900197401642798\n",
      "Average test loss: 0.0013429188043293026\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019692305396828386\n",
      "Average test loss: 0.0013624790921393367\n",
      "Epoch 194/300\n",
      "Average training loss: 0.019496151121126282\n",
      "Average test loss: 0.0015184446709851423\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019806293119986853\n",
      "Average test loss: 0.0016308492239978578\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019672801540957557\n",
      "Average test loss: 0.0013762126233842638\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01992510884669092\n",
      "Average test loss: 0.0013751013024399677\n",
      "Epoch 198/300\n",
      "Average training loss: 0.019520902703205743\n",
      "Average test loss: 0.0013533347570854756\n",
      "Epoch 199/300\n",
      "Average training loss: 0.019532701964179673\n",
      "Average test loss: 0.001382182206544611\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020357752222153875\n",
      "Average test loss: 0.0014627068583957022\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019527870603733594\n",
      "Average test loss: 0.0013356768031501107\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02029348738822672\n",
      "Average test loss: 0.0013610335052427318\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01932120635608832\n",
      "Average test loss: 0.0013596464125439525\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01945275489985943\n",
      "Average test loss: 0.0014194909801913632\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019657152086496352\n",
      "Average test loss: 0.0013806642519517077\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01937036169403129\n",
      "Average test loss: 0.001407020406383607\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0194044029712677\n",
      "Average test loss: 0.001422969506846534\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019413525569770072\n",
      "Average test loss: 0.0014032682700910503\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01950056572092904\n",
      "Average test loss: 0.0013748604248588283\n",
      "Epoch 210/300\n",
      "Average training loss: 0.019500757368074523\n",
      "Average test loss: 0.0018497201817938022\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01929677602979872\n",
      "Average test loss: 0.0013814817853271962\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01953215387629138\n",
      "Average test loss: 0.0014260136619106763\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019248994189831947\n",
      "Average test loss: 0.0014239196286847194\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0193402883708477\n",
      "Average test loss: 0.001454315688771506\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02237835614548789\n",
      "Average test loss: 0.0023022952015615173\n",
      "Epoch 216/300\n",
      "Average training loss: 0.019351522694031398\n",
      "Average test loss: 0.0014065858162939549\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01899109601974487\n",
      "Average test loss: 0.001450654418932067\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01921755372815662\n",
      "Average test loss: 0.001417430428839806\n",
      "Epoch 219/300\n",
      "Average training loss: 0.019136312797665596\n",
      "Average test loss: 0.0014033041892366276\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02002219496005111\n",
      "Average test loss: 0.0022940785797933736\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01910443361269103\n",
      "Average test loss: 0.0014300510354029637\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01900062855747011\n",
      "Average test loss: 0.0014876256359534132\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01914184151507086\n",
      "Average test loss: 0.0016152875067459212\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01896341304646598\n",
      "Average test loss: 0.0014637166013837688\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019140162891811793\n",
      "Average test loss: 0.0014763554900160266\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01903548201918602\n",
      "Average test loss: 0.0015217240698532097\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018973487180140282\n",
      "Average test loss: 0.001443854878967007\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01998500679930051\n",
      "Average test loss: 0.0014274780201829142\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02016920018196106\n",
      "Average test loss: 0.0014184097866010335\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018901605545646613\n",
      "Average test loss: 0.0014707923765397734\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018903124996357493\n",
      "Average training loss: 0.04265716796782282\n",
      "Average test loss: 0.0014166002482589749\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03560826259023613\n",
      "Average test loss: 0.0013294209324651294\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03178798990779453\n",
      "Average test loss: 0.0012790312856021855\n",
      "Epoch 236/300\n",
      "Average training loss: 0.027305719781253072\n",
      "Average test loss: 0.004270833254274395\n",
      "Epoch 238/300\n",
      "Average training loss: 0.025652361828419897\n",
      "Average test loss: 0.0012998120247696837\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02423686866958936\n",
      "Average test loss: 0.001372336391152607\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02303581056992213\n",
      "Average test loss: 0.001351679219527998\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02190682717826631\n",
      "Average test loss: 0.00138347664480615\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021061627361509534\n",
      "Average test loss: 0.0013990275904329286\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020331202112966115\n",
      "Average test loss: 0.0013871693742564984\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01980487228764428\n",
      "Average test loss: 0.0013879684223276046\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02004202008826865\n",
      "Average test loss: 0.0014024682186750902\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01923648994995488\n",
      "Average test loss: 0.0013931611368639602\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01893555402921306\n",
      "Average test loss: 0.0014457782216680546\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01889661968416638\n",
      "Average test loss: 0.001433700904664066\n",
      "Epoch 250/300\n",
      "Average training loss: 0.019469087762965097\n",
      "Average test loss: 0.0019400602739511265\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01885415245178673\n",
      "Average test loss: 0.0016380162317719724\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018868797663185333\n",
      "Average test loss: 0.001441320826506449\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021574728320042293\n",
      "Average test loss: 0.0014049141560163762\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01930845511621899\n",
      "Average test loss: 0.001413019022386935\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019481728391514883\n",
      "Average test loss: 0.0014319155948857465\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018794243095649612\n",
      "Average test loss: 0.001426663818458716\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018831248100433084\n",
      "Average test loss: 0.0015552265220839115\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018763040884501404\n",
      "Average test loss: 0.0015560538803951608\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01907856693201595\n",
      "Average test loss: 0.0017709996052500275\n",
      "Epoch 260/300\n",
      "Average training loss: 0.018819734791914622\n",
      "Average test loss: 0.0013820582816584243\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018816945119036567\n",
      "Average test loss: 0.001431771779484633\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018665030553936958\n",
      "Average test loss: 0.0014455947858384914\n",
      "Epoch 263/300\n",
      "Average training loss: 0.019118857845664024\n",
      "Average test loss: 0.0013906762860715388\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018954571468962563\n",
      "Average test loss: 0.0014395612398576405\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018778452191915776\n",
      "Average test loss: 0.0014465425613646706\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018722843815883\n",
      "Average test loss: 0.0014168494697660208\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018680806123548083\n",
      "Average test loss: 0.0014651541670577394\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01865506779650847\n",
      "Average test loss: 0.0014533494971692561\n",
      "Epoch 269/300\n",
      "Average training loss: 0.018544002107448047\n",
      "Average test loss: 0.0013832831353776985\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018825904248489275\n",
      "Average test loss: 0.0014448769599613217\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01862374816669358\n",
      "Average test loss: 0.001550940110348165\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018884020890626643\n",
      "Average test loss: 0.0014745910950005054\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018553101941115327\n",
      "Average test loss: 0.0014663327843364741\n",
      "Epoch 274/300\n",
      "Average training loss: 0.018435568576057753\n",
      "Average test loss: 0.0014270866516356666\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01850712768567933\n",
      "Average test loss: 0.0014600690330068271\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018752136308285924\n",
      "Average test loss: 0.0014452782020800644\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018423908169070878\n",
      "Average test loss: 0.0014243655130784544\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01899048684537411\n",
      "Average test loss: 0.0014323338987305761\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018543762061331008\n",
      "Average test loss: 0.0014582679622496169\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01833593753973643\n",
      "Average test loss: 0.0025103416364226075\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01852177899082502\n",
      "Average training loss: 0.018463974787129297\n",
      "Average test loss: 0.0014486748817273312\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018462288079990282\n",
      "Average test loss: 0.001463433733727369\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020040265114770996\n",
      "Average test loss: 0.0013870133105665446\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018321878829763997\n",
      "Average test loss: 0.0014406241704709828\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018372104389799967\n",
      "Average test loss: 0.0014215911836363375\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01867896364132563\n",
      "Average test loss: 0.0014459720886208945\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018208814524941976\n",
      "Average test loss: 0.0014571456529407039\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01819211633503437\n",
      "Average test loss: 0.0014326964807179238\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018258093825644917\n",
      "Average test loss: 0.0014573405547998845\n",
      "Epoch 292/300\n",
      "Average training loss: 0.018652560182743604\n",
      "Average test loss: 0.0014129924735882217\n",
      "Epoch 293/300\n",
      "Average training loss: 0.018590362603465718\n",
      "Average test loss: 0.0014355087761456769\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018201065493954553\n",
      "Average test loss: 0.0014130075625661347\n",
      "Epoch 295/300\n",
      "Average test loss: 0.001431898010066814\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018330407422449853\n",
      "Average test loss: 0.0014662680880477032\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01818963072200616\n",
      "Average test loss: 0.0014641022234120303\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01821023439698749\n",
      "Average test loss: 0.0014107528298886287\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01830161098225249\n",
      "Average test loss: 0.001485829933029082\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_50_Depth15/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 19.27\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.22\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.70\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.17\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.04\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.30\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.79\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.75\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.08\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.77\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.05\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.86\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.94\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.61\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.96\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.19\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.34\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.82\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.09\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.29\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.62\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.06\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.89\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.68\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.82\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.58\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.63\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.99\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.30\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 65.07700766669379\n",
      "Average test loss: 4.821692624325554\n",
      "Epoch 2/300\n",
      "Average training loss: 32.12723956807454\n",
      "Average test loss: 0.020783852537473044\n",
      "Epoch 4/300\n",
      "Average training loss: 29.73961782667372\n",
      "Average test loss: 119.754768352641\n",
      "Epoch 5/300\n",
      "Average training loss: 27.639734525892468\n",
      "Average test loss: 12.781595482501718\n",
      "Epoch 6/300\n",
      "Average training loss: 23.793830022176106\n",
      "Average test loss: 0.025469981169535055\n",
      "Epoch 8/300\n",
      "Average training loss: 22.533632010565864\n",
      "Average test loss: 72.59531485147609\n",
      "Epoch 9/300\n",
      "Average training loss: 21.595216466267903\n",
      "Average test loss: 1.2070502962403826\n",
      "Epoch 10/300\n",
      "Average training loss: 20.65962612406413\n",
      "Average test loss: 0.12026891608205106\n",
      "Epoch 11/300\n",
      "Average training loss: 19.930137412177192\n",
      "Average test loss: 270.34175645503734\n",
      "Epoch 12/300\n",
      "Average training loss: 18.963256486680773\n",
      "Average test loss: 0.012146763412488831\n",
      "Epoch 13/300\n",
      "Average training loss: 17.234512724134657\n",
      "Average test loss: 0.008052324153482913\n",
      "Epoch 15/300\n",
      "Average training loss: 16.399541163126628\n",
      "Average test loss: 1.9904429433080886\n",
      "Epoch 16/300\n",
      "Average training loss: 15.647366655985515\n",
      "Average test loss: 0.0582015025201771\n",
      "Epoch 17/300\n",
      "Average training loss: 14.922969570583767\n",
      "Average test loss: 11.919737612856759\n",
      "Epoch 18/300\n",
      "Average training loss: 14.298475659688314\n",
      "Average test loss: 0.00698577885909213\n",
      "Epoch 19/300\n",
      "Average training loss: 13.697840816921659\n",
      "Average training loss: 13.154955849541558\n",
      "Average test loss: 0.6574590094362696\n",
      "Epoch 21/300\n",
      "Average training loss: 12.669040334913467\n",
      "Average test loss: 0.04975611563771963\n",
      "Epoch 22/300\n",
      "Average training loss: 12.185633765326607\n",
      "Average test loss: 12.07311039987372\n",
      "Epoch 23/300\n",
      "Average training loss: 11.765921309577093\n",
      "Average test loss: 0.017924241494801308\n",
      "Epoch 24/300\n",
      "Average training loss: 11.3547398469713\n",
      "Average test loss: 0.005914389294882616\n",
      "Epoch 25/300\n",
      "Average training loss: 10.948072503831652\n",
      "Average test loss: 0.20913510097397697\n",
      "Epoch 26/300\n",
      "Average training loss: 10.158565728929307\n",
      "Average test loss: 0.007672626647684309\n",
      "Epoch 28/300\n",
      "Average training loss: 9.750467589484321\n",
      "Average test loss: 0.005670734533833133\n",
      "Epoch 29/300\n",
      "Average training loss: 9.363114183213975\n",
      "Average test loss: 0.017008287231541342\n",
      "Epoch 30/300\n",
      "Average training loss: 8.961048531426323\n",
      "Average test loss: 0.00932483857538965\n",
      "Epoch 31/300\n",
      "Average training loss: 8.608084075927735\n",
      "Average test loss: 0.005620660086265869\n",
      "Epoch 32/300\n",
      "Average training loss: 7.871049070570204\n",
      "Average test loss: 0.0063554289659692184\n",
      "Epoch 34/300\n",
      "Average training loss: 7.533478858523899\n",
      "Average test loss: 167353.24608412015\n",
      "Epoch 35/300\n",
      "Average training loss: 7.173953387366401\n",
      "Average test loss: 0.0058089517090055675\n",
      "Epoch 36/300\n",
      "Average training loss: 6.781220382266574\n",
      "Average test loss: 2.4588161572135157\n",
      "Epoch 37/300\n",
      "Average training loss: 6.406956665039062\n",
      "Average test loss: 0.005292920582617323\n",
      "Epoch 38/300\n",
      "Average training loss: 6.016694984436035\n",
      "Average test loss: 0.01077279050482644\n",
      "Epoch 39/300\n",
      "Average training loss: 5.66058184644911\n",
      "Average test loss: 0.04963901982622014\n",
      "Epoch 40/300\n",
      "Average training loss: 5.285775989108616\n",
      "Average test loss: 0.005859154659840796\n",
      "Epoch 41/300\n",
      "Average training loss: 4.950555619133843\n",
      "Average test loss: 1012.7145721222527\n",
      "Epoch 42/300\n",
      "Average training loss: 4.628612836201985\n",
      "Average test loss: 0.0051061183495654\n",
      "Epoch 43/300\n",
      "Average training loss: 4.308681635538737\n",
      "Average test loss: 0.04093934663385153\n",
      "Epoch 44/300\n",
      "Average training loss: 4.004018529256185\n",
      "Average test loss: 0.01219512541550729\n",
      "Epoch 45/300\n",
      "Average training loss: 3.7228398672739664\n",
      "Average test loss: 3.8052320704969267\n",
      "Epoch 46/300\n",
      "Average training loss: 3.4938368176354304\n",
      "Average test loss: 0.00664049552132686\n",
      "Epoch 47/300\n",
      "Average training loss: 3.294644781536526\n",
      "Average test loss: 0.005146498348149988\n",
      "Epoch 48/300\n",
      "Average training loss: 3.105728090922038\n",
      "Average test loss: 0.0057977029892305534\n",
      "Epoch 49/300\n",
      "Average training loss: 2.9322765725453697\n",
      "Average test loss: 0.009063740255932012\n",
      "Epoch 50/300\n",
      "Average training loss: 2.75738364537557\n",
      "Average test loss: 1.135196323359592\n",
      "Epoch 51/300\n",
      "Average training loss: 2.5899423739115397\n",
      "Average test loss: 5.557101215720177\n",
      "Epoch 52/300\n",
      "Average training loss: 2.439409326341417\n",
      "Average test loss: 0.0061383202212552225\n",
      "Epoch 53/300\n",
      "Average training loss: 2.295266846338908\n",
      "Average test loss: 0.005684200918922822\n",
      "Epoch 54/300\n",
      "Average training loss: 2.149862851460775\n",
      "Average test loss: 0.007463392958872848\n",
      "Epoch 55/300\n",
      "Average training loss: 2.0122797694736057\n",
      "Average test loss: 0.005433659483575159\n",
      "Epoch 56/300\n",
      "Average training loss: 1.8717841798994277\n",
      "Average test loss: 0.04635286439996627\n",
      "Epoch 57/300\n",
      "Average training loss: 1.740685941696167\n",
      "Average test loss: 0.009480092136396303\n",
      "Epoch 58/300\n",
      "Average training loss: 1.6227061547173394\n",
      "Average test loss: 0.33883180158336956\n",
      "Epoch 59/300\n",
      "Average training loss: 1.5177761919233534\n",
      "Average test loss: 5.149208997689188\n",
      "Epoch 60/300\n",
      "Average training loss: 1.417932082388136\n",
      "Average test loss: 0.005102314037167364\n",
      "Epoch 61/300\n",
      "Average training loss: 1.3263604716195\n",
      "Average test loss: 0.005178379767884811\n",
      "Epoch 62/300\n",
      "Average training loss: 1.2381431557337443\n",
      "Average test loss: 0.005632659864094522\n",
      "Epoch 63/300\n",
      "Average training loss: 1.1509930358462863\n",
      "Average test loss: 0.043327964487175144\n",
      "Epoch 64/300\n",
      "Average training loss: 1.0727386678059896\n",
      "Average test loss: 1.6553899017444087\n",
      "Epoch 65/300\n",
      "Average training loss: 0.9985547427071465\n",
      "Average test loss: 0.008978541650291947\n",
      "Epoch 66/300\n",
      "Average training loss: 0.8626495006349352\n",
      "Average test loss: 0.03663779375288222\n",
      "Epoch 68/300\n",
      "Average training loss: 0.8075420984692043\n",
      "Average test loss: 0.00516087311361399\n",
      "Epoch 69/300\n",
      "Average training loss: 0.7378795771598816\n",
      "Average test loss: 0.005999545658628146\n",
      "Epoch 70/300\n",
      "Average training loss: 0.6782761969566345\n",
      "Average test loss: 0.007669576847718821\n",
      "Epoch 71/300\n",
      "Average training loss: 0.6209763366381328\n",
      "Average test loss: 0.005291227301375733\n",
      "Epoch 72/300\n",
      "Average training loss: 0.5715050223668416\n",
      "Average test loss: 0.008491445701569319\n",
      "Epoch 73/300\n",
      "Average training loss: 0.5246755668322245\n",
      "Average test loss: 38.932899820226766\n",
      "Epoch 74/300\n",
      "Average training loss: 0.48665429057015314\n",
      "Average test loss: 0.005496438307894601\n",
      "Epoch 75/300\n",
      "Average training loss: 0.4520189140902625\n",
      "Average test loss: 0.1157144869764646\n",
      "Epoch 76/300\n",
      "Average training loss: 0.4211278558307224\n",
      "Average test loss: 0.010449491274025705\n",
      "Epoch 77/300\n",
      "Average training loss: 0.3933585092491574\n",
      "Average test loss: 0.03331169976211257\n",
      "Epoch 78/300\n",
      "Average training loss: 0.36570781305101185\n",
      "Average test loss: 0.04222161109579934\n",
      "Epoch 79/300\n",
      "Average training loss: 0.3412993613878886\n",
      "Average test loss: 0.005374585463561945\n",
      "Epoch 80/300\n",
      "Average training loss: 0.31978457230991786\n",
      "Average test loss: 0.007345094708104928\n",
      "Epoch 81/300\n",
      "Average training loss: 0.2975000470479329\n",
      "Average test loss: 0.009857438484827678\n",
      "Epoch 82/300\n",
      "Average training loss: 0.27889206144544815\n",
      "Average test loss: 0.1510902170240879\n",
      "Epoch 83/300\n",
      "Average training loss: 0.2659758973916372\n",
      "Average test loss: 0.007395790368732479\n",
      "Epoch 84/300\n",
      "Average training loss: 0.25338600479231943\n",
      "Average test loss: 0.00787431323942211\n",
      "Epoch 85/300\n",
      "Average training loss: 0.24489611536926692\n",
      "Average test loss: 7.181663583271205\n",
      "Epoch 86/300\n",
      "Average training loss: 0.2368474660449558\n",
      "Average test loss: 0.00588925840643545\n",
      "Epoch 87/300\n",
      "Average training loss: 0.2293451257414288\n",
      "Average test loss: 3.51655982005265\n",
      "Epoch 88/300\n",
      "Average training loss: 0.22433384357558356\n",
      "Average test loss: 0.0824948718564378\n",
      "Epoch 89/300\n",
      "Average training loss: 0.21775131889184315\n",
      "Average test loss: 0.011764077039228545\n",
      "Epoch 90/300\n",
      "Average training loss: 0.21293566022978888\n",
      "Average test loss: 0.005483127807784411\n",
      "Epoch 91/300\n",
      "Average training loss: 0.208179486407174\n",
      "Average test loss: 0.005660295731905434\n",
      "Epoch 92/300\n",
      "Average training loss: 0.20313133413261839\n",
      "Average test loss: 0.01204377587719096\n",
      "Epoch 93/300\n",
      "Average training loss: 0.20003473881880443\n",
      "Average test loss: 0.006689837041828368\n",
      "Epoch 94/300\n",
      "Average training loss: 0.1981516352229648\n",
      "Average test loss: 0.007698939829236931\n",
      "Epoch 95/300\n",
      "Average training loss: 0.19126785474353367\n",
      "Average test loss: 0.005673462622695499\n",
      "Epoch 96/300\n",
      "Average training loss: 0.19069044421778786\n",
      "Average test loss: 0.00728679707315233\n",
      "Epoch 97/300\n",
      "Average training loss: 0.18551937753624387\n",
      "Average test loss: 5.313154250294798\n",
      "Epoch 98/300\n",
      "Average training loss: 0.18207734253671434\n",
      "Average test loss: 13.302829884424806\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1793663775788413\n",
      "Average test loss: 0.008875881242669291\n",
      "Epoch 100/300\n",
      "Average training loss: 0.17549751289685567\n",
      "Average test loss: 0.9793785824767417\n",
      "Epoch 101/300\n",
      "Average training loss: 29387.15529718015\n",
      "Average test loss: 3295994297.443247\n",
      "Epoch 102/300\n",
      "Average training loss: 23.070422319200304\n",
      "Average test loss: 6725930270.139828\n",
      "Epoch 103/300\n",
      "Average training loss: 21.384450539482966\n",
      "Average test loss: 12721561029.206268\n",
      "Epoch 104/300\n",
      "Average training loss: 20.463883724636503\n",
      "Average test loss: 34207139738.332603\n",
      "Epoch 105/300\n",
      "Average training loss: 19.734063715616863\n",
      "Average test loss: 62668497.09870417\n",
      "Epoch 106/300\n",
      "Average training loss: 19.11344020758735\n",
      "Average test loss: 1894041664377871.8\n",
      "Epoch 107/300\n",
      "Average training loss: 18.54347733900282\n",
      "Average test loss: 15453947757286.66\n",
      "Epoch 108/300\n",
      "Average training loss: 17.978008812798393\n",
      "Average test loss: 146575056.7964355\n",
      "Epoch 109/300\n",
      "Average training loss: 17.356201688978405\n",
      "Average test loss: 1589313387.1147516\n",
      "Epoch 110/300\n",
      "Average training loss: 16.69183061557346\n",
      "Average test loss: 5134153756751.017\n",
      "Epoch 111/300\n",
      "Average training loss: 15.994909200032552\n",
      "Average test loss: 11593013609.138979\n",
      "Epoch 112/300\n",
      "Average training loss: 15.280037487453885\n",
      "Average test loss: 1031.6083919795744\n",
      "Epoch 113/300\n",
      "Average training loss: 14.54579549407959\n",
      "Average test loss: 5789.534007834209\n",
      "Epoch 114/300\n",
      "Average training loss: 13.787204392327203\n",
      "Average test loss: 5289044908.81139\n",
      "Epoch 115/300\n",
      "Average training loss: 13.002059122721354\n",
      "Average test loss: 1874416.4115170853\n",
      "Epoch 116/300\n",
      "Average training loss: 12.189274710761175\n",
      "Average test loss: 28478.600777418473\n",
      "Epoch 117/300\n",
      "Average training loss: 11.380093565199111\n",
      "Average test loss: 9318.27366816531\n",
      "Epoch 118/300\n",
      "Average training loss: 10.583034047444661\n",
      "Average test loss: 561777946.4688789\n",
      "Epoch 119/300\n",
      "Average training loss: 9.7626341603597\n",
      "Average test loss: 97537.86983948066\n",
      "Epoch 120/300\n",
      "Average training loss: 8.906391695658366\n",
      "Average test loss: 31486.676659807377\n",
      "Epoch 121/300\n",
      "Average training loss: 8.038426449245877\n",
      "Average test loss: 318.5791066335986\n",
      "Epoch 122/300\n",
      "Average training loss: 7.125458540598552\n",
      "Average test loss: 765664328388.1528\n",
      "Epoch 123/300\n",
      "Average training loss: 6.1573898357815215\n",
      "Average test loss: 0.02590945753041241\n",
      "Epoch 124/300\n",
      "Average training loss: 5.242266092936198\n",
      "Average test loss: 4853216.9723488595\n",
      "Epoch 125/300\n",
      "Average training loss: 4.460145607418484\n",
      "Average test loss: 88010240.8029852\n",
      "Epoch 126/300\n",
      "Average training loss: 3.744095660527547\n",
      "Average test loss: 831687574.3391111\n",
      "Epoch 127/300\n",
      "Average training loss: 3.091829786936442\n",
      "Average test loss: 221.81410297858218\n",
      "Epoch 128/300\n",
      "Average training loss: 2.4996121321784126\n",
      "Average test loss: 75322941.22888903\n",
      "Epoch 129/300\n",
      "Average training loss: 1.9561355815463597\n",
      "Average test loss: 915.0711100897689\n",
      "Epoch 130/300\n",
      "Average training loss: 1.5360626011954412\n",
      "Average test loss: 361755102231164.75\n",
      "Epoch 131/300\n",
      "Average training loss: 1.2191502864625718\n",
      "Average test loss: 67612543.58519891\n",
      "Epoch 132/300\n",
      "Average training loss: 0.9874030556148953\n",
      "Average test loss: 0.005502300185461839\n",
      "Epoch 133/300\n",
      "Average training loss: 0.8129312341478135\n",
      "Average test loss: 25.11765609958292\n",
      "Epoch 134/300\n",
      "Average training loss: 0.6790977592468261\n",
      "Average test loss: 209.67010628501905\n",
      "Epoch 135/300\n",
      "Average training loss: 0.5786327201525371\n",
      "Average test loss: 87831.94130103977\n",
      "Epoch 136/300\n",
      "Average training loss: 0.49508973715040416\n",
      "Average test loss: 23116.293310145487\n",
      "Epoch 137/300\n",
      "Average training loss: 0.4363981889353858\n",
      "Average test loss: 3316784.96545087\n",
      "Epoch 138/300\n",
      "Average training loss: 0.39385713887214663\n",
      "Average test loss: 206.30530183728496\n",
      "Epoch 139/300\n",
      "Average training loss: 0.3660703330569797\n",
      "Average test loss: 1436.0275708182012\n",
      "Epoch 140/300\n",
      "Average training loss: 0.3387156133386824\n",
      "Average test loss: 2729566.0741267363\n",
      "Epoch 141/300\n",
      "Average training loss: 0.3190556561681959\n",
      "Average test loss: 84778257102.77689\n",
      "Epoch 142/300\n",
      "Average training loss: 0.305652409447564\n",
      "Average test loss: 84959.72667171423\n",
      "Epoch 143/300\n",
      "Average training loss: 0.28166348793771534\n",
      "Average test loss: 8.802584464508627\n",
      "Epoch 145/300\n",
      "Average training loss: 0.2691099856959449\n",
      "Average test loss: 244224.59296644625\n",
      "Epoch 146/300\n",
      "Average training loss: 0.25867915399869285\n",
      "Average test loss: 482633354.0352335\n",
      "Epoch 147/300\n",
      "Average training loss: 0.24878596226374308\n",
      "Average test loss: 3336388.5401695888\n",
      "Epoch 148/300\n",
      "Average training loss: 0.2421818901962704\n",
      "Average test loss: 315.45171187051426\n",
      "Epoch 149/300\n",
      "Average training loss: 0.24214061583413019\n",
      "Average test loss: 1.0273987685541313\n",
      "Epoch 150/300\n",
      "Average training loss: 0.22951135085688698\n",
      "Average test loss: 189123564.04614982\n",
      "Epoch 151/300\n",
      "Average training loss: 0.22475449743535783\n",
      "Average test loss: 8175587.860838174\n",
      "Epoch 152/300\n",
      "Average training loss: 0.21993607660134634\n",
      "Average test loss: 163.19079150809182\n",
      "Epoch 153/300\n",
      "Average training loss: 0.21354444868034786\n",
      "Average test loss: 41391.555529132565\n",
      "Epoch 154/300\n",
      "Average training loss: 0.20559758016798232\n",
      "Average test loss: 227.60865875279737\n",
      "Epoch 155/300\n",
      "Average training loss: 0.20090348347028097\n",
      "Average test loss: 0.009801579712579648\n",
      "Epoch 156/300\n",
      "Average training loss: 0.1993500365283754\n",
      "Average test loss: 0.1880797054099126\n",
      "Epoch 157/300\n",
      "Average training loss: 0.19388348831070795\n",
      "Average test loss: 719863.1258241389\n",
      "Epoch 158/300\n",
      "Average training loss: 0.1908814860317442\n",
      "Average test loss: 47472.088943432595\n",
      "Epoch 159/300\n",
      "Average training loss: 0.18992658829689027\n",
      "Average test loss: 2754.8087180967264\n",
      "Epoch 160/300\n",
      "Average training loss: 0.18750072175926633\n",
      "Average test loss: 0.028548373863101006\n",
      "Epoch 161/300\n",
      "Average training loss: 0.18144725300206077\n",
      "Average test loss: 102.75735971754293\n",
      "Epoch 162/300\n",
      "Average training loss: 0.17945693753825293\n",
      "Average test loss: 0.005395897280010912\n",
      "Epoch 163/300\n",
      "Average training loss: 0.17618996107578277\n",
      "Average test loss: 0.005628443292031686\n",
      "Epoch 164/300\n",
      "Average training loss: 0.17881597420904372\n",
      "Average test loss: 0.005431681675215562\n",
      "Epoch 165/300\n",
      "Average training loss: 1032002.0366790147\n",
      "Average test loss: 42928509806.13102\n",
      "Epoch 166/300\n",
      "Average training loss: 32.06945889451769\n",
      "Average test loss: 150.74018345123199\n",
      "Epoch 167/300\n",
      "Average training loss: 24.163494066026477\n",
      "Average test loss: 59149535.97575352\n",
      "Epoch 168/300\n",
      "Average training loss: 21.265922453138565\n",
      "Average test loss: 1204833.3229785352\n",
      "Epoch 169/300\n",
      "Average training loss: 19.613202638414172\n",
      "Average test loss: 2650347.613847222\n",
      "Epoch 170/300\n",
      "Average training loss: 18.465871529473198\n",
      "Average test loss: 161460.40705931158\n",
      "Epoch 171/300\n",
      "Average training loss: 17.60741351826986\n",
      "Average test loss: 111360948403.36604\n",
      "Epoch 172/300\n",
      "Average training loss: 16.856684678819445\n",
      "Average test loss: 8644.124113061818\n",
      "Epoch 173/300\n",
      "Average training loss: 16.189514639960397\n",
      "Average test loss: 0.2984580209958884\n",
      "Epoch 174/300\n",
      "Average training loss: 15.540714644538031\n",
      "Average test loss: 924990824.0672355\n",
      "Epoch 175/300\n",
      "Average training loss: 14.879678024291993\n",
      "Average test loss: 3223354414.9113293\n",
      "Epoch 176/300\n",
      "Average training loss: 14.234396217346191\n",
      "Average test loss: 208093.2422873948\n",
      "Epoch 177/300\n",
      "Average training loss: 13.599882411532931\n",
      "Average test loss: 64448.277312003425\n",
      "Epoch 178/300\n",
      "Average training loss: 13.00597014872233\n",
      "Average test loss: 14392959795.648304\n",
      "Epoch 179/300\n",
      "Average training loss: 12.494301710340713\n",
      "Average test loss: 10506742.683995126\n",
      "Epoch 180/300\n",
      "Average training loss: 12.075176223754882\n",
      "Average test loss: 0.007108612611475918\n",
      "Epoch 181/300\n",
      "Average training loss: 11.708957950168186\n",
      "Average test loss: 41495.98611846974\n",
      "Epoch 182/300\n",
      "Average training loss: 11.400349200778537\n",
      "Average test loss: 16515.990074094007\n",
      "Epoch 183/300\n",
      "Average training loss: 11.095720777723525\n",
      "Average test loss: 0.3630842144936323\n",
      "Epoch 184/300\n",
      "Average training loss: 10.821408170912001\n",
      "Average test loss: 0.006846385175155269\n",
      "Epoch 185/300\n",
      "Average training loss: 10.535486836751302\n",
      "Average test loss: 0.0062954449363880685\n",
      "Epoch 186/300\n",
      "Average training loss: 10.25851426018609\n",
      "Average test loss: 391.02145747138724\n",
      "Epoch 187/300\n",
      "Average training loss: 9.947253445943197\n",
      "Average test loss: 3974139.407281982\n",
      "Epoch 188/300\n",
      "Average training loss: 9.63149697028266\n",
      "Average test loss: 1494852669.1938949\n",
      "Epoch 189/300\n",
      "Average training loss: 9.272651189168295\n",
      "Average test loss: 4550.752178966999\n",
      "Epoch 190/300\n",
      "Average training loss: 8.899082778930664\n",
      "Average test loss: 14668062.662602078\n",
      "Epoch 191/300\n",
      "Average training loss: 8.487331999884711\n",
      "Average test loss: 0.005745754055678845\n",
      "Epoch 192/300\n",
      "Average training loss: 8.024446535746257\n",
      "Average test loss: 0.4853998466932939\n",
      "Epoch 193/300\n",
      "Average training loss: 7.519098019917806\n",
      "Average test loss: 82481.57158059393\n",
      "Epoch 194/300\n",
      "Average training loss: 6.969817529466417\n",
      "Average test loss: 0.01678095687408414\n",
      "Epoch 195/300\n",
      "Average training loss: 6.40170506922404\n",
      "Average test loss: 0.2053031602959252\n",
      "Epoch 196/300\n",
      "Average training loss: 5.756129957411025\n",
      "Average test loss: 24.04058489023811\n",
      "Epoch 197/300\n",
      "Average training loss: 5.04282026969062\n",
      "Average test loss: 0.005377898234460089\n",
      "Epoch 198/300\n",
      "Average training loss: 4.423329011281331\n",
      "Average test loss: 12602037.443162048\n",
      "Epoch 199/300\n",
      "Average training loss: 3.8220697650909425\n",
      "Average test loss: 0.010509042844797175\n",
      "Epoch 200/300\n",
      "Average training loss: 3.2322602507273355\n",
      "Average test loss: 3.3688815844464632\n",
      "Epoch 201/300\n",
      "Average training loss: 2.7423577914767794\n",
      "Average test loss: 30052.27263478952\n",
      "Epoch 202/300\n",
      "Average training loss: 2.321226628621419\n",
      "Average test loss: 1889035.6350321008\n",
      "Epoch 203/300\n",
      "Average training loss: 1.9312073433134291\n",
      "Average test loss: 0.0052672630813386705\n",
      "Epoch 204/300\n",
      "Average training loss: 1.583901087443034\n",
      "Average test loss: 0.05080769356174601\n",
      "Epoch 205/300\n",
      "Average training loss: 1.3061024588478936\n",
      "Average test loss: 408.7000990944989\n",
      "Epoch 206/300\n",
      "Average training loss: 1.0814127111434937\n",
      "Average test loss: 21837.577212357486\n",
      "Epoch 207/300\n",
      "Average training loss: 0.896235429710812\n",
      "Average test loss: 8.890860563878384\n",
      "Epoch 208/300\n",
      "Average training loss: 0.7480968675083585\n",
      "Average test loss: 56240983602724.55\n",
      "Epoch 209/300\n",
      "Average training loss: 0.6338619848887126\n",
      "Average test loss: 63202.878295267634\n",
      "Epoch 210/300\n",
      "Average training loss: 0.5479030532307095\n",
      "Average test loss: 143042.9978478812\n",
      "Epoch 211/300\n",
      "Average training loss: 0.4879982753859626\n",
      "Average test loss: 42471.931980571135\n",
      "Epoch 212/300\n",
      "Average training loss: 0.4414811615943909\n",
      "Average test loss: 1457187904.8584135\n",
      "Epoch 213/300\n",
      "Average training loss: 0.40257658134566415\n",
      "Average test loss: 63.12116285608047\n",
      "Epoch 214/300\n",
      "Average training loss: 0.37375815285576713\n",
      "Average test loss: 0.007664431802514526\n",
      "Epoch 215/300\n",
      "Average training loss: 0.34769965365197925\n",
      "Average test loss: 110404.82530798379\n",
      "Epoch 216/300\n",
      "Average training loss: 0.32990860671467254\n",
      "Average test loss: 78.84814462807734\n",
      "Epoch 217/300\n",
      "Average training loss: 0.30941687650150723\n",
      "Average test loss: 33347.23670152217\n",
      "Epoch 218/300\n",
      "Average training loss: 0.2945951915052202\n",
      "Average test loss: 0.04042058780375454\n",
      "Epoch 219/300\n",
      "Average training loss: 0.2784185795519087\n",
      "Average test loss: 805.5034535264807\n",
      "Epoch 220/300\n",
      "Average training loss: 0.26583391597535877\n",
      "Average test loss: 202.12830371273557\n",
      "Epoch 221/300\n",
      "Average training loss: 0.2533673345512814\n",
      "Average test loss: 55506.30497307913\n",
      "Epoch 222/300\n",
      "Average training loss: 0.24329180290963914\n",
      "Average test loss: 0.005994653737793366\n",
      "Epoch 223/300\n",
      "Average training loss: 0.23519162694613138\n",
      "Average test loss: 0.005313005190342664\n",
      "Epoch 224/300\n",
      "Average training loss: 0.2254511411190033\n",
      "Average test loss: 0.0508195558918847\n",
      "Epoch 225/300\n",
      "Average training loss: 0.22078887650701734\n",
      "Average test loss: 0.005497921646055248\n",
      "Epoch 226/300\n",
      "Average training loss: 0.21352764550844827\n",
      "Average test loss: 0.00584387868270278\n",
      "Epoch 227/300\n",
      "Average training loss: 2225591.0537978704\n",
      "Average test loss: 8272325.1729511125\n",
      "Epoch 228/300\n",
      "Average training loss: 21.013488552517362\n",
      "Average test loss: 54226843.80709871\n",
      "Epoch 229/300\n",
      "Average training loss: 19.785681613498262\n",
      "Average test loss: 1600201.731474042\n",
      "Epoch 230/300\n",
      "Average training loss: 19.02501517232259\n",
      "Average test loss: 2049937.0969539096\n",
      "Epoch 231/300\n",
      "Average training loss: 18.41630450608995\n",
      "Average test loss: 1138489.1109125395\n",
      "Epoch 232/300\n",
      "Average training loss: 17.874669982910156\n",
      "Average test loss: 4643467.42996478\n",
      "Epoch 233/300\n",
      "Average training loss: 17.360101626926\n",
      "Average test loss: 91009530406.10542\n",
      "Epoch 234/300\n",
      "Average training loss: 16.85374633449978\n",
      "Average test loss: 6455043.672646425\n",
      "Epoch 235/300\n",
      "Average training loss: 16.351807167900933\n",
      "Average test loss: 203089130255.11996\n",
      "Epoch 236/300\n",
      "Average training loss: 15.855545745849609\n",
      "Average test loss: 10527.750443796062\n",
      "Epoch 237/300\n",
      "Average training loss: 15.34244652472602\n",
      "Average test loss: 2358910563.2151427\n",
      "Epoch 238/300\n",
      "Average training loss: 14.825059206644694\n",
      "Average test loss: 41835871393.894066\n",
      "Epoch 239/300\n",
      "Average training loss: 14.327824246724447\n",
      "Average test loss: 183.38069683400624\n",
      "Epoch 240/300\n",
      "Average training loss: 13.827810521443684\n",
      "Average test loss: 7829515.573807592\n",
      "Epoch 241/300\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_50_Depth15/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
