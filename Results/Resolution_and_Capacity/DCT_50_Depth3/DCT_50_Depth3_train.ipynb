{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.059878229952520796\n",
      "Average test loss: 0.00505892163183954\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0237033550987641\n",
      "Average test loss: 0.004625541869550944\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02259769659737746\n",
      "Average test loss: 0.004451418885754214\n",
      "Epoch 4/300\n",
      "Average training loss: 0.022111078517304525\n",
      "Average test loss: 0.004463412481877539\n",
      "Epoch 5/300\n",
      "Average training loss: 0.021834363435705502\n",
      "Average test loss: 0.00447680143515269\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02163060095409552\n",
      "Average test loss: 0.004327445674273703\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02147464610801803\n",
      "Average test loss: 0.004299701874868738\n",
      "Epoch 8/300\n",
      "Average training loss: 0.021356885979572932\n",
      "Average test loss: 0.0042976498641073705\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021250971224572923\n",
      "Average test loss: 0.004255052684495847\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021160373427801662\n",
      "Average test loss: 0.0042784006394859814\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021091292712423538\n",
      "Average test loss: 0.004248946674582031\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02100812730524275\n",
      "Average test loss: 0.004214003589418199\n",
      "Epoch 13/300\n",
      "Average training loss: 0.020954358296261893\n",
      "Average test loss: 0.004196898957507478\n",
      "Epoch 14/300\n",
      "Average training loss: 0.020895916003319952\n",
      "Average test loss: 0.004209040484080712\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020837018721633488\n",
      "Average test loss: 0.004181244718945688\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02079765680929025\n",
      "Average test loss: 0.00416388805065718\n",
      "Epoch 17/300\n",
      "Average training loss: 0.020735127271049553\n",
      "Average test loss: 0.004155442813204394\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020698733801643054\n",
      "Average test loss: 0.004136644715236293\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02063487009704113\n",
      "Average test loss: 0.004161882198933098\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020607707563373778\n",
      "Average test loss: 0.004139235384762287\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020562212414211696\n",
      "Average test loss: 0.004136565356618828\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02053070758614275\n",
      "Average test loss: 0.004112085984605882\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02047679284049405\n",
      "Average test loss: 0.004124330558503668\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020454407188627455\n",
      "Average test loss: 0.004123151045499577\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02042670273118549\n",
      "Average test loss: 0.0040944347514046565\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020395531866285534\n",
      "Average test loss: 0.004090613496800264\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020368512216541502\n",
      "Average test loss: 0.0040818199146952895\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02033282462755839\n",
      "Average test loss: 0.004076446118040217\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020303196385502816\n",
      "Average test loss: 0.004072301143987311\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020286971007784208\n",
      "Average test loss: 0.0041591109875589605\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020267183676362037\n",
      "Average test loss: 0.0040743325729337004\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020233797839946218\n",
      "Average test loss: 0.0040560917543868225\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020222562077972624\n",
      "Average test loss: 0.00405374275561836\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020190242181221642\n",
      "Average test loss: 0.0040529839255743555\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02017274916006459\n",
      "Average test loss: 0.004041170702626308\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020154838312003348\n",
      "Average test loss: 0.004044783200240798\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020152636021375656\n",
      "Average test loss: 0.004062075176586707\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020126436813010108\n",
      "Average test loss: 0.004039883468714025\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020104118062390222\n",
      "Average test loss: 0.004071894826574458\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02008276721338431\n",
      "Average test loss: 0.004027668054733011\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02007029042641322\n",
      "Average test loss: 0.004050916688309775\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020060277629229756\n",
      "Average test loss: 0.004024098503092924\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020046809906760853\n",
      "Average test loss: 0.004018383981453048\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02003338114089436\n",
      "Average test loss: 0.004020120105602675\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020023499266968833\n",
      "Average test loss: 0.004023628284326858\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020002785058485135\n",
      "Average test loss: 0.004016929414123297\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019995448276400565\n",
      "Average test loss: 0.0040225288658920265\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019976914279162882\n",
      "Average test loss: 0.0040190975976487\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01995993823144171\n",
      "Average test loss: 0.004007355422609382\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019947489316264787\n",
      "Average test loss: 0.0040087160745428665\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01994563150074747\n",
      "Average test loss: 0.004009401030010647\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019941358049710593\n",
      "Average test loss: 0.004003913631662726\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019922329919205773\n",
      "Average test loss: 0.004002203276587857\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019904936987492773\n",
      "Average test loss: 0.003999175981101062\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019900080574883357\n",
      "Average test loss: 0.004004795775645309\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019887509723504386\n",
      "Average test loss: 0.004005024123936891\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019875230906738175\n",
      "Average test loss: 0.004006581016091837\n",
      "Epoch 58/300\n",
      "Average training loss: 0.019873703277773328\n",
      "Average test loss: 0.00399668125808239\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019861861737238036\n",
      "Average test loss: 0.003997935673014985\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01984506279726823\n",
      "Average test loss: 0.004004447296675709\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019840091966092585\n",
      "Average test loss: 0.003998867677110765\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01982801882425944\n",
      "Average test loss: 0.003998839922249317\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019815907998217478\n",
      "Average test loss: 0.003991737191461855\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01981667469276322\n",
      "Average test loss: 0.004014435198985868\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019802047669887544\n",
      "Average test loss: 0.004005623312460052\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019794938468270833\n",
      "Average test loss: 0.004002472294080589\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01978322179118792\n",
      "Average test loss: 0.0039870718949370915\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019777109142806795\n",
      "Average test loss: 0.003989348265445895\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019774313115411335\n",
      "Average test loss: 0.0040040464113569926\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019759948624504937\n",
      "Average test loss: 0.003984502944681379\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01975150698920091\n",
      "Average test loss: 0.004026420709159639\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019738200219141112\n",
      "Average test loss: 0.003987048883818918\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01973724550008774\n",
      "Average test loss: 0.003978569635914432\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019725760420163472\n",
      "Average test loss: 0.00397910762950778\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019718180628286466\n",
      "Average test loss: 0.00398251219611201\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019716099493205547\n",
      "Average test loss: 0.003990542852009336\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019704432528879906\n",
      "Average test loss: 0.003979835401392645\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01969514448609617\n",
      "Average test loss: 0.0039834510050714015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019685716576046414\n",
      "Average test loss: 0.003982933240425255\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019680756482813092\n",
      "Average test loss: 0.0039746191305004884\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01967159553865592\n",
      "Average test loss: 0.0039843390726794796\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01967309904512432\n",
      "Average test loss: 0.003974846604383654\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019661335746447246\n",
      "Average test loss: 0.003976947927847504\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019655887044138377\n",
      "Average test loss: 0.00399032756810387\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01964554351568222\n",
      "Average test loss: 0.003976425803576907\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019638443269663387\n",
      "Average test loss: 0.003971246128280957\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01963092620505227\n",
      "Average test loss: 0.003980550021967954\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019626523259613247\n",
      "Average test loss: 0.003976740481124984\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019626919425196118\n",
      "Average test loss: 0.003981832262128592\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01961010912474659\n",
      "Average test loss: 0.003975779566499922\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01960110396809048\n",
      "Average test loss: 0.003983691227725811\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01959963179876407\n",
      "Average test loss: 0.003989215088594291\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019591068494651052\n",
      "Average test loss: 0.0039670896095534165\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019586145758628846\n",
      "Average test loss: 0.0040019405693229705\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019585826507873005\n",
      "Average test loss: 0.0039756430768304405\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019570434391498567\n",
      "Average test loss: 0.003976496461364958\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019568316943115657\n",
      "Average test loss: 0.0039782724361866716\n",
      "Epoch 98/300\n",
      "Average training loss: 0.019560122931996982\n",
      "Average test loss: 0.003991446419101622\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019554150485330157\n",
      "Average test loss: 0.0039859750368114975\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01954761692881584\n",
      "Average test loss: 0.003975020772674018\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019539353587561183\n",
      "Average test loss: 0.003975199948582384\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019525453348126675\n",
      "Average test loss: 0.003974154116171929\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019523332317670186\n",
      "Average test loss: 0.0039987417198717595\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019519007941087088\n",
      "Average test loss: 0.003970004505788286\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019513563417726094\n",
      "Average test loss: 0.0039701301294068495\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019506928010947173\n",
      "Average test loss: 0.003972555475930373\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01949580652018388\n",
      "Average test loss: 0.003989843128042089\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01949212164680163\n",
      "Average test loss: 0.00399539227804376\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019490369512803026\n",
      "Average test loss: 0.003974249196756217\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01948131954835521\n",
      "Average test loss: 0.003966934940467278\n",
      "Epoch 111/300\n",
      "Average training loss: 0.019477325570252207\n",
      "Average test loss: 0.003975407684842745\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01946867332028018\n",
      "Average test loss: 0.003973205802548263\n",
      "Epoch 113/300\n",
      "Average training loss: 0.019464161078135173\n",
      "Average test loss: 0.004000305394331614\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019457369772924317\n",
      "Average test loss: 0.0040078872293233874\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01945078682568338\n",
      "Average test loss: 0.003983459542195002\n",
      "Epoch 116/300\n",
      "Average training loss: 0.019451413756443395\n",
      "Average test loss: 0.0039818520480766895\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019442494478490616\n",
      "Average test loss: 0.003979193475097418\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01943995591170258\n",
      "Average test loss: 0.003997608551548587\n",
      "Epoch 119/300\n",
      "Average training loss: 0.019426727114452255\n",
      "Average test loss: 0.003977071667297019\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01942461651398076\n",
      "Average test loss: 0.0039734565950930115\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019418603968289164\n",
      "Average test loss: 0.003977852845357524\n",
      "Epoch 122/300\n",
      "Average training loss: 0.019410653677251605\n",
      "Average test loss: 0.003983251288429731\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019402339845895768\n",
      "Average test loss: 0.003984062047882212\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019403479071127043\n",
      "Average test loss: 0.0040132578027745084\n",
      "Epoch 125/300\n",
      "Average training loss: 0.019394323380457032\n",
      "Average test loss: 0.004003872349858284\n",
      "Epoch 126/300\n",
      "Average training loss: 0.019391433101561335\n",
      "Average test loss: 0.003990179723335637\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019380684134032992\n",
      "Average test loss: 0.003975413923876153\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019382683970861965\n",
      "Average test loss: 0.003982095442091425\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019365614723828105\n",
      "Average test loss: 0.004005208201706409\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0193669269449181\n",
      "Average test loss: 0.003975060985734065\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01935720641248756\n",
      "Average test loss: 0.003967946466265453\n",
      "Epoch 132/300\n",
      "Average training loss: 0.019357593496640523\n",
      "Average test loss: 0.003970025203294225\n",
      "Epoch 133/300\n",
      "Average training loss: 0.019348977027667892\n",
      "Average test loss: 0.0039736509364512234\n",
      "Epoch 134/300\n",
      "Average training loss: 0.019342456395427386\n",
      "Average test loss: 0.0039757779687643055\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019336128648784427\n",
      "Average test loss: 0.004004788629710675\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019339258428249094\n",
      "Average test loss: 0.003978854300454259\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01932228200468752\n",
      "Average test loss: 0.003973151486987869\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01932218480772442\n",
      "Average test loss: 0.003989180388343003\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019307292371988298\n",
      "Average test loss: 0.004013016139053636\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01931144927855995\n",
      "Average test loss: 0.004004120002190272\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01930241349339485\n",
      "Average test loss: 0.003986325705630912\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01930140603416496\n",
      "Average test loss: 0.003996479781137572\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01929372684492005\n",
      "Average test loss: 0.003998422140876452\n",
      "Epoch 144/300\n",
      "Average training loss: 0.019292779823144278\n",
      "Average test loss: 0.00397803239358796\n",
      "Epoch 145/300\n",
      "Average training loss: 0.019273051308261024\n",
      "Average test loss: 0.003999337939338552\n",
      "Epoch 146/300\n",
      "Average training loss: 0.019287770488195948\n",
      "Average test loss: 0.003974058760537041\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01927146732641591\n",
      "Average test loss: 0.003982982957528697\n",
      "Epoch 148/300\n",
      "Average training loss: 0.019271547166009744\n",
      "Average test loss: 0.003971683006733656\n",
      "Epoch 149/300\n",
      "Average training loss: 0.019254702438910802\n",
      "Average test loss: 0.003991979808650083\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019252909363971817\n",
      "Average test loss: 0.004001247894846731\n",
      "Epoch 151/300\n",
      "Average training loss: 0.019248919241958193\n",
      "Average test loss: 0.003986284408925308\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01924484634647767\n",
      "Average test loss: 0.003977228039668666\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0192345206985871\n",
      "Average test loss: 0.003988227544973294\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019236366912722586\n",
      "Average test loss: 0.00401027802667684\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01922477958103021\n",
      "Average test loss: 0.003987206032913592\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019224048333035575\n",
      "Average test loss: 0.003978909057254593\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01921923290193081\n",
      "Average test loss: 0.0040541299014455745\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019215557608339522\n",
      "Average test loss: 0.0039957828335464\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01920728602177567\n",
      "Average test loss: 0.004002153455797169\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019193822047776646\n",
      "Average test loss: 0.003983625926905208\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01919582849327061\n",
      "Average test loss: 0.004046515121642086\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019191228272186386\n",
      "Average test loss: 0.003983109639750587\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019191830240190028\n",
      "Average test loss: 0.004009541078574127\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01918036658399635\n",
      "Average test loss: 0.003981164913624525\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019178519894679388\n",
      "Average test loss: 0.004038986428744263\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019168315597706372\n",
      "Average test loss: 0.0039889112477087315\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019167885487278304\n",
      "Average test loss: 0.004000249592380391\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019151122863094013\n",
      "Average test loss: 0.003984764815618594\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019150969601339763\n",
      "Average test loss: 0.003989709158738454\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01914612072871791\n",
      "Average test loss: 0.004002771180537012\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019134882183538544\n",
      "Average test loss: 0.0040501154704640305\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019132163166999817\n",
      "Average test loss: 0.004005887162354257\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019133989199995996\n",
      "Average test loss: 0.0039906006939709185\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019126015330354372\n",
      "Average test loss: 0.003992802001328932\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019117871070901554\n",
      "Average test loss: 0.0039814246015416254\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019116986696918806\n",
      "Average test loss: 0.004002930199934376\n",
      "Epoch 177/300\n",
      "Average training loss: 0.019111193482246665\n",
      "Average test loss: 0.004000265635756983\n",
      "Epoch 178/300\n",
      "Average training loss: 0.019106465648445817\n",
      "Average test loss: 0.004008949339803722\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01910021103421847\n",
      "Average test loss: 0.003993119379298554\n",
      "Epoch 180/300\n",
      "Average training loss: 0.019094641795588865\n",
      "Average test loss: 0.00400928259951373\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019087101899087428\n",
      "Average test loss: 0.0039964688018792206\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01908598809937636\n",
      "Average test loss: 0.0040093427821993825\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0190778593139516\n",
      "Average test loss: 0.004030451573224531\n",
      "Epoch 184/300\n",
      "Average training loss: 0.019073207257522477\n",
      "Average test loss: 0.0040703097850912145\n",
      "Epoch 185/300\n",
      "Average training loss: 0.019060185028447044\n",
      "Average test loss: 0.0040215772663553554\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019065276955564817\n",
      "Average test loss: 0.004013314951004253\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019061572475565804\n",
      "Average test loss: 0.0039838044900033205\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01905641353958183\n",
      "Average test loss: 0.004028274601118432\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01904896858914031\n",
      "Average test loss: 0.004005787427226702\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019048609491851595\n",
      "Average test loss: 0.0040096866285635364\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019032123289174505\n",
      "Average test loss: 0.0040209768507629634\n",
      "Epoch 192/300\n",
      "Average training loss: 0.019038158506155013\n",
      "Average test loss: 0.0040278452891442515\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019025137364864348\n",
      "Average test loss: 0.004033899524559578\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01902370300143957\n",
      "Average test loss: 0.004031947176489565\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019012886555658446\n",
      "Average test loss: 0.003999006777380904\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01901224466661612\n",
      "Average test loss: 0.00401912271997167\n",
      "Epoch 197/300\n",
      "Average training loss: 0.019014689101113213\n",
      "Average test loss: 0.004008019908227855\n",
      "Epoch 198/300\n",
      "Average training loss: 0.019001432011524835\n",
      "Average test loss: 0.004009299621606867\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018998339186112087\n",
      "Average test loss: 0.004009126454591751\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018990412500169543\n",
      "Average test loss: 0.004043434385417237\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0189893981003099\n",
      "Average test loss: 0.00402929561254051\n",
      "Epoch 202/300\n",
      "Average training loss: 0.018994459331035614\n",
      "Average test loss: 0.004017201529815793\n",
      "Epoch 203/300\n",
      "Average training loss: 0.018976824618048137\n",
      "Average test loss: 0.004025945768174198\n",
      "Epoch 204/300\n",
      "Average training loss: 0.018977546969221697\n",
      "Average test loss: 0.004029166799866491\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018968493435117934\n",
      "Average test loss: 0.004043714198801253\n",
      "Epoch 206/300\n",
      "Average training loss: 0.018963321463929282\n",
      "Average test loss: 0.004019000957823462\n",
      "Epoch 207/300\n",
      "Average training loss: 0.018963530542949837\n",
      "Average test loss: 0.00402779917501741\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018962268120712705\n",
      "Average test loss: 0.0040587643877499635\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018950974083609053\n",
      "Average test loss: 0.004078336997578542\n",
      "Epoch 210/300\n",
      "Average training loss: 0.018949107953243786\n",
      "Average test loss: 0.004029250522040659\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018944355646769207\n",
      "Average test loss: 0.004039088917689191\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018938317731022836\n",
      "Average test loss: 0.004005507183985578\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018932360245949693\n",
      "Average test loss: 0.00403467307653692\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018946229085326196\n",
      "Average test loss: 0.004039244400130378\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01892264018787278\n",
      "Average test loss: 0.004059644407696194\n",
      "Epoch 216/300\n",
      "Average training loss: 0.018925488758418294\n",
      "Average test loss: 0.004037355737967624\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01890908109396696\n",
      "Average test loss: 0.004121996868194805\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018909306171867583\n",
      "Average test loss: 0.004089525939275821\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018902033511963155\n",
      "Average test loss: 0.004040311527748902\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018901488312416607\n",
      "Average test loss: 0.004068959787074063\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01889801500240962\n",
      "Average test loss: 0.0040654404320650634\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018887991266118155\n",
      "Average test loss: 0.004049340640919076\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018887595924238363\n",
      "Average test loss: 0.004070735180750489\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018873543287316958\n",
      "Average test loss: 0.004050327587458822\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01888036158018642\n",
      "Average test loss: 0.004046421195069949\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01887512050403489\n",
      "Average test loss: 0.004050145802605484\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0188587837624881\n",
      "Average test loss: 0.00407858369147612\n",
      "Epoch 228/300\n",
      "Average training loss: 0.018864126917388705\n",
      "Average test loss: 0.004038362924423482\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018859848875138495\n",
      "Average test loss: 0.004058272699101103\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018854621205064984\n",
      "Average test loss: 0.004048473159472148\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018850482924944824\n",
      "Average test loss: 0.004033812775380082\n",
      "Epoch 232/300\n",
      "Average training loss: 0.018850809295972187\n",
      "Average test loss: 0.004060937290804254\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018839031633403565\n",
      "Average test loss: 0.004046541208608283\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018837504560748737\n",
      "Average test loss: 0.004045612843707204\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01883748315440284\n",
      "Average test loss: 0.004014602775375048\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018826342631545333\n",
      "Average test loss: 0.004115895798843768\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01882471539742417\n",
      "Average test loss: 0.004041744544067316\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01882442364593347\n",
      "Average test loss: 0.004029595460742712\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01881299036575688\n",
      "Average test loss: 0.004047427460551262\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01881293118496736\n",
      "Average test loss: 0.004046875396950377\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01881995689206653\n",
      "Average test loss: 0.004049863441950745\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018805064252681202\n",
      "Average test loss: 0.004059181577629513\n",
      "Epoch 243/300\n",
      "Average training loss: 0.018801873925659393\n",
      "Average test loss: 0.004012791047286656\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01879055064254337\n",
      "Average test loss: 0.004025812333242761\n",
      "Epoch 245/300\n",
      "Average training loss: 0.018797061941689914\n",
      "Average test loss: 0.004097930467170146\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01878724051349693\n",
      "Average test loss: 0.004055861625820398\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01878058228145043\n",
      "Average test loss: 0.004039013665376438\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01878602842655447\n",
      "Average test loss: 0.004049091034051445\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018775807544589043\n",
      "Average test loss: 0.004090613098194202\n",
      "Epoch 250/300\n",
      "Average training loss: 0.018779521187146504\n",
      "Average test loss: 0.004063128917167584\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018762174035112062\n",
      "Average test loss: 0.00405731113627553\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01876070322427485\n",
      "Average test loss: 0.004016116895195511\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018761719599366188\n",
      "Average test loss: 0.004068574456704987\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01875827748245663\n",
      "Average test loss: 0.0040249201473262575\n",
      "Epoch 255/300\n",
      "Average training loss: 0.018742502075102595\n",
      "Average test loss: 0.004053631619653768\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01875314567486445\n",
      "Average test loss: 0.00406981161609292\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018742417467137177\n",
      "Average test loss: 0.004070967197004291\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01873863181306256\n",
      "Average test loss: 0.004081541897936\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018744009759691028\n",
      "Average test loss: 0.004073097522887919\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01872881824274858\n",
      "Average test loss: 0.004075439176004794\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018728283130460315\n",
      "Average test loss: 0.004106777744160758\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018721419476800494\n",
      "Average test loss: 0.004068418864160776\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01871307910647657\n",
      "Average test loss: 0.004140171403686206\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018716149043705728\n",
      "Average test loss: 0.004088846747866935\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018706967929999034\n",
      "Average test loss: 0.004070353150367737\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018702628056208294\n",
      "Average test loss: 0.0041167662342389425\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018701653338140913\n",
      "Average test loss: 0.004065145663917065\n",
      "Epoch 268/300\n",
      "Average training loss: 0.018689828746848637\n",
      "Average test loss: 0.004153235003352165\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01868979256020652\n",
      "Average test loss: 0.004055058074494203\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018704966492123075\n",
      "Average test loss: 0.004062357952197393\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018681196285618675\n",
      "Average test loss: 0.004089689169492986\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01867772979206509\n",
      "Average test loss: 0.004070458810362551\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018684982265035312\n",
      "Average test loss: 0.004096568182938629\n",
      "Epoch 274/300\n",
      "Average training loss: 0.018675835819707977\n",
      "Average test loss: 0.004073855275495185\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018669403462774223\n",
      "Average test loss: 0.004121170685109165\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018667179700401094\n",
      "Average test loss: 0.004070553156650729\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01866127053234312\n",
      "Average test loss: 0.004121522778231236\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018664417405923207\n",
      "Average test loss: 0.004183793558014764\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018662934778465166\n",
      "Average test loss: 0.004057813047120968\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018654437195923593\n",
      "Average test loss: 0.004105784183161126\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01865131872644027\n",
      "Average test loss: 0.004209566731626789\n",
      "Epoch 282/300\n",
      "Average training loss: 0.018657188466025723\n",
      "Average test loss: 0.004057853046597706\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0186490608304739\n",
      "Average test loss: 0.004113924211098088\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018635573791960874\n",
      "Average test loss: 0.0041404180907540855\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01863191937572426\n",
      "Average test loss: 0.004090862937892477\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018632049433887004\n",
      "Average test loss: 0.004100586185024844\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018636071930329005\n",
      "Average test loss: 0.004104283127105898\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01861996895323197\n",
      "Average test loss: 0.00413763993854324\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018632225419912073\n",
      "Average test loss: 0.004081898017062081\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01861841992619965\n",
      "Average test loss: 0.0040922470432188775\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018619684590233696\n",
      "Average test loss: 0.00411602383479476\n",
      "Epoch 292/300\n",
      "Average training loss: 0.018612467046413155\n",
      "Average test loss: 0.0041326411407854825\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01860341003868315\n",
      "Average test loss: 0.004102709361869428\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018602207887503837\n",
      "Average test loss: 0.004084391710244947\n",
      "Epoch 295/300\n",
      "Average training loss: 0.018604812504516708\n",
      "Average test loss: 0.004101044342749649\n",
      "Epoch 296/300\n",
      "Average training loss: 0.018601936110191876\n",
      "Average test loss: 0.004066864027538234\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01859910091923343\n",
      "Average test loss: 0.004222027547864451\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018601071968674658\n",
      "Average test loss: 0.004133420442955361\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018583226252761154\n",
      "Average test loss: 0.004217944030960401\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018585291089283095\n",
      "Average test loss: 0.004115329177015357\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05883227543036143\n",
      "Average test loss: 0.004472814841402902\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02109329736729463\n",
      "Average test loss: 0.004080681128634347\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019799766081074872\n",
      "Average test loss: 0.003858582879934046\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019160782391826312\n",
      "Average test loss: 0.003780959346228176\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018727683095468416\n",
      "Average test loss: 0.00369627458954023\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018402633838355543\n",
      "Average test loss: 0.00363998069986701\n",
      "Epoch 7/300\n",
      "Average training loss: 0.018150696713063453\n",
      "Average test loss: 0.0036183728501200676\n",
      "Epoch 8/300\n",
      "Average training loss: 0.017944111189908452\n",
      "Average test loss: 0.0035516276216755313\n",
      "Epoch 9/300\n",
      "Average training loss: 0.017762799695962006\n",
      "Average test loss: 0.0035161843243986367\n",
      "Epoch 10/300\n",
      "Average training loss: 0.017623172078695563\n",
      "Average test loss: 0.003516807566707333\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017494643582238092\n",
      "Average test loss: 0.003487634795614415\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017378929445313083\n",
      "Average test loss: 0.0034718757234513758\n",
      "Epoch 13/300\n",
      "Average training loss: 0.017269163720309736\n",
      "Average test loss: 0.003406417935465773\n",
      "Epoch 14/300\n",
      "Average training loss: 0.017173279630641143\n",
      "Average test loss: 0.0033971893613537153\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01707771317412456\n",
      "Average test loss: 0.0033981807683077124\n",
      "Epoch 16/300\n",
      "Average training loss: 0.016985350330670675\n",
      "Average test loss: 0.003347903949725959\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01689270615246561\n",
      "Average test loss: 0.003316986807104614\n",
      "Epoch 18/300\n",
      "Average training loss: 0.016809051763680246\n",
      "Average test loss: 0.003316575050354004\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01674300204962492\n",
      "Average test loss: 0.0033037706007146175\n",
      "Epoch 20/300\n",
      "Average training loss: 0.016655468735429977\n",
      "Average test loss: 0.0032941487611581883\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016591914972497356\n",
      "Average test loss: 0.0032683815010305906\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01652384051266644\n",
      "Average test loss: 0.0032626302656200198\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016454345765213173\n",
      "Average test loss: 0.0032743835167752373\n",
      "Epoch 24/300\n",
      "Average training loss: 0.016390590409437817\n",
      "Average test loss: 0.0032463117473655275\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01632864493628343\n",
      "Average test loss: 0.0032106565634409585\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01626858270002736\n",
      "Average test loss: 0.003199693881596128\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016222835020886527\n",
      "Average test loss: 0.003163705853952302\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016179084546036192\n",
      "Average test loss: 0.0031711128080884614\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0161192902152737\n",
      "Average test loss: 0.003151027651089761\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01608281686156988\n",
      "Average test loss: 0.0031672913597689733\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016035119994646973\n",
      "Average test loss: 0.0031398646945340765\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015998396729429563\n",
      "Average test loss: 0.0031633114077978665\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01597009470810493\n",
      "Average test loss: 0.003115161451614565\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01592434974014759\n",
      "Average test loss: 0.003115044741373923\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015897339548501702\n",
      "Average test loss: 0.0031025487054139374\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01586125265641345\n",
      "Average test loss: 0.0030981328102449575\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015833532660371727\n",
      "Average test loss: 0.003095691355359223\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01579804487609201\n",
      "Average test loss: 0.003080957188995348\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015774313305815062\n",
      "Average test loss: 0.003078484297212627\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01575025327834818\n",
      "Average test loss: 0.003073923700592584\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015736288310752973\n",
      "Average test loss: 0.003064104699012306\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01570272612406148\n",
      "Average test loss: 0.0030603533966673743\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015676628716289998\n",
      "Average test loss: 0.003066810448343555\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015650701427625285\n",
      "Average test loss: 0.003059501505560345\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01562963062359227\n",
      "Average test loss: 0.0030683084964338277\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015619325884514385\n",
      "Average test loss: 0.003043242771178484\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015594023750060134\n",
      "Average test loss: 0.0030469189271744757\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015567834905452198\n",
      "Average test loss: 0.0030365648193077907\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01554873293555445\n",
      "Average test loss: 0.0030549841043021942\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015538454319867823\n",
      "Average test loss: 0.0030526647489104008\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015516549450655778\n",
      "Average test loss: 0.0030767859597173003\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015497368694179588\n",
      "Average test loss: 0.003042051744957765\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015478074986073707\n",
      "Average test loss: 0.0030253663098232615\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015466687679290771\n",
      "Average test loss: 0.0030497482100294696\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015450186140835286\n",
      "Average test loss: 0.003036658360519343\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015430340063240794\n",
      "Average test loss: 0.003017179749906063\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015416360780596734\n",
      "Average test loss: 0.003041746538132429\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015394499306049612\n",
      "Average test loss: 0.0030815393270717728\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015383879454599486\n",
      "Average test loss: 0.003020670155684153\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015377492378983233\n",
      "Average test loss: 0.0030251760938101347\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015353555310931471\n",
      "Average test loss: 0.003029229246907764\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015343488588929177\n",
      "Average test loss: 0.003011290747051438\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015333146606882413\n",
      "Average test loss: 0.003025717845393552\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015313239985042148\n",
      "Average test loss: 0.0030056376410648226\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015300192261735597\n",
      "Average test loss: 0.002996357310977247\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01528816516449054\n",
      "Average test loss: 0.11865554391013251\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01532465467767583\n",
      "Average test loss: 0.003052673969624771\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015252937705152564\n",
      "Average test loss: 0.0030309373835722604\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015244281584189999\n",
      "Average test loss: 0.003007407076656818\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015234351995090644\n",
      "Average test loss: 0.0030029286874665153\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015212666536370913\n",
      "Average test loss: 0.002995220940353142\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015220079311066203\n",
      "Average test loss: 0.0030058153110245865\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015195138716035419\n",
      "Average test loss: 0.002990919329639938\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015189816630548901\n",
      "Average test loss: 0.002992747453972697\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015174490287899972\n",
      "Average test loss: 0.003004731931206253\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01516124553191993\n",
      "Average test loss: 0.0030306296338223748\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015149742691881126\n",
      "Average test loss: 0.002998377393310269\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015138913466698593\n",
      "Average test loss: 0.003014491444039676\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015131094587345918\n",
      "Average test loss: 0.0029924491031302347\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015119274891912938\n",
      "Average test loss: 0.0029917250099695393\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015106742715670004\n",
      "Average test loss: 0.0029946027716828715\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015103325073917706\n",
      "Average test loss: 0.0030379888945155672\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015087763078510761\n",
      "Average test loss: 0.0030066044494095777\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015070023774272866\n",
      "Average test loss: 0.0029824266479247146\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015069842262400521\n",
      "Average test loss: 0.0029858351089060305\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015055407914022604\n",
      "Average test loss: 0.002983040659688413\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015048479379051261\n",
      "Average test loss: 0.002990592826157808\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015036779169407155\n",
      "Average test loss: 0.003140557842950026\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015029891137447623\n",
      "Average test loss: 0.00299575787389444\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015019037092311515\n",
      "Average test loss: 0.0029973902168373267\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015009851590626769\n",
      "Average test loss: 0.0029901331791447267\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014989829478992356\n",
      "Average test loss: 0.0030098756675918897\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014982511772049798\n",
      "Average test loss: 0.0030093281637463306\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014976212589277161\n",
      "Average test loss: 0.002989070998504758\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014968162570562627\n",
      "Average test loss: 0.00300081219834586\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014964575567179256\n",
      "Average test loss: 0.0030107048372220663\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014950682127641307\n",
      "Average test loss: 0.0030118519759012594\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014939000893798139\n",
      "Average test loss: 0.002997490898395578\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014928384650084708\n",
      "Average test loss: 0.003058760386374262\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01493115166740285\n",
      "Average test loss: 0.002999626776203513\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014918935682210657\n",
      "Average test loss: 0.0030309997788733906\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014901699523131053\n",
      "Average test loss: 0.003006606411602762\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014905083805322647\n",
      "Average test loss: 0.002999421358729402\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014894267127745681\n",
      "Average test loss: 0.003105728772158424\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014889237513972654\n",
      "Average test loss: 0.0030084480219003226\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014878960952162743\n",
      "Average test loss: 0.003017033574067884\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01486968674345149\n",
      "Average test loss: 0.003007639129749603\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014851037132243316\n",
      "Average test loss: 0.0030110282893810007\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014851718060672283\n",
      "Average test loss: 0.003013254554217888\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014839190256264475\n",
      "Average test loss: 0.003060628883126709\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014832521788775921\n",
      "Average test loss: 0.0030190849126213127\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01482279938293828\n",
      "Average test loss: 0.0030057772509753705\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014815493118431833\n",
      "Average test loss: 0.0029893451057788397\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014810155502623983\n",
      "Average test loss: 0.0030248175755971008\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014815514849291908\n",
      "Average test loss: 0.003018316370331579\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014791612774961525\n",
      "Average test loss: 0.003009013743036323\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014783453817996714\n",
      "Average test loss: 0.0029913241049895683\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014778985618717141\n",
      "Average test loss: 0.0029910586198998823\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014767656198806232\n",
      "Average test loss: 0.002998280229874783\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014761720754206181\n",
      "Average test loss: 0.0030350404692192873\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014752204217844539\n",
      "Average test loss: 0.0030020613252288767\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014749490826494164\n",
      "Average test loss: 0.0029929074951344065\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014737954287893242\n",
      "Average test loss: 0.003005715508841806\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014732858991457357\n",
      "Average test loss: 0.003025557619623012\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014725310257739491\n",
      "Average test loss: 0.0030157211557444597\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014712580143577523\n",
      "Average test loss: 0.002997513125133183\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014716944940388202\n",
      "Average test loss: 0.003044759215373132\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014707082900736067\n",
      "Average test loss: 0.003031141706638866\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014702006400459343\n",
      "Average test loss: 0.0029912204560306336\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01469875564922889\n",
      "Average test loss: 0.003027390502186285\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014689275693562296\n",
      "Average test loss: 0.0030335025067130725\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014673387832111782\n",
      "Average test loss: 0.0030232001497513717\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01467045289443599\n",
      "Average test loss: 0.003021312344819307\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01466507871945699\n",
      "Average test loss: 0.0029993052559180392\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014661064498126507\n",
      "Average test loss: 0.0030518227213372786\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014649497915473248\n",
      "Average test loss: 0.0030109367868345646\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014651184674766329\n",
      "Average test loss: 0.0030138945666452247\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014631438503662745\n",
      "Average test loss: 0.0030422237979041205\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01463212037169271\n",
      "Average test loss: 0.0030763155381298727\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014625914113389121\n",
      "Average test loss: 0.00301658663753834\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014618897904124524\n",
      "Average test loss: 0.0030960565476367873\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014607364180187384\n",
      "Average test loss: 0.0029826823274294537\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014603303832312425\n",
      "Average test loss: 0.00303360071612729\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014601847767829895\n",
      "Average test loss: 0.0030312397635231417\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014596782183481587\n",
      "Average test loss: 0.0030176787945545382\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014594108189973566\n",
      "Average test loss: 0.002999633307672209\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014579420449833074\n",
      "Average test loss: 0.0030472686746054224\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01457232991937134\n",
      "Average test loss: 0.003210167519748211\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014575208066238297\n",
      "Average test loss: 0.0030258720960054135\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014560156967904832\n",
      "Average test loss: 0.0030054530976340176\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014561798719068368\n",
      "Average test loss: 0.0030727432717879612\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014554640983541806\n",
      "Average test loss: 0.0030146347042173146\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014541294278370009\n",
      "Average test loss: 0.0030126886541644734\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014536417849361896\n",
      "Average test loss: 0.003268631375498242\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014539918240573671\n",
      "Average test loss: 0.002984541465010908\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014528131035466988\n",
      "Average test loss: 0.003006435237824917\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014527712820304765\n",
      "Average test loss: 0.0030528621741880975\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014525281872186395\n",
      "Average test loss: 0.003007841272693541\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014510382139020495\n",
      "Average test loss: 0.003044374838471413\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014506137034959263\n",
      "Average test loss: 0.003051477338704798\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014502716249061956\n",
      "Average test loss: 0.0030941427503195073\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01449671973950333\n",
      "Average test loss: 0.0030553194816327757\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01449639791995287\n",
      "Average test loss: 0.0029963545952406195\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014482854956554042\n",
      "Average test loss: 0.0030186500627961423\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014482766408059332\n",
      "Average test loss: 0.0030067550260573627\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014473147785498037\n",
      "Average test loss: 0.0030540291513833735\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01446521843638685\n",
      "Average test loss: 0.0030177568758113517\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014467613122529454\n",
      "Average test loss: 0.0030325892373091645\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01446250698963801\n",
      "Average test loss: 0.0030411849021911622\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014446371818582217\n",
      "Average test loss: 0.0030082620253993404\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01445484019153648\n",
      "Average test loss: 0.003021662943893009\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014440802452464898\n",
      "Average test loss: 0.0072997201188570926\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014445621330704954\n",
      "Average test loss: 0.0030504429961244264\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014431382762889067\n",
      "Average test loss: 0.0030574640956603817\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014421158072021273\n",
      "Average test loss: 0.0030729174663623173\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01442224793550041\n",
      "Average test loss: 0.0030715412362996076\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014418180086546474\n",
      "Average test loss: 0.0030536184716555806\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014405286407305134\n",
      "Average test loss: 0.0030466795915530787\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014410617320901818\n",
      "Average test loss: 0.003034213271199001\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014400949685109986\n",
      "Average test loss: 0.003024378159807788\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014401569512155321\n",
      "Average test loss: 0.003023824161125554\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014383879928953118\n",
      "Average test loss: 0.003051817829410235\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01438723591880666\n",
      "Average test loss: 0.0030325135410659843\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014380306901203262\n",
      "Average test loss: 0.003047419511609607\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014386597058839268\n",
      "Average test loss: 0.0030345205399725174\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01437016582985719\n",
      "Average test loss: 0.0030276331305503844\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014376562894218499\n",
      "Average test loss: 0.0030429621409210892\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014357871524989605\n",
      "Average test loss: 0.003025928158313036\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014356574311024613\n",
      "Average test loss: 0.003088586936601334\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014357950697342555\n",
      "Average test loss: 0.003081843605885903\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014351859495043754\n",
      "Average test loss: 0.0030764912391702333\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014343665900329748\n",
      "Average test loss: 0.003088298866318332\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014335083564950361\n",
      "Average test loss: 0.003041786996854676\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014328893635008071\n",
      "Average test loss: 0.003081148289351\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014331561955312887\n",
      "Average test loss: 0.003076582617022925\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0143366000784768\n",
      "Average test loss: 0.0030947090176244576\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014326195350951618\n",
      "Average test loss: 0.003038243350469404\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014324148163199425\n",
      "Average test loss: 0.003048450602425469\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014325393671790759\n",
      "Average test loss: 0.0030485979347593256\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014313870142731402\n",
      "Average test loss: 0.0030443755946018627\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01430767806371053\n",
      "Average test loss: 0.0030714085818164876\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014294597007334233\n",
      "Average test loss: 0.0030499548618164326\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014295791471170055\n",
      "Average test loss: 0.0030271556615415547\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01429022647274865\n",
      "Average test loss: 0.003049275161077579\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01429403296361367\n",
      "Average test loss: 0.003045603939642509\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014296495224038759\n",
      "Average test loss: 0.0030466685607615446\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01427860728899638\n",
      "Average test loss: 0.0030417396227518717\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014276908203131622\n",
      "Average test loss: 0.0030424808294822773\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01426958251827293\n",
      "Average test loss: 0.003032920712398158\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014266038055635161\n",
      "Average test loss: 0.003046433039009571\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014269542083144187\n",
      "Average test loss: 0.0030342549511955842\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01425721701565716\n",
      "Average test loss: 0.0030731817639122407\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014258167470494906\n",
      "Average test loss: 0.0031161456290218566\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014246004420850012\n",
      "Average test loss: 0.0030796454052130382\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014244336491657628\n",
      "Average test loss: 0.0031118379823035663\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014237927129699124\n",
      "Average test loss: 0.0030434403338780005\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014249709578851858\n",
      "Average test loss: 0.0030576645371814567\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0142371244430542\n",
      "Average test loss: 0.0030831813951954245\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01424230941136678\n",
      "Average test loss: 0.003052614316344261\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014230071110030015\n",
      "Average test loss: 0.003011942584067583\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014221787014769184\n",
      "Average test loss: 0.0031370164185969365\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014227246396243572\n",
      "Average test loss: 0.0030805882521801526\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014215038319428762\n",
      "Average test loss: 0.0031800159779894685\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01422082774920596\n",
      "Average test loss: 0.003047528642954098\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01420989548911651\n",
      "Average test loss: 0.0032209838974393075\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014212315588361687\n",
      "Average test loss: 0.0030905907801869843\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014202497351500722\n",
      "Average test loss: 0.0030823207824594444\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014197403291033374\n",
      "Average test loss: 0.0030904394406825303\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014202403536273374\n",
      "Average test loss: 0.0030490161598556572\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014197812129225995\n",
      "Average test loss: 0.0030559698726154035\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014194812409579753\n",
      "Average test loss: 0.003040145322266552\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014185099158022138\n",
      "Average test loss: 0.0030940696030027338\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014184047992858622\n",
      "Average test loss: 0.0031581665536181795\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014184787305692831\n",
      "Average test loss: 0.0031143102426495818\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014171467704077562\n",
      "Average test loss: 0.0030911352317780257\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014179870218866401\n",
      "Average test loss: 0.0031133504035986133\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014173567646907435\n",
      "Average test loss: 0.0030447600243820087\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01416432019488679\n",
      "Average test loss: 0.0030505177435568636\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014158495249019729\n",
      "Average test loss: 0.003050579112023115\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014158691137201257\n",
      "Average test loss: 0.0030744510947002305\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01415009025649892\n",
      "Average test loss: 0.003046063934556312\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014154099230964978\n",
      "Average test loss: 0.0030638842669626076\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01415718585666683\n",
      "Average test loss: 0.003190316379484203\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014142349576784504\n",
      "Average test loss: 0.0030773730849226314\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014135804132454926\n",
      "Average test loss: 0.0030546824584404626\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014136542316940096\n",
      "Average test loss: 0.0031137412254595093\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014132849433355861\n",
      "Average test loss: 0.0031145294366611374\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014131723264025318\n",
      "Average test loss: 0.0031570944502535793\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014130030103855664\n",
      "Average test loss: 0.0031165209458106093\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014115313151644335\n",
      "Average test loss: 0.0030487020450333754\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014125491089291042\n",
      "Average test loss: 0.0030675922207948235\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01411624949094322\n",
      "Average test loss: 0.0030742223606341416\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014117271498673492\n",
      "Average test loss: 0.0031076389708452755\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014113087712890573\n",
      "Average test loss: 0.003151154987514019\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014111474855906433\n",
      "Average test loss: 0.0034552301884525354\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014106010232534674\n",
      "Average test loss: 0.003037355781429344\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014106679955290424\n",
      "Average test loss: 0.0030956846535619763\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014101986504263348\n",
      "Average test loss: 0.003189253457511465\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014112181409365602\n",
      "Average test loss: 0.0030899742727892266\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014091891610787975\n",
      "Average test loss: 0.0030472377706319094\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014094628067480193\n",
      "Average test loss: 0.0030431359145376416\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01409344800396098\n",
      "Average test loss: 0.0031009790665573545\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014080623359315925\n",
      "Average test loss: 0.003071705656540063\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014076449119382434\n",
      "Average test loss: 0.003091563815457953\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014089948404166434\n",
      "Average test loss: 0.0030867926608771088\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014065378199021022\n",
      "Average test loss: 0.0031473254830473\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014076174814667967\n",
      "Average test loss: 0.0031093807245294253\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014075427345103688\n",
      "Average test loss: 0.0032436305679794815\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014084456250071526\n",
      "Average test loss: 0.003213744352468186\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014077629290521145\n",
      "Average test loss: 0.003093093170887894\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014061179702480634\n",
      "Average test loss: 0.003138584599312809\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014061501083274683\n",
      "Average test loss: 0.003097365709228648\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014060323923826218\n",
      "Average test loss: 0.003078127845397426\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01404975175857544\n",
      "Average test loss: 0.0030300290532824068\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014047999846438567\n",
      "Average test loss: 0.003154931983186139\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014062090343899197\n",
      "Average test loss: 0.003115995552390814\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014038360381291972\n",
      "Average test loss: 0.0031179743671996725\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014038877228067981\n",
      "Average test loss: 0.003102285983040929\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014042119934327073\n",
      "Average test loss: 0.0030909308468302092\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014040102179679605\n",
      "Average test loss: 0.003144725602534082\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014029684416121907\n",
      "Average test loss: 0.0030695866466396386\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014024336429933707\n",
      "Average test loss: 0.0030783500605159335\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014029080025023885\n",
      "Average test loss: 0.0031497484640114837\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014024393387138843\n",
      "Average test loss: 0.0030745138712227347\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014024637456569407\n",
      "Average test loss: 0.003148168928300341\n",
      "Epoch 286/300\n",
      "Average training loss: 0.014024100667072667\n",
      "Average test loss: 0.003103348431280918\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01402079258279668\n",
      "Average test loss: 0.003081822615530756\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014018706417746014\n",
      "Average test loss: 0.003055504839660393\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014011471147338549\n",
      "Average test loss: 0.0030760782572130363\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014007258440057437\n",
      "Average test loss: 0.003068486571725872\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014000135936670833\n",
      "Average test loss: 0.0030567339689781267\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014015069523619281\n",
      "Average test loss: 0.003061034849534432\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01400944805310832\n",
      "Average test loss: 0.003100699670198891\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013995503087010648\n",
      "Average test loss: 0.0030899013510594766\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013993755612936285\n",
      "Average test loss: 0.0031092417045599885\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013994931908117401\n",
      "Average test loss: 0.0031279111972285644\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014002688850793574\n",
      "Average test loss: 0.0032300511550986104\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013979325534568893\n",
      "Average test loss: 0.003091016947188311\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013988285494347413\n",
      "Average test loss: 0.0030778971393075256\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01398873712370793\n",
      "Average test loss: 0.003095125008788374\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.055400826207465594\n",
      "Average test loss: 0.00397502169219984\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01828783111770948\n",
      "Average test loss: 0.003584194839000702\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0168741365439362\n",
      "Average test loss: 0.0033535924918121763\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01615848949468798\n",
      "Average test loss: 0.0031682713923768865\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015664693509538967\n",
      "Average test loss: 0.0031245926277091106\n",
      "Epoch 6/300\n",
      "Average training loss: 0.015295080952346324\n",
      "Average test loss: 0.002988235314273172\n",
      "Epoch 7/300\n",
      "Average training loss: 0.014994956233435207\n",
      "Average test loss: 0.002915899650918113\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014745235602060954\n",
      "Average test loss: 0.002849872402432892\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014513894870877266\n",
      "Average test loss: 0.002804484289139509\n",
      "Epoch 10/300\n",
      "Average training loss: 0.014337298438780838\n",
      "Average test loss: 0.002758947107526991\n",
      "Epoch 11/300\n",
      "Average training loss: 0.014175312115914291\n",
      "Average test loss: 0.0027266035853988593\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014015572408007252\n",
      "Average test loss: 0.002698610704392195\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013878445968859725\n",
      "Average test loss: 0.0026560505928678647\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013759349417355326\n",
      "Average test loss: 0.0026276503952427045\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013628822029464775\n",
      "Average test loss: 0.002595352648033036\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013515650694568952\n",
      "Average test loss: 0.0025923741857210793\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013418975873953767\n",
      "Average test loss: 0.00255918357686864\n",
      "Epoch 18/300\n",
      "Average training loss: 0.013320999324735667\n",
      "Average test loss: 0.0025419587459829117\n",
      "Epoch 19/300\n",
      "Average training loss: 0.013206027275986142\n",
      "Average test loss: 0.0024992344364937807\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013130255041850938\n",
      "Average test loss: 0.002481621600687504\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013052498896916707\n",
      "Average test loss: 0.0024835094182441633\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012967463877465989\n",
      "Average test loss: 0.0024342774711549283\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012898816919989056\n",
      "Average test loss: 0.0024247812094787755\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01284204543630282\n",
      "Average test loss: 0.0024215437492562664\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012790085996190708\n",
      "Average test loss: 0.002404547576689058\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012728428843120735\n",
      "Average test loss: 0.0023951091406246025\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012670706412858434\n",
      "Average test loss: 0.0023974005627549356\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012631763596501616\n",
      "Average test loss: 0.002376790733387073\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01258568792624606\n",
      "Average test loss: 0.0023593998230579828\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012542454307277998\n",
      "Average test loss: 0.0023657667425771556\n",
      "Epoch 31/300\n",
      "Average training loss: 0.012486571678684817\n",
      "Average test loss: 0.002346747607199682\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012457269077499708\n",
      "Average test loss: 0.002334008314957221\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01242269744310114\n",
      "Average test loss: 0.002324847190744347\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012385731874240769\n",
      "Average test loss: 0.0023188814328362543\n",
      "Epoch 35/300\n",
      "Average training loss: 0.012354441756175625\n",
      "Average test loss: 0.002333853877149522\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012321603354480532\n",
      "Average test loss: 0.002303904413142138\n",
      "Epoch 37/300\n",
      "Average training loss: 0.012289987682468362\n",
      "Average test loss: 0.0023129676161333917\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012267589464783669\n",
      "Average test loss: 0.0022808282414658204\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012235129106375905\n",
      "Average test loss: 0.002296390392093195\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012208337056140104\n",
      "Average test loss: 0.002272447963141733\n",
      "Epoch 41/300\n",
      "Average training loss: 0.012182156653453907\n",
      "Average test loss: 0.0022673293047895033\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01215419712000423\n",
      "Average test loss: 0.0022750132170816263\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01214257309999731\n",
      "Average test loss: 0.002259757139099141\n",
      "Epoch 44/300\n",
      "Average training loss: 0.012113581020798948\n",
      "Average test loss: 0.002262227363263567\n",
      "Epoch 45/300\n",
      "Average training loss: 0.012082541368487808\n",
      "Average test loss: 0.002263267100064291\n",
      "Epoch 46/300\n",
      "Average training loss: 0.012070507627394464\n",
      "Average test loss: 0.0022580530835936466\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012050186897317568\n",
      "Average test loss: 0.002257280920114782\n",
      "Epoch 48/300\n",
      "Average training loss: 0.012031472416387664\n",
      "Average test loss: 0.0022423729668888783\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01201428982284334\n",
      "Average test loss: 0.002251744523540967\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011995716516342427\n",
      "Average test loss: 0.0022381525043812063\n",
      "Epoch 51/300\n",
      "Average training loss: 0.011975393535362349\n",
      "Average test loss: 0.002232314395097395\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011965664391716322\n",
      "Average test loss: 0.0022405733248839774\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011944695419735378\n",
      "Average test loss: 0.002245941078186863\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011928143506248792\n",
      "Average test loss: 0.002259989456066655\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011908733264439636\n",
      "Average test loss: 0.0022214277966154947\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01188438960744275\n",
      "Average test loss: 0.002227540386737221\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011878474924299451\n",
      "Average test loss: 0.002225970736808247\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011865438044071197\n",
      "Average test loss: 0.002220650306385424\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01184442020704349\n",
      "Average test loss: 0.0022198551597280634\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011835354747043716\n",
      "Average test loss: 0.0022127734683454036\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011825829282402993\n",
      "Average test loss: 0.0022312165767782266\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011805346953786081\n",
      "Average test loss: 0.0022579113253288798\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011791862770915032\n",
      "Average test loss: 0.002243901642453339\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01177861506988605\n",
      "Average test loss: 0.0022094513182010916\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011769014888339573\n",
      "Average test loss: 0.0022121420041140583\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01175214439133803\n",
      "Average test loss: 0.0022122823785369595\n",
      "Epoch 67/300\n",
      "Average training loss: 0.011746826416916318\n",
      "Average test loss: 0.0022053419287419982\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01173230749865373\n",
      "Average test loss: 0.0022093788110651076\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011717432087494267\n",
      "Average test loss: 0.002200742679544621\n",
      "Epoch 70/300\n",
      "Average training loss: 0.011711834605369302\n",
      "Average test loss: 0.002202314627253347\n",
      "Epoch 71/300\n",
      "Average training loss: 0.011701539892289373\n",
      "Average test loss: 0.0022533245178767377\n",
      "Epoch 72/300\n",
      "Average training loss: 0.011674428198900487\n",
      "Average test loss: 0.002200489622540772\n",
      "Epoch 73/300\n",
      "Average training loss: 0.011673879174722566\n",
      "Average test loss: 0.002198615425059365\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011657607646452055\n",
      "Average test loss: 0.002192222498357296\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011643581890397601\n",
      "Average test loss: 0.002224283154432972\n",
      "Epoch 76/300\n",
      "Average training loss: 0.011652535796165465\n",
      "Average test loss: 0.002197512508266502\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01163784176359574\n",
      "Average test loss: 0.0021889312395619023\n",
      "Epoch 78/300\n",
      "Average training loss: 0.011617328465812735\n",
      "Average test loss: 0.0021882626451551913\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01160987083034383\n",
      "Average test loss: 0.0021963042002171277\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011602821158866088\n",
      "Average test loss: 0.002206894292910066\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01158974989503622\n",
      "Average test loss: 0.0022059141227768527\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01158176882026924\n",
      "Average test loss: 0.0021932209746705162\n",
      "Epoch 83/300\n",
      "Average training loss: 0.011566337654160128\n",
      "Average test loss: 0.002538079747516248\n",
      "Epoch 84/300\n",
      "Average training loss: 0.011563841191430887\n",
      "Average test loss: 0.002201247567931811\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01155327648917834\n",
      "Average test loss: 0.002208640855219629\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011541534151881933\n",
      "Average test loss: 0.002205828141214119\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011538581919338969\n",
      "Average test loss: 0.002204564472246501\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011515016319023238\n",
      "Average test loss: 0.00217855213313467\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011518712552057372\n",
      "Average test loss: 0.0021895825270977286\n",
      "Epoch 90/300\n",
      "Average training loss: 0.011503954102595648\n",
      "Average test loss: 0.0021859944795982705\n",
      "Epoch 91/300\n",
      "Average training loss: 0.011494138114154338\n",
      "Average test loss: 0.002178072600108054\n",
      "Epoch 92/300\n",
      "Average training loss: 0.011491415762652954\n",
      "Average test loss: 0.0022018706508808664\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011482478097081184\n",
      "Average test loss: 0.0021983742544220554\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011469457799361812\n",
      "Average test loss: 0.0022019845304182832\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011476375810801983\n",
      "Average test loss: 0.0021720108317418232\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01145426106121805\n",
      "Average test loss: 0.002205383053463366\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011447604961693288\n",
      "Average test loss: 0.002180126405838463\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011443937593036227\n",
      "Average test loss: 0.002192896672834953\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011431974897782008\n",
      "Average test loss: 0.0022339508564521867\n",
      "Epoch 100/300\n",
      "Average training loss: 0.011424758736458089\n",
      "Average test loss: 0.0021815190100007587\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011418881095117992\n",
      "Average test loss: 0.002183600875859459\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011415280998580985\n",
      "Average test loss: 0.0022029569105555613\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011404681435061825\n",
      "Average test loss: 0.002195326086237199\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011397346679535177\n",
      "Average test loss: 0.002199428134701318\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01139173186570406\n",
      "Average test loss: 0.002183342931378219\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01138058418366644\n",
      "Average test loss: 0.002192876748326752\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011375511922770077\n",
      "Average test loss: 0.002200553050678637\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011367536112666131\n",
      "Average test loss: 0.0021707509698139296\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011360945406059425\n",
      "Average test loss: 0.0022186744581494068\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011358210650169186\n",
      "Average test loss: 0.0022086192599187295\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011342541742655966\n",
      "Average test loss: 0.002201939805928204\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011338933235241306\n",
      "Average test loss: 0.0021720285715742245\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011336349206666152\n",
      "Average test loss: 0.002512132819948925\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011324893856628073\n",
      "Average test loss: 0.002224408816339241\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011316113491853078\n",
      "Average test loss: 0.002178209369381269\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011314273055228922\n",
      "Average test loss: 0.002199919073118104\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011315588090154859\n",
      "Average test loss: 0.00219558757233123\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011300777045389017\n",
      "Average test loss: 0.0021910434918892053\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011295622451437844\n",
      "Average test loss: 0.0021796182400236528\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011291551565130552\n",
      "Average test loss: 0.002208824429350595\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011287397791941961\n",
      "Average test loss: 0.0021724214364464084\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011279069694379966\n",
      "Average test loss: 0.0022966147994415626\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011272979794277086\n",
      "Average test loss: 0.0021847868190250463\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011271221737894747\n",
      "Average test loss: 0.0021733169271093275\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011264738224446774\n",
      "Average test loss: 0.0021859365082863304\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01125093200057745\n",
      "Average test loss: 0.002185282710008323\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011245053123682737\n",
      "Average test loss: 0.0022088645121289626\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011238318127062585\n",
      "Average test loss: 0.0021922196268828378\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011235206955009036\n",
      "Average test loss: 0.002167021003862222\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011228734423716863\n",
      "Average test loss: 0.0021882893919116923\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01122244309882323\n",
      "Average test loss: 0.002178805713645286\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011224662128421995\n",
      "Average test loss: 0.0021919290508247086\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01121706219845348\n",
      "Average test loss: 0.0021928129175470936\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01120876604070266\n",
      "Average test loss: 0.0021696847304701807\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011204164606829485\n",
      "Average test loss: 0.0021941967571361197\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011193180192675855\n",
      "Average test loss: 0.002181310771343609\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011189735491242674\n",
      "Average test loss: 0.002184752255264256\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01118922672337956\n",
      "Average test loss: 0.002210547761577699\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011177794641918606\n",
      "Average test loss: 0.002185985654178593\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011178670045402316\n",
      "Average test loss: 0.002205061943580707\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011167477081219354\n",
      "Average test loss: 0.0022082666251808407\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011162418297595449\n",
      "Average test loss: 0.002204258184052176\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011164142414927483\n",
      "Average test loss: 0.0021755972291446395\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01115944664263063\n",
      "Average test loss: 0.00218569960941871\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011153662133547996\n",
      "Average test loss: 0.0021761246054536765\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011146049640244907\n",
      "Average test loss: 0.0021961810938600037\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01114359548025661\n",
      "Average test loss: 0.0021734146408529747\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011143956130577459\n",
      "Average test loss: 0.0021769306978417766\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011130388322803709\n",
      "Average test loss: 0.0021978766676038503\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011122988616426785\n",
      "Average test loss: 0.002179764590226114\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01111856787568993\n",
      "Average test loss: 0.0021923360912543205\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011119269011749162\n",
      "Average test loss: 0.0022039806288149623\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011118327734371027\n",
      "Average test loss: 0.002201594622184833\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011104856236113443\n",
      "Average test loss: 0.0021880806195032267\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01110211694985628\n",
      "Average test loss: 0.0021860764233602417\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011091148739059767\n",
      "Average test loss: 0.002205424756536053\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011103639008684291\n",
      "Average test loss: 0.002208515973140796\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011091445033748945\n",
      "Average test loss: 0.0021793967635474272\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011085906379752689\n",
      "Average test loss: 0.0021943510454148054\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011077179795338048\n",
      "Average test loss: 0.0022150802721993793\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011074349200559988\n",
      "Average test loss: 0.0021684813766429823\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011069542035460471\n",
      "Average test loss: 0.0021733238022360536\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011062661711540488\n",
      "Average test loss: 0.002188298560265038\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011061606011870834\n",
      "Average test loss: 0.0022211157189061244\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01105917226192024\n",
      "Average test loss: 0.0021807033276806274\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011056270247532262\n",
      "Average test loss: 0.0021788376612174843\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011047075404061212\n",
      "Average test loss: 0.00224445274968942\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011043855075207021\n",
      "Average test loss: 0.002216427073193093\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011046482980251312\n",
      "Average test loss: 0.0022220059531844324\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011035126989086468\n",
      "Average test loss: 0.0021946759588188595\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011026627173026403\n",
      "Average test loss: 0.002177593442197475\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011034196712076665\n",
      "Average test loss: 0.0022211274263552495\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011027613502409724\n",
      "Average test loss: 0.002175286107386152\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011016878466639254\n",
      "Average test loss: 0.002202692585686843\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011013541118138367\n",
      "Average test loss: 0.0021963350132314695\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01101296137770017\n",
      "Average test loss: 0.0021893727170924347\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011008128912498553\n",
      "Average test loss: 0.0022088433052930567\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011003301799297333\n",
      "Average test loss: 0.002185216563857264\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011000324913197093\n",
      "Average test loss: 0.00228314001713362\n",
      "Epoch 180/300\n",
      "Average training loss: 0.010994034459193548\n",
      "Average test loss: 0.0021926439178900584\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010991664742430051\n",
      "Average test loss: 0.002279570682388213\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010992964748707083\n",
      "Average test loss: 0.0021950324742744365\n",
      "Epoch 183/300\n",
      "Average training loss: 0.010988110164801279\n",
      "Average test loss: 0.0021921712848254377\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010982671258350214\n",
      "Average test loss: 0.002204547551874485\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010978597972955969\n",
      "Average test loss: 0.002203592296068867\n",
      "Epoch 186/300\n",
      "Average training loss: 0.010976468360258474\n",
      "Average test loss: 0.002191497691286107\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010979829414023293\n",
      "Average test loss: 0.002198760458682146\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010971453269322713\n",
      "Average test loss: 0.0021766394536114403\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010970610227021907\n",
      "Average test loss: 0.0022558116865758265\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010959620065159268\n",
      "Average test loss: 0.002180976641881797\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01096536915211214\n",
      "Average test loss: 0.0023263527357743844\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010950422996448146\n",
      "Average test loss: 0.0021925716761292684\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010949967212147183\n",
      "Average test loss: 0.0022018668461177083\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010948477216892773\n",
      "Average test loss: 0.0022088629161525104\n",
      "Epoch 195/300\n",
      "Average training loss: 0.010941727152715127\n",
      "Average test loss: 0.002196117989718914\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01095595793508821\n",
      "Average test loss: 0.0022417082511302497\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01093882424549924\n",
      "Average test loss: 0.002182168660271499\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010935514005521934\n",
      "Average test loss: 0.002250925307265586\n",
      "Epoch 199/300\n",
      "Average training loss: 0.010930798262357712\n",
      "Average test loss: 0.0022242201266603335\n",
      "Epoch 200/300\n",
      "Average training loss: 0.010920889376766153\n",
      "Average test loss: 0.0021822480048156448\n",
      "Epoch 201/300\n",
      "Average training loss: 0.010924052271578047\n",
      "Average test loss: 0.0022097968853389224\n",
      "Epoch 202/300\n",
      "Average training loss: 0.010929698012769223\n",
      "Average test loss: 0.002194329026569095\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010914163091944323\n",
      "Average test loss: 0.0022201448547550374\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010907870331572162\n",
      "Average test loss: 0.002200909215129084\n",
      "Epoch 205/300\n",
      "Average training loss: 0.010910587958991528\n",
      "Average test loss: 0.0021928891569582953\n",
      "Epoch 206/300\n",
      "Average training loss: 0.010909481728242503\n",
      "Average test loss: 0.0022261748901671835\n",
      "Epoch 207/300\n",
      "Average training loss: 0.010906135837237041\n",
      "Average test loss: 0.0022640857161540123\n",
      "Epoch 208/300\n",
      "Average training loss: 0.010905319841371643\n",
      "Average test loss: 0.0022016775403171777\n",
      "Epoch 209/300\n",
      "Average training loss: 0.010905709524949393\n",
      "Average test loss: 0.002209168930641479\n",
      "Epoch 210/300\n",
      "Average training loss: 0.010887104580799738\n",
      "Average test loss: 0.00219782634679642\n",
      "Epoch 211/300\n",
      "Average training loss: 0.010884169689483112\n",
      "Average test loss: 0.0022359849042776557\n",
      "Epoch 212/300\n",
      "Average training loss: 0.010892854967051082\n",
      "Average test loss: 0.0022159416365126767\n",
      "Epoch 213/300\n",
      "Average training loss: 0.010885390160812271\n",
      "Average test loss: 0.0021949847946978276\n",
      "Epoch 214/300\n",
      "Average training loss: 0.010877432194021014\n",
      "Average test loss: 0.002207280255336728\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010875429176621968\n",
      "Average test loss: 0.0022159582996327015\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010878831346829732\n",
      "Average test loss: 0.0022112686093896626\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010875788037975629\n",
      "Average test loss: 0.0022438210973309146\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010869224588076274\n",
      "Average test loss: 0.002229418300713102\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010863572489884165\n",
      "Average test loss: 0.002207311381275455\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010862513716850016\n",
      "Average test loss: 0.0023575159114682014\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010864412073045969\n",
      "Average test loss: 0.0022673419792619016\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010858304419451289\n",
      "Average test loss: 0.0022701834499215087\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01085372897113363\n",
      "Average test loss: 0.002225879482097096\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010851936917338106\n",
      "Average test loss: 0.0022278596605691645\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010859340802662902\n",
      "Average test loss: 0.0022151036406349803\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010842933749159176\n",
      "Average test loss: 0.002239354644384649\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010843208332028655\n",
      "Average test loss: 0.0022049545349760186\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010839024781352943\n",
      "Average test loss: 0.0022315009312911167\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010834861330274078\n",
      "Average test loss: 0.002215286673357089\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010822971639533838\n",
      "Average test loss: 0.002327412774372432\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010829312852687305\n",
      "Average test loss: 0.0022096416565279167\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010828484966523118\n",
      "Average test loss: 0.0022639947179704904\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010825494889583853\n",
      "Average test loss: 0.0021968010479791297\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010827041444679102\n",
      "Average test loss: 0.002340959855458803\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01080774112045765\n",
      "Average test loss: 0.0022257168903532954\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010830771909819709\n",
      "Average test loss: 0.00264627544188665\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010812101013130612\n",
      "Average test loss: 0.0022409900166094304\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010810401649110847\n",
      "Average test loss: 0.0022507379322002333\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010810193539907535\n",
      "Average test loss: 0.0022419142298814323\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01081536842799849\n",
      "Average test loss: 0.002279827603863345\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010793368269999822\n",
      "Average test loss: 0.002222664836587177\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010807613217996226\n",
      "Average test loss: 0.0022137717101722958\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010792830755313237\n",
      "Average test loss: 0.002201811768942409\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01080158565276199\n",
      "Average test loss: 0.0022473254564942583\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010795320745971467\n",
      "Average test loss: 0.0022143567186883755\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010799891949527794\n",
      "Average test loss: 0.002249838923414548\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010798087783985668\n",
      "Average test loss: 0.0022417685633732212\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010784593208796447\n",
      "Average test loss: 0.0022220213851994936\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010783199422889286\n",
      "Average test loss: 0.00224085501788391\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010780133809894323\n",
      "Average test loss: 0.0022712256508982845\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010776258401572704\n",
      "Average test loss: 0.0022139623016119003\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010785339349673854\n",
      "Average test loss: 0.002226346682757139\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010776770997378561\n",
      "Average test loss: 0.002255138941316141\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01077477726340294\n",
      "Average test loss: 0.002229734276317888\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01076753192394972\n",
      "Average test loss: 0.002210604103954716\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010785916361129947\n",
      "Average test loss: 0.0022293545953515504\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010760733796490562\n",
      "Average test loss: 0.0022235884241138897\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010770996576382055\n",
      "Average test loss: 0.002230531042234765\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010759540648096137\n",
      "Average test loss: 0.002275505176020993\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01076486034774118\n",
      "Average test loss: 0.0022304132948112155\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010756581264651484\n",
      "Average test loss: 0.002199506222166949\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01074802848448356\n",
      "Average test loss: 0.002219197211787105\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010753487139940262\n",
      "Average test loss: 0.002258853654170202\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010753412046366267\n",
      "Average test loss: 0.002230038878818353\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010746413213511307\n",
      "Average test loss: 0.0022396577157908016\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010754755419161585\n",
      "Average test loss: 0.0022039988179587654\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01074813337624073\n",
      "Average test loss: 0.002256234145619803\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010740358838604556\n",
      "Average test loss: 0.0022339797468028137\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010736937528269158\n",
      "Average test loss: 0.0022517800678809484\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010736735922180943\n",
      "Average test loss: 0.002245473227256702\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010731167565617296\n",
      "Average test loss: 0.00222257735249069\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010738326972557439\n",
      "Average test loss: 0.0022126200360556444\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010728288744886716\n",
      "Average test loss: 0.0022507215721739664\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010724609984291924\n",
      "Average test loss: 0.00227300650636769\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010736601702868939\n",
      "Average test loss: 0.002217614888937937\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010723243595825301\n",
      "Average test loss: 0.0023205076263596615\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010719627204868529\n",
      "Average test loss: 0.0022592873639530604\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010715945783588622\n",
      "Average test loss: 0.0022331654952011176\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010718757988678084\n",
      "Average test loss: 0.0022134793418356114\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010717357075048818\n",
      "Average test loss: 0.002242622439335618\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010723254902081357\n",
      "Average test loss: 0.002231785837560892\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010712674000196987\n",
      "Average test loss: 0.0022481509669580392\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01070777051274975\n",
      "Average test loss: 0.0022415383520225686\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010701754019492202\n",
      "Average test loss: 0.002241365839002861\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010703508681721157\n",
      "Average test loss: 0.0022279604801701174\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010705601914889283\n",
      "Average test loss: 0.002251830450983511\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010700497959223058\n",
      "Average test loss: 0.0022417304184701708\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010704470778091087\n",
      "Average test loss: 0.002224065770705541\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010696813309358226\n",
      "Average test loss: 0.002258748529996309\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010698013035787476\n",
      "Average test loss: 0.0022292286939918993\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010696843663023578\n",
      "Average test loss: 0.002244360901415348\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010693445870031914\n",
      "Average test loss: 0.0022377278133191994\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010690296809706424\n",
      "Average test loss: 0.0022796323070716526\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010699754715793663\n",
      "Average test loss: 0.002266082223918703\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010675410625007417\n",
      "Average test loss: 0.002256764625095659\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010680399265554216\n",
      "Average test loss: 0.0022423381546719206\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01067773088398907\n",
      "Average test loss: 0.0022242973610344862\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010680448295341597\n",
      "Average test loss: 0.0022974370534842212\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010670696430736117\n",
      "Average test loss: 0.002325277622996105\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010668571453127596\n",
      "Average test loss: 0.0022565477924007507\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.050476591292354794\n",
      "Average test loss: 0.0032848728568189672\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01514590958174732\n",
      "Average test loss: 0.002937777779582474\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013727711588144302\n",
      "Average test loss: 0.0026336480128682322\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01298290590610769\n",
      "Average test loss: 0.002562072164896462\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012449506917761432\n",
      "Average test loss: 0.0023812988021721443\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012042538940078682\n",
      "Average test loss: 0.0023150726238058673\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011732283596363333\n",
      "Average test loss: 0.0022071496684932046\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01147653230610821\n",
      "Average test loss: 0.002208245335974627\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011249990444216464\n",
      "Average test loss: 0.0020972901246406966\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011053110353648662\n",
      "Average test loss: 0.002062915286877089\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01088180237925715\n",
      "Average test loss: 0.0020385401035762497\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010733235311177041\n",
      "Average test loss: 0.0019840479465201497\n",
      "Epoch 13/300\n",
      "Average training loss: 0.010598876758582062\n",
      "Average test loss: 0.0019557212923342984\n",
      "Epoch 14/300\n",
      "Average training loss: 0.010490063200394313\n",
      "Average test loss: 0.0019293270183520185\n",
      "Epoch 15/300\n",
      "Average training loss: 0.010354474854138162\n",
      "Average test loss: 0.0019784546192321514\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010266109760436747\n",
      "Average test loss: 0.0018797645192179416\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010161125170273913\n",
      "Average test loss: 0.0019595585882456767\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010062941209723552\n",
      "Average test loss: 0.001851588514426516\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00997493883015381\n",
      "Average test loss: 0.0018442141483020452\n",
      "Epoch 20/300\n",
      "Average training loss: 0.009890787630859349\n",
      "Average test loss: 0.001829598052841094\n",
      "Epoch 21/300\n",
      "Average training loss: 0.009817170206871298\n",
      "Average test loss: 0.001791779653997057\n",
      "Epoch 22/300\n",
      "Average training loss: 0.009757359439300166\n",
      "Average test loss: 0.0017710530379166207\n",
      "Epoch 23/300\n",
      "Average training loss: 0.009688850228157308\n",
      "Average test loss: 0.0017500171253664626\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009640330790645546\n",
      "Average test loss: 0.0017518811172081365\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00957889060344961\n",
      "Average test loss: 0.001722491755357219\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009534209896292951\n",
      "Average test loss: 0.0017161735672917631\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009496743626892566\n",
      "Average test loss: 0.0016894675800576805\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009441490047093895\n",
      "Average test loss: 0.0016870088706103464\n",
      "Epoch 29/300\n",
      "Average training loss: 0.009424570026083125\n",
      "Average test loss: 0.0016896433331486252\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009370674003329542\n",
      "Average test loss: 0.0016922604807962974\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00935357065581613\n",
      "Average test loss: 0.0017312661233461566\n",
      "Epoch 32/300\n",
      "Average training loss: 0.009314987983554601\n",
      "Average test loss: 0.0017138413441263967\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009284012005147007\n",
      "Average test loss: 0.0016607205255164041\n",
      "Epoch 34/300\n",
      "Average training loss: 0.009254922837018967\n",
      "Average test loss: 0.001658751520121263\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009228326809075144\n",
      "Average test loss: 0.001661485242553883\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009202947726266252\n",
      "Average test loss: 0.0016326520476076338\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009181653838604689\n",
      "Average test loss: 0.001625586840013663\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009150287566913499\n",
      "Average test loss: 0.001647240878186292\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00913544889456696\n",
      "Average test loss: 0.0016247691663189066\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009107377195523845\n",
      "Average test loss: 0.0016150888479832147\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009097396371265252\n",
      "Average test loss: 0.0016209103299511805\n",
      "Epoch 42/300\n",
      "Average training loss: 0.009070422549628549\n",
      "Average test loss: 0.0016303289955895807\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009059739052007595\n",
      "Average test loss: 0.001709182120187001\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009024098537034458\n",
      "Average test loss: 0.0016196227223715849\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00901281672798925\n",
      "Average test loss: 0.001609154128159086\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009005904006047382\n",
      "Average test loss: 0.0016087654783493942\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00898424078606897\n",
      "Average test loss: 0.001602133940698372\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008963614222490125\n",
      "Average test loss: 0.001625203347661429\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008952726792130205\n",
      "Average test loss: 0.001591563204717305\n",
      "Epoch 50/300\n",
      "Average training loss: 0.008933496253357993\n",
      "Average test loss: 0.0016035671430743403\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00892095882197221\n",
      "Average test loss: 0.0015899829741360413\n",
      "Epoch 52/300\n",
      "Average training loss: 0.008900830010573069\n",
      "Average test loss: 0.0015872184557633267\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00889018247442113\n",
      "Average test loss: 0.0016060846781927264\n",
      "Epoch 54/300\n",
      "Average training loss: 0.008878566302773025\n",
      "Average test loss: 0.0015719329454004765\n",
      "Epoch 55/300\n",
      "Average training loss: 0.008863140468382174\n",
      "Average test loss: 0.0015884788058077295\n",
      "Epoch 56/300\n",
      "Average training loss: 0.008853072439630827\n",
      "Average test loss: 0.0015758952498435973\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00883728861891561\n",
      "Average test loss: 0.001578968175376455\n",
      "Epoch 58/300\n",
      "Average training loss: 0.008829740327679449\n",
      "Average test loss: 0.0015737102973378367\n",
      "Epoch 59/300\n",
      "Average training loss: 0.008822696111268468\n",
      "Average test loss: 0.0015711477245721553\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008809161751220623\n",
      "Average test loss: 0.0015640833941805694\n",
      "Epoch 61/300\n",
      "Average training loss: 0.008788880217820406\n",
      "Average test loss: 0.0015752413133676682\n",
      "Epoch 62/300\n",
      "Average training loss: 0.008787099120517571\n",
      "Average test loss: 0.0015880558369681238\n",
      "Epoch 63/300\n",
      "Average training loss: 0.008775575535992782\n",
      "Average test loss: 0.0015675800885591243\n",
      "Epoch 64/300\n",
      "Average training loss: 0.00875891820134388\n",
      "Average test loss: 0.0015650091560350523\n",
      "Epoch 65/300\n",
      "Average training loss: 0.008755802172753546\n",
      "Average test loss: 0.0015751336028592454\n",
      "Epoch 66/300\n",
      "Average training loss: 0.008739820505181949\n",
      "Average test loss: 0.0015586416091149051\n",
      "Epoch 67/300\n",
      "Average training loss: 0.008730412649611632\n",
      "Average test loss: 0.001554821449228459\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008722476457556088\n",
      "Average test loss: 0.0015649395777533452\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008707035007576148\n",
      "Average test loss: 0.0015636676895018253\n",
      "Epoch 70/300\n",
      "Average training loss: 0.00869942207593057\n",
      "Average test loss: 0.0015680092351718082\n",
      "Epoch 71/300\n",
      "Average training loss: 0.008690148769981332\n",
      "Average test loss: 0.0015616840899197592\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008690750749160847\n",
      "Average test loss: 0.0015530157938806546\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008672413713402218\n",
      "Average test loss: 0.0015453436747193336\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008660410874419742\n",
      "Average test loss: 0.0015518213272508647\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008653586248970694\n",
      "Average test loss: 0.0015456680665827459\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008648677735692925\n",
      "Average test loss: 0.00154711570052637\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008630976946817505\n",
      "Average test loss: 0.0015451226412422127\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008636723558108012\n",
      "Average test loss: 0.0017934785329529808\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008620271912051572\n",
      "Average test loss: 0.0015474961855345302\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008610951347069608\n",
      "Average test loss: 0.0015571987428185012\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008604220897787148\n",
      "Average test loss: 0.0015465908555520906\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008594934118704663\n",
      "Average test loss: 0.001545490934099588\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00858586819925242\n",
      "Average test loss: 0.0015400317340261407\n",
      "Epoch 84/300\n",
      "Average training loss: 0.00858387764212158\n",
      "Average test loss: 0.0015453264821941653\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008572382097442944\n",
      "Average test loss: 0.001546484992839396\n",
      "Epoch 86/300\n",
      "Average training loss: 0.008566610152522722\n",
      "Average test loss: 0.0015359849532445272\n",
      "Epoch 87/300\n",
      "Average training loss: 0.008560303917775552\n",
      "Average test loss: 0.0015666301225622495\n",
      "Epoch 88/300\n",
      "Average training loss: 0.008550485386202733\n",
      "Average test loss: 0.001583560961505605\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008546398888031641\n",
      "Average test loss: 0.0015353660153018104\n",
      "Epoch 90/300\n",
      "Average training loss: 0.008543440940479437\n",
      "Average test loss: 0.001552917020395398\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00853413228235311\n",
      "Average test loss: 0.0015571386542999083\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008524472446077399\n",
      "Average test loss: 0.0015501014810676377\n",
      "Epoch 93/300\n",
      "Average training loss: 0.008521995390454928\n",
      "Average test loss: 0.0015310483262356784\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008511661705043581\n",
      "Average test loss: 0.0015463201110251248\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008508122030231689\n",
      "Average test loss: 0.0015457882923591469\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008502103689644071\n",
      "Average test loss: 0.0015433227281189627\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008493785159455406\n",
      "Average test loss: 0.001544760423195031\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008490330999510156\n",
      "Average test loss: 0.0015293849546772738\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008477352405587833\n",
      "Average test loss: 0.0015335804021192922\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008476714596566227\n",
      "Average test loss: 0.001563058029446337\n",
      "Epoch 101/300\n",
      "Average training loss: 0.00847077304455969\n",
      "Average test loss: 0.0015401882028414143\n",
      "Epoch 102/300\n",
      "Average training loss: 0.008470810660471518\n",
      "Average test loss: 0.0015521879746682114\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008462147489190101\n",
      "Average test loss: 0.001552354319848948\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00845838104519579\n",
      "Average test loss: 0.0015297822098040747\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008449633055677017\n",
      "Average test loss: 0.0016839388240542676\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008439523183637195\n",
      "Average test loss: 0.0015358373414104183\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008446562401536439\n",
      "Average test loss: 0.0015395139880064461\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008426912066423231\n",
      "Average test loss: 0.0015222190741139153\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008419483965883653\n",
      "Average test loss: 0.0015375113459303975\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008417813415328662\n",
      "Average test loss: 0.0015362276506299774\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008422464749051464\n",
      "Average test loss: 0.001521359742929538\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008407656529711352\n",
      "Average test loss: 0.0015319199307511251\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008402563530537817\n",
      "Average test loss: 0.0015255439933389424\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00839743496518996\n",
      "Average test loss: 0.0015382694447827008\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008393529785176119\n",
      "Average test loss: 0.0015377552114013168\n",
      "Epoch 116/300\n",
      "Average training loss: 0.00839979505745901\n",
      "Average test loss: 0.0015534877826770147\n",
      "Epoch 117/300\n",
      "Average training loss: 0.008387582870821158\n",
      "Average test loss: 0.0015355929415673017\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008379610674248802\n",
      "Average test loss: 0.0015392633835888572\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008372365344729689\n",
      "Average test loss: 0.00153854477374504\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008369142401963472\n",
      "Average test loss: 0.0015347947319452133\n",
      "Epoch 121/300\n",
      "Average training loss: 0.008363002235276832\n",
      "Average test loss: 0.0015608229576092626\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008360776754717032\n",
      "Average test loss: 0.0015614277560056912\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008351386555367046\n",
      "Average test loss: 0.0015315085137262941\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008355095072752899\n",
      "Average test loss: 0.001531571000814438\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008349259225858583\n",
      "Average test loss: 0.0018013081334324348\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00833814481480254\n",
      "Average test loss: 0.0015345654154403341\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008341766134732299\n",
      "Average test loss: 0.0015368807448281183\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008331690015892188\n",
      "Average test loss: 0.0015746038899653488\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008330781371229225\n",
      "Average test loss: 0.0015291968984529375\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00832099981730183\n",
      "Average test loss: 0.001520854667863912\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008320672200371821\n",
      "Average test loss: 0.0015400527672221264\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008320096053597\n",
      "Average test loss: 0.0015206262723853191\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008310334715578292\n",
      "Average test loss: 0.0015524704929234254\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008308511128856076\n",
      "Average test loss: 0.0015259574964228604\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008301701712111632\n",
      "Average test loss: 0.0015460073732667498\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008300330062707265\n",
      "Average test loss: 0.0015301175232873194\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008292761868900723\n",
      "Average test loss: 0.0015293868264804283\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008293339925921626\n",
      "Average test loss: 0.001546256142978867\n",
      "Epoch 139/300\n",
      "Average training loss: 0.008291054049299824\n",
      "Average test loss: 0.0015288480288452573\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00828338994209965\n",
      "Average test loss: 0.0015500889321168265\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00827926887695988\n",
      "Average test loss: 0.0015449084405683808\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008275431825055017\n",
      "Average test loss: 0.0015460397025777234\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008276228667547306\n",
      "Average test loss: 0.0015498150316998363\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008264969411823484\n",
      "Average test loss: 0.0015462286880032884\n",
      "Epoch 145/300\n",
      "Average training loss: 0.008262413214892149\n",
      "Average test loss: 0.0015371537312037415\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008267408159457975\n",
      "Average test loss: 0.0015244840229344037\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008266932887335618\n",
      "Average test loss: 0.0015314859491255548\n",
      "Epoch 148/300\n",
      "Average training loss: 0.008249118472139041\n",
      "Average test loss: 0.001558293084355278\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008248989359786112\n",
      "Average test loss: 0.0015341865804253353\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008250107515189382\n",
      "Average test loss: 0.0015331670422520903\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008242187639077504\n",
      "Average test loss: 0.0015554518338499797\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008241918894979688\n",
      "Average test loss: 0.0015322095174342394\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008239824518147441\n",
      "Average test loss: 0.0015410006592671075\n",
      "Epoch 154/300\n",
      "Average training loss: 0.00823580287148555\n",
      "Average test loss: 0.0015238525071698758\n",
      "Epoch 155/300\n",
      "Average training loss: 0.00823911539465189\n",
      "Average test loss: 0.001543127320965545\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008231290804429187\n",
      "Average test loss: 0.001532831866811547\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008215974582980077\n",
      "Average test loss: 0.001528785976374315\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00821979928140839\n",
      "Average test loss: 0.0015540727391425105\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008222715586423873\n",
      "Average test loss: 0.0015281503759324551\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008216370742768049\n",
      "Average test loss: 0.0015624541427112287\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008204147898902496\n",
      "Average test loss: 0.0015276536779581672\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008204166013747453\n",
      "Average test loss: 0.0015390285997548038\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008200279338492288\n",
      "Average test loss: 0.0015414455726535784\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008206029021905528\n",
      "Average test loss: 0.0015234125782218244\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00819474584940407\n",
      "Average test loss: 0.0015458805944977535\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008198086759696405\n",
      "Average test loss: 0.001527874186117616\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008184310554216305\n",
      "Average test loss: 0.0015387941190145081\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008201391487485832\n",
      "Average test loss: 0.00153307243457271\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008185316131760677\n",
      "Average test loss: 0.0015486506493762135\n",
      "Epoch 170/300\n",
      "Average training loss: 0.008179256086134248\n",
      "Average test loss: 0.0015553119702057705\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008172142280472649\n",
      "Average test loss: 0.0015395038237070871\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008179317988869216\n",
      "Average test loss: 0.0015284720892086625\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008178520016786124\n",
      "Average test loss: 0.0015463464700927337\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008169993667138947\n",
      "Average test loss: 0.0015367920708118214\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008170356050133705\n",
      "Average test loss: 0.0015347885454280509\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008168080007450448\n",
      "Average test loss: 0.001538220061817103\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008160362743255165\n",
      "Average test loss: 0.0015394185228894154\n",
      "Epoch 178/300\n",
      "Average training loss: 0.008159412326084243\n",
      "Average test loss: 0.0015254208563516536\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0081583884043826\n",
      "Average test loss: 0.0015282650233970749\n",
      "Epoch 180/300\n",
      "Average training loss: 0.008154469615055455\n",
      "Average test loss: 0.001530648585771107\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008147575958321492\n",
      "Average test loss: 0.0015311860494936506\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008146740981274181\n",
      "Average test loss: 0.0015228419018288454\n",
      "Epoch 183/300\n",
      "Average training loss: 0.00814689505721132\n",
      "Average test loss: 0.0015286005751954184\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008138066459033224\n",
      "Average test loss: 0.0015223783377764954\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008132218267354701\n",
      "Average test loss: 0.0015433835550728772\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008140027644733589\n",
      "Average test loss: 0.0015231869891285895\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008142763148579332\n",
      "Average test loss: 0.001534291333415442\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008130288218458494\n",
      "Average test loss: 0.0015586694097146391\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00813260797949301\n",
      "Average test loss: 0.0016013374394840665\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008126784859846036\n",
      "Average test loss: 0.0015612557167187333\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008131501842290162\n",
      "Average test loss: 0.001525684046662516\n",
      "Epoch 192/300\n",
      "Average training loss: 0.00811490557094415\n",
      "Average test loss: 0.0015401014651482304\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00811646479450994\n",
      "Average test loss: 0.0015319288034612935\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008113427227983873\n",
      "Average test loss: 0.0015633574752137065\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008122120555904177\n",
      "Average test loss: 0.0015309919497619074\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008114592973970705\n",
      "Average test loss: 0.0015530052663137515\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008107894688016839\n",
      "Average test loss: 0.0015500520596073734\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008108983964555793\n",
      "Average test loss: 0.0015569892942698465\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008101527544773288\n",
      "Average test loss: 0.001549032313697454\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008103468127134774\n",
      "Average test loss: 0.0015358906941902307\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008095251932740211\n",
      "Average test loss: 0.0015246039292671615\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008104837179597882\n",
      "Average test loss: 0.0015336514034618935\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00809010812929935\n",
      "Average test loss: 0.0015384429898113012\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008086908461732998\n",
      "Average test loss: 0.0015346528455201123\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008092720871998204\n",
      "Average test loss: 0.0015330473891355925\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008088275335729122\n",
      "Average test loss: 0.0015665894249040219\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00808786427643564\n",
      "Average test loss: 0.0015439996755578452\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008079189398636421\n",
      "Average test loss: 0.001541992465241088\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008095197879605822\n",
      "Average test loss: 0.001670304333894617\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008074293545964692\n",
      "Average test loss: 0.0015339054986834525\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008076449778344897\n",
      "Average test loss: 0.001537677077576518\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00807200396930178\n",
      "Average test loss: 0.0015478870027388135\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00807231786598762\n",
      "Average test loss: 0.001564937828419109\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008066884210126267\n",
      "Average test loss: 0.0016088343251806994\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008065767034474347\n",
      "Average test loss: 0.0015414032553219134\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008061368987792068\n",
      "Average test loss: 0.0019061942920088769\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008063039970066813\n",
      "Average test loss: 0.0015461298043115271\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008058745954599645\n",
      "Average test loss: 0.001543621863755915\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008057820788274208\n",
      "Average test loss: 0.0015487714081795678\n",
      "Epoch 220/300\n",
      "Average training loss: 0.00805903231104215\n",
      "Average test loss: 0.0015492066458488503\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008056974860529104\n",
      "Average test loss: 0.0015362992509164743\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00805622286390927\n",
      "Average test loss: 0.001589230731026166\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008044777705437607\n",
      "Average test loss: 0.0015296389188410508\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008039936805350914\n",
      "Average test loss: 0.0015417335683272945\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008046998102631833\n",
      "Average test loss: 0.001541650019482606\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0080424097718464\n",
      "Average test loss: 0.001533051196485758\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008039431730906169\n",
      "Average test loss: 0.0015651119359665446\n",
      "Epoch 228/300\n",
      "Average training loss: 0.008037748947739601\n",
      "Average test loss: 0.0015312701530961527\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008037667339460717\n",
      "Average test loss: 0.001562491760071781\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008039956997252172\n",
      "Average test loss: 0.0015515519900040494\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008036303964340024\n",
      "Average test loss: 0.0015451214150008228\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00803109804002775\n",
      "Average test loss: 0.0015961212470299668\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008029090586221881\n",
      "Average test loss: 0.0015422082739985651\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008019280129422744\n",
      "Average test loss: 0.0015392751075430876\n",
      "Epoch 235/300\n",
      "Average training loss: 0.008028162041058143\n",
      "Average test loss: 0.0015544424838282995\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008026152715500858\n",
      "Average test loss: 0.0015475664414051507\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008021250705752108\n",
      "Average test loss: 0.0015764687874664863\n",
      "Epoch 238/300\n",
      "Average training loss: 0.008021038194911347\n",
      "Average test loss: 0.0015357100597272317\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008024988450109958\n",
      "Average test loss: 0.001564812211940686\n",
      "Epoch 240/300\n",
      "Average training loss: 0.00802935452800658\n",
      "Average test loss: 0.0015448986070437565\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008013437229726049\n",
      "Average test loss: 0.0015488812514684266\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008014230612251493\n",
      "Average test loss: 0.001540249965671036\n",
      "Epoch 243/300\n",
      "Average training loss: 0.008008551754885249\n",
      "Average test loss: 0.0015317588191893366\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008006483950548702\n",
      "Average test loss: 0.0015407852787627941\n",
      "Epoch 245/300\n",
      "Average training loss: 0.00800851941936546\n",
      "Average test loss: 0.0015555584983279309\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00800796078476641\n",
      "Average test loss: 0.001537489872943196\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008009534914460447\n",
      "Average test loss: 0.0015412112846453156\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008009847661273347\n",
      "Average test loss: 0.0016499792987273798\n",
      "Epoch 249/300\n",
      "Average training loss: 0.00799860315852695\n",
      "Average test loss: 0.0015470289315303995\n",
      "Epoch 250/300\n",
      "Average training loss: 0.007986886324154006\n",
      "Average test loss: 0.0015522182962029345\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007999703952007824\n",
      "Average test loss: 0.0015520562747907308\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008000386907822557\n",
      "Average test loss: 0.0015508894460896652\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007995975010097026\n",
      "Average test loss: 0.0015524421686099635\n",
      "Epoch 254/300\n",
      "Average training loss: 0.007993322791738642\n",
      "Average test loss: 0.0015760063517631757\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00799254841523038\n",
      "Average test loss: 0.0015418313058714072\n",
      "Epoch 256/300\n",
      "Average training loss: 0.007986042729682393\n",
      "Average test loss: 0.0015490191416918403\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007984467516342799\n",
      "Average test loss: 0.0015412794335020912\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007988326357056697\n",
      "Average test loss: 0.0015381868295371531\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00798935356694791\n",
      "Average test loss: 0.0015495983656599289\n",
      "Epoch 260/300\n",
      "Average training loss: 0.007978007686220936\n",
      "Average test loss: 0.0015974924716477593\n",
      "Epoch 261/300\n",
      "Average training loss: 0.007976410436547465\n",
      "Average test loss: 0.0015434601366416448\n",
      "Epoch 262/300\n",
      "Average training loss: 0.007980390482892593\n",
      "Average test loss: 0.0015562975654999415\n",
      "Epoch 263/300\n",
      "Average training loss: 0.007973548492209779\n",
      "Average test loss: 0.0015461940744684803\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00796831775829196\n",
      "Average test loss: 0.0015345323714945052\n",
      "Epoch 265/300\n",
      "Average training loss: 0.007972295042955213\n",
      "Average test loss: 0.0015715428778073854\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00796822424274352\n",
      "Average test loss: 0.0015430383095517755\n",
      "Epoch 267/300\n",
      "Average training loss: 0.007974703341308568\n",
      "Average test loss: 0.0015343326644765005\n",
      "Epoch 268/300\n",
      "Average training loss: 0.007967856823363238\n",
      "Average test loss: 0.0015821163672953844\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007967832128206889\n",
      "Average test loss: 0.0015554106330706013\n",
      "Epoch 270/300\n",
      "Average training loss: 0.007965414318773482\n",
      "Average test loss: 0.0015350493264074127\n",
      "Epoch 271/300\n",
      "Average training loss: 0.007962069959276253\n",
      "Average test loss: 0.0015769636018408669\n",
      "Epoch 272/300\n",
      "Average training loss: 0.007965951391806204\n",
      "Average test loss: 0.0015448490380206043\n",
      "Epoch 273/300\n",
      "Average training loss: 0.007960165355768468\n",
      "Average test loss: 0.0015707338337476054\n",
      "Epoch 274/300\n",
      "Average training loss: 0.007961277343746688\n",
      "Average test loss: 0.0015423151538189914\n",
      "Epoch 275/300\n",
      "Average training loss: 0.007955112697763575\n",
      "Average test loss: 0.001564422111854785\n",
      "Epoch 276/300\n",
      "Average training loss: 0.007954509899020195\n",
      "Average test loss: 0.0015460579964435763\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007951848485403592\n",
      "Average test loss: 0.001539961360808876\n",
      "Epoch 278/300\n",
      "Average training loss: 0.007953659494303995\n",
      "Average test loss: 0.0015520626749429439\n",
      "Epoch 279/300\n",
      "Average training loss: 0.00794755259445972\n",
      "Average test loss: 0.001549927379237488\n",
      "Epoch 280/300\n",
      "Average training loss: 0.007952428296622302\n",
      "Average test loss: 0.0015428102195469869\n",
      "Epoch 281/300\n",
      "Average training loss: 0.007948869624899494\n",
      "Average test loss: 0.0015434443491200607\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00794426391315129\n",
      "Average test loss: 0.0015721363216224644\n",
      "Epoch 283/300\n",
      "Average training loss: 0.007951206043362618\n",
      "Average test loss: 0.00154052788629714\n",
      "Epoch 284/300\n",
      "Average training loss: 0.007944702837616205\n",
      "Average test loss: 0.0015452695816962256\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00793524849994315\n",
      "Average test loss: 0.0015480201224692995\n",
      "Epoch 286/300\n",
      "Average training loss: 0.00793601084045238\n",
      "Average test loss: 0.001548925184386058\n",
      "Epoch 287/300\n",
      "Average training loss: 0.007941160721497403\n",
      "Average test loss: 0.001579056604247954\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007940893933176995\n",
      "Average test loss: 0.0015646446947422293\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007934759168161287\n",
      "Average test loss: 0.0015615556016046967\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007936147758116324\n",
      "Average test loss: 0.0015420090939021773\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007937176773117649\n",
      "Average test loss: 0.0015936760349820058\n",
      "Epoch 292/300\n",
      "Average training loss: 0.007935685826258526\n",
      "Average test loss: 0.0015672157891612087\n",
      "Epoch 293/300\n",
      "Average training loss: 0.007930257417675522\n",
      "Average test loss: 0.0015597284523149331\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007929233765850464\n",
      "Average test loss: 0.00158563510959761\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007926991545491749\n",
      "Average test loss: 0.0015597657368828853\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007931905226161082\n",
      "Average test loss: 0.0015374863668241436\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007925191390017669\n",
      "Average test loss: 0.0015440003617356222\n",
      "Epoch 298/300\n",
      "Average training loss: 0.007924482367518875\n",
      "Average test loss: 0.0015559983280383877\n",
      "Epoch 299/300\n",
      "Average training loss: 0.007922844618558883\n",
      "Average test loss: 0.001572713162439565\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007922152274184757\n",
      "Average test loss: 0.001556173370530208\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_50_Depth3/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.72\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.10\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 32.00\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.06\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7455143596728643\n",
      "Average test loss: 0.005281473765770595\n",
      "Epoch 2/300\n",
      "Average training loss: 0.10815578252077103\n",
      "Average test loss: 0.00505375278658337\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08190644828478495\n",
      "Average test loss: 0.004712681906090842\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07529504154788123\n",
      "Average test loss: 0.004587554984622532\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07208764573600557\n",
      "Average test loss: 0.0045071326593558\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07005264282226563\n",
      "Average test loss: 0.004493286086039411\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06860190012057622\n",
      "Average test loss: 0.0043944606714778475\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0674762673874696\n",
      "Average test loss: 0.00438654429341356\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06653287304772271\n",
      "Average test loss: 0.004425410505798128\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0657874796324306\n",
      "Average test loss: 0.00429542232139243\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06520090101162593\n",
      "Average test loss: 0.00428992792011963\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06469027052323023\n",
      "Average test loss: 0.004271621783988344\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06422184926933712\n",
      "Average test loss: 0.0042391695645120405\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06385254099965096\n",
      "Average test loss: 0.004261672714932097\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06349429527587361\n",
      "Average test loss: 0.00422044226879047\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06322623367110888\n",
      "Average test loss: 0.004191758211288187\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06290962091419432\n",
      "Average test loss: 0.004191608341824677\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06264595505926344\n",
      "Average test loss: 0.004219615641774403\n",
      "Epoch 19/300\n",
      "Average training loss: 0.062396608382463456\n",
      "Average test loss: 0.004161217889438073\n",
      "Epoch 20/300\n",
      "Average training loss: 0.062158946557177436\n",
      "Average test loss: 0.004139988499383132\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06205050067106883\n",
      "Average test loss: 0.004134995518045293\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06182904554406802\n",
      "Average test loss: 0.004213228978216648\n",
      "Epoch 23/300\n",
      "Average training loss: 0.061657693376143775\n",
      "Average test loss: 0.004119522006561359\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06142435320880678\n",
      "Average test loss: 0.00409544268829955\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06127504405710432\n",
      "Average test loss: 0.0041292788241472506\n",
      "Epoch 26/300\n",
      "Average training loss: 0.061134654011991287\n",
      "Average test loss: 0.0040887722029454175\n",
      "Epoch 27/300\n",
      "Average training loss: 0.060972083654668593\n",
      "Average test loss: 0.004073653494318327\n",
      "Epoch 28/300\n",
      "Average training loss: 0.060812087681558394\n",
      "Average test loss: 0.00406697594291634\n",
      "Epoch 29/300\n",
      "Average training loss: 0.060670919405089486\n",
      "Average test loss: 0.004055891812468569\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06052741647760073\n",
      "Average test loss: 0.004058934132051137\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06041293778684404\n",
      "Average test loss: 0.0040679760771907035\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06029663239916166\n",
      "Average test loss: 0.0040404841668075986\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06018804786602656\n",
      "Average test loss: 0.004031390681241949\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06010195707612567\n",
      "Average test loss: 0.004081578551481168\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05997306260135439\n",
      "Average test loss: 0.004010632096479337\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05991681103242768\n",
      "Average test loss: 0.004004842666702138\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05978688015871578\n",
      "Average test loss: 0.004008071059981982\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05971977397137218\n",
      "Average test loss: 0.003996641800014509\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05962936011950175\n",
      "Average test loss: 0.003998194340823425\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05958859468499819\n",
      "Average test loss: 0.004000759113579989\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05947180086043146\n",
      "Average test loss: 0.003997100716663732\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05942544487449858\n",
      "Average test loss: 0.003995222210263213\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05936047938797209\n",
      "Average test loss: 0.003985746188296212\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05928517130348417\n",
      "Average test loss: 0.0039735218464500375\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05923358334766494\n",
      "Average test loss: 0.003984177862190538\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05918966605265935\n",
      "Average test loss: 0.003992335649621155\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05911090235246552\n",
      "Average test loss: 0.003974100726553136\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05906653829084502\n",
      "Average test loss: 0.003965121956749095\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05898034218947093\n",
      "Average test loss: 0.003965674110170868\n",
      "Epoch 50/300\n",
      "Average training loss: 0.058937852640946704\n",
      "Average test loss: 0.0039736300203949216\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0588986760692464\n",
      "Average test loss: 0.00395872190905114\n",
      "Epoch 52/300\n",
      "Average training loss: 0.058817242867416804\n",
      "Average test loss: 0.003970914080739021\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05880537215537495\n",
      "Average test loss: 0.0039516763931347265\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05873911831114027\n",
      "Average test loss: 0.003973304222648342\n",
      "Epoch 55/300\n",
      "Average training loss: 0.058690005991193984\n",
      "Average test loss: 0.003960662374272943\n",
      "Epoch 56/300\n",
      "Average training loss: 0.058645441042052375\n",
      "Average test loss: 0.00393790371177925\n",
      "Epoch 57/300\n",
      "Average training loss: 0.058595183312892915\n",
      "Average test loss: 0.003950601299810741\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05856920241647297\n",
      "Average test loss: 0.003950993352466159\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05849791218174828\n",
      "Average test loss: 0.0039589360803365704\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05846073394351536\n",
      "Average test loss: 0.003954704432230857\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05846271544032627\n",
      "Average test loss: 0.003950557669831647\n",
      "Epoch 62/300\n",
      "Average training loss: 0.058379724833700394\n",
      "Average test loss: 0.003952359768458539\n",
      "Epoch 63/300\n",
      "Average training loss: 0.058321891089280445\n",
      "Average test loss: 0.00393954114201996\n",
      "Epoch 64/300\n",
      "Average training loss: 0.058359025140603385\n",
      "Average test loss: 0.003931679775110549\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05825146643155151\n",
      "Average test loss: 0.003983750865691238\n",
      "Epoch 66/300\n",
      "Average training loss: 0.058239138414462405\n",
      "Average test loss: 0.003946523139046298\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05819956194692188\n",
      "Average test loss: 0.003937587412281169\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05813057275944286\n",
      "Average test loss: 0.003948538189339969\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05809859673844443\n",
      "Average test loss: 0.003928187131053871\n",
      "Epoch 70/300\n",
      "Average training loss: 0.058080542067686716\n",
      "Average test loss: 0.003930259302258492\n",
      "Epoch 71/300\n",
      "Average training loss: 0.058064054936170575\n",
      "Average test loss: 0.003931886822813087\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05799769124057558\n",
      "Average test loss: 0.003943105795317226\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05797487699985504\n",
      "Average test loss: 0.003938250340314375\n",
      "Epoch 74/300\n",
      "Average training loss: 0.057909576935900584\n",
      "Average test loss: 0.003943530332297086\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05788194295101696\n",
      "Average test loss: 0.003940811324864626\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05788097233242459\n",
      "Average test loss: 0.003936012507312828\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05780971851282649\n",
      "Average test loss: 0.003950815241783858\n",
      "Epoch 78/300\n",
      "Average training loss: 0.057756275273031656\n",
      "Average test loss: 0.003932987727017866\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05775773402386242\n",
      "Average test loss: 0.003931215772198306\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05774007621076372\n",
      "Average test loss: 0.0039300940280987155\n",
      "Epoch 81/300\n",
      "Average training loss: 0.057649527268277274\n",
      "Average test loss: 0.003924790714763933\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05765865418314934\n",
      "Average test loss: 0.003930518212210801\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05761569434404373\n",
      "Average test loss: 0.0039372283019539385\n",
      "Epoch 84/300\n",
      "Average training loss: 0.057560864362451766\n",
      "Average test loss: 0.004015609462228086\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05751488552159733\n",
      "Average test loss: 0.003937461455249124\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05752311519119475\n",
      "Average test loss: 0.003921850842941138\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05742207404308849\n",
      "Average test loss: 0.004092819711814324\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0574004099700186\n",
      "Average test loss: 0.003952981987554166\n",
      "Epoch 89/300\n",
      "Average training loss: 0.057387168827984066\n",
      "Average test loss: 0.0039434725950575535\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05732827193538348\n",
      "Average test loss: 0.003941135225610601\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05731479881207148\n",
      "Average test loss: 0.003931120177523957\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05726313089993265\n",
      "Average test loss: 0.003927456260141399\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05721965138448609\n",
      "Average test loss: 0.003933634288609028\n",
      "Epoch 94/300\n",
      "Average training loss: 0.057189198318454953\n",
      "Average test loss: 0.003921836266915004\n",
      "Epoch 95/300\n",
      "Average training loss: 0.057143127199676305\n",
      "Average test loss: 0.003923167411237955\n",
      "Epoch 96/300\n",
      "Average training loss: 0.057136839164627924\n",
      "Average test loss: 0.003917360075645976\n",
      "Epoch 97/300\n",
      "Average training loss: 0.057092232823371884\n",
      "Average test loss: 0.003943564371930228\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05707824960185422\n",
      "Average test loss: 0.003943399580402507\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05700694183839692\n",
      "Average test loss: 0.003931848634862237\n",
      "Epoch 100/300\n",
      "Average training loss: 0.056981401917007235\n",
      "Average test loss: 0.003922175566148427\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05699015377296342\n",
      "Average test loss: 0.003949257338626517\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05692930741442574\n",
      "Average test loss: 0.0039309785382615195\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05688101163506508\n",
      "Average test loss: 0.003935197789222002\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05686376600464185\n",
      "Average test loss: 0.0039645895771682265\n",
      "Epoch 105/300\n",
      "Average training loss: 0.056808673368559943\n",
      "Average test loss: 0.003941004211290015\n",
      "Epoch 106/300\n",
      "Average training loss: 0.056768040428558986\n",
      "Average test loss: 0.003925018713499109\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05673684777153863\n",
      "Average test loss: 0.003954648167101873\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05669525535239114\n",
      "Average test loss: 0.0039472730762014786\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05668449838956197\n",
      "Average test loss: 0.003941216100835138\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05663625191648801\n",
      "Average test loss: 0.003925297428129448\n",
      "Epoch 111/300\n",
      "Average training loss: 0.056578386280271745\n",
      "Average test loss: 0.003958302244130108\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05656095073620478\n",
      "Average test loss: 0.0039407363933407595\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05650187843044599\n",
      "Average test loss: 0.003923661448475387\n",
      "Epoch 114/300\n",
      "Average training loss: 0.056490554134051005\n",
      "Average test loss: 0.003928918122417397\n",
      "Epoch 115/300\n",
      "Average training loss: 0.056449121014939416\n",
      "Average test loss: 0.003935153952282336\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05642642754978604\n",
      "Average test loss: 0.003987665031312241\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05636270915468534\n",
      "Average test loss: 0.003921066820621491\n",
      "Epoch 118/300\n",
      "Average training loss: 0.056333215597603056\n",
      "Average test loss: 0.003925102177593443\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05631960420807203\n",
      "Average test loss: 0.003989308828901914\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05623271674911181\n",
      "Average test loss: 0.003966177464773258\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05619906080100272\n",
      "Average test loss: 0.003959758208857642\n",
      "Epoch 122/300\n",
      "Average training loss: 0.056221419940392175\n",
      "Average test loss: 0.003964378582106696\n",
      "Epoch 123/300\n",
      "Average training loss: 0.056177504728237786\n",
      "Average test loss: 0.003948940079659223\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05612700753741794\n",
      "Average test loss: 0.003972308629088932\n",
      "Epoch 125/300\n",
      "Average training loss: 0.056073495553599466\n",
      "Average test loss: 0.003928865204254786\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05605834382441309\n",
      "Average test loss: 0.003964217419425646\n",
      "Epoch 127/300\n",
      "Average training loss: 0.056029076122575336\n",
      "Average test loss: 0.003969185088657671\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05596499262915717\n",
      "Average test loss: 0.003953126539786657\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05593268016311857\n",
      "Average test loss: 0.00396819115926822\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05585130468673176\n",
      "Average test loss: 0.003968329468948974\n",
      "Epoch 131/300\n",
      "Average training loss: 0.055865062322881485\n",
      "Average test loss: 0.003951530509524875\n",
      "Epoch 132/300\n",
      "Average training loss: 0.055818136009905074\n",
      "Average test loss: 0.003959264118224382\n",
      "Epoch 133/300\n",
      "Average training loss: 0.055751124418444105\n",
      "Average test loss: 0.003974426397846805\n",
      "Epoch 134/300\n",
      "Average training loss: 0.055741041428512994\n",
      "Average test loss: 0.003959681509269609\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05569705011447271\n",
      "Average test loss: 0.003984376974817779\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05567285925812191\n",
      "Average test loss: 0.004000728538880746\n",
      "Epoch 137/300\n",
      "Average training loss: 0.055673374076684316\n",
      "Average test loss: 0.004007673829380009\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05554968369007111\n",
      "Average test loss: 0.003983069009251065\n",
      "Epoch 139/300\n",
      "Average training loss: 0.055562374863359666\n",
      "Average test loss: 0.0039667514281140435\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05553634840250015\n",
      "Average test loss: 0.003990239288243982\n",
      "Epoch 141/300\n",
      "Average training loss: 0.055498511321014826\n",
      "Average test loss: 0.003954203803299202\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05542943823006418\n",
      "Average test loss: 0.003982598361455732\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0554746068848504\n",
      "Average test loss: 0.004037235594871971\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05537096526887682\n",
      "Average test loss: 0.003958621940885981\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05533934297826555\n",
      "Average test loss: 0.00398656869182984\n",
      "Epoch 146/300\n",
      "Average training loss: 0.055311790933211646\n",
      "Average test loss: 0.003971857249322865\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05522633184327019\n",
      "Average test loss: 0.003963726039148039\n",
      "Epoch 148/300\n",
      "Average training loss: 0.055217395342058606\n",
      "Average test loss: 0.0039897134813169635\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05520322113898065\n",
      "Average test loss: 0.0039710477694041195\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05517431363132265\n",
      "Average test loss: 0.004019486928979556\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05513716077473429\n",
      "Average test loss: 0.0039965925069732795\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0550717904733287\n",
      "Average test loss: 0.004004622737152709\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05506053342752987\n",
      "Average test loss: 0.00406105480177535\n",
      "Epoch 154/300\n",
      "Average training loss: 0.054959693719943366\n",
      "Average test loss: 0.004001114738070303\n",
      "Epoch 155/300\n",
      "Average training loss: 0.054957152333524495\n",
      "Average test loss: 0.00396709182113409\n",
      "Epoch 156/300\n",
      "Average training loss: 0.054944761474927266\n",
      "Average test loss: 0.004059560349004137\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05491478677921825\n",
      "Average test loss: 0.004055689710709784\n",
      "Epoch 158/300\n",
      "Average training loss: 0.054894032501512105\n",
      "Average test loss: 0.004036626115234362\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05487038013339043\n",
      "Average test loss: 0.0040211265263044174\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05481974188155598\n",
      "Average test loss: 0.004051389524920119\n",
      "Epoch 161/300\n",
      "Average training loss: 0.054789147949881024\n",
      "Average test loss: 0.004037077370617125\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05472517608271705\n",
      "Average test loss: 0.004031693949674567\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05471043387055397\n",
      "Average test loss: 0.004016397489545246\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05467043070991834\n",
      "Average test loss: 0.004021269673688544\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05465725954704814\n",
      "Average test loss: 0.004046024925178951\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0545771716899342\n",
      "Average test loss: 0.0039831279230614506\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05455138514770402\n",
      "Average test loss: 0.006913761489921146\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05455080039964782\n",
      "Average test loss: 0.004371018803988894\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05449934292170736\n",
      "Average test loss: 0.004022485978073544\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05444781951440705\n",
      "Average test loss: 0.004025472971300284\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05453377372026443\n",
      "Average test loss: 0.004010869549380409\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05437964239385393\n",
      "Average test loss: 0.004001592089318566\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05431756090786722\n",
      "Average test loss: 0.004028280599249734\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05432879305879275\n",
      "Average test loss: 0.004055425318164958\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05429177168508371\n",
      "Average test loss: 0.004044982887390587\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05428657219807307\n",
      "Average test loss: 0.004116199327425824\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05421542772319582\n",
      "Average test loss: 0.0040563800773686836\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05415701436665323\n",
      "Average test loss: 0.004036480090684361\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05413228266437849\n",
      "Average test loss: 0.004022427070885896\n",
      "Epoch 180/300\n",
      "Average training loss: 0.054184839149316155\n",
      "Average test loss: 0.004041634099557996\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05415608598126306\n",
      "Average test loss: 0.004066833389716016\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05405218413803312\n",
      "Average test loss: 0.00402540336901115\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05404273134801123\n",
      "Average test loss: 0.0040756663353078895\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05401893804139561\n",
      "Average test loss: 0.0040750777812467685\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05403014588024881\n",
      "Average test loss: 0.00425358996188475\n",
      "Epoch 186/300\n",
      "Average training loss: 0.053945454203420215\n",
      "Average test loss: 0.004043704704278045\n",
      "Epoch 187/300\n",
      "Average training loss: 0.053941655976904763\n",
      "Average test loss: 0.0040186228605194226\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05391129676832093\n",
      "Average test loss: 0.004004940091321866\n",
      "Epoch 189/300\n",
      "Average training loss: 0.053830556088023715\n",
      "Average test loss: 0.004041613282221887\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05383236754271719\n",
      "Average test loss: 0.004030669357213709\n",
      "Epoch 191/300\n",
      "Average training loss: 0.053801111618677774\n",
      "Average test loss: 0.004043953912953536\n",
      "Epoch 192/300\n",
      "Average training loss: 0.053797745505968726\n",
      "Average test loss: 0.004035529553683268\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0537612171139982\n",
      "Average test loss: 0.0040573987466179665\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05370359440644582\n",
      "Average test loss: 0.004052660730150011\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05366161386171977\n",
      "Average test loss: 0.0040463938454373015\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05375013797647423\n",
      "Average test loss: 0.004052878181553549\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05362774057189623\n",
      "Average test loss: 0.004074443844871389\n",
      "Epoch 198/300\n",
      "Average training loss: 0.053620380093653996\n",
      "Average test loss: 0.004112078424543143\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0535637517273426\n",
      "Average test loss: 0.004012754638989767\n",
      "Epoch 200/300\n",
      "Average training loss: 0.053533172408739726\n",
      "Average test loss: 0.004057488257272376\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0535436156872246\n",
      "Average test loss: 0.004086283797191249\n",
      "Epoch 202/300\n",
      "Average training loss: 0.053510170343849396\n",
      "Average test loss: 0.004060461098949114\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0534591246313519\n",
      "Average test loss: 0.004062744524329901\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0534352335135142\n",
      "Average test loss: 0.004128059532493353\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05338314556413227\n",
      "Average test loss: 0.004085694282833073\n",
      "Epoch 206/300\n",
      "Average training loss: 0.053410035517480636\n",
      "Average test loss: 0.004098599710812171\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05334334068497022\n",
      "Average test loss: 0.004139350446768933\n",
      "Epoch 208/300\n",
      "Average training loss: 0.053370165172550416\n",
      "Average test loss: 0.004088001741303338\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05329653917749723\n",
      "Average test loss: 0.0040651748706069255\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0533278863661819\n",
      "Average test loss: 0.004032528820965025\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05328685304853651\n",
      "Average test loss: 0.004070891465991736\n",
      "Epoch 212/300\n",
      "Average training loss: 0.053256207933028536\n",
      "Average test loss: 0.004066397189680073\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05320664570397801\n",
      "Average test loss: 0.004132559486975273\n",
      "Epoch 214/300\n",
      "Average training loss: 0.053211355490816965\n",
      "Average test loss: 0.004077512813938988\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05315387567215495\n",
      "Average test loss: 0.0041179100771745044\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05312177288532257\n",
      "Average test loss: 0.00407551959860656\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05307359778549936\n",
      "Average test loss: 0.004050647570027245\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05304836509956254\n",
      "Average test loss: 0.004107530746608972\n",
      "Epoch 219/300\n",
      "Average training loss: 0.053067212058438194\n",
      "Average test loss: 0.004051449106385311\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05299491083621979\n",
      "Average test loss: 0.004121237163742383\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05299342887269126\n",
      "Average test loss: 0.0040621992372390295\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05299467956357532\n",
      "Average test loss: 0.0040525955913795365\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05301541982425584\n",
      "Average test loss: 0.004183398673931757\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05292359863387214\n",
      "Average test loss: 0.004067365697895487\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0528286911547184\n",
      "Average test loss: 0.004058250297274854\n",
      "Epoch 226/300\n",
      "Average training loss: 0.052874371293518276\n",
      "Average test loss: 0.004076181252383524\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05289459291100502\n",
      "Average test loss: 0.004103587848444779\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0529047210348977\n",
      "Average test loss: 0.004116643120431238\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05282668779624833\n",
      "Average test loss: 0.00412207377081116\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05280484225021468\n",
      "Average test loss: 0.004104865290224552\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05280890112453037\n",
      "Average test loss: 0.004087038416001532\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05268726032310062\n",
      "Average test loss: 0.004273675636284881\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05275072354740567\n",
      "Average test loss: 0.00407839662167761\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05269294313589732\n",
      "Average test loss: 0.004036113081499934\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05268831230368879\n",
      "Average test loss: 0.00411842750168095\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05264219927456644\n",
      "Average test loss: 0.004086927282727427\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05264347837368647\n",
      "Average test loss: 0.004115911677479744\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05261109227604336\n",
      "Average test loss: 0.004089430979970428\n",
      "Epoch 239/300\n",
      "Average training loss: 0.052564133299721615\n",
      "Average test loss: 0.004126982747680611\n",
      "Epoch 240/300\n",
      "Average training loss: 0.052578245566950906\n",
      "Average test loss: 0.004128944198704428\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05261116180486149\n",
      "Average test loss: 0.0041356736603710385\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05255402360028691\n",
      "Average test loss: 0.004157118989361657\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05249344849917624\n",
      "Average test loss: 0.004115896786666579\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05249816182255745\n",
      "Average test loss: 0.0042529092956748275\n",
      "Epoch 245/300\n",
      "Average training loss: 0.052414570758740106\n",
      "Average test loss: 0.00413540655374527\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05240519056717555\n",
      "Average test loss: 0.0042414548883421554\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05250341554813915\n",
      "Average test loss: 0.0041494932462357815\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0523894189827972\n",
      "Average test loss: 0.004155509878363875\n",
      "Epoch 249/300\n",
      "Average training loss: 0.052377980020311146\n",
      "Average test loss: 0.004199310898987783\n",
      "Epoch 250/300\n",
      "Average training loss: 0.052372911940018335\n",
      "Average test loss: 0.00409566151847442\n",
      "Epoch 251/300\n",
      "Average training loss: 0.052363232129149964\n",
      "Average test loss: 0.00406017311455475\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05232564252614975\n",
      "Average test loss: 0.004151218390713135\n",
      "Epoch 253/300\n",
      "Average training loss: 0.052319938371578854\n",
      "Average test loss: 0.004167268755949206\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05230060510171784\n",
      "Average test loss: 0.0050000496659841805\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05227500256896019\n",
      "Average test loss: 0.004100818265022503\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05226035070750448\n",
      "Average test loss: 0.004179535744090875\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05221586336361037\n",
      "Average test loss: 0.004157502452118529\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05220879786213239\n",
      "Average test loss: 0.004199681332541836\n",
      "Epoch 259/300\n",
      "Average training loss: 0.052144619348976344\n",
      "Average test loss: 0.004361622489160962\n",
      "Epoch 260/300\n",
      "Average training loss: 0.052176677819755345\n",
      "Average test loss: 0.004182660892191861\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05213203293747372\n",
      "Average test loss: 0.004133489584550261\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05210657623741362\n",
      "Average test loss: 0.004084887924500637\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05212255230545998\n",
      "Average test loss: 0.004129961657855246\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05210669568843312\n",
      "Average test loss: 0.004186987469593684\n",
      "Epoch 265/300\n",
      "Average training loss: 0.052117044727007546\n",
      "Average test loss: 0.004192765319926871\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05200307960311572\n",
      "Average test loss: 0.0041830342482361525\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05204536215133137\n",
      "Average test loss: 0.004062269738564888\n",
      "Epoch 268/300\n",
      "Average training loss: 0.051973077764113744\n",
      "Average test loss: 0.004158891085535288\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0520517547097471\n",
      "Average test loss: 0.004236526822464334\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05196642322010464\n",
      "Average test loss: 0.004201551834742228\n",
      "Epoch 271/300\n",
      "Average training loss: 0.051936559243334665\n",
      "Average test loss: 0.004116515779246887\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0519598846965366\n",
      "Average test loss: 0.004193323326400585\n",
      "Epoch 273/300\n",
      "Average training loss: 0.051943600363201566\n",
      "Average test loss: 0.004214974908779065\n",
      "Epoch 274/300\n",
      "Average training loss: 0.051921402172909845\n",
      "Average test loss: 0.004104436717927456\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05186661592457029\n",
      "Average test loss: 0.004198368994519115\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05194550443026755\n",
      "Average test loss: 0.004145853022527363\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05181573129031393\n",
      "Average test loss: 0.00411199851334095\n",
      "Epoch 278/300\n",
      "Average training loss: 0.051843181282281874\n",
      "Average test loss: 0.0041561088379886415\n",
      "Epoch 279/300\n",
      "Average training loss: 0.051789381765657\n",
      "Average test loss: 0.004134052185134756\n",
      "Epoch 280/300\n",
      "Average training loss: 0.051826847390996085\n",
      "Average test loss: 0.00415918213625749\n",
      "Epoch 281/300\n",
      "Average training loss: 0.051817014548513625\n",
      "Average test loss: 0.00419204011083477\n",
      "Epoch 282/300\n",
      "Average training loss: 0.051801996211210885\n",
      "Average test loss: 0.004159965786668989\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05173779738611645\n",
      "Average test loss: 0.004268168601310915\n",
      "Epoch 284/300\n",
      "Average training loss: 0.051718495809369616\n",
      "Average test loss: 0.004158756741633018\n",
      "Epoch 285/300\n",
      "Average training loss: 0.051755192346043054\n",
      "Average test loss: 0.004148637415841222\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05174919198618995\n",
      "Average test loss: 0.00414063833364182\n",
      "Epoch 287/300\n",
      "Average training loss: 0.051676226688755884\n",
      "Average test loss: 0.004170217950517933\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0516784743865331\n",
      "Average test loss: 0.004122041097531716\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05163781279325485\n",
      "Average test loss: 0.004147850940624873\n",
      "Epoch 290/300\n",
      "Average training loss: 0.051637417236963905\n",
      "Average test loss: 0.004077518487970035\n",
      "Epoch 291/300\n",
      "Average training loss: 0.051640105886591806\n",
      "Average test loss: 0.004143921922892332\n",
      "Epoch 292/300\n",
      "Average training loss: 0.051635958694749406\n",
      "Average test loss: 0.0041642731548183496\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05165490322311719\n",
      "Average test loss: 0.004243631148710847\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05160025155875418\n",
      "Average test loss: 0.004187029064115551\n",
      "Epoch 295/300\n",
      "Average training loss: 0.051554050789939035\n",
      "Average test loss: 0.004140998469872607\n",
      "Epoch 296/300\n",
      "Average training loss: 0.051533104022343956\n",
      "Average test loss: 0.004131617387963666\n",
      "Epoch 297/300\n",
      "Average training loss: 0.051532011952665116\n",
      "Average test loss: 0.004284355499160787\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05156767421298557\n",
      "Average test loss: 0.004216888015054994\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05149148362874985\n",
      "Average test loss: 0.004232942608909475\n",
      "Epoch 300/300\n",
      "Average training loss: 0.051509181472990245\n",
      "Average test loss: 0.004186779509608944\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7618208205832375\n",
      "Average test loss: 0.005423649564799335\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09809555539157655\n",
      "Average test loss: 0.0046134050521585675\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07790353243218529\n",
      "Average test loss: 0.004421828150335285\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07055892435709635\n",
      "Average test loss: 0.004197924687216679\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06630915694766575\n",
      "Average test loss: 0.004095104668082462\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06349639444881015\n",
      "Average test loss: 0.004017474364075396\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06140236866474152\n",
      "Average test loss: 0.004007870700210333\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05979863446619776\n",
      "Average test loss: 0.003802982741139001\n",
      "Epoch 9/300\n",
      "Average training loss: 0.058427197370264264\n",
      "Average test loss: 0.0037749454370803303\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05723205370373196\n",
      "Average test loss: 0.003686599392443895\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05617633730504248\n",
      "Average test loss: 0.0036337961877385776\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05524539219670826\n",
      "Average test loss: 0.003568296095770266\n",
      "Epoch 13/300\n",
      "Average training loss: 0.054426565796136854\n",
      "Average test loss: 0.003534887584133281\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05365246710181236\n",
      "Average test loss: 0.0034859636078278223\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05299754187795851\n",
      "Average test loss: 0.0034973103751738868\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05228244842092196\n",
      "Average test loss: 0.003452451630598969\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05175213743911849\n",
      "Average test loss: 0.00334408497520619\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05127291508515676\n",
      "Average test loss: 0.003315325441252854\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05070470235413975\n",
      "Average test loss: 0.0033313497313194806\n",
      "Epoch 20/300\n",
      "Average training loss: 0.050298782808913124\n",
      "Average test loss: 0.0032686609514057637\n",
      "Epoch 21/300\n",
      "Average training loss: 0.049928934388690525\n",
      "Average test loss: 0.003271894765810834\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04950055555833711\n",
      "Average test loss: 0.003277820786668195\n",
      "Epoch 23/300\n",
      "Average training loss: 0.049117963449822534\n",
      "Average test loss: 0.0032209559054010442\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04878178029259046\n",
      "Average test loss: 0.003217407276026077\n",
      "Epoch 25/300\n",
      "Average training loss: 0.048483896462453734\n",
      "Average test loss: 0.0031645487726976473\n",
      "Epoch 26/300\n",
      "Average training loss: 0.048188039667076535\n",
      "Average test loss: 0.003195592865968744\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04784101062019666\n",
      "Average test loss: 0.003113226671185758\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04759602735108799\n",
      "Average test loss: 0.00313236829597089\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0473624579972691\n",
      "Average test loss: 0.003130996638908982\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04717108051313294\n",
      "Average test loss: 0.0030965779862470095\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04689985946151946\n",
      "Average test loss: 0.0031377151204893985\n",
      "Epoch 32/300\n",
      "Average training loss: 0.046742858896652854\n",
      "Average test loss: 0.0030461477376520635\n",
      "Epoch 33/300\n",
      "Average training loss: 0.046515914152065914\n",
      "Average test loss: 0.003050634606430928\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0463088653617435\n",
      "Average test loss: 0.0030367276701662275\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04614111062884331\n",
      "Average test loss: 0.003035782422249516\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04595971033970515\n",
      "Average test loss: 0.0030345607900785073\n",
      "Epoch 37/300\n",
      "Average training loss: 0.045806426439020366\n",
      "Average test loss: 0.00299934963716401\n",
      "Epoch 38/300\n",
      "Average training loss: 0.045651425821913616\n",
      "Average test loss: 0.0030127228144556286\n",
      "Epoch 39/300\n",
      "Average training loss: 0.045546744038661324\n",
      "Average test loss: 0.003003714943718579\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04538386624389225\n",
      "Average test loss: 0.003000612241526445\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04525219311316808\n",
      "Average test loss: 0.0029949749952389135\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04518184761868583\n",
      "Average test loss: 0.002967432764462299\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04502002224988408\n",
      "Average test loss: 0.002966821348294616\n",
      "Epoch 44/300\n",
      "Average training loss: 0.044890910973151524\n",
      "Average test loss: 0.002995916950619883\n",
      "Epoch 45/300\n",
      "Average training loss: 0.044787455121676124\n",
      "Average test loss: 0.00295856079335014\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04463891118102604\n",
      "Average test loss: 0.002953134441955222\n",
      "Epoch 47/300\n",
      "Average training loss: 0.044559629807869594\n",
      "Average test loss: 0.0029611175045784976\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04444321817490789\n",
      "Average test loss: 0.0029424455573575366\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04436194242371453\n",
      "Average test loss: 0.0029426223751571443\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04423508132828607\n",
      "Average test loss: 0.0029258103681107364\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0441708957321114\n",
      "Average test loss: 0.0029257092587649824\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04410267171594832\n",
      "Average test loss: 0.002949272016891175\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04400302811463674\n",
      "Average test loss: 0.002934460765061279\n",
      "Epoch 54/300\n",
      "Average training loss: 0.043919306397438046\n",
      "Average test loss: 0.0029196367695306738\n",
      "Epoch 55/300\n",
      "Average training loss: 0.043828406224648155\n",
      "Average test loss: 0.0029742077433814605\n",
      "Epoch 56/300\n",
      "Average training loss: 0.043735197607013915\n",
      "Average test loss: 0.0029256447376683355\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04364137249853876\n",
      "Average test loss: 0.0029374104088379276\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04359473004274898\n",
      "Average test loss: 0.0029246956650167705\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04349933093455103\n",
      "Average test loss: 0.0029259341545403005\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04341593880785836\n",
      "Average test loss: 0.002938531372696161\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04335254042678409\n",
      "Average test loss: 0.0029063929427001214\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04325644135971864\n",
      "Average test loss: 0.002905215793599685\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04319741319616636\n",
      "Average test loss: 0.002955977583511008\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04312081321411663\n",
      "Average test loss: 0.002904058992034859\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04302812372313605\n",
      "Average test loss: 0.0028985148045337864\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04293070822623041\n",
      "Average test loss: 0.0028931437730789185\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0429095577398936\n",
      "Average test loss: 0.0029131518544422254\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04283594793164068\n",
      "Average test loss: 0.002945902122391595\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04277329615751902\n",
      "Average test loss: 0.0029193890219968224\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04273870001071029\n",
      "Average test loss: 0.002921121514091889\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04260405585832066\n",
      "Average test loss: 0.002911948470398784\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04254409578442574\n",
      "Average test loss: 0.0029050115299307636\n",
      "Epoch 73/300\n",
      "Average training loss: 0.042494206312629915\n",
      "Average test loss: 0.002901293033733964\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04241287203298675\n",
      "Average test loss: 0.0028984026211417383\n",
      "Epoch 75/300\n",
      "Average training loss: 0.042338432401418685\n",
      "Average test loss: 0.002898532812587089\n",
      "Epoch 76/300\n",
      "Average training loss: 0.042317448758416705\n",
      "Average test loss: 0.0029345149877998563\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04223024532530043\n",
      "Average test loss: 0.0029061446450650693\n",
      "Epoch 78/300\n",
      "Average training loss: 0.042186120195521246\n",
      "Average test loss: 0.002918383514405125\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04208702303800318\n",
      "Average test loss: 0.002934617671908604\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04206500406894419\n",
      "Average test loss: 0.0029235084485262634\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04196792717112435\n",
      "Average test loss: 0.002895265501820379\n",
      "Epoch 82/300\n",
      "Average training loss: 0.041979710280895234\n",
      "Average test loss: 0.0029048874974250795\n",
      "Epoch 83/300\n",
      "Average training loss: 0.041855177798204954\n",
      "Average test loss: 0.0029131741873506044\n",
      "Epoch 84/300\n",
      "Average training loss: 0.041800097243653404\n",
      "Average test loss: 0.0028783289063721894\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0417209075556861\n",
      "Average test loss: 0.0029015530654125745\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04161366865370009\n",
      "Average test loss: 0.002898357566859987\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04164150920344724\n",
      "Average test loss: 0.0028883024789392946\n",
      "Epoch 88/300\n",
      "Average training loss: 0.041546099490589565\n",
      "Average test loss: 0.00289820588607755\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04150310430592961\n",
      "Average test loss: 0.0029043893905149566\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04144248858425352\n",
      "Average test loss: 0.002891416613943875\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04142166799803575\n",
      "Average test loss: 0.0028994202363408273\n",
      "Epoch 92/300\n",
      "Average training loss: 0.041329675495624545\n",
      "Average test loss: 0.002950191147211525\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04127253239684635\n",
      "Average test loss: 0.00288218242675066\n",
      "Epoch 94/300\n",
      "Average training loss: 0.041221740659740234\n",
      "Average test loss: 0.003008214297807879\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0411601082202461\n",
      "Average test loss: 0.002915335847892695\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04111333257622189\n",
      "Average test loss: 0.002891014091877474\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04103714945250087\n",
      "Average test loss: 0.0029272336995022166\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04099132336841689\n",
      "Average test loss: 0.002908781854228841\n",
      "Epoch 99/300\n",
      "Average training loss: 0.040922434349854785\n",
      "Average test loss: 0.002904382559367352\n",
      "Epoch 100/300\n",
      "Average training loss: 0.040881299876504476\n",
      "Average test loss: 0.0029005253822025324\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04085421980089612\n",
      "Average test loss: 0.0028958756058580348\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04078552279207442\n",
      "Average test loss: 0.0029027964044362306\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04068703435526954\n",
      "Average test loss: 0.0028880348636044395\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04066143826312489\n",
      "Average test loss: 0.0029107249395714865\n",
      "Epoch 105/300\n",
      "Average training loss: 0.040637984461254544\n",
      "Average test loss: 0.002926486172609859\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04059599399235513\n",
      "Average test loss: 0.0029339486472308635\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04050494143035677\n",
      "Average test loss: 0.002906551939745744\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0404321682188246\n",
      "Average test loss: 0.002930607384070754\n",
      "Epoch 109/300\n",
      "Average training loss: 0.040443455891476736\n",
      "Average test loss: 0.002980733903331889\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04038269420464834\n",
      "Average test loss: 0.0029118854489384428\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04029959547519684\n",
      "Average test loss: 0.0030494633503258227\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04027679131759537\n",
      "Average test loss: 0.0029453095253556966\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04022136028773255\n",
      "Average test loss: 0.0029287904298139944\n",
      "Epoch 114/300\n",
      "Average training loss: 0.040200224333339266\n",
      "Average test loss: 0.002927278140352832\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04015239393048816\n",
      "Average test loss: 0.002907046954250998\n",
      "Epoch 116/300\n",
      "Average training loss: 0.040064353826973174\n",
      "Average test loss: 0.002931110278599792\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04003946728838815\n",
      "Average test loss: 0.002963707652770811\n",
      "Epoch 118/300\n",
      "Average training loss: 0.039954678939448464\n",
      "Average test loss: 0.002929375327709648\n",
      "Epoch 119/300\n",
      "Average training loss: 0.039965420090489916\n",
      "Average test loss: 0.0029227417136232056\n",
      "Epoch 120/300\n",
      "Average training loss: 0.039982020472486814\n",
      "Average test loss: 0.0029445281117740606\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03986330227553844\n",
      "Average test loss: 0.0029805690952473215\n",
      "Epoch 122/300\n",
      "Average training loss: 0.039808650119437114\n",
      "Average test loss: 0.002940444221099218\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03974516365925471\n",
      "Average test loss: 0.0029715926431947283\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03973345542947451\n",
      "Average test loss: 0.0029217064465499585\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03966308055652512\n",
      "Average test loss: 0.0030138582707279257\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03963791444235378\n",
      "Average test loss: 0.0029323898473133645\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03960963093572193\n",
      "Average test loss: 0.0029239731422728964\n",
      "Epoch 128/300\n",
      "Average training loss: 0.039553930674990016\n",
      "Average test loss: 0.002954006729233596\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03953574776980612\n",
      "Average test loss: 0.0029432213223642774\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03945209360784954\n",
      "Average test loss: 0.0029456236352109247\n",
      "Epoch 131/300\n",
      "Average training loss: 0.039432304647233754\n",
      "Average test loss: 0.002947739349057277\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03941055073671871\n",
      "Average test loss: 0.0029317340051962268\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03935375855863094\n",
      "Average test loss: 0.0030164521048880286\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03929421458641688\n",
      "Average test loss: 0.0029519904810521337\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03926994879378213\n",
      "Average test loss: 0.0029329179488122464\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03922283668816089\n",
      "Average test loss: 0.0029560100367913645\n",
      "Epoch 137/300\n",
      "Average training loss: 0.039206437971856856\n",
      "Average test loss: 0.0029522944622569613\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03914292550749249\n",
      "Average test loss: 0.002979774287591378\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03911094606584973\n",
      "Average test loss: 0.003011241379814843\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03912467787663142\n",
      "Average test loss: 0.00294204198072354\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03900486623909738\n",
      "Average test loss: 0.0029303711582389144\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03903544838892089\n",
      "Average test loss: 0.00299515698901895\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03898559115992652\n",
      "Average test loss: 0.002975410884867112\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0389322109553549\n",
      "Average test loss: 0.003040796694242292\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03892020899388525\n",
      "Average test loss: 0.0029815996912204556\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03889005120264159\n",
      "Average test loss: 0.0029817115246421762\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03884822487996684\n",
      "Average test loss: 0.0030371424769982695\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03881932491064072\n",
      "Average test loss: 0.0029563591556830537\n",
      "Epoch 149/300\n",
      "Average training loss: 0.038782539937231277\n",
      "Average test loss: 0.0029579174862139753\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03872765328983466\n",
      "Average test loss: 0.003034754847072893\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03868503328661124\n",
      "Average test loss: 0.0030305312654624384\n",
      "Epoch 152/300\n",
      "Average training loss: 0.038675243182314765\n",
      "Average test loss: 0.002964267355079452\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03862097372611364\n",
      "Average test loss: 0.003003199076693919\n",
      "Epoch 154/300\n",
      "Average training loss: 0.038619792779286705\n",
      "Average test loss: 0.0030064728197952114\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03858476337128215\n",
      "Average test loss: 0.002977892761532631\n",
      "Epoch 156/300\n",
      "Average training loss: 0.038572812911536955\n",
      "Average test loss: 0.003032020743108458\n",
      "Epoch 157/300\n",
      "Average training loss: 0.038519282301266985\n",
      "Average test loss: 0.003042864879282812\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0384957028263145\n",
      "Average test loss: 0.0029731864865041443\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03847608232498169\n",
      "Average test loss: 0.002991447226351334\n",
      "Epoch 160/300\n",
      "Average training loss: 0.038421222239732745\n",
      "Average test loss: 0.002977352240226335\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03840333578321669\n",
      "Average test loss: 0.0030054044287858737\n",
      "Epoch 162/300\n",
      "Average training loss: 0.038366777456469006\n",
      "Average test loss: 0.003058904683424367\n",
      "Epoch 163/300\n",
      "Average training loss: 0.038359507509403756\n",
      "Average test loss: 0.003079932718641228\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03836648354430994\n",
      "Average test loss: 0.0029926745891571045\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03825667776995235\n",
      "Average test loss: 0.0030443703372859295\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03826739012863901\n",
      "Average test loss: 0.0030170416318707994\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03825546859039201\n",
      "Average test loss: 0.002982281722438832\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03817392692466577\n",
      "Average test loss: 0.0030707970131188633\n",
      "Epoch 169/300\n",
      "Average training loss: 0.038205272239115504\n",
      "Average test loss: 0.003033961014615165\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03817851773897807\n",
      "Average test loss: 0.0030472227392925157\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03807966790265507\n",
      "Average test loss: 0.0029619797720677322\n",
      "Epoch 172/300\n",
      "Average training loss: 0.038072736422220865\n",
      "Average test loss: 0.0030269588770137894\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03807269373204973\n",
      "Average test loss: 0.00299245802188913\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03801460124386682\n",
      "Average test loss: 0.0029930491236348947\n",
      "Epoch 175/300\n",
      "Average training loss: 0.038005407369799085\n",
      "Average test loss: 0.0029991877343919543\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03797588862313164\n",
      "Average test loss: 0.003032970597139663\n",
      "Epoch 177/300\n",
      "Average training loss: 0.037980949858824414\n",
      "Average test loss: 0.0030036714762035344\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03793163146244155\n",
      "Average test loss: 0.002991606047583951\n",
      "Epoch 179/300\n",
      "Average training loss: 0.037912890072498054\n",
      "Average test loss: 0.0030428671805808943\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03794454432858361\n",
      "Average test loss: 0.003014469027519226\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03789262583355109\n",
      "Average test loss: 0.003064232174721029\n",
      "Epoch 182/300\n",
      "Average training loss: 0.037808396776517234\n",
      "Average test loss: 0.002971288927934236\n",
      "Epoch 183/300\n",
      "Average training loss: 0.037792390185925694\n",
      "Average test loss: 0.003018820540358623\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03783814760380321\n",
      "Average test loss: 0.003019272630620334\n",
      "Epoch 185/300\n",
      "Average training loss: 0.037761461289392576\n",
      "Average test loss: 0.0030073516166044608\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03772025356358952\n",
      "Average test loss: 0.0030187423037779\n",
      "Epoch 187/300\n",
      "Average training loss: 0.037720867690112854\n",
      "Average test loss: 0.003256093993369076\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03772243212991291\n",
      "Average test loss: 0.00299470840787722\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03769610329800182\n",
      "Average test loss: 0.0030839903545048504\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03763913691706128\n",
      "Average test loss: 0.002993328026185433\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03759141179588106\n",
      "Average test loss: 0.003027346862687005\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0376293395989471\n",
      "Average test loss: 0.003071722752518124\n",
      "Epoch 193/300\n",
      "Average training loss: 0.037562355652451515\n",
      "Average test loss: 0.0030653212879680927\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03757203010718028\n",
      "Average test loss: 0.0030499265196008815\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03758369580242369\n",
      "Average test loss: 0.0030305544603616\n",
      "Epoch 196/300\n",
      "Average training loss: 0.037513588378826776\n",
      "Average test loss: 0.00306569562976559\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03752477974361843\n",
      "Average test loss: 0.0030578726318975288\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0375263691842556\n",
      "Average test loss: 0.00302633632802301\n",
      "Epoch 199/300\n",
      "Average training loss: 0.037458910829491085\n",
      "Average test loss: 0.0030739157715191446\n",
      "Epoch 200/300\n",
      "Average training loss: 0.037385793616374334\n",
      "Average test loss: 0.0030303317780296006\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03752703082230356\n",
      "Average test loss: 0.0030437645382351344\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03740358957648277\n",
      "Average test loss: 0.0030075052010102403\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0373429792424043\n",
      "Average test loss: 0.0030850444605780973\n",
      "Epoch 204/300\n",
      "Average training loss: 0.037314998424715466\n",
      "Average test loss: 0.0030549285229709414\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03731959153546227\n",
      "Average test loss: 0.003065887929457757\n",
      "Epoch 206/300\n",
      "Average training loss: 0.037330485744608774\n",
      "Average test loss: 0.003060365779325366\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03730411737826136\n",
      "Average test loss: 0.0030149909916023414\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03723672436012162\n",
      "Average test loss: 0.003004316112647454\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03727527012096511\n",
      "Average test loss: 0.003025421445982324\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0372631734377808\n",
      "Average test loss: 0.0030730188664876753\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0371993122001489\n",
      "Average test loss: 0.0030728984634495445\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0373047364734941\n",
      "Average test loss: 0.003035918478336599\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0372185659872161\n",
      "Average test loss: 0.0029754144528673756\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03714110422134399\n",
      "Average test loss: 0.0030683525192240872\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03715744417574671\n",
      "Average test loss: 0.00307744739515086\n",
      "Epoch 216/300\n",
      "Average training loss: 0.037142342562476796\n",
      "Average test loss: 0.003242827123651902\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03712081171572208\n",
      "Average test loss: 0.0031089346307433315\n",
      "Epoch 218/300\n",
      "Average training loss: 0.037121049053139156\n",
      "Average test loss: 0.0030188386865581075\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03704444484247102\n",
      "Average test loss: 0.0030105202694733937\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0370312826997704\n",
      "Average test loss: 0.0030109307534164852\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03702980587714248\n",
      "Average test loss: 0.0030832830857899455\n",
      "Epoch 222/300\n",
      "Average training loss: 0.037009426827232045\n",
      "Average test loss: 0.0030506242466055685\n",
      "Epoch 223/300\n",
      "Average training loss: 0.037025007724761964\n",
      "Average test loss: 0.0031812176300833623\n",
      "Epoch 224/300\n",
      "Average training loss: 0.036970120168394516\n",
      "Average test loss: 0.0032246844495336215\n",
      "Epoch 225/300\n",
      "Average training loss: 0.037006042503648334\n",
      "Average test loss: 0.003093782228624655\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03699854658709632\n",
      "Average test loss: 0.003145365485921502\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03699475213223034\n",
      "Average test loss: 0.0030605148211535482\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03688615255554517\n",
      "Average test loss: 0.0030066164692656864\n",
      "Epoch 229/300\n",
      "Average training loss: 0.036951266394721134\n",
      "Average test loss: 0.003284548655980163\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03687800260716014\n",
      "Average test loss: 0.0030351153020229606\n",
      "Epoch 231/300\n",
      "Average training loss: 0.036862620169917745\n",
      "Average test loss: 0.0030299282423737976\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03683432342939907\n",
      "Average test loss: 0.003031526514225536\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0368671069857147\n",
      "Average test loss: 0.003116680715129607\n",
      "Epoch 234/300\n",
      "Average training loss: 0.036915350995130006\n",
      "Average test loss: 0.0031323436645583976\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03682332963579231\n",
      "Average test loss: 0.0030768452127360635\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03681594667169783\n",
      "Average test loss: 0.0030863998317056233\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03679613072342343\n",
      "Average test loss: 0.0030375998984608385\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03679081763823827\n",
      "Average test loss: 0.003053310407946507\n",
      "Epoch 239/300\n",
      "Average training loss: 0.036734750860267215\n",
      "Average test loss: 0.0030875609221143853\n",
      "Epoch 240/300\n",
      "Average training loss: 0.036761070933606886\n",
      "Average test loss: 0.003073639875277877\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03672183205352889\n",
      "Average test loss: 0.0030663324046052163\n",
      "Epoch 242/300\n",
      "Average training loss: 0.036756744712591174\n",
      "Average test loss: 0.0030796392150223254\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03673009036646949\n",
      "Average test loss: 0.0030491666744152704\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0366282069997655\n",
      "Average test loss: 0.00312814923748374\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0366546385023329\n",
      "Average test loss: 0.003047054138655464\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03664431189166175\n",
      "Average test loss: 0.003124729091922442\n",
      "Epoch 247/300\n",
      "Average training loss: 0.036585184925132325\n",
      "Average test loss: 0.003073531804192397\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03665987741450469\n",
      "Average test loss: 0.0030399595993674464\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03657366613547007\n",
      "Average test loss: 0.0031668553209553162\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03658382335636351\n",
      "Average test loss: 0.0030801821558011904\n",
      "Epoch 251/300\n",
      "Average training loss: 0.036589062419202595\n",
      "Average test loss: 0.003053936384411322\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03654087715678745\n",
      "Average test loss: 0.0030314900308019586\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0365576115945975\n",
      "Average test loss: 0.0032192492791348034\n",
      "Epoch 254/300\n",
      "Average training loss: 0.036561151282654865\n",
      "Average test loss: 0.003013685044729047\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03647997956805759\n",
      "Average test loss: 0.003022351634792156\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03646991426083777\n",
      "Average test loss: 0.0030109792914655474\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03650210411515501\n",
      "Average test loss: 0.0030518887038860055\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03654586022761133\n",
      "Average test loss: 0.003055384496019946\n",
      "Epoch 259/300\n",
      "Average training loss: 0.036426735353138714\n",
      "Average test loss: 0.0031408993295497364\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03643791061308649\n",
      "Average test loss: 0.0030940036901997195\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03647065867318047\n",
      "Average test loss: 0.0030889978361212547\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03642910032305453\n",
      "Average test loss: 0.0031560268938127492\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03645423038966126\n",
      "Average test loss: 0.0030944560122572714\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03642455505993631\n",
      "Average test loss: 0.003104054823103878\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03647375831670231\n",
      "Average test loss: 0.0031090151703813008\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03637086040774981\n",
      "Average test loss: 0.0032434006958372064\n",
      "Epoch 267/300\n",
      "Average training loss: 0.036324756658739514\n",
      "Average test loss: 0.0030686280998504824\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03634951276746061\n",
      "Average test loss: 0.0030538810692313646\n",
      "Epoch 269/300\n",
      "Average training loss: 0.036365259425507654\n",
      "Average test loss: 0.0030883326621519193\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03635830957690875\n",
      "Average test loss: 0.00307857029305564\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03628128746151924\n",
      "Average test loss: 0.0030486629290713205\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03629892334010866\n",
      "Average test loss: 0.0030943709092421663\n",
      "Epoch 273/300\n",
      "Average training loss: 0.036252010186513264\n",
      "Average test loss: 0.003083369671056668\n",
      "Epoch 274/300\n",
      "Average training loss: 0.036238400152987904\n",
      "Average test loss: 0.00311851008153624\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03624849543140994\n",
      "Average test loss: 0.0031448150699337324\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03623107378019227\n",
      "Average test loss: 0.0031634602343870535\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03630089371734195\n",
      "Average test loss: 0.0030821738576309547\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03623401326272223\n",
      "Average test loss: 0.003065625798371103\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03622643400231997\n",
      "Average test loss: 0.0030875818058848383\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03622001162916422\n",
      "Average test loss: 0.0030793440598580572\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0361849520570702\n",
      "Average test loss: 0.003136695111584332\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03618133725722631\n",
      "Average test loss: 0.0030790656424231\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03615517573720879\n",
      "Average test loss: 0.0030854034216867554\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03621884772843785\n",
      "Average test loss: 0.0030748778426398834\n",
      "Epoch 285/300\n",
      "Average training loss: 0.036152760063608486\n",
      "Average test loss: 0.003060349081953367\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0361255256831646\n",
      "Average test loss: 0.0032774310140973994\n",
      "Epoch 287/300\n",
      "Average training loss: 0.036181483560138276\n",
      "Average test loss: 0.00313199017672903\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03614900251560741\n",
      "Average test loss: 0.0030986219748026793\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03605279530088107\n",
      "Average test loss: 0.0031095047899418406\n",
      "Epoch 290/300\n",
      "Average training loss: 0.036109242934319705\n",
      "Average test loss: 0.0030915810020847455\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03608118109901746\n",
      "Average test loss: 0.0031131533470211757\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03603187668654654\n",
      "Average test loss: 0.0031506343053446877\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03607869185010592\n",
      "Average test loss: 0.003114338806312945\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03606626822219955\n",
      "Average test loss: 0.003060970569236411\n",
      "Epoch 295/300\n",
      "Average training loss: 0.035991284987992714\n",
      "Average test loss: 0.003117152862250805\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03606713957587878\n",
      "Average test loss: 0.0031635356911768517\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03600835821198092\n",
      "Average test loss: 0.003141137062054541\n",
      "Epoch 298/300\n",
      "Average training loss: 0.036050001972251465\n",
      "Average test loss: 0.0031137442481186657\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03598994791507721\n",
      "Average test loss: 0.0031536970792545214\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03599301593999068\n",
      "Average test loss: 0.0030940594066762262\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7082303292089038\n",
      "Average test loss: 0.004848352161546548\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08926323878765106\n",
      "Average test loss: 0.004109572582567732\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07049914805756675\n",
      "Average test loss: 0.003988909646454784\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06281180793709225\n",
      "Average test loss: 0.0036028058820714555\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05821957265337308\n",
      "Average test loss: 0.003612933484216531\n",
      "Epoch 6/300\n",
      "Average training loss: 0.055122212443086835\n",
      "Average test loss: 0.00336310688406229\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0527544783088896\n",
      "Average test loss: 0.003385151978996065\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05083930522534583\n",
      "Average test loss: 0.0032389913006789155\n",
      "Epoch 9/300\n",
      "Average training loss: 0.049258946120738986\n",
      "Average test loss: 0.003087723827610413\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0478059449493885\n",
      "Average test loss: 0.0030063747794677816\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04650093024969101\n",
      "Average test loss: 0.002927237926878863\n",
      "Epoch 12/300\n",
      "Average training loss: 0.045334389395183985\n",
      "Average test loss: 0.002974048463213775\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04435527602665954\n",
      "Average test loss: 0.0028562588223980533\n",
      "Epoch 14/300\n",
      "Average training loss: 0.043374742014540564\n",
      "Average test loss: 0.002722648997480671\n",
      "Epoch 15/300\n",
      "Average training loss: 0.042498928755521775\n",
      "Average test loss: 0.002685312525679668\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04170588405264748\n",
      "Average test loss: 0.0026395846996456387\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04098046007917987\n",
      "Average test loss: 0.0026326024898638326\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0403516705930233\n",
      "Average test loss: 0.0026031830594357517\n",
      "Epoch 19/300\n",
      "Average training loss: 0.039727111041545866\n",
      "Average test loss: 0.002507044700284799\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03928594547675716\n",
      "Average test loss: 0.002451213752023048\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03874027670092053\n",
      "Average test loss: 0.002427164702779717\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03825153139233589\n",
      "Average test loss: 0.0024153426438570024\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03785593295262919\n",
      "Average test loss: 0.0023987287875368362\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03745820696072446\n",
      "Average test loss: 0.002366002017425166\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03706438409619861\n",
      "Average test loss: 0.0023445208398625256\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03681316064960427\n",
      "Average test loss: 0.002325039741479688\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03643159715003438\n",
      "Average test loss: 0.0023111217270294824\n",
      "Epoch 28/300\n",
      "Average training loss: 0.036134930993119874\n",
      "Average test loss: 0.0022829927564081216\n",
      "Epoch 29/300\n",
      "Average training loss: 0.035879188077317346\n",
      "Average test loss: 0.00227538768771208\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03562328730854723\n",
      "Average test loss: 0.0023137097052401967\n",
      "Epoch 31/300\n",
      "Average training loss: 0.035481097708145774\n",
      "Average test loss: 0.002313525005347199\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03524363918602467\n",
      "Average test loss: 0.0022273917990840142\n",
      "Epoch 33/300\n",
      "Average training loss: 0.035005756601691244\n",
      "Average test loss: 0.0022398957112390134\n",
      "Epoch 34/300\n",
      "Average training loss: 0.034824363440275195\n",
      "Average test loss: 0.002221651688425077\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03466847918430964\n",
      "Average test loss: 0.002220473437052634\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0345274588962396\n",
      "Average test loss: 0.002198244515599476\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03430946231219504\n",
      "Average test loss: 0.002181495951799055\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03418714254432254\n",
      "Average test loss: 0.0021816659522139363\n",
      "Epoch 39/300\n",
      "Average training loss: 0.034051740043693116\n",
      "Average test loss: 0.002190526769599981\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03394229498008887\n",
      "Average test loss: 0.0022023383623196017\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03382991586956713\n",
      "Average test loss: 0.002155007040128112\n",
      "Epoch 42/300\n",
      "Average training loss: 0.033663810201817086\n",
      "Average test loss: 0.0021822065636515615\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03358030641244517\n",
      "Average test loss: 0.002206691393732197\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03350770894024107\n",
      "Average test loss: 0.0021788467390255794\n",
      "Epoch 45/300\n",
      "Average training loss: 0.033370961639616226\n",
      "Average test loss: 0.0021598593348430262\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03323959310021665\n",
      "Average test loss: 0.002123982770058016\n",
      "Epoch 47/300\n",
      "Average training loss: 0.033153611866964236\n",
      "Average test loss: 0.00215068299540629\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03304439186718729\n",
      "Average test loss: 0.002133091930920879\n",
      "Epoch 49/300\n",
      "Average training loss: 0.032944015171792775\n",
      "Average test loss: 0.0021280340251202384\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03288272679348787\n",
      "Average test loss: 0.0021283943404754\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03283020068042808\n",
      "Average test loss: 0.0021233156629734567\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03267943421668477\n",
      "Average test loss: 0.0021586600235766833\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03265134082569016\n",
      "Average test loss: 0.002125345262388388\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03254570421079794\n",
      "Average test loss: 0.0021048247104303703\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0324942114452521\n",
      "Average test loss: 0.0021087678097602394\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03238096996313996\n",
      "Average test loss: 0.002117089226221045\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03234311079647806\n",
      "Average test loss: 0.0021111246903116505\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0322147181481123\n",
      "Average test loss: 0.0020964005345271695\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03213413467175431\n",
      "Average test loss: 0.0021065696304043134\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0321422548012601\n",
      "Average test loss: 0.002107020255799095\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0320205914295382\n",
      "Average test loss: 0.0020978380201591386\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0319903430160549\n",
      "Average test loss: 0.002094371548957295\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03187825825976001\n",
      "Average test loss: 0.0021198982443246576\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03182073443300194\n",
      "Average test loss: 0.002106314059967796\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03179470629493396\n",
      "Average test loss: 0.0020981668358047803\n",
      "Epoch 66/300\n",
      "Average training loss: 0.031697779392202696\n",
      "Average test loss: 0.002092361011231939\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03170666549934281\n",
      "Average test loss: 0.002094175001813306\n",
      "Epoch 68/300\n",
      "Average training loss: 0.031598517720897994\n",
      "Average test loss: 0.002099312366503808\n",
      "Epoch 69/300\n",
      "Average training loss: 0.031502370483345456\n",
      "Average test loss: 0.002093481605872512\n",
      "Epoch 70/300\n",
      "Average training loss: 0.031446531939837666\n",
      "Average test loss: 0.0020778453215542767\n",
      "Epoch 71/300\n",
      "Average training loss: 0.031396544277668\n",
      "Average test loss: 0.002084250502909223\n",
      "Epoch 72/300\n",
      "Average training loss: 0.031325148853990764\n",
      "Average test loss: 0.0020854532337850995\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03127837540871567\n",
      "Average test loss: 0.0020943924012697406\n",
      "Epoch 74/300\n",
      "Average training loss: 0.031197591957118777\n",
      "Average test loss: 0.0021253804568615226\n",
      "Epoch 75/300\n",
      "Average training loss: 0.031152626951535544\n",
      "Average test loss: 0.002079457530958785\n",
      "Epoch 76/300\n",
      "Average training loss: 0.031141827932662434\n",
      "Average test loss: 0.0020849673222336504\n",
      "Epoch 77/300\n",
      "Average training loss: 0.031090039256546232\n",
      "Average test loss: 0.0021085395477712155\n",
      "Epoch 78/300\n",
      "Average training loss: 0.030998906180262565\n",
      "Average test loss: 0.0020946699920006925\n",
      "Epoch 79/300\n",
      "Average training loss: 0.030908583435747358\n",
      "Average test loss: 0.0020998923929615152\n",
      "Epoch 80/300\n",
      "Average training loss: 0.030928305945462652\n",
      "Average test loss: 0.0020768000113053453\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03085802067981826\n",
      "Average test loss: 0.002100926115281052\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0307766434856587\n",
      "Average test loss: 0.002135031699306435\n",
      "Epoch 83/300\n",
      "Average training loss: 0.030763190251257684\n",
      "Average test loss: 0.0020817854760421645\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03068965100414223\n",
      "Average test loss: 0.002087317789801293\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0306743130352762\n",
      "Average test loss: 0.0021157490849081015\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0305715665039089\n",
      "Average test loss: 0.002103278072240452\n",
      "Epoch 87/300\n",
      "Average training loss: 0.030515554871824054\n",
      "Average test loss: 0.0020859534232877193\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03048101758460204\n",
      "Average test loss: 0.002083975779720479\n",
      "Epoch 89/300\n",
      "Average training loss: 0.030402678090665075\n",
      "Average test loss: 0.002077688787132502\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0304047967178954\n",
      "Average test loss: 0.0020951890796422957\n",
      "Epoch 91/300\n",
      "Average training loss: 0.030358343846268124\n",
      "Average test loss: 0.002085861699759132\n",
      "Epoch 92/300\n",
      "Average training loss: 0.030305271729826926\n",
      "Average test loss: 0.0020959564064525894\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03023128545615408\n",
      "Average test loss: 0.0020880754699723588\n",
      "Epoch 94/300\n",
      "Average training loss: 0.030223317492339346\n",
      "Average test loss: 0.002080517628747556\n",
      "Epoch 95/300\n",
      "Average training loss: 0.030175692498683928\n",
      "Average test loss: 0.0021009044825202887\n",
      "Epoch 96/300\n",
      "Average training loss: 0.030110160799490082\n",
      "Average test loss: 0.002105175580415461\n",
      "Epoch 97/300\n",
      "Average training loss: 0.030064111371835074\n",
      "Average test loss: 0.002092103861013634\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03000565658840868\n",
      "Average test loss: 0.002105011142893798\n",
      "Epoch 99/300\n",
      "Average training loss: 0.029999171497093306\n",
      "Average test loss: 0.0020908874947991635\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0299928593966696\n",
      "Average test loss: 0.002106268181362086\n",
      "Epoch 101/300\n",
      "Average training loss: 0.029877838384773998\n",
      "Average test loss: 0.00210122586434914\n",
      "Epoch 102/300\n",
      "Average training loss: 0.029842041505707635\n",
      "Average test loss: 0.002117431894048221\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02986895067493121\n",
      "Average test loss: 0.0020938686410793\n",
      "Epoch 104/300\n",
      "Average training loss: 0.029775040035446485\n",
      "Average test loss: 0.0020945290950023466\n",
      "Epoch 105/300\n",
      "Average training loss: 0.029726456670297517\n",
      "Average test loss: 0.002144155065736009\n",
      "Epoch 106/300\n",
      "Average training loss: 0.029706209222475688\n",
      "Average test loss: 0.0021282586243210567\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02965844041109085\n",
      "Average test loss: 0.002106320443045762\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0295872903002633\n",
      "Average test loss: 0.0021061627421942023\n",
      "Epoch 109/300\n",
      "Average training loss: 0.029557332808772725\n",
      "Average test loss: 0.002094385809161597\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02952529758380519\n",
      "Average test loss: 0.002101528658014205\n",
      "Epoch 111/300\n",
      "Average training loss: 0.029478064715862274\n",
      "Average test loss: 0.0020837348372571996\n",
      "Epoch 112/300\n",
      "Average training loss: 0.029479791303475698\n",
      "Average test loss: 0.0020936504267156123\n",
      "Epoch 113/300\n",
      "Average training loss: 0.029413214931885402\n",
      "Average test loss: 0.002117555596348312\n",
      "Epoch 114/300\n",
      "Average training loss: 0.029375546688834825\n",
      "Average test loss: 0.002092110519607862\n",
      "Epoch 115/300\n",
      "Average training loss: 0.029347421584857834\n",
      "Average test loss: 0.002089964523187114\n",
      "Epoch 116/300\n",
      "Average training loss: 0.029320045550664267\n",
      "Average test loss: 0.0021205377433862953\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02927010423441728\n",
      "Average test loss: 0.002095834579732683\n",
      "Epoch 118/300\n",
      "Average training loss: 0.029218507577975592\n",
      "Average test loss: 0.002087624070958959\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02917801646557119\n",
      "Average test loss: 0.0021114733055647874\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02915656876895163\n",
      "Average test loss: 0.002140094166828526\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02911841936575042\n",
      "Average test loss: 0.002107883338506023\n",
      "Epoch 122/300\n",
      "Average training loss: 0.029083728533652094\n",
      "Average test loss: 0.0021374732290084164\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02904944097664621\n",
      "Average test loss: 0.0021072102694047823\n",
      "Epoch 124/300\n",
      "Average training loss: 0.029059417792492444\n",
      "Average test loss: 0.002145498344053825\n",
      "Epoch 125/300\n",
      "Average training loss: 0.029067251529958513\n",
      "Average test loss: 0.002129925329445137\n",
      "Epoch 126/300\n",
      "Average training loss: 0.028926847966180908\n",
      "Average test loss: 0.0021080699728594885\n",
      "Epoch 127/300\n",
      "Average training loss: 0.028908490450845824\n",
      "Average test loss: 0.002113573975343671\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02889074121746752\n",
      "Average test loss: 0.0021277136132121085\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02883393320772383\n",
      "Average test loss: 0.002145178469725781\n",
      "Epoch 130/300\n",
      "Average training loss: 0.028794564363029268\n",
      "Average test loss: 0.0024114570162362525\n",
      "Epoch 131/300\n",
      "Average training loss: 0.028793303676777415\n",
      "Average test loss: 0.0021559779165933528\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02878253708779812\n",
      "Average test loss: 0.0021434741914272307\n",
      "Epoch 133/300\n",
      "Average training loss: 0.028763724588685567\n",
      "Average test loss: 0.0021430012999723354\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02871270291838381\n",
      "Average test loss: 0.0021087896671767034\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028656840238306256\n",
      "Average test loss: 0.0022017349153757096\n",
      "Epoch 136/300\n",
      "Average training loss: 0.028632727594839202\n",
      "Average test loss: 0.002132849128295978\n",
      "Epoch 137/300\n",
      "Average training loss: 0.028562645675407516\n",
      "Average test loss: 0.002122189975033204\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0285617541588015\n",
      "Average test loss: 0.002114044106254975\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02856497888929314\n",
      "Average test loss: 0.0021666098948982027\n",
      "Epoch 140/300\n",
      "Average training loss: 0.028524820514851147\n",
      "Average test loss: 0.0021287429009874664\n",
      "Epoch 141/300\n",
      "Average training loss: 0.028488601790534127\n",
      "Average test loss: 0.002191261215135455\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02846364807917012\n",
      "Average test loss: 0.002106042075695263\n",
      "Epoch 143/300\n",
      "Average training loss: 0.028445831924676895\n",
      "Average test loss: 0.002125585292155544\n",
      "Epoch 144/300\n",
      "Average training loss: 0.028389909265769852\n",
      "Average test loss: 0.002112947088355819\n",
      "Epoch 145/300\n",
      "Average training loss: 0.028343927285737463\n",
      "Average test loss: 0.002147815930760569\n",
      "Epoch 146/300\n",
      "Average training loss: 0.028362399967180357\n",
      "Average test loss: 0.0021465748488489127\n",
      "Epoch 147/300\n",
      "Average training loss: 0.028252009517616694\n",
      "Average test loss: 0.0021181760082642238\n",
      "Epoch 148/300\n",
      "Average training loss: 0.028261294118232198\n",
      "Average test loss: 0.002115020845913225\n",
      "Epoch 149/300\n",
      "Average training loss: 0.028249614147676363\n",
      "Average test loss: 0.002125676369191044\n",
      "Epoch 150/300\n",
      "Average training loss: 0.028247902287377252\n",
      "Average test loss: 0.0021562304798927573\n",
      "Epoch 151/300\n",
      "Average training loss: 0.028191969480779434\n",
      "Average test loss: 0.0021653091250401404\n",
      "Epoch 152/300\n",
      "Average training loss: 0.028178388610482216\n",
      "Average test loss: 0.002105069129003419\n",
      "Epoch 153/300\n",
      "Average training loss: 0.028157692088021172\n",
      "Average test loss: 0.0021460338851643935\n",
      "Epoch 154/300\n",
      "Average training loss: 0.028128492751055292\n",
      "Average test loss: 0.0022020100911872256\n",
      "Epoch 155/300\n",
      "Average training loss: 0.028158029307921727\n",
      "Average test loss: 0.0021458038635965852\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0280914034181171\n",
      "Average test loss: 0.002126956659472651\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02807082144253784\n",
      "Average test loss: 0.002143982398013274\n",
      "Epoch 158/300\n",
      "Average training loss: 0.028007573390172586\n",
      "Average test loss: 0.002156504050931997\n",
      "Epoch 159/300\n",
      "Average training loss: 0.027996411735812823\n",
      "Average test loss: 0.0021518990950038035\n",
      "Epoch 160/300\n",
      "Average training loss: 0.027948208063840865\n",
      "Average test loss: 0.0021302966518948474\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02795194090737237\n",
      "Average test loss: 0.002158405108584298\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02794542895257473\n",
      "Average test loss: 0.0021473143864423035\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02791612869832251\n",
      "Average test loss: 0.0021274389532498186\n",
      "Epoch 164/300\n",
      "Average training loss: 0.027877172509829202\n",
      "Average test loss: 0.0021419226876977417\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02785762173930804\n",
      "Average test loss: 0.0021402287592904435\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02783160223894649\n",
      "Average test loss: 0.0022011788667490083\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02779669457508458\n",
      "Average test loss: 0.002180118075158033\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02775330159564813\n",
      "Average test loss: 0.002111758782942262\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02777641212940216\n",
      "Average test loss: 0.002129263463533587\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02775811786784066\n",
      "Average test loss: 0.002201622394017047\n",
      "Epoch 171/300\n",
      "Average training loss: 0.027811784283982382\n",
      "Average test loss: 0.0022207590641660823\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0277140341848135\n",
      "Average test loss: 0.002180547599784202\n",
      "Epoch 173/300\n",
      "Average training loss: 0.027682355081041655\n",
      "Average test loss: 0.002169851557041208\n",
      "Epoch 174/300\n",
      "Average training loss: 0.027652771522601444\n",
      "Average test loss: 0.0021597307367871204\n",
      "Epoch 175/300\n",
      "Average training loss: 0.027640335314803654\n",
      "Average test loss: 0.002165752144944337\n",
      "Epoch 176/300\n",
      "Average training loss: 0.027661972688304053\n",
      "Average test loss: 0.0022110545829766327\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02762976278861364\n",
      "Average test loss: 0.0021792373727593157\n",
      "Epoch 178/300\n",
      "Average training loss: 0.027597717848089006\n",
      "Average test loss: 0.002188875607214868\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02755386660827531\n",
      "Average test loss: 0.0022100170040503142\n",
      "Epoch 180/300\n",
      "Average training loss: 0.027497035392456584\n",
      "Average test loss: 0.0022276221399919853\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02750547172791428\n",
      "Average test loss: 0.002196639113749067\n",
      "Epoch 182/300\n",
      "Average training loss: 0.027464275156458218\n",
      "Average test loss: 0.002181106482528978\n",
      "Epoch 183/300\n",
      "Average training loss: 0.027505535115798313\n",
      "Average test loss: 0.0021616524927732016\n",
      "Epoch 184/300\n",
      "Average training loss: 0.027463090252545144\n",
      "Average test loss: 0.002148057345093952\n",
      "Epoch 185/300\n",
      "Average training loss: 0.027455939945247438\n",
      "Average test loss: 0.002163008237671521\n",
      "Epoch 186/300\n",
      "Average training loss: 0.027401459712121223\n",
      "Average test loss: 0.0021508558589137263\n",
      "Epoch 187/300\n",
      "Average training loss: 0.027376496258709165\n",
      "Average test loss: 0.002208864622231987\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02744222880899906\n",
      "Average test loss: 0.0021766030398300954\n",
      "Epoch 189/300\n",
      "Average training loss: 0.027357185579008528\n",
      "Average test loss: 0.0021878499899887376\n",
      "Epoch 190/300\n",
      "Average training loss: 0.027335476444827186\n",
      "Average test loss: 0.0021498376495308344\n",
      "Epoch 191/300\n",
      "Average training loss: 0.027334222161107594\n",
      "Average test loss: 0.0022092547015183503\n",
      "Epoch 192/300\n",
      "Average training loss: 0.027321773575411903\n",
      "Average test loss: 0.002208632288500667\n",
      "Epoch 193/300\n",
      "Average training loss: 0.027292772288123768\n",
      "Average test loss: 0.00215692253700561\n",
      "Epoch 194/300\n",
      "Average training loss: 0.027286759740776486\n",
      "Average test loss: 0.0021982839670446185\n",
      "Epoch 195/300\n",
      "Average training loss: 0.027255269817180103\n",
      "Average test loss: 0.0021863553130792245\n",
      "Epoch 196/300\n",
      "Average training loss: 0.027210666865110398\n",
      "Average test loss: 0.002196161002334621\n",
      "Epoch 197/300\n",
      "Average training loss: 0.027211743022004763\n",
      "Average test loss: 0.0021824764083657\n",
      "Epoch 198/300\n",
      "Average training loss: 0.027177972990605568\n",
      "Average test loss: 0.0021767545050630967\n",
      "Epoch 199/300\n",
      "Average training loss: 0.027165576547384263\n",
      "Average test loss: 0.002157834041449759\n",
      "Epoch 200/300\n",
      "Average training loss: 0.027183414068486955\n",
      "Average test loss: 0.0021667615216639305\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02719819946752654\n",
      "Average test loss: 0.00222765451307512\n",
      "Epoch 202/300\n",
      "Average training loss: 0.027152548934022587\n",
      "Average test loss: 0.0021683739081232085\n",
      "Epoch 203/300\n",
      "Average training loss: 0.027138291973206732\n",
      "Average test loss: 0.002210165137425065\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0270854429817862\n",
      "Average test loss: 0.0022076227088562318\n",
      "Epoch 205/300\n",
      "Average training loss: 0.027059315121836132\n",
      "Average test loss: 0.002176002996870213\n",
      "Epoch 206/300\n",
      "Average training loss: 0.027044800716969703\n",
      "Average test loss: 0.0021777167131917345\n",
      "Epoch 207/300\n",
      "Average training loss: 0.027049015863074198\n",
      "Average test loss: 0.002232550855932964\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02699007320072916\n",
      "Average test loss: 0.002189119657501578\n",
      "Epoch 209/300\n",
      "Average training loss: 0.027038773809870083\n",
      "Average test loss: 0.0021575523138874105\n",
      "Epoch 210/300\n",
      "Average training loss: 0.027025149131814637\n",
      "Average test loss: 0.0021866400457090803\n",
      "Epoch 211/300\n",
      "Average training loss: 0.027024334400892257\n",
      "Average test loss: 0.0022181707403312128\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026952125976483027\n",
      "Average test loss: 0.002172106136671371\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02695387674288617\n",
      "Average test loss: 0.0022549697077936595\n",
      "Epoch 214/300\n",
      "Average training loss: 0.026917706502808465\n",
      "Average test loss: 0.0022896101458205116\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0269206686930524\n",
      "Average test loss: 0.0022211391292512417\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026922879340747993\n",
      "Average test loss: 0.0021924288742658166\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0269013835158613\n",
      "Average test loss: 0.0021840491054786575\n",
      "Epoch 218/300\n",
      "Average training loss: 0.026899837237265373\n",
      "Average test loss: 0.0022183121620780893\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026899438720610406\n",
      "Average test loss: 0.0021676662956467935\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026859958040217558\n",
      "Average test loss: 0.0022153084420909485\n",
      "Epoch 221/300\n",
      "Average training loss: 0.026907803977529208\n",
      "Average test loss: 0.002216632741917339\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026837985621558296\n",
      "Average test loss: 0.0021860012718372874\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026825356263253424\n",
      "Average test loss: 0.0021926751945995624\n",
      "Epoch 224/300\n",
      "Average training loss: 0.026824920172492665\n",
      "Average test loss: 0.0022319584250864053\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026754902597930695\n",
      "Average test loss: 0.003428274220062627\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02678654591076904\n",
      "Average test loss: 0.002215649284215437\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02680475503868527\n",
      "Average test loss: 0.002218304809803764\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02679335451291667\n",
      "Average test loss: 0.0022189737383483186\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026706761095258925\n",
      "Average test loss: 0.002253648768903481\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026710324852002993\n",
      "Average test loss: 0.0022205852079722616\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026718937524490886\n",
      "Average test loss: 0.002166737421001825\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02667625513838397\n",
      "Average test loss: 0.0022229290045797825\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02669734474685457\n",
      "Average test loss: 0.0021792739645267525\n",
      "Epoch 234/300\n",
      "Average training loss: 0.026694289619723956\n",
      "Average test loss: 0.0022426374976833663\n",
      "Epoch 235/300\n",
      "Average training loss: 0.026638309571478103\n",
      "Average test loss: 0.002212294832906789\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0266716429044803\n",
      "Average test loss: 0.0022433346509933474\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02665349180996418\n",
      "Average test loss: 0.0021873459193027683\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026622996047139167\n",
      "Average test loss: 0.0022331672337734037\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026633807568086518\n",
      "Average test loss: 0.0023213408647312058\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026596316628985935\n",
      "Average test loss: 0.0022270636132711336\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026568011350101894\n",
      "Average test loss: 0.002188875023068653\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026574517344435056\n",
      "Average test loss: 0.0022236885482238397\n",
      "Epoch 243/300\n",
      "Average training loss: 0.026628727649648985\n",
      "Average test loss: 0.0021601906141473187\n",
      "Epoch 244/300\n",
      "Average training loss: 0.026522725603646703\n",
      "Average test loss: 0.002202867312149869\n",
      "Epoch 245/300\n",
      "Average training loss: 0.026539753016498354\n",
      "Average test loss: 0.002217561235340933\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02651500453054905\n",
      "Average test loss: 0.002183444747908248\n",
      "Epoch 247/300\n",
      "Average training loss: 0.026496540867620044\n",
      "Average test loss: 0.002210149544187718\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026467648034294446\n",
      "Average test loss: 0.002165094765006668\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026496701300144195\n",
      "Average test loss: 0.0022268240940239692\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026453822844558293\n",
      "Average test loss: 0.0022308224849402905\n",
      "Epoch 251/300\n",
      "Average training loss: 0.026475075364112855\n",
      "Average test loss: 0.0022155845498459206\n",
      "Epoch 252/300\n",
      "Average training loss: 0.026458179694083\n",
      "Average test loss: 0.002303108802272214\n",
      "Epoch 253/300\n",
      "Average training loss: 0.026459116248620882\n",
      "Average test loss: 0.0021999941166076396\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026420116836826008\n",
      "Average test loss: 0.0022696091240892806\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02643174425760905\n",
      "Average test loss: 0.002214192075240943\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026415671658184793\n",
      "Average test loss: 0.0022134307965429294\n",
      "Epoch 257/300\n",
      "Average training loss: 0.026379914131429462\n",
      "Average test loss: 0.002210608480705155\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026342476747102207\n",
      "Average test loss: 0.0022048436750968297\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026428307693865564\n",
      "Average test loss: 0.002180954323046737\n",
      "Epoch 260/300\n",
      "Average training loss: 0.026359724637534884\n",
      "Average test loss: 0.0022231455486681727\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026324683095018068\n",
      "Average test loss: 0.0022282217173940607\n",
      "Epoch 262/300\n",
      "Average training loss: 0.026335963999231656\n",
      "Average test loss: 0.0021948770098388195\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02633399886224005\n",
      "Average test loss: 0.002192693337591158\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02635611109932264\n",
      "Average test loss: 0.0022321298723626468\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02631750416590108\n",
      "Average test loss: 0.0022263192754859726\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026335123530692523\n",
      "Average test loss: 0.0021662898554156225\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026268688041302894\n",
      "Average test loss: 0.002233262677470015\n",
      "Epoch 268/300\n",
      "Average training loss: 0.026315633214182323\n",
      "Average test loss: 0.0021885774104545516\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02627489180697335\n",
      "Average test loss: 0.0023095162957906725\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02628035175965892\n",
      "Average test loss: 0.0022054714659849803\n",
      "Epoch 271/300\n",
      "Average training loss: 0.026276643680201637\n",
      "Average test loss: 0.002271326010218925\n",
      "Epoch 272/300\n",
      "Average training loss: 0.026253058110674223\n",
      "Average test loss: 0.0022336282617309027\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02623408544063568\n",
      "Average test loss: 0.0022319673270814948\n",
      "Epoch 274/300\n",
      "Average training loss: 0.026280656043026183\n",
      "Average test loss: 0.0022094550031340784\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02622822428577476\n",
      "Average test loss: 0.002217841215017769\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02629515713453293\n",
      "Average test loss: 0.0022286143307056693\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02620024998486042\n",
      "Average test loss: 0.002253820620270239\n",
      "Epoch 278/300\n",
      "Average training loss: 0.026195838219589658\n",
      "Average test loss: 0.0022932524387207294\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02622111770013968\n",
      "Average test loss: 0.0022668928421205946\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026152439930372768\n",
      "Average test loss: 0.0022376934679018126\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02614286677042643\n",
      "Average test loss: 0.002229759485564298\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02613481871618165\n",
      "Average test loss: 0.0022391297469536464\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02615188852780395\n",
      "Average test loss: 0.002222124859690666\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026148021252618895\n",
      "Average test loss: 0.002244595052053531\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026106009282999568\n",
      "Average test loss: 0.0022168986892534626\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02613998691075378\n",
      "Average test loss: 0.00225350655356629\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026111680052346653\n",
      "Average test loss: 0.0022259281238334047\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02611806231737137\n",
      "Average test loss: 0.0022076973985466694\n",
      "Epoch 289/300\n",
      "Average training loss: 0.026068995944327777\n",
      "Average test loss: 0.0022122333782414595\n",
      "Epoch 290/300\n",
      "Average training loss: 0.026112645369437006\n",
      "Average test loss: 0.002270716808995025\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026023960000938838\n",
      "Average test loss: 0.002276952694170177\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02605815910630756\n",
      "Average test loss: 0.002280139750076665\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02607333145125045\n",
      "Average test loss: 0.0022805463874505626\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02606389023694727\n",
      "Average test loss: 0.0023424274255004194\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02599897816777229\n",
      "Average test loss: 0.002229772021787034\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02604573220676846\n",
      "Average test loss: 0.0022412137515428992\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025971918074621094\n",
      "Average test loss: 0.002245502969664004\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025990910251935322\n",
      "Average test loss: 0.0022354572748558388\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025978379590643776\n",
      "Average test loss: 0.0021992778163403273\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025991149855984583\n",
      "Average test loss: 0.0022185128084901305\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.6380870717366537\n",
      "Average test loss: 0.00434016429964039\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08348330173227522\n",
      "Average test loss: 0.003584825406057967\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06147156074643135\n",
      "Average test loss: 0.0032697169871793853\n",
      "Epoch 4/300\n",
      "Average training loss: 0.053248442558778655\n",
      "Average test loss: 0.003395036045875814\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04838052390681373\n",
      "Average test loss: 0.002921391015872359\n",
      "Epoch 6/300\n",
      "Average training loss: 0.045115368568234976\n",
      "Average test loss: 0.0027267307976467743\n",
      "Epoch 7/300\n",
      "Average training loss: 0.042648268514209324\n",
      "Average test loss: 0.0026511554467595288\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04069610736767451\n",
      "Average test loss: 0.0025923972541673315\n",
      "Epoch 9/300\n",
      "Average training loss: 0.039089176379972036\n",
      "Average test loss: 0.0024517848873510956\n",
      "Epoch 10/300\n",
      "Average training loss: 0.037546905630164676\n",
      "Average test loss: 0.002339915422308776\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03618868225812912\n",
      "Average test loss: 0.0023663246697849696\n",
      "Epoch 12/300\n",
      "Average training loss: 0.034993781379527515\n",
      "Average test loss: 0.0021809442612446017\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03379397262632847\n",
      "Average test loss: 0.00211940313078877\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03278387861284945\n",
      "Average test loss: 0.0020123599749058483\n",
      "Epoch 15/300\n",
      "Average training loss: 0.031890765926904145\n",
      "Average test loss: 0.001973089851025078\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03106043268740177\n",
      "Average test loss: 0.0019223900167788896\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03036097221573194\n",
      "Average test loss: 0.0019006729006974233\n",
      "Epoch 18/300\n",
      "Average training loss: 0.029749803650710317\n",
      "Average test loss: 0.00184991860224141\n",
      "Epoch 19/300\n",
      "Average training loss: 0.029174284093909795\n",
      "Average test loss: 0.0017966124048042628\n",
      "Epoch 20/300\n",
      "Average training loss: 0.028692434010406335\n",
      "Average test loss: 0.001764422512302796\n",
      "Epoch 21/300\n",
      "Average training loss: 0.028294979800780613\n",
      "Average test loss: 0.0017510409544532499\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02790605216887262\n",
      "Average test loss: 0.0017243998239023818\n",
      "Epoch 23/300\n",
      "Average training loss: 0.027545004276765716\n",
      "Average test loss: 0.0016916716946288944\n",
      "Epoch 24/300\n",
      "Average training loss: 0.027224924676948124\n",
      "Average test loss: 0.0016837778937899405\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02695489086624649\n",
      "Average test loss: 0.001711987844357888\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02665430055061976\n",
      "Average test loss: 0.001657257207462357\n",
      "Epoch 27/300\n",
      "Average training loss: 0.026463411963648265\n",
      "Average test loss: 0.0016662180320256286\n",
      "Epoch 28/300\n",
      "Average training loss: 0.026269349859820473\n",
      "Average test loss: 0.0016201804423083862\n",
      "Epoch 29/300\n",
      "Average training loss: 0.026021734202901523\n",
      "Average test loss: 0.0016127205768393146\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02582225825223658\n",
      "Average test loss: 0.001603650304592318\n",
      "Epoch 31/300\n",
      "Average training loss: 0.025655865454839336\n",
      "Average test loss: 0.001594025439893206\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02548314085106055\n",
      "Average test loss: 0.001595144946542051\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02532294542921914\n",
      "Average test loss: 0.001579264202879535\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02519307270977232\n",
      "Average test loss: 0.0015777205020292766\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02501433202293184\n",
      "Average test loss: 0.0015653664595964881\n",
      "Epoch 36/300\n",
      "Average training loss: 0.024950631802280743\n",
      "Average test loss: 0.0015603384625994497\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024793275870382787\n",
      "Average test loss: 0.0015433827661391762\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024658317176832093\n",
      "Average test loss: 0.0015401618834584951\n",
      "Epoch 39/300\n",
      "Average training loss: 0.024525584919585123\n",
      "Average test loss: 0.0015455173421651126\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02448763135075569\n",
      "Average test loss: 0.0015418073282473617\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02431522274348471\n",
      "Average test loss: 0.0015292366559410261\n",
      "Epoch 42/300\n",
      "Average training loss: 0.024246926719943683\n",
      "Average test loss: 0.001514694307309886\n",
      "Epoch 43/300\n",
      "Average training loss: 0.024156008824706077\n",
      "Average test loss: 0.0015154189695604145\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02412853754725721\n",
      "Average test loss: 0.001503092797473073\n",
      "Epoch 45/300\n",
      "Average training loss: 0.023968508718742263\n",
      "Average test loss: 0.0015064486327270668\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023867463929785622\n",
      "Average test loss: 0.0014951157773741418\n",
      "Epoch 47/300\n",
      "Average training loss: 0.023787386738591723\n",
      "Average test loss: 0.001531016213612424\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023701328948140143\n",
      "Average test loss: 0.001490090105475651\n",
      "Epoch 49/300\n",
      "Average training loss: 0.023651385941439205\n",
      "Average test loss: 0.0014985899103598461\n",
      "Epoch 50/300\n",
      "Average training loss: 0.023598905361360972\n",
      "Average test loss: 0.0014965703592946132\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02349592461188634\n",
      "Average test loss: 0.001485493494818608\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023465505863229433\n",
      "Average test loss: 0.0015009890320814318\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023376238904065556\n",
      "Average test loss: 0.0014864002269589238\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023344223875138494\n",
      "Average test loss: 0.0014811533737099833\n",
      "Epoch 55/300\n",
      "Average training loss: 0.023259704617990387\n",
      "Average test loss: 0.0014701040236072408\n",
      "Epoch 56/300\n",
      "Average training loss: 0.023205467167827818\n",
      "Average test loss: 0.0014755271748743124\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023097989247904883\n",
      "Average test loss: 0.0014725977418323358\n",
      "Epoch 58/300\n",
      "Average training loss: 0.023043931274778315\n",
      "Average test loss: 0.0014688134112705788\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02300746412575245\n",
      "Average test loss: 0.0015203262005622188\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02296876564787494\n",
      "Average test loss: 0.001461046828991837\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022904755587379137\n",
      "Average test loss: 0.0014785255638158155\n",
      "Epoch 62/300\n",
      "Average training loss: 0.022825448484884367\n",
      "Average test loss: 0.0014621606160783104\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0227946527534061\n",
      "Average test loss: 0.0014814343058193724\n",
      "Epoch 64/300\n",
      "Average training loss: 0.022768664570318328\n",
      "Average test loss: 0.0014606504749713673\n",
      "Epoch 65/300\n",
      "Average training loss: 0.022689310509297584\n",
      "Average test loss: 0.0014678482019032041\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02263896839817365\n",
      "Average test loss: 0.0014732843578482667\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02259348651766777\n",
      "Average test loss: 0.0014709898504532046\n",
      "Epoch 68/300\n",
      "Average training loss: 0.022529092007213168\n",
      "Average test loss: 0.001454119808351\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022473733234736653\n",
      "Average test loss: 0.0014564692983403801\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02246126150422626\n",
      "Average test loss: 0.0014584990283474325\n",
      "Epoch 71/300\n",
      "Average training loss: 0.022399997956222956\n",
      "Average test loss: 0.0014503857862307793\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022366290726595454\n",
      "Average test loss: 0.001453444076391558\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022287420685092606\n",
      "Average test loss: 0.0014458739669062197\n",
      "Epoch 74/300\n",
      "Average training loss: 0.022241361449162167\n",
      "Average test loss: 0.0014632999296817514\n",
      "Epoch 75/300\n",
      "Average training loss: 0.022209452981750172\n",
      "Average test loss: 0.0014502545564642384\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02216318876379066\n",
      "Average test loss: 0.0015060386024415492\n",
      "Epoch 77/300\n",
      "Average training loss: 0.022111130661434598\n",
      "Average test loss: 0.0014506597327482368\n",
      "Epoch 78/300\n",
      "Average training loss: 0.022076499336295657\n",
      "Average test loss: 0.0014605693829556306\n",
      "Epoch 79/300\n",
      "Average training loss: 0.022039925237496694\n",
      "Average test loss: 0.001564736447090076\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02203617915345563\n",
      "Average test loss: 0.0014617363778460356\n",
      "Epoch 81/300\n",
      "Average training loss: 0.021943958835469353\n",
      "Average test loss: 0.001461980421613488\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0219074497371912\n",
      "Average test loss: 0.001464836894431048\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02187540126260784\n",
      "Average test loss: 0.0014496342982682918\n",
      "Epoch 84/300\n",
      "Average training loss: 0.021864150621824795\n",
      "Average test loss: 0.001456176239479747\n",
      "Epoch 85/300\n",
      "Average training loss: 0.021779173074497116\n",
      "Average test loss: 0.0014552073563552565\n",
      "Epoch 86/300\n",
      "Average training loss: 0.021774959594011305\n",
      "Average test loss: 0.0014383918919290105\n",
      "Epoch 87/300\n",
      "Average training loss: 0.021730035462313228\n",
      "Average test loss: 0.0014445801831574903\n",
      "Epoch 88/300\n",
      "Average training loss: 0.021687610644433232\n",
      "Average test loss: 0.0014477281764977508\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02165529025097688\n",
      "Average test loss: 0.001463406145055261\n",
      "Epoch 90/300\n",
      "Average training loss: 0.021605521445473036\n",
      "Average test loss: 0.0014491662587970495\n",
      "Epoch 91/300\n",
      "Average training loss: 0.021569776735372012\n",
      "Average test loss: 0.0014351745914253924\n",
      "Epoch 92/300\n",
      "Average training loss: 0.021542374471823375\n",
      "Average test loss: 0.0014489706094480224\n",
      "Epoch 93/300\n",
      "Average training loss: 0.021492135451899635\n",
      "Average test loss: 0.0014497734110595452\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02146287252340052\n",
      "Average test loss: 0.0014491986023883025\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02142806386699279\n",
      "Average test loss: 0.0014573182039376762\n",
      "Epoch 96/300\n",
      "Average training loss: 0.021430325103302797\n",
      "Average test loss: 0.0014521162219138608\n",
      "Epoch 97/300\n",
      "Average training loss: 0.021357687471641434\n",
      "Average test loss: 0.0014779040557849738\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02132830130226082\n",
      "Average test loss: 0.0014456133408885863\n",
      "Epoch 99/300\n",
      "Average training loss: 0.021309922138849895\n",
      "Average test loss: 0.0014519380434519715\n",
      "Epoch 100/300\n",
      "Average training loss: 0.021317014841569795\n",
      "Average test loss: 0.0014603947990884383\n",
      "Epoch 101/300\n",
      "Average training loss: 0.021241510528657172\n",
      "Average test loss: 0.0014586865120670863\n",
      "Epoch 102/300\n",
      "Average training loss: 0.021190247683061495\n",
      "Average test loss: 0.001447263797124227\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02115607450240188\n",
      "Average test loss: 0.0014601623218299614\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02115197929408815\n",
      "Average test loss: 0.0014441637948362364\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02109529980354839\n",
      "Average test loss: 0.0014655518283446629\n",
      "Epoch 106/300\n",
      "Average training loss: 0.021063431379695734\n",
      "Average test loss: 0.0014609277733187709\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02103972163465288\n",
      "Average test loss: 0.001453100581549936\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02102130220996009\n",
      "Average test loss: 0.0014841142718990644\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02099353516846895\n",
      "Average test loss: 0.0014669419500148958\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02097999764730533\n",
      "Average test loss: 0.0014732527742162347\n",
      "Epoch 111/300\n",
      "Average training loss: 0.020931596654984685\n",
      "Average test loss: 0.0014685857587804396\n",
      "Epoch 112/300\n",
      "Average training loss: 0.020900627376304732\n",
      "Average test loss: 0.0014498167862701747\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02088630019625028\n",
      "Average test loss: 0.0014538866760623124\n",
      "Epoch 114/300\n",
      "Average training loss: 0.020871864780783653\n",
      "Average test loss: 0.0014721578052267431\n",
      "Epoch 115/300\n",
      "Average training loss: 0.020804770266844165\n",
      "Average test loss: 0.0014492522932382094\n",
      "Epoch 116/300\n",
      "Average training loss: 0.020783201091819338\n",
      "Average test loss: 0.0014595403012095227\n",
      "Epoch 117/300\n",
      "Average training loss: 0.020755922633740636\n",
      "Average test loss: 0.0014796453166960014\n",
      "Epoch 118/300\n",
      "Average training loss: 0.020730008027619786\n",
      "Average test loss: 0.0014673776591403616\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02071025545232826\n",
      "Average test loss: 0.0014725136736200915\n",
      "Epoch 120/300\n",
      "Average training loss: 0.020664901942014695\n",
      "Average test loss: 0.0014518853281107213\n",
      "Epoch 121/300\n",
      "Average training loss: 0.020629855529301695\n",
      "Average test loss: 0.0014781618791942794\n",
      "Epoch 122/300\n",
      "Average training loss: 0.020623224238554637\n",
      "Average test loss: 0.001445312683780988\n",
      "Epoch 123/300\n",
      "Average training loss: 0.020597946057717004\n",
      "Average test loss: 0.0014512178955806627\n",
      "Epoch 124/300\n",
      "Average training loss: 0.020553191781044006\n",
      "Average test loss: 0.0014642946794629096\n",
      "Epoch 125/300\n",
      "Average training loss: 0.020582341820001602\n",
      "Average test loss: 0.001471295616692967\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02049093760218885\n",
      "Average test loss: 0.0014615172002878454\n",
      "Epoch 127/300\n",
      "Average training loss: 0.020510916713211272\n",
      "Average test loss: 0.0014614168401393625\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02046495478683048\n",
      "Average test loss: 0.0015083716113327278\n",
      "Epoch 129/300\n",
      "Average training loss: 0.020475561920139523\n",
      "Average test loss: 0.001465364569487671\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02056419472148021\n",
      "Average test loss: 0.0015012887335485881\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02038673543350564\n",
      "Average test loss: 0.001455837706828283\n",
      "Epoch 132/300\n",
      "Average training loss: 0.020369498181674217\n",
      "Average test loss: 0.0014706427454948426\n",
      "Epoch 133/300\n",
      "Average training loss: 0.020397672312955063\n",
      "Average test loss: 0.0014975597345166737\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02033631723622481\n",
      "Average test loss: 0.0014577960814866754\n",
      "Epoch 135/300\n",
      "Average training loss: 0.020293293059700065\n",
      "Average test loss: 0.0014653631432188883\n",
      "Epoch 136/300\n",
      "Average training loss: 0.020284162508116828\n",
      "Average test loss: 0.0014636591049946017\n",
      "Epoch 137/300\n",
      "Average training loss: 0.020245030489232804\n",
      "Average test loss: 0.0014736078941366738\n",
      "Epoch 138/300\n",
      "Average training loss: 0.020234362208180957\n",
      "Average test loss: 0.0014663700078510575\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02025466023385525\n",
      "Average test loss: 0.0014709309506643977\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02019206066429615\n",
      "Average test loss: 0.0014793423101719884\n",
      "Epoch 141/300\n",
      "Average training loss: 0.020159578796062203\n",
      "Average test loss: 0.0014734084947655597\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02016206651677688\n",
      "Average test loss: 0.0014786044148107369\n",
      "Epoch 143/300\n",
      "Average training loss: 0.020142197446690664\n",
      "Average test loss: 0.0014882247026802764\n",
      "Epoch 144/300\n",
      "Average training loss: 0.020126233003619643\n",
      "Average test loss: 0.0015043100509792566\n",
      "Epoch 145/300\n",
      "Average training loss: 0.020119199365377427\n",
      "Average test loss: 0.0014890231623624762\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02009172247846921\n",
      "Average test loss: 0.0014712798015938866\n",
      "Epoch 147/300\n",
      "Average training loss: 0.020055013527472815\n",
      "Average test loss: 0.0014755785291393599\n",
      "Epoch 148/300\n",
      "Average training loss: 0.020041168060567643\n",
      "Average test loss: 0.001482875769233538\n",
      "Epoch 149/300\n",
      "Average training loss: 0.020002808910277156\n",
      "Average test loss: 0.0014829993731238775\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019997137223680815\n",
      "Average test loss: 0.0015583698700906501\n",
      "Epoch 151/300\n",
      "Average training loss: 0.019999415929118792\n",
      "Average test loss: 0.0015668599042627546\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019948052699367204\n",
      "Average test loss: 0.001465432892450028\n",
      "Epoch 153/300\n",
      "Average training loss: 0.019914874533812205\n",
      "Average test loss: 0.0015410857275128364\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019928326370815437\n",
      "Average test loss: 0.0014928425360057089\n",
      "Epoch 155/300\n",
      "Average training loss: 0.019897742105854883\n",
      "Average test loss: 0.0014749631403634946\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01989948562781016\n",
      "Average test loss: 0.001488583832875722\n",
      "Epoch 157/300\n",
      "Average training loss: 0.019844061742226284\n",
      "Average test loss: 0.0015055021153142056\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019832070197496148\n",
      "Average test loss: 0.0014963089241128828\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019824236454235183\n",
      "Average test loss: 0.0014782000077474449\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019792108232776325\n",
      "Average test loss: 0.001474155696729819\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019819001994199222\n",
      "Average test loss: 0.0014806413013074133\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019794997917281256\n",
      "Average test loss: 0.0014956661636113292\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019764344389239945\n",
      "Average test loss: 0.0014953268650505277\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019749980732798576\n",
      "Average test loss: 0.001486450328077707\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019708206370472908\n",
      "Average test loss: 0.0015174750359728932\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019705798929764165\n",
      "Average test loss: 0.0014739655587408277\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019681500910884803\n",
      "Average test loss: 0.0015198407612947954\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019670748552514446\n",
      "Average test loss: 0.0015020247573653857\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019662079917887848\n",
      "Average test loss: 0.001520478597893897\n",
      "Epoch 170/300\n",
      "Average training loss: 0.019641167713536155\n",
      "Average test loss: 0.0015114014144572948\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01963304068479273\n",
      "Average test loss: 0.0014868316690747937\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01958507693476147\n",
      "Average test loss: 0.0014741850425489247\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01959995635516114\n",
      "Average test loss: 0.0015030904322241743\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019585564527246687\n",
      "Average test loss: 0.001484255341709488\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019553723398182128\n",
      "Average test loss: 0.0014851666679088441\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019520095016393398\n",
      "Average test loss: 0.0015181706622242927\n",
      "Epoch 177/300\n",
      "Average training loss: 0.019514742413328755\n",
      "Average test loss: 0.001508604560357829\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01954722253812684\n",
      "Average test loss: 0.0014831548317645986\n",
      "Epoch 179/300\n",
      "Average training loss: 0.019507677250438265\n",
      "Average test loss: 0.0014942950937483046\n",
      "Epoch 180/300\n",
      "Average training loss: 0.019507397547364237\n",
      "Average test loss: 0.0014991565984156397\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019455254862705867\n",
      "Average test loss: 0.0015153321182976166\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019449707647164662\n",
      "Average test loss: 0.001626291124874519\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019458063615692987\n",
      "Average test loss: 0.0014881542987293668\n",
      "Epoch 184/300\n",
      "Average training loss: 0.019421293968955675\n",
      "Average test loss: 0.0015147774904552434\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01942906408591403\n",
      "Average test loss: 0.0015171395211170118\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01941377569569482\n",
      "Average test loss: 0.0015253008200476568\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019379127633240487\n",
      "Average test loss: 0.0014969405115375087\n",
      "Epoch 188/300\n",
      "Average training loss: 0.019372323325938648\n",
      "Average test loss: 0.0015215828819200397\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019335518174701267\n",
      "Average test loss: 0.0015384019894732369\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019356454461812972\n",
      "Average test loss: 0.001511026838225209\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019334098493059476\n",
      "Average test loss: 0.0015129097227214111\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01933000101149082\n",
      "Average test loss: 0.0015309461594248811\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01930694317817688\n",
      "Average test loss: 0.001517319163100587\n",
      "Epoch 194/300\n",
      "Average training loss: 0.019290849404202566\n",
      "Average test loss: 0.0014962311598161857\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019310767484207948\n",
      "Average test loss: 0.0015940044226331843\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019269644147819943\n",
      "Average test loss: 0.0014979960871342984\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01926720198823346\n",
      "Average test loss: 0.0015104637561986843\n",
      "Epoch 198/300\n",
      "Average training loss: 0.019244328585763772\n",
      "Average test loss: 0.00151783646001584\n",
      "Epoch 199/300\n",
      "Average training loss: 0.019220845410393345\n",
      "Average test loss: 0.001516868953127414\n",
      "Epoch 200/300\n",
      "Average training loss: 0.019200219339794582\n",
      "Average test loss: 0.0014995022747769124\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01922101418673992\n",
      "Average test loss: 0.0015051138450702032\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01919604973991712\n",
      "Average test loss: 0.0015393964120497307\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01918920617302259\n",
      "Average test loss: 0.001505655830932988\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019169418912794854\n",
      "Average test loss: 0.0015429281457844707\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019144964348110888\n",
      "Average test loss: 0.0015343629664017094\n",
      "Epoch 206/300\n",
      "Average training loss: 0.019150116531385316\n",
      "Average test loss: 0.0015124891015390555\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019122250167859924\n",
      "Average test loss: 0.0015007256376039651\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019111588302585813\n",
      "Average test loss: 0.0015093329585571255\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019113338952263196\n",
      "Average test loss: 0.0015124884825199842\n",
      "Epoch 210/300\n",
      "Average training loss: 0.019080337092280387\n",
      "Average test loss: 0.0015428972569190794\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019082469244798023\n",
      "Average test loss: 0.0015184021812553207\n",
      "Epoch 212/300\n",
      "Average training loss: 0.019063103957308664\n",
      "Average test loss: 0.0016228157936905821\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019059973945220313\n",
      "Average test loss: 0.0015706638410273525\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01906258257975181\n",
      "Average test loss: 0.0015128006795421243\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0190329865200652\n",
      "Average test loss: 0.0015259105101641682\n",
      "Epoch 216/300\n",
      "Average training loss: 0.019044269932640922\n",
      "Average test loss: 0.001566068964285983\n",
      "Epoch 217/300\n",
      "Average training loss: 0.018993124218450654\n",
      "Average test loss: 0.001512770701924132\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01901538567327791\n",
      "Average test loss: 0.0015391349341306421\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018989563802878064\n",
      "Average test loss: 0.0015139333030415906\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01897646788093779\n",
      "Average test loss: 0.0015257672032134401\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01895838706360923\n",
      "Average test loss: 0.0015193271949990757\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01900062968664699\n",
      "Average test loss: 0.0015413512260549597\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01899507118595971\n",
      "Average test loss: 0.0015225326703964836\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01893611705634329\n",
      "Average test loss: 0.0015431324752668539\n",
      "Epoch 225/300\n",
      "Average training loss: 0.018925224891967244\n",
      "Average test loss: 0.0015028283920966916\n",
      "Epoch 226/300\n",
      "Average training loss: 0.018932352114054894\n",
      "Average test loss: 0.0015159839612121384\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018923951713575256\n",
      "Average test loss: 0.0015339384813689524\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01887084945042928\n",
      "Average test loss: 0.0015470624930328794\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0188863535374403\n",
      "Average test loss: 0.0016104484355698029\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018865466548336877\n",
      "Average test loss: 0.0015181140781690677\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018888069078326224\n",
      "Average test loss: 0.001578391171163983\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01886039151665237\n",
      "Average test loss: 0.0015441018601672516\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018837187823322086\n",
      "Average test loss: 0.0015158084935198227\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018859151313702267\n",
      "Average test loss: 0.0015592613669319286\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018836608188019858\n",
      "Average test loss: 0.0015548768328088854\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018818561618526776\n",
      "Average test loss: 0.0015116929956194428\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01881048677364985\n",
      "Average test loss: 0.00151899062241945\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01882042794012361\n",
      "Average test loss: 0.0015650528281306226\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01881017435259289\n",
      "Average test loss: 0.0016210959778270788\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01881089495536354\n",
      "Average test loss: 0.0015304085738543007\n",
      "Epoch 241/300\n",
      "Average training loss: 0.018778904626766842\n",
      "Average test loss: 0.0015123574980017212\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018755887010031277\n",
      "Average test loss: 0.0015850167078897356\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01876734918769863\n",
      "Average test loss: 0.0015378608285552926\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018787828594446183\n",
      "Average test loss: 0.0015177485993545915\n",
      "Epoch 245/300\n",
      "Average training loss: 0.018732937129007445\n",
      "Average test loss: 0.0015661747997833622\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018718667406174873\n",
      "Average test loss: 0.0015386945258619056\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018717014119029044\n",
      "Average test loss: 0.0015678009754046798\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01871925662126806\n",
      "Average test loss: 0.0015140163255855442\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018700645761357413\n",
      "Average test loss: 0.0015329139491336214\n",
      "Epoch 250/300\n",
      "Average training loss: 0.018702683607737223\n",
      "Average test loss: 0.001578547201015883\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018715648255414434\n",
      "Average test loss: 0.0015632405829512411\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018684519689944057\n",
      "Average test loss: 0.0015107552708747485\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018695809211995866\n",
      "Average test loss: 0.0016987190981292062\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018652441141506035\n",
      "Average test loss: 0.0015370069994694656\n",
      "Epoch 255/300\n",
      "Average training loss: 0.018627339551846187\n",
      "Average test loss: 0.0015723177211152183\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018645047772261832\n",
      "Average test loss: 0.001548721725328101\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018639498116241562\n",
      "Average test loss: 0.0015385515613274441\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018637899859084022\n",
      "Average test loss: 0.0015609608777902193\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018608027507861457\n",
      "Average test loss: 0.001546577157970104\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0186163797436489\n",
      "Average test loss: 0.0018305740320227212\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018645881011254258\n",
      "Average test loss: 0.0015101480527470509\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018599370138512716\n",
      "Average test loss: 0.001535804140691956\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01859401631520854\n",
      "Average test loss: 0.001529602712020278\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01858935718735059\n",
      "Average test loss: 0.0015200250673418243\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01855680934091409\n",
      "Average test loss: 0.0015838977362339695\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018561320919129584\n",
      "Average test loss: 0.0015795869570121997\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018588225208222866\n",
      "Average test loss: 0.001582217793032113\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01853691803581185\n",
      "Average test loss: 0.0015603999040193029\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0185988347315126\n",
      "Average test loss: 0.0015824407013133167\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018530890681677393\n",
      "Average test loss: 0.0015312871132045984\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018546468021969\n",
      "Average test loss: 0.001548774774496754\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018506943294571504\n",
      "Average test loss: 0.001551919653908246\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018513689663675097\n",
      "Average test loss: 0.001565561404865649\n",
      "Epoch 274/300\n",
      "Average training loss: 0.018525222177306813\n",
      "Average test loss: 0.0015666239709696836\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018499984802471267\n",
      "Average test loss: 0.0015603894038746754\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018542596417996618\n",
      "Average test loss: 0.0015529523010158705\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018468065443966123\n",
      "Average test loss: 0.0016160225133515066\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018485692487822638\n",
      "Average test loss: 0.0015538465571072366\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018474809024069044\n",
      "Average test loss: 0.0015654354695644644\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01847684951954418\n",
      "Average test loss: 0.0015336425669698252\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018447361182835367\n",
      "Average test loss: 0.0015517423642385337\n",
      "Epoch 282/300\n",
      "Average training loss: 0.018440987014108234\n",
      "Average test loss: 0.0016340886388077503\n",
      "Epoch 283/300\n",
      "Average training loss: 0.018474928281373447\n",
      "Average test loss: 0.0016210249312635925\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018450934872031213\n",
      "Average test loss: 0.0015376668719367847\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01844187216129568\n",
      "Average test loss: 0.0015572341063784228\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01844432156946924\n",
      "Average test loss: 0.0015276508807308144\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01843286396149132\n",
      "Average test loss: 0.0015859367818468148\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01844690414435334\n",
      "Average test loss: 0.0015486797015700075\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0184107380674945\n",
      "Average test loss: 0.0015764271919098166\n",
      "Epoch 290/300\n",
      "Average training loss: 0.018393554478883745\n",
      "Average test loss: 0.0016146262171160843\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01838593065324757\n",
      "Average test loss: 0.0016114592165168788\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01839382069392337\n",
      "Average test loss: 0.0015609057282822\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0184223957285285\n",
      "Average test loss: 0.0015412243879917595\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01836710418926345\n",
      "Average test loss: 0.0015314075648267236\n",
      "Epoch 295/300\n",
      "Average training loss: 0.018385091823008324\n",
      "Average test loss: 0.0015515140848648217\n",
      "Epoch 296/300\n",
      "Average training loss: 0.018370531748566364\n",
      "Average test loss: 0.0015628544615788591\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018364191484120156\n",
      "Average test loss: 0.0016129178166803385\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018350812726550633\n",
      "Average test loss: 0.0015444746452073256\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018340059146285056\n",
      "Average test loss: 0.0015466167490101523\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018326075056360826\n",
      "Average test loss: 0.001542870726229416\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_50_Depth3/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.62\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.25\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.69\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.70\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.93\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.92\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.85\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.48\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.67\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.73\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 33.07\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.37\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.44\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.59\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.72\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.73\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.75\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.1606845117145115\n",
      "Average test loss: 0.01731794699612591\n",
      "Epoch 2/300\n",
      "Average training loss: 1.3112437380684747\n",
      "Average test loss: 0.0050169256544775435\n",
      "Epoch 3/300\n",
      "Average training loss: 0.6424555864334106\n",
      "Average test loss: 0.004831111596276362\n",
      "Epoch 4/300\n",
      "Average training loss: 0.4276854136519962\n",
      "Average test loss: 0.004862756302580237\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3225175467597114\n",
      "Average test loss: 0.004678793730007277\n",
      "Epoch 6/300\n",
      "Average training loss: 0.25656428082784016\n",
      "Average test loss: 0.004577660752874282\n",
      "Epoch 7/300\n",
      "Average training loss: 0.21257474099265206\n",
      "Average test loss: 0.004465760550151268\n",
      "Epoch 8/300\n",
      "Average training loss: 0.18610161481963264\n",
      "Average test loss: 0.004454413044369883\n",
      "Epoch 9/300\n",
      "Average training loss: 0.16956200431452856\n",
      "Average test loss: 0.004421705493496524\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15916111369927724\n",
      "Average test loss: 0.004412677036391364\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1520701220764054\n",
      "Average test loss: 0.004387136860854096\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14688597966565026\n",
      "Average test loss: 0.004323898065835238\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1429821776681476\n",
      "Average test loss: 0.0044550977879100374\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1398672157658471\n",
      "Average test loss: 0.004309840066979329\n",
      "Epoch 15/300\n",
      "Average training loss: 0.13737550611628427\n",
      "Average test loss: 0.004295242450303501\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13537247143189113\n",
      "Average test loss: 0.0042343108732667235\n",
      "Epoch 17/300\n",
      "Average training loss: 0.133577665746212\n",
      "Average test loss: 0.0042054057829082015\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1321910867161221\n",
      "Average test loss: 0.004186708896110455\n",
      "Epoch 19/300\n",
      "Average training loss: 0.13080067241191864\n",
      "Average test loss: 0.004177915131466256\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1298663632803493\n",
      "Average test loss: 0.004212883034514056\n",
      "Epoch 21/300\n",
      "Average training loss: 0.12881243851449756\n",
      "Average test loss: 0.004158578489803605\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12797864713933732\n",
      "Average test loss: 0.004164324693795707\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12726872840192582\n",
      "Average test loss: 0.004168454878653089\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1266602853735288\n",
      "Average test loss: 0.004121182819207509\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12599912016921574\n",
      "Average test loss: 0.004131805448689394\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12542454192373487\n",
      "Average test loss: 0.0041354126810199685\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12494983708196217\n",
      "Average test loss: 0.004100014491006732\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1243648735748397\n",
      "Average test loss: 0.004100838972048627\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1240683561232355\n",
      "Average test loss: 0.0040662693770395384\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12362704202201631\n",
      "Average test loss: 0.0040702773113217615\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12316739681694243\n",
      "Average test loss: 0.004059704591002729\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12285789326826731\n",
      "Average test loss: 0.004033849923561017\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12250203220380677\n",
      "Average test loss: 0.004066418298830588\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12207867340246836\n",
      "Average test loss: 0.004114998290936152\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12168448281288147\n",
      "Average test loss: 0.004033996058834924\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12139255160755581\n",
      "Average test loss: 0.0040220819467471705\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12104141320784886\n",
      "Average test loss: 0.004034114602953195\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12085731800397237\n",
      "Average test loss: 0.004014939318514533\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12054143149322934\n",
      "Average test loss: 0.004021951359800166\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12035001177920236\n",
      "Average test loss: 0.003987992061095106\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1200056913892428\n",
      "Average test loss: 0.004000067551516824\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1197995886736446\n",
      "Average test loss: 0.003977476149383518\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11952229558759266\n",
      "Average test loss: 0.004013772124217616\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11934989670250151\n",
      "Average test loss: 0.00400781530348791\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11910828623506758\n",
      "Average test loss: 0.004001392535037464\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1189567839635743\n",
      "Average test loss: 0.003993143808303607\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11869093359178967\n",
      "Average test loss: 0.003973510502734118\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11852537031968435\n",
      "Average test loss: 0.003955972170043323\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11830994568930732\n",
      "Average test loss: 0.003953477586102154\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11810631569226583\n",
      "Average test loss: 0.003962478346087866\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11795742454793719\n",
      "Average test loss: 0.003952144073943297\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11778234409623675\n",
      "Average test loss: 0.00394710818450484\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11767234706216388\n",
      "Average test loss: 0.003991639383551147\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11753866569863425\n",
      "Average test loss: 0.003959022861801916\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11744811109039519\n",
      "Average test loss: 0.003945202378142211\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11729899072647094\n",
      "Average test loss: 0.0040266643149985204\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11717980784840054\n",
      "Average test loss: 0.003963798808347848\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11706268993351195\n",
      "Average test loss: 0.0040075952663189834\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11697952226797739\n",
      "Average test loss: 0.0039281536945038375\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11677789786126878\n",
      "Average test loss: 0.003936567276302311\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1166909807920456\n",
      "Average test loss: 0.003932913229283359\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11667177718215518\n",
      "Average test loss: 0.003930453293439415\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11638013344340854\n",
      "Average test loss: 0.0039235212397244245\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11639028117391798\n",
      "Average test loss: 0.003929480196287235\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1162656696372562\n",
      "Average test loss: 0.0039200146003729765\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11607333121697108\n",
      "Average test loss: 0.003909805244455735\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11603195835484399\n",
      "Average test loss: 0.0039254627827968866\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11595165350702073\n",
      "Average test loss: 0.00391600398802095\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1158699873222245\n",
      "Average test loss: 0.0039171755247645905\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11570110695229636\n",
      "Average test loss: 0.003942811267657413\n",
      "Epoch 71/300\n",
      "Average training loss: 0.1156764147149192\n",
      "Average test loss: 0.003916345469239685\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11549689490265316\n",
      "Average test loss: 0.003917624716543489\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1154965986278322\n",
      "Average test loss: 0.003933735294474496\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11535518861148092\n",
      "Average test loss: 0.0039075338103704985\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11525191709068086\n",
      "Average test loss: 0.003906577330910497\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1151359114382002\n",
      "Average test loss: 0.003911869143032366\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11509447749786907\n",
      "Average test loss: 0.003906357174532281\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1149454034500652\n",
      "Average test loss: 0.003905441633529133\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11482542330688901\n",
      "Average test loss: 0.003915624339133501\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11469673102431827\n",
      "Average test loss: 0.003911285522083441\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11464265620046192\n",
      "Average test loss: 0.0038895846234841477\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11452685503164928\n",
      "Average test loss: 0.003913920289733344\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11446472209029727\n",
      "Average test loss: 0.0038986615257130728\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11437409439351824\n",
      "Average test loss: 0.003907622303399774\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11430474140908983\n",
      "Average test loss: 0.003920625177522501\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11422321837478214\n",
      "Average test loss: 0.00390940832429462\n",
      "Epoch 87/300\n",
      "Average training loss: 0.1141387632422977\n",
      "Average test loss: 0.0039041671972307895\n",
      "Epoch 88/300\n",
      "Average training loss: 0.1140730974872907\n",
      "Average test loss: 0.0039052002045015495\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11390928857194053\n",
      "Average test loss: 0.0039179439168009495\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11379371490412288\n",
      "Average test loss: 0.003946847289386723\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11374595687124464\n",
      "Average test loss: 0.0039112655013385745\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1136368541651302\n",
      "Average test loss: 0.003936297050366799\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11361191059483422\n",
      "Average test loss: 0.003904793958697054\n",
      "Epoch 94/300\n",
      "Average training loss: 0.113467438240846\n",
      "Average test loss: 0.0039045357201248406\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11336652162339952\n",
      "Average test loss: 0.003925552615481946\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11324603899319967\n",
      "Average test loss: 0.003919998792724477\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11329059447182549\n",
      "Average test loss: 0.0038939508957167466\n",
      "Epoch 98/300\n",
      "Average training loss: 0.11312791972690159\n",
      "Average test loss: 0.00392067004657454\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11294584001435173\n",
      "Average test loss: 0.00391533642510573\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11292583119869232\n",
      "Average test loss: 0.003898273634620839\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1128889346188969\n",
      "Average test loss: 0.003910622429930502\n",
      "Epoch 102/300\n",
      "Average training loss: 0.11280572113725874\n",
      "Average test loss: 0.003909801073786285\n",
      "Epoch 103/300\n",
      "Average training loss: 0.11261090975337558\n",
      "Average test loss: 0.003905621711164713\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11252617953883277\n",
      "Average test loss: 0.003918226405150361\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11238533186912536\n",
      "Average test loss: 0.0039031441025435924\n",
      "Epoch 106/300\n",
      "Average training loss: 0.1124115272892846\n",
      "Average test loss: 0.0039448541651169455\n",
      "Epoch 107/300\n",
      "Average training loss: 0.1122133104801178\n",
      "Average test loss: 0.003935411947054996\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11224465709924698\n",
      "Average test loss: 0.0038989390950236054\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11214254409074784\n",
      "Average test loss: 0.003957217892424928\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11196986916992399\n",
      "Average test loss: 0.003910796573385596\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11190506013234457\n",
      "Average test loss: 0.00392746300322728\n",
      "Epoch 112/300\n",
      "Average training loss: 0.11171798884868622\n",
      "Average test loss: 0.004924325922297107\n",
      "Epoch 113/300\n",
      "Average training loss: 0.11181901888714896\n",
      "Average test loss: 0.003915002045532068\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11153637064827814\n",
      "Average test loss: 0.0039093814388745365\n",
      "Epoch 115/300\n",
      "Average training loss: 0.1114355427424113\n",
      "Average test loss: 0.003910890094521973\n",
      "Epoch 116/300\n",
      "Average training loss: 0.11134945489300623\n",
      "Average test loss: 0.004045628749248054\n",
      "Epoch 117/300\n",
      "Average training loss: 0.11138035145733091\n",
      "Average test loss: 0.003915111719734139\n",
      "Epoch 118/300\n",
      "Average training loss: 0.11130724818838968\n",
      "Average test loss: 0.0039306930378079415\n",
      "Epoch 119/300\n",
      "Average training loss: 0.1109599242872662\n",
      "Average test loss: 0.003974419661694103\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11096030982335409\n",
      "Average test loss: 0.003939947470815645\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11102559983730316\n",
      "Average test loss: 0.00390729431145721\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11081751377715005\n",
      "Average test loss: 0.003905537184741762\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11065647001398934\n",
      "Average test loss: 0.003958100680675772\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11067120430866877\n",
      "Average test loss: 0.004027458265423774\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11051638519763947\n",
      "Average test loss: 0.004927135197652711\n",
      "Epoch 126/300\n",
      "Average training loss: 0.11042285652955373\n",
      "Average test loss: 0.003993492060651382\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11023985734913085\n",
      "Average test loss: 0.003933462119971713\n",
      "Epoch 128/300\n",
      "Average training loss: 0.1102670690615972\n",
      "Average test loss: 0.003922377177203695\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11004130229022768\n",
      "Average test loss: 0.004100329340332084\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10994064429733488\n",
      "Average test loss: 0.003925595339801576\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10979206583897273\n",
      "Average test loss: 0.004211120220108165\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10973145866394043\n",
      "Average test loss: 0.003976325322356489\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10975706586572859\n",
      "Average test loss: 0.003951268959376547\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10952572127845553\n",
      "Average test loss: 0.0039773930687871245\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10947173342439863\n",
      "Average test loss: 0.003934244953923755\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10937305524614122\n",
      "Average test loss: 0.003932752695141567\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10931493327352736\n",
      "Average test loss: 0.0039908928432398376\n",
      "Epoch 138/300\n",
      "Average training loss: 0.1092399967180358\n",
      "Average test loss: 0.003970870074298647\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10895355665683747\n",
      "Average test loss: 0.004033781277843648\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10891899769836003\n",
      "Average test loss: 0.003993751118166579\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1088249082101716\n",
      "Average test loss: 0.003981498140427801\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10892350141869651\n",
      "Average test loss: 0.003938271013398965\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10875952370299233\n",
      "Average test loss: 0.003966705475002527\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10851484017239676\n",
      "Average test loss: 0.003939001153740618\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10849523006545173\n",
      "Average test loss: 0.004040749822846718\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10834090799093246\n",
      "Average test loss: 0.003959373469567961\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10819827063878378\n",
      "Average test loss: 0.003976297337147924\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10815348158280055\n",
      "Average test loss: 0.003961871327832342\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10812136582533519\n",
      "Average test loss: 0.00397468283917341\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10781372116009394\n",
      "Average test loss: 0.004008434787806538\n",
      "Epoch 151/300\n",
      "Average training loss: 0.10799500920375188\n",
      "Average test loss: 0.003982758861035109\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10780465557177861\n",
      "Average test loss: 0.003971659714563026\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10765489490164651\n",
      "Average test loss: 0.003956444691866636\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10768097153637145\n",
      "Average test loss: 0.006278867072943184\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10745094196001688\n",
      "Average test loss: 0.004128490252213346\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10751891983879937\n",
      "Average test loss: 0.00396512303666936\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10725474593374464\n",
      "Average test loss: 0.003994472093052334\n",
      "Epoch 158/300\n",
      "Average training loss: 0.10724042466613981\n",
      "Average test loss: 0.003996744334076842\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10695728994078106\n",
      "Average test loss: 0.00396536836359236\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10700851213932037\n",
      "Average test loss: 0.004151974526130491\n",
      "Epoch 161/300\n",
      "Average training loss: 0.10697567213243908\n",
      "Average test loss: 0.004051742256515556\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10713048389554024\n",
      "Average test loss: 0.004041334489153491\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10661364062627157\n",
      "Average test loss: 0.0039933325884242855\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10685245631800758\n",
      "Average test loss: 0.00401993431068129\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10647027599811554\n",
      "Average test loss: 0.004027465347200632\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10649623621834649\n",
      "Average test loss: 0.003971240781868498\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10621063933107588\n",
      "Average test loss: 0.004107252845126722\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10627693489193916\n",
      "Average test loss: 0.004049473447311255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.10642385586765077\n",
      "Average test loss: 0.003991607886428634\n",
      "Epoch 170/300\n",
      "Average training loss: 0.10600283176369137\n",
      "Average test loss: 0.00410911763459444\n",
      "Epoch 171/300\n",
      "Average training loss: 0.10600366364585029\n",
      "Average test loss: 0.004012701304008564\n",
      "Epoch 172/300\n",
      "Average training loss: 0.10574983455737431\n",
      "Average test loss: 0.004015269316732883\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10580880526039335\n",
      "Average test loss: 0.004015753923397925\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10562578792042203\n",
      "Average test loss: 0.004040309864613745\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10551365259620879\n",
      "Average test loss: 0.0040694004529052315\n",
      "Epoch 176/300\n",
      "Average training loss: 0.10564642053842545\n",
      "Average test loss: 0.003999988037265009\n",
      "Epoch 177/300\n",
      "Average training loss: 0.1055128289328681\n",
      "Average test loss: 0.004065605740994215\n",
      "Epoch 178/300\n",
      "Average training loss: 0.10542133135928047\n",
      "Average test loss: 0.004108530403011375\n",
      "Epoch 179/300\n",
      "Average training loss: 0.10523337570826212\n",
      "Average test loss: 0.004054973825398419\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10509172131617864\n",
      "Average test loss: 0.00400794277091821\n",
      "Epoch 181/300\n",
      "Average training loss: 0.10508430325984955\n",
      "Average test loss: 0.00403626525733206\n",
      "Epoch 182/300\n",
      "Average training loss: 0.10499160852034886\n",
      "Average test loss: 0.004032069768549668\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10501419275336796\n",
      "Average test loss: 0.004034442979014582\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10475745607746972\n",
      "Average test loss: 0.004053365867584944\n",
      "Epoch 185/300\n",
      "Average training loss: 0.1048205850256814\n",
      "Average test loss: 0.004069073441541857\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10460141191217634\n",
      "Average test loss: 0.004103771484146515\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10470852704180611\n",
      "Average test loss: 0.004045578594009082\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10467388336526023\n",
      "Average test loss: 0.004082820775608222\n",
      "Epoch 189/300\n",
      "Average training loss: 0.10445415886905458\n",
      "Average test loss: 0.0040011706699927645\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10435655360089408\n",
      "Average test loss: 0.00403303785663512\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10432158571481705\n",
      "Average test loss: 0.004116309758275747\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1041268108288447\n",
      "Average test loss: 0.0041003197555740676\n",
      "Epoch 193/300\n",
      "Average training loss: 0.10408735113011466\n",
      "Average test loss: 0.004078709696729978\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10402030452092488\n",
      "Average test loss: 0.004068940266966819\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1041521414120992\n",
      "Average test loss: 0.004106245988566014\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10391766220993466\n",
      "Average test loss: 0.004041838056304389\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10383931528197395\n",
      "Average test loss: 0.004170318307562007\n",
      "Epoch 198/300\n",
      "Average training loss: 0.10385274508264329\n",
      "Average test loss: 0.0041110572670069005\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10355399984121323\n",
      "Average test loss: 0.004123736331653264\n",
      "Epoch 200/300\n",
      "Average training loss: 0.10353798555665546\n",
      "Average test loss: 0.004074646760606104\n",
      "Epoch 201/300\n",
      "Average training loss: 0.10364260419209798\n",
      "Average test loss: 0.004046160524917974\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10338690914048089\n",
      "Average test loss: 0.004129770159721375\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10325044798851013\n",
      "Average test loss: 0.00407453322576152\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10363114253017637\n",
      "Average test loss: 0.004076888165540166\n",
      "Epoch 205/300\n",
      "Average training loss: 0.10321595349576738\n",
      "Average test loss: 0.00413695015406443\n",
      "Epoch 206/300\n",
      "Average training loss: 0.10320767390065723\n",
      "Average test loss: 0.004074769686493608\n",
      "Epoch 207/300\n",
      "Average training loss: 0.10314882965882619\n",
      "Average test loss: 0.004412546288015114\n",
      "Epoch 208/300\n",
      "Average training loss: 0.10302886503272586\n",
      "Average test loss: 0.004043489347729418\n",
      "Epoch 209/300\n",
      "Average training loss: 0.10298865018288295\n",
      "Average test loss: 0.0042465099742015205\n",
      "Epoch 210/300\n",
      "Average training loss: 0.1028436818851365\n",
      "Average test loss: 0.004146398944987191\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10285420670774248\n",
      "Average test loss: 0.004118836780803071\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10265768594874276\n",
      "Average test loss: 0.00407749733204643\n",
      "Epoch 213/300\n",
      "Average training loss: 0.10264668078555002\n",
      "Average test loss: 0.0040986974893344776\n",
      "Epoch 214/300\n",
      "Average training loss: 0.10271772718429566\n",
      "Average test loss: 0.004035155187671383\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10246352573898103\n",
      "Average test loss: 0.004048495066041748\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10244644310739305\n",
      "Average test loss: 0.004110695393549071\n",
      "Epoch 217/300\n",
      "Average training loss: 0.10241251539521747\n",
      "Average test loss: 0.00414432641532686\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10274236020114687\n",
      "Average test loss: 0.004196392315957281\n",
      "Epoch 219/300\n",
      "Average training loss: 0.10220614333285226\n",
      "Average test loss: 0.004166053753760126\n",
      "Epoch 220/300\n",
      "Average training loss: 0.10218723239501318\n",
      "Average test loss: 0.004156593742883867\n",
      "Epoch 221/300\n",
      "Average training loss: 0.10204048145479626\n",
      "Average test loss: 0.00411454775225785\n",
      "Epoch 222/300\n",
      "Average training loss: 0.10212306278944015\n",
      "Average test loss: 0.0041127865297926796\n",
      "Epoch 223/300\n",
      "Average training loss: 0.10216325346628825\n",
      "Average test loss: 0.004047894383884139\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10204479279120764\n",
      "Average test loss: 0.004100121233198378\n",
      "Epoch 225/300\n",
      "Average training loss: 0.10180726377831566\n",
      "Average test loss: 0.004075845477067762\n",
      "Epoch 226/300\n",
      "Average training loss: 0.10191189506981108\n",
      "Average test loss: 0.004080160207218594\n",
      "Epoch 227/300\n",
      "Average training loss: 0.1018572227689955\n",
      "Average test loss: 0.004145825910071532\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10192859533098009\n",
      "Average test loss: 0.004084063718509343\n",
      "Epoch 229/300\n",
      "Average training loss: 0.10154550218582153\n",
      "Average test loss: 0.004114602943675385\n",
      "Epoch 230/300\n",
      "Average training loss: 0.10144069993495941\n",
      "Average test loss: 0.004205571845173835\n",
      "Epoch 231/300\n",
      "Average training loss: 0.1016517693930202\n",
      "Average test loss: 0.004108576522519191\n",
      "Epoch 232/300\n",
      "Average training loss: 0.10141308371225993\n",
      "Average test loss: 0.004076449955089225\n",
      "Epoch 233/300\n",
      "Average training loss: 0.10143306985828611\n",
      "Average test loss: 0.004046285821331872\n",
      "Epoch 234/300\n",
      "Average training loss: 0.10133991823593776\n",
      "Average test loss: 0.0041364139521287546\n",
      "Epoch 235/300\n",
      "Average training loss: 0.10130650565359328\n",
      "Average test loss: 0.004077534904910458\n",
      "Epoch 236/300\n",
      "Average training loss: 0.10112471768591139\n",
      "Average test loss: 0.004161979182312886\n",
      "Epoch 237/300\n",
      "Average training loss: 0.10128209481636684\n",
      "Average test loss: 0.004135675635188818\n",
      "Epoch 238/300\n",
      "Average training loss: 0.10114610020319621\n",
      "Average test loss: 0.004118869872556792\n",
      "Epoch 239/300\n",
      "Average training loss: 0.10124439700444539\n",
      "Average test loss: 0.004084337009323968\n",
      "Epoch 240/300\n",
      "Average training loss: 0.10100849809911516\n",
      "Average test loss: 0.004110185554871956\n",
      "Epoch 241/300\n",
      "Average training loss: 0.10085565789540608\n",
      "Average test loss: 0.004224643338471651\n",
      "Epoch 242/300\n",
      "Average training loss: 0.10092690883742439\n",
      "Average test loss: 0.004145781232354541\n",
      "Epoch 243/300\n",
      "Average training loss: 0.10075863284534878\n",
      "Average test loss: 0.00411093087018364\n",
      "Epoch 244/300\n",
      "Average training loss: 0.10075120190117094\n",
      "Average test loss: 0.00411906704513563\n",
      "Epoch 245/300\n",
      "Average training loss: 0.10068229862054189\n",
      "Average test loss: 0.004255501967337396\n",
      "Epoch 246/300\n",
      "Average training loss: 0.10086810853083929\n",
      "Average test loss: 0.004132225436054998\n",
      "Epoch 247/300\n",
      "Average training loss: 0.10058887704213461\n",
      "Average test loss: 0.004271294974204567\n",
      "Epoch 248/300\n",
      "Average training loss: 0.10062854201926126\n",
      "Average test loss: 0.00419336991591586\n",
      "Epoch 249/300\n",
      "Average training loss: 0.10066275874773661\n",
      "Average test loss: 0.004103287450555298\n",
      "Epoch 250/300\n",
      "Average training loss: 0.10045029558075799\n",
      "Average test loss: 0.0040989762875768875\n",
      "Epoch 251/300\n",
      "Average training loss: 0.10038787700070276\n",
      "Average test loss: 0.004161939728591177\n",
      "Epoch 252/300\n",
      "Average training loss: 0.10040812806288402\n",
      "Average test loss: 0.0041555151842120625\n",
      "Epoch 253/300\n",
      "Average training loss: 0.10036734843916363\n",
      "Average test loss: 0.004104172316276365\n",
      "Epoch 254/300\n",
      "Average training loss: 0.10030288355880314\n",
      "Average test loss: 0.00413428575421373\n",
      "Epoch 255/300\n",
      "Average training loss: 0.10024011246363322\n",
      "Average test loss: 0.004155365965639551\n",
      "Epoch 256/300\n",
      "Average training loss: 0.10017455072535408\n",
      "Average test loss: 0.004096061111945245\n",
      "Epoch 257/300\n",
      "Average training loss: 0.10008190905385547\n",
      "Average test loss: 0.005517910201102495\n",
      "Epoch 258/300\n",
      "Average training loss: 0.1001312214202351\n",
      "Average test loss: 0.004171372235649162\n",
      "Epoch 259/300\n",
      "Average training loss: 0.10003558864858415\n",
      "Average test loss: 0.004248025786959463\n",
      "Epoch 260/300\n",
      "Average training loss: 0.10009623019562827\n",
      "Average test loss: 0.004232849093361034\n",
      "Epoch 261/300\n",
      "Average training loss: 0.1000028870039516\n",
      "Average test loss: 0.004145687739468283\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09969993368122312\n",
      "Average test loss: 0.004157578503506051\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09979629076851738\n",
      "Average test loss: 0.004143170745422442\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09987531459331513\n",
      "Average test loss: 0.00412252771999273\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09964516855610742\n",
      "Average test loss: 0.0041799744781520634\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09968770613935259\n",
      "Average test loss: 0.004080372667354015\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09960836653908094\n",
      "Average test loss: 0.004263399224314425\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09950939039389292\n",
      "Average test loss: 0.0040840644025140335\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09945818343427446\n",
      "Average test loss: 0.004184827394576537\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09950287279817793\n",
      "Average test loss: 0.004225410581048992\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09939232670598559\n",
      "Average test loss: 0.0041724338746733135\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09983378156026204\n",
      "Average test loss: 0.004236311205973228\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09932175786627663\n",
      "Average test loss: 0.004184371305008729\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09939889842271805\n",
      "Average test loss: 0.004276028145311608\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09924154390891393\n",
      "Average test loss: 0.0042561750354038344\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09926714596483442\n",
      "Average test loss: 0.004170992349998818\n",
      "Epoch 277/300\n",
      "Average training loss: 0.09927162211471134\n",
      "Average test loss: 0.004120844452745385\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09916635182831023\n",
      "Average test loss: 0.00419095936541756\n",
      "Epoch 279/300\n",
      "Average training loss: 0.09897370223866568\n",
      "Average test loss: 0.0042009873605436746\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0991764450404379\n",
      "Average test loss: 0.00428451933297846\n",
      "Epoch 281/300\n",
      "Average training loss: 0.09894264447026782\n",
      "Average test loss: 0.004202342602941725\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09905988581313027\n",
      "Average test loss: 0.004260547744420667\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09891975438594817\n",
      "Average test loss: 0.0040858830230103595\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09907805344131258\n",
      "Average test loss: 0.004366488901898265\n",
      "Epoch 285/300\n",
      "Average training loss: 0.09887745557890998\n",
      "Average test loss: 0.004163672092474169\n",
      "Epoch 286/300\n",
      "Average training loss: 0.09876709220806758\n",
      "Average test loss: 0.004184250017007192\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09881856457392375\n",
      "Average test loss: 0.00416866023465991\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09861908961335818\n",
      "Average test loss: 0.004280823212737838\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09869944162501229\n",
      "Average test loss: 0.004215943467285898\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09859807840320799\n",
      "Average test loss: 0.004228584489888615\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0986292281879319\n",
      "Average test loss: 0.004216215231145422\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09848951515886518\n",
      "Average test loss: 0.004224099232711726\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09856279888417986\n",
      "Average test loss: 0.00422323710223039\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0984000752568245\n",
      "Average test loss: 0.004212483036021392\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09829741528961393\n",
      "Average test loss: 0.004181090921991401\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0983319365647104\n",
      "Average test loss: 0.004197552087820238\n",
      "Epoch 297/300\n",
      "Average training loss: 0.09824656409687466\n",
      "Average test loss: 0.004167280045441455\n",
      "Epoch 298/300\n",
      "Average training loss: 0.09829305167330636\n",
      "Average test loss: 0.004194626167209612\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09818302969137828\n",
      "Average test loss: 0.004119154771582948\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09831501059399711\n",
      "Average test loss: 0.00421228934907251\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.8118192571004235\n",
      "Average test loss: 0.006205071386363771\n",
      "Epoch 2/300\n",
      "Average training loss: 1.0308938715193006\n",
      "Average test loss: 0.004920390035957098\n",
      "Epoch 3/300\n",
      "Average training loss: 0.55265820479393\n",
      "Average test loss: 0.004597054443839524\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3850429716375139\n",
      "Average test loss: 0.0043650054703984\n",
      "Epoch 5/300\n",
      "Average training loss: 0.29628210673067307\n",
      "Average test loss: 0.004302481653375758\n",
      "Epoch 6/300\n",
      "Average training loss: 0.23944066385428112\n",
      "Average test loss: 0.0043468874920573495\n",
      "Epoch 7/300\n",
      "Average training loss: 0.20372338236702814\n",
      "Average test loss: 0.004212929107662704\n",
      "Epoch 8/300\n",
      "Average training loss: 0.18020745942327712\n",
      "Average test loss: 0.004247051702605353\n",
      "Epoch 9/300\n",
      "Average training loss: 0.16303097936842176\n",
      "Average test loss: 0.00393678798733486\n",
      "Epoch 10/300\n",
      "Average training loss: 0.149686524146133\n",
      "Average test loss: 0.003880147721618414\n",
      "Epoch 11/300\n",
      "Average training loss: 0.14065047817760043\n",
      "Average test loss: 0.0037745600102676285\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1341413633161121\n",
      "Average test loss: 0.00372705576589538\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1289617489443885\n",
      "Average test loss: 0.0036917242623037763\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12486957534154257\n",
      "Average test loss: 0.0035995062920782302\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12135613649421267\n",
      "Average test loss: 0.0035547619681391453\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1184493880007002\n",
      "Average test loss: 0.003552995309440626\n",
      "Epoch 17/300\n",
      "Average training loss: 0.11577033444245656\n",
      "Average test loss: 0.003456446513119671\n",
      "Epoch 18/300\n",
      "Average training loss: 0.113479581548108\n",
      "Average test loss: 0.0034309156582587293\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11145830212699043\n",
      "Average test loss: 0.0034605054853277073\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10964254028267331\n",
      "Average test loss: 0.0033472225612236396\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10785762079556784\n",
      "Average test loss: 0.0033670731476611562\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10645376655790541\n",
      "Average test loss: 0.0032606638841744928\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10513669681549072\n",
      "Average test loss: 0.0032590662476917106\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10401154490974214\n",
      "Average test loss: 0.003205326577027639\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10276931695143382\n",
      "Average test loss: 0.0032263282442258462\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10162765123446782\n",
      "Average test loss: 0.0032222981345322397\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10095252920852767\n",
      "Average test loss: 0.003133049750079711\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09970592673619588\n",
      "Average test loss: 0.003132026777913173\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09888937209049861\n",
      "Average test loss: 0.003129575396163596\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09808589502506786\n",
      "Average test loss: 0.0031086948942393063\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0971766579416063\n",
      "Average test loss: 0.003048753230108155\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09644711913002862\n",
      "Average test loss: 0.003067352522164583\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09567386872900857\n",
      "Average test loss: 0.003057115835655067\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0950756099621455\n",
      "Average test loss: 0.003028270829675926\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09441862705681059\n",
      "Average test loss: 0.003040850868448615\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09365516138739056\n",
      "Average test loss: 0.002999817991629243\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09326347476243972\n",
      "Average test loss: 0.0030054170994295013\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09261868464284473\n",
      "Average test loss: 0.003098262630816963\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09217180769311058\n",
      "Average test loss: 0.0030223542417710026\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09178524293502172\n",
      "Average test loss: 0.0029545848680039245\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09126527443197038\n",
      "Average test loss: 0.002964454870671034\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09079798887835609\n",
      "Average test loss: 0.0030509978578322464\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09039374038908217\n",
      "Average test loss: 0.0029481721671505107\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09007424659861459\n",
      "Average test loss: 0.002984162707709604\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08973106762435701\n",
      "Average test loss: 0.002929044011566374\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08931733320156733\n",
      "Average test loss: 0.002922525589250856\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08903543466991848\n",
      "Average test loss: 0.002916589675264226\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08862689016924964\n",
      "Average test loss: 0.00295497266182469\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08838933315210873\n",
      "Average test loss: 0.002920544928767615\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08804307419392798\n",
      "Average test loss: 0.002906317563727498\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08779045436117384\n",
      "Average test loss: 0.002915125179207987\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08751561743683285\n",
      "Average test loss: 0.0028927717016388973\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08718408421013091\n",
      "Average test loss: 0.0029166281684819195\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08697364142868254\n",
      "Average test loss: 0.0029066523334218396\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08686112184325855\n",
      "Average test loss: 0.0028709770476238595\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0865547300113572\n",
      "Average test loss: 0.0028937080297619104\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08633133947849274\n",
      "Average test loss: 0.002887445628643036\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08606443744566705\n",
      "Average test loss: 0.0029136648004253704\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0859368198050393\n",
      "Average test loss: 0.0028623148161504002\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08561807312568029\n",
      "Average test loss: 0.002888322676635451\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08541783033476935\n",
      "Average test loss: 0.002861618945789006\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08524543517827987\n",
      "Average test loss: 0.002865385240047342\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08495171202553643\n",
      "Average test loss: 0.002855640379505025\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0848629444109069\n",
      "Average test loss: 0.002847657056939271\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08452510160870022\n",
      "Average test loss: 0.0028741177024526727\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08429846463600794\n",
      "Average test loss: 0.002846360213847624\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08423787122302585\n",
      "Average test loss: 0.0028869919669297006\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0840421618686782\n",
      "Average test loss: 0.0028583102567742266\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08369137164288097\n",
      "Average test loss: 0.0028712424917353525\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08356556365887324\n",
      "Average test loss: 0.002845013219449255\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08342766023344464\n",
      "Average test loss: 0.002882232124399808\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08324158438046773\n",
      "Average test loss: 0.0028644279069784616\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08302871736221844\n",
      "Average test loss: 0.002886099881078634\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08275113353795475\n",
      "Average test loss: 0.0028533269798176157\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08270455851157506\n",
      "Average test loss: 0.0028699936585293876\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08242557197146945\n",
      "Average test loss: 0.002842671843452586\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08217169733842214\n",
      "Average test loss: 0.0028556385975744988\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08209508292542564\n",
      "Average test loss: 0.002871458533530434\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08201115532053842\n",
      "Average test loss: 0.0028594327771829233\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08172492208745745\n",
      "Average test loss: 0.0028472265261742804\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08144989741510815\n",
      "Average test loss: 0.0028760767596670324\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08133871726195017\n",
      "Average test loss: 0.002857632575867077\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0811480412185192\n",
      "Average test loss: 0.002872792174004846\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08095440918869443\n",
      "Average test loss: 0.002860562968171305\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08082744058304363\n",
      "Average test loss: 0.0028717941089222826\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08061091091566616\n",
      "Average test loss: 0.0028805566717767053\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0804094143178728\n",
      "Average test loss: 0.0028658148734312917\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08040120991402203\n",
      "Average test loss: 0.0029137599682435393\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08022663483023644\n",
      "Average test loss: 0.0028837183475908306\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0800322908030616\n",
      "Average test loss: 0.0028885947461757397\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07972630778286192\n",
      "Average test loss: 0.002854085430709852\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07961855531401105\n",
      "Average test loss: 0.0028483754562007055\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07948327386379242\n",
      "Average test loss: 0.0028617734236435757\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07921294576923052\n",
      "Average test loss: 0.002920712034321494\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07907469881243176\n",
      "Average test loss: 0.00286173915821645\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07892923676305347\n",
      "Average test loss: 0.0028672019314641755\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07877061423659325\n",
      "Average test loss: 0.0028851237119072014\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07866144099831582\n",
      "Average test loss: 0.002864207256999281\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07845843303534719\n",
      "Average test loss: 0.0028722252146237428\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0783751294347975\n",
      "Average test loss: 0.002874419488840633\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07819827867547671\n",
      "Average test loss: 0.0028769313662002486\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07794004323747424\n",
      "Average test loss: 0.0029397690209249654\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07784992420011097\n",
      "Average test loss: 0.0028793758458147446\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07781609308388499\n",
      "Average test loss: 0.002906217942635218\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07764891152249442\n",
      "Average test loss: 0.00305410945115404\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07739465137985017\n",
      "Average test loss: 0.0028677302733477617\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07722232434153557\n",
      "Average test loss: 0.002865647284934918\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07702782765361998\n",
      "Average test loss: 0.0029408110491931437\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07691273596551683\n",
      "Average test loss: 0.0029171014494366117\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07686230182647705\n",
      "Average test loss: 0.00291470041188101\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07665097856852743\n",
      "Average test loss: 0.002865326427026755\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07662829879919687\n",
      "Average test loss: 0.0028706561449087327\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07640918500555886\n",
      "Average test loss: 0.0029786955238216453\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0762620964580112\n",
      "Average test loss: 0.0028955486145698364\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07617812141776086\n",
      "Average test loss: 0.0029078943183024723\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07647711730003356\n",
      "Average test loss: 0.0029132358231064347\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07587493536207411\n",
      "Average test loss: 0.002932710010972288\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07566916790273455\n",
      "Average test loss: 0.0029125900585204364\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07553983009523815\n",
      "Average test loss: 0.002887870190044244\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07568773608406384\n",
      "Average test loss: 0.0029408053118321633\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07546163880493906\n",
      "Average test loss: 0.002906387162912223\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07521079951524734\n",
      "Average test loss: 0.002938823522379001\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07505291852023867\n",
      "Average test loss: 0.0029033959541055893\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07489260870880551\n",
      "Average test loss: 0.0029126991002509993\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07495615214771695\n",
      "Average test loss: 0.0029081177606971728\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07484278003705873\n",
      "Average test loss: 0.0029771469638993344\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07465499226914511\n",
      "Average test loss: 0.002936064664895336\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07451971407069101\n",
      "Average test loss: 0.002944786496977839\n",
      "Epoch 129/300\n",
      "Average training loss: 0.074508710331387\n",
      "Average test loss: 0.003007196363268627\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07440369717942344\n",
      "Average test loss: 0.0029502856973558666\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07403911494877603\n",
      "Average test loss: 0.002994089008619388\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07403779055674871\n",
      "Average test loss: 0.002932120471364922\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07399620322386424\n",
      "Average test loss: 0.003084267942027913\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07385676991608407\n",
      "Average test loss: 0.0029561242481900584\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07371404776639409\n",
      "Average test loss: 0.0029596339777732887\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07361324704355664\n",
      "Average test loss: 0.0029147910324649677\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07358988416194916\n",
      "Average test loss: 0.00295242131418652\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07347842588027319\n",
      "Average test loss: 0.0029655794919364984\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07334495450059574\n",
      "Average test loss: 0.002957469762199455\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07321355734268824\n",
      "Average test loss: 0.003035235498721401\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07305867839521832\n",
      "Average test loss: 0.0029767361825539006\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07308430924680498\n",
      "Average test loss: 0.0030074278187627594\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07301300402482351\n",
      "Average test loss: 0.002985639174779256\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07289320803682009\n",
      "Average test loss: 0.002982531839981675\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07272206581963434\n",
      "Average test loss: 0.002983243222037951\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07265759462118149\n",
      "Average test loss: 0.002957053189476331\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07247357305884361\n",
      "Average test loss: 0.0029622155448628795\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07255521406067743\n",
      "Average test loss: 0.0029974332712590693\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07241064315040906\n",
      "Average test loss: 0.00300515035001768\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07228377406133546\n",
      "Average test loss: 0.002970778501075175\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07217841109964583\n",
      "Average test loss: 0.0030265901045252877\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07219608447949091\n",
      "Average test loss: 0.002988784769549966\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07219392257266574\n",
      "Average test loss: 0.003010389268812206\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07183820064200296\n",
      "Average test loss: 0.0030428209002647134\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07188784922162691\n",
      "Average test loss: 0.0029692683217840063\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07184026704894172\n",
      "Average test loss: 0.0029869011876483757\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07172258864508735\n",
      "Average test loss: 0.0029737859103414746\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07167234852247768\n",
      "Average test loss: 0.002944841025190221\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07152694030602773\n",
      "Average test loss: 0.0030337835467524\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0714764636821217\n",
      "Average test loss: 0.002945482392484943\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07123689877324633\n",
      "Average test loss: 0.0029970881649189524\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07132339838147163\n",
      "Average test loss: 0.003003894105139706\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07120286887221866\n",
      "Average test loss: 0.002976723482211431\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07105137043529086\n",
      "Average test loss: 0.00296592852121426\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07104377202192942\n",
      "Average test loss: 0.0029660134522451293\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0709705800778336\n",
      "Average test loss: 0.00298983862499396\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07113279287020366\n",
      "Average test loss: 0.002934219353211423\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07098102363944053\n",
      "Average test loss: 0.0030456013292488125\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07073448249697685\n",
      "Average test loss: 0.002946134334223138\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07076638420091735\n",
      "Average test loss: 0.0030164547899944913\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07051808561881383\n",
      "Average test loss: 0.002983282050324811\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07063446882035997\n",
      "Average test loss: 0.0029838806187940966\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07045010871026251\n",
      "Average test loss: 0.0030822828012622063\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0704031117260456\n",
      "Average test loss: 0.003068655990064144\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07039687367280324\n",
      "Average test loss: 0.0029963623020384045\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07038511205381817\n",
      "Average test loss: 0.0030411191266030073\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07015988936689165\n",
      "Average test loss: 0.0030193744061721697\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07014491139517891\n",
      "Average test loss: 0.002985551826862825\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07014791642957263\n",
      "Average test loss: 0.0030205696950563127\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06998259358273612\n",
      "Average test loss: 0.0030568943100257053\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06995565226342942\n",
      "Average test loss: 0.0030441176088319886\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06991663291719225\n",
      "Average test loss: 0.0030798073553790647\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06996515912479824\n",
      "Average test loss: 0.0030208206340256666\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0698097258872456\n",
      "Average test loss: 0.0029883957815666994\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0697355489300357\n",
      "Average test loss: 0.003028547366046243\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06967684469620387\n",
      "Average test loss: 0.0029884807252221635\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06960384855005476\n",
      "Average test loss: 0.002990662593808439\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06953699300686518\n",
      "Average test loss: 0.002992279449891713\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06954860040214327\n",
      "Average test loss: 0.003013884497599469\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06943592605988184\n",
      "Average test loss: 0.003012914410067929\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06945539031757249\n",
      "Average test loss: 0.0030217180198265445\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06916365723477469\n",
      "Average test loss: 0.0030382551203171414\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06918973869747586\n",
      "Average test loss: 0.0030302854225867323\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06920077251063453\n",
      "Average test loss: 0.0030049507290952735\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06917842162979973\n",
      "Average test loss: 0.0030376815216408837\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06918892247809304\n",
      "Average test loss: 0.0030574071539772882\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06897298646966617\n",
      "Average test loss: 0.0030562923618902764\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06888061898946762\n",
      "Average test loss: 0.0030136888939887286\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06886184769537713\n",
      "Average test loss: 0.0030183427654620672\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06901989556683434\n",
      "Average test loss: 0.003050783357479506\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06891449993186527\n",
      "Average test loss: 0.00302499530899028\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06877829986148411\n",
      "Average test loss: 0.003054167745427953\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06852046273152033\n",
      "Average test loss: 0.003024773181312614\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06854955559968948\n",
      "Average test loss: 0.0030313688835336104\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06858391668399175\n",
      "Average test loss: 0.0030207929112431075\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0685212657517857\n",
      "Average test loss: 0.003016071816285451\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06864567831489775\n",
      "Average test loss: 0.003022239699131913\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06844519808557298\n",
      "Average test loss: 0.003030853919684887\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06838242461615139\n",
      "Average test loss: 0.0030244649712824158\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0683544110622671\n",
      "Average test loss: 0.0030592790599912405\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06822398059897952\n",
      "Average test loss: 0.0030550190715326205\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06820691036515766\n",
      "Average test loss: 0.0030476515321061015\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06814529031846259\n",
      "Average test loss: 0.0031239194605085585\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06818854669729868\n",
      "Average test loss: 0.003074201960530546\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06808358932203716\n",
      "Average test loss: 0.0031071466714557676\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06789314890570111\n",
      "Average test loss: 0.0030887506302032207\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06798472109105852\n",
      "Average test loss: 0.0030803042939967578\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06787821253471904\n",
      "Average test loss: 0.0030807908287064896\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06802569532394409\n",
      "Average test loss: 0.0030456128362566235\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06779815516869227\n",
      "Average test loss: 0.0031547366053693822\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06786732525295681\n",
      "Average test loss: 0.0030862612825714878\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06770805161529117\n",
      "Average test loss: 0.003121822893412577\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06773785485492813\n",
      "Average test loss: 0.0030462025896542603\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06762764380044407\n",
      "Average test loss: 0.003136016080362929\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06749575175179376\n",
      "Average test loss: 0.003032006667719947\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06751046462853749\n",
      "Average test loss: 0.0030199097405291266\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06767077444659339\n",
      "Average test loss: 0.003049093258463674\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06746296309431395\n",
      "Average test loss: 0.0030424348339438438\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0674217638903194\n",
      "Average test loss: 0.0030308375207500327\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06727852232257525\n",
      "Average test loss: 0.0031322553729017577\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06743922496173117\n",
      "Average test loss: 0.003041357436320848\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06721226696504487\n",
      "Average test loss: 0.003133850492330061\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06719523207677736\n",
      "Average test loss: 0.0030421858268479506\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06716142530904876\n",
      "Average test loss: 0.0031645511326690513\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06718619975447655\n",
      "Average test loss: 0.003090578726389342\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06708919377128283\n",
      "Average test loss: 0.003018042601748473\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06706855272915628\n",
      "Average test loss: 0.003115956880980068\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06705198976066377\n",
      "Average test loss: 0.0031217844438635642\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06721193968918589\n",
      "Average test loss: 0.003022431718185544\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06689345659812292\n",
      "Average test loss: 0.0030512164067476987\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06684566096464793\n",
      "Average test loss: 0.0030762865164627633\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06689160207907359\n",
      "Average test loss: 0.0031034391346491047\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06689488387438985\n",
      "Average test loss: 0.0030896900705993176\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06692960727877087\n",
      "Average test loss: 0.003092992060714298\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06695008572936058\n",
      "Average test loss: 0.0030750112884367505\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06656434261136585\n",
      "Average test loss: 0.0031118781701144245\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06679490419891146\n",
      "Average test loss: 0.0031231069225403996\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06682128842340575\n",
      "Average test loss: 0.003064526071771979\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06663916445440717\n",
      "Average test loss: 0.0030817144529687035\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06659152651164267\n",
      "Average test loss: 0.0031325329481106667\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06656286862161424\n",
      "Average test loss: 0.0030468266055815748\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0663741990327835\n",
      "Average test loss: 0.003128916939927472\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06642873687214322\n",
      "Average test loss: 0.0030584039921975796\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06650379504760107\n",
      "Average test loss: 0.003124848135643535\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06647155208388965\n",
      "Average test loss: 0.0031721899813661972\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06626739078097874\n",
      "Average test loss: 0.0031185850563148656\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06638770956463284\n",
      "Average test loss: 0.003059220828736822\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06626853819025888\n",
      "Average test loss: 0.00330679658614099\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06628737069831954\n",
      "Average test loss: 0.00313073146281143\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06614194034867817\n",
      "Average test loss: 0.003135340380999777\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06620121160811848\n",
      "Average test loss: 0.0031997681214577622\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06602777438031303\n",
      "Average test loss: 0.0031826497229437032\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06617393169138167\n",
      "Average test loss: 0.0031109427474439143\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06611883232328628\n",
      "Average test loss: 0.0030844723716792134\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06619408153825336\n",
      "Average test loss: 0.0031420161734438605\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06593749580780665\n",
      "Average test loss: 0.0031432517415119544\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06603030367361175\n",
      "Average test loss: 0.0031493895026958652\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06595758411950535\n",
      "Average test loss: 0.003130475570758184\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06599090076486269\n",
      "Average test loss: 0.0030655254282885127\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06609145281712214\n",
      "Average test loss: 0.003106091607362032\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06588141681750616\n",
      "Average test loss: 0.0031406004859341518\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0658873423602846\n",
      "Average test loss: 0.0030796887665573095\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0657757462726699\n",
      "Average test loss: 0.003109269300889638\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06582657805747456\n",
      "Average test loss: 0.0030980082373652192\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06570698539084859\n",
      "Average test loss: 0.0031070989535914527\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0657222698132197\n",
      "Average test loss: 0.0031346361318396196\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06579191021124522\n",
      "Average test loss: 0.0031563786131640274\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06576423585414887\n",
      "Average test loss: 0.0032214260740826527\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06554685794976023\n",
      "Average test loss: 0.0030654151083694564\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06555829454461734\n",
      "Average test loss: 0.0030883808543698656\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06572268194622463\n",
      "Average test loss: 0.003122682042626871\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06559906977083949\n",
      "Average test loss: 0.0031011509204076394\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06561794858177503\n",
      "Average test loss: 0.0030525792299045456\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06539068426688512\n",
      "Average test loss: 0.0031296424437314272\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06543821855386098\n",
      "Average test loss: 0.003110132670029998\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06544608048929108\n",
      "Average test loss: 0.0031250052156133784\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06545092139310307\n",
      "Average test loss: 0.003089126068063908\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06536667767167091\n",
      "Average test loss: 0.0031239456666840447\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06532408714956707\n",
      "Average test loss: 0.003111425322170059\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06518983803192774\n",
      "Average test loss: 0.0030568822901695966\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06521474318703016\n",
      "Average test loss: 0.0031627609377933874\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06533378392457961\n",
      "Average test loss: 0.0030703306843837103\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06534365353981654\n",
      "Average test loss: 0.003133377361525264\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06519851987229454\n",
      "Average test loss: 0.0030902385347419317\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06516433403889338\n",
      "Average test loss: 0.003061791768918435\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06518600506252713\n",
      "Average test loss: 0.003147426888346672\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0652249466445711\n",
      "Average test loss: 0.003085106548749738\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06499067356189092\n",
      "Average test loss: 0.0030663247420969937\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06499659149183168\n",
      "Average test loss: 0.0031318140394157833\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06498319352666537\n",
      "Average test loss: 0.003197503032990628\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.8770124952528215\n",
      "Average test loss: 0.0060923077637950575\n",
      "Epoch 2/300\n",
      "Average training loss: 0.9950490934054057\n",
      "Average test loss: 0.004912190157920122\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5004041940636105\n",
      "Average test loss: 0.004393992478234901\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3333892924785614\n",
      "Average test loss: 0.003985499161812994\n",
      "Epoch 5/300\n",
      "Average training loss: 0.24581746917300754\n",
      "Average test loss: 0.0037684040524893336\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1968827067878511\n",
      "Average test loss: 0.003627160124066803\n",
      "Epoch 7/300\n",
      "Average training loss: 0.16755669677257537\n",
      "Average test loss: 0.004169295278481311\n",
      "Epoch 8/300\n",
      "Average training loss: 0.14932481066385905\n",
      "Average test loss: 0.003425686314702034\n",
      "Epoch 9/300\n",
      "Average training loss: 0.13673494098583858\n",
      "Average test loss: 0.0033621316538709734\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1274621489180459\n",
      "Average test loss: 0.0032684300711585414\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12031117606825299\n",
      "Average test loss: 0.003123882653605607\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11450802969932557\n",
      "Average test loss: 0.0031005113625691997\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10948530328936047\n",
      "Average test loss: 0.0030583360915382705\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10533308452367783\n",
      "Average test loss: 0.0028796319760796096\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1016005604200893\n",
      "Average test loss: 0.002833374000257916\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09830686841408412\n",
      "Average test loss: 0.002796234659022755\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09543866186009513\n",
      "Average test loss: 0.0026847042387558354\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0929105022350947\n",
      "Average test loss: 0.0026254215679234927\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09043674890862571\n",
      "Average test loss: 0.0025993074321498475\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08837894926468531\n",
      "Average test loss: 0.0025010809167805644\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08634294960896174\n",
      "Average test loss: 0.002492672488507297\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08458156801594628\n",
      "Average test loss: 0.002425851452474793\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08304268879360623\n",
      "Average test loss: 0.0024122204844736392\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08173870304226875\n",
      "Average test loss: 0.0023892867641730443\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08026992529961798\n",
      "Average test loss: 0.0023800178700023227\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07906770998570654\n",
      "Average test loss: 0.0023446976190639865\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07799436940749487\n",
      "Average test loss: 0.0023082390795979234\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07678848518596755\n",
      "Average test loss: 0.0022986114871584706\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07588696369859907\n",
      "Average test loss: 0.0023120136697673136\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07498778087562985\n",
      "Average test loss: 0.0023575957868662146\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07405137219031652\n",
      "Average test loss: 0.002257600645224253\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07329500502679083\n",
      "Average test loss: 0.0022410869147214624\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07253749263948864\n",
      "Average test loss: 0.0022474502033243576\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0720965609020657\n",
      "Average test loss: 0.0022270134285920194\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07113674647609393\n",
      "Average test loss: 0.0022275117478436895\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0706621176302433\n",
      "Average test loss: 0.0022505125713845092\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0700474841064877\n",
      "Average test loss: 0.0021849577675263085\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06950383904245165\n",
      "Average test loss: 0.0021750576612022186\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0688360919588142\n",
      "Average test loss: 0.0021661263066861365\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06848077210121685\n",
      "Average test loss: 0.002155190694042378\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06797495580381817\n",
      "Average test loss: 0.0021761103223802316\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06763579397731358\n",
      "Average test loss: 0.002141171656548977\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0673324926396211\n",
      "Average test loss: 0.0021499794498085977\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06693423887756136\n",
      "Average test loss: 0.0021182125883383884\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06646120755208863\n",
      "Average test loss: 0.002119252191442582\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06613675875133938\n",
      "Average test loss: 0.0021469077647974094\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06572511464357376\n",
      "Average test loss: 0.0021241929889139204\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06563133460945553\n",
      "Average test loss: 0.0020888570999312733\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06518412579430474\n",
      "Average test loss: 0.002095592902248932\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06497187188267708\n",
      "Average test loss: 0.002090402600872848\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0646807617313332\n",
      "Average test loss: 0.0020899330435527694\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06443287092116144\n",
      "Average test loss: 0.0020723541509360074\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06415487965279155\n",
      "Average test loss: 0.002072184890612132\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06394504970974392\n",
      "Average test loss: 0.0020704292132415706\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06359600756896867\n",
      "Average test loss: 0.0020804490074515345\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06342284517155754\n",
      "Average test loss: 0.002062009754952871\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06325020066897075\n",
      "Average test loss: 0.0020581231055160363\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06291721221473481\n",
      "Average test loss: 0.0020491422826631202\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06286206465628412\n",
      "Average test loss: 0.002080833961152368\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06258848322762384\n",
      "Average test loss: 0.0020625917832884522\n",
      "Epoch 61/300\n",
      "Average training loss: 0.062438311679495706\n",
      "Average test loss: 0.002055277054094606\n",
      "Epoch 62/300\n",
      "Average training loss: 0.062251297901074094\n",
      "Average test loss: 0.0020796087399745982\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06202464634180069\n",
      "Average test loss: 0.0020439722235831947\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06177310171061092\n",
      "Average test loss: 0.0020466335863909787\n",
      "Epoch 65/300\n",
      "Average training loss: 0.061493915491633945\n",
      "Average test loss: 0.0020636075966888002\n",
      "Epoch 66/300\n",
      "Average training loss: 0.061418852266338135\n",
      "Average test loss: 0.0020512932789408496\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0612928808497058\n",
      "Average test loss: 0.0020865239064312645\n",
      "Epoch 68/300\n",
      "Average training loss: 0.061076907796992194\n",
      "Average test loss: 0.0020346376509923075\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0609070812828011\n",
      "Average test loss: 0.002061214165762067\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06086552107665274\n",
      "Average test loss: 0.002195277642665638\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06050477180381616\n",
      "Average test loss: 0.0020452495232845345\n",
      "Epoch 72/300\n",
      "Average training loss: 0.060352892292870414\n",
      "Average test loss: 0.0020356180311905015\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06022410952217049\n",
      "Average test loss: 0.002040027334251338\n",
      "Epoch 74/300\n",
      "Average training loss: 0.060070667101277245\n",
      "Average test loss: 0.00203529016694261\n",
      "Epoch 75/300\n",
      "Average training loss: 0.059848992735147474\n",
      "Average test loss: 0.002047973406604595\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05971450479163064\n",
      "Average test loss: 0.0020485248514968486\n",
      "Epoch 77/300\n",
      "Average training loss: 0.059617322620418335\n",
      "Average test loss: 0.002042159602873855\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05930491108033392\n",
      "Average test loss: 0.0020338864335790277\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05925499243537585\n",
      "Average test loss: 0.0020310606201075845\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05907553314169248\n",
      "Average test loss: 0.0020548326859457624\n",
      "Epoch 81/300\n",
      "Average training loss: 0.058863949997557535\n",
      "Average test loss: 0.0020332700707432296\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05874115682641665\n",
      "Average test loss: 0.00203406265605655\n",
      "Epoch 83/300\n",
      "Average training loss: 0.058550875647200476\n",
      "Average test loss: 0.00203264268529084\n",
      "Epoch 84/300\n",
      "Average training loss: 0.058411662532223595\n",
      "Average test loss: 0.0020406588083133103\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05828402516576979\n",
      "Average test loss: 0.0020477014212972587\n",
      "Epoch 86/300\n",
      "Average training loss: 0.058138937317662766\n",
      "Average test loss: 0.0020600312548793025\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05808281753791703\n",
      "Average test loss: 0.0020942074808602534\n",
      "Epoch 88/300\n",
      "Average training loss: 0.057773374242915045\n",
      "Average test loss: 0.0020322704821204144\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05763904957638846\n",
      "Average test loss: 0.0020780842062085867\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05753971982002258\n",
      "Average test loss: 0.0020528854129629004\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05732443371084001\n",
      "Average test loss: 0.00203545550060355\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05725907351573308\n",
      "Average test loss: 0.0020723629016429186\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05713080170750618\n",
      "Average test loss: 0.0020494303394936854\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05689769429630703\n",
      "Average test loss: 0.0020394837202297317\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05679402667946286\n",
      "Average test loss: 0.002050705918110907\n",
      "Epoch 96/300\n",
      "Average training loss: 0.056804788728555045\n",
      "Average test loss: 0.0020570523771974774\n",
      "Epoch 97/300\n",
      "Average training loss: 0.056553422484132976\n",
      "Average test loss: 0.002055168301074041\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05633181805411975\n",
      "Average test loss: 0.0020591005469775864\n",
      "Epoch 99/300\n",
      "Average training loss: 0.056205173038774064\n",
      "Average test loss: 0.002062793254438374\n",
      "Epoch 100/300\n",
      "Average training loss: 0.056172857430246144\n",
      "Average test loss: 0.0020568621231036054\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05601807723442714\n",
      "Average test loss: 0.0020444318904644914\n",
      "Epoch 102/300\n",
      "Average training loss: 0.055838360965251924\n",
      "Average test loss: 0.0020690547014690108\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05578796956936519\n",
      "Average test loss: 0.002063011250148217\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05565885014997588\n",
      "Average test loss: 0.0020818455651816395\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05550874461068048\n",
      "Average test loss: 0.0020794980737070244\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05533837496572071\n",
      "Average test loss: 0.002059388932254579\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05521813040309482\n",
      "Average test loss: 0.0020893479991290305\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05511168630917867\n",
      "Average test loss: 0.002067193103229834\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05495488343305058\n",
      "Average test loss: 0.0020549295112076735\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05489572192894088\n",
      "Average test loss: 0.0020933442915686304\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05471864244672987\n",
      "Average test loss: 0.0021155931790255837\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05481252361006207\n",
      "Average test loss: 0.0020571516112734872\n",
      "Epoch 113/300\n",
      "Average training loss: 0.054621588743395275\n",
      "Average test loss: 0.002087586721508867\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05442082091503673\n",
      "Average test loss: 0.0020768093491593995\n",
      "Epoch 115/300\n",
      "Average training loss: 0.054247982574833764\n",
      "Average test loss: 0.0020633450431956184\n",
      "Epoch 116/300\n",
      "Average training loss: 0.054139174583885404\n",
      "Average test loss: 0.0021059964816603394\n",
      "Epoch 117/300\n",
      "Average training loss: 0.054140478607681064\n",
      "Average test loss: 0.0020710557298734785\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05394172997275988\n",
      "Average test loss: 0.0020909160373525485\n",
      "Epoch 119/300\n",
      "Average training loss: 0.053956630657116575\n",
      "Average test loss: 0.002061121940612793\n",
      "Epoch 120/300\n",
      "Average training loss: 0.053675646420982145\n",
      "Average test loss: 0.0020688881704376805\n",
      "Epoch 121/300\n",
      "Average training loss: 0.053587394177913666\n",
      "Average test loss: 0.0021191181817816363\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05353061463104354\n",
      "Average test loss: 0.0020994079541414978\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05356071929136912\n",
      "Average test loss: 0.002105163888488379\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05334128099017673\n",
      "Average test loss: 0.002119222261632482\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05325026641289393\n",
      "Average test loss: 0.002134927480067644\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05321597740385268\n",
      "Average test loss: 0.002107294263525142\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05313619627555211\n",
      "Average test loss: 0.0020875135568074053\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05288223017255465\n",
      "Average test loss: 0.0021235336975918874\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05273534274432394\n",
      "Average test loss: 0.0021439526192843913\n",
      "Epoch 130/300\n",
      "Average training loss: 0.052827303919527266\n",
      "Average test loss: 0.0021455701722039116\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05263660465677579\n",
      "Average test loss: 0.0021495994119387534\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05282545715901587\n",
      "Average test loss: 0.0021126126437965367\n",
      "Epoch 133/300\n",
      "Average training loss: 0.052617270204756\n",
      "Average test loss: 0.00209075824999147\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05228570771217346\n",
      "Average test loss: 0.002090927768084738\n",
      "Epoch 135/300\n",
      "Average training loss: 0.052224023371934894\n",
      "Average test loss: 0.002097543161776331\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05222031214170986\n",
      "Average test loss: 0.0020947488288705547\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05211838901705212\n",
      "Average test loss: 0.0021002924970040717\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05197172623872757\n",
      "Average test loss: 0.0021314394283625817\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05204957585202323\n",
      "Average test loss: 0.0021629611160606144\n",
      "Epoch 140/300\n",
      "Average training loss: 0.051820989221334456\n",
      "Average test loss: 0.002090626331460145\n",
      "Epoch 141/300\n",
      "Average training loss: 0.051756975303093596\n",
      "Average test loss: 0.0021168561968952417\n",
      "Epoch 142/300\n",
      "Average training loss: 0.051870391100645064\n",
      "Average test loss: 0.002102608209631095\n",
      "Epoch 143/300\n",
      "Average training loss: 0.051669216023551096\n",
      "Average test loss: 0.0021427553546511467\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05163517721162902\n",
      "Average test loss: 0.0021141974209911295\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05138198517759641\n",
      "Average test loss: 0.0021220649497376547\n",
      "Epoch 146/300\n",
      "Average training loss: 0.051441710342963534\n",
      "Average test loss: 0.0021214298421723976\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05132559497488869\n",
      "Average test loss: 0.002123840591352847\n",
      "Epoch 148/300\n",
      "Average training loss: 0.051184255242347716\n",
      "Average test loss: 0.0021007286748952335\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05118665229943063\n",
      "Average test loss: 0.002118864112326668\n",
      "Epoch 150/300\n",
      "Average training loss: 0.051016012930207785\n",
      "Average test loss: 0.002127817178558972\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0509986649023162\n",
      "Average test loss: 0.0021292265723976823\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05097221869561407\n",
      "Average test loss: 0.002134439389531811\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05084407233198484\n",
      "Average test loss: 0.0021129174914417993\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05072935198744138\n",
      "Average test loss: 0.0021154935936340026\n",
      "Epoch 155/300\n",
      "Average training loss: 0.050787529600991144\n",
      "Average test loss: 0.002112394501765569\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05064831967486275\n",
      "Average test loss: 0.002431396109258963\n",
      "Epoch 157/300\n",
      "Average training loss: 0.050557204190227724\n",
      "Average test loss: 0.0022119248966789907\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05054094800021913\n",
      "Average test loss: 0.0021742557199257943\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0504305782020092\n",
      "Average test loss: 0.0021150105935521424\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05039474504854944\n",
      "Average test loss: 0.0021659072859005796\n",
      "Epoch 161/300\n",
      "Average training loss: 0.050330829275978935\n",
      "Average test loss: 0.002213001837953925\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05036079307728344\n",
      "Average test loss: 0.002151432057842612\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05029837027192116\n",
      "Average test loss: 0.0021044040923524233\n",
      "Epoch 164/300\n",
      "Average training loss: 0.050187201496627594\n",
      "Average test loss: 0.002150269132107496\n",
      "Epoch 165/300\n",
      "Average training loss: 0.050130941073099775\n",
      "Average test loss: 0.0021441121422168283\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05003806632757187\n",
      "Average test loss: 0.002141259248058001\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05003056792749299\n",
      "Average test loss: 0.0021543022820519077\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04987672888901499\n",
      "Average test loss: 0.002155492061749101\n",
      "Epoch 169/300\n",
      "Average training loss: 0.049854944404628544\n",
      "Average test loss: 0.0021654862513144813\n",
      "Epoch 170/300\n",
      "Average training loss: 0.049861351132392887\n",
      "Average test loss: 0.0021612018443023166\n",
      "Epoch 171/300\n",
      "Average training loss: 0.049652926862239835\n",
      "Average test loss: 0.0021989570104827484\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04974202214346991\n",
      "Average test loss: 0.002153164265573853\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04963956112994088\n",
      "Average test loss: 0.002186458037752244\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04953449957238303\n",
      "Average test loss: 0.0021521092160708376\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04958452954557207\n",
      "Average test loss: 0.0021856743099374905\n",
      "Epoch 176/300\n",
      "Average training loss: 0.049356529106696446\n",
      "Average test loss: 0.002129497257578704\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04938315141532156\n",
      "Average test loss: 0.00219511652054886\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04932175752520561\n",
      "Average test loss: 0.002133374476598369\n",
      "Epoch 179/300\n",
      "Average training loss: 0.049332342935933006\n",
      "Average test loss: 0.0021721080139072407\n",
      "Epoch 180/300\n",
      "Average training loss: 0.049167093988921905\n",
      "Average test loss: 0.0021718248638014\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04914540445804596\n",
      "Average test loss: 0.002232279722785784\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04934351978699366\n",
      "Average test loss: 0.0021776668998516268\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0490223711265458\n",
      "Average test loss: 0.0021913257483392955\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04900875425338745\n",
      "Average test loss: 0.002146336940634582\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04891133944524659\n",
      "Average test loss: 0.002189990323140389\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04886928169594871\n",
      "Average test loss: 0.002131924321874976\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04893362138999833\n",
      "Average test loss: 0.0021330266922919288\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04886480185058382\n",
      "Average test loss: 0.0021942970446414416\n",
      "Epoch 189/300\n",
      "Average training loss: 0.048931161291069454\n",
      "Average test loss: 0.0022032906976011063\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04869377166363928\n",
      "Average test loss: 0.0021541837021294566\n",
      "Epoch 191/300\n",
      "Average training loss: 0.048781740340921614\n",
      "Average test loss: 0.002397434772923589\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04870161402887768\n",
      "Average test loss: 0.002143937201446129\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0486091052558687\n",
      "Average test loss: 0.0022352537899795505\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04858791199988789\n",
      "Average test loss: 0.0021526321273090112\n",
      "Epoch 195/300\n",
      "Average training loss: 0.048529282679160436\n",
      "Average test loss: 0.002156440141507321\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04838075005677011\n",
      "Average test loss: 0.0021778755185918675\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04848065240515603\n",
      "Average test loss: 0.002170104978606105\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04845522768298785\n",
      "Average test loss: 0.0022042189542618063\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04834221128953828\n",
      "Average test loss: 0.002220834219414327\n",
      "Epoch 200/300\n",
      "Average training loss: 0.048205755058262086\n",
      "Average test loss: 0.002166653829523259\n",
      "Epoch 201/300\n",
      "Average training loss: 0.048163587666220135\n",
      "Average test loss: 0.002208013218206664\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04829930485619439\n",
      "Average test loss: 0.0022140650215248265\n",
      "Epoch 203/300\n",
      "Average training loss: 0.048344524241156046\n",
      "Average test loss: 0.0021940199638613394\n",
      "Epoch 204/300\n",
      "Average training loss: 0.048142433712879816\n",
      "Average test loss: 0.0021525635295547543\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04802567195892334\n",
      "Average test loss: 0.0022045287514726323\n",
      "Epoch 206/300\n",
      "Average training loss: 0.048032064921326105\n",
      "Average test loss: 0.002203333916970425\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04808744950426949\n",
      "Average test loss: 0.0021874411693877643\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04792726695537567\n",
      "Average test loss: 0.0021997559083004794\n",
      "Epoch 209/300\n",
      "Average training loss: 0.047996600470609135\n",
      "Average test loss: 0.002277349291369319\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04784637202156915\n",
      "Average test loss: 0.0021924844280713134\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04782339446412193\n",
      "Average test loss: 0.002171328227242662\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04791121936175558\n",
      "Average test loss: 0.002190483773541119\n",
      "Epoch 213/300\n",
      "Average training loss: 0.047857998589674634\n",
      "Average test loss: 0.0022062782247861228\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04765601411792967\n",
      "Average test loss: 0.0021592960293508236\n",
      "Epoch 215/300\n",
      "Average training loss: 0.047815472579664654\n",
      "Average test loss: 0.0022126896700097455\n",
      "Epoch 216/300\n",
      "Average training loss: 0.047687093400292924\n",
      "Average test loss: 0.0021958110516683924\n",
      "Epoch 217/300\n",
      "Average training loss: 0.047676512911915776\n",
      "Average test loss: 0.002232242173825701\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04759263461165958\n",
      "Average test loss: 0.0022701616941226853\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04753018381032679\n",
      "Average test loss: 0.002251285551943713\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04749019844664468\n",
      "Average test loss: 0.002220244736928079\n",
      "Epoch 221/300\n",
      "Average training loss: 0.047461232761542\n",
      "Average test loss: 0.002230799802061584\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04747158322731654\n",
      "Average test loss: 0.002205957783437851\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04736525325973829\n",
      "Average test loss: 0.002207907657449444\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04742655358049604\n",
      "Average test loss: 0.0021871575330280595\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04742008512384362\n",
      "Average test loss: 0.0022307565698607102\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04733938419487741\n",
      "Average test loss: 0.0022056322501351434\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04730086856749323\n",
      "Average test loss: 0.0021908745652892522\n",
      "Epoch 228/300\n",
      "Average training loss: 0.047176523281468284\n",
      "Average test loss: 0.002257892141532567\n",
      "Epoch 229/300\n",
      "Average training loss: 0.047270071085956364\n",
      "Average test loss: 0.0021932690298805636\n",
      "Epoch 230/300\n",
      "Average training loss: 0.047194807089037366\n",
      "Average test loss: 0.0022306109683381185\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04718655580282211\n",
      "Average test loss: 0.0022598447232610648\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04704686015844345\n",
      "Average test loss: 0.0022058832405342\n",
      "Epoch 233/300\n",
      "Average training loss: 0.046946098878979686\n",
      "Average test loss: 0.0021994701957123147\n",
      "Epoch 234/300\n",
      "Average training loss: 0.046978271103567545\n",
      "Average test loss: 0.0022326568278173606\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04707272148132324\n",
      "Average test loss: 0.00222949835513201\n",
      "Epoch 236/300\n",
      "Average training loss: 0.047003753052817454\n",
      "Average test loss: 0.0022235429327314097\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04700887848271264\n",
      "Average test loss: 0.0021874709643630516\n",
      "Epoch 238/300\n",
      "Average training loss: 0.046920966257651645\n",
      "Average test loss: 0.0022285239259298476\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04696810013386939\n",
      "Average test loss: 0.0022329721668114264\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04689443243543307\n",
      "Average test loss: 0.0023473238007475934\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04688749442166752\n",
      "Average test loss: 0.002237787754999267\n",
      "Epoch 242/300\n",
      "Average training loss: 0.046697322497765224\n",
      "Average test loss: 0.0022745515066716405\n",
      "Epoch 243/300\n",
      "Average training loss: 0.046867934882640835\n",
      "Average test loss: 0.002208389731331004\n",
      "Epoch 244/300\n",
      "Average training loss: 0.046784010115596984\n",
      "Average test loss: 0.002206865816584064\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04666407600707478\n",
      "Average test loss: 0.0022056873010264503\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04677293034394582\n",
      "Average test loss: 0.0022293935331205526\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0465822012954288\n",
      "Average test loss: 0.0022141152082218066\n",
      "Epoch 248/300\n",
      "Average training loss: 0.046786971451507675\n",
      "Average test loss: 0.002226110047350327\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04663009125987689\n",
      "Average test loss: 0.002209992244425747\n",
      "Epoch 250/300\n",
      "Average training loss: 0.046643873734606635\n",
      "Average test loss: 0.0022245674108465514\n",
      "Epoch 251/300\n",
      "Average training loss: 0.046547730790244206\n",
      "Average test loss: 0.0022114651819898023\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04649188656939401\n",
      "Average test loss: 0.0022821684216873515\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04649320119950506\n",
      "Average test loss: 0.0022402530267006822\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04659380007783572\n",
      "Average test loss: 0.00219158691022959\n",
      "Epoch 255/300\n",
      "Average training loss: 0.046405911260181\n",
      "Average test loss: 0.002198105795929829\n",
      "Epoch 256/300\n",
      "Average training loss: 0.046425422959857514\n",
      "Average test loss: 0.0022430038762589295\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04639352366990513\n",
      "Average test loss: 0.002244088821630511\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0463617744743824\n",
      "Average test loss: 0.0022496859396083486\n",
      "Epoch 259/300\n",
      "Average training loss: 0.046260535238517655\n",
      "Average test loss: 0.002227458519861102\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04636434775922033\n",
      "Average test loss: 0.0021639307046102153\n",
      "Epoch 261/300\n",
      "Average training loss: 0.046341068310870065\n",
      "Average test loss: 0.002234540967063771\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04618800034953488\n",
      "Average test loss: 0.002353270185490449\n",
      "Epoch 263/300\n",
      "Average training loss: 0.046262609157297345\n",
      "Average test loss: 0.0021970949894231226\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04617233680685361\n",
      "Average test loss: 0.0022481907856547172\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04624739711152183\n",
      "Average test loss: 0.002261883375027941\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0462024421857463\n",
      "Average test loss: 0.0022202032817941575\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04609696970714463\n",
      "Average test loss: 0.002182871367989315\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04612206815348731\n",
      "Average test loss: 0.0022310134254189\n",
      "Epoch 269/300\n",
      "Average training loss: 0.045986609399318694\n",
      "Average test loss: 0.0022481352605132595\n",
      "Epoch 270/300\n",
      "Average training loss: 0.046009363445970744\n",
      "Average test loss: 0.00226348209567368\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04606637977891498\n",
      "Average test loss: 0.0022503462529016864\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04603520753979683\n",
      "Average test loss: 0.002209886211487982\n",
      "Epoch 273/300\n",
      "Average training loss: 0.045984584930870265\n",
      "Average test loss: 0.0023326303992006513\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04585999688506127\n",
      "Average test loss: 0.0022778686911074653\n",
      "Epoch 275/300\n",
      "Average training loss: 0.045908704068925645\n",
      "Average test loss: 0.0022266873701786\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04594545144173834\n",
      "Average test loss: 0.0022044634367856713\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04585339769389894\n",
      "Average test loss: 0.002229330123712619\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04586825133363406\n",
      "Average test loss: 0.0022025177623662684\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04587762042052216\n",
      "Average test loss: 0.0022682069606251186\n",
      "Epoch 280/300\n",
      "Average training loss: 0.045820067677232956\n",
      "Average test loss: 0.0022590410464132828\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04578257876634598\n",
      "Average test loss: 0.0022410986879840495\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04571625558866395\n",
      "Average test loss: 0.0022628428696965176\n",
      "Epoch 283/300\n",
      "Average training loss: 0.045750240307715205\n",
      "Average test loss: 0.002234042121718327\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04584895600047376\n",
      "Average test loss: 0.0022832709888203277\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04564333690537347\n",
      "Average test loss: 0.0022003753621959023\n",
      "Epoch 286/300\n",
      "Average training loss: 0.045770789560344485\n",
      "Average test loss: 0.0023173580522545508\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04559320582615005\n",
      "Average test loss: 0.0022355011012405155\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04558369860715336\n",
      "Average test loss: 0.0022640287309057183\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04570975883801778\n",
      "Average test loss: 0.0022260753508243298\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04561451910932859\n",
      "Average test loss: 0.002200616699643433\n",
      "Epoch 291/300\n",
      "Average training loss: 0.045581712322102654\n",
      "Average test loss: 0.0023156324226616155\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04558593280778991\n",
      "Average test loss: 0.0022482042023912073\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04552625805801815\n",
      "Average test loss: 0.0023141644563939834\n",
      "Epoch 294/300\n",
      "Average training loss: 0.045476432614856295\n",
      "Average test loss: 0.0022791841307448018\n",
      "Epoch 295/300\n",
      "Average training loss: 0.045397039035956065\n",
      "Average test loss: 0.0022827266920357942\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04558193463087082\n",
      "Average test loss: 0.0022498394124623802\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04548244349161784\n",
      "Average test loss: 0.004307057305549582\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04538809430268076\n",
      "Average test loss: 0.0022404900558499826\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04537534413735072\n",
      "Average test loss: 0.0021936932328260605\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04570091976390945\n",
      "Average test loss: 0.0022501697565118473\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.129682058122423\n",
      "Average test loss: 0.005595487978309393\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7350813777711657\n",
      "Average test loss: 0.0041753193005505534\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3649655107922024\n",
      "Average test loss: 0.0036690423701786334\n",
      "Epoch 4/300\n",
      "Average training loss: 0.24582677246464624\n",
      "Average test loss: 0.0035106642414919204\n",
      "Epoch 5/300\n",
      "Average training loss: 0.18904268607828353\n",
      "Average test loss: 0.003164266286107401\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1564395361211565\n",
      "Average test loss: 0.0030475918195313877\n",
      "Epoch 7/300\n",
      "Average training loss: 0.13544180716408624\n",
      "Average test loss: 0.0029794446200960213\n",
      "Epoch 8/300\n",
      "Average training loss: 0.12137668519549899\n",
      "Average test loss: 0.002786688400225507\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11070920677317514\n",
      "Average test loss: 0.00271623226462139\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10288329546981388\n",
      "Average test loss: 0.00269194531130294\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0964737900627984\n",
      "Average test loss: 0.0027028522174805405\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09121757078170777\n",
      "Average test loss: 0.002414962049159739\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08658950913614696\n",
      "Average test loss: 0.002303647866886523\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0826181514064471\n",
      "Average test loss: 0.002244492513851987\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07890877986285422\n",
      "Average test loss: 0.0021642414221747053\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07576666341225306\n",
      "Average test loss: 0.0020542065534326767\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07284807328051991\n",
      "Average test loss: 0.002008654328270091\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07032018052869372\n",
      "Average test loss: 0.001912722437332074\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06798340831531419\n",
      "Average test loss: 0.0018770031923842098\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06594576112098165\n",
      "Average test loss: 0.0018295864736040434\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06399723307622804\n",
      "Average test loss: 0.0018139797295961115\n",
      "Epoch 22/300\n",
      "Average training loss: 0.062316115534967846\n",
      "Average test loss: 0.001749016491489278\n",
      "Epoch 23/300\n",
      "Average training loss: 0.060706077453162935\n",
      "Average test loss: 0.001714447791998585\n",
      "Epoch 24/300\n",
      "Average training loss: 0.059411094066169526\n",
      "Average test loss: 0.0017196802181295222\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05819817210899459\n",
      "Average test loss: 0.0017323472921012177\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05695869806243314\n",
      "Average test loss: 0.0016486432710662484\n",
      "Epoch 27/300\n",
      "Average training loss: 0.056031794726848605\n",
      "Average test loss: 0.0016403846716404788\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05508511556519402\n",
      "Average test loss: 0.0016386467303252882\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05409402790665627\n",
      "Average test loss: 0.0016483458359208372\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05353898715310627\n",
      "Average test loss: 0.001613866079184744\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05297428171171083\n",
      "Average test loss: 0.0015912710660033755\n",
      "Epoch 32/300\n",
      "Average training loss: 0.052026278681225245\n",
      "Average test loss: 0.0015829155625154575\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05142979886796739\n",
      "Average test loss: 0.0015714500248432159\n",
      "Epoch 34/300\n",
      "Average training loss: 0.050994216276539696\n",
      "Average test loss: 0.0015742328301486042\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05038708300391833\n",
      "Average test loss: 0.0015560295467989312\n",
      "Epoch 36/300\n",
      "Average training loss: 0.050021932754251694\n",
      "Average test loss: 0.0015570095642987224\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0495574069056246\n",
      "Average test loss: 0.0015414369516074658\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04912309809194671\n",
      "Average test loss: 0.0015190233813805712\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04867902465164661\n",
      "Average test loss: 0.0015149278506222698\n",
      "Epoch 40/300\n",
      "Average training loss: 0.048304114899701545\n",
      "Average test loss: 0.0014981788918375968\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04799912836816576\n",
      "Average test loss: 0.0015136006160949666\n",
      "Epoch 42/300\n",
      "Average training loss: 0.047824204978015684\n",
      "Average test loss: 0.0015068132928572596\n",
      "Epoch 43/300\n",
      "Average training loss: 0.047436209085914824\n",
      "Average test loss: 0.001498682404971785\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04711532248722183\n",
      "Average test loss: 0.0014979871526981394\n",
      "Epoch 45/300\n",
      "Average training loss: 0.046866241304410826\n",
      "Average test loss: 0.001488762732491725\n",
      "Epoch 46/300\n",
      "Average training loss: 0.046569790995783275\n",
      "Average test loss: 0.0014979543128154345\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04631982284784317\n",
      "Average test loss: 0.0014789025012610687\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04617726514405674\n",
      "Average test loss: 0.0014627760673045283\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04591164185851812\n",
      "Average test loss: 0.0014513438841741947\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04570377309123675\n",
      "Average test loss: 0.0014614780706663927\n",
      "Epoch 51/300\n",
      "Average training loss: 0.045507307201623916\n",
      "Average test loss: 0.00145353712565783\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04520188127292527\n",
      "Average test loss: 0.0014658421191076439\n",
      "Epoch 53/300\n",
      "Average training loss: 0.045065887772374684\n",
      "Average test loss: 0.0014489562397615777\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04483942242132293\n",
      "Average test loss: 0.0014442018588694434\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04477922732300228\n",
      "Average test loss: 0.0014376676473249164\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04451693586508433\n",
      "Average test loss: 0.001444178954180744\n",
      "Epoch 57/300\n",
      "Average training loss: 0.044361852589580746\n",
      "Average test loss: 0.0014426217005691594\n",
      "Epoch 58/300\n",
      "Average training loss: 0.044203187952438994\n",
      "Average test loss: 0.0014627403271281058\n",
      "Epoch 59/300\n",
      "Average training loss: 0.044021051433351305\n",
      "Average test loss: 0.0014325495486458143\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04377538092931112\n",
      "Average test loss: 0.0014198504265190827\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04372094609008895\n",
      "Average test loss: 0.001424521281487412\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04353886604309082\n",
      "Average test loss: 0.0014387016776535246\n",
      "Epoch 63/300\n",
      "Average training loss: 0.043447227709823184\n",
      "Average test loss: 0.001439166440266288\n",
      "Epoch 64/300\n",
      "Average training loss: 0.043252754617068505\n",
      "Average test loss: 0.0014393046318242947\n",
      "Epoch 65/300\n",
      "Average training loss: 0.043164362016651366\n",
      "Average test loss: 0.0014222321523249977\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04293952208095127\n",
      "Average test loss: 0.0014288186355390481\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04280270463228226\n",
      "Average test loss: 0.001429545141549574\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0426671335167355\n",
      "Average test loss: 0.0014388455336706507\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04270501863294177\n",
      "Average test loss: 0.0014367900742735299\n",
      "Epoch 70/300\n",
      "Average training loss: 0.042414416432380676\n",
      "Average test loss: 0.0014206011073870791\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04228926030132506\n",
      "Average test loss: 0.0014203811409986681\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0421233033074273\n",
      "Average test loss: 0.0014166869568741985\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04208591840333409\n",
      "Average test loss: 0.0014258230353395144\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04187575412624412\n",
      "Average test loss: 0.0014158359875695572\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04172235301964813\n",
      "Average test loss: 0.0014156757313758136\n",
      "Epoch 76/300\n",
      "Average training loss: 0.041802504332529175\n",
      "Average test loss: 0.0014101868451883396\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04152354149023692\n",
      "Average test loss: 0.0014327626591030922\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04134050720930099\n",
      "Average test loss: 0.00144722430785704\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0412719446586238\n",
      "Average test loss: 0.0014135959373994006\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04109566331572003\n",
      "Average test loss: 0.001412814085992674\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04101328475442197\n",
      "Average test loss: 0.0014208867044912443\n",
      "Epoch 82/300\n",
      "Average training loss: 0.040837241695986855\n",
      "Average test loss: 0.0014084762446582317\n",
      "Epoch 83/300\n",
      "Average training loss: 0.040786528299252195\n",
      "Average test loss: 0.0014101017977421481\n",
      "Epoch 84/300\n",
      "Average training loss: 0.040746990187300575\n",
      "Average test loss: 0.001419446692491571\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04053017629517449\n",
      "Average test loss: 0.0014196220822632313\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04041521004339059\n",
      "Average test loss: 0.0014208574599275986\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04037384291158782\n",
      "Average test loss: 0.0014265258668197526\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04017342986332045\n",
      "Average test loss: 0.0014255739193823602\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04006742489337921\n",
      "Average test loss: 0.001439064723594735\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04001068535943826\n",
      "Average test loss: 0.0014209135327902106\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03985619424117936\n",
      "Average test loss: 0.0014149125646799802\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03978126871751415\n",
      "Average test loss: 0.0014235497688253722\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03964282655384806\n",
      "Average test loss: 0.0014255899290243784\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03948805770609114\n",
      "Average test loss: 0.0014263104575479196\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03947252027524842\n",
      "Average test loss: 0.00143152731946773\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03935688793328073\n",
      "Average test loss: 0.0014401746485382318\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03929254592789544\n",
      "Average test loss: 0.001433488757401291\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03917614308827453\n",
      "Average test loss: 0.0014374299660945933\n",
      "Epoch 99/300\n",
      "Average training loss: 0.039146516627735564\n",
      "Average test loss: 0.001420431810224222\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03894698904289139\n",
      "Average test loss: 0.0014329746736006603\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03887169578340319\n",
      "Average test loss: 0.001416459945237471\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03873168470958869\n",
      "Average test loss: 0.0014202152643766669\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03861907343400849\n",
      "Average test loss: 0.001435350655991998\n",
      "Epoch 104/300\n",
      "Average training loss: 0.038492699169450334\n",
      "Average test loss: 0.0014105838184348411\n",
      "Epoch 105/300\n",
      "Average training loss: 0.038537926091088186\n",
      "Average test loss: 0.0014523663260042668\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03830990950597657\n",
      "Average test loss: 0.001444020943923129\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03834203659163581\n",
      "Average test loss: 0.0014414032803744905\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03811333940592077\n",
      "Average test loss: 0.0014842896512192157\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0381399567855729\n",
      "Average test loss: 0.0014541119154956605\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03802238736218876\n",
      "Average test loss: 0.0014311253072486984\n",
      "Epoch 111/300\n",
      "Average training loss: 0.038067718868454295\n",
      "Average test loss: 0.0014482588491713008\n",
      "Epoch 112/300\n",
      "Average training loss: 0.037860669962233966\n",
      "Average test loss: 0.0014332013952856263\n",
      "Epoch 113/300\n",
      "Average training loss: 0.037746464666393066\n",
      "Average test loss: 0.001430770593902303\n",
      "Epoch 114/300\n",
      "Average training loss: 0.037623974576592445\n",
      "Average test loss: 0.0014827670856999854\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03760326251387596\n",
      "Average test loss: 0.0014631204789297449\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03758465132448408\n",
      "Average test loss: 0.0014550013279852768\n",
      "Epoch 117/300\n",
      "Average training loss: 0.037373111142052544\n",
      "Average test loss: 0.0014314713429452645\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03728743697702885\n",
      "Average test loss: 0.001461851382886784\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03725227932466401\n",
      "Average test loss: 0.0014343415315573413\n",
      "Epoch 120/300\n",
      "Average training loss: 0.037153853548897636\n",
      "Average test loss: 0.0014681416221687363\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03712891734805372\n",
      "Average test loss: 0.0014467022734073302\n",
      "Epoch 122/300\n",
      "Average training loss: 0.037148824764622584\n",
      "Average test loss: 0.0014963222522702481\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03694272325105137\n",
      "Average test loss: 0.0014506200050107307\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03689809177319209\n",
      "Average test loss: 0.0014755179629557663\n",
      "Epoch 125/300\n",
      "Average training loss: 0.036774345863196584\n",
      "Average test loss: 0.0014236300771849023\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03670255439149009\n",
      "Average test loss: 0.0014518048342110383\n",
      "Epoch 127/300\n",
      "Average training loss: 0.036587070975038744\n",
      "Average test loss: 0.0014399374073578253\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03655100370777978\n",
      "Average test loss: 0.0014523607593340177\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03648644803298844\n",
      "Average test loss: 0.0014722699010744692\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03641820618510246\n",
      "Average test loss: 0.0014747297479253676\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03642360093858507\n",
      "Average test loss: 0.00145162789332163\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03632168643673261\n",
      "Average test loss: 0.0014845231977394886\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03624124946528011\n",
      "Average test loss: 0.0014459087053934733\n",
      "Epoch 134/300\n",
      "Average training loss: 0.036200891017913815\n",
      "Average test loss: 0.00145917448360059\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03607692379918363\n",
      "Average test loss: 0.0014513187366020348\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03609922699464692\n",
      "Average test loss: 0.001475488341723879\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03596894137561321\n",
      "Average test loss: 0.0014948487862323722\n",
      "Epoch 138/300\n",
      "Average training loss: 0.035934403636389306\n",
      "Average test loss: 0.001500546481460333\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03581411478916804\n",
      "Average test loss: 0.0015242808350465363\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0357644451343351\n",
      "Average test loss: 0.0014855377474385831\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03566577601267232\n",
      "Average test loss: 0.0014578202803515725\n",
      "Epoch 142/300\n",
      "Average training loss: 0.035605517463551624\n",
      "Average test loss: 0.0015151074573708078\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03559369460410542\n",
      "Average test loss: 0.0014998760342391\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03561671075556013\n",
      "Average test loss: 0.0014896432464528416\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03573621146877607\n",
      "Average test loss: 0.0014803624275243944\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03540757612056202\n",
      "Average test loss: 0.0014488811226975587\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03529319889015622\n",
      "Average test loss: 0.0014769761320203544\n",
      "Epoch 148/300\n",
      "Average training loss: 0.035322619282537035\n",
      "Average test loss: 0.001477130750608113\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03526027200122674\n",
      "Average test loss: 0.0014856136888265609\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03514903966585795\n",
      "Average test loss: 0.001457703915424645\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03507878989312384\n",
      "Average test loss: 0.001474156167668601\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0351863581041495\n",
      "Average test loss: 0.0014624055944797065\n",
      "Epoch 153/300\n",
      "Average training loss: 0.035055208249224555\n",
      "Average test loss: 0.0015022150659933686\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03495648552146223\n",
      "Average test loss: 0.00146872849493391\n",
      "Epoch 155/300\n",
      "Average training loss: 0.034861622894803684\n",
      "Average test loss: 0.0014988129480431476\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03489717447757721\n",
      "Average test loss: 0.0014918217683831851\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03485863007936213\n",
      "Average test loss: 0.0014962369789265923\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03473577064606879\n",
      "Average test loss: 0.0015347654443855086\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03470562974611918\n",
      "Average test loss: 0.0014851687523639865\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0346404904557599\n",
      "Average test loss: 0.001479537592838622\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03461124325129721\n",
      "Average test loss: 0.0015415744847721523\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03456751345925861\n",
      "Average test loss: 0.0014947765995230939\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03456952279806137\n",
      "Average test loss: 0.0014859139180431764\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03448872929480341\n",
      "Average test loss: 0.0014919503281513849\n",
      "Epoch 165/300\n",
      "Average training loss: 0.034443100843164653\n",
      "Average test loss: 0.0014664143930292791\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03443790553013484\n",
      "Average test loss: 0.0014783888541989856\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03438993001315329\n",
      "Average test loss: 0.0015011067039643726\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03427718017995358\n",
      "Average test loss: 0.0015114117672459946\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0342065849006176\n",
      "Average test loss: 0.0015094603962368435\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0342516668488582\n",
      "Average test loss: 0.0015052671420077482\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03423108012974262\n",
      "Average test loss: 0.0014906930302580198\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03409155295292536\n",
      "Average test loss: 0.001500145438955062\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03407521668904358\n",
      "Average test loss: 0.0017989683716247479\n",
      "Epoch 174/300\n",
      "Average training loss: 0.034075833366976845\n",
      "Average test loss: 0.0014999155323021113\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03402835426893499\n",
      "Average test loss: 0.001538309051344792\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03396093114879396\n",
      "Average test loss: 0.0014827057373606497\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03389520001742575\n",
      "Average test loss: 0.0015396953352933957\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03381793388227622\n",
      "Average test loss: 0.0015380106538327204\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03379432001047664\n",
      "Average test loss: 0.001529205375454492\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03379927718970511\n",
      "Average test loss: 0.0014943519542511138\n",
      "Epoch 181/300\n",
      "Average training loss: 0.033862787192066514\n",
      "Average test loss: 0.0015329312846168047\n",
      "Epoch 182/300\n",
      "Average training loss: 0.033762025790082084\n",
      "Average test loss: 0.001534451130260196\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03365236460003588\n",
      "Average test loss: 0.0015073586055595013\n",
      "Epoch 184/300\n",
      "Average training loss: 0.033648674915234246\n",
      "Average test loss: 0.0015307972157994907\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0335152715643247\n",
      "Average test loss: 0.00157580794300884\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03359481897950172\n",
      "Average test loss: 0.0015025635505509045\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03352530827621619\n",
      "Average test loss: 0.0015186367368118631\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03362128328449196\n",
      "Average test loss: 0.0015297129291834102\n",
      "Epoch 189/300\n",
      "Average training loss: 0.033454652385579216\n",
      "Average test loss: 0.001517304489078621\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03339253364668952\n",
      "Average test loss: 0.0014875687528401614\n",
      "Epoch 191/300\n",
      "Average training loss: 0.033417632165882324\n",
      "Average test loss: 0.0014892681156699029\n",
      "Epoch 192/300\n",
      "Average training loss: 0.033334585708048606\n",
      "Average test loss: 0.001561642350628972\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03333396160105864\n",
      "Average test loss: 0.0015237938751363092\n",
      "Epoch 194/300\n",
      "Average training loss: 0.033340009263820115\n",
      "Average test loss: 0.0015134054128494528\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03331900105873744\n",
      "Average test loss: 0.0015512224003258678\n",
      "Epoch 196/300\n",
      "Average training loss: 0.033230449636777244\n",
      "Average test loss: 0.0015045540594599313\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03322850990626547\n",
      "Average test loss: 0.0015591762023460533\n",
      "Epoch 198/300\n",
      "Average training loss: 0.033174862563610075\n",
      "Average test loss: 0.0015360452425148751\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03309124276704258\n",
      "Average test loss: 0.0015300583338571918\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03315398272871971\n",
      "Average test loss: 0.00150838309350527\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03305602403481801\n",
      "Average test loss: 0.0015413950009581943\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03303897580007712\n",
      "Average test loss: 0.00151709927411543\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03305217553509606\n",
      "Average test loss: 0.0015753720818708341\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03302549024671316\n",
      "Average test loss: 0.0015579921781188912\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03289825460645888\n",
      "Average test loss: 0.001503466814963354\n",
      "Epoch 206/300\n",
      "Average training loss: 0.032956897540224926\n",
      "Average test loss: 0.0015243872409272524\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03295894325110647\n",
      "Average test loss: 0.001527376221699847\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03292204153041045\n",
      "Average test loss: 0.001511131392377946\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0327893648362822\n",
      "Average test loss: 0.0015421881868193548\n",
      "Epoch 210/300\n",
      "Average training loss: 0.032818180254764025\n",
      "Average test loss: 0.0015449204434537225\n",
      "Epoch 211/300\n",
      "Average training loss: 0.032767326535450086\n",
      "Average test loss: 0.0015222736909571621\n",
      "Epoch 212/300\n",
      "Average training loss: 0.032713508299655386\n",
      "Average test loss: 0.0015451525573929152\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03281758505933815\n",
      "Average test loss: 0.001566536965366039\n",
      "Epoch 214/300\n",
      "Average training loss: 0.032739280098014405\n",
      "Average test loss: 0.0015220555766589112\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0325565009812514\n",
      "Average test loss: 0.0015605220522524582\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03263525292608473\n",
      "Average test loss: 0.001626891617042323\n",
      "Epoch 217/300\n",
      "Average training loss: 0.032591296202606625\n",
      "Average test loss: 0.001543815691748427\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03256208115484979\n",
      "Average test loss: 0.0015484897958942586\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03258080230818854\n",
      "Average test loss: 0.0015199683257378638\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03253176776236958\n",
      "Average test loss: 0.0017313242086933718\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03251027087701692\n",
      "Average test loss: 0.001569327431730926\n",
      "Epoch 222/300\n",
      "Average training loss: 0.032439221704999605\n",
      "Average test loss: 0.0015475539864144392\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03237954977154732\n",
      "Average test loss: 0.0015451442537208397\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03244562929371993\n",
      "Average test loss: 0.0015412689284938904\n",
      "Epoch 225/300\n",
      "Average training loss: 0.032347780850198536\n",
      "Average test loss: 0.001510493153706193\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03230901885198222\n",
      "Average test loss: 0.0015503694189505444\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03240707539187537\n",
      "Average test loss: 0.0016741579158438577\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03233041332330969\n",
      "Average test loss: 0.0015387200455491742\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03229005510939492\n",
      "Average test loss: 0.001598841508022613\n",
      "Epoch 230/300\n",
      "Average training loss: 0.032277945852941935\n",
      "Average test loss: 0.00153533076432844\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03221081818474664\n",
      "Average test loss: 0.0015043471232056619\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0322175559633308\n",
      "Average test loss: 0.0015296203291768002\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03223612024552292\n",
      "Average test loss: 0.0015443726124034988\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03215428677532408\n",
      "Average test loss: 0.0016061635134327742\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03215202547113101\n",
      "Average test loss: 0.001555967916889737\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03221555124223232\n",
      "Average test loss: 0.00154492937007712\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03212705423103439\n",
      "Average test loss: 0.001574623489441971\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03212835996515221\n",
      "Average test loss: 0.0015592743481198947\n",
      "Epoch 239/300\n",
      "Average training loss: 0.032023092079493734\n",
      "Average test loss: 0.001536228049805181\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03195673867397838\n",
      "Average test loss: 0.001657305197376344\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03204462765653928\n",
      "Average test loss: 0.001528985826091634\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03203442162109746\n",
      "Average test loss: 0.0015406783510827356\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03188769570324156\n",
      "Average test loss: 0.0015666312020685938\n",
      "Epoch 244/300\n",
      "Average training loss: 0.031861613379584416\n",
      "Average test loss: 0.0015706426424181297\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03190012250840664\n",
      "Average test loss: 0.0015921091925766733\n",
      "Epoch 246/300\n",
      "Average training loss: 0.031874171164300705\n",
      "Average test loss: 0.0015134575525298714\n",
      "Epoch 247/300\n",
      "Average training loss: 0.031965149191518626\n",
      "Average test loss: 0.0016182881634061535\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03183472653561168\n",
      "Average test loss: 0.001566628510494613\n",
      "Epoch 249/300\n",
      "Average training loss: 0.031848402361075084\n",
      "Average test loss: 0.0015803049287448326\n",
      "Epoch 250/300\n",
      "Average training loss: 0.031810229520003\n",
      "Average test loss: 0.001557592436671257\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0317340094546477\n",
      "Average test loss: 0.0016426804963913229\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03179796631468667\n",
      "Average test loss: 0.0015945860707304543\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03170788951549265\n",
      "Average test loss: 0.001565314753808909\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03171972730093532\n",
      "Average test loss: 0.0015323470928188827\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03173381589684222\n",
      "Average test loss: 0.0015643338397559193\n",
      "Epoch 256/300\n",
      "Average training loss: 0.031674892137447994\n",
      "Average test loss: 0.0015690908753830525\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03168154740995831\n",
      "Average test loss: 0.0015683034544603691\n",
      "Epoch 258/300\n",
      "Average training loss: 0.031692397788167\n",
      "Average test loss: 0.001557964919341935\n",
      "Epoch 259/300\n",
      "Average training loss: 0.031580511243806945\n",
      "Average test loss: 0.0015533109326950378\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03162268376515971\n",
      "Average test loss: 0.0015130507268735932\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03156778503788842\n",
      "Average test loss: 0.0015468466935886277\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03161789741449886\n",
      "Average test loss: 0.0015453699812706973\n",
      "Epoch 263/300\n",
      "Average training loss: 0.031530864179134366\n",
      "Average test loss: 0.001567338056448433\n",
      "Epoch 264/300\n",
      "Average training loss: 0.031552950595815976\n",
      "Average test loss: 0.0016220642500039603\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03149402878681819\n",
      "Average test loss: 0.0015492829503491522\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03150840760105186\n",
      "Average test loss: 0.001563271950930357\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03153900868859556\n",
      "Average test loss: 0.0016016638684603903\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03149645784000556\n",
      "Average test loss: 0.0015561837705059184\n",
      "Epoch 269/300\n",
      "Average training loss: 0.031422415234976346\n",
      "Average test loss: 0.0015406031422317027\n",
      "Epoch 270/300\n",
      "Average training loss: 0.031403095392717253\n",
      "Average test loss: 0.0015380487783501545\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03141244673397806\n",
      "Average test loss: 0.001616139016010695\n",
      "Epoch 272/300\n",
      "Average training loss: 0.031405308879084055\n",
      "Average test loss: 0.0015756302436606751\n",
      "Epoch 273/300\n",
      "Average training loss: 0.031301165882084106\n",
      "Average test loss: 0.001566291801010569\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03134400623540084\n",
      "Average test loss: 0.001540145876713925\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03135270182126098\n",
      "Average test loss: 0.0015257329512387513\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03135483310951127\n",
      "Average test loss: 0.0015473927311185334\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03134808466169569\n",
      "Average test loss: 0.0016058413084182475\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0312093233399921\n",
      "Average test loss: 0.0015670742218693097\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03130310237076547\n",
      "Average test loss: 0.0015572787928394972\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03124877865612507\n",
      "Average test loss: 0.0015580184254795314\n",
      "Epoch 281/300\n",
      "Average training loss: 0.031240325974093545\n",
      "Average test loss: 0.0015330434137334427\n",
      "Epoch 282/300\n",
      "Average training loss: 0.031434516358706685\n",
      "Average test loss: 0.0015647061241583693\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03124680275387234\n",
      "Average test loss: 0.0015643152284125488\n",
      "Epoch 284/300\n",
      "Average training loss: 0.031137062167127926\n",
      "Average test loss: 0.0016048708730894657\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03127480507890383\n",
      "Average test loss: 0.0016007815845724609\n",
      "Epoch 286/300\n",
      "Average training loss: 0.031122177941931617\n",
      "Average test loss: 0.0016475578335424265\n",
      "Epoch 287/300\n",
      "Average training loss: 0.031156570537222757\n",
      "Average test loss: 0.0015439015112610327\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03123652822441525\n",
      "Average test loss: 0.0016432335175987746\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03103202529417144\n",
      "Average test loss: 0.0016026329572002094\n",
      "Epoch 290/300\n",
      "Average training loss: 0.031080153965287738\n",
      "Average test loss: 0.0015530948294326663\n",
      "Epoch 291/300\n",
      "Average training loss: 0.031113251189390817\n",
      "Average test loss: 0.0015780163964049683\n",
      "Epoch 292/300\n",
      "Average training loss: 0.031112763025694424\n",
      "Average test loss: 0.001722907905673815\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03104850335419178\n",
      "Average test loss: 0.0016072486812869708\n",
      "Epoch 294/300\n",
      "Average training loss: 0.031011917834480603\n",
      "Average test loss: 0.0015650535850889153\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030963178015417522\n",
      "Average test loss: 0.0016023514554318454\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030997476526432567\n",
      "Average test loss: 0.0015676556722157532\n",
      "Epoch 297/300\n",
      "Average training loss: 0.031021527724133597\n",
      "Average test loss: 0.0015637958769996962\n",
      "Epoch 298/300\n",
      "Average training loss: 0.030995083048939706\n",
      "Average test loss: 0.001587040085138546\n",
      "Epoch 299/300\n",
      "Average training loss: 0.030942571674784026\n",
      "Average test loss: 0.0016145987413409683\n",
      "Epoch 300/300\n",
      "Average training loss: 0.030962059868706596\n",
      "Average test loss: 0.0016081035127863289\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_50_Depth3/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.24\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.80\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.19\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.39\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.34\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.72\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.69\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.79\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.86\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.00\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.99\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.95\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.11\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.01\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.10\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.14\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.71\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.60\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.21\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.35\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.89\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.73\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.41\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.01\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.26\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.29\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 33.54\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.35\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.64\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.62\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.63\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.78\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.69\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.82\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.89\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.90\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.85\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.91\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.85\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.87\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6baa57-dd11-42fa-a232-da029e59096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder DCT_50_Depth3 has been zipped successfully into DCT_50_Depth3_Out.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "folder_to_zip = 'DCT_50_Depth3'\n",
    "\n",
    "# Define the output zip file name\n",
    "output_filename = 'DCT_50_Depth3_Out'\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(output_filename, 'zip', folder_to_zip)\n",
    "\n",
    "print(f'Folder {folder_to_zip} has been zipped successfully into {output_filename}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7e850-5279-44aa-976e-9e33e56d6ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
