{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_32x32.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15912665901581446\n",
      "Average test loss: 0.010819571048435239\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05982513772447904\n",
      "Average test loss: 0.009283918023109437\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05363385879000028\n",
      "Average test loss: 0.010215775200062328\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05055068678657214\n",
      "Average test loss: 0.00897401708861192\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0482915865249104\n",
      "Average test loss: 0.008952076668540638\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04682824528217316\n",
      "Average test loss: 0.008733264753801956\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0452038317322731\n",
      "Average test loss: 0.008384317281345527\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04422675510909822\n",
      "Average test loss: 0.008420532531208462\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04346556088990635\n",
      "Average test loss: 0.00859405030641291\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04279420127140151\n",
      "Average test loss: 0.007689820237457752\n",
      "Epoch 11/300\n",
      "Average training loss: 0.042084310316377216\n",
      "Average test loss: 0.007866067089968258\n",
      "Epoch 12/300\n",
      "Average training loss: 0.041520668006605574\n",
      "Average test loss: 0.007608515149603287\n",
      "Epoch 13/300\n",
      "Average training loss: 0.040845125450028316\n",
      "Average test loss: 0.007515749391582277\n",
      "Epoch 14/300\n",
      "Average training loss: 0.040473187761174305\n",
      "Average test loss: 0.007540808306386073\n",
      "Epoch 15/300\n",
      "Average training loss: 0.040026504728529186\n",
      "Average test loss: 0.007610702993141281\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03955737008651097\n",
      "Average test loss: 0.007432947791284985\n",
      "Epoch 17/300\n",
      "Average training loss: 0.039175998038715784\n",
      "Average test loss: 0.007268247616787751\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03879858929912249\n",
      "Average test loss: 0.007139989857044485\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03835615291529232\n",
      "Average test loss: 0.007358825921598408\n",
      "Epoch 20/300\n",
      "Average training loss: 0.038031032486094365\n",
      "Average test loss: 0.00715655650322636\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03764970882733663\n",
      "Average test loss: 0.007149100400093529\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03736926802661684\n",
      "Average test loss: 0.007134498571770059\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03717394398649534\n",
      "Average test loss: 0.00699377631313271\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03683353154857953\n",
      "Average test loss: 0.00706876667175028\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03671528642707401\n",
      "Average test loss: 0.007057014426837364\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03646618997057279\n",
      "Average test loss: 0.007297783407072226\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03623044215308295\n",
      "Average test loss: 0.006864156140635411\n",
      "Epoch 28/300\n",
      "Average training loss: 0.035946509298351075\n",
      "Average test loss: 0.007213507860898972\n",
      "Epoch 29/300\n",
      "Average training loss: 0.035863324536217586\n",
      "Average test loss: 0.00681965919625428\n",
      "Epoch 30/300\n",
      "Average training loss: 0.035518833574321534\n",
      "Average test loss: 0.009022287401888105\n",
      "Epoch 31/300\n",
      "Average training loss: 0.035391626359687914\n",
      "Average test loss: 0.0068295216280966995\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03523359554509322\n",
      "Average test loss: 0.0068198519229061075\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03505140837199158\n",
      "Average test loss: 0.00689915388243066\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03491121494107776\n",
      "Average test loss: 0.00692366196670466\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03471527767512533\n",
      "Average test loss: 0.0070621314702762495\n",
      "Epoch 36/300\n",
      "Average training loss: 0.034619794891940225\n",
      "Average test loss: 0.008832735778143008\n",
      "Epoch 37/300\n",
      "Average training loss: 0.034382281919320426\n",
      "Average test loss: 0.006688538499590424\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03433041042917304\n",
      "Average test loss: 0.007345884751528501\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03413856569925944\n",
      "Average test loss: 0.006949677827457587\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03395630794763565\n",
      "Average test loss: 0.0067759953298502495\n",
      "Epoch 41/300\n",
      "Average training loss: 0.033947241728504496\n",
      "Average test loss: 0.006715973059336344\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03377720566921764\n",
      "Average test loss: 0.006754765529599455\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03361695394251082\n",
      "Average test loss: 0.006629978166272243\n",
      "Epoch 44/300\n",
      "Average training loss: 0.033574299140108954\n",
      "Average test loss: 0.007591663741403156\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03341026809149318\n",
      "Average test loss: 0.006649499389860365\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03332436950008075\n",
      "Average test loss: 0.0067433066152864036\n",
      "Epoch 47/300\n",
      "Average training loss: 0.033232920413215954\n",
      "Average test loss: 0.006889932355533044\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03316136741969321\n",
      "Average test loss: 0.006798404697742727\n",
      "Epoch 49/300\n",
      "Average training loss: 0.032990669220685956\n",
      "Average test loss: 0.006818800678683652\n",
      "Epoch 50/300\n",
      "Average training loss: 0.032950660636027654\n",
      "Average test loss: 0.006731718759983778\n",
      "Epoch 51/300\n",
      "Average training loss: 0.032846069077650704\n",
      "Average test loss: 0.006717126175347301\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03271508418023586\n",
      "Average test loss: 0.006728241878251235\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03270157392985291\n",
      "Average test loss: 0.0067430619580878155\n",
      "Epoch 54/300\n",
      "Average training loss: 0.032546408010853664\n",
      "Average test loss: 0.007283339751677381\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03236453400552273\n",
      "Average test loss: 0.006678499276439349\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03237032793958982\n",
      "Average test loss: 0.007298091234018405\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03224937394592497\n",
      "Average test loss: 0.006962057935694854\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03230319209231271\n",
      "Average test loss: 0.006844946065296729\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03213377334343062\n",
      "Average test loss: 0.006690566665182511\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03198055921163824\n",
      "Average test loss: 0.006677040711873108\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03193205060892635\n",
      "Average test loss: 0.006612203009840515\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03187701383564207\n",
      "Average test loss: 0.009393108317835464\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03180192979673545\n",
      "Average test loss: 0.006747936453256343\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03177277938856019\n",
      "Average test loss: 0.006885570763299862\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03169267756740252\n",
      "Average test loss: 0.084011644118362\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03185401111841202\n",
      "Average test loss: 0.006768037167688211\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03149229850702816\n",
      "Average test loss: 0.006889132767915726\n",
      "Epoch 68/300\n",
      "Average training loss: 0.031449907037946916\n",
      "Average test loss: 0.006686126766105493\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03141161060995526\n",
      "Average test loss: 0.006832638390362263\n",
      "Epoch 70/300\n",
      "Average training loss: 0.031372050202555124\n",
      "Average test loss: 0.006740971573111084\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03125995586150222\n",
      "Average test loss: 0.006686583589762449\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03114689150452614\n",
      "Average test loss: 0.008377369018892447\n",
      "Epoch 73/300\n",
      "Average training loss: 0.031101099022560648\n",
      "Average test loss: 0.007193411897453997\n",
      "Epoch 74/300\n",
      "Average training loss: 0.031118610746330686\n",
      "Average test loss: 0.006781229183077812\n",
      "Epoch 75/300\n",
      "Average training loss: 0.030968568172719743\n",
      "Average test loss: 0.006589510082784626\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03094788202146689\n",
      "Average test loss: 0.007205042824563053\n",
      "Epoch 77/300\n",
      "Average training loss: 0.030986186466283268\n",
      "Average test loss: 0.006756265939109855\n",
      "Epoch 78/300\n",
      "Average training loss: 0.030846350060568916\n",
      "Average test loss: 0.00681126784077949\n",
      "Epoch 79/300\n",
      "Average training loss: 0.030788597954644097\n",
      "Average test loss: 0.007054780643433333\n",
      "Epoch 80/300\n",
      "Average training loss: 0.030729032130704985\n",
      "Average test loss: 0.006767386980354786\n",
      "Epoch 81/300\n",
      "Average training loss: 0.030723869795600572\n",
      "Average test loss: 0.006999978430569172\n",
      "Epoch 82/300\n",
      "Average training loss: 0.030698833480477333\n",
      "Average test loss: 0.012133460909956031\n",
      "Epoch 83/300\n",
      "Average training loss: 0.030625676323970157\n",
      "Average test loss: 0.006709417808800936\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03048375615477562\n",
      "Average test loss: 0.007241584681802326\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0304591906203164\n",
      "Average test loss: 0.006849791615373559\n",
      "Epoch 86/300\n",
      "Average training loss: 0.030455114073223537\n",
      "Average test loss: 0.006808346038477288\n",
      "Epoch 87/300\n",
      "Average training loss: 0.030364068337612682\n",
      "Average test loss: 0.00701398391276598\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03031121718717946\n",
      "Average test loss: 0.006748709257692099\n",
      "Epoch 89/300\n",
      "Average training loss: 0.030323220757974518\n",
      "Average test loss: 0.006903335868484444\n",
      "Epoch 90/300\n",
      "Average training loss: 0.030198996860120033\n",
      "Average test loss: 0.0070987893036670155\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03020610239936246\n",
      "Average test loss: 0.0071591699454519485\n",
      "Epoch 92/300\n",
      "Average training loss: 0.030151089959674412\n",
      "Average test loss: 0.006842702709966236\n",
      "Epoch 93/300\n",
      "Average training loss: 0.030112485430306857\n",
      "Average test loss: 0.006736513767805365\n",
      "Epoch 94/300\n",
      "Average training loss: 0.030039888996216985\n",
      "Average test loss: 0.006822379002140628\n",
      "Epoch 95/300\n",
      "Average training loss: 0.030010235940416654\n",
      "Average test loss: 0.006953204063491689\n",
      "Epoch 96/300\n",
      "Average training loss: 0.029947959668106504\n",
      "Average test loss: 0.007381225506878562\n",
      "Epoch 97/300\n",
      "Average training loss: 0.029880522380272546\n",
      "Average test loss: 0.006826320143623485\n",
      "Epoch 98/300\n",
      "Average training loss: 0.029854776846037972\n",
      "Average test loss: 0.006875428427424696\n",
      "Epoch 99/300\n",
      "Average training loss: 0.029838624828391606\n",
      "Average test loss: 0.006882638468096653\n",
      "Epoch 100/300\n",
      "Average training loss: 0.029772819669710265\n",
      "Average test loss: 0.006794417624672254\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0297234976771805\n",
      "Average test loss: 0.006830007316751613\n",
      "Epoch 102/300\n",
      "Average training loss: 0.029711154454284246\n",
      "Average test loss: 0.0068647732428378525\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02961183897819784\n",
      "Average test loss: 0.006755104875812928\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02959651568863127\n",
      "Average test loss: 0.006897392592496342\n",
      "Epoch 105/300\n",
      "Average training loss: 0.029646260056230757\n",
      "Average test loss: 0.006890138801187277\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02957598565187719\n",
      "Average test loss: 0.006988482191330856\n",
      "Epoch 107/300\n",
      "Average training loss: 0.029488422037826645\n",
      "Average test loss: 0.006932489651772711\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02939694801138507\n",
      "Average test loss: 0.006754103246662352\n",
      "Epoch 109/300\n",
      "Average training loss: 0.029479268845584657\n",
      "Average test loss: 0.007098287533140845\n",
      "Epoch 110/300\n",
      "Average training loss: 0.029429562446143892\n",
      "Average test loss: 0.0068135368141035236\n",
      "Epoch 111/300\n",
      "Average training loss: 0.029376802715990278\n",
      "Average test loss: 0.006801819562084145\n",
      "Epoch 112/300\n",
      "Average training loss: 0.029279728078179888\n",
      "Average test loss: 0.00682819112141927\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02927370076543755\n",
      "Average test loss: 0.006988038120584355\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02925987494488557\n",
      "Average test loss: 0.007269474713752667\n",
      "Epoch 115/300\n",
      "Average training loss: 0.029212509454952344\n",
      "Average test loss: 0.007037912514888578\n",
      "Epoch 116/300\n",
      "Average training loss: 0.029213570856385762\n",
      "Average test loss: 0.007075569190084934\n",
      "Epoch 117/300\n",
      "Average training loss: 0.029188462913036346\n",
      "Average test loss: 0.006892102626048857\n",
      "Epoch 118/300\n",
      "Average training loss: 0.029121818754408096\n",
      "Average test loss: 0.006921800138221847\n",
      "Epoch 119/300\n",
      "Average training loss: 0.029031694976819886\n",
      "Average test loss: 0.007099299108816517\n",
      "Epoch 120/300\n",
      "Average training loss: 0.029083251203099885\n",
      "Average test loss: 0.006855691746705108\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02898655050330692\n",
      "Average test loss: 0.006937695709781515\n",
      "Epoch 122/300\n",
      "Average training loss: 0.028991537935203975\n",
      "Average test loss: 0.007280767593946722\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02893061787221167\n",
      "Average test loss: 0.006832468236900038\n",
      "Epoch 124/300\n",
      "Average training loss: 0.028930420673555796\n",
      "Average test loss: 0.0072209857031703\n",
      "Epoch 125/300\n",
      "Average training loss: 0.028943403277132247\n",
      "Average test loss: 0.006981255670388539\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02887569413913621\n",
      "Average test loss: 0.007920752887924513\n",
      "Epoch 127/300\n",
      "Average training loss: 0.028848329424858092\n",
      "Average test loss: 0.006949893013884624\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02877852575480938\n",
      "Average test loss: 0.007159554419418176\n",
      "Epoch 129/300\n",
      "Average training loss: 0.028816443661848705\n",
      "Average test loss: 0.006987630676064226\n",
      "Epoch 130/300\n",
      "Average training loss: 0.028772275669707192\n",
      "Average test loss: 0.006963159950657024\n",
      "Epoch 131/300\n",
      "Average training loss: 0.028735386189487247\n",
      "Average test loss: 0.006959650893592172\n",
      "Epoch 132/300\n",
      "Average training loss: 0.028710942467053732\n",
      "Average test loss: 0.007289349812186426\n",
      "Epoch 133/300\n",
      "Average training loss: 0.028673951576153437\n",
      "Average test loss: 0.007772303736458222\n",
      "Epoch 134/300\n",
      "Average training loss: 0.028657507085137895\n",
      "Average test loss: 0.006838332364749578\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028598150718543266\n",
      "Average test loss: 0.00750211684902509\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0285597672180997\n",
      "Average test loss: 0.006956604938540193\n",
      "Epoch 137/300\n",
      "Average training loss: 0.028526488865415256\n",
      "Average test loss: 0.007042738394190868\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02853372178806199\n",
      "Average test loss: 0.006913882289081812\n",
      "Epoch 139/300\n",
      "Average training loss: 0.028518882570995226\n",
      "Average test loss: 0.0072450648364093565\n",
      "Epoch 140/300\n",
      "Average training loss: 0.028493259885244898\n",
      "Average test loss: 0.00737003200666772\n",
      "Epoch 141/300\n",
      "Average training loss: 0.028445985299017693\n",
      "Average test loss: 0.007032002713117334\n",
      "Epoch 142/300\n",
      "Average training loss: 0.028462622803118495\n",
      "Average test loss: 0.0071989608206268815\n",
      "Epoch 143/300\n",
      "Average training loss: 0.028362668606970044\n",
      "Average test loss: 0.007076818749308586\n",
      "Epoch 144/300\n",
      "Average training loss: 0.028363254500759973\n",
      "Average test loss: 0.007152944412910276\n",
      "Epoch 145/300\n",
      "Average training loss: 0.028333292735947504\n",
      "Average test loss: 0.007075620943473445\n",
      "Epoch 146/300\n",
      "Average training loss: 0.028360609147283767\n",
      "Average test loss: 0.006895071650544802\n",
      "Epoch 147/300\n",
      "Average training loss: 0.028304385834270054\n",
      "Average test loss: 0.006961504037181536\n",
      "Epoch 148/300\n",
      "Average training loss: 0.028355544320411152\n",
      "Average test loss: 0.007056122282312976\n",
      "Epoch 149/300\n",
      "Average training loss: 0.028277902816732724\n",
      "Average test loss: 0.006920116460571686\n",
      "Epoch 150/300\n",
      "Average training loss: 0.028187760445806714\n",
      "Average test loss: 0.006913742554270559\n",
      "Epoch 151/300\n",
      "Average training loss: 0.028251829624176026\n",
      "Average test loss: 0.007219619476132923\n",
      "Epoch 152/300\n",
      "Average training loss: 0.028227228237522974\n",
      "Average test loss: 0.00788144461893373\n",
      "Epoch 153/300\n",
      "Average training loss: 0.028143963663114443\n",
      "Average test loss: 0.007121928911656142\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02814512911438942\n",
      "Average test loss: 0.007247121753378047\n",
      "Epoch 155/300\n",
      "Average training loss: 0.028130843720502324\n",
      "Average test loss: 0.006994305133405659\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02806638722618421\n",
      "Average test loss: 0.007080905862152576\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02808830714225769\n",
      "Average test loss: 0.007324884861707687\n",
      "Epoch 158/300\n",
      "Average training loss: 0.028059715737899144\n",
      "Average test loss: 0.007264908365905285\n",
      "Epoch 159/300\n",
      "Average training loss: 0.028089428189727997\n",
      "Average test loss: 0.0070422866899106235\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02803822073009279\n",
      "Average test loss: 0.007244427958710326\n",
      "Epoch 161/300\n",
      "Average training loss: 0.027951625949806637\n",
      "Average test loss: 0.007545783344242308\n",
      "Epoch 162/300\n",
      "Average training loss: 0.028357983915342225\n",
      "Average test loss: 0.007089359279308054\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027831032450000445\n",
      "Average test loss: 0.007021338378389677\n",
      "Epoch 164/300\n",
      "Average training loss: 0.027932634540730052\n",
      "Average test loss: 0.007530438032415178\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02787173753314548\n",
      "Average test loss: 0.006971930232726865\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02786334336962965\n",
      "Average test loss: 0.007152702379143901\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02794299194879002\n",
      "Average test loss: 0.007404322279824151\n",
      "Epoch 168/300\n",
      "Average training loss: 0.027851674776938225\n",
      "Average test loss: 0.007147186390227741\n",
      "Epoch 169/300\n",
      "Average training loss: 0.027834781212939155\n",
      "Average test loss: 0.007289121536744965\n",
      "Epoch 170/300\n",
      "Average training loss: 0.027779458491338622\n",
      "Average test loss: 0.007142588109605842\n",
      "Epoch 171/300\n",
      "Average training loss: 0.027813220342000327\n",
      "Average test loss: 0.007401590251674255\n",
      "Epoch 172/300\n",
      "Average training loss: 0.027761544485886892\n",
      "Average test loss: 0.007006215260260635\n",
      "Epoch 173/300\n",
      "Average training loss: 0.027782049929102264\n",
      "Average test loss: 0.007301848789056142\n",
      "Epoch 174/300\n",
      "Average training loss: 0.027823577211962807\n",
      "Average test loss: 0.00747152254709767\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0276878994471497\n",
      "Average test loss: 0.007127642252378994\n",
      "Epoch 176/300\n",
      "Average training loss: 0.027754684487978618\n",
      "Average test loss: 0.007333765789452527\n",
      "Epoch 177/300\n",
      "Average training loss: 0.027656078678038386\n",
      "Average test loss: 0.0071429350483748644\n",
      "Epoch 178/300\n",
      "Average training loss: 0.027655370028482545\n",
      "Average test loss: 0.008512084656291538\n",
      "Epoch 179/300\n",
      "Average training loss: 0.027628184111581907\n",
      "Average test loss: 0.006991616137325764\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02766977064973778\n",
      "Average test loss: 0.007086800895631313\n",
      "Epoch 181/300\n",
      "Average training loss: 0.027612765004237493\n",
      "Average test loss: 0.007199404383285178\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02766952983703878\n",
      "Average test loss: 0.007144181244903141\n",
      "Epoch 183/300\n",
      "Average training loss: 0.027631306755873893\n",
      "Average test loss: 0.0069118903494543495\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02755229633549849\n",
      "Average test loss: 0.008481305018895203\n",
      "Epoch 185/300\n",
      "Average training loss: 0.027521205087502797\n",
      "Average test loss: 0.007082431245595217\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02757811668846342\n",
      "Average test loss: 0.007512527267552084\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02750727516412735\n",
      "Average test loss: 0.008878670132822461\n",
      "Epoch 188/300\n",
      "Average training loss: 0.027502216357323857\n",
      "Average test loss: 0.007114626427491506\n",
      "Epoch 189/300\n",
      "Average training loss: 0.027497364724675816\n",
      "Average test loss: 0.007563905653026369\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02751214039325714\n",
      "Average test loss: 0.00701550115727716\n",
      "Epoch 191/300\n",
      "Average training loss: 0.027430161833763123\n",
      "Average test loss: 0.007372222248050901\n",
      "Epoch 192/300\n",
      "Average training loss: 0.027521518675817383\n",
      "Average test loss: 0.00760309716189901\n",
      "Epoch 193/300\n",
      "Average training loss: 0.027410931325621075\n",
      "Average test loss: 0.007111114819016722\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02738897428744369\n",
      "Average test loss: 0.008571677081286908\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02738719370133347\n",
      "Average test loss: 0.007273465267899963\n",
      "Epoch 196/300\n",
      "Average training loss: 0.027330972853634092\n",
      "Average test loss: 0.007377093920691146\n",
      "Epoch 197/300\n",
      "Average training loss: 0.027325652231772742\n",
      "Average test loss: 0.007024885735371047\n",
      "Epoch 198/300\n",
      "Average training loss: 0.027307145764430364\n",
      "Average test loss: 0.007122057137389978\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0273708805375629\n",
      "Average test loss: 0.00774838716785113\n",
      "Epoch 200/300\n",
      "Average training loss: 0.027321424906452497\n",
      "Average test loss: 0.0070753778939445814\n",
      "Epoch 201/300\n",
      "Average training loss: 0.027343136337068346\n",
      "Average test loss: 0.007289425036145581\n",
      "Epoch 202/300\n",
      "Average training loss: 0.027326158134473696\n",
      "Average test loss: 0.007566440556198359\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02726221539742417\n",
      "Average test loss: 0.007248709030863312\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02725061473912663\n",
      "Average test loss: 0.007239193530132373\n",
      "Epoch 205/300\n",
      "Average training loss: 0.027217587562070954\n",
      "Average test loss: 0.007139622896909714\n",
      "Epoch 206/300\n",
      "Average training loss: 0.027247390730513465\n",
      "Average test loss: 0.007149884576598803\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02718333545823892\n",
      "Average test loss: 0.007123833486603366\n",
      "Epoch 208/300\n",
      "Average training loss: 0.027142427795463137\n",
      "Average test loss: 0.007075649311145147\n",
      "Epoch 209/300\n",
      "Average training loss: 0.027187994892398516\n",
      "Average test loss: 0.007146473119242324\n",
      "Epoch 210/300\n",
      "Average training loss: 0.027166865282588534\n",
      "Average test loss: 0.0072236628491017555\n",
      "Epoch 211/300\n",
      "Average training loss: 0.027121721679965655\n",
      "Average test loss: 0.007324228077299065\n",
      "Epoch 212/300\n",
      "Average training loss: 0.027152634794513384\n",
      "Average test loss: 0.007244283746927977\n",
      "Epoch 213/300\n",
      "Average training loss: 0.027146787698070208\n",
      "Average test loss: 0.007594275477445788\n",
      "Epoch 214/300\n",
      "Average training loss: 0.027166757704483138\n",
      "Average test loss: 0.007289896183957656\n",
      "Epoch 215/300\n",
      "Average training loss: 0.027030936820639503\n",
      "Average test loss: 0.007478089591695203\n",
      "Epoch 216/300\n",
      "Average training loss: 0.027051665456758606\n",
      "Average test loss: 0.007435994496775998\n",
      "Epoch 217/300\n",
      "Average training loss: 0.027038763008183902\n",
      "Average test loss: 0.007155580679161681\n",
      "Epoch 218/300\n",
      "Average training loss: 0.027154754315813382\n",
      "Average test loss: 0.007355456133683522\n",
      "Epoch 219/300\n",
      "Average training loss: 0.027057042110297413\n",
      "Average test loss: 0.0072488468715714086\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026967702102329995\n",
      "Average test loss: 0.0072409636928803395\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0270107410815027\n",
      "Average test loss: 0.007167738235659069\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026976523356305227\n",
      "Average test loss: 0.007355081915027565\n",
      "Epoch 223/300\n",
      "Average training loss: 0.027013287401861614\n",
      "Average test loss: 0.007384746876027849\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02700496553050147\n",
      "Average test loss: 0.0071880899870561225\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026973593013154135\n",
      "Average test loss: 0.00723552201812466\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02695069339705838\n",
      "Average test loss: 0.00783606314824687\n",
      "Epoch 227/300\n",
      "Average training loss: 0.027003439873456957\n",
      "Average test loss: 0.007272956277761195\n",
      "Epoch 228/300\n",
      "Average training loss: 0.026923581595222155\n",
      "Average test loss: 0.007283602457493544\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026901377651426528\n",
      "Average test loss: 0.007121529379652606\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026986257317993374\n",
      "Average test loss: 0.007171567302404178\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02693038489917914\n",
      "Average test loss: 0.0072897086553275585\n",
      "Epoch 232/300\n",
      "Average training loss: 0.026830074167913862\n",
      "Average test loss: 0.007155133215917481\n",
      "Epoch 233/300\n",
      "Average training loss: 0.026947924744751717\n",
      "Average test loss: 0.0075420698101321855\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02685087441570229\n",
      "Average test loss: 0.007610900717890925\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02682394164469507\n",
      "Average test loss: 0.007304340398973889\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02681104295452436\n",
      "Average test loss: 0.007461986892753177\n",
      "Epoch 237/300\n",
      "Average training loss: 0.026827178329229356\n",
      "Average test loss: 0.0071596350015865435\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026905888042516177\n",
      "Average test loss: 0.007248195935454634\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026771769334872565\n",
      "Average test loss: 0.007201471667736769\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026816068596310087\n",
      "Average test loss: 0.007346455066982243\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026709275452627076\n",
      "Average test loss: 0.007344682360688845\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02675581239991718\n",
      "Average test loss: 0.007479920288340913\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02665955948167377\n",
      "Average test loss: 0.008791930254962709\n",
      "Epoch 244/300\n",
      "Average training loss: 0.026788464532958137\n",
      "Average test loss: 0.00754518604112996\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0267331645919217\n",
      "Average test loss: 0.0072131528705358505\n",
      "Epoch 246/300\n",
      "Average training loss: 0.026717586510711246\n",
      "Average test loss: 0.007550482962694433\n",
      "Epoch 247/300\n",
      "Average training loss: 0.026751694626278348\n",
      "Average test loss: 0.007289684606095155\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02662358601225747\n",
      "Average test loss: 0.0072094500073128275\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026764758167995346\n",
      "Average test loss: 0.0072808845233586095\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026703791533907256\n",
      "Average test loss: 0.00760431798795859\n",
      "Epoch 251/300\n",
      "Average training loss: 0.026855730598999393\n",
      "Average test loss: 0.007311238801313771\n",
      "Epoch 252/300\n",
      "Average training loss: 0.026585038531157706\n",
      "Average test loss: 0.007493802322281732\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02661149658759435\n",
      "Average test loss: 0.0072699258310927285\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02658115765452385\n",
      "Average test loss: 0.007356138594448567\n",
      "Epoch 255/300\n",
      "Average training loss: 0.026670461406310398\n",
      "Average test loss: 0.0071746899320019614\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026590682370795145\n",
      "Average test loss: 0.007081284846282667\n",
      "Epoch 257/300\n",
      "Average training loss: 0.026632273309760625\n",
      "Average test loss: 0.007462165664053625\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026649479063848656\n",
      "Average test loss: 0.007341774768299527\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026528844328390228\n",
      "Average test loss: 0.00744980960422092\n",
      "Epoch 260/300\n",
      "Average training loss: 0.026570370531744426\n",
      "Average test loss: 0.007234666357023849\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026580014435781372\n",
      "Average test loss: 0.007672571367273728\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02653893921772639\n",
      "Average test loss: 0.007220122159355216\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02649398877388901\n",
      "Average test loss: 0.007523798332446151\n",
      "Epoch 264/300\n",
      "Average training loss: 0.026636412569218212\n",
      "Average test loss: 0.009828361297647158\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026549607786867355\n",
      "Average test loss: 0.007390210546967056\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026481892897023095\n",
      "Average test loss: 0.007361876695106427\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026532640682326424\n",
      "Average test loss: 0.007241344459768799\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02653858600391282\n",
      "Average test loss: 0.050618944731023574\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02649300912850433\n",
      "Average test loss: 0.007537288403345479\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02643933907813496\n",
      "Average test loss: 0.007915719012419383\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02651967155436675\n",
      "Average test loss: 0.02150843474931187\n",
      "Epoch 272/300\n",
      "Average training loss: 0.026576704758736823\n",
      "Average test loss: 0.007903718204961883\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02642646387219429\n",
      "Average test loss: 0.008111307242678271\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02641368676887618\n",
      "Average test loss: 0.007132162542806731\n",
      "Epoch 275/300\n",
      "Average training loss: 0.026438921911021075\n",
      "Average test loss: 0.007497241351339552\n",
      "Epoch 276/300\n",
      "Average training loss: 0.026435512125492097\n",
      "Average test loss: 0.007355042977051603\n",
      "Epoch 277/300\n",
      "Average training loss: 0.026420300818151898\n",
      "Average test loss: 0.00759183446649048\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02648426649471124\n",
      "Average test loss: 0.007363466593540377\n",
      "Epoch 279/300\n",
      "Average training loss: 0.026365971684455872\n",
      "Average test loss: 0.008046232462757163\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026406937835945023\n",
      "Average test loss: 0.007243226785212755\n",
      "Epoch 281/300\n",
      "Average training loss: 0.026385015378395715\n",
      "Average test loss: 0.007208521297822396\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02635059075554212\n",
      "Average test loss: 0.007294056083593104\n",
      "Epoch 283/300\n",
      "Average training loss: 0.026329156382216348\n",
      "Average test loss: 0.007820658424248297\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02633942525419924\n",
      "Average test loss: 0.00741694317261378\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026275064600838557\n",
      "Average test loss: 0.0071962731430927915\n",
      "Epoch 286/300\n",
      "Average training loss: 0.026370937122239008\n",
      "Average test loss: 0.007217718365291754\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02630107061068217\n",
      "Average test loss: 0.007513484593894747\n",
      "Epoch 288/300\n",
      "Average training loss: 0.026260176516241497\n",
      "Average test loss: 0.007587484429693884\n",
      "Epoch 289/300\n",
      "Average training loss: 0.026340023092097705\n",
      "Average test loss: 0.007545016796638568\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0263100252929661\n",
      "Average test loss: 0.0076404578172498275\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026274506042400995\n",
      "Average test loss: 0.00737205760470695\n",
      "Epoch 292/300\n",
      "Average training loss: 0.026285991902152698\n",
      "Average test loss: 0.0073876909464597705\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02627287760708067\n",
      "Average test loss: 0.007395509024047189\n",
      "Epoch 294/300\n",
      "Average training loss: 0.026281477481126785\n",
      "Average test loss: 0.007137651079230838\n",
      "Epoch 295/300\n",
      "Average training loss: 0.026308912333514956\n",
      "Average test loss: 0.0073894269710613626\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02621448500951131\n",
      "Average test loss: 0.007546727668080065\n",
      "Epoch 297/300\n",
      "Average training loss: 0.026191456761625077\n",
      "Average test loss: 0.0073348592850897044\n",
      "Epoch 298/300\n",
      "Average training loss: 0.026240416828129026\n",
      "Average test loss: 0.0074050663606160215\n",
      "Epoch 299/300\n",
      "Average training loss: 0.026149661037656997\n",
      "Average test loss: 0.007520359373341004\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02625453262362215\n",
      "Average test loss: 0.0075862025250163344\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14029990879032347\n",
      "Average test loss: 0.0077191736077268916\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04968432695335812\n",
      "Average test loss: 0.010482107687327597\n",
      "Epoch 3/300\n",
      "Average training loss: 0.043703718877500956\n",
      "Average test loss: 0.008022740623189343\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0401193708313836\n",
      "Average test loss: 0.006451423132999076\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0375210209886233\n",
      "Average test loss: 0.00575776117377811\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03554089254140854\n",
      "Average test loss: 0.006128806141929494\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03405114719602797\n",
      "Average test loss: 0.005605334199137158\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03291126823094156\n",
      "Average test loss: 0.005557822808209393\n",
      "Epoch 9/300\n",
      "Average training loss: 0.032147221646375124\n",
      "Average test loss: 0.005491870697587728\n",
      "Epoch 10/300\n",
      "Average training loss: 0.031369719425837196\n",
      "Average test loss: 0.005276685453537437\n",
      "Epoch 11/300\n",
      "Average training loss: 0.030643129540814294\n",
      "Average test loss: 0.00513121316023171\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0299480669134193\n",
      "Average test loss: 0.00496590287776457\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02945744797752963\n",
      "Average test loss: 0.0050036121637870865\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02891764006184207\n",
      "Average test loss: 0.004813961850686206\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02842560407353772\n",
      "Average test loss: 0.004710811785318785\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02814885037806299\n",
      "Average test loss: 0.005022296702696217\n",
      "Epoch 17/300\n",
      "Average training loss: 0.027720667408572302\n",
      "Average test loss: 0.004495512011978361\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02743591694202688\n",
      "Average test loss: 0.0044529648719148504\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02698866328265932\n",
      "Average test loss: 0.004679422314796183\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02669813911947939\n",
      "Average test loss: 0.004426672393870023\n",
      "Epoch 21/300\n",
      "Average training loss: 0.026525490588612026\n",
      "Average test loss: 0.004406967819150951\n",
      "Epoch 22/300\n",
      "Average training loss: 0.026190362743205495\n",
      "Average test loss: 0.0044532298942406975\n",
      "Epoch 23/300\n",
      "Average training loss: 0.026016415041353966\n",
      "Average test loss: 0.00420976768164999\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02573228290180365\n",
      "Average test loss: 0.00424599518854585\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02552293617857827\n",
      "Average test loss: 0.004188130153136121\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025363081864184803\n",
      "Average test loss: 0.004181093413175808\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02512351772520277\n",
      "Average test loss: 0.004383025762107637\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025023926536242167\n",
      "Average test loss: 0.004636475762352347\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02492016158832444\n",
      "Average test loss: 0.004219953328164087\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024676892812881206\n",
      "Average test loss: 0.00408550365227792\n",
      "Epoch 31/300\n",
      "Average training loss: 0.024505182991425196\n",
      "Average test loss: 0.004080730523086256\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02442244259516398\n",
      "Average test loss: 0.005167438824557596\n",
      "Epoch 33/300\n",
      "Average training loss: 0.024380931654738057\n",
      "Average test loss: 0.004033050062134862\n",
      "Epoch 34/300\n",
      "Average training loss: 0.024362211811873646\n",
      "Average test loss: 0.004135972688595454\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02406977424522241\n",
      "Average test loss: 0.0042748555377539665\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02393443073829015\n",
      "Average test loss: 0.004153572473675013\n",
      "Epoch 37/300\n",
      "Average training loss: 0.023861804524229632\n",
      "Average test loss: 0.004016031150602632\n",
      "Epoch 38/300\n",
      "Average training loss: 0.023822248362832598\n",
      "Average test loss: 0.004122976212244895\n",
      "Epoch 39/300\n",
      "Average training loss: 0.023653326206737095\n",
      "Average test loss: 0.004035965228039357\n",
      "Epoch 40/300\n",
      "Average training loss: 0.023563111229075326\n",
      "Average test loss: 0.00403065116227501\n",
      "Epoch 41/300\n",
      "Average training loss: 0.023481626919574207\n",
      "Average test loss: 0.003980004729082187\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0234197376155191\n",
      "Average test loss: 0.004028133028497299\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02334047193494108\n",
      "Average test loss: 0.0040395895950496195\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02323845512006018\n",
      "Average test loss: 0.003989344294079476\n",
      "Epoch 45/300\n",
      "Average training loss: 0.023198609477116\n",
      "Average test loss: 0.003949575544852349\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02308506776061323\n",
      "Average test loss: 0.003965452901605103\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02308870015376144\n",
      "Average test loss: 0.003911237308548556\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02297257852057616\n",
      "Average test loss: 0.004156751722925239\n",
      "Epoch 49/300\n",
      "Average training loss: 0.022899216938349936\n",
      "Average test loss: 0.003977548289837108\n",
      "Epoch 50/300\n",
      "Average training loss: 0.022859909743070603\n",
      "Average test loss: 0.004010297458618879\n",
      "Epoch 51/300\n",
      "Average training loss: 0.022773278448316787\n",
      "Average test loss: 0.003903657000925806\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02271368386844794\n",
      "Average test loss: 0.003963296961039304\n",
      "Epoch 53/300\n",
      "Average training loss: 0.022631766433517136\n",
      "Average test loss: 0.003932324390444491\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02260819747712877\n",
      "Average test loss: 0.003971114944873584\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02251953384362989\n",
      "Average test loss: 0.0040769952572882176\n",
      "Epoch 56/300\n",
      "Average training loss: 0.022541267454624175\n",
      "Average test loss: 0.004075138745208581\n",
      "Epoch 57/300\n",
      "Average training loss: 0.022384250090354017\n",
      "Average test loss: 0.003914565499044127\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02234595707224475\n",
      "Average test loss: 0.004341104340636068\n",
      "Epoch 59/300\n",
      "Average training loss: 0.022306942034098838\n",
      "Average test loss: 0.004016609129599399\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02224042497575283\n",
      "Average test loss: 0.003953401412814856\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022215900773803393\n",
      "Average test loss: 0.003932335487877329\n",
      "Epoch 62/300\n",
      "Average training loss: 0.022208205214805075\n",
      "Average test loss: 0.004116799441476663\n",
      "Epoch 63/300\n",
      "Average training loss: 0.022137175591455567\n",
      "Average test loss: 0.004089244050491187\n",
      "Epoch 64/300\n",
      "Average training loss: 0.022081617060634825\n",
      "Average test loss: 0.004015947939621078\n",
      "Epoch 65/300\n",
      "Average training loss: 0.021986726919809976\n",
      "Average test loss: 0.0040712165116435954\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02198582806520992\n",
      "Average test loss: 0.004009384493240052\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02196073269016213\n",
      "Average test loss: 0.004040713434418043\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02187493121292856\n",
      "Average test loss: 0.004137320999883943\n",
      "Epoch 69/300\n",
      "Average training loss: 0.021895915294686954\n",
      "Average test loss: 0.004042621827787823\n",
      "Epoch 70/300\n",
      "Average training loss: 0.021814527520702946\n",
      "Average test loss: 0.004046829972830084\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02174744188123279\n",
      "Average test loss: 0.003935598472340239\n",
      "Epoch 72/300\n",
      "Average training loss: 0.021791885414057306\n",
      "Average test loss: 0.003965069709552659\n",
      "Epoch 73/300\n",
      "Average training loss: 0.021684810330470403\n",
      "Average test loss: 0.004036718770033783\n",
      "Epoch 74/300\n",
      "Average training loss: 0.021683001880844432\n",
      "Average test loss: 0.007044494864841302\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0215484316514598\n",
      "Average test loss: 0.004014972207860814\n",
      "Epoch 76/300\n",
      "Average training loss: 0.021611749784813988\n",
      "Average test loss: 0.004003649476294716\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02154113775657283\n",
      "Average test loss: 0.003921131601557135\n",
      "Epoch 78/300\n",
      "Average training loss: 0.021539898607465954\n",
      "Average test loss: 0.0039459782112389805\n",
      "Epoch 79/300\n",
      "Average training loss: 0.021469909022251765\n",
      "Average test loss: 0.004001261601845423\n",
      "Epoch 80/300\n",
      "Average training loss: 0.021430316431654824\n",
      "Average test loss: 0.004025398993657695\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02137631066309081\n",
      "Average test loss: 0.004073173619392845\n",
      "Epoch 82/300\n",
      "Average training loss: 0.021386077283157244\n",
      "Average test loss: 0.004923382014864021\n",
      "Epoch 83/300\n",
      "Average training loss: 0.021303095092376073\n",
      "Average test loss: 0.0040548491734597415\n",
      "Epoch 84/300\n",
      "Average training loss: 0.021342843774292204\n",
      "Average test loss: 0.004006187193716566\n",
      "Epoch 85/300\n",
      "Average training loss: 0.021263100827733677\n",
      "Average test loss: 0.004048242267635133\n",
      "Epoch 86/300\n",
      "Average training loss: 0.021227299200163946\n",
      "Average test loss: 0.0039869085223310525\n",
      "Epoch 87/300\n",
      "Average training loss: 0.021244639562235938\n",
      "Average test loss: 0.004003589450278216\n",
      "Epoch 88/300\n",
      "Average training loss: 0.021192369396487873\n",
      "Average test loss: 0.00415001893043518\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02117721155865325\n",
      "Average test loss: 0.004033831014401383\n",
      "Epoch 90/300\n",
      "Average training loss: 0.021346826760305297\n",
      "Average test loss: 0.004188259626014365\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02109000210960706\n",
      "Average test loss: 0.003936535538070732\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02104056438803673\n",
      "Average test loss: 0.004052469922022687\n",
      "Epoch 93/300\n",
      "Average training loss: 0.021090716550747554\n",
      "Average test loss: 0.004211560723268323\n",
      "Epoch 94/300\n",
      "Average training loss: 0.020982827986280123\n",
      "Average test loss: 0.004551254726325472\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020976253305872283\n",
      "Average test loss: 0.003966845930450493\n",
      "Epoch 96/300\n",
      "Average training loss: 0.020927009633845754\n",
      "Average test loss: 0.004012114269038042\n",
      "Epoch 97/300\n",
      "Average training loss: 0.020938981427086722\n",
      "Average test loss: 0.004006221062814196\n",
      "Epoch 98/300\n",
      "Average training loss: 0.020915570959448813\n",
      "Average test loss: 0.004431944285415941\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02088474882642428\n",
      "Average test loss: 0.004001764149301582\n",
      "Epoch 100/300\n",
      "Average training loss: 0.020839401132530636\n",
      "Average test loss: 0.003983000952957405\n",
      "Epoch 101/300\n",
      "Average training loss: 0.020823868481649292\n",
      "Average test loss: 0.00411936697570814\n",
      "Epoch 102/300\n",
      "Average training loss: 0.020831234567695193\n",
      "Average test loss: 0.004018731527030468\n",
      "Epoch 103/300\n",
      "Average training loss: 0.020807141813966964\n",
      "Average test loss: 0.004049533857653539\n",
      "Epoch 104/300\n",
      "Average training loss: 0.020740531134936544\n",
      "Average test loss: 0.004065173605249987\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02073941262728638\n",
      "Average test loss: 0.004320604054878155\n",
      "Epoch 106/300\n",
      "Average training loss: 0.020693917190035185\n",
      "Average test loss: 0.0039965410696135625\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02073001517024305\n",
      "Average test loss: 0.0040775199549065696\n",
      "Epoch 108/300\n",
      "Average training loss: 0.020657388215263684\n",
      "Average test loss: 0.0042733513861894605\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0206728464629915\n",
      "Average test loss: 0.004080350490493907\n",
      "Epoch 110/300\n",
      "Average training loss: 0.020610366894139184\n",
      "Average test loss: 0.00404387574063407\n",
      "Epoch 111/300\n",
      "Average training loss: 0.020571928166680867\n",
      "Average test loss: 0.0041311979106523926\n",
      "Epoch 112/300\n",
      "Average training loss: 0.020586253641380205\n",
      "Average test loss: 0.004308941421409448\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02055961776773135\n",
      "Average test loss: 0.004091292942977614\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02054869536558787\n",
      "Average test loss: 0.004817763695700301\n",
      "Epoch 115/300\n",
      "Average training loss: 0.020553197314341864\n",
      "Average test loss: 0.004100599687546492\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02049824689163102\n",
      "Average test loss: 0.004025778856335415\n",
      "Epoch 117/300\n",
      "Average training loss: 0.020456976145505904\n",
      "Average test loss: 0.0040426836998926265\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02041631790995598\n",
      "Average test loss: 0.004125794454581208\n",
      "Epoch 119/300\n",
      "Average training loss: 0.020438840779993268\n",
      "Average test loss: 0.004080333793121908\n",
      "Epoch 120/300\n",
      "Average training loss: 0.020431842504276168\n",
      "Average test loss: 0.004092021887087159\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02038550930387444\n",
      "Average test loss: 0.004197794824010796\n",
      "Epoch 122/300\n",
      "Average training loss: 0.020342956996626325\n",
      "Average test loss: 0.004087212724404202\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02038839637570911\n",
      "Average test loss: 0.004532092733515633\n",
      "Epoch 124/300\n",
      "Average training loss: 0.020312690163652104\n",
      "Average test loss: 0.004093421561850442\n",
      "Epoch 125/300\n",
      "Average training loss: 0.020598460348116028\n",
      "Average test loss: 0.004123216682838069\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02028945008913676\n",
      "Average test loss: 0.0040692453597568805\n",
      "Epoch 127/300\n",
      "Average training loss: 0.020236381803949675\n",
      "Average test loss: 0.004342244169571334\n",
      "Epoch 128/300\n",
      "Average training loss: 0.020262786625160113\n",
      "Average test loss: 0.004262167283644279\n",
      "Epoch 129/300\n",
      "Average training loss: 0.020266231482227642\n",
      "Average test loss: 0.004166893341061142\n",
      "Epoch 130/300\n",
      "Average training loss: 0.020185738219155206\n",
      "Average test loss: 0.004131359248939487\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02025126412842009\n",
      "Average test loss: 0.004152678633315696\n",
      "Epoch 132/300\n",
      "Average training loss: 0.020196778943141303\n",
      "Average test loss: 0.004431577380125722\n",
      "Epoch 133/300\n",
      "Average training loss: 0.020177381959226397\n",
      "Average test loss: 0.004144230775535106\n",
      "Epoch 134/300\n",
      "Average training loss: 0.020136576663288806\n",
      "Average test loss: 0.0041809150367561315\n",
      "Epoch 135/300\n",
      "Average training loss: 0.020137095297376315\n",
      "Average test loss: 0.00410369400266144\n",
      "Epoch 136/300\n",
      "Average training loss: 0.020158848941326142\n",
      "Average test loss: 0.004167206092427174\n",
      "Epoch 137/300\n",
      "Average training loss: 0.020080698008338612\n",
      "Average test loss: 0.0050689508894251455\n",
      "Epoch 138/300\n",
      "Average training loss: 0.020070401234759224\n",
      "Average test loss: 0.004179034508350822\n",
      "Epoch 139/300\n",
      "Average training loss: 0.020070248554150263\n",
      "Average test loss: 0.004275961663160059\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02001940792467859\n",
      "Average test loss: 0.0042086121319896645\n",
      "Epoch 141/300\n",
      "Average training loss: 0.020071942309538522\n",
      "Average test loss: 0.004153133623922865\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02008784238828553\n",
      "Average test loss: 0.0046303076350854505\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01998376611371835\n",
      "Average test loss: 0.00412088574303521\n",
      "Epoch 144/300\n",
      "Average training loss: 0.020002407607105043\n",
      "Average test loss: 0.004078529255671634\n",
      "Epoch 145/300\n",
      "Average training loss: 0.019942538847525913\n",
      "Average test loss: 0.0042686874742309255\n",
      "Epoch 146/300\n",
      "Average training loss: 0.020009109541773797\n",
      "Average test loss: 0.00420387491914961\n",
      "Epoch 147/300\n",
      "Average training loss: 0.019942840756641492\n",
      "Average test loss: 0.00435901695345011\n",
      "Epoch 148/300\n",
      "Average training loss: 0.019927962334619628\n",
      "Average test loss: 0.0042559952893190915\n",
      "Epoch 149/300\n",
      "Average training loss: 0.019904578667547967\n",
      "Average test loss: 0.0042559480520172254\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019884921806554\n",
      "Average test loss: 0.004335931578858031\n",
      "Epoch 151/300\n",
      "Average training loss: 0.019912245619628163\n",
      "Average test loss: 0.004166699742484424\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019852546489901014\n",
      "Average test loss: 0.004221075817735659\n",
      "Epoch 153/300\n",
      "Average training loss: 0.020074764137466748\n",
      "Average test loss: 0.004149130437937048\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01985214982264572\n",
      "Average test loss: 0.004312340732249949\n",
      "Epoch 155/300\n",
      "Average training loss: 0.019803829708033138\n",
      "Average test loss: 0.004230674092554384\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019805148081647024\n",
      "Average test loss: 0.004205014621011085\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01982248161898719\n",
      "Average test loss: 0.004079445398102204\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019777846400936445\n",
      "Average test loss: 0.0040867075535158314\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019811796630422273\n",
      "Average test loss: 0.004094364205271834\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019754825259248415\n",
      "Average test loss: 0.004298346852676736\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019779232776827284\n",
      "Average test loss: 0.004209283635848098\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019688954967591498\n",
      "Average test loss: 0.0041506195494698155\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019741660608185663\n",
      "Average test loss: 0.0041974230777058336\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01973413555158509\n",
      "Average test loss: 0.0041899585318234235\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019645056401689846\n",
      "Average test loss: 0.004370320327166054\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019704524076647228\n",
      "Average test loss: 0.004246934394041697\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01966820145977868\n",
      "Average test loss: 0.004173343262531691\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019683247120843992\n",
      "Average test loss: 0.0042793485758205255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01965593210193846\n",
      "Average test loss: 0.004213891110486454\n",
      "Epoch 170/300\n",
      "Average training loss: 0.019626379708449045\n",
      "Average test loss: 0.0041795613322820925\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019642834290862082\n",
      "Average test loss: 0.004157165748377641\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0196181095490853\n",
      "Average test loss: 0.0044673738914231455\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019655644713176622\n",
      "Average test loss: 0.004262606294618713\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019578721635871463\n",
      "Average test loss: 0.004261196274724271\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019593861907720566\n",
      "Average test loss: 0.004365060171422859\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019552711725234987\n",
      "Average test loss: 0.00428494517153336\n",
      "Epoch 177/300\n",
      "Average training loss: 0.019567508031924565\n",
      "Average test loss: 0.004266612571560674\n",
      "Epoch 178/300\n",
      "Average training loss: 0.019530720187558068\n",
      "Average test loss: 0.004209893678832385\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01951909325354629\n",
      "Average test loss: 0.004213997571418683\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01952725349366665\n",
      "Average test loss: 0.004347099653755625\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01955179879731602\n",
      "Average test loss: 0.004457703109830618\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019492197843061554\n",
      "Average test loss: 0.004217011387563414\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019503562521603374\n",
      "Average test loss: 0.004411754886309306\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01949112106528547\n",
      "Average test loss: 0.004175184296444059\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01948292035692268\n",
      "Average test loss: 0.004768921511454714\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019489056502779326\n",
      "Average test loss: 0.004398042405852013\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0195739800632\n",
      "Average test loss: 0.004206821490493086\n",
      "Epoch 188/300\n",
      "Average training loss: 0.019468470316380264\n",
      "Average test loss: 0.004397195859915681\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019400047798951468\n",
      "Average test loss: 0.004160366543672151\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019382074902455013\n",
      "Average test loss: 0.004204316909114519\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019378952214287388\n",
      "Average test loss: 0.004355303414993816\n",
      "Epoch 192/300\n",
      "Average training loss: 0.019407635109292137\n",
      "Average test loss: 0.004245611978901757\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019356629500786465\n",
      "Average test loss: 0.004370980756564273\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01937568357255724\n",
      "Average test loss: 0.004320360409302844\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01937670148909092\n",
      "Average test loss: 0.004236433674684829\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01935871714187993\n",
      "Average test loss: 0.004273275324867831\n",
      "Epoch 197/300\n",
      "Average training loss: 0.019383073905275928\n",
      "Average test loss: 0.004376929723140266\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01935064289636082\n",
      "Average test loss: 0.0043817432079878114\n",
      "Epoch 199/300\n",
      "Average training loss: 0.019376251050167614\n",
      "Average test loss: 0.004219425527378916\n",
      "Epoch 200/300\n",
      "Average training loss: 0.019321395236584875\n",
      "Average test loss: 0.00432560151318709\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01934839800828033\n",
      "Average test loss: 0.0042627405199325745\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01933874228099982\n",
      "Average test loss: 0.004253228041860792\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019282435904774402\n",
      "Average test loss: 0.004376752309708132\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019287829857733516\n",
      "Average test loss: 0.004156616662939389\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01927389874557654\n",
      "Average test loss: 0.004263383540428347\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01928806849486298\n",
      "Average test loss: 0.004375694159418344\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01927174557083183\n",
      "Average test loss: 0.004165181765125857\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019248538280526795\n",
      "Average test loss: 0.004309973059015142\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019265294341577425\n",
      "Average test loss: 0.004220836067365275\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01921016725235515\n",
      "Average test loss: 0.004298592197812266\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01921579500867261\n",
      "Average test loss: 0.004239488227085935\n",
      "Epoch 212/300\n",
      "Average training loss: 0.019248310289449164\n",
      "Average test loss: 0.0043203034133960806\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01921728064450953\n",
      "Average test loss: 0.00437290315495597\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01919326246281465\n",
      "Average test loss: 0.004320667017251253\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019188440127505195\n",
      "Average test loss: 0.004520124298416906\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01921044396609068\n",
      "Average test loss: 0.0042779570395747825\n",
      "Epoch 217/300\n",
      "Average training loss: 0.019139482256438998\n",
      "Average test loss: 0.004407475701429778\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0191780325728986\n",
      "Average test loss: 0.004307393035127057\n",
      "Epoch 219/300\n",
      "Average training loss: 0.019149282349480524\n",
      "Average test loss: 0.004383749972201056\n",
      "Epoch 220/300\n",
      "Average training loss: 0.019123128679891428\n",
      "Average test loss: 0.004183687919336889\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01919321595132351\n",
      "Average test loss: 0.004179982979678446\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019153336768349013\n",
      "Average test loss: 0.0043657158528351125\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019086356189515857\n",
      "Average test loss: 0.0043254071602390875\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01911904949032598\n",
      "Average test loss: 0.004342386737258898\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019096064714921847\n",
      "Average test loss: 0.004416598789393902\n",
      "Epoch 226/300\n",
      "Average training loss: 0.019117171592182583\n",
      "Average test loss: 0.004517124738958147\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019092216931283473\n",
      "Average test loss: 0.004476485372003582\n",
      "Epoch 228/300\n",
      "Average training loss: 0.019128218988577524\n",
      "Average test loss: 0.004704760346354709\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01904681881599956\n",
      "Average test loss: 0.004271255757038792\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019061692759394646\n",
      "Average test loss: 0.004227064556338721\n",
      "Epoch 231/300\n",
      "Average training loss: 0.019046368694967692\n",
      "Average test loss: 0.004914135739621189\n",
      "Epoch 232/300\n",
      "Average training loss: 0.019067021429538727\n",
      "Average test loss: 0.004240361461622847\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019110385962658457\n",
      "Average test loss: 0.0045525336178640526\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01902618570956919\n",
      "Average test loss: 0.004254383171598116\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018991586517956523\n",
      "Average test loss: 0.004386226370516751\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01907298444873757\n",
      "Average test loss: 0.004541214187526041\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01899239987300502\n",
      "Average test loss: 0.004368473180259268\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01900769420630402\n",
      "Average test loss: 0.0044532236156778204\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018983420208096503\n",
      "Average test loss: 0.0042402858576840825\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018995101832681233\n",
      "Average test loss: 0.004262750323033995\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01901324764556355\n",
      "Average test loss: 0.004324168723904424\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019022625317176182\n",
      "Average test loss: 0.004295095610949728\n",
      "Epoch 243/300\n",
      "Average training loss: 0.018949202282561195\n",
      "Average test loss: 0.0043717715156575045\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01897396380537086\n",
      "Average test loss: 0.0051580771965285144\n",
      "Epoch 245/300\n",
      "Average training loss: 0.018951167571875784\n",
      "Average test loss: 0.004436460903328326\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018970536841286554\n",
      "Average test loss: 0.004286295392447048\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018935600883430905\n",
      "Average test loss: 0.004464676876035001\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01896996302323209\n",
      "Average test loss: 0.004402857965479294\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018930469901197487\n",
      "Average test loss: 0.004665050139443742\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01896582950817214\n",
      "Average test loss: 0.004310385823042856\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018910162950555483\n",
      "Average test loss: 0.004437632388124863\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01889961022635301\n",
      "Average test loss: 0.004389641610077686\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018885469519429737\n",
      "Average test loss: 0.0042705234355396695\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01892872193704049\n",
      "Average test loss: 0.004437051127768225\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01889082270860672\n",
      "Average test loss: 0.004646918199542496\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018891650476389462\n",
      "Average test loss: 0.004311617978538076\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018870906417568525\n",
      "Average test loss: 0.004419721856299374\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018845205455190604\n",
      "Average test loss: 0.004346420309402877\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018869895317488245\n",
      "Average test loss: 0.004397831487158934\n",
      "Epoch 260/300\n",
      "Average training loss: 0.018837549462914466\n",
      "Average test loss: 0.004282814962582456\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018844530688391793\n",
      "Average test loss: 0.004507329640703069\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018839501309725972\n",
      "Average test loss: 0.004412775391919746\n",
      "Epoch 263/300\n",
      "Average training loss: 0.018854859405093724\n",
      "Average test loss: 0.00447187612992194\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01879970339106189\n",
      "Average test loss: 0.00474508844771319\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018917493929465613\n",
      "Average test loss: 0.00429391914813055\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018883321952488687\n",
      "Average test loss: 0.004303479334960381\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018848261111312443\n",
      "Average test loss: 0.004379077742290165\n",
      "Epoch 268/300\n",
      "Average training loss: 0.018815464392304422\n",
      "Average test loss: 0.00449331038672891\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01876072700901164\n",
      "Average test loss: 0.004479765052803689\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018793145577940675\n",
      "Average test loss: 0.004443294584337208\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018799627496136558\n",
      "Average test loss: 0.004580401529868444\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01882117007838355\n",
      "Average test loss: 0.004673601672467258\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01881379122204251\n",
      "Average test loss: 0.004381294810523589\n",
      "Epoch 274/300\n",
      "Average training loss: 0.018804769953091938\n",
      "Average test loss: 0.00471310537142886\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01880667681660917\n",
      "Average test loss: 0.0044548380079989635\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018740010001593164\n",
      "Average test loss: 0.0043047819700505995\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018739360478189256\n",
      "Average test loss: 0.004311271322270234\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018748479904399977\n",
      "Average test loss: 0.004493536096894079\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01875021371245384\n",
      "Average test loss: 0.004439677412311236\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018751612883475093\n",
      "Average test loss: 0.004363395098389851\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018727926513387096\n",
      "Average test loss: 0.006371871671949823\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0187601026147604\n",
      "Average test loss: 0.004596920914534066\n",
      "Epoch 283/300\n",
      "Average training loss: 0.018744882573684056\n",
      "Average test loss: 0.004645279893858565\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01868817983733283\n",
      "Average test loss: 0.004375514618431529\n",
      "Epoch 285/300\n",
      "Average training loss: 0.018715529133048324\n",
      "Average test loss: 0.004463570990289251\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018724955686264567\n",
      "Average test loss: 0.004320493343389697\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018668825189272563\n",
      "Average test loss: 0.004719876860992776\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01870933257540067\n",
      "Average test loss: 0.004344679093816214\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018681699828969108\n",
      "Average test loss: 0.004777109738439322\n",
      "Epoch 290/300\n",
      "Average training loss: 0.018656049627396795\n",
      "Average test loss: 0.004850646147297489\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01869748176137606\n",
      "Average test loss: 0.00429065678268671\n",
      "Epoch 292/300\n",
      "Average training loss: 0.018679609515600736\n",
      "Average test loss: 0.004774552684484256\n",
      "Epoch 293/300\n",
      "Average training loss: 0.018684519896904627\n",
      "Average test loss: 0.004394754523825315\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018666034933593537\n",
      "Average test loss: 0.004399226607961787\n",
      "Epoch 295/300\n",
      "Average training loss: 0.018629572459393076\n",
      "Average test loss: 0.004444429047819641\n",
      "Epoch 296/300\n",
      "Average training loss: 0.018694107674062252\n",
      "Average test loss: 0.004512602849139107\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018638532650139598\n",
      "Average test loss: 0.004428118976246979\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018636496202813255\n",
      "Average test loss: 0.0046218348097883995\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018664045757717556\n",
      "Average test loss: 0.004378188547367851\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018636898193094466\n",
      "Average test loss: 0.004351171401225858\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12408799285689989\n",
      "Average test loss: 0.006241309413893355\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04257384835018052\n",
      "Average test loss: 0.005936251040548086\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03754965849055184\n",
      "Average test loss: 0.005394554391089413\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03425886788798703\n",
      "Average test loss: 0.004682376711525851\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03164923389918274\n",
      "Average test loss: 0.004445572456551923\n",
      "Epoch 6/300\n",
      "Average training loss: 0.029899585488769743\n",
      "Average test loss: 0.004238787041356166\n",
      "Epoch 7/300\n",
      "Average training loss: 0.028102942963441215\n",
      "Average test loss: 0.004179169544536207\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0268526375691096\n",
      "Average test loss: 0.003984059206934439\n",
      "Epoch 9/300\n",
      "Average training loss: 0.025881279211905268\n",
      "Average test loss: 0.004094889945040147\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02489317356215583\n",
      "Average test loss: 0.0037681333486818606\n",
      "Epoch 11/300\n",
      "Average training loss: 0.024320136670437125\n",
      "Average test loss: 0.004306419447478321\n",
      "Epoch 12/300\n",
      "Average training loss: 0.023623598042461606\n",
      "Average test loss: 0.0035294941109087733\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02315337449643347\n",
      "Average test loss: 0.003993579572687546\n",
      "Epoch 14/300\n",
      "Average training loss: 0.022728221679727236\n",
      "Average test loss: 0.003729869935868515\n",
      "Epoch 15/300\n",
      "Average training loss: 0.022265052719248665\n",
      "Average test loss: 0.003511055237510138\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021855246434609094\n",
      "Average test loss: 0.003396465348286761\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021515463648570907\n",
      "Average test loss: 0.0031750343477146494\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02121492630077733\n",
      "Average test loss: 0.0031143350632240373\n",
      "Epoch 19/300\n",
      "Average training loss: 0.020921998298002613\n",
      "Average test loss: 0.0031254993201129968\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02068116640879048\n",
      "Average test loss: 0.0030351039353344175\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020442789854274855\n",
      "Average test loss: 0.003286723473212785\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020249312909113037\n",
      "Average test loss: 0.00306022671258284\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020064549705220595\n",
      "Average test loss: 0.002949775524230467\n",
      "Epoch 24/300\n",
      "Average training loss: 0.019850337128672336\n",
      "Average test loss: 0.002952023130738073\n",
      "Epoch 25/300\n",
      "Average training loss: 0.019735312241646978\n",
      "Average test loss: 0.0029275916918284363\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01955041536026531\n",
      "Average test loss: 0.0028860436394396757\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019404192338387173\n",
      "Average test loss: 0.0032298007770958873\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019314216933316655\n",
      "Average test loss: 0.0027955125117053586\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019161169259084596\n",
      "Average test loss: 0.05538427778085073\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01915150227314896\n",
      "Average test loss: 0.002795900009158585\n",
      "Epoch 31/300\n",
      "Average training loss: 0.018923658814695148\n",
      "Average test loss: 0.002772608075497879\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01883449857847558\n",
      "Average test loss: 0.002891475972202089\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018711167899270853\n",
      "Average test loss: 0.0027522679724627072\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018698587213953337\n",
      "Average test loss: 0.0027632533195945954\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018553822739256754\n",
      "Average test loss: 0.0027232672567996715\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01846505535973443\n",
      "Average test loss: 0.00285675587091181\n",
      "Epoch 37/300\n",
      "Average training loss: 0.018422663024730154\n",
      "Average test loss: 0.002719791712032424\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018363443692525228\n",
      "Average test loss: 0.002685359188252025\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018286785675419703\n",
      "Average test loss: 0.002722822854295373\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018169400153060754\n",
      "Average test loss: 0.002679447267204523\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018190918422407575\n",
      "Average test loss: 0.0027085771235740848\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018066034249133535\n",
      "Average test loss: 0.002685509677562449\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018002513825893404\n",
      "Average test loss: 0.0027792106593648594\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01802707045442528\n",
      "Average test loss: 0.0027274238146427605\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01789541819774442\n",
      "Average test loss: 0.0027168200299557714\n",
      "Epoch 46/300\n",
      "Average training loss: 0.017850445775522125\n",
      "Average test loss: 0.002672269126607312\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017775043962730303\n",
      "Average test loss: 0.0026427934039384127\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0177600167757935\n",
      "Average test loss: 0.0026381181774453983\n",
      "Epoch 49/300\n",
      "Average training loss: 0.017712470355961057\n",
      "Average test loss: 0.0027200373639870022\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01768679683903853\n",
      "Average test loss: 0.0027254001855229336\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01759808997809887\n",
      "Average test loss: 0.002686154032850431\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01757537110315429\n",
      "Average test loss: 0.002738144116062257\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017502710123856863\n",
      "Average test loss: 0.0026494495363699067\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018446943261557155\n",
      "Average test loss: 0.0027423024440391196\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017504144940111373\n",
      "Average test loss: 0.0026386819695019058\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017376392068134412\n",
      "Average test loss: 0.00265494642748187\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017405510758360228\n",
      "Average test loss: 0.0027211912397502197\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01731123240954346\n",
      "Average test loss: 0.0026197560817624132\n",
      "Epoch 59/300\n",
      "Average training loss: 0.017329249367117883\n",
      "Average test loss: 0.0026646563359018828\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017258547004726198\n",
      "Average test loss: 0.002715131896858414\n",
      "Epoch 61/300\n",
      "Average test loss: 0.00275197623980542\n",
      "Epoch 62/300\n",
      "Average training loss: 0.017216520454320643\n",
      "Average test loss: 0.0026962286714050506\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0171513495279683\n",
      "Average test loss: 0.0026609555274869005\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017154643467730945\n",
      "Average test loss: 0.00267831423609621\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01712933756162723\n",
      "Average test loss: 0.002779516707484921\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017065996496213806\n",
      "Average test loss: 0.002791976986452937\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017027072199516827\n",
      "Average test loss: 0.002735919993991653\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016984137694040936\n",
      "Average test loss: 0.0026707321510960657\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016983355051941342\n",
      "Average test loss: 0.002817740972464283\n",
      "Epoch 70/300\n",
      "Average training loss: 0.016970323301023906\n",
      "Average test loss: 0.002690456670191553\n",
      "Epoch 71/300\n",
      "Average training loss: 0.016913517349296147\n",
      "Average test loss: 0.0026872280324912734\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01686151332490974\n",
      "Average test loss: 0.00278187318953375\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016861775412327713\n",
      "Average test loss: 0.002710711050364706\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016821690448456342\n",
      "Average test loss: 0.0027098219018015595\n",
      "Epoch 75/300\n",
      "Average training loss: 0.016802694669200313\n",
      "Average test loss: 0.00269646479582621\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01675149128006564\n",
      "Average test loss: 0.002850928139148487\n",
      "Epoch 77/300\n",
      "Average training loss: 0.016770655905207\n",
      "Average test loss: 0.002692178073028723\n",
      "Epoch 78/300\n",
      "Average training loss: 0.016695208848350577\n",
      "Average test loss: 0.0027049161442038085\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01669001967045996\n",
      "Average test loss: 0.00286466391881307\n",
      "Epoch 80/300\n",
      "Average training loss: 0.016683947950601578\n",
      "Average test loss: 0.0027389135882258417\n",
      "Epoch 81/300\n",
      "Average training loss: 0.016669168865515126\n",
      "Average test loss: 0.0027984420222540696\n",
      "Epoch 82/300\n",
      "Average training loss: 0.016610656294557782\n",
      "Average test loss: 0.0027577694927652676\n",
      "Epoch 83/300\n",
      "Average training loss: 0.016599825572636392\n",
      "Average test loss: 0.0026916931092532143\n",
      "Epoch 84/300\n",
      "Average training loss: 0.016574777838256623\n",
      "Average test loss: 0.00277617591774712\n",
      "Epoch 85/300\n",
      "Average training loss: 0.016535087084604633\n",
      "Average test loss: 0.0028306344993826414\n",
      "Epoch 86/300\n",
      "Average training loss: 0.016520000632438393\n",
      "Average test loss: 0.0028216651717407836\n",
      "Epoch 87/300\n",
      "Average training loss: 0.016521218556496832\n",
      "Average test loss: 0.0027972311776959234\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01650058462884691\n",
      "Average test loss: 0.0027197818433245024\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016468538157641887\n",
      "Average test loss: 0.0027349672416845956\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016442252826359537\n",
      "Average test loss: 0.00263188706483278\n",
      "Epoch 91/300\n",
      "Average training loss: 0.016455252051353456\n",
      "Average test loss: 0.002940853646025062\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01639536556767093\n",
      "Average test loss: 0.00281592353578243\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01637185357676612\n",
      "Average test loss: 0.002831388235816525\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01635542941507366\n",
      "Average test loss: 0.002704236342985597\n",
      "Epoch 95/300\n",
      "Average training loss: 0.016360926636391215\n",
      "Average test loss: 0.0027986351313690344\n",
      "Epoch 96/300\n",
      "Average training loss: 0.016297225561406878\n",
      "Average test loss: 0.0027928741249359317\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016288867228560978\n",
      "Average test loss: 0.0030169630450093085\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016298765430847805\n",
      "Average test loss: 0.002712639410669605\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016249173674318527\n",
      "Average test loss: 0.002664408568913738\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01622582752837075\n",
      "Average test loss: 0.002692747461092141\n",
      "Epoch 101/300\n",
      "Average training loss: 0.016243603794111147\n",
      "Average test loss: 0.003086245708891915\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01619462278402514\n",
      "Average test loss: 0.002934820028228892\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01622697915716304\n",
      "Average test loss: 0.002699703865684569\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016159863628447055\n",
      "Average test loss: 0.002707545111473236\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01619222755730152\n",
      "Average test loss: 0.00273422450468772\n",
      "Epoch 106/300\n",
      "Average training loss: 0.016144414314793215\n",
      "Average test loss: 0.0027623063330021167\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016107123664683767\n",
      "Average test loss: 0.002784764172716273\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01611293467465374\n",
      "Average test loss: 0.002798130465878381\n",
      "Epoch 109/300\n",
      "Average training loss: 0.016076615335212815\n",
      "Average test loss: 0.002737114524882701\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016077219411730768\n",
      "Average test loss: 0.0027507099718269376\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016055152954326735\n",
      "Average test loss: 0.0027701985995388693\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016023879730039173\n",
      "Average test loss: 0.0027614895320600933\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01601144523008002\n",
      "Average test loss: 0.0027721847103287776\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015991928686698277\n",
      "Average test loss: 0.003608787131185333\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01599348298460245\n",
      "Average test loss: 0.0032011555599876576\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01599430751800537\n",
      "Average test loss: 0.0029441751918445032\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015948025373948946\n",
      "Average test loss: 0.002745617499678499\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01591642330504126\n",
      "Average test loss: 0.002852175782331162\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01597869465831253\n",
      "Average test loss: 0.0028554862338221734\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015892091772622532\n",
      "Average test loss: 0.0027545804993973837\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015906909249722958\n",
      "Average test loss: 0.0027968168358008067\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015887112555404505\n",
      "Average test loss: 0.0030297424943289825\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015874052244755958\n",
      "Average test loss: 0.0028557943486505082\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01588582927899228\n",
      "Average test loss: 0.0026955739474958844\n",
      "Epoch 125/300\n",
      "Average training loss: 0.015804858596788512\n",
      "Average test loss: 0.0028244716334674094\n",
      "Epoch 126/300\n",
      "Average training loss: 0.015833857773078813\n",
      "Average test loss: 0.0028255354401965937\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015818988980518447\n",
      "Average test loss: 0.0027185118376380868\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01580259467992518\n",
      "Average test loss: 0.002877873450724615\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01579603765077061\n",
      "Average test loss: 0.0027895364419867597\n",
      "Epoch 130/300\n",
      "Average training loss: 0.015783522496620815\n",
      "Average test loss: 0.0028821766775929264\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015767801261610453\n",
      "Average test loss: 0.0027856651490761174\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015744355061815846\n",
      "Average test loss: 0.002734761438229018\n",
      "Epoch 133/300\n",
      "Average training loss: 0.015755709336863623\n",
      "Average test loss: 0.0027961266823112963\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015710669243501292\n",
      "Average test loss: 0.0027134307511150835\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015732074962721932\n",
      "Average test loss: 0.002993820222301616\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015685780104663635\n",
      "Average test loss: 0.0028548416400121317\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015665772614379726\n",
      "Average test loss: 0.0027687762803915474\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015696234499414763\n",
      "Average test loss: 0.002866779813956883\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01569219043933683\n",
      "Average test loss: 0.0027802492651260564\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015665812613235578\n",
      "Average test loss: 0.002799408170912001\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015614810374048022\n",
      "Average test loss: 0.0027757802282770473\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015617647075818645\n",
      "Average test loss: 0.0031778663934932815\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015628174611263805\n",
      "Average test loss: 0.0028360165961914593\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015603105824854639\n",
      "Average test loss: 0.0028568080173184476\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015619661622577243\n",
      "Average test loss: 0.0027953108878185353\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01558939162393411\n",
      "Average test loss: 0.0028711858772569234\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015577261997593773\n",
      "Average test loss: 0.002834501347401076\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015573559833897485\n",
      "Average test loss: 0.0028983978405594827\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015574816342029306\n",
      "Average test loss: 0.003163243227120903\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015546572099129359\n",
      "Average test loss: 0.0027685707110083765\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01553924643413888\n",
      "Average test loss: 0.002827774487539298\n",
      "Epoch 152/300\n",
      "Average training loss: 0.015538475102848478\n",
      "Average test loss: 0.0028193394329605833\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015517430786457327\n",
      "Average test loss: 0.002834528779817952\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015531139176752832\n",
      "Average test loss: 0.0028011971120205192\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015474123215509785\n",
      "Average test loss: 0.002815190664182107\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015506251040432188\n",
      "Average test loss: 0.00280190560573505\n",
      "Epoch 157/300\n",
      "Average training loss: 0.015469891294836998\n",
      "Average test loss: 0.002888657732763224\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01547930141290029\n",
      "Average test loss: 0.003024125211975641\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015452178344130516\n",
      "Average test loss: 0.002772006309694714\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015434586803946231\n",
      "Average test loss: 0.002954530894756317\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015516274394260513\n",
      "Average test loss: 0.0027907916125324036\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015432037317090565\n",
      "Average test loss: 0.0027645538314763044\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015421296830806468\n",
      "Average test loss: 0.0028645192493374147\n",
      "Epoch 164/300\n",
      "Average training loss: 0.015413237837453683\n",
      "Average test loss: 0.0028711767699569463\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015412851876682706\n",
      "Average test loss: 0.002812172473097841\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015404594363437758\n",
      "Average test loss: 0.0027577793637497557\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015407443755202824\n",
      "Average test loss: 0.003353919493034482\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015341734654373593\n",
      "Average test loss: 0.0028913106069796616\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01541219841937224\n",
      "Average test loss: 0.003619698433412446\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015344982368250689\n",
      "Average test loss: 0.0030746176965120767\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015347847190996012\n",
      "Average test loss: 0.0029013935905984707\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015313406470749113\n",
      "Average test loss: 0.003034738354384899\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01533356343375312\n",
      "Average test loss: 0.0027995000227044025\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015340970425970024\n",
      "Average test loss: 0.0028707465891622836\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015313633715113004\n",
      "Average test loss: 0.0029047349337488412\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015310622841119767\n",
      "Average test loss: 0.004378235874490605\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015281293796168433\n",
      "Average test loss: 0.0028402502973460487\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015272542059421539\n",
      "Average test loss: 0.0031418217575798434\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015288133134444555\n",
      "Average test loss: 0.0028958537224680185\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01528108452591631\n",
      "Average test loss: 0.0029485113966382214\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015286053047411971\n",
      "Average test loss: 0.003726112485345867\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015236915131409963\n",
      "Average test loss: 0.002848227406334546\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015234264168474409\n",
      "Average test loss: 0.002819271223826541\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015251821844114197\n",
      "Average test loss: 0.003078067089844909\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015244278411070506\n",
      "Average test loss: 0.0032551940453963146\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01518493626680639\n",
      "Average test loss: 0.0028714574937605195\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015214947912428113\n",
      "Average test loss: 0.003251354630622599\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015194877120355764\n",
      "Average test loss: 0.0028629504601574607\n",
      "Epoch 189/300\n",
      "Average training loss: 0.015221870507631036\n",
      "Average test loss: 0.002918640206257502\n",
      "Epoch 190/300\n",
      "Average training loss: 0.015191139408283764\n",
      "Average test loss: 0.002949959482376774\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015175263072053592\n",
      "Average test loss: 0.0028921849692447317\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015196882160173522\n",
      "Average test loss: 0.003001918774512079\n",
      "Epoch 193/300\n",
      "Average training loss: 0.015169712543487548\n",
      "Average test loss: 0.002958319182611174\n",
      "Epoch 194/300\n",
      "Average training loss: 0.015157059945993954\n",
      "Average test loss: 0.002858824246459537\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01516099651902914\n",
      "Average test loss: 0.002839377275771565\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015158147375616762\n",
      "Average test loss: 0.0028511115666478874\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015109573908150197\n",
      "Average test loss: 0.003018968289718032\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015125419202778075\n",
      "Average test loss: 0.0028873169693268007\n",
      "Epoch 199/300\n",
      "Average training loss: 0.015127032296525107\n",
      "Average test loss: 0.005361750499655803\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01514717467294799\n",
      "Average test loss: 0.002871684430167079\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01511856781774097\n",
      "Average test loss: 0.0029770060473432143\n",
      "Epoch 202/300\n",
      "Average training loss: 0.015128316496809324\n",
      "Average test loss: 0.002856066366036733\n",
      "Epoch 203/300\n",
      "Average training loss: 0.015126230306923389\n",
      "Average test loss: 0.0029144205109526713\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015091998612715138\n",
      "Average test loss: 0.0029161618805179992\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015102723170485761\n",
      "Average test loss: 0.0028580102623543806\n",
      "Epoch 206/300\n",
      "Average training loss: 0.015065848685801029\n",
      "Average test loss: 0.0030352086866688396\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015080704112019803\n",
      "Average test loss: 0.0028382973606801694\n",
      "Epoch 208/300\n",
      "Average training loss: 0.015073676431344616\n",
      "Average test loss: 0.002873401425778866\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015064610309071011\n",
      "Average test loss: 0.002905599378877216\n",
      "Epoch 210/300\n",
      "Average training loss: 0.015080209838019477\n",
      "Average test loss: 0.0029005033106853564\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015043478504651122\n",
      "Average test loss: 0.0028990252593325246\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01503652506073316\n",
      "Average test loss: 0.002901891455054283\n",
      "Epoch 213/300\n",
      "Average training loss: 0.015028302248981265\n",
      "Average test loss: 0.0029038722672396237\n",
      "Epoch 214/300\n",
      "Average training loss: 0.015029411789443758\n",
      "Average test loss: 0.0031418281897074647\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015014029713140593\n",
      "Average test loss: 0.002836201027242674\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015015798540578948\n",
      "Average test loss: 0.0029404682419780226\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01504047697203027\n",
      "Average test loss: 0.0030215677997718253\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015058129570550388\n",
      "Average test loss: 0.002883610927706791\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01497455751688944\n",
      "Average test loss: 0.003099691878590319\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014997914801041285\n",
      "Average test loss: 0.0030222020759764646\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014982154758440124\n",
      "Average test loss: 0.019076468974351883\n",
      "Epoch 222/300\n",
      "Average training loss: 0.015313850705822309\n",
      "Average test loss: 0.0030651872859646877\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014958561701907052\n",
      "Average test loss: 0.0030195018756720756\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0149719031949838\n",
      "Average test loss: 0.0028904548076291877\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014939441449112363\n",
      "Average test loss: 0.0029257174105279974\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014940610548688307\n",
      "Average test loss: 0.0029250385535673964\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014954015597701072\n",
      "Average test loss: 0.0028725472912192345\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014959778144127793\n",
      "Average test loss: 0.002926500400321351\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014931096408930089\n",
      "Average test loss: 0.0030923673129744\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014953605362110668\n",
      "Average test loss: 0.002880236075570186\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014916335079405043\n",
      "Average test loss: 0.0029025995812068384\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014898309915430017\n",
      "Average test loss: 0.0028706503162781397\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014913004291554292\n",
      "Average test loss: 0.002947394443882836\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014925485091904798\n",
      "Average test loss: 0.002915962176190482\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014939214368661245\n",
      "Average test loss: 0.0029263117752141425\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014913859179450406\n",
      "Average test loss: 0.0030103705109407503\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014908607790867487\n",
      "Average test loss: 0.0030300085346938836\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01488688816792435\n",
      "Average test loss: 0.0029829773128860525\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014898036630617248\n",
      "Average test loss: 0.002915401700263222\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014875296036402384\n",
      "Average test loss: 0.003028231876798802\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014880427814192241\n",
      "Average test loss: 0.002838160820098387\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014867097742855549\n",
      "Average test loss: 0.0030097949430346487\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014882768932316038\n",
      "Average test loss: 0.002844034616731935\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014874007038772107\n",
      "Average test loss: 0.002896311376657751\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014839909057650301\n",
      "Average test loss: 0.002945086870032052\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014889600656926632\n",
      "Average test loss: 0.003035782780705227\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014826924260291788\n",
      "Average test loss: 0.0034340634526064\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014815587722592883\n",
      "Average test loss: 0.0030511038839403128\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014832866158750322\n",
      "Average test loss: 0.0029076534174382685\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014844994391004245\n",
      "Average test loss: 0.0030138115277513863\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014817650616168975\n",
      "Average test loss: 0.0029065562906778523\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014844460790356\n",
      "Average test loss: 0.0030336303427401516\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014768149800598622\n",
      "Average test loss: 0.002943377952505317\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014825322333309385\n",
      "Average test loss: 0.0029170459558566413\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014807094748649332\n",
      "Average test loss: 0.0029222814978824723\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014811181417769857\n",
      "Average test loss: 0.0029432913615471787\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014813129909336567\n",
      "Average test loss: 0.002853624606298076\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014821993100146452\n",
      "Average test loss: 0.0031207879939013057\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014772193976574474\n",
      "Average test loss: 0.0029272272849662434\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014790815853410296\n",
      "Average test loss: 0.0030603519038607677\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01475566483537356\n",
      "Average test loss: 0.003126897621072001\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014778538519309627\n",
      "Average test loss: 0.0029822484627366067\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014807148983909025\n",
      "Average test loss: 0.002910107424068782\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014790792826977041\n",
      "Average test loss: 0.0030388189457977812\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014759055688977242\n",
      "Average test loss: 0.003003866679759489\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014789038572046493\n",
      "Average test loss: 0.0028785072627166906\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014723406463861466\n",
      "Average test loss: 0.003148367223019401\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014737853334181839\n",
      "Average test loss: 0.0030128286766509214\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014765691567626265\n",
      "Average test loss: 0.002960772505650918\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014760893421868483\n",
      "Average test loss: 0.0030409532493601244\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014701452949808704\n",
      "Average test loss: 0.0030751576899654335\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01474166667378611\n",
      "Average test loss: 0.0029675573067118725\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014718057479295466\n",
      "Average test loss: 0.0030541317334605586\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014735381919476721\n",
      "Average test loss: 0.003048113132516543\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014706888630158371\n",
      "Average test loss: 0.0029845356723914545\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014702332759896915\n",
      "Average test loss: 0.0031169346931080024\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014712791447838148\n",
      "Average test loss: 0.0029218601305037737\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01471950363036659\n",
      "Average test loss: 0.003118499956197209\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014715159498155116\n",
      "Average test loss: 0.003098842075922423\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014711219129463037\n",
      "Average test loss: 0.0029274427102257807\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014684458623329798\n",
      "Average test loss: 0.0030116754410167536\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014712644619246323\n",
      "Average test loss: 0.07009527243508233\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014950950827863482\n",
      "Average test loss: 0.003069399690255523\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014653464844657314\n",
      "Average test loss: 0.0028733763428818847\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014632189223335848\n",
      "Average test loss: 0.0030119103545116054\n",
      "Epoch 286/300\n",
      "Average training loss: 0.014664586022496224\n",
      "Average test loss: 0.0030412756111472847\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014667837806873852\n",
      "Average test loss: 0.002917955611418519\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014665077305502362\n",
      "Average test loss: 0.00301401890317599\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014686938874423504\n",
      "Average test loss: 0.0030874210794766743\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014640042623711956\n",
      "Average test loss: 0.003026116482499573\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014661226072245175\n",
      "Average test loss: 0.0029921529460698367\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014631152431997987\n",
      "Average test loss: 0.0030025357260472243\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014645702983770105\n",
      "Average test loss: 0.002984319661019577\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014649873909850916\n",
      "Average test loss: 0.003048066067198912\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014635723791188665\n",
      "Average test loss: 0.003040132630512946\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014613396540284157\n",
      "Average test loss: 0.002986254904833105\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014671406467755636\n",
      "Average test loss: 0.0029594956383936933\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014626951321131653\n",
      "Average test loss: 0.0029963033854340515\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014591720667978128\n",
      "Average test loss: 0.003041179509833455\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01463819832023647\n",
      "Average test loss: 0.0030455019331226746\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11129995451039738\n",
      "Average test loss: 0.005234455751048194\n",
      "Epoch 2/300\n",
      "Average training loss: 0.036502867450316744\n",
      "Average test loss: 0.004713567945278353\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03155947271320555\n",
      "Average test loss: 0.00450753976052834\n",
      "Epoch 4/300\n",
      "Average training loss: 0.028915004948774973\n",
      "Average test loss: 0.004498642949387431\n",
      "Epoch 5/300\n",
      "Average training loss: 0.027118334694041145\n",
      "Average test loss: 0.0034444809092415703\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02515869368612766\n",
      "Average test loss: 0.0034727976193858518\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0238546903596984\n",
      "Average test loss: 0.0031801371009399495\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022490511584613057\n",
      "Average test loss: 0.0031120425319919984\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021506934735510085\n",
      "Average test loss: 0.002914383671970831\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02079214009311464\n",
      "Average test loss: 0.0027314359661605625\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02002359734310044\n",
      "Average test loss: 0.0028092311854577725\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019496726262900565\n",
      "Average test loss: 0.0026388486901091205\n",
      "Epoch 13/300\n",
      "Average training loss: 0.019069764632317757\n",
      "Average test loss: 0.002656497222267919\n",
      "Epoch 14/300\n",
      "Average training loss: 0.018711246952414513\n",
      "Average test loss: 0.0025514945447858836\n",
      "Epoch 15/300\n",
      "Average training loss: 0.018210944404204687\n",
      "Average test loss: 0.0023970237918612028\n",
      "Epoch 16/300\n",
      "Average training loss: 0.017898627839154666\n",
      "Average test loss: 0.002324896458329426\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01763768571532435\n",
      "Average test loss: 0.0024064659256384604\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017324849830733405\n",
      "Average test loss: 0.002275523055034379\n",
      "Epoch 19/300\n",
      "Average training loss: 0.017121429337395563\n",
      "Average test loss: 0.0022565519319226344\n",
      "Epoch 20/300\n",
      "Average training loss: 0.016884043720033434\n",
      "Average test loss: 0.0022632915594925484\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016770790449447102\n",
      "Average test loss: 0.002200730924391084\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016540542383160856\n",
      "Average test loss: 0.002171266556200054\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01637100702441401\n",
      "Average test loss: 0.002172960544625918\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01625698381745153\n",
      "Average test loss: 0.002224267275590036\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01615497144146098\n",
      "Average test loss: 0.0020894645408002866\n",
      "Epoch 26/300\n",
      "Average training loss: 0.016023613965345754\n",
      "Average test loss: 0.002061222554494937\n",
      "Epoch 27/300\n",
      "Average training loss: 0.015899831908444562\n",
      "Average test loss: 0.002024511986101667\n",
      "Epoch 28/300\n",
      "Average training loss: 0.015783120119737253\n",
      "Average test loss: 0.002119467484868235\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01565981170700656\n",
      "Average test loss: 0.002024696385901835\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01560551263888677\n",
      "Average test loss: 0.0020096621956262324\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015518945158355765\n",
      "Average test loss: 0.001956332317036059\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015388276331954533\n",
      "Average test loss: 0.002014632652203242\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015344371159871419\n",
      "Average test loss: 0.002003471918300622\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015318330291244719\n",
      "Average test loss: 0.0019577725135410825\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01523201320734289\n",
      "Average test loss: 0.0019721986380302242\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015127645134925842\n",
      "Average test loss: 0.001963458779992329\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015097037711905108\n",
      "Average test loss: 0.0019452248511628973\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015019469768636757\n",
      "Average test loss: 0.0024069045175694756\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01498288845602009\n",
      "Average test loss: 0.0019667274658050804\n",
      "Epoch 40/300\n",
      "Average training loss: 0.014892090023391777\n",
      "Average test loss: 0.0019198283948418167\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014875239396260844\n",
      "Average test loss: 0.0018917025496355362\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014809302916957272\n",
      "Average test loss: 0.0019481738050364785\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014759761470887396\n",
      "Average test loss: 0.001900982544446985\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014701653474734889\n",
      "Average test loss: 0.0018913126757575407\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014674944077928862\n",
      "Average test loss: 0.0019059536980672014\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014671038861903879\n",
      "Average test loss: 0.0019114084093727998\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014588178887963295\n",
      "Average test loss: 0.0019126647033521698\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01454721289459202\n",
      "Average test loss: 0.00202393398537404\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014522932031088405\n",
      "Average test loss: 0.0019377683234504527\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014482062550054657\n",
      "Average test loss: 0.001917928124467532\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014445113148126337\n",
      "Average test loss: 0.001919333829647965\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014407313093543053\n",
      "Average test loss: 0.001881332951494389\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01438339998986986\n",
      "Average test loss: 0.0019279051905290948\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014313068060411348\n",
      "Average test loss: 0.002103259999718931\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014365189245177639\n",
      "Average test loss: 0.0019404818976504934\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0142432430361708\n",
      "Average test loss: 0.0019046762227598163\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014238024664421877\n",
      "Average test loss: 0.0019451329058243168\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01419941908452246\n",
      "Average test loss: 0.001945997071141998\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0142081933816274\n",
      "Average test loss: 0.001960913489883145\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014140034885042244\n",
      "Average test loss: 0.00201371840470367\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01413460633241468\n",
      "Average test loss: 0.00193727457181861\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014103223096993234\n",
      "Average test loss: 0.002060407623441683\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014086487896740437\n",
      "Average test loss: 0.0019833068678983385\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014035331668953101\n",
      "Average test loss: 0.001897586717373795\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014010022833943366\n",
      "Average test loss: 0.0019069303127212656\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013978120418058501\n",
      "Average test loss: 0.0019403429939928983\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013969257762034734\n",
      "Average test loss: 0.0019190122509996097\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013946434601313538\n",
      "Average test loss: 0.0019279491085973052\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013931877977318234\n",
      "Average test loss: 0.0019481874350458384\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013936460319492552\n",
      "Average test loss: 0.0019081631844035454\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013887114906476603\n",
      "Average test loss: 0.0019011418458281292\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01406696684161822\n",
      "Average test loss: 0.0019196252249595192\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013827769382132425\n",
      "Average test loss: 0.0019232423826017313\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01378825560460488\n",
      "Average test loss: 0.0020001725574127502\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013771037888195779\n",
      "Average test loss: 0.0019291888000443577\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013777159181733925\n",
      "Average test loss: 0.0018917383710957236\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013744437042209838\n",
      "Average test loss: 0.0019156419351283047\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013714832395315171\n",
      "Average test loss: 0.0019471237203106285\n",
      "Epoch 79/300\n",
      "Average training loss: 0.013699810139834881\n",
      "Average test loss: 0.0019254794996231795\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013692505896919303\n",
      "Average test loss: 0.0019590403183052936\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013667612631287839\n",
      "Average test loss: 0.001924810134805739\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01364768692685498\n",
      "Average test loss: 0.0018952306117862464\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013630961635046535\n",
      "Average test loss: 0.0019384543587350183\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013608548035224278\n",
      "Average test loss: 0.0019128262371652656\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013586183195312818\n",
      "Average test loss: 0.0019412955252660646\n",
      "Epoch 86/300\n",
      "Average training loss: 0.013584851966963874\n",
      "Average test loss: 0.0019782125170653064\n",
      "Epoch 87/300\n",
      "Average training loss: 0.013540992223968108\n",
      "Average test loss: 0.0020100185317504736\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01354500729093949\n",
      "Average test loss: 0.0019044496441880862\n",
      "Epoch 89/300\n",
      "Average training loss: 0.013539686938126882\n",
      "Average test loss: 0.0020461050311310425\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013531073672903909\n",
      "Average test loss: 0.0022995903013894956\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013479978010886245\n",
      "Average test loss: 0.0019434232101258305\n",
      "Epoch 92/300\n",
      "Average training loss: 0.013439379745059543\n",
      "Average test loss: 0.002072483224173387\n",
      "Epoch 93/300\n",
      "Average training loss: 0.013427486371662882\n",
      "Average test loss: 0.001900289588711328\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013414537058108383\n",
      "Average test loss: 0.0020297073447662923\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013427981483853525\n",
      "Average test loss: 0.002005918980679578\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013392863618002997\n",
      "Average test loss: 0.0019929074577780233\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013437665024565325\n",
      "Average test loss: 0.066140356792344\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015454073666284481\n",
      "Average test loss: 0.002002983270626929\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013482936479151249\n",
      "Average test loss: 0.0020863194082760147\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013330650948815876\n",
      "Average test loss: 0.0019446292569239934\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013307375425265895\n",
      "Average test loss: 0.002020872870563633\n",
      "Epoch 102/300\n",
      "Average training loss: 0.013311511745055517\n",
      "Average test loss: 0.0019724391671932405\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01329947727256351\n",
      "Average test loss: 0.0019392988624879056\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013290178254246712\n",
      "Average test loss: 0.001973504384358724\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013301838943527804\n",
      "Average test loss: 0.001967298316872782\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013288721739417976\n",
      "Average test loss: 0.002007718166336417\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013267801146540376\n",
      "Average test loss: 0.001945275243682166\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013301841442783674\n",
      "Average test loss: 0.0019595354231488372\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0132256249545349\n",
      "Average test loss: 0.0020432698664565882\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013230589882367187\n",
      "Average test loss: 0.0020547755619304046\n",
      "Epoch 111/300\n",
      "Average training loss: 0.013204728870756096\n",
      "Average test loss: 0.0021439056048790615\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01320309432513184\n",
      "Average test loss: 0.0020616966920594375\n",
      "Epoch 113/300\n",
      "Average training loss: 0.013219531934294435\n",
      "Average test loss: 0.001959642232706149\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013186378597385353\n",
      "Average test loss: 0.001971532167142464\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013173467971384526\n",
      "Average test loss: 0.0019707295722845527\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013170102293706602\n",
      "Average test loss: 0.0020199036598205567\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013163983271767696\n",
      "Average test loss: 0.0020547301542634764\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013128428781198132\n",
      "Average test loss: 0.002001872128703528\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013110968795087602\n",
      "Average test loss: 0.0020156939973433813\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013103816652463542\n",
      "Average test loss: 0.0019444659534427855\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013117155525419448\n",
      "Average test loss: 0.002047152335445086\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013100545937816302\n",
      "Average test loss: 0.0019893923103809358\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013059375665254063\n",
      "Average test loss: 0.002004189463849697\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013072582241561678\n",
      "Average test loss: 0.0019548168575598134\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013046648552848233\n",
      "Average test loss: 0.0020047462359070777\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013050387693776025\n",
      "Average test loss: 0.001961068780782322\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013024507026705478\n",
      "Average test loss: 0.002030444502416584\n",
      "Epoch 128/300\n",
      "Average training loss: 0.013012123021814558\n",
      "Average test loss: 0.0020763997732558184\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013037924783925216\n",
      "Average test loss: 0.0020097466523034705\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012977405459516578\n",
      "Average test loss: 0.002039914332744148\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01297747011648284\n",
      "Average test loss: 0.002121140085471173\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012966792990763983\n",
      "Average test loss: 0.0020179110136297014\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01296599304344919\n",
      "Average test loss: 0.001983525974676013\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01294986239903503\n",
      "Average test loss: 0.0021346066765901115\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012934655139015781\n",
      "Average test loss: 0.0021040941247095664\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012932381592690945\n",
      "Average test loss: 0.002043377600196335\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012918537503315343\n",
      "Average test loss: 0.0020074238830970394\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012901617693404357\n",
      "Average test loss: 0.0020173311120726995\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01291606516391039\n",
      "Average test loss: 0.0020894901365455655\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012894014261662961\n",
      "Average test loss: 0.0020907117939657637\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012887153230607509\n",
      "Average test loss: 0.002090582027203507\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012886118898789089\n",
      "Average test loss: 0.002069961092952225\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012876089242597422\n",
      "Average test loss: 0.0022016562968492507\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012849504368172752\n",
      "Average test loss: 0.0020975076463073494\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012842493842045466\n",
      "Average test loss: 0.001997667372330195\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01285278052588304\n",
      "Average test loss: 0.0021090193337036504\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01284160354733467\n",
      "Average test loss: 0.0020003256555646658\n",
      "Epoch 148/300\n",
      "Average training loss: 0.012805674359202384\n",
      "Average test loss: 0.002010080014458961\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012806082794235812\n",
      "Average test loss: 0.0019752325533578794\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012782456135584248\n",
      "Average test loss: 0.0020532709527760745\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012791160640617211\n",
      "Average test loss: 0.0020994730622818075\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01278031773202949\n",
      "Average test loss: 0.0019782125393135682\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01279012102385362\n",
      "Average test loss: 0.0019965581852528784\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012752536041869057\n",
      "Average test loss: 0.002015946567782925\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012773661774893601\n",
      "Average test loss: 0.002279428313486278\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012725613258779049\n",
      "Average test loss: 0.002100342235631413\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012776739654441675\n",
      "Average test loss: 0.0020591445186485847\n",
      "Epoch 158/300\n",
      "Average training loss: 0.012719362884759902\n",
      "Average test loss: 0.0021514791482024724\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012715478401217195\n",
      "Average test loss: 0.0020576037283365927\n",
      "Epoch 160/300\n",
      "Average training loss: 0.012722276865608163\n",
      "Average test loss: 0.0020613964723630086\n",
      "Epoch 161/300\n",
      "Average training loss: 0.012708736687070794\n",
      "Average test loss: 0.002081140562374559\n",
      "Epoch 162/300\n",
      "Average training loss: 0.012709029267231624\n",
      "Average test loss: 0.0020007300848762194\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012694229055609968\n",
      "Average test loss: 0.0020966585599299934\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01272225138793389\n",
      "Average test loss: 0.0020233209828535714\n",
      "Epoch 165/300\n",
      "Average training loss: 0.012677413752509487\n",
      "Average test loss: 0.0020439498054070607\n",
      "Epoch 166/300\n",
      "Average training loss: 0.012674869856072796\n",
      "Average test loss: 0.002094476470309827\n",
      "Epoch 167/300\n",
      "Average training loss: 0.012672771731184588\n",
      "Average test loss: 0.002020758316955633\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01266394283539719\n",
      "Average test loss: 0.0021169808105462127\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012642459632621872\n",
      "Average test loss: 0.0021125518065980738\n",
      "Epoch 170/300\n",
      "Average training loss: 0.012641808537145456\n",
      "Average test loss: 0.0022908110207774572\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012633288260963228\n",
      "Average test loss: 0.0019922561792449817\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012655838492015998\n",
      "Average test loss: 0.002047557285055518\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012629915865759055\n",
      "Average test loss: 0.0021735809234281383\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01262278298785289\n",
      "Average test loss: 0.002061177853287922\n",
      "Epoch 175/300\n",
      "Average training loss: 0.012588562972015804\n",
      "Average test loss: 0.002103715298490392\n",
      "Epoch 176/300\n",
      "Average training loss: 0.012606848537094064\n",
      "Average test loss: 0.0020887760783856115\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012577472981479433\n",
      "Average test loss: 0.0020430757956993247\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012589415732357237\n",
      "Average test loss: 0.0020808112123567196\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012594563560353385\n",
      "Average test loss: 0.002074156762411197\n",
      "Epoch 180/300\n",
      "Average training loss: 0.012573614613877402\n",
      "Average test loss: 0.0020645442590531376\n",
      "Epoch 181/300\n",
      "Average training loss: 0.012596714989178711\n",
      "Average test loss: 0.0020594309222780995\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01256349468479554\n",
      "Average test loss: 0.0020492439731541606\n",
      "Epoch 183/300\n",
      "Average training loss: 0.012556977967421213\n",
      "Average test loss: 0.0020349845728940435\n",
      "Epoch 184/300\n",
      "Average training loss: 0.012561692546639178\n",
      "Average test loss: 0.002136377023946908\n",
      "Epoch 185/300\n",
      "Average training loss: 0.012546560088793436\n",
      "Average test loss: 0.0021742094204657606\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012550350151956081\n",
      "Average test loss: 0.002098500433481402\n",
      "Epoch 187/300\n",
      "Average training loss: 0.012521064747538831\n",
      "Average test loss: 0.002028328052825398\n",
      "Epoch 188/300\n",
      "Average training loss: 0.012520969630115562\n",
      "Average test loss: 0.002066538156786313\n",
      "Epoch 189/300\n",
      "Average training loss: 0.012513801693088479\n",
      "Average test loss: 0.0022947156205773353\n",
      "Epoch 190/300\n",
      "Average training loss: 0.012518203588823478\n",
      "Average test loss: 0.0022722417305534085\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012510604865021176\n",
      "Average test loss: 0.002171819758291046\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012493731318248642\n",
      "Average test loss: 0.0020795046105566953\n",
      "Epoch 193/300\n",
      "Average training loss: 0.012534139864974552\n",
      "Average test loss: 0.0020690228766244317\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012508163799014356\n",
      "Average test loss: 0.002115549791811241\n",
      "Epoch 195/300\n",
      "Average training loss: 0.012458198438915942\n",
      "Average test loss: 0.0021157009811658\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012458260102404488\n",
      "Average test loss: 0.0021005382168417176\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012466008639170063\n",
      "Average test loss: 0.0020510216859272786\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012463458225131036\n",
      "Average test loss: 0.002088872865566777\n",
      "Epoch 199/300\n",
      "Average training loss: 0.012442515866623984\n",
      "Average test loss: 0.0024452920171121758\n",
      "Epoch 200/300\n",
      "Average training loss: 0.012469972011115816\n",
      "Average test loss: 0.0025168746024784115\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012461789795094066\n",
      "Average test loss: 0.002120719051609437\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01243383186393314\n",
      "Average test loss: 0.00206801860469083\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012456142888300948\n",
      "Average test loss: 0.0020986345996045403\n",
      "Epoch 204/300\n",
      "Average training loss: 0.012425126196609603\n",
      "Average test loss: 0.0021300020954675143\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012408639867272642\n",
      "Average test loss: 0.002078831399480502\n",
      "Epoch 206/300\n",
      "Average training loss: 0.012420408265458214\n",
      "Average test loss: 0.0021528612251083055\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012428554430603981\n",
      "Average test loss: 0.002184071201624142\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012406710985634062\n",
      "Average test loss: 0.0020828244082836643\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012408290178411536\n",
      "Average test loss: 0.0020578829743382004\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01243026154819462\n",
      "Average test loss: 0.002076260630041361\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012393180718852414\n",
      "Average test loss: 0.008700931830538643\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012390346772968769\n",
      "Average test loss: 0.0020807129635165137\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012408689890470769\n",
      "Average test loss: 0.002082909661448664\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012407618301610152\n",
      "Average test loss: 0.002154679849019481\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012387618125312859\n",
      "Average test loss: 0.002108796839705772\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012372829506794612\n",
      "Average test loss: 0.002101808655800091\n",
      "Epoch 217/300\n",
      "Average training loss: 0.012381300979190402\n",
      "Average test loss: 0.0021043185093957517\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012369440046449503\n",
      "Average test loss: 0.0021095413758109014\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012367268054021729\n",
      "Average test loss: 0.0021433229992787045\n",
      "Epoch 220/300\n",
      "Average training loss: 0.012385066086219417\n",
      "Average test loss: 0.002220365058320264\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012330637243058946\n",
      "Average test loss: 0.0022380454941756194\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012361728354460663\n",
      "Average test loss: 0.0020842594892407458\n",
      "Epoch 223/300\n",
      "Average training loss: 0.012328571430510945\n",
      "Average test loss: 0.0020937944158083864\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0123831551472346\n",
      "Average test loss: 0.0021378901927835412\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012332326932913727\n",
      "Average test loss: 0.002128250295503272\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012340778867817587\n",
      "Average test loss: 0.002170224861138397\n",
      "Epoch 227/300\n",
      "Average training loss: 0.012336559309727615\n",
      "Average test loss: 0.0022088472350604003\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012317049471868409\n",
      "Average test loss: 0.0020942266498588853\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012322952364053992\n",
      "Average test loss: 0.0021553965710724393\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01232359363635381\n",
      "Average test loss: 0.0020856538289743994\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012302252055870162\n",
      "Average test loss: 0.0021141265050197643\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012305452959405052\n",
      "Average test loss: 0.002180625900005301\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012302962967091137\n",
      "Average test loss: 0.002106203068047762\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012302517574694421\n",
      "Average test loss: 0.002057761462405324\n",
      "Epoch 235/300\n",
      "Average training loss: 0.012312188732955191\n",
      "Average test loss: 0.0021537875822848743\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012280411518282361\n",
      "Average test loss: 0.002193181245173845\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012260604171289338\n",
      "Average test loss: 0.002335981866137849\n",
      "Epoch 238/300\n",
      "Average training loss: 0.012292305855287447\n",
      "Average test loss: 0.0021015399333296552\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012265203758246369\n",
      "Average test loss: 0.0020825390924389163\n",
      "Epoch 240/300\n",
      "Average training loss: 0.012253234826028346\n",
      "Average test loss: 0.002098581557472547\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0122661154874497\n",
      "Average test loss: 0.0022262050160724257\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012274599820375442\n",
      "Average test loss: 0.004701900011135472\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012254179374211364\n",
      "Average test loss: 0.0021566345126678545\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012252811442646715\n",
      "Average test loss: 0.0022080449182540177\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012277793768379423\n",
      "Average test loss: 0.002159089606669214\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012239047571188873\n",
      "Average test loss: 0.0021364780673239795\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012238530103531148\n",
      "Average test loss: 0.0021007659969230494\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012231671078337563\n",
      "Average test loss: 0.0020880595431145696\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012232349095245202\n",
      "Average test loss: 0.0021409892555740145\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01223735844095548\n",
      "Average test loss: 0.002136485808218519\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012235932368371222\n",
      "Average test loss: 0.002155378472680847\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012212349456217554\n",
      "Average test loss: 0.0021301713658289775\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01221568791485495\n",
      "Average test loss: 0.0021496404651552437\n",
      "Epoch 254/300\n",
      "Average training loss: 0.012229899203611744\n",
      "Average test loss: 0.0022237013320749004\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01221528479953607\n",
      "Average test loss: 0.0020926682729687954\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01219930125772953\n",
      "Average test loss: 0.002199749355049183\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012214643513162931\n",
      "Average test loss: 0.0021351785444551044\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012184695133732425\n",
      "Average test loss: 0.002119648455745644\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012212445759938823\n",
      "Average test loss: 0.0022591617663080495\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012170304418438012\n",
      "Average test loss: 0.0022547097228881384\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012177963965468936\n",
      "Average test loss: 0.0023873925430493224\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012185321527222792\n",
      "Average test loss: 0.002125774181758364\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012174779146909714\n",
      "Average test loss: 0.0021024792375456958\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012183027343617545\n",
      "Average test loss: 0.0021860357876867056\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01214654678851366\n",
      "Average test loss: 0.0022935140726880895\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012176786598232058\n",
      "Average test loss: 0.0021174548454582693\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01217301475091113\n",
      "Average test loss: 0.0021675749243133596\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012168114171259932\n",
      "Average test loss: 0.002127856367474629\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012125242205129729\n",
      "Average test loss: 0.0021469600156156553\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012152335206667583\n",
      "Average test loss: 0.002197658333823913\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012146831682158842\n",
      "Average test loss: 0.002110022294645508\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012170766782429484\n",
      "Average test loss: 0.0021721155585514174\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012144219391047954\n",
      "Average test loss: 0.0020842671304320297\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012139975426097712\n",
      "Average test loss: 0.002225691780861881\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012115756520794498\n",
      "Average test loss: 0.0021753405031437675\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012131304482618968\n",
      "Average test loss: 0.0025213463610659044\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012135038656493027\n",
      "Average test loss: 0.0025303141911410623\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012107763276331954\n",
      "Average test loss: 0.002108809377377232\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012133407533168793\n",
      "Average test loss: 0.0021240310188796784\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012107442400521702\n",
      "Average test loss: 0.0021621001509742606\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012101100308199723\n",
      "Average test loss: 0.003281638192012906\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012116670798924234\n",
      "Average test loss: 0.0022235536841261716\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012098538671102789\n",
      "Average test loss: 0.0021638400138666233\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012104354478418827\n",
      "Average test loss: 0.0021840388126050433\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012086585024164783\n",
      "Average test loss: 0.002280401246415244\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012117176314194997\n",
      "Average test loss: 0.002131181516788072\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012091730717155668\n",
      "Average test loss: 0.002190610924942626\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012098321985039446\n",
      "Average test loss: 0.005497299069745673\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012091157871815894\n",
      "Average test loss: 0.0020931380960262486\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012082126850883166\n",
      "Average test loss: 0.002211711586970422\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012082154317034615\n",
      "Average test loss: 0.0020721961041498516\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012065215590927336\n",
      "Average test loss: 0.0030212595148219\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012073545774651898\n",
      "Average test loss: 0.0022609386525841223\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012080296978354454\n",
      "Average test loss: 0.0021389996558945214\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01206337431155973\n",
      "Average test loss: 0.002157239661655492\n",
      "Epoch 296/300\n",
      "Average training loss: 0.012060229681432248\n",
      "Average test loss: 0.0022017536889761685\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012059118593732515\n",
      "Average test loss: 0.002126852369349864\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012066222788559067\n",
      "Average test loss: 0.0021274673217493625\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012048173737194802\n",
      "Average test loss: 0.0021875549802142713\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01202169670826859\n",
      "Average test loss: 0.0026491995324484175\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_32_Depth5/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 23.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.38\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.04\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.48\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.02\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.04\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.67\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.32\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 25.21\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.24\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.04\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.9697937263912624\n",
      "Average test loss: 0.01722721204989486\n",
      "Epoch 2/300\n",
      "Average training loss: 0.6766519974072774\n",
      "Average test loss: 0.0138532974206739\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4526707661151886\n",
      "Average test loss: 0.00940169268184238\n",
      "Epoch 4/300\n",
      "Average training loss: 0.34922791228029465\n",
      "Average test loss: 0.008912668987280792\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2860657325850593\n",
      "Average test loss: 0.008417026974260807\n",
      "Epoch 6/300\n",
      "Average training loss: 0.24803358181317647\n",
      "Average test loss: 0.00860671584473716\n",
      "Epoch 7/300\n",
      "Average training loss: 0.21980841138627794\n",
      "Average test loss: 0.009276473277144962\n",
      "Epoch 8/300\n",
      "Average training loss: 0.19704046665297614\n",
      "Average test loss: 0.01015992000616259\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1847867894437578\n",
      "Average test loss: 0.007966439492586587\n",
      "Epoch 10/300\n",
      "Average training loss: 0.17205119502544403\n",
      "Average test loss: 0.00815279882401228\n",
      "Epoch 11/300\n",
      "Average training loss: 0.16238108274671767\n",
      "Average test loss: 0.007716891631484032\n",
      "Epoch 12/300\n",
      "Average training loss: 0.15444007513258193\n",
      "Average test loss: 0.00802221779152751\n",
      "Epoch 13/300\n",
      "Average training loss: 0.14803318060768975\n",
      "Average test loss: 0.008069524688025315\n",
      "Epoch 14/300\n",
      "Average training loss: 0.14441467212306128\n",
      "Average test loss: 0.007247313900540273\n",
      "Epoch 15/300\n",
      "Average training loss: 0.13953438562817044\n",
      "Average test loss: 0.007216235881050428\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1348003624810113\n",
      "Average test loss: 0.008825470853183005\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13120989763736726\n",
      "Average test loss: 0.007593827729837762\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12857553275426228\n",
      "Average test loss: 0.00858221423294809\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12480115071270201\n",
      "Average test loss: 0.007380483965492911\n",
      "Epoch 20/300\n",
      "Average training loss: 0.12208156665828493\n",
      "Average test loss: 0.006903945889737871\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1193731794224845\n",
      "Average test loss: 0.01988858363777399\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11778666877084308\n",
      "Average test loss: 0.006754449055840572\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11493818355931176\n",
      "Average test loss: 0.00648565159820848\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11328916096025043\n",
      "Average test loss: 0.006886650817675723\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11186220098204083\n",
      "Average test loss: 0.0063580667128165565\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11036840881241693\n",
      "Average test loss: 0.006692801863369014\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1086799314154519\n",
      "Average test loss: 0.006556674886080954\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10759604597753948\n",
      "Average test loss: 0.006643602365007003\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10615543992651834\n",
      "Average test loss: 0.006253124211397436\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10498987513780594\n",
      "Average test loss: 0.00635413862619963\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1040425952606731\n",
      "Average test loss: 0.006249319248315361\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10276563761631648\n",
      "Average test loss: 0.007099964362879594\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10175028564532598\n",
      "Average test loss: 0.006090263452380896\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10065314047866397\n",
      "Average test loss: 0.006166650205022759\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09968118647072051\n",
      "Average test loss: 0.0075038679730561044\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09866925575998094\n",
      "Average test loss: 0.0060401098997228675\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09801205460892783\n",
      "Average test loss: 0.006225312701943848\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09711473241117266\n",
      "Average test loss: 0.0061336308415565225\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0964458105299208\n",
      "Average test loss: 0.006607036711855067\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09558927198913363\n",
      "Average test loss: 0.0099679307838281\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0947478298942248\n",
      "Average test loss: 0.006261426632189088\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0940197279519505\n",
      "Average test loss: 0.006182572214553753\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09344462222523159\n",
      "Average test loss: 0.006189927779138088\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09276044105158912\n",
      "Average test loss: 0.040134037983086374\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09203847418891059\n",
      "Average test loss: 0.006102472942322493\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09144994727108213\n",
      "Average test loss: 0.006045398935675621\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09084630386034648\n",
      "Average test loss: 0.005984408094651169\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09041112324926588\n",
      "Average test loss: 0.006273216726879279\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08973644798994064\n",
      "Average test loss: 0.006302684404990739\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0892042952378591\n",
      "Average test loss: 0.0060984029086927575\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0889480799900161\n",
      "Average test loss: 0.007524898863087098\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08845747991402944\n",
      "Average test loss: 0.006428191103455093\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08774892477194468\n",
      "Average test loss: 0.0070871065323137574\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08740361675951216\n",
      "Average test loss: 0.00605687299867471\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08704086926910612\n",
      "Average test loss: 0.006434140771213505\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08667265736394458\n",
      "Average test loss: 0.006996277427507772\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08608141030536758\n",
      "Average test loss: 0.006936971115155352\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08591698319382138\n",
      "Average test loss: 0.006458638076980909\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08547096539868249\n",
      "Average test loss: 0.006441093162529998\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08506558992465336\n",
      "Average test loss: 0.006262644824468427\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08469596144888136\n",
      "Average test loss: 0.006395244167082839\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08414743190341525\n",
      "Average test loss: 0.0060810433415075145\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08399446846379174\n",
      "Average test loss: 0.006719750436643759\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08351662729846107\n",
      "Average test loss: 0.05452699525985453\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0934361200067732\n",
      "Average test loss: 0.006075629437135325\n",
      "Epoch 66/300\n",
      "Average training loss: 0.41133095749219256\n",
      "Average test loss: 0.02761012120379342\n",
      "Epoch 67/300\n",
      "Average training loss: 0.9312482060591379\n",
      "Average test loss: 0.006995425546748771\n",
      "Epoch 68/300\n",
      "Average training loss: 0.2656513482199775\n",
      "Average test loss: 0.006649279560893774\n",
      "Epoch 69/300\n",
      "Average training loss: 0.19262080656157599\n",
      "Average test loss: 0.00665393551480439\n",
      "Epoch 70/300\n",
      "Average training loss: 0.16465389213297102\n",
      "Average test loss: 0.00626268710113234\n",
      "Epoch 71/300\n",
      "Average training loss: 0.14837312696377436\n",
      "Average test loss: 0.006416210473825534\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1365635159081883\n",
      "Average test loss: 0.006117179702139563\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12796063032415178\n",
      "Average test loss: 0.006117808086590635\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12227079298761157\n",
      "Average test loss: 0.006866897984511323\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11752778026792739\n",
      "Average test loss: 0.006084386617773109\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11356459846099218\n",
      "Average test loss: 0.006302789383464389\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1101344484421942\n",
      "Average test loss: 0.006137803860008717\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10740519078572591\n",
      "Average test loss: 0.006033304070846902\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10472810426685546\n",
      "Average test loss: 0.006061667105803887\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10252615180942748\n",
      "Average test loss: 0.006218513132797347\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10062935231791602\n",
      "Average test loss: 0.006010358727640576\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09902256860335668\n",
      "Average test loss: 0.006231587824722131\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09730409818225437\n",
      "Average test loss: 0.006105821562723981\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0959995446536276\n",
      "Average test loss: 0.007099956550531917\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09476453053951263\n",
      "Average test loss: 0.006014124106615782\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09344106551011404\n",
      "Average test loss: 0.006072536932511462\n",
      "Epoch 87/300\n",
      "Average training loss: 0.092432678076956\n",
      "Average test loss: 0.006073441594011254\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09110521638393403\n",
      "Average test loss: 0.008418160163280036\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09021940945916705\n",
      "Average test loss: 0.0061715357700983684\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08899499558077918\n",
      "Average test loss: 0.007347244568996959\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08798252377245161\n",
      "Average test loss: 0.006414660796109173\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08698839961820179\n",
      "Average test loss: 0.006204769455310372\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08653244237105051\n",
      "Average test loss: 0.006116407235049539\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08627916174464756\n",
      "Average test loss: 0.006247838817536831\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08545013276735941\n",
      "Average test loss: 0.0061819430527587736\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08481730641921362\n",
      "Average test loss: 0.00619412927950422\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08398109940025542\n",
      "Average test loss: 0.007955203022807836\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0834265586203999\n",
      "Average test loss: 0.0061395540601677364\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0831291792260276\n",
      "Average test loss: 0.006149862832079331\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08270730254716344\n",
      "Average test loss: 0.006300855251650015\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08285690803660287\n",
      "Average test loss: 0.006127797732336654\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08200802485148112\n",
      "Average test loss: 0.006345937800904115\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08133081079853906\n",
      "Average test loss: 0.0064109521653089255\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08102296003368166\n",
      "Average test loss: 0.013226117458608416\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08074065954817666\n",
      "Average test loss: 0.0067422893146673836\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0804106024371253\n",
      "Average test loss: 0.006157273091789749\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08029188244872623\n",
      "Average test loss: 0.006217071282780833\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07962246488531431\n",
      "Average test loss: 0.006496653099026945\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07925497394800186\n",
      "Average test loss: 0.0062742578238248825\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0791547368367513\n",
      "Average test loss: 0.00621339577767584\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07909510131676992\n",
      "Average test loss: 0.007564461064007547\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07869406418005626\n",
      "Average test loss: 0.006488386943108506\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07817177187071907\n",
      "Average test loss: 0.006223372098058462\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0782159107791053\n",
      "Average test loss: 0.006302809132470025\n",
      "Epoch 115/300\n",
      "Average training loss: 0.08871219515138203\n",
      "Average test loss: 0.006253433568610085\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07811024486025174\n",
      "Average test loss: 0.0062948189733756914\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07713618677192265\n",
      "Average test loss: 0.00618746313576897\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07682647843493355\n",
      "Average test loss: 0.00636431913698713\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0768055942191018\n",
      "Average test loss: 0.008640301320701838\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07668542122178608\n",
      "Average test loss: 0.00636843148411976\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07662887785832087\n",
      "Average test loss: 0.006657500469436248\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0764122449292077\n",
      "Average test loss: 0.006212975524365902\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0763305852479405\n",
      "Average test loss: 0.007054502504567305\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0760081629090839\n",
      "Average test loss: 0.008728288629816639\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07584218529860179\n",
      "Average test loss: 0.006486174106597901\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07569625066386329\n",
      "Average test loss: 0.007036581066333585\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07547040392292871\n",
      "Average test loss: 0.006321632078952259\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07518813031249576\n",
      "Average test loss: 0.019935618003623354\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07514409578839938\n",
      "Average test loss: 0.0066121845274335804\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07555154593785604\n",
      "Average test loss: 0.011859621122479439\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0748084340956476\n",
      "Average test loss: 0.0068080905009475015\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0742460339334276\n",
      "Average test loss: 0.006769558791278137\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07506343495845795\n",
      "Average test loss: 0.006514029471410645\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07402924648920695\n",
      "Average test loss: 0.0064040664159175425\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07367874975005785\n",
      "Average test loss: 0.007498904393364985\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07380342601074112\n",
      "Average test loss: 0.007992740731272433\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07371855669220288\n",
      "Average test loss: 0.006401390529341168\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07339667994446225\n",
      "Average test loss: 0.0066636640764772895\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07321697235769696\n",
      "Average test loss: 0.006397650181419319\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07327099308040406\n",
      "Average test loss: 0.006372055824018187\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07311597408850988\n",
      "Average test loss: 0.006376550508042177\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07267225817177031\n",
      "Average test loss: 0.00672770914932092\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07265131388770209\n",
      "Average test loss: 0.006351694799545738\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07262425595853064\n",
      "Average test loss: 0.006718606375985675\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07284440811475118\n",
      "Average test loss: 0.006824109902812375\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07337663067711724\n",
      "Average test loss: 0.006371752839535475\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07239245740572611\n",
      "Average test loss: 0.00633921033806271\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0719039202398724\n",
      "Average test loss: 0.006428293258779578\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07163462008701431\n",
      "Average test loss: 0.007329567005236943\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07181494284338422\n",
      "Average test loss: 0.006516283107714521\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07142585538493262\n",
      "Average test loss: 0.006349908807211452\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0717519378066063\n",
      "Average test loss: 0.0065244634590215156\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07139383123649491\n",
      "Average test loss: 0.007351412641919321\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07111337630616293\n",
      "Average test loss: 0.006447482965058751\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07115271931555536\n",
      "Average test loss: 0.006354250270459387\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07118631771537992\n",
      "Average test loss: 0.007420984910180171\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07090453402201334\n",
      "Average test loss: 0.006526875035216411\n",
      "Epoch 158/300\n",
      "Average training loss: 0.070569129884243\n",
      "Average test loss: 0.008466955345537928\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07068767042292488\n",
      "Average test loss: 0.006827482262833251\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07106958910491731\n",
      "Average test loss: 0.006631269223160214\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07001152445872624\n",
      "Average test loss: 0.006524673007428646\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07025795357757145\n",
      "Average test loss: 0.007426671860118707\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07071038210391999\n",
      "Average test loss: 0.006561107457511955\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06992994334962632\n",
      "Average test loss: 0.006663374190943108\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07004274928569794\n",
      "Average test loss: 0.006647321921255853\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06980386433998743\n",
      "Average test loss: 0.006577626456816991\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06987129404147466\n",
      "Average test loss: 0.006624244661380847\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06975636027256647\n",
      "Average test loss: 0.006616563639293115\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07126609919468561\n",
      "Average test loss: 0.006635189614362187\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06891152422295677\n",
      "Average test loss: 0.006685632700307502\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06946538866228527\n",
      "Average test loss: 0.006736680179006523\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06921443039178848\n",
      "Average test loss: 0.006735845972680383\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06905813513861762\n",
      "Average test loss: 0.006741790334590607\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06923831021454599\n",
      "Average test loss: 0.007135134030961328\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06899986021386252\n",
      "Average test loss: 0.009269347362220287\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06901148229837417\n",
      "Average test loss: 0.006635119514332878\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06885977505975299\n",
      "Average test loss: 0.008156082299020555\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06890638370977507\n",
      "Average test loss: 0.006790578388505512\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06897583621078067\n",
      "Average test loss: 0.006519692942914036\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06851992863416671\n",
      "Average test loss: 0.013849129991398918\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0684931466182073\n",
      "Average test loss: 0.006530106191005971\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06831972158617443\n",
      "Average test loss: 0.006590895272377465\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06805353108379576\n",
      "Average test loss: 0.006960521625975768\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06821888252430491\n",
      "Average test loss: 0.007716671645641327\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06840550582607587\n",
      "Average test loss: 0.0066746976354883775\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06884493081437217\n",
      "Average test loss: 0.0065497456019123395\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06769263860252168\n",
      "Average test loss: 0.00671028484735224\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06781383507781559\n",
      "Average test loss: 0.006773947316325373\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06825827427705129\n",
      "Average test loss: 0.006495445278783639\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06766448600755798\n",
      "Average test loss: 0.006637717458936903\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0676591625743442\n",
      "Average test loss: 0.006673690317405594\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06752905903922186\n",
      "Average test loss: 0.006770437930193212\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06770224119557275\n",
      "Average test loss: 0.006562799746791522\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06737622897492515\n",
      "Average test loss: 0.006912940480228927\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06755696401993434\n",
      "Average test loss: 0.006547634527501133\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0672881037261751\n",
      "Average test loss: 0.006613849229282803\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06712088349130418\n",
      "Average test loss: 0.012914393196503322\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06729168777333365\n",
      "Average test loss: 0.006735665832542711\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06714786101712121\n",
      "Average test loss: 0.016779085747069784\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0668720341126124\n",
      "Average test loss: 0.006533373485008875\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06703857278161579\n",
      "Average test loss: 0.03186022456155883\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0670181801782714\n",
      "Average test loss: 0.006640923418104649\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0669376738137669\n",
      "Average test loss: 0.006679827633003394\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06670675551229054\n",
      "Average test loss: 0.007017732174860107\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06674393625722991\n",
      "Average test loss: 0.007126033141381211\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06649277316861682\n",
      "Average test loss: 0.0064903622099922766\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06677378377318383\n",
      "Average test loss: 0.007882032739205493\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06639706041084395\n",
      "Average test loss: 0.008276540146105819\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06644254349006547\n",
      "Average test loss: 0.006951467129505343\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06643456041812897\n",
      "Average test loss: 0.006741699827214082\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0664987293349372\n",
      "Average test loss: 0.00741762344001068\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06666219478845596\n",
      "Average test loss: 0.006705458916723728\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06596797440449396\n",
      "Average test loss: 0.0067074204004473155\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06610761565632291\n",
      "Average test loss: 0.006743304392529859\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06592608111103376\n",
      "Average test loss: 0.006814746895598041\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06634652847713894\n",
      "Average test loss: 0.006429918861223591\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06586841756105423\n",
      "Average test loss: 0.006646152345670594\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06599522002206909\n",
      "Average test loss: 0.006733919174720843\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06588799192508062\n",
      "Average test loss: 0.006804443970322609\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06594794201188617\n",
      "Average test loss: 0.006579610614726941\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06577688327762815\n",
      "Average test loss: 0.006995167439182599\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06568647566106585\n",
      "Average test loss: 0.008068652791695462\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06548073878553179\n",
      "Average test loss: 0.006699056324859461\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06542952933576372\n",
      "Average test loss: 0.006659555402480893\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06568616344531378\n",
      "Average test loss: 0.006641311042838626\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06669006400638157\n",
      "Average test loss: 0.006620858934190539\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0653849085436927\n",
      "Average test loss: 0.006755757878224055\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06497952314217885\n",
      "Average test loss: 0.006908669577704535\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06521119580335087\n",
      "Average test loss: 0.006939740834136804\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06552595687574811\n",
      "Average test loss: 0.0065291876292063135\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06494952460461192\n",
      "Average test loss: 0.006898306531624662\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06491224535968569\n",
      "Average test loss: 0.006838109662135442\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06521951290633943\n",
      "Average test loss: 0.006812684338125917\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06466888055867619\n",
      "Average test loss: 0.006996274796624978\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06499208975169393\n",
      "Average test loss: 0.006819452713761065\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06507815541492568\n",
      "Average test loss: 0.00661156801879406\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06481847783592012\n",
      "Average test loss: 0.010750075689206521\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06472544588976437\n",
      "Average test loss: 0.006664686484469308\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06485910198754734\n",
      "Average test loss: 0.028687803814808528\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06464781911505593\n",
      "Average test loss: 0.006650777725295888\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06470713663101196\n",
      "Average test loss: 0.006759127250562111\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06457209388746156\n",
      "Average test loss: 0.006706773339460293\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06473037187920676\n",
      "Average test loss: 0.00699000614715947\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06462557566165925\n",
      "Average test loss: 0.007721040106895897\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06449096827374565\n",
      "Average test loss: 0.006907671211908261\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06438506019115448\n",
      "Average test loss: 0.006881873783138063\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06431565137041939\n",
      "Average test loss: 0.006943979606032371\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06445500037405226\n",
      "Average test loss: 0.006949049257983764\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06464374823040432\n",
      "Average test loss: 0.010134791175524394\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06444382548994489\n",
      "Average test loss: 0.006729013211197323\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06382117633687125\n",
      "Average test loss: 0.008809562457932366\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06430391995112102\n",
      "Average test loss: 0.006749500212156111\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06412031035953097\n",
      "Average test loss: 0.006761952624966701\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06400864838560422\n",
      "Average test loss: 0.00690281946460406\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06391703655984667\n",
      "Average test loss: 0.006895149919307894\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0639611074063513\n",
      "Average test loss: 0.007982615209288068\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06422655729783906\n",
      "Average test loss: 0.007537555673884021\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06482265377044678\n",
      "Average test loss: 0.006929956576062574\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0634955003592703\n",
      "Average test loss: 0.006754961816800965\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06390081097020044\n",
      "Average test loss: 0.006989457322905461\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06358342766099506\n",
      "Average test loss: 0.0067467173151671885\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06396243164936702\n",
      "Average test loss: 0.008169619212134016\n",
      "Epoch 263/300\n",
      "Average training loss: 0.063678965833452\n",
      "Average test loss: 0.006965493382265171\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06356190578142802\n",
      "Average test loss: 0.00710464185062382\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06337825229432847\n",
      "Average test loss: 0.006827599681913853\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0634946182039049\n",
      "Average test loss: 0.006599981551782952\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06434868546326955\n",
      "Average test loss: 0.006877065603103903\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06327631775869264\n",
      "Average test loss: 0.00718208487249083\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06338820886611939\n",
      "Average test loss: 0.007801984868115849\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06319860042466058\n",
      "Average test loss: 0.010226335326002704\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06353116354677413\n",
      "Average test loss: 0.0067661907797058425\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06353922884994083\n",
      "Average test loss: 0.008504925002654394\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06322092977497312\n",
      "Average test loss: 0.006800275770740377\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06309207436111239\n",
      "Average test loss: 0.006769146498292684\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06293958075841267\n",
      "Average test loss: 0.006943735991501146\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06329349231388834\n",
      "Average test loss: 0.006881716955867079\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0629602876537376\n",
      "Average test loss: 0.00671228355417649\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06314583136638005\n",
      "Average test loss: 0.0066707303933799264\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0631224272052447\n",
      "Average test loss: 0.007085558909922838\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06298175018363529\n",
      "Average test loss: 0.006679287575185299\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06273712533050113\n",
      "Average test loss: 0.007491551294094986\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06280668392446306\n",
      "Average test loss: 0.007141020218117369\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06333549449510044\n",
      "Average test loss: 0.006857879572444492\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06285791844460699\n",
      "Average test loss: 0.013291298301476571\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06267165584696664\n",
      "Average test loss: 0.00867392348166969\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06283023725615608\n",
      "Average test loss: 0.006874517619195912\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06255292364954948\n",
      "Average test loss: 0.0067240716901918254\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06260937504967054\n",
      "Average test loss: 0.0068183548094497785\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06273133509026634\n",
      "Average test loss: 0.007306244825323422\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0638892451259825\n",
      "Average test loss: 0.006712303417424361\n",
      "Epoch 291/300\n",
      "Average training loss: 0.062205509662628174\n",
      "Average test loss: 0.009822555787033505\n",
      "Epoch 292/300\n",
      "Average training loss: 0.062427716149224176\n",
      "Average test loss: 0.006961650272210439\n",
      "Epoch 293/300\n",
      "Average training loss: 0.062302644812398486\n",
      "Average test loss: 0.006901167577339543\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06233652284410265\n",
      "Average test loss: 0.0068405634346935485\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06260933507813347\n",
      "Average test loss: 0.006817701631122166\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06258483516176541\n",
      "Average test loss: 0.006946819192005529\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0622943207985825\n",
      "Average test loss: 0.007305362104541725\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0622564937406116\n",
      "Average test loss: 0.007144042673210303\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06250391590595246\n",
      "Average test loss: 0.0075176862378915155\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06298051669200261\n",
      "Average test loss: 0.007036318253311846\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.8778922802607219\n",
      "Average test loss: 0.00891051093240579\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5985907217131721\n",
      "Average test loss: 0.006776822036339177\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3915940593613519\n",
      "Average test loss: 0.006736138452672296\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2923339104652405\n",
      "Average test loss: 0.006147198307845328\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2314107080830468\n",
      "Average test loss: 0.00652317364845011\n",
      "Epoch 6/300\n",
      "Average training loss: 0.19413932481076981\n",
      "Average test loss: 0.005830036833054489\n",
      "Epoch 7/300\n",
      "Average training loss: 0.16931214849154155\n",
      "Average test loss: 0.005524571844273143\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15165738256110087\n",
      "Average test loss: 0.005065488689475589\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1392551088200675\n",
      "Average test loss: 0.005877177926401298\n",
      "Epoch 10/300\n",
      "Average training loss: 0.13007449913024902\n",
      "Average test loss: 0.0062614461419483024\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12229227426979276\n",
      "Average test loss: 0.005271390512171719\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11614920796288385\n",
      "Average test loss: 0.034556236313449014\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11071040731668472\n",
      "Average test loss: 0.005280771212031444\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10702731368276808\n",
      "Average test loss: 0.004442389036632246\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10261698894368278\n",
      "Average test loss: 0.005489744426475631\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09956384836302863\n",
      "Average test loss: 0.0053488612228797545\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09544428488281038\n",
      "Average test loss: 0.004244886898746093\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09269398221704694\n",
      "Average test loss: 0.003936845233043035\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09029438117808766\n",
      "Average test loss: 0.003970248924982217\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08717020237445831\n",
      "Average test loss: 0.004206147374999192\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08436289407147302\n",
      "Average test loss: 0.003975726791554027\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08169431031412548\n",
      "Average test loss: 0.005624088661952151\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07948407352632947\n",
      "Average test loss: 0.0037397366186810864\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07766660698917177\n",
      "Average test loss: 0.003801115413920747\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0751062440276146\n",
      "Average test loss: 0.00381290997688969\n",
      "Epoch 26/300\n",
      "Average training loss: 0.073425622648663\n",
      "Average test loss: 0.003748847588689791\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07177277092138926\n",
      "Average test loss: 0.003591940915212035\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07031434822744793\n",
      "Average test loss: 0.003856839820742607\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07122643199563027\n",
      "Average test loss: 0.003624428696309527\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06801819253961246\n",
      "Average test loss: 0.0035826105434033605\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06697724816534255\n",
      "Average test loss: 0.0035664764332274594\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06607353016402986\n",
      "Average test loss: 0.003632158578890893\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06535240050156911\n",
      "Average test loss: 0.0036820152236355674\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06432167447937859\n",
      "Average test loss: 0.003618273360447751\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06340387279788653\n",
      "Average test loss: 0.0035414275657385586\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06278450263871087\n",
      "Average test loss: 0.004154630635761552\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06219606871406237\n",
      "Average test loss: 0.003510980703557531\n",
      "Epoch 38/300\n",
      "Average training loss: 0.061347590042485134\n",
      "Average test loss: 0.003978265259311431\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06128016096684668\n",
      "Average test loss: 0.0035771256807363696\n",
      "Epoch 40/300\n",
      "Average training loss: 0.060534470352861615\n",
      "Average test loss: 0.0036971036384089127\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0597533731924163\n",
      "Average test loss: 0.003508480467730098\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06701304379436705\n",
      "Average test loss: 0.004064259732349051\n",
      "Epoch 43/300\n",
      "Average training loss: 0.3051552485062016\n",
      "Average test loss: 0.00424008491428362\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1302139539519946\n",
      "Average test loss: 0.0037183330787552726\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08967983943886228\n",
      "Average test loss: 0.0037293829226659404\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08090502107143402\n",
      "Average test loss: 0.003534407458992468\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07675527189837562\n",
      "Average test loss: 0.003604134898011883\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0738011848827203\n",
      "Average test loss: 0.003572659106511209\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0713906327188015\n",
      "Average test loss: 0.003574837894489368\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0694813844660918\n",
      "Average test loss: 0.003602184451702568\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06797717850075828\n",
      "Average test loss: 0.0035926249894416996\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06672233837180667\n",
      "Average test loss: 0.0038676288622534936\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06558796635270119\n",
      "Average test loss: 0.0034801021926105023\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0645925130546093\n",
      "Average test loss: 0.0034674353659566908\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06398679103453954\n",
      "Average test loss: 0.003489386650009288\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06295059920019573\n",
      "Average test loss: 0.0034932064165671666\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06244480826788478\n",
      "Average test loss: 0.0035223230371872584\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06198956220679813\n",
      "Average test loss: 761.1494953342013\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06163835205634435\n",
      "Average test loss: 0.0035206917961024577\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06082180137435595\n",
      "Average test loss: 0.003418923356052902\n",
      "Epoch 61/300\n",
      "Average training loss: 0.060376138935486476\n",
      "Average test loss: 0.0037482589388059245\n",
      "Epoch 62/300\n",
      "Average training loss: 0.060123353540897366\n",
      "Average test loss: 0.0034085858093781604\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05946509790089395\n",
      "Average test loss: 0.003427182658471995\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05899866298503346\n",
      "Average test loss: 0.005518722345017725\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05882986338271035\n",
      "Average test loss: 0.0034820807977683013\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05828489412864049\n",
      "Average test loss: 0.003427690932320224\n",
      "Epoch 67/300\n",
      "Average training loss: 0.057888289716508654\n",
      "Average test loss: 0.003484593570853273\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05792340941230456\n",
      "Average test loss: 0.003797664126381278\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10375459339221318\n",
      "Average test loss: 0.0036744769865439996\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06824871684114138\n",
      "Average test loss: 0.003497781802382734\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06437270782060094\n",
      "Average test loss: 0.003452884359492196\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06229053149289555\n",
      "Average test loss: 0.0034154934444361263\n",
      "Epoch 73/300\n",
      "Average training loss: 0.060798130394683944\n",
      "Average test loss: 0.003568173228452603\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06007881541384591\n",
      "Average test loss: 0.003420262507473429\n",
      "Epoch 75/300\n",
      "Average training loss: 0.059033033635881214\n",
      "Average test loss: 0.004247674648960431\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05830996569825543\n",
      "Average test loss: 0.0036862462311983107\n",
      "Epoch 77/300\n",
      "Average training loss: 0.057760235961940555\n",
      "Average test loss: 0.0034436204180949264\n",
      "Epoch 78/300\n",
      "Average training loss: 0.057527933597564695\n",
      "Average test loss: 0.0034306967573033437\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05698823854989476\n",
      "Average test loss: 0.0034808517957313195\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05655467047625118\n",
      "Average test loss: 0.0034601691563924154\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05637503071460459\n",
      "Average test loss: 0.004676789911670818\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05637496164441109\n",
      "Average test loss: 0.003540726991991202\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05567846528026793\n",
      "Average test loss: 0.004044556977434291\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05534769043657515\n",
      "Average test loss: 0.0034842875577095482\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05497232537799411\n",
      "Average test loss: 0.0034024560718486706\n",
      "Epoch 86/300\n",
      "Average training loss: 0.054645490338404976\n",
      "Average test loss: 0.0035604924108419153\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05442876822915342\n",
      "Average test loss: 0.0035396826155483724\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05397710251145893\n",
      "Average test loss: 0.003451816556354364\n",
      "Epoch 89/300\n",
      "Average training loss: 0.053977326217624876\n",
      "Average test loss: 0.004064679297722048\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05352316244939963\n",
      "Average test loss: 0.0035194648700869746\n",
      "Epoch 91/300\n",
      "Average training loss: 0.053589832660224705\n",
      "Average test loss: 0.003439161101149188\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05437864700290892\n",
      "Average test loss: 0.003696515114977956\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05333492325080766\n",
      "Average test loss: 0.0034202559501346616\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05224919693668684\n",
      "Average test loss: 0.0034815391689125034\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05217795073654916\n",
      "Average test loss: 0.003396806222283178\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05234836834669113\n",
      "Average test loss: 0.0035517237722459765\n",
      "Epoch 97/300\n",
      "Average training loss: 0.051730070842636956\n",
      "Average test loss: 0.003611402833213409\n",
      "Epoch 98/300\n",
      "Average training loss: 0.051407323125335906\n",
      "Average test loss: 0.0035356485715342893\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05133930878175629\n",
      "Average test loss: 0.0035850491122239167\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05106778566704856\n",
      "Average test loss: 0.003490191932146748\n",
      "Epoch 101/300\n",
      "Average training loss: 0.051089031865199405\n",
      "Average test loss: 0.003799428804881043\n",
      "Epoch 102/300\n",
      "Average training loss: 0.054884334607256786\n",
      "Average test loss: 0.003578524135053158\n",
      "Epoch 103/300\n",
      "Average training loss: 0.051232973794142406\n",
      "Average test loss: 0.003596600625043114\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05040412181946966\n",
      "Average test loss: 0.00354704919602308\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04991443777746624\n",
      "Average test loss: 0.0035737120180080336\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05004184271560775\n",
      "Average test loss: 0.004137280696589086\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04981242565976249\n",
      "Average test loss: 0.0035591591973271634\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04962969179617034\n",
      "Average test loss: 0.0045732608723143735\n",
      "Epoch 109/300\n",
      "Average training loss: 0.049387724806865055\n",
      "Average test loss: 0.003605500888493326\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04923821394311057\n",
      "Average test loss: 0.003945274668021335\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04931434577041202\n",
      "Average test loss: 0.0036690724599692556\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0492334038582113\n",
      "Average test loss: 0.004283260124839015\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04874477469589975\n",
      "Average test loss: 0.00960891417786479\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04871790753470527\n",
      "Average test loss: 0.004447712248398198\n",
      "Epoch 115/300\n",
      "Average training loss: 0.048319426986906264\n",
      "Average test loss: 0.003541922481109699\n",
      "Epoch 116/300\n",
      "Average training loss: 0.048261758075820074\n",
      "Average test loss: 0.017644465838869413\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04820500079790751\n",
      "Average test loss: 0.0039657983844065\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04798186284965939\n",
      "Average test loss: 0.004535176703395943\n",
      "Epoch 119/300\n",
      "Average training loss: 0.047818045208851495\n",
      "Average test loss: 0.003550769638684061\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04775873589184549\n",
      "Average test loss: 0.0067593733113673\n",
      "Epoch 121/300\n",
      "Average training loss: 0.047425919049315986\n",
      "Average test loss: 0.0039854885670873854\n",
      "Epoch 122/300\n",
      "Average training loss: 0.047827120307419035\n",
      "Average test loss: 0.0036523438725206588\n",
      "Epoch 123/300\n",
      "Average training loss: 0.047528704096873604\n",
      "Average test loss: 0.0035839153418524396\n",
      "Epoch 124/300\n",
      "Average training loss: 0.047521693905194604\n",
      "Average test loss: 0.0036406493774718707\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04690811187028885\n",
      "Average test loss: 0.0037733725263840623\n",
      "Epoch 126/300\n",
      "Average training loss: 0.046842188086774615\n",
      "Average test loss: 0.003674397177787291\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0467395893914832\n",
      "Average test loss: 0.0037193361611829864\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04660533410973019\n",
      "Average test loss: 0.003572751052470671\n",
      "Epoch 129/300\n",
      "Average training loss: 0.046689205353458725\n",
      "Average test loss: 0.0037914433947039976\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04643438749512037\n",
      "Average test loss: 0.0035873922809130615\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04654693528347545\n",
      "Average test loss: 0.0035820148773491383\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04612963735063871\n",
      "Average test loss: 0.004021238774061203\n",
      "Epoch 133/300\n",
      "Average training loss: 0.046369267579582\n",
      "Average test loss: 0.005417290141185124\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04585502114560869\n",
      "Average test loss: 0.004150707095240553\n",
      "Epoch 135/300\n",
      "Average training loss: 0.045851066917181015\n",
      "Average test loss: 0.0036011412361015875\n",
      "Epoch 136/300\n",
      "Average training loss: 0.045612588193681504\n",
      "Average test loss: 0.004079055394977331\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04584442416826884\n",
      "Average test loss: 0.0036321204871767095\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0456875938044654\n",
      "Average test loss: 0.003733955915603373\n",
      "Epoch 139/300\n",
      "Average training loss: 0.045274873346090316\n",
      "Average test loss: 0.003714056107526024\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04544375865658124\n",
      "Average test loss: 0.006233626627259784\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04585213873452611\n",
      "Average test loss: 0.003639638582865397\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04534793357716666\n",
      "Average test loss: 0.0040163356593499585\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04502203995154964\n",
      "Average test loss: 0.00410303819200231\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04505887274609672\n",
      "Average test loss: 0.00516739135608077\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04499122849106789\n",
      "Average test loss: 0.004027508656804761\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04510852926969528\n",
      "Average test loss: 0.003628026159687175\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04490685037440724\n",
      "Average test loss: 0.003728295917312304\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04450620692306095\n",
      "Average test loss: 0.0039263533569044535\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04474207045634587\n",
      "Average test loss: 0.0037381090447306635\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04464825368920962\n",
      "Average test loss: 0.003898517425275511\n",
      "Epoch 151/300\n",
      "Average training loss: 0.044527692566315336\n",
      "Average test loss: 0.004130138670404752\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04443223193287849\n",
      "Average test loss: 0.004013212966836161\n",
      "Epoch 153/300\n",
      "Average training loss: 0.044319594032234615\n",
      "Average test loss: 0.004301141824780239\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04412421981824769\n",
      "Average test loss: 0.003665088081939353\n",
      "Epoch 155/300\n",
      "Average training loss: 0.044308176812198424\n",
      "Average test loss: 0.004823100788725747\n",
      "Epoch 156/300\n",
      "Average training loss: 0.044154006156656475\n",
      "Average test loss: 0.0038199156456523472\n",
      "Epoch 157/300\n",
      "Average training loss: 0.043984709905253515\n",
      "Average test loss: 0.0038458539731800554\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04400744870636198\n",
      "Average test loss: 0.004044708364539676\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04386109501123428\n",
      "Average test loss: 0.0037761795131696597\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04382777662740813\n",
      "Average test loss: 0.0036189441145915123\n",
      "Epoch 161/300\n",
      "Average training loss: 0.043836351146300634\n",
      "Average test loss: 0.0037311974832167226\n",
      "Epoch 162/300\n",
      "Average training loss: 0.043995211982064776\n",
      "Average test loss: 0.0037813260013030633\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04378854877418942\n",
      "Average test loss: 0.003893215608679586\n",
      "Epoch 164/300\n",
      "Average training loss: 0.043545162651273936\n",
      "Average test loss: 0.003725854049126307\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04340804621577263\n",
      "Average test loss: 0.004003233030852344\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04351940865152412\n",
      "Average test loss: 0.003923804765256743\n",
      "Epoch 167/300\n",
      "Average training loss: 0.043323006560405095\n",
      "Average test loss: 0.0055239562868244114\n",
      "Epoch 168/300\n",
      "Average training loss: 0.043262720339828066\n",
      "Average test loss: 0.004094141019715203\n",
      "Epoch 169/300\n",
      "Average training loss: 0.043368057426479124\n",
      "Average test loss: 0.0037977557974971004\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0433844420115153\n",
      "Average test loss: 0.00560620029312041\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04328232150276502\n",
      "Average test loss: 0.02632147967484262\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04316002461645338\n",
      "Average test loss: 0.004188445931093561\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0428016817967097\n",
      "Average test loss: 0.00381934958572189\n",
      "Epoch 174/300\n",
      "Average training loss: 0.043100701570510866\n",
      "Average test loss: 0.0037370472310317886\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04301692356003655\n",
      "Average test loss: 0.003870567923204766\n",
      "Epoch 176/300\n",
      "Average training loss: 0.042984254442983205\n",
      "Average test loss: 0.003658094287953443\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04283015975687239\n",
      "Average test loss: 0.0037854225707964766\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04268574754728211\n",
      "Average test loss: 0.003850237119735943\n",
      "Epoch 179/300\n",
      "Average training loss: 0.042699348257647624\n",
      "Average test loss: 0.005768593377123276\n",
      "Epoch 180/300\n",
      "Average training loss: 0.042669288582272\n",
      "Average test loss: 0.0037890961478567785\n",
      "Epoch 181/300\n",
      "Average training loss: 0.042779447578721576\n",
      "Average test loss: 0.08722359267870586\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04274192887710201\n",
      "Average test loss: 0.003870721235871315\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04253870839542813\n",
      "Average test loss: 0.003989639642751879\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04249264949891302\n",
      "Average test loss: 0.004544301657627026\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04247080728411674\n",
      "Average test loss: 0.0044710388171176115\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04233324663837751\n",
      "Average test loss: 0.0037137501326700052\n",
      "Epoch 187/300\n",
      "Average training loss: 0.042391774157683054\n",
      "Average test loss: 0.003912134914969404\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04239747606052293\n",
      "Average test loss: 0.0038491787806981138\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04223583149909973\n",
      "Average test loss: 0.004442725585773587\n",
      "Epoch 190/300\n",
      "Average training loss: 0.042179526540968154\n",
      "Average test loss: 0.003727503464039829\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04208228781984912\n",
      "Average test loss: 0.0044627838728742465\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04214415306184027\n",
      "Average test loss: 0.004007202819403675\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04225544113914172\n",
      "Average test loss: 0.003792727463775211\n",
      "Epoch 194/300\n",
      "Average training loss: 0.042039566314882704\n",
      "Average test loss: 0.0037630579065945414\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04207480571667353\n",
      "Average test loss: 0.003791233958883418\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04186404303709666\n",
      "Average test loss: 0.00387392924601833\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04193092358774609\n",
      "Average test loss: 0.003943112169495887\n",
      "Epoch 198/300\n",
      "Average training loss: 0.041835737009843194\n",
      "Average test loss: 0.003819410839428504\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04185654645495945\n",
      "Average test loss: 0.007075179498642683\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0417476569712162\n",
      "Average test loss: 0.004238136919836203\n",
      "Epoch 201/300\n",
      "Average training loss: 0.041645156224568686\n",
      "Average test loss: 0.005066528051677678\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04166708060436779\n",
      "Average test loss: 0.003697241907732354\n",
      "Epoch 203/300\n",
      "Average training loss: 0.041957518113984\n",
      "Average test loss: 0.0038291148640629317\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04177511780791812\n",
      "Average test loss: 0.004551232297387388\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04161210244894028\n",
      "Average test loss: 0.0037261801217165257\n",
      "Epoch 206/300\n",
      "Average training loss: 0.041389808734258016\n",
      "Average test loss: 0.0038713999018073083\n",
      "Epoch 207/300\n",
      "Average training loss: 0.041481035033861795\n",
      "Average test loss: 0.00392897259319822\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04156208722624514\n",
      "Average test loss: 0.00406728968086342\n",
      "Epoch 209/300\n",
      "Average training loss: 0.041320605085955725\n",
      "Average test loss: 0.005189608397583167\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04138435380988651\n",
      "Average test loss: 0.004536753678487407\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04138993368877305\n",
      "Average test loss: 0.003915821429342031\n",
      "Epoch 212/300\n",
      "Average training loss: 0.041323332094483904\n",
      "Average test loss: 0.004697752765276366\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04118456998467445\n",
      "Average test loss: 0.0039079582509067325\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04128869567645921\n",
      "Average test loss: 0.003943244976715909\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04178000346157286\n",
      "Average test loss: 0.004377480689022276\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0410349012778865\n",
      "Average test loss: 0.003994016356559263\n",
      "Epoch 217/300\n",
      "Average training loss: 0.041162849869992996\n",
      "Average test loss: 0.0038045086993111505\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04110905598600705\n",
      "Average test loss: 0.0045798211275703375\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04102137904365857\n",
      "Average test loss: 0.0038761445263193715\n",
      "Epoch 220/300\n",
      "Average training loss: 0.041031671568751336\n",
      "Average test loss: 0.004131217272745238\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0410004174609979\n",
      "Average test loss: 0.0038871127998249396\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04113035090764364\n",
      "Average test loss: 0.0038940359505100384\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04095274102522267\n",
      "Average test loss: 0.004402948797163036\n",
      "Epoch 224/300\n",
      "Average training loss: 0.041031586478153866\n",
      "Average test loss: 0.003891116344059507\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04089433587425285\n",
      "Average test loss: 0.00388059100550082\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04068163302871916\n",
      "Average test loss: 0.014861451444940435\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04086105222834481\n",
      "Average test loss: 0.0040548361937205\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0408742902295457\n",
      "Average test loss: 0.003970777874605523\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04070691800117493\n",
      "Average test loss: 0.003877894030469987\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04073292756080627\n",
      "Average test loss: 0.0039563901287813985\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04059042292171054\n",
      "Average test loss: 0.0038488222261269887\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0407632228417529\n",
      "Average test loss: 0.0039480710308998825\n",
      "Epoch 233/300\n",
      "Average training loss: 0.040659562882449894\n",
      "Average test loss: 0.0038035241603437396\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04052443655663066\n",
      "Average test loss: 0.003935771521387829\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04069649928477075\n",
      "Average test loss: 0.004196680191076464\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04056315740942955\n",
      "Average test loss: 0.0038709298194282583\n",
      "Epoch 237/300\n",
      "Average training loss: 0.040342286480797664\n",
      "Average test loss: 0.00392339908455809\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04055451300077968\n",
      "Average test loss: 0.005356931957519717\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04053451502323151\n",
      "Average test loss: 0.003908816079091695\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04048792275124126\n",
      "Average test loss: 0.004044465260994104\n",
      "Epoch 241/300\n",
      "Average training loss: 0.040406258263521726\n",
      "Average test loss: 0.0038586142065210475\n",
      "Epoch 242/300\n",
      "Average training loss: 0.040387657350964015\n",
      "Average test loss: 0.004234801213774416\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0403583663387431\n",
      "Average test loss: 0.004318704086459345\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04041962714658843\n",
      "Average test loss: 0.005747886180463764\n",
      "Epoch 245/300\n",
      "Average training loss: 0.040142920308642915\n",
      "Average test loss: 0.003957856832899981\n",
      "Epoch 246/300\n",
      "Average training loss: 0.040329028808408315\n",
      "Average test loss: 0.003960530873801973\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04015801852279239\n",
      "Average test loss: 0.003845396605423755\n",
      "Epoch 248/300\n",
      "Average training loss: 0.040426803512705695\n",
      "Average test loss: 0.004394716406861941\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04006908967759874\n",
      "Average test loss: 0.003788388960270418\n",
      "Epoch 250/300\n",
      "Average training loss: 0.039987386100822024\n",
      "Average test loss: 0.0076365962194071876\n",
      "Epoch 251/300\n",
      "Average training loss: 0.040154705431726244\n",
      "Average test loss: 0.0041559959583812285\n",
      "Epoch 252/300\n",
      "Average training loss: 0.040237986713647846\n",
      "Average test loss: 0.005850637152377102\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04000981503393915\n",
      "Average test loss: 0.0038963790769792265\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03985566699835989\n",
      "Average test loss: 0.003914038011597262\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04012769530216853\n",
      "Average test loss: 0.003957915655440754\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04033997471796142\n",
      "Average test loss: 0.00425770634609378\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03996553745865822\n",
      "Average test loss: 0.004016932545022832\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03983593952324655\n",
      "Average test loss: 0.004955199908465147\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04007243377632565\n",
      "Average test loss: 0.003936912453836865\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03987591523594326\n",
      "Average test loss: 0.004011288688828548\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04004801282286644\n",
      "Average test loss: 0.003897406887056099\n",
      "Epoch 262/300\n",
      "Average training loss: 0.039836759313941005\n",
      "Average test loss: 0.004040923135769036\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03967344539695316\n",
      "Average test loss: 0.003967468505518304\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03994051628477044\n",
      "Average test loss: 0.003976520682788557\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03971570250723097\n",
      "Average test loss: 0.004324976806425386\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03969888891114129\n",
      "Average test loss: 0.005223386431733767\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03989761902226342\n",
      "Average test loss: 0.005303563145920634\n",
      "Epoch 268/300\n",
      "Average training loss: 0.039642518328295816\n",
      "Average test loss: 0.003995528592831559\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03970612770650122\n",
      "Average test loss: 0.0040636646946271265\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03960439249542024\n",
      "Average test loss: 0.0042454486950818035\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03956483550866445\n",
      "Average test loss: 0.003847979730616013\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03981186246540811\n",
      "Average test loss: 0.0038609525656534568\n",
      "Epoch 273/300\n",
      "Average training loss: 0.039559468436572286\n",
      "Average test loss: 0.004465179766010907\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03959702117244403\n",
      "Average test loss: 0.004228840089920494\n",
      "Epoch 275/300\n",
      "Average training loss: 0.039365381677945455\n",
      "Average test loss: 0.004805738913930125\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0395897148417102\n",
      "Average test loss: 0.004180273568050729\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03943265453974406\n",
      "Average test loss: 0.0042494795345183875\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03952782019972801\n",
      "Average test loss: 0.003927515821738376\n",
      "Epoch 279/300\n",
      "Average training loss: 0.039446632320682205\n",
      "Average test loss: 0.004156661035906937\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03956704163054625\n",
      "Average test loss: 0.003915412248422702\n",
      "Epoch 281/300\n",
      "Average training loss: 0.039416694783502154\n",
      "Average test loss: 0.003941745239827368\n",
      "Epoch 282/300\n",
      "Average training loss: 0.039445120486948225\n",
      "Average test loss: 0.003960935950900118\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03944416092170609\n",
      "Average test loss: 0.00418421006989148\n",
      "Epoch 284/300\n",
      "Average training loss: 0.039409700595670276\n",
      "Average test loss: 0.003992001303782066\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03923738267355495\n",
      "Average test loss: 0.0038772666970681813\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03939736722244157\n",
      "Average test loss: 0.004032333038333389\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0393046152624819\n",
      "Average test loss: 0.003988808291446832\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03924026235441367\n",
      "Average test loss: 0.003986032142200404\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03922992776168717\n",
      "Average test loss: 0.004034191098064184\n",
      "Epoch 290/300\n",
      "Average training loss: 0.039181561979982585\n",
      "Average test loss: 0.004010563617158267\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03923185382617844\n",
      "Average test loss: 0.0039006800225211513\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03920764042271508\n",
      "Average test loss: 0.004105769799194402\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0393330191704962\n",
      "Average test loss: 0.004750567477196455\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03916816608773337\n",
      "Average test loss: 0.0039749269696573416\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03907528259522385\n",
      "Average test loss: 0.004027125108366211\n",
      "Epoch 296/300\n",
      "Average training loss: 0.039076748473776714\n",
      "Average test loss: 0.0040406438747627865\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03899499173296823\n",
      "Average test loss: 0.004007138358636034\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03924658952818977\n",
      "Average test loss: 0.003927107732329104\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03903496580322584\n",
      "Average test loss: 0.004222520866741737\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03892716587252087\n",
      "Average test loss: 0.004303225210557381\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.5936118687523735\n",
      "Average test loss: 0.00628169165758623\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4683494184282091\n",
      "Average test loss: 0.006334068446109693\n",
      "Epoch 3/300\n",
      "Average training loss: 0.29550544589095645\n",
      "Average test loss: 0.0047323732202251756\n",
      "Epoch 4/300\n",
      "Average training loss: 0.215001897043652\n",
      "Average test loss: 0.004451008493287696\n",
      "Epoch 5/300\n",
      "Average training loss: 0.17049409414662256\n",
      "Average test loss: 0.004266534879803658\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1439022778802448\n",
      "Average test loss: 0.004081550591935714\n",
      "Epoch 7/300\n",
      "Average training loss: 0.12518436719311607\n",
      "Average test loss: 0.003912603901492225\n",
      "Epoch 8/300\n",
      "Average training loss: 0.11287794205877516\n",
      "Average test loss: 0.004515266101393435\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10402465141481823\n",
      "Average test loss: 0.004206024039950636\n",
      "Epoch 10/300\n",
      "Average training loss: 0.09703340617153379\n",
      "Average test loss: 0.0038327877012391885\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09185893597867754\n",
      "Average test loss: 0.003447388786201676\n",
      "Epoch 12/300\n",
      "Average training loss: 0.08706240772538715\n",
      "Average test loss: 0.00335120121224059\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08371753350231383\n",
      "Average test loss: 0.0034536462384793494\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07990434870455\n",
      "Average test loss: 0.0031588067896664144\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07698830415805182\n",
      "Average test loss: 0.0030763200687037575\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07345621985859341\n",
      "Average test loss: 0.0029100056520352763\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07092038462559382\n",
      "Average test loss: 0.0029109574740545617\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06865106850531366\n",
      "Average test loss: 0.002885576215469175\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06538654749923282\n",
      "Average test loss: 0.002845544528837005\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06300310804777676\n",
      "Average test loss: 0.003061089631376995\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0607996015548706\n",
      "Average test loss: 0.17595283359289168\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05844871352116267\n",
      "Average test loss: 0.002559237099977003\n",
      "Epoch 23/300\n",
      "Average training loss: 0.056299018515480886\n",
      "Average test loss: 0.0024963947526282734\n",
      "Epoch 24/300\n",
      "Average training loss: 0.054905883522497284\n",
      "Average test loss: 0.00260528671534525\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05310359407133526\n",
      "Average test loss: 0.002509953297053774\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05186812588903639\n",
      "Average test loss: 0.0024275561817404295\n",
      "Epoch 27/300\n",
      "Average training loss: 0.050635625468360056\n",
      "Average test loss: 0.002476532688881788\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04957744496067365\n",
      "Average test loss: 0.002703586035925481\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04864032701651255\n",
      "Average test loss: 0.0024820673674758936\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04776189756393433\n",
      "Average test loss: 0.0023487260840419264\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04691219287117322\n",
      "Average test loss: 0.0024055460225790738\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04622023902667893\n",
      "Average test loss: 0.002431168032396171\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04544850443138017\n",
      "Average test loss: 0.0023481915986372365\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04497633033328586\n",
      "Average test loss: 0.0023726694161693256\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04453183590041267\n",
      "Average test loss: 0.0025258662371585765\n",
      "Epoch 36/300\n",
      "Average training loss: 0.044806324263413744\n",
      "Average test loss: 0.0022451181564893986\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09299146455195215\n",
      "Average test loss: 0.0026037392821162937\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05643476685881615\n",
      "Average test loss: 0.0026188198634319836\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05146693323718177\n",
      "Average test loss: 0.0031300169055660564\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04909087643027306\n",
      "Average test loss: 0.0024199266312643886\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04772021776437759\n",
      "Average test loss: 0.002334012993507915\n",
      "Epoch 42/300\n",
      "Average training loss: 0.046676782260338466\n",
      "Average test loss: 0.002373258137040668\n",
      "Epoch 43/300\n",
      "Average training loss: 0.046137235217624244\n",
      "Average test loss: 0.002269027480855584\n",
      "Epoch 44/300\n",
      "Average training loss: 0.045170986870924634\n",
      "Average test loss: 0.0022631191230482526\n",
      "Epoch 45/300\n",
      "Average training loss: 0.044645929247140885\n",
      "Average test loss: 0.0022727744856642354\n",
      "Epoch 46/300\n",
      "Average training loss: 0.044182258672184414\n",
      "Average test loss: 0.002269062629176511\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04411033876405822\n",
      "Average test loss: 0.0023422752421142327\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0432819376554754\n",
      "Average test loss: 0.0022515209501402247\n",
      "Epoch 49/300\n",
      "Average training loss: 0.043068094907535444\n",
      "Average test loss: 0.0024071370540186764\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04278790677090486\n",
      "Average test loss: 0.00226217323458857\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04231277822123634\n",
      "Average test loss: 0.0022181728362209267\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04206741432348887\n",
      "Average test loss: 0.0024216261667509872\n",
      "Epoch 53/300\n",
      "Average training loss: 0.041786399526728524\n",
      "Average test loss: 0.002473522281067239\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04161349696748787\n",
      "Average test loss: 0.0022414305423282916\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04101807868480682\n",
      "Average test loss: 0.0026948173445545966\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04078402194877465\n",
      "Average test loss: 0.0022327607187132043\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04045859088831478\n",
      "Average test loss: 0.002442126248445776\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04027276707026693\n",
      "Average test loss: 0.0022444711859441464\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03988311110933622\n",
      "Average test loss: 0.0022420318039755027\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0398506677614318\n",
      "Average test loss: 0.0022609521240616838\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03969996443225278\n",
      "Average test loss: 0.0022241649887421067\n",
      "Epoch 62/300\n",
      "Average training loss: 0.039341093132893246\n",
      "Average test loss: 0.0023331899158656596\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03885394761297438\n",
      "Average test loss: 0.002256317422207859\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03862836058272256\n",
      "Average test loss: 0.002192798650720053\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03848000552919176\n",
      "Average test loss: 0.0027097081281244753\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03810885384678841\n",
      "Average test loss: 0.003606674398606022\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03809408181740178\n",
      "Average test loss: 0.002281215347142683\n",
      "Epoch 68/300\n",
      "Average training loss: 0.037701483615570595\n",
      "Average test loss: 0.002283655511836211\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03747106668187512\n",
      "Average test loss: 0.002640695801211728\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03838221185406049\n",
      "Average test loss: 0.002245657311131557\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04318830500211981\n",
      "Average test loss: 0.0024228308842413953\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04034036930070983\n",
      "Average test loss: 0.002507902038594087\n",
      "Epoch 73/300\n",
      "Average training loss: 0.038029348083668285\n",
      "Average test loss: 0.0028089990955260063\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03735672152042389\n",
      "Average test loss: 0.002508717319617669\n",
      "Epoch 75/300\n",
      "Average training loss: 0.037012448287672466\n",
      "Average test loss: 0.002236703652681576\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03686376513540745\n",
      "Average test loss: 0.0023285901366422576\n",
      "Epoch 77/300\n",
      "Average training loss: 0.036542351533969245\n",
      "Average test loss: 0.0022829130253651077\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03635347870323393\n",
      "Average test loss: 0.0028125009313225745\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03634674330221282\n",
      "Average test loss: 0.0022615778967738152\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03636141580343247\n",
      "Average test loss: 0.002358242792284323\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03621125051379204\n",
      "Average test loss: 0.0031257851127949026\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03604550923572646\n",
      "Average test loss: 0.0023780327652477558\n",
      "Epoch 83/300\n",
      "Average training loss: 0.035853962689638134\n",
      "Average test loss: 0.002271161128456394\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03548147856361336\n",
      "Average test loss: 0.002336815347895026\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03546358703242408\n",
      "Average test loss: 0.0024663195809763337\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0352662246161037\n",
      "Average test loss: 0.0023273005564179686\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03517523477805985\n",
      "Average test loss: 0.002306018665019009\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03519526505635844\n",
      "Average test loss: 0.0024518062237443195\n",
      "Epoch 89/300\n",
      "Average training loss: 0.035140473335981366\n",
      "Average test loss: 0.002875939325119058\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03486210741930538\n",
      "Average test loss: 0.0023671523622340626\n",
      "Epoch 91/300\n",
      "Average training loss: 0.034635981655783125\n",
      "Average test loss: 0.0026405247898979318\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03450298453701867\n",
      "Average test loss: 0.0023432332859891986\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03426575086183018\n",
      "Average test loss: 0.021867129989796216\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03415660164422459\n",
      "Average test loss: 0.002352893479064935\n",
      "Epoch 95/300\n",
      "Average training loss: 0.034224330031209525\n",
      "Average test loss: 0.0023283247790402838\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03403789425889651\n",
      "Average test loss: 0.0023095704620290135\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0339868753320641\n",
      "Average test loss: 0.003391559829107589\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03398612860341867\n",
      "Average test loss: 0.002416530302208331\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03373239963253339\n",
      "Average test loss: 0.0022569073417948353\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03382270904050933\n",
      "Average test loss: 0.002355044959526923\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03334920803374714\n",
      "Average test loss: 0.002378633948870831\n",
      "Epoch 102/300\n",
      "Average training loss: 0.033408827136788104\n",
      "Average test loss: 0.0023970555948714415\n",
      "Epoch 103/300\n",
      "Average training loss: 0.033252702209684584\n",
      "Average test loss: 0.0036609889341311323\n",
      "Epoch 104/300\n",
      "Average training loss: 0.033300831182135476\n",
      "Average test loss: 0.002335391946654353\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03304135124219788\n",
      "Average test loss: 0.0024101867330157093\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03306372044814958\n",
      "Average test loss: 0.002634256827955445\n",
      "Epoch 107/300\n",
      "Average training loss: 0.032862750492162175\n",
      "Average test loss: 0.002439260139440497\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03299685409333971\n",
      "Average test loss: 0.002559456030527751\n",
      "Epoch 109/300\n",
      "Average training loss: 0.032901745706796645\n",
      "Average test loss: 0.0029036324912061293\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03278680752052201\n",
      "Average test loss: 0.0024531888171202606\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03260474750896295\n",
      "Average test loss: 0.002343443291261792\n",
      "Epoch 112/300\n",
      "Average training loss: 0.032471784351600544\n",
      "Average test loss: 0.00253505674459868\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03239410077532132\n",
      "Average test loss: 0.004163524241083198\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0324002491235733\n",
      "Average test loss: 0.002374414815256993\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03236796142326461\n",
      "Average test loss: 0.002554671990374724\n",
      "Epoch 116/300\n",
      "Average training loss: 0.032425105104843775\n",
      "Average test loss: 0.0023078295888586176\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03229086772104104\n",
      "Average test loss: 0.0028421906465664506\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03209679166475932\n",
      "Average test loss: 0.0024134219073586993\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03203681073420578\n",
      "Average test loss: 0.0024741692246041365\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03194809551040331\n",
      "Average test loss: 0.0024929136737353273\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03187888075576888\n",
      "Average test loss: 0.002582479765224788\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03189246737957001\n",
      "Average test loss: 0.0027062523021466204\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03181046776142385\n",
      "Average test loss: 0.0023674224741342996\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03173128169112735\n",
      "Average test loss: 0.0024209965111480817\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0316205456091298\n",
      "Average test loss: 0.002392511566893922\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03193044775393274\n",
      "Average test loss: 0.002377963075828221\n",
      "Epoch 127/300\n",
      "Average training loss: 0.031555887535214426\n",
      "Average test loss: 0.002414269531559613\n",
      "Epoch 128/300\n",
      "Average training loss: 0.031397295516398215\n",
      "Average test loss: 0.002439447113623222\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03146251937084728\n",
      "Average test loss: 0.0023539888848447136\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03150923169652621\n",
      "Average test loss: 0.002350932074503766\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031426865100860596\n",
      "Average test loss: 0.0023961250074207782\n",
      "Epoch 132/300\n",
      "Average training loss: 0.031575842373900945\n",
      "Average test loss: 0.0025566258320791854\n",
      "Epoch 133/300\n",
      "Average training loss: 0.031123199429776933\n",
      "Average test loss: 0.002376529259710676\n",
      "Epoch 134/300\n",
      "Average training loss: 0.031310434379511407\n",
      "Average test loss: 0.002697598277280728\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031067645798126858\n",
      "Average test loss: 0.004533342783649762\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03099266090989113\n",
      "Average test loss: 0.002597357776429918\n",
      "Epoch 137/300\n",
      "Average training loss: 0.030965142061312994\n",
      "Average test loss: 0.002449321118493875\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03101769541700681\n",
      "Average test loss: 0.0027730417862120603\n",
      "Epoch 139/300\n",
      "Average training loss: 0.031038910478353502\n",
      "Average test loss: 0.003107521382677886\n",
      "Epoch 140/300\n",
      "Average training loss: 0.031003699221544796\n",
      "Average test loss: 0.0040132291389422285\n",
      "Epoch 141/300\n",
      "Average training loss: 0.030992148980498315\n",
      "Average test loss: 0.010958945925037065\n",
      "Epoch 142/300\n",
      "Average training loss: 0.031265006605121826\n",
      "Average test loss: 0.0025370774850663213\n",
      "Epoch 143/300\n",
      "Average training loss: 0.030595547339982456\n",
      "Average test loss: 0.0030521674565970897\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03061252103580369\n",
      "Average test loss: 0.0024585940110393696\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030839548581176334\n",
      "Average test loss: 0.002583068346604705\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030707942232489585\n",
      "Average test loss: 0.0027778052577955856\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030642076288660367\n",
      "Average test loss: 0.0025979832160390086\n",
      "Epoch 148/300\n",
      "Average training loss: 0.030569707706570626\n",
      "Average test loss: 0.002428100417368114\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03042761227157381\n",
      "Average test loss: 0.0024194977580466203\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03058960999382867\n",
      "Average test loss: 0.0024889247693742314\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030364745650026535\n",
      "Average test loss: 0.0025411635591752\n",
      "Epoch 152/300\n",
      "Average training loss: 0.030376203258832297\n",
      "Average test loss: 0.00499487853070928\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030318586337897514\n",
      "Average test loss: 0.0024940339653856222\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030396471192439396\n",
      "Average test loss: 0.0024597846117491522\n",
      "Epoch 155/300\n",
      "Average training loss: 0.030421993970870972\n",
      "Average test loss: 0.0025270631487170855\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03021510824229982\n",
      "Average test loss: 0.0024856168931970996\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03045792223016421\n",
      "Average test loss: 0.00263313018820352\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030022571737567585\n",
      "Average test loss: 0.0024788383208215236\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030081511537233988\n",
      "Average test loss: 0.002606840599742201\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03008697238895628\n",
      "Average test loss: 0.0024009304938630927\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03007427420384354\n",
      "Average test loss: 0.0024942773015548788\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03005927813053131\n",
      "Average test loss: 0.002555246244598594\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030142930461300742\n",
      "Average test loss: 0.0024935009751675856\n",
      "Epoch 164/300\n",
      "Average training loss: 0.029924128744337294\n",
      "Average test loss: 0.0026440281669298806\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030084568993912802\n",
      "Average test loss: 0.003482973494877418\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02986911051803165\n",
      "Average test loss: 0.002470477155513234\n",
      "Epoch 167/300\n",
      "Average training loss: 0.029992995887994766\n",
      "Average test loss: 0.0026165178433681527\n",
      "Epoch 168/300\n",
      "Average training loss: 0.029834261013401878\n",
      "Average test loss: 0.002488957797280616\n",
      "Epoch 169/300\n",
      "Average training loss: 0.029886239949199887\n",
      "Average test loss: 0.0027342532297803296\n",
      "Epoch 170/300\n",
      "Average training loss: 0.029757962529857955\n",
      "Average test loss: 0.0026093782192716996\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02975597304933601\n",
      "Average test loss: 0.002457328031874365\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02986190864443779\n",
      "Average test loss: 0.0025302923038187956\n",
      "Epoch 173/300\n",
      "Average training loss: 0.029604484687248866\n",
      "Average test loss: 0.0025028174474007554\n",
      "Epoch 174/300\n",
      "Average training loss: 0.029973307781749302\n",
      "Average test loss: 0.004618221412102381\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02978914263182216\n",
      "Average test loss: 0.0025868845648235744\n",
      "Epoch 176/300\n",
      "Average training loss: 0.029586314044064945\n",
      "Average test loss: 0.002596928488773604\n",
      "Epoch 177/300\n",
      "Average training loss: 0.029554714024066925\n",
      "Average test loss: 0.002488814839679334\n",
      "Epoch 178/300\n",
      "Average training loss: 0.029515085958772235\n",
      "Average test loss: 0.04593355133508643\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0295262989004453\n",
      "Average test loss: 0.002556914599198434\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02955855546394984\n",
      "Average test loss: 0.004784168122336269\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02944921208421389\n",
      "Average test loss: 0.002595382840786543\n",
      "Epoch 182/300\n",
      "Average training loss: 0.029416437675555546\n",
      "Average test loss: 0.0029058283786806793\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029389830794599323\n",
      "Average test loss: 0.007534963159718447\n",
      "Epoch 184/300\n",
      "Average training loss: 0.029353621615303888\n",
      "Average test loss: 0.0024422883016781676\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02944130365219381\n",
      "Average test loss: 0.0025096617386572893\n",
      "Epoch 186/300\n",
      "Average training loss: 0.029253886206282508\n",
      "Average test loss: 0.0025118015435420804\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0293313527405262\n",
      "Average test loss: 0.002496342724396123\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02927411643664042\n",
      "Average test loss: 0.002736069445187847\n",
      "Epoch 189/300\n",
      "Average training loss: 0.029277558520436286\n",
      "Average test loss: 0.0028693004317788613\n",
      "Epoch 190/300\n",
      "Average training loss: 0.029275405098994574\n",
      "Average test loss: 0.0037401999971932837\n",
      "Epoch 191/300\n",
      "Average training loss: 0.029175187836090723\n",
      "Average test loss: 0.004134048016741872\n",
      "Epoch 192/300\n",
      "Average training loss: 0.029129218293560875\n",
      "Average test loss: 0.002560599894883732\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029125327673223282\n",
      "Average test loss: 0.0025369467853258052\n",
      "Epoch 194/300\n",
      "Average training loss: 0.029147468709283406\n",
      "Average test loss: 0.0028772376941310035\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02909816418422593\n",
      "Average test loss: 0.002899030335144036\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029067144688632755\n",
      "Average test loss: 0.002483642777841952\n",
      "Epoch 197/300\n",
      "Average training loss: 0.029178773226009473\n",
      "Average test loss: 0.0025739901955756875\n",
      "Epoch 198/300\n",
      "Average training loss: 0.029002746406528685\n",
      "Average test loss: 0.002573810639273789\n",
      "Epoch 199/300\n",
      "Average training loss: 0.028981767351428666\n",
      "Average test loss: 0.14482359136972162\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02903557683693038\n",
      "Average test loss: 0.0026558041140023205\n",
      "Epoch 201/300\n",
      "Average training loss: 0.028977385426561038\n",
      "Average test loss: 0.002611156880441639\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0289594005048275\n",
      "Average test loss: 0.0027072503454983235\n",
      "Epoch 203/300\n",
      "Average training loss: 0.028926643578542605\n",
      "Average test loss: 0.004755205657126175\n",
      "Epoch 204/300\n",
      "Average training loss: 0.028826053791575962\n",
      "Average test loss: 0.0026542645365827612\n",
      "Epoch 205/300\n",
      "Average training loss: 0.028912371857298745\n",
      "Average test loss: 0.0025297593474388124\n",
      "Epoch 206/300\n",
      "Average training loss: 0.028900102317333223\n",
      "Average test loss: 0.005874670990639263\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028869637992646958\n",
      "Average test loss: 0.002631029987293813\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0288363609297408\n",
      "Average test loss: 0.0025932271702008114\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02876591717534595\n",
      "Average test loss: 0.002892944671627548\n",
      "Epoch 210/300\n",
      "Average training loss: 0.028695133465859626\n",
      "Average test loss: 0.0025981305266420045\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02879065566427178\n",
      "Average test loss: 0.0025674906020156213\n",
      "Epoch 212/300\n",
      "Average training loss: 0.028803409069776534\n",
      "Average test loss: 0.0027448123287823465\n",
      "Epoch 213/300\n",
      "Average training loss: 0.028806361438499555\n",
      "Average test loss: 0.002613948295306828\n",
      "Epoch 214/300\n",
      "Average training loss: 0.028643795493576263\n",
      "Average test loss: 0.0032855333007044264\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02856297589507368\n",
      "Average test loss: 0.0029279023171919914\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0287725520796246\n",
      "Average test loss: 0.002550394555553794\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028754750420649847\n",
      "Average test loss: 0.0034021053090691564\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028496464908123017\n",
      "Average test loss: 0.002680685308865375\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028514861007531485\n",
      "Average test loss: 0.0027982541885640884\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028476073334614434\n",
      "Average test loss: 0.0026913625076413154\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028660103734996584\n",
      "Average test loss: 0.002745088274487191\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028593913012080724\n",
      "Average test loss: 0.002572474664491084\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02847307736840513\n",
      "Average test loss: 0.0027118368515123923\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028473553993635707\n",
      "Average test loss: 0.002563389677554369\n",
      "Epoch 225/300\n",
      "Average training loss: 0.028480064590771993\n",
      "Average test loss: 0.002702457032062941\n",
      "Epoch 226/300\n",
      "Average training loss: 0.028499728865093655\n",
      "Average test loss: 0.0025239133574068546\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02847189222276211\n",
      "Average test loss: 0.002671118790180319\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02838413475619422\n",
      "Average test loss: 0.0029096944921960435\n",
      "Epoch 229/300\n",
      "Average training loss: 0.028434681142369907\n",
      "Average test loss: 0.00266396245919168\n",
      "Epoch 230/300\n",
      "Average training loss: 0.028293716760145294\n",
      "Average test loss: 0.0025845691230562\n",
      "Epoch 231/300\n",
      "Average training loss: 0.028583659150534207\n",
      "Average test loss: 0.002762919372982449\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02835630886256695\n",
      "Average test loss: 0.002620781555771828\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02836384971936544\n",
      "Average test loss: 0.030349771509567896\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02828038497434722\n",
      "Average test loss: 0.002609439310307304\n",
      "Epoch 235/300\n",
      "Average training loss: 0.028317449708779655\n",
      "Average test loss: 0.0025199743515501418\n",
      "Epoch 236/300\n",
      "Average training loss: 0.028343353271484375\n",
      "Average test loss: 0.002561118260439899\n",
      "Epoch 237/300\n",
      "Average training loss: 0.028316265013482837\n",
      "Average test loss: 0.0030590959965354865\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02832605498035749\n",
      "Average test loss: 0.004193113444993893\n",
      "Epoch 239/300\n",
      "Average training loss: 0.028304556811849277\n",
      "Average test loss: 0.0028323633623205955\n",
      "Epoch 240/300\n",
      "Average training loss: 0.028291507540477646\n",
      "Average test loss: 0.0025492550786584615\n",
      "Epoch 241/300\n",
      "Average training loss: 0.028102352124121453\n",
      "Average test loss: 0.0037725377157330512\n",
      "Epoch 242/300\n",
      "Average training loss: 0.028153805346952544\n",
      "Average test loss: 0.002644105321106811\n",
      "Epoch 243/300\n",
      "Average training loss: 0.028288433313369752\n",
      "Average test loss: 0.0027108562889819345\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02804612897336483\n",
      "Average test loss: 0.0028792066497521267\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02811445978946156\n",
      "Average test loss: 0.0026132234790258936\n",
      "Epoch 246/300\n",
      "Average training loss: 0.028048537419901955\n",
      "Average test loss: 0.002801501920653714\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02820970625678698\n",
      "Average test loss: 0.0026179967986212837\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02812044584585561\n",
      "Average test loss: 0.0029431005696662596\n",
      "Epoch 249/300\n",
      "Average training loss: 0.028141429642836252\n",
      "Average test loss: 0.0027351100761443376\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02814247270921866\n",
      "Average test loss: 0.002584210078749392\n",
      "Epoch 251/300\n",
      "Average training loss: 0.028030202266242768\n",
      "Average test loss: 0.002635934046573109\n",
      "Epoch 252/300\n",
      "Average training loss: 0.028060055062174797\n",
      "Average test loss: 0.002650235669273469\n",
      "Epoch 253/300\n",
      "Average training loss: 0.027981618323259884\n",
      "Average test loss: 0.0026672662604703672\n",
      "Epoch 254/300\n",
      "Average training loss: 0.028073675000005297\n",
      "Average test loss: 0.004669198900461197\n",
      "Epoch 255/300\n",
      "Average training loss: 0.027926701075500913\n",
      "Average test loss: 0.0028439266754107344\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02801374739077356\n",
      "Average test loss: 0.0026178230895764296\n",
      "Epoch 257/300\n",
      "Average training loss: 0.027951859964264764\n",
      "Average test loss: 2.7047388439178466\n",
      "Epoch 258/300\n",
      "Average training loss: 0.027955356856187184\n",
      "Average test loss: 0.0026417072173208\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02780421765645345\n",
      "Average test loss: 0.002815359168789453\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02798255502846506\n",
      "Average test loss: 0.002686507207651933\n",
      "Epoch 261/300\n",
      "Average training loss: 0.027886230483651162\n",
      "Average test loss: 0.0025965430322620604\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027912122968170378\n",
      "Average test loss: 0.0027311345961772734\n",
      "Epoch 263/300\n",
      "Average training loss: 0.027969252700606982\n",
      "Average test loss: 0.002713168434591757\n",
      "Epoch 264/300\n",
      "Average training loss: 0.028119155473179287\n",
      "Average test loss: 0.002628429353237152\n",
      "Epoch 265/300\n",
      "Average training loss: 0.027790258574816915\n",
      "Average test loss: 0.002728729339523448\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02786992422408528\n",
      "Average test loss: 0.00259055345505476\n",
      "Epoch 267/300\n",
      "Average training loss: 0.027737274694773887\n",
      "Average test loss: 0.0025819699094734258\n",
      "Epoch 268/300\n",
      "Average training loss: 0.027829331434435313\n",
      "Average test loss: 0.002661641449564033\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02778558776775996\n",
      "Average test loss: 0.0026344676081919006\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027820961371064185\n",
      "Average test loss: 0.0031788070341572166\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027679248720407487\n",
      "Average test loss: 0.002495533479998509\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027727070949143832\n",
      "Average test loss: 0.0030284411506727336\n",
      "Epoch 273/300\n",
      "Average training loss: 0.027790913831856515\n",
      "Average test loss: 0.002755999121401045\n",
      "Epoch 274/300\n",
      "Average training loss: 0.027781656046708424\n",
      "Average test loss: 0.0027909480925235484\n",
      "Epoch 275/300\n",
      "Average training loss: 0.027829946218265428\n",
      "Average test loss: 0.0026784967130257025\n",
      "Epoch 276/300\n",
      "Average training loss: 0.027659616460402808\n",
      "Average test loss: 0.0026568413275397484\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02756425236331092\n",
      "Average test loss: 0.0026133718960401086\n",
      "Epoch 278/300\n",
      "Average training loss: 0.027653160681327185\n",
      "Average test loss: 0.002589352344059282\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02770431742734379\n",
      "Average test loss: 0.003714992686899172\n",
      "Epoch 280/300\n",
      "Average training loss: 0.027672314213381875\n",
      "Average test loss: 0.002807432568942507\n",
      "Epoch 281/300\n",
      "Average training loss: 0.027658530655834408\n",
      "Average test loss: 0.0026265254139693247\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027625389594170782\n",
      "Average test loss: 0.00261500232708123\n",
      "Epoch 283/300\n",
      "Average training loss: 0.027558303228682943\n",
      "Average test loss: 0.0026150639154430893\n",
      "Epoch 284/300\n",
      "Average training loss: 0.027615632938014136\n",
      "Average test loss: 0.002617552243794004\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02766556059817473\n",
      "Average test loss: 0.0027148185511016183\n",
      "Epoch 286/300\n",
      "Average training loss: 0.027470458870132764\n",
      "Average test loss: 0.002657878497408496\n",
      "Epoch 287/300\n",
      "Average training loss: 0.027526329699489805\n",
      "Average test loss: 0.0026253140122733183\n",
      "Epoch 288/300\n",
      "Average training loss: 0.027603018573588795\n",
      "Average test loss: 0.002734522323124111\n",
      "Epoch 289/300\n",
      "Average training loss: 0.027582031591071022\n",
      "Average test loss: 0.0027250983127289348\n",
      "Epoch 290/300\n",
      "Average training loss: 0.027548688189850914\n",
      "Average test loss: 0.0027991598734839096\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02745387985640102\n",
      "Average test loss: 0.0026210043931172954\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02748167475561301\n",
      "Average test loss: 0.0025996140632778405\n",
      "Epoch 293/300\n",
      "Average training loss: 0.027390766726599798\n",
      "Average test loss: 0.002594336470278601\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02752085080411699\n",
      "Average test loss: 0.0026128854393545125\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02763246830470032\n",
      "Average test loss: 0.003767223314485616\n",
      "Epoch 296/300\n",
      "Average training loss: 0.027394017080465953\n",
      "Average test loss: 0.0033822170454594824\n",
      "Epoch 297/300\n",
      "Average training loss: 0.027467092840207947\n",
      "Average test loss: 0.002826012417053183\n",
      "Epoch 298/300\n",
      "Average training loss: 0.027516785484221245\n",
      "Average test loss: 0.002679901766487294\n",
      "Epoch 299/300\n",
      "Average training loss: 0.027318638862835035\n",
      "Average test loss: 0.005741179816735288\n",
      "Epoch 300/300\n",
      "Average training loss: 0.027358285796311166\n",
      "Average test loss: 0.002610463550314307\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4738319887055291\n",
      "Average test loss: 0.00672581194796496\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4450456155141195\n",
      "Average test loss: 0.004109010566439894\n",
      "Epoch 3/300\n",
      "Average training loss: 0.2754434284236696\n",
      "Average test loss: 0.0038049969180590578\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1952994295756022\n",
      "Average test loss: 0.0034250919845783046\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1511715498632855\n",
      "Average test loss: 0.0031968593297319278\n",
      "Epoch 6/300\n",
      "Average training loss: 0.12432662955919901\n",
      "Average test loss: 0.003210853132729729\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1072770853969786\n",
      "Average test loss: 0.006431219828211599\n",
      "Epoch 8/300\n",
      "Average training loss: 0.09554526089959674\n",
      "Average test loss: 0.0034082782920449974\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08675094050168991\n",
      "Average test loss: 0.002659021274600592\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08047559085819456\n",
      "Average test loss: 0.003129380926075909\n",
      "Epoch 11/300\n",
      "Average training loss: 0.07578466504149967\n",
      "Average test loss: 0.002665721774308218\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07169528024395307\n",
      "Average test loss: 0.0025662424398793116\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06799860334396363\n",
      "Average test loss: 0.0025533091797389916\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06452009966638353\n",
      "Average test loss: 0.0027322464150687058\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06194076496362686\n",
      "Average test loss: 0.0022204955587577487\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05953364457686742\n",
      "Average test loss: 0.002333124400013023\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05643772449758318\n",
      "Average test loss: 0.002599156617704365\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05412956500384543\n",
      "Average test loss: 0.002145918707466788\n",
      "Epoch 19/300\n",
      "Average training loss: 0.052784897949960496\n",
      "Average test loss: 0.0021112010362040667\n",
      "Epoch 20/300\n",
      "Average training loss: 0.050167780035071906\n",
      "Average test loss: 0.0018839993711767926\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04824288437101576\n",
      "Average test loss: 0.0023015827290299865\n",
      "Epoch 22/300\n",
      "Average training loss: 0.046572424319055346\n",
      "Average test loss: 0.0019127275275273455\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0450493172009786\n",
      "Average test loss: 0.00281729884362883\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04307438893450631\n",
      "Average test loss: 0.0018557804044750002\n",
      "Epoch 25/300\n",
      "Average training loss: 0.041528316764367953\n",
      "Average test loss: 0.0016879284374622834\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0402869818839762\n",
      "Average test loss: 0.002337483604542083\n",
      "Epoch 27/300\n",
      "Average training loss: 0.039010392571489014\n",
      "Average test loss: 0.0016510731560281581\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03793962920374341\n",
      "Average test loss: 0.001841310041025281\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03723405733042293\n",
      "Average test loss: 0.0015881463575901256\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04019483084148831\n",
      "Average test loss: 0.0017099574935725993\n",
      "Epoch 31/300\n",
      "Average training loss: 0.037575182206100885\n",
      "Average test loss: 0.0018416087838510672\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03632731523778703\n",
      "Average test loss: 0.0016130646217821373\n",
      "Epoch 33/300\n",
      "Average training loss: 0.036617714636855654\n",
      "Average test loss: 0.0016603051259492835\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03529914960596296\n",
      "Average test loss: 0.0018318567302905851\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03457600013746156\n",
      "Average test loss: 0.0016089560856214827\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03404703956676854\n",
      "Average test loss: 0.004447152289665407\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033695954963564874\n",
      "Average test loss: 0.0017044549110449023\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03348372168342272\n",
      "Average test loss: 0.0015624103971446555\n",
      "Epoch 39/300\n",
      "Average training loss: 0.032869953306184876\n",
      "Average test loss: 0.001552992408681247\n",
      "Epoch 40/300\n",
      "Average training loss: 0.032624023569954766\n",
      "Average test loss: 0.002239016079447336\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03235205873515871\n",
      "Average test loss: 0.0016003590796349776\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03192576729754607\n",
      "Average test loss: 0.0015557268280535937\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03153613216678301\n",
      "Average test loss: 0.0015066124368458987\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03143537931144238\n",
      "Average test loss: 0.001697692405225502\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0312042681839731\n",
      "Average test loss: 0.0015196580019676022\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03082462117738194\n",
      "Average test loss: 0.0024349483812434805\n",
      "Epoch 47/300\n",
      "Average training loss: 0.030443726231654485\n",
      "Average test loss: 0.0016430321658651033\n",
      "Epoch 48/300\n",
      "Average training loss: 0.030274427470233706\n",
      "Average test loss: 0.001519629911881768\n",
      "Epoch 49/300\n",
      "Average training loss: 0.030511061864594617\n",
      "Average test loss: 0.0015605960830580444\n",
      "Epoch 50/300\n",
      "Average training loss: 0.030008994094199604\n",
      "Average test loss: 0.0015612951759248971\n",
      "Epoch 51/300\n",
      "Average training loss: 0.029639562060435613\n",
      "Average test loss: 0.0017223835731339124\n",
      "Epoch 52/300\n",
      "Average training loss: 0.029431246577037707\n",
      "Average test loss: 0.0018287897996294002\n",
      "Epoch 53/300\n",
      "Average training loss: 0.029393844371040662\n",
      "Average test loss: 0.0015602593684775961\n",
      "Epoch 54/300\n",
      "Average training loss: 0.029274046647879814\n",
      "Average test loss: 0.0018863545521679852\n",
      "Epoch 55/300\n",
      "Average training loss: 0.028860635765724712\n",
      "Average test loss: 0.001551599185810321\n",
      "Epoch 56/300\n",
      "Average training loss: 0.028758588837252724\n",
      "Average test loss: 0.001549385268551608\n",
      "Epoch 57/300\n",
      "Average training loss: 0.028803286977940136\n",
      "Average test loss: 0.0015846179652338227\n",
      "Epoch 58/300\n",
      "Average training loss: 0.028302264523175028\n",
      "Average test loss: 0.0015556493912720019\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02820952314386765\n",
      "Average test loss: 0.0016984571115010315\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02819825608200497\n",
      "Average test loss: 0.0019907612674352195\n",
      "Epoch 61/300\n",
      "Average training loss: 0.028133987203240396\n",
      "Average test loss: 0.0015877648962454663\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02799988515012794\n",
      "Average test loss: 0.0015897259030284152\n",
      "Epoch 63/300\n",
      "Average training loss: 0.027612420414884885\n",
      "Average test loss: 0.0015479725608602167\n",
      "Epoch 64/300\n",
      "Average training loss: 0.027523001276784472\n",
      "Average test loss: 0.00167185957212415\n",
      "Epoch 65/300\n",
      "Average training loss: 0.027496803866492378\n",
      "Average test loss: 0.001516412089061406\n",
      "Epoch 66/300\n",
      "Average training loss: 0.027341011996070544\n",
      "Average test loss: 0.0015347681595012546\n",
      "Epoch 67/300\n",
      "Average training loss: 0.027256475397282178\n",
      "Average test loss: 0.0016772300824522972\n",
      "Epoch 68/300\n",
      "Average training loss: 0.027368553808993765\n",
      "Average test loss: 0.0016502569595144856\n",
      "Epoch 69/300\n",
      "Average training loss: 0.027007649431626\n",
      "Average test loss: 0.0018794048104642167\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0268424894048108\n",
      "Average test loss: 0.0032083544664912754\n",
      "Epoch 71/300\n",
      "Average training loss: 0.026808411752184232\n",
      "Average test loss: 0.009228723900185691\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02656853844722112\n",
      "Average test loss: 0.0017089863100813495\n",
      "Epoch 73/300\n",
      "Average training loss: 0.026545600425865915\n",
      "Average test loss: 0.0015362671924651497\n",
      "Epoch 74/300\n",
      "Average training loss: 0.026432484961218305\n",
      "Average test loss: 0.0015644785314798354\n",
      "Epoch 75/300\n",
      "Average training loss: 0.026579941115445562\n",
      "Average test loss: 0.0021442977192087306\n",
      "Epoch 76/300\n",
      "Average training loss: 0.026256554239326053\n",
      "Average test loss: 0.0017819181652739644\n",
      "Epoch 77/300\n",
      "Average training loss: 0.026194215102328194\n",
      "Average test loss: 0.001751644075848162\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0260982090930144\n",
      "Average test loss: 0.0015995967515433829\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02665046350326803\n",
      "Average test loss: 0.0022796962418489986\n",
      "Epoch 80/300\n",
      "Average training loss: 0.025835673093795777\n",
      "Average test loss: 0.0016475943346611328\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02571641820503606\n",
      "Average test loss: 0.0016874641865077945\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025700337765945328\n",
      "Average test loss: 0.0015814445132596626\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02573196977045801\n",
      "Average test loss: 0.00160178016540077\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02575521439479457\n",
      "Average test loss: 0.0015998034542426467\n",
      "Epoch 85/300\n",
      "Average training loss: 0.025716682601306173\n",
      "Average test loss: 0.010830333326839739\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02534236892561118\n",
      "Average test loss: 0.0017559711144616206\n",
      "Epoch 87/300\n",
      "Average training loss: 0.025966108188033105\n",
      "Average test loss: 0.0016201912082421284\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025282773728171986\n",
      "Average test loss: 0.0016487788274470302\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02522056355079015\n",
      "Average test loss: 0.0016563407376201616\n",
      "Epoch 90/300\n",
      "Average training loss: 0.025216758333974413\n",
      "Average test loss: 0.0015806213573863108\n",
      "Epoch 91/300\n",
      "Average training loss: 0.025076646900839276\n",
      "Average test loss: 0.0016253324651883708\n",
      "Epoch 92/300\n",
      "Average training loss: 0.024998028682337865\n",
      "Average test loss: 0.0016158694394139779\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02502482092877229\n",
      "Average test loss: 0.0016055342515723574\n",
      "Epoch 94/300\n",
      "Average training loss: 0.024836828123364185\n",
      "Average test loss: 0.0017620971602284245\n",
      "Epoch 95/300\n",
      "Average training loss: 0.025020824145939615\n",
      "Average test loss: 0.0016868222209417986\n",
      "Epoch 96/300\n",
      "Average training loss: 0.024835770633485584\n",
      "Average test loss: 0.0022974875652127798\n",
      "Epoch 97/300\n",
      "Average training loss: 0.024713237350185713\n",
      "Average test loss: 0.00324737721019321\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024587925434112548\n",
      "Average test loss: 0.001628769136344393\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02466220636169116\n",
      "Average test loss: 0.002510318401373095\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02454202885263496\n",
      "Average test loss: 0.001612119983467791\n",
      "Epoch 101/300\n",
      "Average training loss: 0.024595756944682863\n",
      "Average test loss: 0.0017613451297705373\n",
      "Epoch 102/300\n",
      "Average training loss: 0.024552770824895966\n",
      "Average test loss: 0.001717425789270136\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02436954681575298\n",
      "Average test loss: 0.001605178193965306\n",
      "Epoch 104/300\n",
      "Average training loss: 0.025097848604122796\n",
      "Average test loss: 0.003174858046281669\n",
      "Epoch 105/300\n",
      "Average training loss: 0.024382640913128854\n",
      "Average test loss: 0.0016439611789666943\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02420953130722046\n",
      "Average test loss: 0.0016945040540562737\n",
      "Epoch 107/300\n",
      "Average training loss: 0.024204857448736825\n",
      "Average test loss: 0.001705289244134393\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024249212184713948\n",
      "Average test loss: 0.0017114911710636484\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02408982661035326\n",
      "Average test loss: 0.0030393479689955713\n",
      "Epoch 110/300\n",
      "Average training loss: 0.024236571087605425\n",
      "Average test loss: 0.001685597081358234\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02406327923304505\n",
      "Average test loss: 0.0016686796084460285\n",
      "Epoch 112/300\n",
      "Average training loss: 0.023906529563996526\n",
      "Average test loss: 0.0017239555246714089\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02407521264586184\n",
      "Average test loss: 0.0017441025111410353\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023963665443989965\n",
      "Average test loss: 0.0016271117916848096\n",
      "Epoch 115/300\n",
      "Average training loss: 0.024219085946679115\n",
      "Average test loss: 0.001635856535182231\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02380650482740667\n",
      "Average test loss: 0.006025875233113766\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02388760551644696\n",
      "Average test loss: 0.0017404626160860061\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023943787495295205\n",
      "Average test loss: 0.004921630799770356\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023729671834243668\n",
      "Average test loss: 0.0016674198833190733\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02385099059012201\n",
      "Average test loss: 0.0016533614651610454\n",
      "Epoch 121/300\n",
      "Average training loss: 0.023561085851656068\n",
      "Average test loss: 0.0016598940871448982\n",
      "Epoch 122/300\n",
      "Average training loss: 0.023679451179173257\n",
      "Average test loss: 0.0017377640155868397\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02361786516673035\n",
      "Average test loss: 0.0016352618446366653\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023703870341181755\n",
      "Average test loss: 0.003139498354246219\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02346360287235843\n",
      "Average test loss: 0.0017738805765079128\n",
      "Epoch 126/300\n",
      "Average training loss: 0.023534883446163602\n",
      "Average test loss: 0.001660514256503019\n",
      "Epoch 127/300\n",
      "Average training loss: 0.023482808464103274\n",
      "Average test loss: 0.0017260816308359306\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023385167125198576\n",
      "Average test loss: 0.0018427054072833724\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02341884612209267\n",
      "Average test loss: 0.0016808584520800246\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02336727918022209\n",
      "Average test loss: 0.0017044343139148422\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0233918912096156\n",
      "Average test loss: 0.0042659117416996096\n",
      "Epoch 132/300\n",
      "Average training loss: 0.023236599168843693\n",
      "Average test loss: 0.0018024805564847257\n",
      "Epoch 133/300\n",
      "Average training loss: 0.023260230251484448\n",
      "Average test loss: 0.001764078392750687\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02325018945667479\n",
      "Average test loss: 0.0017454550019982789\n",
      "Epoch 135/300\n",
      "Average training loss: 0.023276448163721295\n",
      "Average test loss: 0.0017237942514734136\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02311107942627536\n",
      "Average test loss: 0.0018290313430027\n",
      "Epoch 137/300\n",
      "Average training loss: 0.023216000848346286\n",
      "Average test loss: 0.0017297304647250309\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02312227172487312\n",
      "Average test loss: 0.0016888783904206422\n",
      "Epoch 139/300\n",
      "Average training loss: 0.023111401599314477\n",
      "Average test loss: 0.0016431304266572827\n",
      "Epoch 140/300\n",
      "Average training loss: 0.023094791748457483\n",
      "Average test loss: 0.0017483655359182093\n",
      "Epoch 141/300\n",
      "Average training loss: 0.023180460042423672\n",
      "Average test loss: 0.0018793030842724774\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02308455874853664\n",
      "Average test loss: 0.001960955246972541\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02296596341662937\n",
      "Average test loss: 0.0017576775203148524\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02290335218939516\n",
      "Average test loss: 0.001623402068287962\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023009438296159108\n",
      "Average test loss: 0.0016981381421080895\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02293838404946857\n",
      "Average test loss: 0.0017198639342354404\n",
      "Epoch 147/300\n",
      "Average training loss: 0.022837500211265353\n",
      "Average test loss: 0.0016519492798381381\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022931821745302942\n",
      "Average test loss: 0.0018453173581510782\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02277154571314653\n",
      "Average test loss: 0.0019477298586732812\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022822653606534005\n",
      "Average test loss: 0.0018095432335200408\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02280627065234714\n",
      "Average test loss: 0.0017055859914463427\n",
      "Epoch 152/300\n",
      "Average training loss: 0.022787499219179153\n",
      "Average test loss: 0.001829730949882004\n",
      "Epoch 153/300\n",
      "Average training loss: 0.023008592802617286\n",
      "Average test loss: 0.0017373114478670887\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022657329673568406\n",
      "Average test loss: 0.0017057861080393196\n",
      "Epoch 155/300\n",
      "Average training loss: 0.022697360701031156\n",
      "Average test loss: 13.23984119509326\n",
      "Epoch 156/300\n",
      "Average training loss: 0.023142245979772675\n",
      "Average test loss: 0.0016794801806099713\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022566258311271667\n",
      "Average test loss: 0.0018591076185823314\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0227131610347165\n",
      "Average test loss: 0.0016592259777502881\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02269836079246468\n",
      "Average test loss: 0.0016778338980964489\n",
      "Epoch 160/300\n",
      "Average training loss: 0.022587240089972815\n",
      "Average test loss: 0.0017427602942205137\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0224954850624005\n",
      "Average test loss: 0.0017666087932884692\n",
      "Epoch 162/300\n",
      "Average training loss: 0.022574104814065826\n",
      "Average test loss: 0.001693870740632216\n",
      "Epoch 163/300\n",
      "Average training loss: 0.022606192047397295\n",
      "Average test loss: 0.002749394634945525\n",
      "Epoch 164/300\n",
      "Average training loss: 0.022493933005465403\n",
      "Average test loss: 0.0017405538955806857\n",
      "Epoch 165/300\n",
      "Average training loss: 0.022424447483486598\n",
      "Average test loss: 0.0017882858227110572\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02247617259207699\n",
      "Average test loss: 0.001732406600792375\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02251017439365387\n",
      "Average test loss: 0.0018749728651924265\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022326926231384277\n",
      "Average test loss: 0.0018037226638860173\n",
      "Epoch 169/300\n",
      "Average training loss: 0.022479640614655284\n",
      "Average test loss: 0.0018955221937762366\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02239103843437301\n",
      "Average test loss: 0.0072108320651782885\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022397399543060196\n",
      "Average test loss: 0.002366571235160033\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02238871053689056\n",
      "Average test loss: 0.0036824758948965203\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02231824815273285\n",
      "Average test loss: 0.0017351306699630287\n",
      "Epoch 174/300\n",
      "Average training loss: 0.022371810898184775\n",
      "Average test loss: 0.04770867723888821\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02233938904106617\n",
      "Average test loss: 0.0019127862879799473\n",
      "Epoch 176/300\n",
      "Average training loss: 0.022472231613265142\n",
      "Average test loss: 0.0017335887143077949\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022296413655082385\n",
      "Average test loss: 0.001785913242958486\n",
      "Epoch 178/300\n",
      "Average training loss: 0.022161000915699534\n",
      "Average test loss: 0.001769740876638227\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022392693541116185\n",
      "Average test loss: 0.0016961889716072215\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02217011273569531\n",
      "Average test loss: 0.001760775066498253\n",
      "Epoch 181/300\n",
      "Average training loss: 0.022131825912329886\n",
      "Average test loss: 0.0019720660747132366\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02213604996767309\n",
      "Average test loss: 0.013123852839072546\n",
      "Epoch 183/300\n",
      "Average training loss: 0.022148385304543706\n",
      "Average test loss: 0.001814444579391016\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022091780846317608\n",
      "Average test loss: 0.002032095282752481\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02206679907772276\n",
      "Average test loss: 0.001755686541294886\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02210164729422993\n",
      "Average test loss: 0.0017783931392348475\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02208605450557338\n",
      "Average test loss: 0.0017449309231920376\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02204274596273899\n",
      "Average test loss: 0.0018290057107806206\n",
      "Epoch 189/300\n",
      "Average training loss: 0.022214425317115254\n",
      "Average test loss: 0.0017322315738225976\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02216892917950948\n",
      "Average test loss: 0.0018325355665551291\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021917960935168797\n",
      "Average test loss: 0.0019660370498895646\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02201639635446999\n",
      "Average test loss: 0.0023525947493811448\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021989410738150278\n",
      "Average test loss: 0.001721438632139729\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02196260280576017\n",
      "Average test loss: 0.0017997716774957049\n",
      "Epoch 195/300\n",
      "Average training loss: 0.021925918911894163\n",
      "Average test loss: 0.001818593025724921\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021909151503609288\n",
      "Average test loss: 0.0017621529035063254\n",
      "Epoch 197/300\n",
      "Average training loss: 0.021906770871745217\n",
      "Average test loss: 0.0018256353247496817\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0218642146140337\n",
      "Average test loss: 0.0017664718471674455\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02192046387990316\n",
      "Average test loss: 0.0017321580613238944\n",
      "Epoch 200/300\n",
      "Average training loss: 0.021833938274118635\n",
      "Average test loss: 1.6497048511033257\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023085525206393665\n",
      "Average test loss: 0.001993912107207709\n",
      "Epoch 202/300\n",
      "Average training loss: 0.021888330201307933\n",
      "Average test loss: 0.0017531035834302505\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021605867897470793\n",
      "Average test loss: 0.0029568926791350048\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02178283160262638\n",
      "Average test loss: 0.0018206918632818593\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02177095725801256\n",
      "Average test loss: 0.0017359308632504608\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021805872552924686\n",
      "Average test loss: 0.0017336296640957395\n",
      "Epoch 207/300\n",
      "Average training loss: 0.021742118976182407\n",
      "Average test loss: 0.0017804792240365512\n",
      "Epoch 208/300\n",
      "Average training loss: 0.021779039017028278\n",
      "Average test loss: 0.0018490486460117001\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02180595319635338\n",
      "Average test loss: 0.0019184144104106558\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021790642514824866\n",
      "Average test loss: 0.0018434734966398941\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021737193839417565\n",
      "Average test loss: 0.0021319999299529526\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02229385759267542\n",
      "Average test loss: 0.0017943826818631755\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02157405323535204\n",
      "Average test loss: 0.00184752866687874\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021738101217481823\n",
      "Average test loss: 0.0017244015204616718\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02163009399175644\n",
      "Average test loss: 0.0018862403841275307\n",
      "Epoch 216/300\n",
      "Average training loss: 0.021599393150872656\n",
      "Average test loss: 0.0017878840710553858\n",
      "Epoch 217/300\n",
      "Average training loss: 0.021696130942967202\n",
      "Average test loss: 0.0019119482747175626\n",
      "Epoch 218/300\n",
      "Average training loss: 0.021579784605238173\n",
      "Average test loss: 0.0019188045234833327\n",
      "Epoch 219/300\n",
      "Average training loss: 0.021540614454282656\n",
      "Average test loss: 0.0018086322963030803\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02179879081249237\n",
      "Average test loss: 0.0017191776488390234\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021544347066018318\n",
      "Average test loss: 0.0017986566579590241\n",
      "Epoch 222/300\n",
      "Average training loss: 0.021587495254145727\n",
      "Average test loss: 0.0017521148160513905\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02153277392519845\n",
      "Average test loss: 0.0017670501137359274\n",
      "Epoch 224/300\n",
      "Average training loss: 0.021554458916187288\n",
      "Average test loss: 0.0019116977858874534\n",
      "Epoch 225/300\n",
      "Average training loss: 0.021638252672221924\n",
      "Average test loss: 0.001782768429360456\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0215564637887809\n",
      "Average test loss: 0.0019832777053945595\n",
      "Epoch 227/300\n",
      "Average training loss: 0.021430661170019044\n",
      "Average test loss: 0.001853409746558302\n",
      "Epoch 228/300\n",
      "Average training loss: 0.021465315664807954\n",
      "Average test loss: 0.0018655729805015855\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02163513251311249\n",
      "Average test loss: 0.0017246373406507903\n",
      "Epoch 230/300\n",
      "Average training loss: 0.021411982707679272\n",
      "Average test loss: 0.0019224591818120744\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02149311140841908\n",
      "Average test loss: 0.0019327608834848636\n",
      "Epoch 232/300\n",
      "Average training loss: 0.021446123422847855\n",
      "Average test loss: 0.018166314411494466\n",
      "Epoch 233/300\n",
      "Average training loss: 0.021406735858983464\n",
      "Average test loss: 0.002377860603440139\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0214964799930652\n",
      "Average test loss: 0.0038239060503741104\n",
      "Epoch 235/300\n",
      "Average training loss: 0.021376567588912116\n",
      "Average test loss: 0.0029386731839428346\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02138154150876734\n",
      "Average test loss: 0.004929056126210424\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02136697378589047\n",
      "Average test loss: 0.001763555068626172\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02131264941063192\n",
      "Average test loss: 0.0017845647299869192\n",
      "Epoch 239/300\n",
      "Average training loss: 0.021424502501885097\n",
      "Average test loss: 0.0017916273320507672\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02136363764769501\n",
      "Average test loss: 0.001821691803737647\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0212822123600377\n",
      "Average test loss: 0.0018087120305539833\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021272483531799582\n",
      "Average test loss: 0.0018365381262265147\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021313961004217466\n",
      "Average test loss: 0.0017376162524645527\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021389012606607544\n",
      "Average test loss: 0.0018590429267949528\n",
      "Epoch 245/300\n",
      "Average training loss: 0.021259695102771124\n",
      "Average test loss: 0.0018329176658557521\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021328859511348938\n",
      "Average test loss: 0.001742269224487245\n",
      "Epoch 247/300\n",
      "Average training loss: 0.021254881666766274\n",
      "Average test loss: 0.002217160459090438\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02134903021156788\n",
      "Average test loss: 0.0026090411020235882\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021255936119291517\n",
      "Average test loss: 0.0018829854796123173\n",
      "Epoch 250/300\n",
      "Average training loss: 0.021206746430860627\n",
      "Average test loss: 0.0018213062964172826\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02128829036653042\n",
      "Average test loss: 0.0018577164202514622\n",
      "Epoch 252/300\n",
      "Average training loss: 0.021337342494063907\n",
      "Average test loss: 0.0019812981645162733\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021119304130474727\n",
      "Average test loss: 0.0017295338010622394\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02120547976427608\n",
      "Average test loss: 0.007969945737471184\n",
      "Epoch 255/300\n",
      "Average training loss: 0.021165805586510236\n",
      "Average test loss: 0.0019321662013729412\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021166928809550074\n",
      "Average test loss: 0.001809177216142416\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021216613810923365\n",
      "Average test loss: 0.0039011709025750557\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021104845257269012\n",
      "Average test loss: 0.0018143257633265522\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021298600875669055\n",
      "Average test loss: 0.001836034074322217\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021297994895113838\n",
      "Average test loss: 0.0018327157594677475\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021026715434259838\n",
      "Average test loss: 0.005661267286373509\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02115762157075935\n",
      "Average test loss: 0.0018332601219622625\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020988393283552592\n",
      "Average test loss: 0.0017697371479330792\n",
      "Epoch 264/300\n",
      "Average training loss: 0.021122895105017556\n",
      "Average test loss: 0.040193244012693564\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0211129732777675\n",
      "Average test loss: 0.002296320330662032\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02109955833024449\n",
      "Average test loss: 0.0017996096128804816\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021020172485046916\n",
      "Average test loss: 0.0017971307440764375\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021119140742553603\n",
      "Average test loss: 0.0018341008448559378\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0210297279159228\n",
      "Average test loss: 0.002132158135374387\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021068360512455305\n",
      "Average test loss: 0.001875756832667523\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020961141462127367\n",
      "Average test loss: 0.002579690340699421\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020975413598948056\n",
      "Average test loss: 0.0017828353287445175\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0209634095231692\n",
      "Average test loss: 0.0017415910427355105\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021093436820639504\n",
      "Average test loss: 0.0017967686810944643\n",
      "Epoch 275/300\n",
      "Average training loss: 0.020968101463384098\n",
      "Average test loss: 0.0018430951955831713\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020996507316827773\n",
      "Average test loss: 0.0017839480626086395\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020977908450696203\n",
      "Average test loss: 0.0018201686969647804\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021029356805814636\n",
      "Average test loss: 0.0017941076943857803\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020836506946219337\n",
      "Average test loss: 0.0018390477132052183\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020955484989616607\n",
      "Average test loss: 0.0018387870902402534\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02092526616487238\n",
      "Average test loss: 0.001785842657658375\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02089697523911794\n",
      "Average test loss: 0.0018035983801301982\n",
      "Epoch 283/300\n",
      "Average training loss: 0.020994441237714555\n",
      "Average test loss: 0.0021089850765549476\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020875567325287395\n",
      "Average test loss: 0.001836373763365878\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020790938896437485\n",
      "Average test loss: 0.0018423029640689493\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02093757134510411\n",
      "Average test loss: 0.0018998381998389959\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02100289814339744\n",
      "Average test loss: 0.0021017075791541075\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020914633626739183\n",
      "Average test loss: 0.001758076145624121\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02081517936951584\n",
      "Average test loss: 0.0017717334982007742\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020859129923913214\n",
      "Average test loss: 0.0017984000636885563\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020791807621717454\n",
      "Average test loss: 0.0023216747966491513\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020777734984954197\n",
      "Average test loss: 0.0017886329335677954\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02076792818804582\n",
      "Average test loss: 0.0019097476800282796\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021001178671916327\n",
      "Average test loss: 0.0017870484251115057\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020730632044374944\n",
      "Average test loss: 0.0018539581830199394\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020788257426685757\n",
      "Average test loss: 0.0020941866425176463\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020711982453862826\n",
      "Average test loss: 0.0020797198583475418\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020836238589551713\n",
      "Average test loss: 0.001762347609632545\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020773325926727718\n",
      "Average test loss: 0.0021285354817907015\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02088295716709561\n",
      "Average test loss: 0.0019084132518619298\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_32_Depth5/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.82\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.25\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.59\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.98\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.42\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.60\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.79\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.79\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.08\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.27\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.62\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.76\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.86\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.96\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.43\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.74\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.59\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.34\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.53\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.62\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.76\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.59\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.39\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.89\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.21\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.98\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.25\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.47\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.65\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.68\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 33.549119559393986\n",
      "Average test loss: 0.014039838488731119\n",
      "Epoch 2/300\n",
      "Average training loss: 11.851975824144152\n",
      "Average test loss: 0.012925866759485668\n",
      "Epoch 3/300\n",
      "Average training loss: 9.763783721923827\n",
      "Average test loss: 0.009941233302983972\n",
      "Epoch 4/300\n",
      "Average training loss: 7.6770303916931155\n",
      "Average test loss: 0.013787912584841251\n",
      "Epoch 5/300\n",
      "Average training loss: 5.901800501929389\n",
      "Average test loss: 0.00935806358522839\n",
      "Epoch 6/300\n",
      "Average training loss: 4.806134271409777\n",
      "Average test loss: 0.00907590950694349\n",
      "Epoch 7/300\n",
      "Average training loss: 4.203221078660753\n",
      "Average test loss: 0.012018403864155213\n",
      "Epoch 8/300\n",
      "Average training loss: 3.415643413119846\n",
      "Average test loss: 0.00856539433159762\n",
      "Epoch 9/300\n",
      "Average training loss: 2.9654658391740587\n",
      "Average test loss: 0.009882607690989972\n",
      "Epoch 10/300\n",
      "Average training loss: 2.4576866346995034\n",
      "Average test loss: 0.008452673967513773\n",
      "Epoch 11/300\n",
      "Average training loss: 2.0655829990175034\n",
      "Average test loss: 0.008796751826587651\n",
      "Epoch 12/300\n",
      "Average training loss: 1.7521466339959038\n",
      "Average test loss: 0.007795401136494345\n",
      "Epoch 13/300\n",
      "Average training loss: 1.4731490169101291\n",
      "Average test loss: 0.007296145520276493\n",
      "Epoch 14/300\n",
      "Average training loss: 1.2451435560650295\n",
      "Average test loss: 0.0072434841112958065\n",
      "Epoch 15/300\n",
      "Average training loss: 1.030211187362671\n",
      "Average test loss: 0.007824753562195433\n",
      "Epoch 16/300\n",
      "Average training loss: 0.8633193811310662\n",
      "Average test loss: 0.008620728424853749\n",
      "Epoch 17/300\n",
      "Average training loss: 0.7346490491761102\n",
      "Average test loss: 0.007392492920160294\n",
      "Epoch 18/300\n",
      "Average training loss: 0.6343159255451626\n",
      "Average test loss: 0.0068611684056619805\n",
      "Epoch 19/300\n",
      "Average training loss: 0.5506883993148803\n",
      "Average test loss: 0.006872836047576533\n",
      "Epoch 20/300\n",
      "Average training loss: 0.4889003349410163\n",
      "Average test loss: 0.006800758727308777\n",
      "Epoch 21/300\n",
      "Average training loss: 0.443434076362186\n",
      "Average test loss: 0.006737959419273668\n",
      "Epoch 22/300\n",
      "Average training loss: 0.4052127923700545\n",
      "Average test loss: 0.007138179786503315\n",
      "Epoch 23/300\n",
      "Average training loss: 0.3761761273807949\n",
      "Average test loss: 0.006593699833585156\n",
      "Epoch 24/300\n",
      "Average training loss: 0.35438980049557156\n",
      "Average test loss: 0.006454966693702671\n",
      "Epoch 25/300\n",
      "Average training loss: 0.33099974873330856\n",
      "Average test loss: 0.006501537057674593\n",
      "Epoch 26/300\n",
      "Average training loss: 0.3121645017729865\n",
      "Average test loss: 0.006691925526907046\n",
      "Epoch 27/300\n",
      "Average training loss: 0.29856954632865057\n",
      "Average test loss: 0.007122684379832612\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2886565966341231\n",
      "Average test loss: 0.006373790172239145\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2780966668791241\n",
      "Average test loss: 0.006253132467468579\n",
      "Epoch 30/300\n",
      "Average training loss: 0.2673730929427677\n",
      "Average test loss: 0.006389649546394745\n",
      "Epoch 31/300\n",
      "Average training loss: 0.26198982837465074\n",
      "Average test loss: 1.5456725435256957\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2549882318443722\n",
      "Average test loss: 0.006058838732126686\n",
      "Epoch 33/300\n",
      "Average training loss: 0.24939758531252543\n",
      "Average test loss: 0.006363023399064938\n",
      "Epoch 34/300\n",
      "Average training loss: 0.24383208901352352\n",
      "Average test loss: 0.006537044659256935\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2359651903576321\n",
      "Average test loss: 0.006051992375817564\n",
      "Epoch 36/300\n",
      "Average training loss: 0.23132408481174044\n",
      "Average test loss: 0.006122016244050529\n",
      "Epoch 37/300\n",
      "Average training loss: 0.22668779011567433\n",
      "Average test loss: 0.00603522524692946\n",
      "Epoch 38/300\n",
      "Average training loss: 0.22235758063528274\n",
      "Average test loss: 0.0061128189029792945\n",
      "Epoch 39/300\n",
      "Average training loss: 0.2180203479131063\n",
      "Average test loss: 0.006244342777464125\n",
      "Epoch 40/300\n",
      "Average training loss: 0.21539654723803203\n",
      "Average test loss: 0.005961007141404681\n",
      "Epoch 41/300\n",
      "Average training loss: 0.21161369570096333\n",
      "Average test loss: 0.005969239107436604\n",
      "Epoch 42/300\n",
      "Average training loss: 0.2090739153093762\n",
      "Average test loss: 0.005896087425864405\n",
      "Epoch 43/300\n",
      "Average training loss: 0.2059332468509674\n",
      "Average test loss: 0.007351416386663914\n",
      "Epoch 44/300\n",
      "Average training loss: 0.20271617484092713\n",
      "Average test loss: 0.1723049446079466\n",
      "Epoch 45/300\n",
      "Average training loss: 0.20044886236720616\n",
      "Average test loss: 0.006570492910014258\n",
      "Epoch 46/300\n",
      "Average training loss: 0.19976332698927984\n",
      "Average test loss: 0.005803654869190521\n",
      "Epoch 47/300\n",
      "Average training loss: 0.19673183037175074\n",
      "Average test loss: 0.006315147988912132\n",
      "Epoch 48/300\n",
      "Average training loss: 0.19528731554746628\n",
      "Average test loss: 0.005907910504688819\n",
      "Epoch 49/300\n",
      "Average training loss: 0.19351660577456156\n",
      "Average test loss: 0.007352701929708322\n",
      "Epoch 50/300\n",
      "Average training loss: 0.19090538522932265\n",
      "Average test loss: 0.008287920859124925\n",
      "Epoch 51/300\n",
      "Average training loss: 0.18956158736679288\n",
      "Average test loss: 0.005807604879140854\n",
      "Epoch 52/300\n",
      "Average training loss: 0.18769485403431788\n",
      "Average test loss: 0.005933234351790614\n",
      "Epoch 53/300\n",
      "Average training loss: 0.18615246272087096\n",
      "Average test loss: 0.006004253596895271\n",
      "Epoch 54/300\n",
      "Average training loss: 0.18390092855029636\n",
      "Average test loss: 0.06944785040120284\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1829042963054445\n",
      "Average test loss: 0.006543249689456489\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1831223298576143\n",
      "Average test loss: 0.006310812951376041\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1792344592942132\n",
      "Average test loss: 0.005789703412602345\n",
      "Epoch 58/300\n",
      "Average training loss: 0.17895148040188683\n",
      "Average test loss: 0.008196687206212017\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1774374771250619\n",
      "Average test loss: 0.0058254035264253615\n",
      "Epoch 60/300\n",
      "Average training loss: 0.17563127913077672\n",
      "Average test loss: 0.005944513945736819\n",
      "Epoch 61/300\n",
      "Average training loss: 0.17458929003609552\n",
      "Average test loss: 0.006440991081297398\n",
      "Epoch 62/300\n",
      "Average training loss: 0.17584912852446238\n",
      "Average test loss: 0.005922267920441098\n",
      "Epoch 63/300\n",
      "Average training loss: 0.17227758853965336\n",
      "Average test loss: 0.015534648762808906\n",
      "Epoch 64/300\n",
      "Average training loss: 0.17113638243410323\n",
      "Average test loss: 0.005921809456414647\n",
      "Epoch 65/300\n",
      "Average training loss: 0.17033957441647848\n",
      "Average test loss: 0.008661929968330595\n",
      "Epoch 66/300\n",
      "Average training loss: 0.16881782511207793\n",
      "Average test loss: 0.006123328258593877\n",
      "Epoch 67/300\n",
      "Average training loss: 0.16865499713685778\n",
      "Average test loss: 0.00710329050798383\n",
      "Epoch 68/300\n",
      "Average training loss: 0.16721247368388706\n",
      "Average test loss: 0.006306835786749919\n",
      "Epoch 69/300\n",
      "Average training loss: 0.16653813697894415\n",
      "Average test loss: 0.006294504197935263\n",
      "Epoch 70/300\n",
      "Average training loss: 0.16537668914927375\n",
      "Average test loss: 0.008191442718108495\n",
      "Epoch 71/300\n",
      "Average training loss: 0.1644715947707494\n",
      "Average test loss: 0.005897379889049464\n",
      "Epoch 72/300\n",
      "Average training loss: 0.16369411174456278\n",
      "Average test loss: 0.014983431175351144\n",
      "Epoch 73/300\n",
      "Average training loss: 0.16249407111273872\n",
      "Average test loss: 0.006154226224041648\n",
      "Epoch 74/300\n",
      "Average training loss: 0.16212566189633476\n",
      "Average test loss: 0.005985919784754515\n",
      "Epoch 75/300\n",
      "Average training loss: 0.16255089269744025\n",
      "Average test loss: 0.0059042631905112004\n",
      "Epoch 76/300\n",
      "Average training loss: 0.15984915945265027\n",
      "Average test loss: 0.006007303715373079\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1605745521850056\n",
      "Average test loss: 0.006471978524906768\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1588073943588469\n",
      "Average test loss: 0.006113860091401471\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1657381819619073\n",
      "Average test loss: 0.007211452675776349\n",
      "Epoch 80/300\n",
      "Average training loss: 30425.61304567015\n",
      "Average test loss: 0.15592027767830424\n",
      "Epoch 81/300\n",
      "Average training loss: 30.49840759616428\n",
      "Average test loss: 0.1282201816373401\n",
      "Epoch 82/300\n",
      "Average training loss: 25.464257720947266\n",
      "Average test loss: 0.01459286491986778\n",
      "Epoch 83/300\n",
      "Average training loss: 22.656751254611546\n",
      "Average test loss: 17.606300240503415\n",
      "Epoch 84/300\n",
      "Average training loss: 20.386544118245443\n",
      "Average test loss: 0.45146186846494674\n",
      "Epoch 85/300\n",
      "Average training loss: 18.41761432732476\n",
      "Average test loss: 0.009402556175159083\n",
      "Epoch 86/300\n",
      "Average training loss: 16.764458572387696\n",
      "Average test loss: 1.5017836015224457\n",
      "Epoch 87/300\n",
      "Average training loss: 15.39585173034668\n",
      "Average test loss: 0.033301255577968224\n",
      "Epoch 88/300\n",
      "Average training loss: 14.179832077874078\n",
      "Average test loss: 0.009011954608890746\n",
      "Epoch 89/300\n",
      "Average training loss: 13.078775971306694\n",
      "Average test loss: 0.011429622873663902\n",
      "Epoch 90/300\n",
      "Average training loss: 12.067299984402126\n",
      "Average test loss: 0.008344534582561916\n",
      "Epoch 91/300\n",
      "Average training loss: 11.077314531962077\n",
      "Average test loss: 0.009203399829152558\n",
      "Epoch 92/300\n",
      "Average training loss: 10.140223928663465\n",
      "Average test loss: 0.008115346238017082\n",
      "Epoch 93/300\n",
      "Average training loss: 9.322379137674968\n",
      "Average test loss: 0.008396762897984848\n",
      "Epoch 94/300\n",
      "Average training loss: 8.609416423373752\n",
      "Average test loss: 0.007368101310398844\n",
      "Epoch 95/300\n",
      "Average training loss: 7.883664420233832\n",
      "Average test loss: 1.2078128644227981\n",
      "Epoch 96/300\n",
      "Average training loss: 7.142993839263916\n",
      "Average test loss: 0.024956805198556847\n",
      "Epoch 97/300\n",
      "Average training loss: 6.444938851674398\n",
      "Average test loss: 0.00767894992356499\n",
      "Epoch 98/300\n",
      "Average training loss: 5.83750443564521\n",
      "Average test loss: 0.007337268273035685\n",
      "Epoch 99/300\n",
      "Average training loss: 5.299841729058159\n",
      "Average test loss: 0.8980454692310758\n",
      "Epoch 100/300\n",
      "Average training loss: 4.831514374203152\n",
      "Average test loss: 0.007136564191430807\n",
      "Epoch 101/300\n",
      "Average training loss: 4.411740968916151\n",
      "Average test loss: 0.00659758515862955\n",
      "Epoch 102/300\n",
      "Average training loss: 4.032307358635796\n",
      "Average test loss: 0.007298688349210554\n",
      "Epoch 103/300\n",
      "Average training loss: 3.67777602619595\n",
      "Average test loss: 0.011448383801099327\n",
      "Epoch 104/300\n",
      "Average training loss: 3.383138605753581\n",
      "Average test loss: 0.006960805024123854\n",
      "Epoch 105/300\n",
      "Average training loss: 3.1291161325242784\n",
      "Average test loss: 0.00784249006004797\n",
      "Epoch 106/300\n",
      "Average training loss: 2.893433835771349\n",
      "Average test loss: 0.006164507273170683\n",
      "Epoch 107/300\n",
      "Average training loss: 2.6674139783647326\n",
      "Average test loss: 0.006887677516788244\n",
      "Epoch 108/300\n",
      "Average training loss: 2.455673678927951\n",
      "Average test loss: 0.006608065510375632\n",
      "Epoch 109/300\n",
      "Average training loss: 2.2369335085550945\n",
      "Average test loss: 0.0073101387504074305\n",
      "Epoch 110/300\n",
      "Average training loss: 1.9860007202360366\n",
      "Average test loss: 0.007189787808805704\n",
      "Epoch 111/300\n",
      "Average training loss: 1.699536036491394\n",
      "Average test loss: 0.006080173860821459\n",
      "Epoch 112/300\n",
      "Average training loss: 1.4498305825127495\n",
      "Average test loss: 0.0060321367474065885\n",
      "Epoch 113/300\n",
      "Average training loss: 1.2282947255240546\n",
      "Average test loss: 0.014502033424874147\n",
      "Epoch 114/300\n",
      "Average training loss: 1.041798052681817\n",
      "Average test loss: 0.8153728909393152\n",
      "Epoch 115/300\n",
      "Average training loss: 0.8904698687659369\n",
      "Average test loss: 36.15459704615838\n",
      "Epoch 116/300\n",
      "Average training loss: 0.7636063383420308\n",
      "Average test loss: 0.0437293522324827\n",
      "Epoch 117/300\n",
      "Average training loss: 0.6563494917551677\n",
      "Average test loss: 0.021698583165804544\n",
      "Epoch 118/300\n",
      "Average training loss: 0.575240244017707\n",
      "Average test loss: 0.006832001322673427\n",
      "Epoch 119/300\n",
      "Average training loss: 0.5184241579108768\n",
      "Average test loss: 0.010023475289344788\n",
      "Epoch 120/300\n",
      "Average training loss: 0.4701328370571137\n",
      "Average test loss: 0.005913187383570605\n",
      "Epoch 121/300\n",
      "Average training loss: 0.4283496031761169\n",
      "Average test loss: 0.009461371775302622\n",
      "Epoch 122/300\n",
      "Average training loss: 0.39570968103408816\n",
      "Average test loss: 4.046446003089349\n",
      "Epoch 123/300\n",
      "Average training loss: 0.3675308068593343\n",
      "Average test loss: 35.14299498095115\n",
      "Epoch 124/300\n",
      "Average training loss: 0.3482896891434987\n",
      "Average test loss: 0.007425595777730147\n",
      "Epoch 125/300\n",
      "Average training loss: 0.3269353480074141\n",
      "Average test loss: 0.006167956810444594\n",
      "Epoch 126/300\n",
      "Average training loss: 0.30829090073373583\n",
      "Average test loss: 0.0060581867194010154\n",
      "Epoch 127/300\n",
      "Average training loss: 0.2895361971060435\n",
      "Average test loss: 0.006103117742886146\n",
      "Epoch 128/300\n",
      "Average training loss: 0.26029779566658867\n",
      "Average test loss: 0.0060867019229465065\n",
      "Epoch 129/300\n",
      "Average training loss: 0.22389305629995135\n",
      "Average test loss: 0.005861535727977753\n",
      "Epoch 130/300\n",
      "Average training loss: 0.20896955031818815\n",
      "Average test loss: 0.005901571377077037\n",
      "Epoch 131/300\n",
      "Average training loss: 0.21125752394729191\n",
      "Average test loss: 0.009433617491688993\n",
      "Epoch 132/300\n",
      "Average training loss: 0.20013348812527126\n",
      "Average test loss: 0.024744207496444383\n",
      "Epoch 133/300\n",
      "Average training loss: 0.19250584823555417\n",
      "Average test loss: 0.006956997575031387\n",
      "Epoch 134/300\n",
      "Average training loss: 0.18882647812366485\n",
      "Average test loss: 148.33656997246254\n",
      "Epoch 135/300\n",
      "Average training loss: 0.18582945291201275\n",
      "Average test loss: 0.00884557603382402\n",
      "Epoch 136/300\n",
      "Average training loss: 0.1828368941810396\n",
      "Average test loss: 0.005849416757209434\n",
      "Epoch 137/300\n",
      "Average training loss: 0.1798930951886707\n",
      "Average test loss: 0.0060369835363494025\n",
      "Epoch 138/300\n",
      "Average training loss: 0.17776681775516934\n",
      "Average test loss: 0.005947888495193588\n",
      "Epoch 139/300\n",
      "Average training loss: 0.1750887160698573\n",
      "Average test loss: 0.005835205539233155\n",
      "Epoch 140/300\n",
      "Average training loss: 0.17351728886365891\n",
      "Average test loss: 0.006224898198826445\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1710062209367752\n",
      "Average test loss: 0.007744318269193172\n",
      "Epoch 142/300\n",
      "Average training loss: 0.16975836780336168\n",
      "Average test loss: 0.006446731470525264\n",
      "Epoch 143/300\n",
      "Average training loss: 0.16729306021001603\n",
      "Average test loss: 0.0059193494725558495\n",
      "Epoch 144/300\n",
      "Average training loss: 0.16883652018176185\n",
      "Average test loss: 0.007427584875168072\n",
      "Epoch 145/300\n",
      "Average training loss: 0.1644125451379352\n",
      "Average test loss: 0.010630853621910016\n",
      "Epoch 146/300\n",
      "Average training loss: 0.16464771663480335\n",
      "Average test loss: 0.09820823664963245\n",
      "Epoch 147/300\n",
      "Average training loss: 0.16090424193276298\n",
      "Average test loss: 0.006375638934887118\n",
      "Epoch 148/300\n",
      "Average training loss: 0.161646577556928\n",
      "Average test loss: 0.006727452005777094\n",
      "Epoch 149/300\n",
      "Average training loss: 0.158991826944881\n",
      "Average test loss: 0.0058118157705499066\n",
      "Epoch 150/300\n",
      "Average training loss: 0.15864964075883228\n",
      "Average test loss: 0.010003255609009\n",
      "Epoch 151/300\n",
      "Average training loss: 0.1579636625316408\n",
      "Average test loss: 0.006651190734571881\n",
      "Epoch 152/300\n",
      "Average training loss: 0.15662621854411232\n",
      "Average test loss: 0.006391525573614571\n",
      "Epoch 153/300\n",
      "Average training loss: 0.1555077881945504\n",
      "Average test loss: 0.005963446048191852\n",
      "Epoch 154/300\n",
      "Average training loss: 0.15552125590377383\n",
      "Average test loss: 0.0061889124525090055\n",
      "Epoch 155/300\n",
      "Average training loss: 0.15437943094306522\n",
      "Average test loss: 0.006452279099987613\n",
      "Epoch 156/300\n",
      "Average training loss: 0.15232684581809575\n",
      "Average test loss: 0.3798293144206206\n",
      "Epoch 157/300\n",
      "Average training loss: 0.15321101636356776\n",
      "Average test loss: 0.006308345452778869\n",
      "Epoch 158/300\n",
      "Average training loss: 0.15152253958914014\n",
      "Average test loss: 0.0061309875527189835\n",
      "Epoch 159/300\n",
      "Average training loss: 0.15057394003868102\n",
      "Average test loss: 0.007026844823112091\n",
      "Epoch 160/300\n",
      "Average training loss: 0.1866932958761851\n",
      "Average test loss: 0.006543324848740465\n",
      "Epoch 161/300\n",
      "Average training loss: 0.15928444839848413\n",
      "Average test loss: 0.014946533884439204\n",
      "Epoch 162/300\n",
      "Average training loss: 0.15088256870375738\n",
      "Average test loss: 0.017444052865107855\n",
      "Epoch 163/300\n",
      "Average training loss: 0.14876134921444786\n",
      "Average test loss: 0.006167699611021413\n",
      "Epoch 164/300\n",
      "Average training loss: 0.14777545475959777\n",
      "Average test loss: 0.006130699477261967\n",
      "Epoch 165/300\n",
      "Average training loss: 0.16562265259689754\n",
      "Average test loss: 0.006678201970126894\n",
      "Epoch 166/300\n",
      "Average training loss: 0.14770462058650122\n",
      "Average test loss: 0.07438889101478789\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1467956823905309\n",
      "Average test loss: 0.006167332742156254\n",
      "Epoch 168/300\n",
      "Average training loss: 0.14678653352790408\n",
      "Average test loss: 0.008533242354376448\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1476627631849713\n",
      "Average test loss: 0.5046867264244291\n",
      "Epoch 170/300\n",
      "Average training loss: 0.14592165235678356\n",
      "Average test loss: 0.0063232386687563525\n",
      "Epoch 171/300\n",
      "Average training loss: 0.15085387505425346\n",
      "Average test loss: 0.0062474373144408065\n",
      "Epoch 172/300\n",
      "Average training loss: 0.14415059158537122\n",
      "Average test loss: 0.006390889795290099\n",
      "Epoch 173/300\n",
      "Average training loss: 0.14458578822347853\n",
      "Average test loss: 0.006360978003591299\n",
      "Epoch 174/300\n",
      "Average training loss: 0.14586382198333742\n",
      "Average test loss: 0.051644383235110176\n",
      "Epoch 175/300\n",
      "Average training loss: 0.14349669278992547\n",
      "Average test loss: 0.0061786665564609895\n",
      "Epoch 176/300\n",
      "Average training loss: 0.14399673639403449\n",
      "Average test loss: 0.00599998857287897\n",
      "Epoch 177/300\n",
      "Average training loss: 0.14291938789685568\n",
      "Average test loss: 0.010434031543632349\n",
      "Epoch 178/300\n",
      "Average training loss: 0.15106814997063742\n",
      "Average test loss: 0.0062361362779306045\n",
      "Epoch 179/300\n",
      "Average training loss: 0.14120122026072607\n",
      "Average test loss: 0.006122907296237019\n",
      "Epoch 180/300\n",
      "Average training loss: 0.1445302129983902\n",
      "Average test loss: 0.0065162601218455365\n",
      "Epoch 181/300\n",
      "Average training loss: 0.14125755033228132\n",
      "Average test loss: 0.006022880651884609\n",
      "Epoch 182/300\n",
      "Average training loss: 0.1402144103580051\n",
      "Average test loss: 0.006212531934802731\n",
      "Epoch 183/300\n",
      "Average training loss: 0.13979701138867273\n",
      "Average test loss: 0.0062320989424155815\n",
      "Epoch 184/300\n",
      "Average training loss: 0.14130537806616889\n",
      "Average test loss: 0.006833937741402123\n",
      "Epoch 185/300\n",
      "Average training loss: 0.1387728960381614\n",
      "Average test loss: 0.006245868604216311\n",
      "Epoch 186/300\n",
      "Average training loss: 0.1394329095085462\n",
      "Average test loss: 0.0067586034118301335\n",
      "Epoch 187/300\n",
      "Average training loss: 0.14119868510299258\n",
      "Average test loss: 0.006655889412595166\n",
      "Epoch 188/300\n",
      "Average training loss: 0.15512490038077037\n",
      "Average test loss: 0.0060909057214028305\n",
      "Epoch 189/300\n",
      "Average training loss: 976.0827878105773\n",
      "Average test loss: 0.03416261637210846\n",
      "Epoch 190/300\n",
      "Average training loss: 16.881394608391656\n",
      "Average test loss: 0.015126715311573611\n",
      "Epoch 191/300\n",
      "Average training loss: 12.483726737976074\n",
      "Average test loss: 0.25453084756268396\n",
      "Epoch 192/300\n",
      "Average training loss: 10.749255212571885\n",
      "Average test loss: 0.013902158186667495\n",
      "Epoch 193/300\n",
      "Average training loss: 9.68181520928277\n",
      "Average test loss: 0.01007854287740257\n",
      "Epoch 194/300\n",
      "Average training loss: 8.832912052578397\n",
      "Average test loss: 0.014004913866519928\n",
      "Epoch 195/300\n",
      "Average training loss: 8.078246557447645\n",
      "Average test loss: 0.008505070865568188\n",
      "Epoch 196/300\n",
      "Average training loss: 7.326883397420247\n",
      "Average test loss: 0.009658993823660745\n",
      "Epoch 197/300\n",
      "Average training loss: 6.526644599066841\n",
      "Average test loss: 0.00870596091532045\n",
      "Epoch 198/300\n",
      "Average training loss: 5.825344418843588\n",
      "Average test loss: 0.009218842534555328\n",
      "Epoch 199/300\n",
      "Average training loss: 5.249225340949165\n",
      "Average test loss: 0.007986483049061563\n",
      "Epoch 200/300\n",
      "Average training loss: 4.731222808414035\n",
      "Average test loss: 0.007426651081691185\n",
      "Epoch 201/300\n",
      "Average training loss: 4.279560448116726\n",
      "Average test loss: 0.008460214607417584\n",
      "Epoch 202/300\n",
      "Average training loss: 3.8688854849073624\n",
      "Average test loss: 0.007240713112056255\n",
      "Epoch 203/300\n",
      "Average training loss: 3.491087504280938\n",
      "Average test loss: 0.006917484794639879\n",
      "Epoch 204/300\n",
      "Average training loss: 3.1505234820048016\n",
      "Average test loss: 0.010169040743675497\n",
      "Epoch 205/300\n",
      "Average training loss: 2.837500582800971\n",
      "Average test loss: 0.008683490494059191\n",
      "Epoch 206/300\n",
      "Average training loss: 2.547394370396932\n",
      "Average test loss: 0.006524253283523851\n",
      "Epoch 207/300\n",
      "Average training loss: 2.2756393574608698\n",
      "Average test loss: 0.00652783098568519\n",
      "Epoch 208/300\n",
      "Average training loss: 2.0176551694869995\n",
      "Average test loss: 0.006719835047920545\n",
      "Epoch 209/300\n",
      "Average training loss: 1.7762964522043865\n",
      "Average test loss: 0.0064129186785883375\n",
      "Epoch 210/300\n",
      "Average training loss: 1.5533223536809286\n",
      "Average test loss: 0.021421800600157845\n",
      "Epoch 211/300\n",
      "Average training loss: 1.3465735885832044\n",
      "Average test loss: 0.0644298027108113\n",
      "Epoch 212/300\n",
      "Average training loss: 1.1712663689719307\n",
      "Average test loss: 0.006827352003504833\n",
      "Epoch 213/300\n",
      "Average training loss: 1.012215109507243\n",
      "Average test loss: 0.0060296450257301334\n",
      "Epoch 214/300\n",
      "Average training loss: 0.8803886578877766\n",
      "Average test loss: 0.006360421648869912\n",
      "Epoch 215/300\n",
      "Average training loss: 0.764361065811581\n",
      "Average test loss: 0.014247325114077992\n",
      "Epoch 216/300\n",
      "Average training loss: 0.665842099931505\n",
      "Average test loss: 0.0062144865182538825\n",
      "Epoch 217/300\n",
      "Average training loss: 0.5779563225110372\n",
      "Average test loss: 0.006293864770895905\n",
      "Epoch 218/300\n",
      "Average training loss: 0.49874588635232714\n",
      "Average test loss: 0.0067481860319773355\n",
      "Epoch 219/300\n",
      "Average training loss: 0.4287701454957326\n",
      "Average test loss: 0.008242738709681565\n",
      "Epoch 220/300\n",
      "Average training loss: 0.3712790618472629\n",
      "Average test loss: 0.005966648062070211\n",
      "Epoch 221/300\n",
      "Average training loss: 0.3196088262663947\n",
      "Average test loss: 0.007025975475294722\n",
      "Epoch 222/300\n",
      "Average training loss: 0.2797432570722368\n",
      "Average test loss: 1.552339379949702\n",
      "Epoch 223/300\n",
      "Average training loss: 0.25365764202011953\n",
      "Average test loss: 0.005906766432854865\n",
      "Epoch 224/300\n",
      "Average training loss: 0.27115158772468567\n",
      "Average test loss: 0.01128882275852892\n",
      "Epoch 225/300\n",
      "Average training loss: 0.22789694905281066\n",
      "Average test loss: 0.005872457835409376\n",
      "Epoch 226/300\n",
      "Average training loss: 0.21376578998565673\n",
      "Average test loss: 8.51107774617937\n",
      "Epoch 227/300\n",
      "Average training loss: 0.20376866812176175\n",
      "Average test loss: 0.005838375832057661\n",
      "Epoch 228/300\n",
      "Average training loss: 0.19677190017700194\n",
      "Average test loss: 0.0059400565905703435\n",
      "Epoch 229/300\n",
      "Average training loss: 0.19068948998716143\n",
      "Average test loss: 0.00878686350170109\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1868355141878128\n",
      "Average test loss: 0.005938864653723107\n",
      "Epoch 231/300\n",
      "Average training loss: 0.18075533678796557\n",
      "Average test loss: 0.005933618893225988\n",
      "Epoch 232/300\n",
      "Average training loss: 0.17661345521608987\n",
      "Average test loss: 0.005864242045829694\n",
      "Epoch 233/300\n",
      "Average training loss: 0.17539960951275296\n",
      "Average test loss: 0.012658245359443956\n",
      "Epoch 234/300\n",
      "Average training loss: 0.17059050031503042\n",
      "Average test loss: 0.006080678766075935\n",
      "Epoch 235/300\n",
      "Average training loss: 680.5504682660236\n",
      "Average test loss: 1429.6406160200288\n",
      "Epoch 236/300\n",
      "Average training loss: 13.582715872870551\n",
      "Average test loss: 4.323976283629736\n",
      "Epoch 237/300\n",
      "Average training loss: 11.054670180426703\n",
      "Average test loss: 0.15640637855728468\n",
      "Epoch 238/300\n",
      "Average training loss: 9.748962815178766\n",
      "Average test loss: 0.008027630500495434\n",
      "Epoch 239/300\n",
      "Average training loss: 8.734981829325358\n",
      "Average test loss: 0.007772662294821607\n",
      "Epoch 240/300\n",
      "Average training loss: 7.849411699930827\n",
      "Average test loss: 0.012430005171232753\n",
      "Epoch 241/300\n",
      "Average training loss: 7.032802480485704\n",
      "Average test loss: 0.007434681024816301\n",
      "Epoch 242/300\n",
      "Average training loss: 6.27158889134725\n",
      "Average test loss: 0.007242886791212691\n",
      "Epoch 243/300\n",
      "Average training loss: 5.601378392537435\n",
      "Average test loss: 0.006997787548022137\n",
      "Epoch 244/300\n",
      "Average training loss: 5.0384007602267795\n",
      "Average test loss: 0.006956009059316582\n",
      "Epoch 245/300\n",
      "Average training loss: 4.541396578894721\n",
      "Average test loss: 0.01139200091196431\n",
      "Epoch 246/300\n",
      "Average training loss: 4.026900362438626\n",
      "Average test loss: 0.06933410369646219\n",
      "Epoch 247/300\n",
      "Average training loss: 3.497974848005507\n",
      "Average test loss: 0.006600883611374431\n",
      "Epoch 248/300\n",
      "Average training loss: 3.0614916106330026\n",
      "Average test loss: 0.007044475862549411\n",
      "Epoch 249/300\n",
      "Average training loss: 2.6833411305745445\n",
      "Average test loss: 0.006391210941390859\n",
      "Epoch 250/300\n",
      "Average training loss: 2.3152694068484836\n",
      "Average test loss: 0.006229602261963818\n",
      "Epoch 251/300\n",
      "Average training loss: 1.9623243480258519\n",
      "Average test loss: 0.006296630629234844\n",
      "Epoch 252/300\n",
      "Average training loss: 1.6424030635621814\n",
      "Average test loss: 0.006750378992408514\n",
      "Epoch 253/300\n",
      "Average training loss: 1.3671218418545192\n",
      "Average test loss: 0.006142929688096046\n",
      "Epoch 254/300\n",
      "Average training loss: 1.1537001215616862\n",
      "Average test loss: 0.010244420618646674\n",
      "Epoch 255/300\n",
      "Average training loss: 0.9885665273136562\n",
      "Average test loss: 16.61084779079755\n",
      "Epoch 256/300\n",
      "Average training loss: 0.8559773974948459\n",
      "Average test loss: 0.026234447457724148\n",
      "Epoch 257/300\n",
      "Average training loss: 0.7428483839035034\n",
      "Average test loss: 0.02370890904797448\n",
      "Epoch 258/300\n",
      "Average training loss: 0.6432008788320753\n",
      "Average test loss: 0.006318275312168731\n",
      "Epoch 259/300\n",
      "Average training loss: 0.551857550991906\n",
      "Average test loss: 0.010811762909094493\n",
      "Epoch 260/300\n",
      "Average training loss: 0.45804349364174735\n",
      "Average test loss: 0.055852202905548945\n",
      "Epoch 261/300\n",
      "Average training loss: 0.3743380729622311\n",
      "Average test loss: 0.0060431514419615265\n",
      "Epoch 262/300\n",
      "Average training loss: 0.3269424492783017\n",
      "Average test loss: 0.006482860074688991\n",
      "Epoch 263/300\n",
      "Average training loss: 0.291469452990426\n",
      "Average test loss: 0.006321843454407321\n",
      "Epoch 264/300\n",
      "Average training loss: 0.263691586083836\n",
      "Average test loss: 0.005906978100538254\n",
      "Epoch 265/300\n",
      "Average training loss: 0.24305531409051684\n",
      "Average test loss: 0.006283797637455993\n",
      "Epoch 266/300\n",
      "Average training loss: 0.22766252198484208\n",
      "Average test loss: 0.006105671358605226\n",
      "Epoch 267/300\n",
      "Average training loss: 0.2118466131952074\n",
      "Average test loss: 0.006405125211510394\n",
      "Epoch 268/300\n",
      "Average training loss: 0.2006162555747562\n",
      "Average test loss: 0.006001471637437741\n",
      "Epoch 269/300\n",
      "Average training loss: 0.19319035661882825\n",
      "Average test loss: 0.00868103895170821\n",
      "Epoch 270/300\n",
      "Average training loss: 0.19467242236269844\n",
      "Average test loss: 0.0063215747794343365\n",
      "Epoch 271/300\n",
      "Average training loss: 0.1835017211172316\n",
      "Average test loss: 0.006223060660478142\n",
      "Epoch 272/300\n",
      "Average training loss: 0.18055036987198725\n",
      "Average test loss: 0.006477473115548492\n",
      "Epoch 273/300\n",
      "Average training loss: 0.17931545146306355\n",
      "Average test loss: 0.00616655517824822\n",
      "Epoch 274/300\n",
      "Average training loss: 0.17813478434085847\n",
      "Average test loss: 0.0059883029055264264\n",
      "Epoch 275/300\n",
      "Average training loss: 0.17068532872200012\n",
      "Average test loss: 0.006195946332481173\n",
      "Epoch 276/300\n",
      "Average training loss: 0.16769262235694463\n",
      "Average test loss: 0.005944214503384298\n",
      "Epoch 277/300\n",
      "Average training loss: 0.1723041994439231\n",
      "Average test loss: 0.005887026190757752\n",
      "Epoch 278/300\n",
      "Average training loss: 0.17843341567781237\n",
      "Average test loss: 2.324672105338838\n",
      "Epoch 279/300\n",
      "Average training loss: 0.1610763730737898\n",
      "Average test loss: 0.005997013077967696\n",
      "Epoch 280/300\n",
      "Average training loss: 0.15665320689148374\n",
      "Average test loss: 0.00794249269945754\n",
      "Epoch 281/300\n",
      "Average training loss: 0.15587900611427094\n",
      "Average test loss: 0.5491338533957799\n",
      "Epoch 282/300\n",
      "Average training loss: 0.15074310700098673\n",
      "Average test loss: 0.006273950018816524\n",
      "Epoch 283/300\n",
      "Average training loss: 0.1486141212383906\n",
      "Average test loss: 0.006475689683937364\n",
      "Epoch 284/300\n",
      "Average training loss: 0.1486720952987671\n",
      "Average test loss: 0.007196595709356997\n",
      "Epoch 285/300\n",
      "Average training loss: 0.14579325351450179\n",
      "Average test loss: 0.006186876710090372\n",
      "Epoch 286/300\n",
      "Average training loss: 0.1620201267864969\n",
      "Average test loss: 0.006162281728453106\n",
      "Epoch 287/300\n",
      "Average training loss: 0.14428574671347935\n",
      "Average test loss: 0.0062972908454636736\n",
      "Epoch 288/300\n",
      "Average training loss: 0.14325712552335526\n",
      "Average test loss: 0.00606266849156883\n",
      "Epoch 289/300\n",
      "Average training loss: 0.14920341653294034\n",
      "Average test loss: 0.01152438780417045\n",
      "Epoch 290/300\n",
      "Average training loss: 0.14420719684494868\n",
      "Average test loss: 0.006480368946161535\n",
      "Epoch 291/300\n",
      "Average training loss: 0.1430563968817393\n",
      "Average test loss: 0.007820930944548712\n",
      "Epoch 292/300\n",
      "Average training loss: 0.14237867355346678\n",
      "Average test loss: 0.006289607232643498\n",
      "Epoch 293/300\n",
      "Average training loss: 0.14161401907602947\n",
      "Average test loss: 3.0455016812715265\n",
      "Epoch 294/300\n",
      "Average training loss: 0.14720966662300958\n",
      "Average test loss: 0.006340787703792254\n",
      "Epoch 295/300\n",
      "Average training loss: 0.13995909647146862\n",
      "Average test loss: 0.006935887054436736\n",
      "Epoch 296/300\n",
      "Average training loss: 0.18220389572779339\n",
      "Average test loss: 0.0062865148050089675\n",
      "Epoch 297/300\n",
      "Average training loss: 0.14095512986183167\n",
      "Average test loss: 0.006281151039732827\n",
      "Epoch 298/300\n",
      "Average training loss: 0.1395832034614351\n",
      "Average test loss: 0.006176396917551756\n",
      "Epoch 299/300\n",
      "Average training loss: 0.13669365301397113\n",
      "Average test loss: 0.006250974995808469\n",
      "Epoch 300/300\n",
      "Average training loss: 0.13768380627367233\n",
      "Average test loss: 0.0062155492371983\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 17.449439173380533\n",
      "Average test loss: 37.51556698438856\n",
      "Epoch 2/300\n",
      "Average training loss: 8.2786037173801\n",
      "Average test loss: 0.007693130045715306\n",
      "Epoch 3/300\n",
      "Average training loss: 6.098061813354493\n",
      "Average test loss: 0.006612831281705035\n",
      "Epoch 4/300\n",
      "Average training loss: 5.2146051928202315\n",
      "Average test loss: 0.006376517174144586\n",
      "Epoch 5/300\n",
      "Average training loss: 4.521860262976753\n",
      "Average test loss: 0.006984403447558483\n",
      "Epoch 6/300\n",
      "Average training loss: 3.8414696275922986\n",
      "Average test loss: 0.005941515497863293\n",
      "Epoch 7/300\n",
      "Average training loss: 3.1925229725307886\n",
      "Average test loss: 0.006073408709632026\n",
      "Epoch 8/300\n",
      "Average training loss: 2.6488854546017118\n",
      "Average test loss: 0.005605386955870522\n",
      "Epoch 9/300\n",
      "Average training loss: 2.2407047237820095\n",
      "Average test loss: 0.005797199149512582\n",
      "Epoch 10/300\n",
      "Average training loss: 1.9922176115247938\n",
      "Average test loss: 0.005324376725074318\n",
      "Epoch 11/300\n",
      "Average training loss: 1.628041857931349\n",
      "Average test loss: 0.005020646476497252\n",
      "Epoch 12/300\n",
      "Average training loss: 1.257450550503201\n",
      "Average test loss: 0.004808367721736431\n",
      "Epoch 13/300\n",
      "Average training loss: 1.022817217932807\n",
      "Average test loss: 0.004562682504455249\n",
      "Epoch 14/300\n",
      "Average training loss: 0.8507248900731404\n",
      "Average test loss: 0.004548949046060443\n",
      "Epoch 15/300\n",
      "Average training loss: 0.7044769354396396\n",
      "Average test loss: 0.004450952796472444\n",
      "Epoch 16/300\n",
      "Average training loss: 0.6002263953420851\n",
      "Average test loss: 0.00448445257710086\n",
      "Epoch 17/300\n",
      "Average training loss: 0.5177832139068179\n",
      "Average test loss: 0.004701829453723298\n",
      "Epoch 18/300\n",
      "Average training loss: 0.4571408722135756\n",
      "Average test loss: 0.004187457737823327\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4077260304292043\n",
      "Average test loss: 0.00685121755881442\n",
      "Epoch 20/300\n",
      "Average training loss: 0.3640959210925632\n",
      "Average test loss: 0.00397649275780552\n",
      "Epoch 21/300\n",
      "Average training loss: 0.33059736691580877\n",
      "Average test loss: 0.004392743275811275\n",
      "Epoch 22/300\n",
      "Average training loss: 0.30104369503921935\n",
      "Average test loss: 0.0039142164247524406\n",
      "Epoch 23/300\n",
      "Average training loss: 0.27486218384901684\n",
      "Average test loss: 0.003953650927378071\n",
      "Epoch 24/300\n",
      "Average training loss: 0.25448719088236493\n",
      "Average test loss: 0.003967780837996138\n",
      "Epoch 25/300\n",
      "Average training loss: 0.23687192897001902\n",
      "Average test loss: 0.0039423738475888965\n",
      "Epoch 26/300\n",
      "Average training loss: 0.22244568150573307\n",
      "Average test loss: 0.00442438109839956\n",
      "Epoch 27/300\n",
      "Average training loss: 0.21136860753430262\n",
      "Average test loss: 0.004669597638563977\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2017115261024899\n",
      "Average test loss: 0.0036337646641251115\n",
      "Epoch 29/300\n",
      "Average training loss: 0.19298514768812391\n",
      "Average test loss: 0.004248997259471151\n",
      "Epoch 30/300\n",
      "Average training loss: 0.18520309126377105\n",
      "Average test loss: 0.0036837415794531503\n",
      "Epoch 31/300\n",
      "Average training loss: 0.17945323905679914\n",
      "Average test loss: 0.003761033033952117\n",
      "Epoch 32/300\n",
      "Average training loss: 0.17399839100572798\n",
      "Average test loss: 0.003504154721274972\n",
      "Epoch 33/300\n",
      "Average training loss: 0.16791235058837467\n",
      "Average test loss: 0.003873319898214605\n",
      "Epoch 34/300\n",
      "Average training loss: 0.16219499670134652\n",
      "Average test loss: 0.003482790679153469\n",
      "Epoch 35/300\n",
      "Average training loss: 0.15807980901665158\n",
      "Average test loss: 0.0034602283435977168\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15461974981096055\n",
      "Average test loss: 0.0034980322205358082\n",
      "Epoch 37/300\n",
      "Average training loss: 0.15024326962894863\n",
      "Average test loss: 0.0036237905505630704\n",
      "Epoch 38/300\n",
      "Average training loss: 0.14577162272400326\n",
      "Average test loss: 0.0034592772190355594\n",
      "Epoch 39/300\n",
      "Average training loss: 0.14305210216840109\n",
      "Average test loss: 0.004983540643420484\n",
      "Epoch 40/300\n",
      "Average training loss: 0.13886625759469137\n",
      "Average test loss: 0.003476169179711077\n",
      "Epoch 41/300\n",
      "Average training loss: 0.13498376878764895\n",
      "Average test loss: 0.003356821685201592\n",
      "Epoch 42/300\n",
      "Average training loss: 0.13259110662672255\n",
      "Average test loss: 0.003427257859458526\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1290392327507337\n",
      "Average test loss: 0.004400438340794709\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12729197693533367\n",
      "Average test loss: 0.003709091541667779\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12455830067396163\n",
      "Average test loss: 0.0037313189055356716\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12299391048484379\n",
      "Average test loss: 0.0034439652491774823\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12147179410192702\n",
      "Average test loss: 0.003757513003837731\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11870802904499901\n",
      "Average test loss: 0.003630307594107257\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1170553488665157\n",
      "Average test loss: 0.003414147677934832\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11705212051338619\n",
      "Average test loss: 0.15699253071182304\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11498121468226115\n",
      "Average test loss: 0.0033701728348516755\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11281671566433377\n",
      "Average test loss: 0.0033367854877271587\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11297447561555439\n",
      "Average test loss: 0.009714497984697422\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11065390082862642\n",
      "Average test loss: 0.0036089744369188943\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11008387831846873\n",
      "Average test loss: 0.003605415464275413\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10822793849309285\n",
      "Average test loss: 0.007022578432742092\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10817747700214386\n",
      "Average test loss: 0.003331446269734038\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10642563625839022\n",
      "Average test loss: 0.0033182775759034687\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1078809380001492\n",
      "Average test loss: 0.003359347397668494\n",
      "Epoch 60/300\n",
      "Average training loss: 52.88321290832096\n",
      "Average test loss: 0.004844108367131816\n",
      "Epoch 61/300\n",
      "Average training loss: 5.638152907477485\n",
      "Average test loss: 0.004459079455998209\n",
      "Epoch 62/300\n",
      "Average training loss: 4.0645512286292185\n",
      "Average test loss: 0.004429269353755646\n",
      "Epoch 63/300\n",
      "Average training loss: 3.0737143109639486\n",
      "Average test loss: 0.0041908986986511285\n",
      "Epoch 64/300\n",
      "Average training loss: 2.4012854457431367\n",
      "Average test loss: 0.004025828551501035\n",
      "Epoch 65/300\n",
      "Average training loss: 1.9089200018776789\n",
      "Average test loss: 0.003909839262978898\n",
      "Epoch 66/300\n",
      "Average training loss: 1.4979902137120564\n",
      "Average test loss: 0.00508459551508228\n",
      "Epoch 67/300\n",
      "Average training loss: 1.1553004520734151\n",
      "Average test loss: 0.0037696233395900993\n",
      "Epoch 68/300\n",
      "Average training loss: 0.878013068623013\n",
      "Average test loss: 0.0036764158395429453\n",
      "Epoch 69/300\n",
      "Average training loss: 0.6554268356959025\n",
      "Average test loss: 0.003616588906902406\n",
      "Epoch 70/300\n",
      "Average training loss: 0.5015777713457743\n",
      "Average test loss: 0.0036133592199120257\n",
      "Epoch 71/300\n",
      "Average training loss: 0.40269935043652855\n",
      "Average test loss: 0.0037006826069619922\n",
      "Epoch 72/300\n",
      "Average training loss: 0.3356660797331068\n",
      "Average test loss: 0.0035984542696840234\n",
      "Epoch 73/300\n",
      "Average training loss: 0.2846040600405799\n",
      "Average test loss: 0.003483113768614001\n",
      "Epoch 74/300\n",
      "Average training loss: 0.24687263316578334\n",
      "Average test loss: 0.0034281958679979047\n",
      "Epoch 75/300\n",
      "Average training loss: 0.21795477724075318\n",
      "Average test loss: 0.0034553126100864673\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1960076438585917\n",
      "Average test loss: 0.0034212071194003025\n",
      "Epoch 77/300\n",
      "Average training loss: 0.17829375416702695\n",
      "Average test loss: 0.0034521994387937916\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1669165462652842\n",
      "Average test loss: 0.0035066633129285443\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1582744714948866\n",
      "Average test loss: 0.003386880545152558\n",
      "Epoch 80/300\n",
      "Average training loss: 0.15178607518143125\n",
      "Average test loss: 0.0034605354447331694\n",
      "Epoch 81/300\n",
      "Average training loss: 0.14628107093440162\n",
      "Average test loss: 0.003476501941887869\n",
      "Epoch 82/300\n",
      "Average training loss: 0.14167546531889175\n",
      "Average test loss: 0.0033972416582206885\n",
      "Epoch 83/300\n",
      "Average training loss: 0.13754619963963827\n",
      "Average test loss: 0.0033219058439135554\n",
      "Epoch 84/300\n",
      "Average training loss: 0.13432550851504008\n",
      "Average test loss: 0.003428680243798428\n",
      "Epoch 85/300\n",
      "Average training loss: 0.131135626786285\n",
      "Average test loss: 0.0033094783760607245\n",
      "Epoch 86/300\n",
      "Average training loss: 0.12835250161091485\n",
      "Average test loss: 0.004976308128693038\n",
      "Epoch 87/300\n",
      "Average training loss: 0.12576055543290243\n",
      "Average test loss: 0.0033092625863436194\n",
      "Epoch 88/300\n",
      "Average training loss: 0.1232775024705463\n",
      "Average test loss: 0.0034946030349367195\n",
      "Epoch 89/300\n",
      "Average training loss: 0.1215828293495708\n",
      "Average test loss: 0.013841824090315235\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11955590467982823\n",
      "Average test loss: 0.003674490416008565\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11767916176054213\n",
      "Average test loss: 0.004942093099984858\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1160020846525828\n",
      "Average test loss: 0.0033755646273493766\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11461004041963153\n",
      "Average test loss: 0.003678873294757472\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11373163657718234\n",
      "Average test loss: 0.0033472229190584686\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11245992275079092\n",
      "Average test loss: 0.003990637216923966\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11105969428353839\n",
      "Average test loss: 0.0035182040023307008\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10999623505936729\n",
      "Average test loss: 0.003371307339519262\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10946923688385221\n",
      "Average test loss: 0.0033406353735675413\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10813359468513065\n",
      "Average test loss: 0.003385688313179546\n",
      "Epoch 100/300\n",
      "Average training loss: 0.1080186392598682\n",
      "Average test loss: 0.005415468933474686\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10655214599106047\n",
      "Average test loss: 0.004092394182665481\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10596526841653718\n",
      "Average test loss: 0.007798035909318261\n",
      "Epoch 103/300\n",
      "Average training loss: 0.1049327853653166\n",
      "Average test loss: 0.004000869190734294\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10562600608004465\n",
      "Average test loss: 0.0034818404333459008\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10361528417799208\n",
      "Average test loss: 0.003633856593320767\n",
      "Epoch 106/300\n",
      "Average training loss: 0.1029301406343778\n",
      "Average test loss: 0.0043847823561065724\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11284027816851934\n",
      "Average test loss: 0.0035316095476349194\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10440567628542582\n",
      "Average test loss: 0.0034864909464700352\n",
      "Epoch 109/300\n",
      "Average training loss: 0.1015982594953643\n",
      "Average test loss: 0.003456786884408858\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10064032280445098\n",
      "Average test loss: 0.003671950266799993\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10022650051116944\n",
      "Average test loss: 0.0037237225294941\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09935695805814532\n",
      "Average test loss: 0.003410901404192878\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09921754038333892\n",
      "Average test loss: 0.003479713523139556\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09876217492421469\n",
      "Average test loss: 0.0033882592080367938\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09774491477674908\n",
      "Average test loss: 0.003413577014166448\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09695998451444837\n",
      "Average test loss: 0.005413794040679932\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09637042766809463\n",
      "Average test loss: 0.004121960977506307\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09572885570923487\n",
      "Average test loss: 0.003514472137722704\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09493068368236224\n",
      "Average test loss: 0.011408720198604796\n",
      "Epoch 120/300\n",
      "Average training loss: 22.313785101360747\n",
      "Average test loss: 2971.235115751182\n",
      "Epoch 121/300\n",
      "Average training loss: 3.64950332154168\n",
      "Average test loss: 0.004471255999886328\n",
      "Epoch 122/300\n",
      "Average training loss: 2.233275021870931\n",
      "Average test loss: 0.004511386578902602\n",
      "Epoch 123/300\n",
      "Average training loss: 1.6672331665886773\n",
      "Average test loss: 0.004100525543921524\n",
      "Epoch 124/300\n",
      "Average training loss: 1.287460191514757\n",
      "Average test loss: 0.0039013370176156363\n",
      "Epoch 125/300\n",
      "Average training loss: 1.0045881362491185\n",
      "Average test loss: 0.003891435138674246\n",
      "Epoch 126/300\n",
      "Average training loss: 0.787915876282586\n",
      "Average test loss: 0.00370460587574376\n",
      "Epoch 127/300\n",
      "Average training loss: 0.628528784275055\n",
      "Average test loss: 0.003622149610478017\n",
      "Epoch 128/300\n",
      "Average training loss: 0.5168675083584255\n",
      "Average test loss: 0.003817805389977164\n",
      "Epoch 129/300\n",
      "Average training loss: 0.4328431063228183\n",
      "Average test loss: 0.0036520446826600367\n",
      "Epoch 130/300\n",
      "Average training loss: 0.36847882040341695\n",
      "Average test loss: 0.003523892355668876\n",
      "Epoch 131/300\n",
      "Average training loss: 0.3164665882587433\n",
      "Average test loss: 0.05245748094055388\n",
      "Epoch 132/300\n",
      "Average training loss: 0.27801372769143845\n",
      "Average test loss: 0.0034275621571060685\n",
      "Epoch 133/300\n",
      "Average training loss: 0.24677670547697278\n",
      "Average test loss: 0.0034240218914217417\n",
      "Epoch 134/300\n",
      "Average training loss: 0.22195834845966764\n",
      "Average test loss: 0.0036096619829121563\n",
      "Epoch 135/300\n",
      "Average training loss: 0.20282098586029476\n",
      "Average test loss: 0.0034021085529691644\n",
      "Epoch 136/300\n",
      "Average training loss: 0.18668007161882189\n",
      "Average test loss: 0.0033971333325737052\n",
      "Epoch 137/300\n",
      "Average training loss: 0.17327412672837575\n",
      "Average test loss: 0.003395088561500112\n",
      "Epoch 138/300\n",
      "Average training loss: 0.16143378028604718\n",
      "Average test loss: 0.0033742146959735288\n",
      "Epoch 139/300\n",
      "Average training loss: 0.1501706262032191\n",
      "Average test loss: 0.0034662915298508275\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1409926986893018\n",
      "Average test loss: 0.0033901157743401\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1343074999915229\n",
      "Average test loss: 0.0033739230630712375\n",
      "Epoch 142/300\n",
      "Average training loss: 0.12889817389183575\n",
      "Average test loss: 0.003372238097919358\n",
      "Epoch 143/300\n",
      "Average training loss: 0.12490657122929891\n",
      "Average test loss: 0.0033484757331510386\n",
      "Epoch 144/300\n",
      "Average training loss: 0.12135358350144493\n",
      "Average test loss: 0.0037815348346614177\n",
      "Epoch 145/300\n",
      "Average training loss: 0.1181906315088272\n",
      "Average test loss: 0.0033615223084472946\n",
      "Epoch 146/300\n",
      "Average training loss: 0.11553489257229699\n",
      "Average test loss: 0.0034023680972556274\n",
      "Epoch 147/300\n",
      "Average training loss: 0.11317788639995786\n",
      "Average test loss: 0.0033275825751738417\n",
      "Epoch 148/300\n",
      "Average training loss: 0.1111180500123236\n",
      "Average test loss: 0.0034237420643783278\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10917795875337388\n",
      "Average test loss: 0.005279844753030273\n",
      "Epoch 150/300\n",
      "Average training loss: 0.1072421211666531\n",
      "Average test loss: 0.0033985251504927874\n",
      "Epoch 151/300\n",
      "Average training loss: 0.10546836935811572\n",
      "Average test loss: 0.0070500145289633005\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10455119395918316\n",
      "Average test loss: 0.11314352174599965\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10511398804187774\n",
      "Average test loss: 0.0038907236059506733\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1014831278555923\n",
      "Average test loss: 0.0035272282413724397\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10050458572970496\n",
      "Average test loss: 0.003420801921023263\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0998939485086335\n",
      "Average test loss: 0.003444637478846643\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0985353490114212\n",
      "Average test loss: 0.0034077738250295323\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09801762898763021\n",
      "Average test loss: 0.003602925326882137\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09695399947961171\n",
      "Average test loss: 0.003951847793327437\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09610284876823426\n",
      "Average test loss: 0.003497036259621382\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09506987007459004\n",
      "Average test loss: 0.0035180976486040485\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09848908362785976\n",
      "Average test loss: 0.0034387659478104775\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09556128303872215\n",
      "Average test loss: 0.003400580078156458\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09380692703856362\n",
      "Average test loss: 0.004076605468781458\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09338516351249483\n",
      "Average test loss: 0.003529989595214526\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09275394988722271\n",
      "Average test loss: 0.005567725533412562\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09217946824762556\n",
      "Average test loss: 0.0033599092095262474\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09148877276976904\n",
      "Average test loss: 0.0033918314251220887\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09152473536464903\n",
      "Average test loss: 0.003678980439901352\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09094892071353064\n",
      "Average test loss: 0.0035904838939507804\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09081403003798591\n",
      "Average test loss: 0.004306757335861524\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09002671380837758\n",
      "Average test loss: 0.0035014818554951086\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08954903027746412\n",
      "Average test loss: 0.003550568893965748\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08926038804981444\n",
      "Average test loss: 0.003613828644156456\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08826937078436216\n",
      "Average test loss: 0.00358908404616846\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08839781925413344\n",
      "Average test loss: 0.0035282447514020736\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08768824782636431\n",
      "Average test loss: 0.003476462678776847\n",
      "Epoch 178/300\n",
      "Average training loss: 0.14953371957937878\n",
      "Average test loss: 0.0034386982915716037\n",
      "Epoch 179/300\n",
      "Average training loss: 0.11206733500957489\n",
      "Average test loss: 0.003378880372270942\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10249007609817717\n",
      "Average test loss: 0.003617649350522293\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0979062828289138\n",
      "Average test loss: 0.003448750972540842\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09485778080092536\n",
      "Average test loss: 0.003584189970460203\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0924501702785492\n",
      "Average test loss: 0.003515948029028045\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09141732466883129\n",
      "Average test loss: 0.0036113315700656836\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0895628769993782\n",
      "Average test loss: 0.004342991420378288\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08917518717050553\n",
      "Average test loss: 0.0035679784909718568\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08781581272350417\n",
      "Average test loss: 0.0035227371158285275\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08733781700664096\n",
      "Average test loss: 0.0036076930173569257\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08676969047387441\n",
      "Average test loss: 0.0034887486282322144\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08669257834222582\n",
      "Average test loss: 0.0036206181121783126\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08607242120636834\n",
      "Average test loss: 0.0034540461945450967\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08557823501030604\n",
      "Average test loss: 0.004047035027295351\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08570171403884888\n",
      "Average test loss: 0.003481683922931552\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0847278009586864\n",
      "Average test loss: 0.7911137901941935\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0877282100783454\n",
      "Average test loss: 0.0035316620866457623\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08451836282014846\n",
      "Average test loss: 0.003612179012141294\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08365028618441687\n",
      "Average test loss: 0.004592424627807405\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08341361332270834\n",
      "Average test loss: 0.0035050383164650866\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08342346148358451\n",
      "Average test loss: 0.0037099003032263784\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08283441991276211\n",
      "Average test loss: 0.003787146764290002\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08304396326674356\n",
      "Average test loss: 0.0037392332094411053\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08222777825593948\n",
      "Average test loss: 0.007164197572403484\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08191857242584229\n",
      "Average test loss: 0.003727641472385989\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08166505065891477\n",
      "Average test loss: 0.0035011363219883706\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08314026272296905\n",
      "Average test loss: 0.0035422491741677127\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08075514229138692\n",
      "Average test loss: 0.004069950957886047\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0810190745194753\n",
      "Average test loss: 0.003590334988095694\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08042416405015522\n",
      "Average test loss: 0.0035574982977575726\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08032757758431965\n",
      "Average test loss: 0.003514414322665996\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08070247278610865\n",
      "Average test loss: 0.0035295521693511143\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07978421444363064\n",
      "Average test loss: 0.05044425317645073\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08024690784348382\n",
      "Average test loss: 0.0035938594212962523\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07935985225439071\n",
      "Average test loss: 0.0039867480912556255\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07936305133832826\n",
      "Average test loss: 0.003552345079059402\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07900381482309765\n",
      "Average test loss: 0.003689775136609872\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07885899723900688\n",
      "Average test loss: 0.003506336092328032\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07852291235658858\n",
      "Average test loss: 0.003644313758860032\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07881680510772598\n",
      "Average test loss: 0.0035604771706793043\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07811490485403273\n",
      "Average test loss: 0.0036235205866396425\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07869112452202373\n",
      "Average test loss: 0.003608619755340947\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07958273292912377\n",
      "Average test loss: 0.0041630397321035465\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08668219599458907\n",
      "Average test loss: 0.005232094321399927\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07788429090049531\n",
      "Average test loss: 0.0037683134354237055\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07785162215762668\n",
      "Average test loss: 0.004535608090874221\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07640484566820992\n",
      "Average test loss: 0.003536973031755123\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07696849058071772\n",
      "Average test loss: 0.003534964933991432\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07677045378088951\n",
      "Average test loss: 0.003648345076168577\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07692234221431944\n",
      "Average test loss: 0.003624998417372505\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0791739932431115\n",
      "Average test loss: 0.003703707894931237\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07610973178678089\n",
      "Average test loss: 0.0036207761441667875\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07586883327033785\n",
      "Average test loss: 0.0037481561907463604\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07619553464982244\n",
      "Average test loss: 0.0036587910999854407\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0759741463330057\n",
      "Average test loss: 0.0037888091651515827\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0874806588821941\n",
      "Average test loss: 0.0035361065169175468\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07941748270061282\n",
      "Average test loss: 0.0036784845735463832\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07579037727249993\n",
      "Average test loss: 0.0036077550802793767\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07491531713141335\n",
      "Average test loss: 0.003865440498623583\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0748340775900417\n",
      "Average test loss: 0.003583710071734256\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07498455500602721\n",
      "Average test loss: 0.0036231052447110415\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0746917181942198\n",
      "Average test loss: 0.003710802351228065\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0751702892449167\n",
      "Average test loss: 0.003913761295378208\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08486195107632213\n",
      "Average test loss: 0.0036597145419153903\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07449802529149585\n",
      "Average test loss: 0.0035970992884702155\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07382823995086882\n",
      "Average test loss: 0.0037446435283248623\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07389561443196403\n",
      "Average test loss: 0.003637648143288162\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07423970077435176\n",
      "Average test loss: 0.003813804643642571\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07415934898455938\n",
      "Average test loss: 0.0041514829982899955\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07436316350433561\n",
      "Average test loss: 0.0036157465515037376\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07418978690769937\n",
      "Average test loss: 0.003787654899474647\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07378519433074528\n",
      "Average test loss: 0.00765182208104266\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07359731187091934\n",
      "Average test loss: 0.0036887385335026518\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07498227990998162\n",
      "Average test loss: 0.0037261985246506003\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0727725979619556\n",
      "Average test loss: 0.004165760388804806\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07321192583110597\n",
      "Average test loss: 0.003816724435115854\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07341033985217413\n",
      "Average test loss: 0.0036478337701410057\n",
      "Epoch 256/300\n",
      "Average training loss: 0.073983500679334\n",
      "Average test loss: 0.0036811331969996293\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0728942856589953\n",
      "Average test loss: 0.003920679514606794\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07258899089362886\n",
      "Average test loss: 0.003614631908221377\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07264881979756885\n",
      "Average test loss: 0.0036622621471890143\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07238821602198812\n",
      "Average test loss: 0.0036500785224553613\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07304497479730182\n",
      "Average test loss: 0.0038985911696735355\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07217547534571754\n",
      "Average test loss: 0.0037591281903700696\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07287706119484372\n",
      "Average test loss: 0.0036452753483835194\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07254831126332283\n",
      "Average test loss: 0.003696993455290794\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07174438185162015\n",
      "Average test loss: 0.003688128241751757\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07624667448467679\n",
      "Average test loss: 0.0036809401793612374\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07141231558720271\n",
      "Average test loss: 0.003634058872030841\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07137474608421325\n",
      "Average test loss: 0.00976480452881919\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07130590325593948\n",
      "Average test loss: 0.0042099513672292236\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07515165946880976\n",
      "Average test loss: 0.003748681986083587\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07093067169851727\n",
      "Average test loss: 0.0037309745045171843\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07065874757369359\n",
      "Average test loss: 0.003749092259754737\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07119758441713121\n",
      "Average test loss: 0.0036904053743928673\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07142105618450377\n",
      "Average test loss: 0.004261386446240876\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07189139821794298\n",
      "Average test loss: 0.004358559887856245\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07193762485848533\n",
      "Average test loss: 0.0037015879423254066\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07205596399307251\n",
      "Average test loss: 0.004039234769427114\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07205106996827655\n",
      "Average test loss: 0.003753728598149286\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07028099238872527\n",
      "Average test loss: 0.003734056447011729\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0699688528113895\n",
      "Average test loss: 0.00370512180423571\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0760605124897427\n",
      "Average test loss: 0.0036772823145406115\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0712887442111969\n",
      "Average test loss: 0.0038914782953345115\n",
      "Epoch 283/300\n",
      "Average training loss: 0.069596175260014\n",
      "Average test loss: 0.003723119644655122\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06959193160467678\n",
      "Average test loss: 0.003688506031822827\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07884620356228617\n",
      "Average test loss: 0.003668326722871926\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06987895834114817\n",
      "Average test loss: 0.003733405486577087\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06983776340881984\n",
      "Average test loss: 0.003969551605275935\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07591344311502245\n",
      "Average test loss: 0.003473193713774284\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07219957356651624\n",
      "Average test loss: 0.0039048413881618118\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06895132146941291\n",
      "Average test loss: 0.003674783911762966\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0691849255959193\n",
      "Average test loss: 0.00378674726974633\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06965156227350235\n",
      "Average test loss: 0.00367563019775682\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06960456989208857\n",
      "Average test loss: 0.004075579021539953\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07134721321529812\n",
      "Average test loss: 0.0038166523484720123\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06896388620469306\n",
      "Average test loss: 0.0037678239335202507\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06897864504655202\n",
      "Average test loss: 0.003794474823607339\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07654789915349748\n",
      "Average test loss: 0.0037925810157838796\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06903469387359089\n",
      "Average test loss: 0.0037504031533996266\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06963468201292886\n",
      "Average test loss: 0.003759032999475797\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06855101843674978\n",
      "Average test loss: 0.0037264959410660795\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 14.354911016252306\n",
      "Average test loss: 0.08004962993164857\n",
      "Epoch 2/300\n",
      "Average training loss: 6.479040568033854\n",
      "Average test loss: 0.014510463314751783\n",
      "Epoch 3/300\n",
      "Average training loss: 4.72622809431288\n",
      "Average test loss: 0.005415744632482528\n",
      "Epoch 4/300\n",
      "Average training loss: 3.4342029147677953\n",
      "Average test loss: 0.01447565256535179\n",
      "Epoch 5/300\n",
      "Average training loss: 2.5495127311282686\n",
      "Average test loss: 0.07217042500194576\n",
      "Epoch 6/300\n",
      "Average training loss: 2.3012782560984295\n",
      "Average test loss: 0.004248371803719136\n",
      "Epoch 7/300\n",
      "Average training loss: 1.8498675469292534\n",
      "Average test loss: 0.26022562244617276\n",
      "Epoch 8/300\n",
      "Average training loss: 1.5123036337958442\n",
      "Average test loss: 0.004060904852632019\n",
      "Epoch 9/300\n",
      "Average training loss: 1.2347753265168933\n",
      "Average test loss: 0.08437096475395892\n",
      "Epoch 10/300\n",
      "Average training loss: 0.9923738665050931\n",
      "Average test loss: 0.003733841940553652\n",
      "Epoch 11/300\n",
      "Average training loss: 0.8067313540776571\n",
      "Average test loss: 0.0035348230941842\n",
      "Epoch 12/300\n",
      "Average training loss: 0.6637984607484606\n",
      "Average test loss: 0.0034977704965405993\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5534191439946492\n",
      "Average test loss: 0.003749092794954777\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4694959624873267\n",
      "Average test loss: 0.003178695375306739\n",
      "Epoch 15/300\n",
      "Average training loss: 0.40390401090515987\n",
      "Average test loss: 0.003267030267044902\n",
      "Epoch 16/300\n",
      "Average training loss: 0.35216170287132265\n",
      "Average test loss: 0.0028830952857517534\n",
      "Epoch 17/300\n",
      "Average training loss: 0.31098720399538676\n",
      "Average test loss: 0.003921718865633011\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2749150034851498\n",
      "Average test loss: 0.003484174370765686\n",
      "Epoch 19/300\n",
      "Average training loss: 0.24519871690538195\n",
      "Average test loss: 0.0027156185605045824\n",
      "Epoch 20/300\n",
      "Average training loss: 0.22222789724667866\n",
      "Average test loss: 0.002846640065105425\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2055947489473555\n",
      "Average test loss: 0.002872680087056425\n",
      "Epoch 22/300\n",
      "Average training loss: 0.18766609564092424\n",
      "Average test loss: 0.00251983558283084\n",
      "Epoch 23/300\n",
      "Average training loss: 0.17475757661130692\n",
      "Average test loss: 0.0024920771080586645\n",
      "Epoch 24/300\n",
      "Average training loss: 0.16466373060809242\n",
      "Average test loss: 0.003582300759644972\n",
      "Epoch 25/300\n",
      "Average training loss: 0.15551818535063003\n",
      "Average test loss: 0.0025953210579852263\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1480895280043284\n",
      "Average test loss: 0.002415802995363871\n",
      "Epoch 27/300\n",
      "Average training loss: 0.14161439511511062\n",
      "Average test loss: 0.002986750590304534\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1359979301293691\n",
      "Average test loss: 0.00666394468003677\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13067040244738262\n",
      "Average test loss: 0.002416365980067187\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12580244265000026\n",
      "Average test loss: 0.002691684854320354\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12153200634982851\n",
      "Average test loss: 0.0035368464576701324\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1180661227636867\n",
      "Average test loss: 0.0022819069504944815\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11324689667092429\n",
      "Average test loss: 0.0022763427001320654\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10979694711499743\n",
      "Average test loss: 0.0022984729427844284\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10756174453099569\n",
      "Average test loss: 0.0023597727157175542\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10268666293885972\n",
      "Average test loss: 0.00222228774925073\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10062305822637346\n",
      "Average test loss: 0.00320570140125023\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09743175220489501\n",
      "Average test loss: 0.0029649252629735403\n",
      "Epoch 39/300\n",
      "Average training loss: 0.094369661476877\n",
      "Average test loss: 0.0021968716265012823\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09195596330695682\n",
      "Average test loss: 0.002261765678206252\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08963342616293166\n",
      "Average test loss: 0.0022033103891751834\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08769865309198698\n",
      "Average test loss: 0.002985254526552227\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08559177670213912\n",
      "Average test loss: 0.0021717728910346824\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08542686954140663\n",
      "Average test loss: 0.0034050046915395393\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08251995987362332\n",
      "Average test loss: 0.002138925392180681\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08061522836155362\n",
      "Average test loss: 0.0023194047885222568\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0798050195309851\n",
      "Average test loss: 0.002140989799052477\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1734121654431025\n",
      "Average test loss: 0.39644233348965646\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1510541033082538\n",
      "Average test loss: 0.0024546310065521135\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10805671104457644\n",
      "Average test loss: 0.0023876263642062744\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09882923389143414\n",
      "Average test loss: 0.0023653625866605177\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09336372585429085\n",
      "Average test loss: 0.0022285015483697254\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0901054208278656\n",
      "Average test loss: 0.0022951623351416656\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08743909085128042\n",
      "Average test loss: 0.0022952981573633024\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08693196621206072\n",
      "Average test loss: 0.002181801258896788\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08386089684565862\n",
      "Average test loss: 0.002189183872296578\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08237762968407737\n",
      "Average test loss: 0.002219922010683351\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08139399111933178\n",
      "Average test loss: 0.0021646095191438994\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08042253806524806\n",
      "Average test loss: 0.0022358395440710914\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07927016918526755\n",
      "Average test loss: 0.0044972859318885535\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08004498669836256\n",
      "Average test loss: 0.0022446616563118168\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07770538659228218\n",
      "Average test loss: 0.002154730425733659\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07771648069222768\n",
      "Average test loss: 0.0021655315173168977\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07720060398843553\n",
      "Average test loss: 0.0024150537963335715\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07818009144067764\n",
      "Average test loss: 0.0021581396222528485\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07571881985002094\n",
      "Average test loss: 0.0021558432409332856\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07449238537417517\n",
      "Average test loss: 0.0021921597184199424\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07395830398135715\n",
      "Average test loss: 0.0021325021845081614\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07563050283326044\n",
      "Average test loss: 0.002152928680698905\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07291600260138512\n",
      "Average test loss: 0.002480877337563369\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07217196845677164\n",
      "Average test loss: 0.0021219677940631904\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07144564651118385\n",
      "Average test loss: 0.002138026870890624\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0713173268172476\n",
      "Average test loss: 0.0022394188969499535\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07006028580665588\n",
      "Average test loss: 0.0021706030139078698\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06962815498974588\n",
      "Average test loss: 0.0022825444702886874\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06897756202022234\n",
      "Average test loss: 0.0021723489159097276\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07277752068969938\n",
      "Average test loss: 0.002930954252887103\n",
      "Epoch 78/300\n",
      "Average training loss: 3.2392786512374876\n",
      "Average test loss: 0.003024238567592369\n",
      "Epoch 79/300\n",
      "Average training loss: 0.770946866856681\n",
      "Average test loss: 0.0024863865027825037\n",
      "Epoch 80/300\n",
      "Average training loss: 0.36503237223625185\n",
      "Average test loss: 0.0023519308767798873\n",
      "Epoch 81/300\n",
      "Average training loss: 0.2603156273100111\n",
      "Average test loss: 0.0023106432776484223\n",
      "Epoch 82/300\n",
      "Average training loss: 0.20625001929865944\n",
      "Average test loss: 0.002252422035878731\n",
      "Epoch 83/300\n",
      "Average training loss: 0.17295346196492514\n",
      "Average test loss: 0.0022450630377150245\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1502266271246804\n",
      "Average test loss: 0.0022513742844263713\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1335317814482583\n",
      "Average test loss: 0.0022576556683828433\n",
      "Epoch 86/300\n",
      "Average training loss: 0.12107099699311787\n",
      "Average test loss: 0.002158919516743885\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11187607386377123\n",
      "Average test loss: 0.00214151809261077\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10438461216290792\n",
      "Average test loss: 0.0021462341104634105\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09883923261033165\n",
      "Average test loss: 0.0021158293889214593\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09411143524779214\n",
      "Average test loss: 0.0020924721256726318\n",
      "Epoch 91/300\n",
      "Average training loss: 0.09027367486887508\n",
      "Average test loss: 0.0033213565490312047\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08736122983031802\n",
      "Average test loss: 0.0021152024030064542\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08463431222571267\n",
      "Average test loss: 0.0021636746242228483\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08231750277015898\n",
      "Average test loss: 0.002124984975490305\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08027864638302061\n",
      "Average test loss: 0.0021391526543431813\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07859701081779268\n",
      "Average test loss: 0.0021133894787894354\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07696278992957539\n",
      "Average test loss: 0.0022397980444754163\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07578582439157698\n",
      "Average test loss: 0.0022194809577324322\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0744755589697096\n",
      "Average test loss: 0.0022051618678702247\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07338212799032529\n",
      "Average test loss: 0.0021863550191952123\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07256755983167225\n",
      "Average test loss: 0.0021667253250877063\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07175680889023675\n",
      "Average test loss: 0.0021480916957888336\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07095201940668953\n",
      "Average test loss: 0.03735052850842476\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07020523545808262\n",
      "Average test loss: 0.002125621658956839\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06932395749953058\n",
      "Average test loss: 0.0027245764452964068\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06873419413632817\n",
      "Average test loss: 0.004599964199794663\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06847526978453\n",
      "Average test loss: 0.002171394802629948\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0677243941757414\n",
      "Average test loss: 0.002231566833642622\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06762313710318671\n",
      "Average test loss: 0.002440551616044508\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07011835384368896\n",
      "Average test loss: 0.0027415259201079606\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06986125124825372\n",
      "Average test loss: 0.005108395154898365\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06655320848359002\n",
      "Average test loss: 0.002345085788725151\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06591798433661461\n",
      "Average test loss: 0.0025286364259405267\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06536462806330787\n",
      "Average test loss: 0.00214026664228489\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06479985068572892\n",
      "Average test loss: 0.002101993884063429\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06452598688999812\n",
      "Average test loss: 0.0021507271944234767\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06396244589487711\n",
      "Average test loss: 0.0021639563239489994\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0636096272567908\n",
      "Average test loss: 0.0022516975183453827\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06341549551818106\n",
      "Average test loss: 0.0022300416119396685\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06301807422439257\n",
      "Average test loss: 0.0022757424612840018\n",
      "Epoch 121/300\n",
      "Average training loss: 0.062261061075660915\n",
      "Average test loss: 0.0022636170683221686\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06242984513441722\n",
      "Average test loss: 0.002336834408135878\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06168494548400243\n",
      "Average test loss: 0.0022889589714921183\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06092283362812466\n",
      "Average test loss: 0.0022396703337629635\n",
      "Epoch 125/300\n",
      "Average training loss: 0.062498426112863756\n",
      "Average test loss: 0.00230310004701217\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06272382346126769\n",
      "Average test loss: 0.0022047936372045014\n",
      "Epoch 127/300\n",
      "Average training loss: 0.060048870126406355\n",
      "Average test loss: 0.0022583419405337837\n",
      "Epoch 128/300\n",
      "Average training loss: 0.059876259754101435\n",
      "Average test loss: 0.0022315562673740916\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05957746949460772\n",
      "Average test loss: 0.0022837682763735454\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05929883798956871\n",
      "Average test loss: 0.0022118699989385074\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05945049214694235\n",
      "Average test loss: 0.002327272499497566\n",
      "Epoch 132/300\n",
      "Average training loss: 0.058540161497063106\n",
      "Average test loss: 0.002220081700529489\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05831169824798902\n",
      "Average test loss: 0.002355180234130886\n",
      "Epoch 134/300\n",
      "Average training loss: 0.059344823996225995\n",
      "Average test loss: 0.002206605402752757\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05762463966674275\n",
      "Average test loss: 0.002228069194489055\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05885966332753499\n",
      "Average test loss: 0.0022897590949303576\n",
      "Epoch 137/300\n",
      "Average training loss: 0.056989487008916005\n",
      "Average test loss: 0.0022556338467531736\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0570920362273852\n",
      "Average test loss: 0.0023927770172142322\n",
      "Epoch 139/300\n",
      "Average training loss: 0.056577707588672636\n",
      "Average test loss: 0.002237525077950623\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05647577597035302\n",
      "Average test loss: 0.002306623795794116\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05610771293441454\n",
      "Average test loss: 0.0023303823366554247\n",
      "Epoch 142/300\n",
      "Average training loss: 0.055592108047670785\n",
      "Average test loss: 0.0023244368001404734\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05597410045067469\n",
      "Average test loss: 0.002249055548208869\n",
      "Epoch 144/300\n",
      "Average training loss: 0.057671498586734135\n",
      "Average test loss: 0.002287882908971773\n",
      "Epoch 145/300\n",
      "Average training loss: 0.055694249206119116\n",
      "Average test loss: 0.002279432659968734\n",
      "Epoch 146/300\n",
      "Average training loss: 0.054751821716626486\n",
      "Average test loss: 0.0028813640793992415\n",
      "Epoch 147/300\n",
      "Average training loss: 0.054520683384603925\n",
      "Average test loss: 0.002645932063874271\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05470407650868098\n",
      "Average test loss: 0.0023118172575616173\n",
      "Epoch 149/300\n",
      "Average training loss: 0.054452352275451026\n",
      "Average test loss: 0.0022569473343383934\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05465882512595918\n",
      "Average test loss: 0.0022209394392040042\n",
      "Epoch 151/300\n",
      "Average training loss: 0.054027340557840135\n",
      "Average test loss: 0.0023740912946975893\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05381198959218131\n",
      "Average test loss: 0.0022318065635239083\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05354959900842773\n",
      "Average test loss: 0.002717131087763442\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05437745459543334\n",
      "Average test loss: 0.00229708402728041\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05347661845551597\n",
      "Average test loss: 0.003116400982356734\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0537608510322041\n",
      "Average test loss: 0.0023666223641484975\n",
      "Epoch 157/300\n",
      "Average training loss: 0.052979515529341166\n",
      "Average test loss: 19412.67611284722\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05280718324250645\n",
      "Average test loss: 0.002348850405775011\n",
      "Epoch 159/300\n",
      "Average training loss: 0.052974675112300446\n",
      "Average test loss: 1.7242676684856415\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0524028252893024\n",
      "Average test loss: 0.002255553724658158\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05249924256404241\n",
      "Average test loss: 0.0023535457383841276\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05231025095118417\n",
      "Average test loss: 0.004324167864604129\n",
      "Epoch 163/300\n",
      "Average training loss: 0.052100504653321375\n",
      "Average test loss: 0.0046291305869817735\n",
      "Epoch 164/300\n",
      "Average training loss: 0.052662746849987244\n",
      "Average test loss: 0.0023470255338276427\n",
      "Epoch 165/300\n",
      "Average training loss: 0.052229265857074\n",
      "Average test loss: 0.0026743110281725723\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05152786998285187\n",
      "Average test loss: 0.00263956067379978\n",
      "Epoch 167/300\n",
      "Average training loss: 0.051433289845784505\n",
      "Average test loss: 0.002809803401430448\n",
      "Epoch 168/300\n",
      "Average training loss: 0.052113842248916624\n",
      "Average test loss: 0.0023327583070430493\n",
      "Epoch 169/300\n",
      "Average training loss: 0.051425411118401425\n",
      "Average test loss: 0.004008895351861914\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05101344627141952\n",
      "Average test loss: 0.002760474368826383\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05217551238338153\n",
      "Average test loss: 0.0023004523294253483\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05124886370367474\n",
      "Average test loss: 0.003595264065700273\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05101830236448182\n",
      "Average test loss: 0.002757258812172545\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05044718470176061\n",
      "Average test loss: 0.002304723922163248\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05111363610956404\n",
      "Average test loss: 0.002375156174931261\n",
      "Epoch 176/300\n",
      "Average training loss: 0.050441011806329095\n",
      "Average test loss: 0.0031822190754529503\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05035831782552931\n",
      "Average test loss: 0.002287792748047246\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05020097546113862\n",
      "Average test loss: 0.0075986149710499575\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05029868890510665\n",
      "Average test loss: 0.0023384226448833942\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04989685449666447\n",
      "Average test loss: 0.0023988131329210267\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05147004127171304\n",
      "Average test loss: 0.002342787884072297\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04961376730600993\n",
      "Average test loss: 0.0023381877360968955\n",
      "Epoch 183/300\n",
      "Average training loss: 0.049773279461595744\n",
      "Average test loss: 0.007205071250390676\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04965704060594241\n",
      "Average test loss: 0.002935577830299735\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04971819236212307\n",
      "Average test loss: 0.002305411643348634\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04968197064101696\n",
      "Average test loss: 0.0025079546163065567\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04983244752883911\n",
      "Average test loss: 0.0024464395575018393\n",
      "Epoch 188/300\n",
      "Average training loss: 0.049560136278470356\n",
      "Average test loss: 0.002649139488219387\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04889680303798782\n",
      "Average test loss: 0.0038498887163069513\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0491185545457734\n",
      "Average test loss: 0.002283301771721906\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04913695808251699\n",
      "Average test loss: 0.0023045393716957835\n",
      "Epoch 192/300\n",
      "Average training loss: 0.049325993968380825\n",
      "Average test loss: 0.002493788097674648\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04899616587327586\n",
      "Average test loss: 0.002441816473379731\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0487700560092926\n",
      "Average test loss: 0.0024456240935251117\n",
      "Epoch 195/300\n",
      "Average training loss: 0.048835364477501975\n",
      "Average test loss: 0.1721674223608441\n",
      "Epoch 196/300\n",
      "Average training loss: 0.049306046783924105\n",
      "Average test loss: 0.0025410538234023586\n",
      "Epoch 197/300\n",
      "Average training loss: 0.048437943892346486\n",
      "Average test loss: 0.0024798384621325465\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04867714824941423\n",
      "Average test loss: 0.004244418730131454\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04837571483188205\n",
      "Average test loss: 0.0023739987468967836\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04849667123291228\n",
      "Average test loss: 0.002493238704899947\n",
      "Epoch 201/300\n",
      "Average training loss: 0.048102397733264496\n",
      "Average test loss: 0.0023152254944046337\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04835787396960788\n",
      "Average test loss: 0.00238018604574932\n",
      "Epoch 203/300\n",
      "Average training loss: 0.048014510598447586\n",
      "Average test loss: 0.0024325229386902517\n",
      "Epoch 204/300\n",
      "Average training loss: 0.048316199792755976\n",
      "Average test loss: 0.002477180439564917\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04781324355800946\n",
      "Average test loss: 0.011479941681855255\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04775622647504012\n",
      "Average test loss: 0.0024736296716663573\n",
      "Epoch 207/300\n",
      "Average training loss: 0.047688079446554184\n",
      "Average test loss: 0.002974852223984069\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04780935561325815\n",
      "Average test loss: 0.0024390536635700197\n",
      "Epoch 209/300\n",
      "Average training loss: 0.047941638436582355\n",
      "Average test loss: 0.0024513142816722394\n",
      "Epoch 210/300\n",
      "Average training loss: 0.047492471943298975\n",
      "Average test loss: 0.0025011695739295746\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04766884603765276\n",
      "Average test loss: 0.002620822051953938\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04759364414546225\n",
      "Average test loss: 0.0025247743645062048\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04805650577445825\n",
      "Average test loss: 0.0026333237302800017\n",
      "Epoch 214/300\n",
      "Average training loss: 0.047793477525313696\n",
      "Average test loss: 0.0023982889983389113\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04711325492792659\n",
      "Average test loss: 0.0027946281383434933\n",
      "Epoch 216/300\n",
      "Average training loss: 0.047187274105019036\n",
      "Average test loss: 0.004214551646262408\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04857372646199332\n",
      "Average test loss: 0.0026405494580459266\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0464642241663403\n",
      "Average test loss: 0.0024345960648109516\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04709806623392635\n",
      "Average test loss: 0.0023792064990848304\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04694861544503106\n",
      "Average test loss: 0.0025609270547413162\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04694916985763444\n",
      "Average test loss: 0.002405000875807471\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04673389371898439\n",
      "Average test loss: 0.0025183160110480254\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04722498490081893\n",
      "Average test loss: 0.002332622100909551\n",
      "Epoch 224/300\n",
      "Average training loss: 0.046404861052831016\n",
      "Average test loss: 0.003850324186599917\n",
      "Epoch 225/300\n",
      "Average training loss: 0.046432556523217094\n",
      "Average test loss: 0.002477767957995335\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04687387838959694\n",
      "Average test loss: 0.002430399716107382\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04624500888254907\n",
      "Average test loss: 0.0024754247973776526\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04658653230468432\n",
      "Average test loss: 0.002904727244542705\n",
      "Epoch 229/300\n",
      "Average training loss: 0.046225928740368946\n",
      "Average test loss: 0.002743675373494625\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04696243125862545\n",
      "Average test loss: 0.0025116498361652095\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04603323135442204\n",
      "Average test loss: 0.002408511810625593\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04596291260586845\n",
      "Average test loss: 0.0025387379893412193\n",
      "Epoch 233/300\n",
      "Average training loss: 0.046220486207140817\n",
      "Average test loss: 0.002432512185432845\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04605861392286089\n",
      "Average test loss: 0.0024335759987847675\n",
      "Epoch 235/300\n",
      "Average training loss: 0.046304833180374566\n",
      "Average test loss: 0.0025322354688412613\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04566510936286714\n",
      "Average test loss: 0.002707356233770649\n",
      "Epoch 237/300\n",
      "Average training loss: 0.047274516668584614\n",
      "Average test loss: 0.0024308760452808604\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04580727710988786\n",
      "Average test loss: 0.002884477941112386\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04579331093695429\n",
      "Average test loss: 0.0023749355202954676\n",
      "Epoch 240/300\n",
      "Average training loss: 0.046466880904303655\n",
      "Average test loss: 0.002515987721996175\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04605401804215378\n",
      "Average test loss: 0.002447293742766811\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04546333395441373\n",
      "Average test loss: 0.0026746061351150276\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04556573114792506\n",
      "Average test loss: 0.00481410883511934\n",
      "Epoch 244/300\n",
      "Average training loss: 0.045701473497682145\n",
      "Average test loss: 0.002380279317084286\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04589209067821503\n",
      "Average test loss: 0.0037197998923559983\n",
      "Epoch 246/300\n",
      "Average training loss: 0.045360867152611416\n",
      "Average test loss: 0.002458824971276853\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04526323679089546\n",
      "Average test loss: 0.0024237092734417983\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04600797515445285\n",
      "Average test loss: 0.0024307688683685327\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04515356364680661\n",
      "Average test loss: 0.002557468402509888\n",
      "Epoch 250/300\n",
      "Average training loss: 0.045567000084453156\n",
      "Average test loss: 0.002468387533600132\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04494492120875253\n",
      "Average test loss: 0.002370431202877727\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04549304286970032\n",
      "Average test loss: 0.002471865133071939\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04698819051848518\n",
      "Average test loss: 0.002412376452444328\n",
      "Epoch 254/300\n",
      "Average training loss: 0.044582423799567755\n",
      "Average test loss: 0.0024668782202319967\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04491553897327847\n",
      "Average test loss: 0.0031768397446721793\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04543180253439479\n",
      "Average test loss: 0.0024218770725031695\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04488044455316332\n",
      "Average test loss: 0.0025183473964118294\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04492290424969461\n",
      "Average test loss: 0.002610643073916435\n",
      "Epoch 259/300\n",
      "Average training loss: 0.044942918052275974\n",
      "Average test loss: 0.0025056664122061595\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0446356687910027\n",
      "Average test loss: 0.003455359264794323\n",
      "Epoch 261/300\n",
      "Average training loss: 0.044795723954836525\n",
      "Average test loss: 14795348969.699556\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06067176502943039\n",
      "Average test loss: 0.002421951885231667\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04497797170943684\n",
      "Average test loss: 0.0024068260346021917\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04384392977754275\n",
      "Average test loss: 0.0024305769563135174\n",
      "Epoch 265/300\n",
      "Average training loss: 0.044119722386201224\n",
      "Average test loss: 0.003507022306116091\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04583857817451159\n",
      "Average test loss: 0.0024701774902641774\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04390423308809598\n",
      "Average test loss: 0.002543180501295461\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04438252349032296\n",
      "Average test loss: 0.0024181463855008282\n",
      "Epoch 269/300\n",
      "Average training loss: 0.044377474735180535\n",
      "Average test loss: 0.0030490899378847747\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04508313315113385\n",
      "Average test loss: 0.006014029730111361\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04462541118264198\n",
      "Average test loss: 0.002605350353444616\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0446807249850697\n",
      "Average test loss: 0.002631385315623548\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04444322374463081\n",
      "Average test loss: 0.0024814473109112845\n",
      "Epoch 274/300\n",
      "Average training loss: 0.044342106411854425\n",
      "Average test loss: 0.003589151065589653\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04422605632742246\n",
      "Average test loss: 0.002632423472694225\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04467321513758765\n",
      "Average test loss: 0.0025138418965248597\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04455479857325554\n",
      "Average test loss: 0.0024557349945728978\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04786693153116438\n",
      "Average test loss: 0.0022895714532997873\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04495402577519417\n",
      "Average test loss: 0.0028925931681361462\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04359408276942041\n",
      "Average test loss: 0.0023763071093190874\n",
      "Epoch 281/300\n",
      "Average training loss: 0.043647861295276216\n",
      "Average test loss: 0.002456080706169208\n",
      "Epoch 282/300\n",
      "Average training loss: 0.043736131293906105\n",
      "Average test loss: 0.002476825667027798\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04444456199804942\n",
      "Average test loss: 0.002450238449912932\n",
      "Epoch 284/300\n",
      "Average training loss: 0.043928792612420185\n",
      "Average test loss: 0.002417788970387644\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04384816865457429\n",
      "Average test loss: 0.0024220388550311328\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04387109047340022\n",
      "Average test loss: 0.0024703341995676357\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04429582162366973\n",
      "Average test loss: 0.0024860539575003917\n",
      "Epoch 288/300\n",
      "Average training loss: 0.043873224155770406\n",
      "Average test loss: 0.003096990432590246\n",
      "Epoch 289/300\n",
      "Average training loss: 0.043611929337183636\n",
      "Average test loss: 0.0025339730747251047\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04461070991224713\n",
      "Average test loss: 0.0025178107145345874\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04369177950753106\n",
      "Average test loss: 0.002615985975290338\n",
      "Epoch 292/300\n",
      "Average training loss: 0.043312906427515875\n",
      "Average test loss: 0.004807556863253315\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04339469362960922\n",
      "Average test loss: 0.005048023611928026\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04378368318743176\n",
      "Average test loss: 0.509389208290726\n",
      "Epoch 295/300\n",
      "Average training loss: 0.045923727651437124\n",
      "Average test loss: 0.0024655171965973243\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04301541352934308\n",
      "Average test loss: 0.0028584040886619026\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04312482868962818\n",
      "Average test loss: 0.0025615859549078675\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04328502079844475\n",
      "Average test loss: 0.002710738026847442\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0438594932059447\n",
      "Average test loss: 0.002600381504951252\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0432019072704845\n",
      "Average test loss: 0.002558652374479506\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.467607362535265\n",
      "Average test loss: 0.06963405754748318\n",
      "Epoch 2/300\n",
      "Average training loss: 5.652425045861138\n",
      "Average test loss: 0.016677248152593773\n",
      "Epoch 3/300\n",
      "Average training loss: 4.078830487781101\n",
      "Average test loss: 0.006980202219138543\n",
      "Epoch 4/300\n",
      "Average training loss: 2.9905342347886825\n",
      "Average test loss: 0.0038254969511181116\n",
      "Epoch 5/300\n",
      "Average training loss: 2.3473191854688857\n",
      "Average test loss: 0.0038924053880489535\n",
      "Epoch 6/300\n",
      "Average training loss: 1.8409364884694417\n",
      "Average test loss: 0.13074115291775928\n",
      "Epoch 7/300\n",
      "Average training loss: 1.5248792582617865\n",
      "Average test loss: 0.003348584101224939\n",
      "Epoch 8/300\n",
      "Average training loss: 1.215051692644755\n",
      "Average test loss: 0.0032440265156328677\n",
      "Epoch 9/300\n",
      "Average training loss: 0.9751700096130371\n",
      "Average test loss: 0.003384958671612872\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7707684643533494\n",
      "Average test loss: 0.002974418662695421\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6231834901173909\n",
      "Average test loss: 0.002905353113801943\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5099903621408675\n",
      "Average test loss: 0.0027032651245180103\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4266825258731842\n",
      "Average test loss: 0.0025902819639692706\n",
      "Epoch 14/300\n",
      "Average training loss: 0.36145377140574986\n",
      "Average test loss: 0.002637007988575432\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3099251590569814\n",
      "Average test loss: 0.0023905997171791062\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2685710363652971\n",
      "Average test loss: 0.00274005997636252\n",
      "Epoch 17/300\n",
      "Average training loss: 0.23547805016570622\n",
      "Average test loss: 0.0024094855692237617\n",
      "Epoch 18/300\n",
      "Average training loss: 0.20789173554049598\n",
      "Average test loss: 0.002210292889840073\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18604630857043797\n",
      "Average test loss: 0.003026189226243231\n",
      "Epoch 20/300\n",
      "Average training loss: 0.16891623865233527\n",
      "Average test loss: 0.002384386385066642\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1541893589231703\n",
      "Average test loss: 0.0020456103450722163\n",
      "Epoch 22/300\n",
      "Average training loss: 0.14374293348524306\n",
      "Average test loss: 0.002595742793960704\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1351370807753669\n",
      "Average test loss: 0.002032580359114541\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12789785363939074\n",
      "Average test loss: 0.002103107999699811\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11995369854238298\n",
      "Average test loss: 0.0018905047492848502\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11581967703501383\n",
      "Average test loss: 0.002158774666591651\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11112209012773303\n",
      "Average test loss: 0.002742959058119191\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10562906050019794\n",
      "Average test loss: 0.0019327936674882141\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10171766440073648\n",
      "Average test loss: 0.001847559581614203\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09692942115333346\n",
      "Average test loss: 0.001743154828954074\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09328796059555478\n",
      "Average test loss: 0.002170367265327109\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0908401677277353\n",
      "Average test loss: 0.0019559180681697195\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08627458139922883\n",
      "Average test loss: 0.002066815374108652\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08287316255437004\n",
      "Average test loss: 0.0019064608377714952\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08038588456975089\n",
      "Average test loss: 0.0018526901519960828\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07843643490473429\n",
      "Average test loss: 0.0016725732795894147\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07422058024009069\n",
      "Average test loss: 0.0015620785247948434\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07268156085411707\n",
      "Average test loss: 0.0015602563937298125\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07049294774399863\n",
      "Average test loss: 0.001606326746961309\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06754101390971078\n",
      "Average test loss: 0.002210633915124668\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06608248720897568\n",
      "Average test loss: 0.0015487242081306048\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06480387190977732\n",
      "Average test loss: 0.0025555902189678616\n",
      "Epoch 43/300\n",
      "Average training loss: 0.061976453363895415\n",
      "Average test loss: 0.0016218893163734013\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06082874356872506\n",
      "Average test loss: 0.0014734061625268724\n",
      "Epoch 45/300\n",
      "Average training loss: 0.062319139656093384\n",
      "Average test loss: 0.004944602835302551\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06122853827807638\n",
      "Average test loss: 0.0017203731320591437\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06143957260251045\n",
      "Average test loss: 0.001791167533232106\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05764860176046689\n",
      "Average test loss: 0.0015453418377372953\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05718779724174076\n",
      "Average test loss: 0.0018485822087774674\n",
      "Epoch 50/300\n",
      "Average training loss: 0.056669064978758496\n",
      "Average test loss: 0.0016639060235271851\n",
      "Epoch 51/300\n",
      "Average training loss: 0.056073335064782036\n",
      "Average test loss: 0.016272457621991636\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05510221746563911\n",
      "Average test loss: 0.0016964570246119466\n",
      "Epoch 53/300\n",
      "Average training loss: 0.054375386675198874\n",
      "Average test loss: 0.0015265570476444231\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05375474953651428\n",
      "Average test loss: 0.0016130420522143443\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0532988339430756\n",
      "Average test loss: 0.002209679887888746\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05377101206117206\n",
      "Average test loss: 0.0014530900405305956\n",
      "Epoch 57/300\n",
      "Average training loss: 0.052602494534518984\n",
      "Average test loss: 21.880341957092284\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05128850927948952\n",
      "Average test loss: 0.0015748570908067955\n",
      "Epoch 59/300\n",
      "Average training loss: 0.052763959291908476\n",
      "Average test loss: 0.001468927880956067\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05159278664655156\n",
      "Average test loss: 0.001453349353486879\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04971669598420461\n",
      "Average test loss: 0.001972445630778869\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04923509563340081\n",
      "Average test loss: 0.0019119589644380742\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04894455554750231\n",
      "Average test loss: 0.0014480120769391456\n",
      "Epoch 64/300\n",
      "Average training loss: 0.048661109424299666\n",
      "Average test loss: 0.01588849522628718\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04872562803162469\n",
      "Average test loss: 0.001547397002370821\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04791608417034149\n",
      "Average test loss: 0.0015345054939389228\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04752849746412701\n",
      "Average test loss: 0.001471331122227841\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04697838688559002\n",
      "Average test loss: 0.0015469349321598808\n",
      "Epoch 69/300\n",
      "Average training loss: 0.046450672172837784\n",
      "Average test loss: 0.002273736445440186\n",
      "Epoch 70/300\n",
      "Average training loss: 0.046249490946531295\n",
      "Average test loss: 0.001463091499482592\n",
      "Epoch 71/300\n",
      "Average training loss: 0.045850836671060984\n",
      "Average test loss: 0.0015503342163024677\n",
      "Epoch 72/300\n",
      "Average training loss: 0.045415254331297346\n",
      "Average test loss: 0.0014740421103520526\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04507021982140011\n",
      "Average test loss: 0.0014894359626082911\n",
      "Epoch 74/300\n",
      "Average training loss: 0.045373124594489736\n",
      "Average test loss: 0.0015636570441226163\n",
      "Epoch 75/300\n",
      "Average training loss: 0.045032457835144465\n",
      "Average test loss: 0.0015281755273851255\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04451528172691663\n",
      "Average test loss: 0.0027121135335829525\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04415424351890882\n",
      "Average test loss: 0.00162495488828669\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04340855474273364\n",
      "Average test loss: 0.002021020439349943\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04352924882703357\n",
      "Average test loss: 0.0014974044099864033\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04301214753256904\n",
      "Average test loss: 0.002272310077937113\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04278886830475595\n",
      "Average test loss: 0.002233000141671962\n",
      "Epoch 82/300\n",
      "Average training loss: 0.042905271626181074\n",
      "Average test loss: 0.002694783356040716\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04276775434613228\n",
      "Average test loss: 0.002184856764351328\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04367836271723111\n",
      "Average test loss: 0.020645061826540363\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0419082973698775\n",
      "Average test loss: 0.0015570593842615685\n",
      "Epoch 86/300\n",
      "Average training loss: 0.041434185961882274\n",
      "Average test loss: 0.0015144646432147257\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04144269037577841\n",
      "Average test loss: 0.0017901458030359612\n",
      "Epoch 88/300\n",
      "Average training loss: 0.041744197045763334\n",
      "Average test loss: 0.0025926505358268817\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04148712607555919\n",
      "Average test loss: 0.0017936291947133012\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04073501730958621\n",
      "Average test loss: 0.0016722976369783283\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04061079919834932\n",
      "Average test loss: 0.0016030869907182124\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04226070217953788\n",
      "Average test loss: 0.0032054816672785415\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04046328223413891\n",
      "Average test loss: 0.0016044993467835915\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04018415845765008\n",
      "Average test loss: 0.0027363467882904743\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04239829378657871\n",
      "Average test loss: 0.0014849937481598722\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0396262767513593\n",
      "Average test loss: 0.006127555495541957\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03957901118199031\n",
      "Average test loss: 0.0016124598170734114\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0399350208375189\n",
      "Average test loss: 0.0019527296333884199\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03943090060022142\n",
      "Average test loss: 0.002159396118380957\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03928874132037163\n",
      "Average test loss: 0.002235561589813895\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04041188484430313\n",
      "Average test loss: 0.005192584850359708\n",
      "Epoch 102/300\n",
      "Average training loss: 0.038799775666660735\n",
      "Average test loss: 0.011365148401094807\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03870931887295511\n",
      "Average test loss: 0.0015780006801295612\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03865110778477457\n",
      "Average test loss: 0.018627102898226843\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03886279681987233\n",
      "Average test loss: 0.0017704995450460248\n",
      "Epoch 106/300\n",
      "Average training loss: 0.038173470073276095\n",
      "Average test loss: 0.0016935142206235064\n",
      "Epoch 107/300\n",
      "Average training loss: 0.038849453839990826\n",
      "Average test loss: 0.0017139258662031756\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03918669628103574\n",
      "Average test loss: 0.267369446169378\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03855905313293139\n",
      "Average test loss: 0.0020003450718811817\n",
      "Epoch 110/300\n",
      "Average training loss: 0.037745957227216825\n",
      "Average test loss: 0.007478776685893536\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03908707064059046\n",
      "Average test loss: 0.0016383847954372565\n",
      "Epoch 112/300\n",
      "Average training loss: 0.037398725506332185\n",
      "Average test loss: 0.006757363105813662\n",
      "Epoch 113/300\n",
      "Average training loss: 0.046414969040287864\n",
      "Average test loss: 0.0015445591371713414\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03859657295213805\n",
      "Average test loss: 0.0016340258685458037\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03730503178636233\n",
      "Average test loss: 0.0015237349353523718\n",
      "Epoch 116/300\n",
      "Average training loss: 0.037139308704270256\n",
      "Average test loss: 0.0018294908341227305\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03725847410493427\n",
      "Average test loss: 1122.8279451700846\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03827755724390348\n",
      "Average test loss: 0.004545868504378531\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03888943167030811\n",
      "Average test loss: 0.026544340094344483\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0366807233211067\n",
      "Average test loss: 0.0016640340402308438\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0369403703543875\n",
      "Average test loss: 0.006559443569793883\n",
      "Epoch 122/300\n",
      "Average training loss: 0.036972952963577374\n",
      "Average test loss: 0.0017752872253250745\n",
      "Epoch 123/300\n",
      "Average training loss: 0.036538461099068324\n",
      "Average test loss: 0.0018139196869192851\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03656408585773574\n",
      "Average test loss: 0.015974140672220125\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03656180716223187\n",
      "Average test loss: 0.0030276993340295224\n",
      "Epoch 126/300\n",
      "Average training loss: 0.037471439987421036\n",
      "Average test loss: 0.07939981130096647\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03613133858309852\n",
      "Average test loss: 0.0017147574356446663\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0366169598268138\n",
      "Average test loss: 0.0016088181081124477\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03639557576841778\n",
      "Average test loss: 0.013991951966451274\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0359545097914007\n",
      "Average test loss: 0.002628955068687598\n",
      "Epoch 131/300\n",
      "Average training loss: 0.037509462078412374\n",
      "Average test loss: 0.0024087614126296506\n",
      "Epoch 132/300\n",
      "Average training loss: 0.035857828941610125\n",
      "Average test loss: 0.0016224690937540597\n",
      "Epoch 133/300\n",
      "Average training loss: 0.035663006961345674\n",
      "Average test loss: 0.00172474039097627\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03689398773842388\n",
      "Average test loss: 0.004803886677448948\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03564429817928208\n",
      "Average test loss: 0.0016108527179393504\n",
      "Epoch 136/300\n",
      "Average training loss: 0.037392188817262646\n",
      "Average test loss: 0.0019956989587388103\n",
      "Epoch 137/300\n",
      "Average training loss: 0.035316029283735485\n",
      "Average test loss: 0.002260130390007463\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03621055052181085\n",
      "Average test loss: 0.0016547166565433144\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03604843171106444\n",
      "Average test loss: 0.0017124463983087077\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03495936445229583\n",
      "Average test loss: 0.0016718639534794623\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03530351883504126\n",
      "Average test loss: 0.0015894555336692267\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03516462244424555\n",
      "Average test loss: 0.004474555575185352\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037507876488897536\n",
      "Average test loss: 0.0016453545792545709\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0349920760790507\n",
      "Average test loss: 0.0015988849096207154\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03498756308025784\n",
      "Average test loss: 0.0025888829182109073\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03507178757256932\n",
      "Average test loss: 0.0016742036585799523\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03786401409573025\n",
      "Average test loss: 0.0015941847743880418\n",
      "Epoch 148/300\n",
      "Average training loss: 0.034836289240254295\n",
      "Average test loss: 0.0016686698817130591\n",
      "Epoch 149/300\n",
      "Average training loss: 0.035296366251177255\n",
      "Average test loss: 0.007548699553435048\n",
      "Epoch 150/300\n",
      "Average training loss: 0.034956154271960256\n",
      "Average test loss: 0.0021457970620443424\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03457171143756972\n",
      "Average test loss: 0.002052953562595778\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03438953268859121\n",
      "Average test loss: 0.002540173457314571\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03421646889713075\n",
      "Average test loss: 0.002134236525123318\n",
      "Epoch 154/300\n",
      "Average training loss: 0.037502505583895576\n",
      "Average test loss: 0.0016217525665544802\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03424216676751773\n",
      "Average test loss: 0.001857757041644719\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03447871195773284\n",
      "Average test loss: 0.001749639267101884\n",
      "Epoch 157/300\n",
      "Average training loss: 0.034228217750787736\n",
      "Average test loss: 0.001786402134431733\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03446511127054691\n",
      "Average test loss: 0.0032292751483619215\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03409949775867992\n",
      "Average test loss: 0.0018086724484649797\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0343405395117071\n",
      "Average test loss: 0.0275613404083997\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03719167132013374\n",
      "Average test loss: 0.004767385019196405\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03408674574891726\n",
      "Average test loss: 0.004494524950782458\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03394619715544912\n",
      "Average test loss: 0.0016977427342078752\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03515695081816779\n",
      "Average test loss: 0.0019124799124482605\n",
      "Epoch 165/300\n",
      "Average training loss: 0.033742987955609954\n",
      "Average test loss: 0.0016942203936891422\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03380798754758305\n",
      "Average test loss: 0.00178881281407343\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03397740237911542\n",
      "Average test loss: 0.0022482317075547243\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03402883682648341\n",
      "Average test loss: 0.0017312122688939173\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03424165257645978\n",
      "Average test loss: 0.0017013209506662355\n",
      "Epoch 170/300\n",
      "Average training loss: 0.033713353276252744\n",
      "Average test loss: 0.007513018568770753\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03347299908432696\n",
      "Average test loss: 0.0019176981397387054\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03358196986383862\n",
      "Average test loss: 0.001855472643652724\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03375022881229719\n",
      "Average test loss: 0.001613466576880051\n",
      "Epoch 174/300\n",
      "Average training loss: 0.033638909237252344\n",
      "Average test loss: 0.002214042055524058\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03405676172176997\n",
      "Average test loss: 0.0030586845514674983\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03354753870765368\n",
      "Average test loss: 0.0016745454934943053\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0339942559964127\n",
      "Average test loss: 0.0017240112713641591\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03334603115916252\n",
      "Average test loss: 0.0017072136166195074\n",
      "Epoch 179/300\n",
      "Average training loss: 0.033428782420025935\n",
      "Average test loss: 0.01883918407559395\n",
      "Epoch 180/300\n",
      "Average training loss: 0.033500574866930644\n",
      "Average test loss: 0.0018777778877152337\n",
      "Epoch 181/300\n",
      "Average training loss: 0.033941789544290966\n",
      "Average test loss: 0.0016904333794696463\n",
      "Epoch 182/300\n",
      "Average training loss: 0.032929916141761675\n",
      "Average test loss: 0.001767004341404471\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03301287652386559\n",
      "Average test loss: 0.016083371687266562\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03373999385701285\n",
      "Average test loss: 0.0017711682826694515\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03307026146186723\n",
      "Average test loss: 0.0016669492868499624\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03326995098259714\n",
      "Average test loss: 0.0017334944924546612\n",
      "Epoch 187/300\n",
      "Average training loss: 0.032807401491536034\n",
      "Average test loss: 1022.6209706962275\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03313860249850485\n",
      "Average test loss: 0.0017834701331125365\n",
      "Epoch 189/300\n",
      "Average training loss: 0.034316813139451875\n",
      "Average test loss: 0.004794457247273789\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03266692025462786\n",
      "Average test loss: 0.0018610404595318767\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03379892807205518\n",
      "Average test loss: 0.0017544934019032453\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03250606992178493\n",
      "Average test loss: 0.009648810582028496\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03275681714216868\n",
      "Average test loss: 0.001653765279178818\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03237554540402359\n",
      "Average test loss: 0.0018030204483204418\n",
      "Epoch 195/300\n",
      "Average training loss: 0.033643413545356854\n",
      "Average test loss: 0.0016594865685328841\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03300232449836201\n",
      "Average test loss: 0.0024760923604998323\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03229863866832521\n",
      "Average test loss: 0.0018684789495956567\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03274130004313257\n",
      "Average test loss: 0.0022038175099425845\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03330470352040397\n",
      "Average test loss: 0.0019120511618546314\n",
      "Epoch 200/300\n",
      "Average training loss: 0.032483969352311556\n",
      "Average test loss: 0.001763080179794795\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03242178711957402\n",
      "Average test loss: 0.0018934203956483138\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03291813400884469\n",
      "Average test loss: 0.015419309191612733\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03249081771406862\n",
      "Average test loss: 0.001686525323531694\n",
      "Epoch 204/300\n",
      "Average training loss: 0.032245199448532526\n",
      "Average test loss: 0.0017866314645442697\n",
      "Epoch 205/300\n",
      "Average training loss: 0.032508355524804855\n",
      "Average test loss: 0.0017400268727085656\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03392352269755469\n",
      "Average test loss: 0.07573490175935957\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03220242288046413\n",
      "Average test loss: 0.0016916673850889007\n",
      "Epoch 208/300\n",
      "Average training loss: 0.031924605851372086\n",
      "Average test loss: 0.0016808309348093139\n",
      "Epoch 209/300\n",
      "Average training loss: 0.032836786210536956\n",
      "Average test loss: 0.0016607371540740133\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03188172210504611\n",
      "Average test loss: 3.975280971561041\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03206338694029384\n",
      "Average test loss: 0.0029365681583682695\n",
      "Epoch 212/300\n",
      "Average training loss: 0.033955342003040845\n",
      "Average test loss: 0.001678812079028123\n",
      "Epoch 213/300\n",
      "Average training loss: 0.031827844772073954\n",
      "Average test loss: 0.0019415668944517772\n",
      "Epoch 214/300\n",
      "Average training loss: 0.24833466487957373\n",
      "Average test loss: 0.0016620347465698918\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08396614754862256\n",
      "Average test loss: 0.0015440320210634835\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06622670058409373\n",
      "Average test loss: 0.0015026400662544702\n",
      "Epoch 217/300\n",
      "Average training loss: 0.058969801694154736\n",
      "Average test loss: 0.0014721029484644531\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05454957859052552\n",
      "Average test loss: 0.0014733615648001431\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05097157867749532\n",
      "Average test loss: 0.0015076642013672325\n",
      "Epoch 220/300\n",
      "Average training loss: 0.048015669403804674\n",
      "Average test loss: 0.0016875723493802878\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04513574797908465\n",
      "Average test loss: 0.0017822135049435828\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0427639004919264\n",
      "Average test loss: 0.0016015904584071703\n",
      "Epoch 223/300\n",
      "Average training loss: 0.040511822617716256\n",
      "Average test loss: 0.0016197232048337658\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03878525977995661\n",
      "Average test loss: 0.006574231033937799\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03800193575852447\n",
      "Average test loss: 0.0018968759386075867\n",
      "Epoch 226/300\n",
      "Average training loss: 0.035897544168763694\n",
      "Average test loss: 0.002137647593704363\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03483032243119346\n",
      "Average test loss: 0.0019429351878869864\n",
      "Epoch 228/300\n",
      "Average training loss: 0.033662112259202535\n",
      "Average test loss: 0.0017455355478450656\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03289768963389927\n",
      "Average test loss: 0.0016771691142477923\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03277419734663434\n",
      "Average test loss: 0.0016820618719276454\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03158087796635098\n",
      "Average test loss: 0.0017958801122796204\n",
      "Epoch 232/300\n",
      "Average training loss: 0.033850241627958085\n",
      "Average test loss: 0.0017519937388391959\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03156131116880311\n",
      "Average test loss: 0.0050313813079976375\n",
      "Epoch 234/300\n",
      "Average training loss: 0.031737454015347694\n",
      "Average test loss: 30323.810534722223\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0331642679936356\n",
      "Average test loss: 0.002340730697123541\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03183410403794713\n",
      "Average test loss: 0.00172627887626489\n",
      "Epoch 237/300\n",
      "Average training loss: 0.031408632751968174\n",
      "Average test loss: 0.002207836643068327\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03227642127374808\n",
      "Average test loss: 0.0022503519950227606\n",
      "Epoch 239/300\n",
      "Average training loss: 0.031666463539004326\n",
      "Average test loss: 0.002190575196304255\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03245721281568209\n",
      "Average test loss: 0.0024044626063356796\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03160146044691404\n",
      "Average test loss: 0.0024491919577121733\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03180986911886268\n",
      "Average test loss: 0.0019837445976833503\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03223484881387816\n",
      "Average test loss: 0.001938826295443707\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0333167046142949\n",
      "Average test loss: 0.0018339274950946371\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03179141710864173\n",
      "Average test loss: 0.32791697936256725\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03143470622930262\n",
      "Average test loss: 0.0018244634997099639\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03153818038105965\n",
      "Average test loss: 0.02258427202867137\n",
      "Epoch 248/300\n",
      "Average training loss: 0.031481952268216346\n",
      "Average test loss: 0.12968943442797495\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03168182119064861\n",
      "Average test loss: 0.002286588768578238\n",
      "Epoch 250/300\n",
      "Average training loss: 0.038813388119141264\n",
      "Average test loss: 0.0029389344867215388\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03168266984158092\n",
      "Average test loss: 0.001696077244149314\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03198132531841596\n",
      "Average test loss: 0.003534419074240658\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03097496195303069\n",
      "Average test loss: 0.001725800779958566\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03126849133438534\n",
      "Average test loss: 0.0017201112665029036\n",
      "Epoch 255/300\n",
      "Average training loss: 0.030939305974377527\n",
      "Average test loss: 7.119508824957742\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03185389963123533\n",
      "Average test loss: 0.33308760056893033\n",
      "Epoch 257/300\n",
      "Average training loss: 0.031540468126535416\n",
      "Average test loss: 0.0030897844324095382\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03161964371965991\n",
      "Average test loss: 0.0018032438761244217\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0397054010728995\n",
      "Average test loss: 0.0016614405675273803\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03303834232356813\n",
      "Average test loss: 0.001701222784920699\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030860063526365493\n",
      "Average test loss: 0.002862709607825511\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030944863981670803\n",
      "Average test loss: 0.0025369688486680387\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03082447128660149\n",
      "Average test loss: 0.0021127675775852468\n",
      "Epoch 264/300\n",
      "Average training loss: 0.032333852777878445\n",
      "Average test loss: 0.0017350012514119348\n",
      "Epoch 265/300\n",
      "Average training loss: 0.031428250736660425\n",
      "Average test loss: 0.002264366111304197\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030880673742956586\n",
      "Average test loss: 0.0018750801645736727\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030845005235738226\n",
      "Average test loss: 0.00379311567131016\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030913423975308735\n",
      "Average test loss: 0.0016891404147156412\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0314635095430745\n",
      "Average test loss: 0.0024002189487218857\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03164258796970049\n",
      "Average test loss: 0.0017771432728817065\n",
      "Epoch 271/300\n",
      "Average training loss: 0.031225761519538032\n",
      "Average test loss: 0.0018553941043921642\n",
      "Epoch 272/300\n",
      "Average training loss: 0.030564708683225843\n",
      "Average test loss: 0.0021954238754179744\n",
      "Epoch 273/300\n",
      "Average training loss: 0.031025032364659838\n",
      "Average test loss: 0.004931303474534717\n",
      "Epoch 274/300\n",
      "Average training loss: 0.031615376227431824\n",
      "Average test loss: 0.0020597544523576895\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030638372096750473\n",
      "Average test loss: 0.01237439183642467\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03145731030570136\n",
      "Average test loss: 0.0016546453273751668\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030781402111053468\n",
      "Average test loss: 0.0038415191709063948\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03395788226193852\n",
      "Average test loss: 0.0020273841646396454\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03062900466720263\n",
      "Average test loss: 0.0021375974584370854\n",
      "Epoch 280/300\n",
      "Average training loss: 0.030421618923544884\n",
      "Average test loss: 0.0017399080373967687\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03455410814947552\n",
      "Average test loss: 0.001589035902896689\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03185568070742819\n",
      "Average test loss: 0.06438927516175641\n",
      "Epoch 283/300\n",
      "Average training loss: 0.031282605174514984\n",
      "Average test loss: 0.0017199362478115493\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03025721020831002\n",
      "Average test loss: 0.0017017532805394795\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030352246668603686\n",
      "Average test loss: 0.009848833235601585\n",
      "Epoch 286/300\n",
      "Average training loss: 0.030667026675409742\n",
      "Average test loss: 0.009124830554135972\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03142321464088228\n",
      "Average test loss: 0.0016865128649191723\n",
      "Epoch 288/300\n",
      "Average training loss: 0.031102261000209384\n",
      "Average test loss: 0.0018190087908878923\n",
      "Epoch 289/300\n",
      "Average training loss: 0.030210892544852364\n",
      "Average test loss: 0.0017604408549765746\n",
      "Epoch 290/300\n",
      "Average training loss: 0.031137030555142296\n",
      "Average test loss: 0.0017368323703606924\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0304967457436853\n",
      "Average test loss: 0.0024744445730207694\n",
      "Epoch 292/300\n",
      "Average training loss: 0.033327337718672224\n",
      "Average test loss: 0.49188551598125035\n",
      "Epoch 293/300\n",
      "Average training loss: 0.030332335523433156\n",
      "Average test loss: 0.0022871348661267095\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02999933722615242\n",
      "Average test loss: 0.0018363034463384086\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030307975358433194\n",
      "Average test loss: 0.0018022705178914798\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030607704788446426\n",
      "Average test loss: 0.0017038678253690402\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03023792152603467\n",
      "Average test loss: 0.0017440969701856374\n",
      "Epoch 298/300\n",
      "Average training loss: 0.030445250941647423\n",
      "Average test loss: 0.02295951281177501\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03137209356327852\n",
      "Average test loss: 0.001749069751240313\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03026653269264433\n",
      "Average test loss: 0.0017246329437734353\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_32_Depth5/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.06\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.39\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.93\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.49\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.52\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.88\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.07\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.42\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.53\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.63\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.90\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.98\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.13\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.21\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.18\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.27\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.33\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.36\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.38\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.41\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.60\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.68\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.80\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.10\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.66\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.07\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.06\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.42\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.49\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.00\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.24\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.90\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.02\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.78\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.74\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.30\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.46\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.69\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.85\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.01\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.52\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.93\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.25\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.39\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.47\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.50\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.02\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.36\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.65\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.78\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.97\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.06\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.16\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.18\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.16\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.20\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.52\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.58\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.55\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.58\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.70\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
