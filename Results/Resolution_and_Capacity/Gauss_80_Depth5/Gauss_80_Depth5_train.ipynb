{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/80 x 80 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_80x80_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1523346507747968\n",
      "Average test loss: 0.011025503511230151\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06284206152624554\n",
      "Average test loss: 0.008548112496319744\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05632188042998314\n",
      "Average test loss: 0.009463746695882744\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05349782455629773\n",
      "Average test loss: 0.008097819913178683\n",
      "Epoch 5/300\n",
      "Average training loss: 0.051962125784820984\n",
      "Average test loss: 0.007564960579905245\n",
      "Epoch 6/300\n",
      "Average training loss: 0.049140865213341184\n",
      "Average test loss: 0.009192768758783738\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04731587822238604\n",
      "Average test loss: 0.007747517646600803\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04592915648884244\n",
      "Average test loss: 0.007222484305500984\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04427451946006881\n",
      "Average test loss: 0.006992770004189677\n",
      "Epoch 10/300\n",
      "Average training loss: 0.043481767975621755\n",
      "Average test loss: 0.00871663389603297\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04234904371698697\n",
      "Average test loss: 0.008081521458923817\n",
      "Epoch 12/300\n",
      "Average training loss: 0.041578732692533066\n",
      "Average test loss: 0.0068012485967742075\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04077662239968777\n",
      "Average test loss: 0.006981864006155067\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04017089480161667\n",
      "Average test loss: 0.00620998481536905\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03958322070042292\n",
      "Average test loss: 0.006881371094120873\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03906499477889803\n",
      "Average test loss: 0.006477473420401414\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03860418416394128\n",
      "Average test loss: 0.006211585318876637\n",
      "Epoch 18/300\n",
      "Average training loss: 0.038301539182662964\n",
      "Average test loss: 0.006069034373594655\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03778944160872036\n",
      "Average test loss: 0.005923510142084625\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03748200916250547\n",
      "Average test loss: 0.005863145451164908\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03701830382148425\n",
      "Average test loss: 0.005720128336714374\n",
      "Epoch 22/300\n",
      "Average training loss: 0.036815604249636334\n",
      "Average test loss: 0.006310984175652266\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03645975954665078\n",
      "Average test loss: 0.005708478809230857\n",
      "Epoch 24/300\n",
      "Average training loss: 0.036234681255287594\n",
      "Average test loss: 0.005631294758783446\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03602799985143874\n",
      "Average test loss: 0.005831390191283491\n",
      "Epoch 26/300\n",
      "Average training loss: 0.035797084682517584\n",
      "Average test loss: 0.005799239365177022\n",
      "Epoch 27/300\n",
      "Average training loss: 0.035566993759738076\n",
      "Average test loss: 0.005799164160672161\n",
      "Epoch 28/300\n",
      "Average training loss: 0.035316512409183716\n",
      "Average test loss: 0.0055061303046014575\n",
      "Epoch 29/300\n",
      "Average training loss: 0.035236098421944514\n",
      "Average test loss: 0.005558030969566769\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03500294809540113\n",
      "Average test loss: 0.005758067817323738\n",
      "Epoch 31/300\n",
      "Average training loss: 0.034846405701504816\n",
      "Average test loss: 0.005628908681579762\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03459084707167413\n",
      "Average test loss: 0.005409707052012285\n",
      "Epoch 33/300\n",
      "Average training loss: 0.034510033819410535\n",
      "Average test loss: 0.005561669178307057\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03439643680552642\n",
      "Average test loss: 0.005636861436482933\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03414312491152022\n",
      "Average test loss: 0.005327606721056832\n",
      "Epoch 36/300\n",
      "Average training loss: 0.034091827773385576\n",
      "Average test loss: 0.006079343684845501\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033962113877137504\n",
      "Average test loss: 0.005254185100810395\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03372644284698698\n",
      "Average test loss: 0.005408007010403606\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03364978820416662\n",
      "Average test loss: 0.005523318480286333\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03363621586561203\n",
      "Average test loss: 0.005369588723199235\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03346870787938436\n",
      "Average test loss: 0.00517555186442203\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03336124008893967\n",
      "Average test loss: 0.00513584097309245\n",
      "Epoch 43/300\n",
      "Average training loss: 0.033307652112510466\n",
      "Average test loss: 0.005986322731193569\n",
      "Epoch 44/300\n",
      "Average training loss: 0.033179572025934856\n",
      "Average test loss: 0.005711507743017541\n",
      "Epoch 45/300\n",
      "Average training loss: 0.033091368734836576\n",
      "Average test loss: 0.00529921864800983\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03308748252855407\n",
      "Average test loss: 0.005100064140227106\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03297321775224474\n",
      "Average test loss: 0.005105062801597847\n",
      "Epoch 48/300\n",
      "Average training loss: 0.032925400263733334\n",
      "Average test loss: 0.005326291281729937\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03274189391732216\n",
      "Average test loss: 0.005035284812251727\n",
      "Epoch 50/300\n",
      "Average training loss: 0.032690326475434836\n",
      "Average test loss: 0.018011712456742924\n",
      "Epoch 51/300\n",
      "Average training loss: 0.032597268740336104\n",
      "Average test loss: 0.005128820793910159\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03253355682889621\n",
      "Average test loss: 0.005724665438342425\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03244847435255845\n",
      "Average test loss: 0.005035603789405691\n",
      "Epoch 54/300\n",
      "Average training loss: 0.032429680731561446\n",
      "Average test loss: 0.005215899293621381\n",
      "Epoch 55/300\n",
      "Average training loss: 0.032357347942060896\n",
      "Average test loss: 0.005253481139739355\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03233960075179736\n",
      "Average test loss: 0.00521345071370403\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03225087155236138\n",
      "Average test loss: 0.005139545352094703\n",
      "Epoch 58/300\n",
      "Average training loss: 0.032206123224563066\n",
      "Average test loss: 0.0051733094410349924\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03224603836072816\n",
      "Average test loss: 0.004973527365053693\n",
      "Epoch 60/300\n",
      "Average training loss: 0.032044112568100296\n",
      "Average test loss: 0.0050379200155536335\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03200020347866747\n",
      "Average test loss: 0.005377490412443876\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03201298655403985\n",
      "Average test loss: 0.005163134130338828\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03194449190298716\n",
      "Average test loss: 0.004992114081978798\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03186459399594201\n",
      "Average test loss: 0.004955330799437232\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03193047080768479\n",
      "Average test loss: 0.00530021327527033\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03184185418486595\n",
      "Average test loss: 0.005287910194032722\n",
      "Epoch 67/300\n",
      "Average training loss: 0.031731828421354295\n",
      "Average test loss: 0.005011793325758643\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03168656421701113\n",
      "Average test loss: 0.004995569900092151\n",
      "Epoch 69/300\n",
      "Average training loss: 0.031690160203311175\n",
      "Average test loss: 0.00509159683311979\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03164017459915744\n",
      "Average test loss: 0.004966304642872678\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03158986479375098\n",
      "Average test loss: 0.00504869755771425\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03154569858147038\n",
      "Average test loss: 0.005036680760482947\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03158777937458621\n",
      "Average test loss: 0.005072608747416073\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03148608695964018\n",
      "Average test loss: 0.004907074912347728\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03143855877882904\n",
      "Average test loss: 0.004984904488755597\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03144831202096409\n",
      "Average test loss: 0.004919316562099589\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03137345998154746\n",
      "Average test loss: 0.005027384703358014\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03133138554626041\n",
      "Average test loss: 0.0048934925813227895\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03129465913110309\n",
      "Average test loss: 0.004903781029499239\n",
      "Epoch 80/300\n",
      "Average training loss: 0.031299739792943\n",
      "Average test loss: 0.005288176179760032\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03125728045900663\n",
      "Average test loss: 0.004907556933661302\n",
      "Epoch 82/300\n",
      "Average training loss: 0.031313006805049046\n",
      "Average test loss: 0.004939574203971359\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03122040502395895\n",
      "Average test loss: 0.004852417747593588\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03122005103362931\n",
      "Average test loss: 0.0050463526327576905\n",
      "Epoch 85/300\n",
      "Average training loss: 0.031149215559164684\n",
      "Average test loss: 0.004988273453381326\n",
      "Epoch 86/300\n",
      "Average training loss: 0.031073476605945165\n",
      "Average test loss: 0.004989456589851115\n",
      "Epoch 87/300\n",
      "Average training loss: 0.031106613602903153\n",
      "Average test loss: 0.005121081688751777\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03108369215329488\n",
      "Average test loss: 0.004970982456372844\n",
      "Epoch 89/300\n",
      "Average training loss: 0.031002717673778536\n",
      "Average test loss: 0.005038129162664215\n",
      "Epoch 90/300\n",
      "Average training loss: 0.031058300594488778\n",
      "Average test loss: 0.004897242965383662\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03096177977323532\n",
      "Average test loss: 0.004829779012335672\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03099108288023207\n",
      "Average test loss: 0.004853744584239192\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03095810866024759\n",
      "Average test loss: 0.004875109704004394\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03092821589443419\n",
      "Average test loss: 0.005093942788739999\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0309337660289473\n",
      "Average test loss: 0.0049715949321786565\n",
      "Epoch 96/300\n",
      "Average training loss: 0.030870568974150553\n",
      "Average test loss: 0.004906280169263482\n",
      "Epoch 97/300\n",
      "Average training loss: 0.030844858321878645\n",
      "Average test loss: 0.004913415003567934\n",
      "Epoch 98/300\n",
      "Average training loss: 0.030814253091812134\n",
      "Average test loss: 0.004939838116367658\n",
      "Epoch 99/300\n",
      "Average training loss: 0.030808149612612194\n",
      "Average test loss: 0.004964897808515363\n",
      "Epoch 100/300\n",
      "Average training loss: 0.030824765529897477\n",
      "Average test loss: 0.0048311933250063\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03077055734064844\n",
      "Average test loss: 0.004919653308060434\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03075474482609166\n",
      "Average test loss: 0.004888360388163063\n",
      "Epoch 103/300\n",
      "Average training loss: 0.030728597078058455\n",
      "Average test loss: 0.005104900613013241\n",
      "Epoch 104/300\n",
      "Average training loss: 0.030747762941651872\n",
      "Average test loss: 0.004825715611792273\n",
      "Epoch 105/300\n",
      "Average training loss: 0.030673903395732244\n",
      "Average test loss: 0.004862196168965763\n",
      "Epoch 106/300\n",
      "Average training loss: 0.030651417647798856\n",
      "Average test loss: 0.004945505166840222\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03065560366710027\n",
      "Average test loss: 0.004954872650818692\n",
      "Epoch 108/300\n",
      "Average training loss: 0.030671553585264417\n",
      "Average test loss: 0.004964395768526528\n",
      "Epoch 109/300\n",
      "Average training loss: 0.030660001946820153\n",
      "Average test loss: 0.004944446345998181\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03060395462645425\n",
      "Average test loss: 0.005065603947887818\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030603634393877453\n",
      "Average test loss: 0.004859704988284244\n",
      "Epoch 112/300\n",
      "Average training loss: 0.030584206346008513\n",
      "Average test loss: 0.005838259194046259\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03055146059062746\n",
      "Average test loss: 0.004911806978699234\n",
      "Epoch 114/300\n",
      "Average training loss: 0.030552785651551354\n",
      "Average test loss: 0.004849211528483365\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03054022749596172\n",
      "Average test loss: 0.004814226859145694\n",
      "Epoch 116/300\n",
      "Average training loss: 0.030541882909006543\n",
      "Average test loss: 0.004847413530366288\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03050808937019772\n",
      "Average test loss: 0.0048305831112795405\n",
      "Epoch 118/300\n",
      "Average training loss: 0.030488213770919377\n",
      "Average test loss: 0.004847244344237778\n",
      "Epoch 119/300\n",
      "Average training loss: 0.030536286435193485\n",
      "Average test loss: 0.004893808480352163\n",
      "Epoch 120/300\n",
      "Average training loss: 0.030446192688412137\n",
      "Average test loss: 0.004740009199827909\n",
      "Epoch 121/300\n",
      "Average training loss: 0.030431165024638177\n",
      "Average test loss: 0.004832547361238135\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03042002449101872\n",
      "Average test loss: 0.004784754444327619\n",
      "Epoch 123/300\n",
      "Average training loss: 0.030474947469102013\n",
      "Average test loss: 0.004814197965587179\n",
      "Epoch 124/300\n",
      "Average training loss: 0.030526290943225225\n",
      "Average test loss: 0.004742002890341812\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03040390820304553\n",
      "Average test loss: 0.0047728901596532924\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03036691788666778\n",
      "Average test loss: 0.005020976259062688\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0303922697554032\n",
      "Average test loss: 0.004813682559256753\n",
      "Epoch 128/300\n",
      "Average training loss: 0.030339718447791205\n",
      "Average test loss: 0.004763291297273504\n",
      "Epoch 129/300\n",
      "Average training loss: 1.4763225882682536\n",
      "Average test loss: 0.061243475106027394\n",
      "Epoch 130/300\n",
      "Average training loss: 0.3055268467532264\n",
      "Average test loss: 0.035323062288264435\n",
      "Epoch 131/300\n",
      "Average training loss: 0.1613624267578125\n",
      "Average test loss: 0.01608509225398302\n",
      "Epoch 132/300\n",
      "Average training loss: 0.1108805135620965\n",
      "Average test loss: 0.009488316670888\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08392154751883613\n",
      "Average test loss: 0.008332467928528786\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07164575076765484\n",
      "Average test loss: 0.006851051965521442\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06302873887287246\n",
      "Average test loss: 0.007298171641925971\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05672971684734027\n",
      "Average test loss: 0.006523783869627449\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05240211197071605\n",
      "Average test loss: 0.006101226714750131\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04873189981116189\n",
      "Average test loss: 0.006646714865333504\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04558598530954785\n",
      "Average test loss: 0.0057792442076736025\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04288472905423906\n",
      "Average test loss: 0.006047089375969436\n",
      "Epoch 141/300\n",
      "Average training loss: 0.040595663766066235\n",
      "Average test loss: 0.007002237577819162\n",
      "Epoch 142/300\n",
      "Average training loss: 0.038995134222838616\n",
      "Average test loss: 0.005431244837327136\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037649751318825614\n",
      "Average test loss: 0.0053311129386226334\n",
      "Epoch 144/300\n",
      "Average training loss: 0.036546854535738624\n",
      "Average test loss: 0.005206465665251017\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0355869465900792\n",
      "Average test loss: 0.005193481023112933\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0349028744995594\n",
      "Average test loss: 0.0054306797863294684\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03411660904023382\n",
      "Average test loss: 0.0053011434057520495\n",
      "Epoch 148/300\n",
      "Average training loss: 0.033630193173885346\n",
      "Average test loss: 0.005142966431462103\n",
      "Epoch 149/300\n",
      "Average training loss: 0.032988975862661994\n",
      "Average test loss: 0.005003529353688161\n",
      "Epoch 150/300\n",
      "Average training loss: 0.032545717732773885\n",
      "Average test loss: 0.0051632450413372786\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03217379096812672\n",
      "Average test loss: 0.005109175052493811\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03175462198257446\n",
      "Average test loss: 0.004929984479728672\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03149346835083432\n",
      "Average test loss: 0.004926422199855248\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03144934485024876\n",
      "Average test loss: 0.004851231527618236\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03110617161128256\n",
      "Average test loss: 0.004871224192695485\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030983322069048882\n",
      "Average test loss: 0.004843605016668638\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030889076976312532\n",
      "Average test loss: 0.004851863619974917\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030788325789901944\n",
      "Average test loss: 0.005143581118434668\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030761844529045953\n",
      "Average test loss: 0.004868583984259102\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03068115821811888\n",
      "Average test loss: 0.00484084494287769\n",
      "Epoch 161/300\n",
      "Average training loss: 0.030647964734170172\n",
      "Average test loss: 0.004895361461987098\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030640440791845323\n",
      "Average test loss: 0.004850368283275101\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03055482730269432\n",
      "Average test loss: 0.004777261445505752\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030551491232381926\n",
      "Average test loss: 0.004838843984943297\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030547054171562196\n",
      "Average test loss: 0.004859964594658878\n",
      "Epoch 166/300\n",
      "Average training loss: 0.030517313268449572\n",
      "Average test loss: 0.004917938897179233\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030438803484042487\n",
      "Average test loss: 0.004804355329937405\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030439235544866987\n",
      "Average test loss: 0.004797807769849897\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030409388075272242\n",
      "Average test loss: 0.0047997700307104325\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03037799793647395\n",
      "Average test loss: 0.004879610332142976\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030349879225095112\n",
      "Average test loss: 0.004849810642500719\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03034960385494762\n",
      "Average test loss: 0.004800173943655358\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03031519915163517\n",
      "Average test loss: 0.0048743968479749225\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030265122724903955\n",
      "Average test loss: 0.004765127347575293\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030322162787119546\n",
      "Average test loss: 0.004841022645433744\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030232827224665217\n",
      "Average test loss: 0.005400119382888079\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030253553584218024\n",
      "Average test loss: 0.005126033440646198\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03022728779580858\n",
      "Average test loss: 0.00475058303711315\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030285287814007866\n",
      "Average test loss: 0.004888065050873492\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030203313706649675\n",
      "Average test loss: 0.005240820296108723\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03017329997652107\n",
      "Average test loss: 0.005030158301194509\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03013819180097845\n",
      "Average test loss: 0.004898390338652664\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030172250158256954\n",
      "Average test loss: 0.004857839289845692\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030186477571725847\n",
      "Average test loss: 0.00474793755966756\n",
      "Epoch 185/300\n",
      "Average training loss: 0.030145052293936412\n",
      "Average test loss: 0.005375843766248888\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03010968851049741\n",
      "Average test loss: 0.004920480401979553\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03007002040412691\n",
      "Average test loss: 0.004782356591481301\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03006397612227334\n",
      "Average test loss: 0.005167859903226296\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030057798994912042\n",
      "Average test loss: 0.0047942320683764085\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03005760227971607\n",
      "Average test loss: 0.004824735115385718\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030071469972531\n",
      "Average test loss: 0.004878322892718845\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030060190689232616\n",
      "Average test loss: 0.00486116623174813\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029998160140381917\n",
      "Average test loss: 0.005242169593771299\n",
      "Epoch 194/300\n",
      "Average training loss: 0.030002788916230202\n",
      "Average test loss: 0.004715941855476962\n",
      "Epoch 195/300\n",
      "Average training loss: 0.029991572394967077\n",
      "Average test loss: 0.004714952758823832\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029967943193184004\n",
      "Average test loss: 0.005788646408667167\n",
      "Epoch 197/300\n",
      "Average training loss: 0.029992197546694015\n",
      "Average test loss: 0.004754844026019176\n",
      "Epoch 198/300\n",
      "Average training loss: 0.029954555910494594\n",
      "Average test loss: 0.00510899426912268\n",
      "Epoch 199/300\n",
      "Average training loss: 0.029923693771163624\n",
      "Average test loss: 0.004947467969109614\n",
      "Epoch 200/300\n",
      "Average training loss: 0.029937609422538015\n",
      "Average test loss: 0.004800021239038971\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02994617481364144\n",
      "Average test loss: 0.004731037905232774\n",
      "Epoch 202/300\n",
      "Average training loss: 0.029918496845497026\n",
      "Average test loss: 0.004777422430821591\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02991225191288524\n",
      "Average test loss: 0.004762568929336138\n",
      "Epoch 204/300\n",
      "Average training loss: 0.029909741524193022\n",
      "Average test loss: 0.004784285557766756\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029880304838220278\n",
      "Average test loss: 0.0047607166344920794\n",
      "Epoch 206/300\n",
      "Average training loss: 0.029921184099382825\n",
      "Average test loss: 0.004888390604406595\n",
      "Epoch 207/300\n",
      "Average training loss: 0.029843643349077967\n",
      "Average test loss: 0.004861025254759524\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029869653238190547\n",
      "Average test loss: 0.004839806558771266\n",
      "Epoch 209/300\n",
      "Average training loss: 0.029831658114989598\n",
      "Average test loss: 0.004783112788365947\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029844999700784684\n",
      "Average test loss: 0.004957291409787204\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029851037287049822\n",
      "Average test loss: 0.004758458772053322\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029797640058729383\n",
      "Average test loss: 0.004976174171600077\n",
      "Epoch 213/300\n",
      "Average training loss: 0.029828285137812296\n",
      "Average test loss: 0.004911361271308528\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02979113307595253\n",
      "Average test loss: 0.004763748061118855\n",
      "Epoch 215/300\n",
      "Average training loss: 0.029789108340938886\n",
      "Average test loss: 0.004839268998967277\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02978587070438597\n",
      "Average test loss: 0.004807105076809724\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02979245959719022\n",
      "Average test loss: 0.004707665788009763\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02977138678232829\n",
      "Average test loss: 0.004754091173410416\n",
      "Epoch 219/300\n",
      "Average training loss: 0.029756911771165\n",
      "Average test loss: 0.004699022320409616\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02978234189417627\n",
      "Average test loss: 0.005083079590151707\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029725484513574176\n",
      "Average test loss: 0.004767947134044435\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02975623047682974\n",
      "Average test loss: 0.004812378095255958\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02974754441446728\n",
      "Average test loss: 0.0049559240368091395\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029729781126810444\n",
      "Average test loss: 0.006162319909367296\n",
      "Epoch 225/300\n",
      "Average training loss: 0.029697742799917857\n",
      "Average test loss: 0.004734542664554384\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02969663334223959\n",
      "Average test loss: 0.0048313376853863395\n",
      "Epoch 227/300\n",
      "Average training loss: 0.029682482961151335\n",
      "Average test loss: 0.00478387938770983\n",
      "Epoch 228/300\n",
      "Average training loss: 0.029664749779635007\n",
      "Average test loss: 0.004781237348086304\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029684030695094003\n",
      "Average test loss: 0.00470306078510152\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029674795877602366\n",
      "Average test loss: 0.004763987689796421\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029668396090467772\n",
      "Average test loss: 0.004757237445563078\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029664696885479822\n",
      "Average test loss: 0.004754798303461737\n",
      "Epoch 233/300\n",
      "Average training loss: 0.029672883311907452\n",
      "Average test loss: 0.005430421639233828\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029620403225223222\n",
      "Average test loss: 0.004717816690189971\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029636608643664256\n",
      "Average test loss: 0.0048288663555350565\n",
      "Epoch 236/300\n",
      "Average training loss: 0.029627284122837914\n",
      "Average test loss: 0.004731426101591852\n",
      "Epoch 237/300\n",
      "Average training loss: 0.029602248314354155\n",
      "Average test loss: 0.004789498900787698\n",
      "Epoch 238/300\n",
      "Average training loss: 0.029612904267178643\n",
      "Average test loss: 0.004867424283590582\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029610347867012023\n",
      "Average test loss: 0.1134755933880806\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029576798697312674\n",
      "Average test loss: 0.004770011958976587\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029547777601414256\n",
      "Average test loss: 0.004766388904303313\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029587512526247237\n",
      "Average test loss: 0.0047850768942799835\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0295769578764836\n",
      "Average test loss: 0.004728417770730124\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029566041698058446\n",
      "Average test loss: 0.004808639066914718\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029576342965165776\n",
      "Average test loss: 0.004766626016961204\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02954291760424773\n",
      "Average test loss: 0.004807059908906619\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02955429203642739\n",
      "Average test loss: 0.004734574554281102\n",
      "Epoch 248/300\n",
      "Average training loss: 0.029521340943045087\n",
      "Average test loss: 0.004877319034188986\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029507183937562836\n",
      "Average test loss: 0.004874358490523365\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0295121577348974\n",
      "Average test loss: 0.004791858423501253\n",
      "Epoch 251/300\n",
      "Average training loss: 0.029507420694662466\n",
      "Average test loss: 0.004824161314508981\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029500046582685575\n",
      "Average test loss: 0.004770077087399033\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02948680926528242\n",
      "Average test loss: 0.004682738056199418\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029502515208390024\n",
      "Average test loss: 0.004737374000665214\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029458197984430525\n",
      "Average test loss: 0.004783787194018563\n",
      "Epoch 256/300\n",
      "Average training loss: 0.029481227225727505\n",
      "Average test loss: 0.004786580013732116\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029483828652236196\n",
      "Average test loss: 0.004815336868994766\n",
      "Epoch 258/300\n",
      "Average training loss: 0.029457255128357146\n",
      "Average test loss: 0.004827279049696194\n",
      "Epoch 259/300\n",
      "Average training loss: 0.029441080858310063\n",
      "Average test loss: 0.004771639816256033\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02944908058643341\n",
      "Average test loss: 0.004805666654474206\n",
      "Epoch 261/300\n",
      "Average training loss: 0.029447877892189556\n",
      "Average test loss: 0.004690864321258333\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029461338003476462\n",
      "Average test loss: 0.005179835170921352\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02943779386414422\n",
      "Average test loss: 0.0047672783355746\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029417798909876083\n",
      "Average test loss: 0.00486411399104529\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02940415056877666\n",
      "Average test loss: 0.004818673013399045\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02942059898542033\n",
      "Average test loss: 0.004785800590697262\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029414591521024703\n",
      "Average test loss: 0.005139782070699666\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02942203716105885\n",
      "Average test loss: 0.004717519178158707\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0293820637067159\n",
      "Average test loss: 0.004724961664113733\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029401092532608242\n",
      "Average test loss: 0.004713072439655661\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029381772973471217\n",
      "Average test loss: 0.004736991159203979\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02938170147438844\n",
      "Average test loss: 0.006070516794506047\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029367624046074018\n",
      "Average test loss: 0.004860084943887261\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029379636072450213\n",
      "Average test loss: 0.004788779988884926\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029365196425053808\n",
      "Average test loss: 0.004749455217272043\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02935756535993682\n",
      "Average test loss: 0.0047609147425327035\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029357135477993224\n",
      "Average test loss: 0.004786734278417296\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029348068333334393\n",
      "Average test loss: 0.004795054788804717\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02935268630584081\n",
      "Average test loss: 0.004877930151091682\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029320565328001976\n",
      "Average test loss: 0.004785256579725279\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029353985889090432\n",
      "Average test loss: 0.00473053856442372\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029318279673655828\n",
      "Average test loss: 0.004722026851442125\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029313847266965443\n",
      "Average test loss: 0.004757158449126614\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02935854042900933\n",
      "Average test loss: 0.004705743981318342\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02928849979241689\n",
      "Average test loss: 0.0047381960074934695\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02928945917884509\n",
      "Average test loss: 0.0049509334212376015\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029280843425128194\n",
      "Average test loss: 0.004974362856398026\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029279014037715063\n",
      "Average test loss: 0.004980254096082515\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029303078239162762\n",
      "Average test loss: 0.004730917709569136\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02923588409523169\n",
      "Average test loss: 0.004766570044060548\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029225531789991592\n",
      "Average test loss: 0.004712945130136278\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02924747161567211\n",
      "Average test loss: 0.00473689352058702\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029246244682206046\n",
      "Average test loss: 0.0047456264086067675\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029257507926887937\n",
      "Average test loss: 0.004763959244100584\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029266704608996708\n",
      "Average test loss: 0.004730612015144692\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029237691086199547\n",
      "Average test loss: 0.004796530336141586\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029243685921033224\n",
      "Average test loss: 0.004754711939642827\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02926199001736111\n",
      "Average test loss: 0.00479848558228049\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029226426230536565\n",
      "Average test loss: 0.004763926994883352\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029174630201525158\n",
      "Average test loss: 0.004856991639981667\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1334582401447826\n",
      "Average test loss: 0.007179523781769805\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04926082641879718\n",
      "Average test loss: 0.008652073374225034\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04385327433215247\n",
      "Average test loss: 0.006206161390162177\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04024094232254558\n",
      "Average test loss: 0.005454506813238064\n",
      "Epoch 5/300\n",
      "Average training loss: 0.037965373820728725\n",
      "Average test loss: 0.005720593920598428\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03623893959654702\n",
      "Average test loss: 0.006267394894940986\n",
      "Epoch 7/300\n",
      "Average training loss: 0.034759536325931546\n",
      "Average test loss: 0.0064599123630258775\n",
      "Epoch 8/300\n",
      "Average training loss: 0.033197024438116285\n",
      "Average test loss: 0.004718902082492908\n",
      "Epoch 9/300\n",
      "Average training loss: 0.032372843808597986\n",
      "Average test loss: 0.00440940209892061\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03144906566871537\n",
      "Average test loss: 0.0044636095025473175\n",
      "Epoch 11/300\n",
      "Average training loss: 0.030683034986257553\n",
      "Average test loss: 0.004612925132736563\n",
      "Epoch 12/300\n",
      "Average training loss: 0.029826928622192807\n",
      "Average test loss: 0.004067052733980947\n",
      "Epoch 13/300\n",
      "Average training loss: 0.029264040518138145\n",
      "Average test loss: 0.003914050506634845\n",
      "Epoch 14/300\n",
      "Average training loss: 0.028670749058326086\n",
      "Average test loss: 0.004009532540208764\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02813509675529268\n",
      "Average test loss: 0.003778976536459393\n",
      "Epoch 16/300\n",
      "Average training loss: 0.027781708363029692\n",
      "Average test loss: 0.004015115575864911\n",
      "Epoch 17/300\n",
      "Average training loss: 0.027210530286033947\n",
      "Average test loss: 0.004003537855835425\n",
      "Epoch 18/300\n",
      "Average training loss: 0.026866760717497933\n",
      "Average test loss: 0.003943667110883528\n",
      "Epoch 19/300\n",
      "Average training loss: 0.026574565685457654\n",
      "Average test loss: 0.003827032528610693\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02619823915263017\n",
      "Average test loss: 0.0035570489474468763\n",
      "Epoch 21/300\n",
      "Average training loss: 0.025992115343610445\n",
      "Average test loss: 0.003391143038454983\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02575065673722161\n",
      "Average test loss: 0.003447853973342313\n",
      "Epoch 23/300\n",
      "Average training loss: 0.025531225709451568\n",
      "Average test loss: 0.003358398957902359\n",
      "Epoch 24/300\n",
      "Average training loss: 0.025262852809495397\n",
      "Average test loss: 0.003414170991215441\n",
      "Epoch 25/300\n",
      "Average training loss: 0.025092927288677958\n",
      "Average test loss: 0.0032648029348088634\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02489406252404054\n",
      "Average test loss: 0.0032276680620594157\n",
      "Epoch 27/300\n",
      "Average training loss: 0.024694420221779083\n",
      "Average test loss: 0.003190883743680186\n",
      "Epoch 28/300\n",
      "Average training loss: 0.024545312247342535\n",
      "Average test loss: 0.0033061630102909274\n",
      "Epoch 29/300\n",
      "Average training loss: 0.024405831378367212\n",
      "Average test loss: 0.0031704483897321755\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024400465745064947\n",
      "Average test loss: 0.003311964323123296\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02413048696021239\n",
      "Average test loss: 0.003101140857881142\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024071880782643953\n",
      "Average test loss: 0.0032541247693200904\n",
      "Epoch 33/300\n",
      "Average training loss: 0.023964449408981535\n",
      "Average test loss: 0.00338791765148441\n",
      "Epoch 34/300\n",
      "Average training loss: 0.023780262087782222\n",
      "Average test loss: 0.0030995348350455363\n",
      "Epoch 35/300\n",
      "Average training loss: 0.023798918227354686\n",
      "Average test loss: 0.003071390350659688\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02365797959930367\n",
      "Average test loss: 0.003399658559097184\n",
      "Epoch 37/300\n",
      "Average training loss: 0.023541062290469806\n",
      "Average test loss: 0.003031877116196685\n",
      "Epoch 38/300\n",
      "Average training loss: 0.023454824878109827\n",
      "Average test loss: 0.00312091924291518\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02338152600493696\n",
      "Average test loss: 0.003174232767894864\n",
      "Epoch 40/300\n",
      "Average training loss: 0.023288882619804805\n",
      "Average test loss: 0.0030716584496614007\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02324076820909977\n",
      "Average test loss: 0.0030206052973452543\n",
      "Epoch 42/300\n",
      "Average training loss: 0.023246826130482887\n",
      "Average test loss: 0.003002188823496302\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02308249169588089\n",
      "Average test loss: 0.003106860864493582\n",
      "Epoch 44/300\n",
      "Average training loss: 0.023050279802746244\n",
      "Average test loss: 0.0029770229891356494\n",
      "Epoch 45/300\n",
      "Average training loss: 0.023020954944193362\n",
      "Average test loss: 0.003031828700668282\n",
      "Epoch 46/300\n",
      "Average training loss: 0.022901071859730615\n",
      "Average test loss: 0.0029789053882575698\n",
      "Epoch 47/300\n",
      "Average training loss: 0.022902167280515036\n",
      "Average test loss: 0.002996692688514789\n",
      "Epoch 48/300\n",
      "Average training loss: 0.022804339134030873\n",
      "Average test loss: 0.0029830634693304697\n",
      "Epoch 49/300\n",
      "Average training loss: 0.022813097776638137\n",
      "Average test loss: 0.0030454232607864673\n",
      "Epoch 50/300\n",
      "Average training loss: 0.022765044271945954\n",
      "Average test loss: 0.0029874526626533933\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02268883671528763\n",
      "Average test loss: 0.0029360752362344\n",
      "Epoch 52/300\n",
      "Average training loss: 0.022622351290451155\n",
      "Average test loss: 0.0030004521881540616\n",
      "Epoch 53/300\n",
      "Average training loss: 0.022592527303430768\n",
      "Average test loss: 0.0029361840807315374\n",
      "Epoch 54/300\n",
      "Average training loss: 0.022578268791238466\n",
      "Average test loss: 0.0030541406174500784\n",
      "Epoch 55/300\n",
      "Average training loss: 0.022529468649791346\n",
      "Average test loss: 0.0030180245607884394\n",
      "Epoch 56/300\n",
      "Average training loss: 0.022467781705988777\n",
      "Average test loss: 0.0031249828253769214\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02249127630227142\n",
      "Average test loss: 0.002973735863549842\n",
      "Epoch 58/300\n",
      "Average training loss: 0.022416821506288317\n",
      "Average test loss: 0.002907569028230177\n",
      "Epoch 59/300\n",
      "Average training loss: 0.022379936592446432\n",
      "Average test loss: 0.0028516119223915867\n",
      "Epoch 60/300\n",
      "Average training loss: 0.022321015919248264\n",
      "Average test loss: 0.002892410882231262\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022326023307111528\n",
      "Average test loss: 0.002881312548700306\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02228253627485699\n",
      "Average test loss: 0.0028569721803069115\n",
      "Epoch 63/300\n",
      "Average training loss: 0.022232375337017905\n",
      "Average test loss: 0.002843387380656269\n",
      "Epoch 64/300\n",
      "Average training loss: 0.022220571239789325\n",
      "Average test loss: 0.002956212334955732\n",
      "Epoch 65/300\n",
      "Average training loss: 0.022185677270094555\n",
      "Average test loss: 0.002896070960495207\n",
      "Epoch 66/300\n",
      "Average training loss: 0.022173470366332267\n",
      "Average test loss: 0.0031129769504898122\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02214548361301422\n",
      "Average test loss: 0.002923931068223384\n",
      "Epoch 68/300\n",
      "Average training loss: 0.022147953980498843\n",
      "Average test loss: 0.0029213124786814054\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022113965296083025\n",
      "Average test loss: 0.002855377233690686\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02203996669087145\n",
      "Average test loss: 0.0028692199782364898\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02204071055352688\n",
      "Average test loss: 0.002830374641964833\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02205745795534717\n",
      "Average test loss: 0.002845077002627982\n",
      "Epoch 73/300\n",
      "Average training loss: 0.021989533505505986\n",
      "Average test loss: 0.0031231738560729555\n",
      "Epoch 74/300\n",
      "Average training loss: 0.021963709771633148\n",
      "Average test loss: 0.0029605971218811143\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02191437679860327\n",
      "Average test loss: 0.002867017233951224\n",
      "Epoch 76/300\n",
      "Average training loss: 0.17375911486479972\n",
      "Average test loss: 0.004969250764283869\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0448254065712293\n",
      "Average test loss: 0.004168836606252524\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03470171225070953\n",
      "Average test loss: 0.0036130939815193414\n",
      "Epoch 79/300\n",
      "Average training loss: 0.030317150901589127\n",
      "Average test loss: 0.0035352437848018277\n",
      "Epoch 80/300\n",
      "Average training loss: 0.028076425295737055\n",
      "Average test loss: 0.003335362428592311\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02676631124317646\n",
      "Average test loss: 0.0032332814996027283\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025727241148551305\n",
      "Average test loss: 0.003188265492932664\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025175909295678138\n",
      "Average test loss: 0.003211070950763921\n",
      "Epoch 84/300\n",
      "Average training loss: 0.024477894986669222\n",
      "Average test loss: 0.0031923006381839513\n",
      "Epoch 85/300\n",
      "Average training loss: 0.023981871194309657\n",
      "Average test loss: 0.00303466303108467\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02362311810751756\n",
      "Average test loss: 0.0029940511420783068\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02323927458955182\n",
      "Average test loss: 0.0029966843221336604\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022920021396544244\n",
      "Average test loss: 0.002938449803325865\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02268869871397813\n",
      "Average test loss: 0.0029384526341325705\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02250070294075542\n",
      "Average test loss: 0.002902040699289905\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022350308974583944\n",
      "Average test loss: 0.002911294190833966\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02220969964729415\n",
      "Average test loss: 0.0028673427829311953\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022165129909912745\n",
      "Average test loss: 0.002858576037817531\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022093220719032816\n",
      "Average test loss: 0.0028790086450883085\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02202828525006771\n",
      "Average test loss: 0.002893768313858244\n",
      "Epoch 96/300\n",
      "Average training loss: 0.021979359767503207\n",
      "Average test loss: 0.0030033503052675063\n",
      "Epoch 97/300\n",
      "Average training loss: 0.021948082119226456\n",
      "Average test loss: 0.0029381025245206224\n",
      "Epoch 98/300\n",
      "Average training loss: 0.021909792161650128\n",
      "Average test loss: 0.0028700084325133097\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02190553477075365\n",
      "Average test loss: 0.0028388982833259637\n",
      "Epoch 100/300\n",
      "Average training loss: 0.021891216178735098\n",
      "Average test loss: 0.002854869571617908\n",
      "Epoch 101/300\n",
      "Average training loss: 0.021864111776153248\n",
      "Average test loss: 0.0029186374354693624\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02185367541346285\n",
      "Average test loss: 0.002828593372574283\n",
      "Epoch 103/300\n",
      "Average training loss: 0.021788854339056543\n",
      "Average test loss: 0.002883299923191468\n",
      "Epoch 104/300\n",
      "Average training loss: 0.021772987951834997\n",
      "Average test loss: 0.002817592304199934\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02174893415470918\n",
      "Average test loss: 0.0029792584665119647\n",
      "Epoch 106/300\n",
      "Average training loss: 0.021745234885149532\n",
      "Average test loss: 0.002903857367941075\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02175879767868254\n",
      "Average test loss: 0.00286290458858841\n",
      "Epoch 108/300\n",
      "Average training loss: 0.021699809612499343\n",
      "Average test loss: 0.0028480236381292342\n",
      "Epoch 109/300\n",
      "Average training loss: 0.021684500593278144\n",
      "Average test loss: 0.0029406419578525754\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021666005793544982\n",
      "Average test loss: 0.0028168290776924955\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02161435536046823\n",
      "Average test loss: 0.0028065647590491505\n",
      "Epoch 112/300\n",
      "Average training loss: 0.021618596696191365\n",
      "Average test loss: 0.00284539347100589\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02165684400498867\n",
      "Average test loss: 0.003490474318878518\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02159757295747598\n",
      "Average test loss: 0.0028375745829608704\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02157016997039318\n",
      "Average test loss: 0.0027994391570488612\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021527100089523528\n",
      "Average test loss: 0.00291426797500915\n",
      "Epoch 117/300\n",
      "Average training loss: 0.021536452116237746\n",
      "Average test loss: 0.0028655753216395774\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021506991245680385\n",
      "Average test loss: 0.0028506056998545927\n",
      "Epoch 119/300\n",
      "Average training loss: 0.021503428674406477\n",
      "Average test loss: 0.0027948608617815707\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021495219258798492\n",
      "Average test loss: 0.002813650166408883\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021481101950009664\n",
      "Average test loss: 0.002870205670077768\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021427088279691007\n",
      "Average test loss: 0.0028391581686834496\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02136746659543779\n",
      "Average test loss: 0.0028229219627877076\n",
      "Epoch 129/300\n",
      "Average training loss: 0.021328120006455314\n",
      "Average test loss: 0.002826943023544219\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02136397639248106\n",
      "Average test loss: 0.0028056431172622577\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02133254654208819\n",
      "Average test loss: 0.0029308414993186793\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021325084625018968\n",
      "Average test loss: 0.0028096490755884183\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021306292845143213\n",
      "Average test loss: 0.0029228792418208387\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02128747803138362\n",
      "Average test loss: 0.002793816798263126\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021284291055467395\n",
      "Average test loss: 0.0027716578278276656\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02128220203022162\n",
      "Average test loss: 0.002837438803373112\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021243497216039235\n",
      "Average test loss: 0.0028228250909596683\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02124991213944223\n",
      "Average test loss: 0.0028992237252079777\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021253518083029322\n",
      "Average test loss: 0.0031710851900279523\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021221113599008985\n",
      "Average test loss: 0.003156097116155757\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021197456197606192\n",
      "Average test loss: 0.0027779464982450007\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02122613432837857\n",
      "Average test loss: 0.00284877907546858\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021197728193468518\n",
      "Average test loss: 0.0028233841674195396\n",
      "Epoch 144/300\n",
      "Average training loss: 0.021177050350440874\n",
      "Average test loss: 0.0029586332682520152\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02115464940832721\n",
      "Average test loss: 0.0028206364928434293\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021146901100873947\n",
      "Average test loss: 0.0028415381030903923\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02113233363793956\n",
      "Average test loss: 0.0028188261075152293\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021108185253209538\n",
      "Average test loss: 0.002801837696590357\n",
      "Epoch 149/300\n",
      "Average training loss: 0.021122505363490845\n",
      "Average test loss: 0.00280928578445067\n",
      "Epoch 150/300\n",
      "Average training loss: 0.021095826094349224\n",
      "Average test loss: 0.0028474704548716546\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021106485633386508\n",
      "Average test loss: 0.0027881742680652276\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021081163144773908\n",
      "Average test loss: 0.0032447678360881076\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021099067399899165\n",
      "Average test loss: 0.0029166500969893404\n",
      "Epoch 154/300\n",
      "Average training loss: 0.021062258235282367\n",
      "Average test loss: 0.0028876504126108355\n",
      "Epoch 155/300\n",
      "Average training loss: 0.021073649283912445\n",
      "Average test loss: 0.0027725567776295873\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021050350290205744\n",
      "Average test loss: 0.0029677272513508796\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021057339308990373\n",
      "Average test loss: 0.0029692439049896265\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02105732682844003\n",
      "Average test loss: 0.0028176166518694827\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021051765660444897\n",
      "Average test loss: 0.0027427303318141236\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021017800723512968\n",
      "Average test loss: 0.002800786207843986\n",
      "Epoch 161/300\n",
      "Average training loss: 0.021030565256873766\n",
      "Average test loss: 0.0029174949917942284\n",
      "Epoch 162/300\n",
      "Average training loss: 0.020999887145227856\n",
      "Average test loss: 0.0028141126645108063\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02097872682578034\n",
      "Average test loss: 0.0027688952057311933\n",
      "Epoch 164/300\n",
      "Average training loss: 0.020990008804533215\n",
      "Average test loss: 0.0028048130681531296\n",
      "Epoch 165/300\n",
      "Average training loss: 0.020978369711173905\n",
      "Average test loss: 0.002898338878320323\n",
      "Epoch 166/300\n",
      "Average training loss: 0.020951978916923204\n",
      "Average test loss: 0.002799769079933564\n",
      "Epoch 167/300\n",
      "Average training loss: 0.020939231920573446\n",
      "Average test loss: 0.0028794362309078375\n",
      "Epoch 168/300\n",
      "Average training loss: 0.020951282906863423\n",
      "Average test loss: 0.002757109122661253\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02094481537077162\n",
      "Average test loss: 0.0027720499597489834\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02096099881331126\n",
      "Average test loss: 0.0027633808507687516\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02092875867254204\n",
      "Average test loss: 0.0027477302484007343\n",
      "Epoch 172/300\n",
      "Average training loss: 0.020923966786927647\n",
      "Average test loss: 0.002999042328033182\n",
      "Epoch 173/300\n",
      "Average training loss: 0.020880487455262078\n",
      "Average test loss: 0.0027730685255179803\n",
      "Epoch 178/300\n",
      "Average training loss: 0.020861421508921518\n",
      "Average test loss: 0.002857009266399675\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02085218450344271\n",
      "Average test loss: 0.0027654216802782483\n",
      "Epoch 180/300\n",
      "Average training loss: 0.020868064317438338\n",
      "Average test loss: 0.0027935483498084876\n",
      "Epoch 181/300\n",
      "Average training loss: 0.020833931386470796\n",
      "Average test loss: 0.0027834281366732384\n",
      "Epoch 182/300\n",
      "Average training loss: 0.020825928936402\n",
      "Average test loss: 0.002787603818707996\n",
      "Epoch 183/300\n",
      "Average training loss: 0.020841449110044374\n",
      "Average test loss: 0.00285990696772933\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02081943623473247\n",
      "Average test loss: 0.0029047007755272918\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02080741773876879\n",
      "Average test loss: 0.002784076563703517\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02081668681734138\n",
      "Average test loss: 0.0028102201320644882\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02082479253411293\n",
      "Average test loss: 0.002845396193779177\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02079972633222739\n",
      "Average test loss: 0.002867594545914067\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02078495192527771\n",
      "Average test loss: 0.002825704733013279\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020797131907608773\n",
      "Average test loss: 0.002763045625657671\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02078235800067584\n",
      "Average test loss: 0.0028104450379808745\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02077072822054227\n",
      "Average test loss: 0.0028083730301716263\n",
      "Epoch 193/300\n",
      "Average training loss: 0.020754621535539627\n",
      "Average test loss: 0.0027715471767716937\n",
      "Epoch 194/300\n",
      "Average training loss: 0.020754193658630054\n",
      "Average test loss: 0.0027962170804126394\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020749087557196618\n",
      "Average test loss: 0.0028785230266965096\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020761394504043792\n",
      "Average test loss: 0.002995766036833326\n",
      "Epoch 197/300\n",
      "Average training loss: 0.020746514981819522\n",
      "Average test loss: 0.0028690630356884663\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020779267645544476\n",
      "Average test loss: 0.0029781104976104368\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020726520261830753\n",
      "Average test loss: 0.002764833574079805\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020697944902711443\n",
      "Average test loss: 0.0027683996734105877\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02070217824810081\n",
      "Average test loss: 0.0027654313852803576\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020697537837757005\n",
      "Average test loss: 0.0027501104399561883\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020692393666340247\n",
      "Average test loss: 0.0027911211252212523\n",
      "Epoch 204/300\n",
      "Average training loss: 0.020695515975356103\n",
      "Average test loss: 0.0027800944753819043\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02070752514898777\n",
      "Average test loss: 0.002779559408624967\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02071205450925562\n",
      "Average test loss: 0.0028469403255730868\n",
      "Epoch 207/300\n",
      "Average training loss: 0.020668416500091553\n",
      "Average test loss: 0.0027648885084523094\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020688025545742776\n",
      "Average test loss: 0.002883812558526794\n",
      "Epoch 209/300\n",
      "Average training loss: 0.020650883907245267\n",
      "Average test loss: 0.002953216630965471\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02066059164537324\n",
      "Average test loss: 0.0028040791360868346\n",
      "Epoch 211/300\n",
      "Average training loss: 0.020659259926941658\n",
      "Average test loss: 0.0027389069284415907\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020635416527589164\n",
      "Average test loss: 0.0027929549331052435\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020633356899023056\n",
      "Average test loss: 0.002772052930461036\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02064365275700887\n",
      "Average test loss: 0.002786877299348513\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020613249409529896\n",
      "Average test loss: 0.002836188537379106\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020609614845779205\n",
      "Average test loss: 0.0028122012534489236\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0206239685482449\n",
      "Average test loss: 0.0029989702173819147\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02061591202351782\n",
      "Average test loss: 0.0028369958880874848\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02056886556165086\n",
      "Average test loss: 0.0029912050095283324\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02063040948410829\n",
      "Average test loss: 0.002797679046996766\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020575389108724064\n",
      "Average test loss: 0.0027417799967030683\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02056220798691114\n",
      "Average test loss: 0.0027809036914259195\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02058529049737586\n",
      "Average test loss: 0.0028029585344096024\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02054725713365608\n",
      "Average test loss: 0.0027331327435870965\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02056851525273588\n",
      "Average test loss: 0.0028596527230822376\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020580487299296592\n",
      "Average test loss: 0.0027427691518225605\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020537831366062163\n",
      "Average test loss: 0.0028410654032809866\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020545803180999227\n",
      "Average test loss: 0.002849962041609817\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02053363592094845\n",
      "Average test loss: 0.0027963289415670767\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02054445067048073\n",
      "Average test loss: 0.0027549433356357945\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02053312572505739\n",
      "Average test loss: 0.0027570426515820955\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02053913678891129\n",
      "Average test loss: 0.002802654854953289\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02051284992860423\n",
      "Average test loss: 0.0027293365236578716\n",
      "Epoch 238/300\n",
      "Average training loss: 0.020500727815760508\n",
      "Average test loss: 0.0029422169495373963\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020510015433033307\n",
      "Average test loss: 0.0029727232919798956\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020500366071859996\n",
      "Average test loss: 0.002739930439119538\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02049760890338156\n",
      "Average test loss: 0.0028607136617518135\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020524021031128034\n",
      "Average test loss: 0.0028344010160201126\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02046026585665014\n",
      "Average test loss: 0.0028215350318286153\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020517373697625265\n",
      "Average test loss: 0.002794274394710859\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020461459760864574\n",
      "Average test loss: 0.0027564555286533304\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02049447372721301\n",
      "Average test loss: 0.002745914807336198\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020457646684514153\n",
      "Average test loss: 0.0027558865812089706\n",
      "Epoch 248/300\n",
      "Average training loss: 0.020453893437153762\n",
      "Average test loss: 0.0030683669400297935\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020450407308008935\n",
      "Average test loss: 0.0027870841672023136\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020451458351479636\n",
      "Average test loss: 0.002860522769184576\n",
      "Epoch 251/300\n",
      "Average training loss: 0.020472857745157346\n",
      "Average test loss: 0.0027765991147607567\n",
      "Epoch 252/300\n",
      "Average training loss: 0.020435890586839783\n",
      "Average test loss: 0.0027308761154611907\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020410829077164332\n",
      "Average test loss: 0.002853561034426093\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02044419424401389\n",
      "Average test loss: 0.002790946665737364\n",
      "Epoch 255/300\n",
      "Average training loss: 0.020422474493583042\n",
      "Average test loss: 0.0028151528793904515\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02043423835436503\n",
      "Average test loss: 0.0028059926006115147\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020410635608765815\n",
      "Average test loss: 0.003085966235647599\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02040259912278917\n",
      "Average test loss: 0.0030514906050844327\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02040289814439085\n",
      "Average test loss: 0.0028311628742764394\n",
      "Epoch 264/300\n",
      "Average training loss: 0.020402247023251323\n",
      "Average test loss: 0.0027617419755293264\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020379915189411905\n",
      "Average test loss: 0.0027946063100049894\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020373630674348938\n",
      "Average test loss: 0.002837856655319532\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020354527639018166\n",
      "Average test loss: 0.0028011393513944416\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020378488207856815\n",
      "Average test loss: 0.002754290335190793\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02037144114656581\n",
      "Average test loss: 0.002815977064685689\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020351388934585782\n",
      "Average test loss: 0.0027624578128258386\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020378746695816515\n",
      "Average test loss: 0.002828174779812495\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02035261417925358\n",
      "Average test loss: 0.002748309547909432\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020351063930326038\n",
      "Average test loss: 0.002958493712047736\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020355166426963275\n",
      "Average test loss: 0.0027366131133296423\n",
      "Epoch 275/300\n",
      "Average training loss: 0.020351501767834027\n",
      "Average test loss: 0.002722630987254282\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020357977439959846\n",
      "Average test loss: 0.002821795160261293\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020329959253470104\n",
      "Average test loss: 0.002790311783966091\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020344931258095635\n",
      "Average test loss: 0.0028244139494167436\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020338709423939387\n",
      "Average test loss: 0.0027713787011388277\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020331921832429038\n",
      "Average test loss: 0.002800148589950469\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020317971267633968\n",
      "Average test loss: 0.0028319763147996533\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02031242214474413\n",
      "Average test loss: 0.0027996825337823893\n",
      "Epoch 283/300\n",
      "Average training loss: 0.020313403432567913\n",
      "Average test loss: 0.002897690061065886\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020318957348664603\n",
      "Average test loss: 0.002812425606780582\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020329488810565736\n",
      "Average test loss: 0.002769860236388114\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02031138225727611\n",
      "Average test loss: 0.002811853084920181\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02029331027302477\n",
      "Average test loss: 0.002810649044604765\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0202955485764477\n",
      "Average test loss: 0.0032339053365091483\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02029844145807955\n",
      "Average test loss: 0.0027515924889594317\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020324741133385234\n",
      "Average test loss: 0.0027652641826619705\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020276715023650062\n",
      "Average test loss: 0.0027798872840487294\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02027724348091417\n",
      "Average test loss: 0.0027977268509566783\n",
      "Epoch 293/300\n",
      "Average training loss: 0.020267299241489834\n",
      "Average test loss: 0.0027994510850144756\n",
      "Epoch 294/300\n",
      "Average training loss: 0.020277427999509705\n",
      "Average test loss: 0.002783676092616386\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020262374197443325\n",
      "Average test loss: 0.0028124323973639145\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020263735765384303\n",
      "Average test loss: 0.0029852687565402854\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020251573792762226\n",
      "Average test loss: 0.0028014674321230914\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02029578056269222\n",
      "Average test loss: 0.0028014589148677056\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02023941794368956\n",
      "Average test loss: 0.0027284278228051134\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02025197544031673\n",
      "Average test loss: 0.002778267740582426\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11731547469562954\n",
      "Average test loss: 0.005266488677925534\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04113579728537135\n",
      "Average test loss: 0.0045604252204712896\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03568770843744278\n",
      "Average test loss: 0.004080110440651576\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03278478883041276\n",
      "Average test loss: 0.004386266282449166\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0309272261692418\n",
      "Average test loss: 0.004192246373742819\n",
      "Epoch 6/300\n",
      "Average training loss: 0.029404295972651904\n",
      "Average test loss: 0.0033518396595286\n",
      "Epoch 7/300\n",
      "Average training loss: 0.028081977845893967\n",
      "Average test loss: 0.003359229851928022\n",
      "Epoch 8/300\n",
      "Average training loss: 0.026828254110283323\n",
      "Average test loss: 0.003187831229633755\n",
      "Epoch 9/300\n",
      "Average training loss: 0.025894347601466708\n",
      "Average test loss: 0.003335092771798372\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02516048917174339\n",
      "Average test loss: 0.0029934215144150787\n",
      "Epoch 11/300\n",
      "Average training loss: 0.024484603646728727\n",
      "Average test loss: 0.0032523277745478683\n",
      "Epoch 12/300\n",
      "Average training loss: 0.023893542952007716\n",
      "Average test loss: 0.0029958775277352996\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02124674369063642\n",
      "Average test loss: 0.0023858153870743182\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02090468678706222\n",
      "Average test loss: 0.0023478631540719006\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020673336798946063\n",
      "Average test loss: 0.002362496867775917\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020465678135553996\n",
      "Average test loss: 0.0022714927602145406\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020276445594098833\n",
      "Average test loss: 0.0023211975695772305\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02009345008432865\n",
      "Average test loss: 0.002211295594978664\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01990052995996343\n",
      "Average test loss: 0.0023745324617872638\n",
      "Epoch 25/300\n",
      "Average training loss: 0.019758434360225997\n",
      "Average test loss: 0.00227340862651666\n",
      "Epoch 26/300\n",
      "Average training loss: 0.019671931798259417\n",
      "Average test loss: 0.0021599155132555298\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019480308426751032\n",
      "Average test loss: 0.002220942594524887\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019345958807402187\n",
      "Average test loss: 0.0021053947238251565\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0192805734872818\n",
      "Average test loss: 0.0022812998549391825\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019148405606547993\n",
      "Average test loss: 0.0022015610037164554\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019069780151049296\n",
      "Average test loss: 0.002087134664464328\n",
      "Epoch 32/300\n",
      "Average training loss: 0.018993035805722078\n",
      "Average test loss: 0.00214262404334214\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018867369532585145\n",
      "Average test loss: 0.002137267985484666\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018812749576237468\n",
      "Average test loss: 0.002072494589827127\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01877792866528034\n",
      "Average test loss: 0.0020842863373044464\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01865781728592184\n",
      "Average test loss: 0.002042254649516609\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0186101792620288\n",
      "Average test loss: 0.002082610114994976\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018530637270874447\n",
      "Average test loss: 0.002071939093681673\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018464457417527835\n",
      "Average test loss: 0.0019967994027667575\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018405265644192696\n",
      "Average test loss: 0.002012623223165671\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018363376781344413\n",
      "Average test loss: 0.002024940029821462\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018365992956691318\n",
      "Average test loss: 0.0021059663616534736\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018253494016826154\n",
      "Average test loss: 0.00207085571386334\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018203804875413575\n",
      "Average test loss: 0.002052893504500389\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018147069748077126\n",
      "Average test loss: 0.0020378881798436242\n",
      "Epoch 46/300\n",
      "Average training loss: 0.018123486863242257\n",
      "Average test loss: 0.0020652841014994513\n",
      "Epoch 47/300\n",
      "Average training loss: 0.018088031760520405\n",
      "Average test loss: 0.00203417998779979\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018056579581565327\n",
      "Average test loss: 0.001976552943595582\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0180074388815297\n",
      "Average test loss: 0.0019766792456309\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018025133588247828\n",
      "Average test loss: 0.0020316183448562193\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01793670224812296\n",
      "Average test loss: 0.0019753026695301136\n",
      "Epoch 52/300\n",
      "Average training loss: 0.017925038599305682\n",
      "Average test loss: 0.001989531429691447\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017902159505420262\n",
      "Average test loss: 0.001980937763531175\n",
      "Epoch 54/300\n",
      "Average training loss: 0.017860373409257994\n",
      "Average test loss: 0.0020387938519318897\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017799363025360636\n",
      "Average test loss: 0.0019606762814025082\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017764887241853607\n",
      "Average test loss: 0.0020230371604363123\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017782480265531274\n",
      "Average test loss: 0.0020666524677847822\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0177177049037483\n",
      "Average test loss: 0.001961753947970768\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01766942430370384\n",
      "Average test loss: 0.0019496499331047139\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017683364662859174\n",
      "Average test loss: 0.0019452120125707654\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017651711356308726\n",
      "Average test loss: 0.0021165537225703397\n",
      "Epoch 62/300\n",
      "Average training loss: 0.017594250755177603\n",
      "Average test loss: 0.00198665267518825\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01747165930271149\n",
      "Average test loss: 0.002018357007453839\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017441260284847684\n",
      "Average test loss: 0.0020312771890312435\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017434000121222603\n",
      "Average test loss: 0.0019617991093546154\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017406972684794002\n",
      "Average test loss: 0.0019351699888292286\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017378772303462027\n",
      "Average test loss: 0.0020053889082951677\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017370071881347232\n",
      "Average test loss: 0.001978057425883081\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01735088821252187\n",
      "Average test loss: 0.0019110948964953423\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017335020773940616\n",
      "Average test loss: 0.0021156339287343954\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017297711489929094\n",
      "Average test loss: 0.0020819767266511916\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017305485940641827\n",
      "Average test loss: 0.0019260786989082892\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01727297002573808\n",
      "Average test loss: 0.001978638114614619\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017264228854742315\n",
      "Average test loss: 0.0019516323171555995\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01724459842675262\n",
      "Average test loss: 0.001964418841111991\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01724451173759169\n",
      "Average test loss: 0.0019428857753260268\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01721253506259786\n",
      "Average test loss: 0.0020225283994028966\n",
      "Epoch 83/300\n",
      "Average training loss: 0.017202684354450968\n",
      "Average test loss: 0.001979498162606938\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017203028197089833\n",
      "Average test loss: 0.0019573087176928917\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01717572497493691\n",
      "Average test loss: 0.0019020092646694845\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017133012843628725\n",
      "Average test loss: 0.001972392759803269\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017145229433145788\n",
      "Average test loss: 0.00193585455759118\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01711054982907242\n",
      "Average test loss: 0.0019181249856741892\n",
      "Epoch 89/300\n",
      "Average training loss: 0.017095284562971855\n",
      "Average test loss: 0.0019059021975845099\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01712272386584017\n",
      "Average test loss: 0.0018986706536056267\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017077998268107575\n",
      "Average test loss: 0.002126679594318072\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017063271421525212\n",
      "Average test loss: 0.001943069138357209\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017056774890257254\n",
      "Average test loss: 0.0019467140648824473\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017062011516756483\n",
      "Average test loss: 0.001897579701203439\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01701749472071727\n",
      "Average test loss: 0.0028585959933698176\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017049767800503306\n",
      "Average test loss: 0.0019012024123221637\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0169870487112138\n",
      "Average test loss: 0.0019604504195352397\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016991044415367973\n",
      "Average test loss: 0.0019135921158724362\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016958084735605453\n",
      "Average test loss: 0.001877414009326862\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01697229590680864\n",
      "Average test loss: 0.0018980486831731266\n",
      "Epoch 101/300\n",
      "Average training loss: 0.016970519534415668\n",
      "Average test loss: 0.0019752301153623396\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01693052675575018\n",
      "Average test loss: 0.0019499427057388756\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016933688057793512\n",
      "Average test loss: 0.0019010409593789115\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016951176202959484\n",
      "Average test loss: 0.001888870279201203\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01690871045241753\n",
      "Average test loss: 0.0019072402583228216\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01689315750201543\n",
      "Average test loss: 0.0020302358969218203\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016887801523009935\n",
      "Average test loss: 0.0018712235291798909\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016858763040767777\n",
      "Average test loss: 0.0018740799594670534\n",
      "Epoch 109/300\n",
      "Average training loss: 0.016869656393925347\n",
      "Average test loss: 0.0019181485596216387\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016847487572166656\n",
      "Average test loss: 0.001875603088695142\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016833425803316964\n",
      "Average test loss: 0.0019021637870205773\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016837004186378585\n",
      "Average test loss: 0.001932082023575074\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016771736121839947\n",
      "Average test loss: 0.002041742795250482\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016741185575723648\n",
      "Average test loss: 0.0022672406925509374\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016762938384380605\n",
      "Average test loss: 0.0018911646578667893\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016724320952263144\n",
      "Average test loss: 0.002123842257592413\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016752341272102463\n",
      "Average test loss: 0.0019197742593888608\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01670964800318082\n",
      "Average test loss: 0.0019042647222263945\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016712803575727676\n",
      "Average test loss: 0.002004970052641713\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016705516431066724\n",
      "Average test loss: 0.001868239446543157\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01668824233031935\n",
      "Average test loss: 0.0019263733646108045\n",
      "Epoch 128/300\n",
      "Average training loss: 0.016662689635323153\n",
      "Average test loss: 0.001979857726643483\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016712970342901017\n",
      "Average test loss: 0.00188806125935581\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016643518672221237\n",
      "Average test loss: 0.0019106167505184809\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01664199999637074\n",
      "Average test loss: 0.001883372420238124\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016655932567185824\n",
      "Average test loss: 0.0018615566189918253\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016630746707320215\n",
      "Average test loss: 0.0020538482322461074\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01662372636795044\n",
      "Average test loss: 0.0018776734504434797\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016625915247533058\n",
      "Average test loss: 0.001891608069029947\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0165986354963647\n",
      "Average test loss: 0.0019375855539821916\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01662221203578843\n",
      "Average test loss: 0.0018555968126489056\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016585387255582545\n",
      "Average test loss: 0.001945914909036623\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01659872149179379\n",
      "Average test loss: 0.0019372079983974496\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016586609923177294\n",
      "Average test loss: 0.0018908954748056\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01654907346599632\n",
      "Average test loss: 0.0018869197312742472\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01656346844136715\n",
      "Average test loss: 0.001855451177805662\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016541878249910144\n",
      "Average test loss: 0.001969929300248623\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016580193243092962\n",
      "Average test loss: 0.0018741558672239382\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01657741544644038\n",
      "Average test loss: 0.0019265689651171366\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016519809937311544\n",
      "Average test loss: 0.0018989639406402905\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01651976006726424\n",
      "Average test loss: 0.0018922927983933025\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01652483319160011\n",
      "Average test loss: 0.0019099666327238084\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016536752339866425\n",
      "Average test loss: 0.001985598950439857\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016506149270468288\n",
      "Average test loss: 0.0019077613968402148\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016491672672331332\n",
      "Average test loss: 0.001940621782404681\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016494970459077093\n",
      "Average test loss: 0.0019104016455304292\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016499962597257563\n",
      "Average test loss: 0.00189354596969982\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01647501605583562\n",
      "Average test loss: 0.001904662056101693\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016456932342714732\n",
      "Average test loss: 0.0018764537045111258\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016472009205983743\n",
      "Average test loss: 0.0018842354164355331\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016465118141637907\n",
      "Average test loss: 0.0018551070938507715\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01644136495722665\n",
      "Average test loss: 0.0019698003248001137\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01645161276559035\n",
      "Average test loss: 0.001908437729192277\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016399089626140066\n",
      "Average test loss: 0.0018495394378486606\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01643268513513936\n",
      "Average test loss: 0.0019261169911672673\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016394611046546034\n",
      "Average test loss: 0.0018547911311810215\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01637306177533335\n",
      "Average test loss: 0.001881391491378761\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01638739111688402\n",
      "Average test loss: 0.001951933366763923\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016376964094738166\n",
      "Average test loss: 0.001941507573136025\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01636718952159087\n",
      "Average test loss: 0.0024919478949159382\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016375795422328844\n",
      "Average test loss: 0.0018920999403215118\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016352328618367513\n",
      "Average test loss: 0.0018916677209652132\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01636190355155203\n",
      "Average test loss: 0.0018611845256139834\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016349123262696795\n",
      "Average test loss: 0.001933308181559874\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01634744768672519\n",
      "Average test loss: 0.0019885190340379875\n",
      "Epoch 176/300\n",
      "Average training loss: 0.016326968419882987\n",
      "Average test loss: 0.0018502022457412548\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016321447784701983\n",
      "Average test loss: 0.00189882692694664\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016317052569654253\n",
      "Average test loss: 0.0019518452994525433\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01632735086977482\n",
      "Average test loss: 0.0018969745414538516\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016307926025655534\n",
      "Average test loss: 0.0018575481278821826\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01629340109229088\n",
      "Average test loss: 0.0019124314826395776\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016300037843485674\n",
      "Average test loss: 0.0019052960899555021\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01627973578704728\n",
      "Average test loss: 0.0018559453880621327\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016312321885592406\n",
      "Average test loss: 0.0018701355025793115\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016284634972612063\n",
      "Average test loss: 0.0018548974368928206\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01626478970050812\n",
      "Average test loss: 0.001875319195083446\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016266397080487674\n",
      "Average test loss: 0.0019045615134139855\n",
      "Epoch 188/300\n",
      "Average training loss: 0.016256012009249792\n",
      "Average test loss: 0.002017169772647321\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01628033784363005\n",
      "Average test loss: 0.0018882202595058414\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016252659589052202\n",
      "Average test loss: 0.0018519925181236531\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016241802184118165\n",
      "Average test loss: 0.0019050296359798974\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01624078332136075\n",
      "Average test loss: 0.0019105130099794931\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016240453650554022\n",
      "Average test loss: 0.0019159701243042946\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016239753294322225\n",
      "Average test loss: 0.001876239584137996\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016224839799933964\n",
      "Average test loss: 0.00208026298135519\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016234875531660187\n",
      "Average test loss: 0.0019066342998089061\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0162005648083157\n",
      "Average test loss: 0.0018972840195314751\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01620182406819529\n",
      "Average test loss: 0.0019019600143656134\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01621196240103907\n",
      "Average test loss: 0.008868889736632506\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016212556708190176\n",
      "Average test loss: 0.0018463285249761408\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016196657532619105\n",
      "Average test loss: 0.0018970722768248784\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016218215136064425\n",
      "Average test loss: 0.001898519334072868\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016172359234756895\n",
      "Average test loss: 0.0019336191508919\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016201248064637182\n",
      "Average test loss: 0.0018871468658455545\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016187521738310656\n",
      "Average test loss: 0.001960474114658104\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016160197305182616\n",
      "Average test loss: 0.0018253298830758367\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016170782019694647\n",
      "Average test loss: 0.001964399729751878\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016165188092324468\n",
      "Average test loss: 0.0018312498233798477\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01614717045591937\n",
      "Average test loss: 0.0018523950486754378\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01615076908717553\n",
      "Average test loss: 0.001921504121273756\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01614792743573586\n",
      "Average test loss: 0.0019225705125265652\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016149224034614034\n",
      "Average test loss: 0.0018747578699969583\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016133465075658427\n",
      "Average test loss: 0.0018676775679406193\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016149223376479414\n",
      "Average test loss: 0.0027931082128650614\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016115978287325965\n",
      "Average test loss: 0.0018727062908518645\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01614306793279118\n",
      "Average test loss: 0.0018951485614395803\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016132024829586346\n",
      "Average test loss: 0.0019156418576215704\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01611320100724697\n",
      "Average test loss: 0.0019188335133302544\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016099485367950465\n",
      "Average test loss: 0.00197736190383633\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016139339976012707\n",
      "Average test loss: 0.00183795592499276\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01611573766089148\n",
      "Average test loss: 0.0019748699741644993\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016115407494207222\n",
      "Average test loss: 0.001960784307163623\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01609526483052307\n",
      "Average test loss: 0.0019225264684193664\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016110689532425667\n",
      "Average test loss: 0.0019489842243492603\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016091835926804278\n",
      "Average test loss: 0.0018819782758752505\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0160800871104002\n",
      "Average test loss: 0.001953586710823907\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01609416026539273\n",
      "Average test loss: 0.0018377210615823667\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016084720842540264\n",
      "Average test loss: 0.0019010553674565422\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016081848382949827\n",
      "Average test loss: 0.0021124424474934737\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016062541647089854\n",
      "Average test loss: 0.0019356840014871624\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016064413386914465\n",
      "Average test loss: 0.0018444027456765374\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016066023212340144\n",
      "Average test loss: 0.0018643337963148952\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016070915800415808\n",
      "Average test loss: 0.0021375929037522937\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016040892375840082\n",
      "Average test loss: 0.0018973537222999666\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01604815350886848\n",
      "Average test loss: 0.0019099652467088567\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016040041313403183\n",
      "Average test loss: 0.0019234496682054467\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01604632407757971\n",
      "Average test loss: 0.0018983548548486499\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01602524955736266\n",
      "Average test loss: 0.001873045014217496\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016045550683306323\n",
      "Average test loss: 0.0019138839627719588\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01601723218626446\n",
      "Average test loss: 0.0019008964269111553\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016019019736184015\n",
      "Average test loss: 0.0018657564986497165\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016007191646430226\n",
      "Average test loss: 0.001932069833804336\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016007700221406088\n",
      "Average test loss: 0.0019857115971131456\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016000486663646167\n",
      "Average test loss: 0.0018912448530188865\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01598544078734186\n",
      "Average test loss: 0.0021477604184506668\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01601566869020462\n",
      "Average test loss: 0.0018561391229223875\n",
      "Epoch 252/300\n",
      "Average training loss: 0.015974494669172497\n",
      "Average test loss: 0.0018929996026886835\n",
      "Epoch 253/300\n",
      "Average training loss: 0.015986982339786158\n",
      "Average test loss: 0.001851428936753008\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01597903771036201\n",
      "Average test loss: 0.0018803758441160122\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015966873925593164\n",
      "Average test loss: 0.0018398403524317675\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015960521419843037\n",
      "Average test loss: 0.0020231821864015527\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015987678946720228\n",
      "Average test loss: 0.001868077520488037\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01597178437643581\n",
      "Average test loss: 0.0018559569117302696\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015959262783328692\n",
      "Average test loss: 0.0018465553226156366\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015955142685108715\n",
      "Average test loss: 0.0018531020550678174\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015960668307211663\n",
      "Average test loss: 0.0018396620680060651\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0159460203167465\n",
      "Average test loss: 0.0018488884716191226\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015952667423420483\n",
      "Average test loss: 0.0018601059442799953\n",
      "Epoch 264/300\n",
      "Average training loss: 0.015937927164965206\n",
      "Average test loss: 0.0019058169801202086\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015951683230698108\n",
      "Average test loss: 0.0018563420048190488\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01594079717165894\n",
      "Average test loss: 0.0018669232527414959\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015932921434442204\n",
      "Average test loss: 0.0022571842088881466\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01592472243309021\n",
      "Average test loss: 0.0019254007707867357\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015929201475448077\n",
      "Average test loss: 0.001867303367704153\n",
      "Epoch 270/300\n",
      "Average training loss: 0.015928740760518444\n",
      "Average test loss: 0.0019092660807073117\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015917656345499888\n",
      "Average test loss: 0.0018551424702422487\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015910666649540266\n",
      "Average test loss: 0.0018978480096492503\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015920719787478447\n",
      "Average test loss: 0.0019685496656845014\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015907127181688945\n",
      "Average test loss: 0.0019220338159551223\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0159190853536129\n",
      "Average test loss: 0.0018881773597353862\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015905427275432482\n",
      "Average test loss: 0.0019072647806670931\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015892895640598405\n",
      "Average test loss: 0.0019271124750375747\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015918627969092793\n",
      "Average test loss: 0.0018959900360140535\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015892848033044072\n",
      "Average test loss: 0.0018558799672043986\n",
      "Epoch 280/300\n",
      "Average training loss: 0.015885603186984856\n",
      "Average test loss: 0.001849346073758271\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015894217912521626\n",
      "Average test loss: 0.0018883798464925752\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01588560833533605\n",
      "Average test loss: 0.0018967536499516831\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01589525644481182\n",
      "Average test loss: 0.0019537506771998272\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01587874240262641\n",
      "Average test loss: 0.0018527157687478594\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01588647255467044\n",
      "Average test loss: 0.0018917042170133855\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015874117079708313\n",
      "Average test loss: 0.0018572642209215297\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015872841048571798\n",
      "Average test loss: 0.0019514947168322073\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01586096701771021\n",
      "Average test loss: 0.0018588844986839428\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015879706944856378\n",
      "Average test loss: 0.0019347603164820207\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01586470531920592\n",
      "Average test loss: 0.00188152721669111\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015849897560973963\n",
      "Average test loss: 0.0018809042241838244\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015843332335352896\n",
      "Average test loss: 0.0018357531500773298\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015834666750497287\n",
      "Average test loss: 0.0019860699261642165\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015865211283167202\n",
      "Average test loss: 0.44039267062478593\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05061172136995527\n",
      "Average test loss: 0.0025265600560233\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10650298654701974\n",
      "Average test loss: 0.003929372917860746\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03575957990686099\n",
      "Average test loss: 0.004046650409698487\n",
      "Epoch 3/300\n",
      "Average training loss: 0.030511796616845662\n",
      "Average test loss: 0.0032939165002769896\n",
      "Epoch 4/300\n",
      "Average training loss: 0.027893604927592807\n",
      "Average test loss: 0.0030774531594167155\n",
      "Epoch 5/300\n",
      "Average training loss: 0.026151742198401027\n",
      "Average test loss: 0.002793167435667581\n",
      "Epoch 6/300\n",
      "Average training loss: 0.024601187076833512\n",
      "Average test loss: 0.0027339472284333573\n",
      "Epoch 7/300\n",
      "Average training loss: 0.023551395267248153\n",
      "Average test loss: 0.0026044496765567195\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02232852333287398\n",
      "Average test loss: 0.0024382226907958588\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021722621040211783\n",
      "Average test loss: 0.002638947124282519\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02098209012216992\n",
      "Average test loss: 0.0021847764189458557\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02032967980702718\n",
      "Average test loss: 0.002118642388532559\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019869515703784094\n",
      "Average test loss: 0.0022298585441377426\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01946687587764528\n",
      "Average test loss: 0.0019734066548860734\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01900425671537717\n",
      "Average test loss: 0.002089469493470258\n",
      "Epoch 15/300\n",
      "Average training loss: 0.018691977590322494\n",
      "Average test loss: 0.001958723253260056\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01825647211405966\n",
      "Average test loss: 0.0019199121166020633\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01793948121368885\n",
      "Average test loss: 0.0018559211655002502\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017701765643225776\n",
      "Average test loss: 0.0020344728706404567\n",
      "Epoch 19/300\n",
      "Average training loss: 0.017454931842784085\n",
      "Average test loss: 0.0017828019054399596\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01721722309291363\n",
      "Average test loss: 0.0018440213626664545\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017102261188129585\n",
      "Average test loss: 0.0016814734720521504\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016914578180346224\n",
      "Average test loss: 0.0016560666177214847\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016723976150982908\n",
      "Average test loss: 0.0017346613741376334\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01666850491695934\n",
      "Average test loss: 0.001881037046098047\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016524457070562576\n",
      "Average test loss: 0.001629403410996828\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01644814621243212\n",
      "Average test loss: 0.0016766431161926853\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01627676430179013\n",
      "Average test loss: 0.0015934885698888038\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016229484569695262\n",
      "Average test loss: 0.001600274742063549\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016153051972389223\n",
      "Average test loss: 0.001549692469338576\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016060049931208294\n",
      "Average test loss: 0.0017453524424797958\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01594908197141356\n",
      "Average test loss: 0.0015181559382213487\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015847908788257176\n",
      "Average test loss: 0.0015195223769793908\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01586520269430346\n",
      "Average test loss: 0.0015309579224429197\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015764739866885875\n",
      "Average test loss: 0.0016160054599038429\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015376684208710988\n",
      "Average test loss: 0.0014836781040454903\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015378727730777529\n",
      "Average test loss: 0.0014487030136709413\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015314837334884538\n",
      "Average test loss: 0.0014594577113683854\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015246763873431417\n",
      "Average test loss: 0.001434689965719978\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015253009951776928\n",
      "Average test loss: 0.0014714020372678836\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01519762632664707\n",
      "Average test loss: 0.0014953356321073241\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015167566302749846\n",
      "Average test loss: 0.0014594736064059866\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015138297905524572\n",
      "Average test loss: 0.0014407559456303715\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015096772006816334\n",
      "Average test loss: 0.0014377392406264942\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01503374608937237\n",
      "Average test loss: 0.0014373887293040752\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015046959064073032\n",
      "Average test loss: 0.0014326617835710446\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014979562073118157\n",
      "Average test loss: 0.0014243918290982643\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01498261632108026\n",
      "Average test loss: 0.0014062015592224068\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014936260442766879\n",
      "Average test loss: 0.0015309296608385112\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014919367020742761\n",
      "Average test loss: 0.0015768834743648766\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014890550990899403\n",
      "Average test loss: 0.0014671057175017065\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01486631344507138\n",
      "Average test loss: 0.001413586057515608\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014848610212405522\n",
      "Average test loss: 0.0014372340568030873\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014813698694109917\n",
      "Average test loss: 0.0014060014250377815\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01480821294337511\n",
      "Average test loss: 0.0015461085351804891\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014768626531793012\n",
      "Average test loss: 0.0015386329135961003\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014764515490995512\n",
      "Average test loss: 0.0013965545431193379\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014715486634108755\n",
      "Average test loss: 0.001509360268815524\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014708284332520432\n",
      "Average test loss: 0.002216318373050955\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014666618242031998\n",
      "Average test loss: 0.0014040972420738802\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01465293257849084\n",
      "Average test loss: 0.0014188424478181535\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014636796959572368\n",
      "Average test loss: 0.0014314389736909006\n",
      "Epoch 68/300\n",
      "Average training loss: 0.014603962533175946\n",
      "Average test loss: 0.0013878899448447758\n",
      "Epoch 69/300\n",
      "Average training loss: 0.014591458567314678\n",
      "Average test loss: 0.0014164983056899575\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014583740835388501\n",
      "Average test loss: 0.0013923518609048591\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014578177988529206\n",
      "Average test loss: 0.0013994954618521862\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014542936529550287\n",
      "Average test loss: 0.0014022615851006575\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014515147627227836\n",
      "Average test loss: 0.0014160414431244135\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014517611963881386\n",
      "Average test loss: 0.0014285005890867777\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01448574632489019\n",
      "Average test loss: 0.0014599902206617924\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014492373809218407\n",
      "Average test loss: 0.0014801398900647958\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014433597046467994\n",
      "Average test loss: 0.001390075720846653\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014449399598770672\n",
      "Average test loss: 0.0014990758769628074\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014423834706346193\n",
      "Average test loss: 0.001427310794384943\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014404565499888526\n",
      "Average test loss: 0.0014371461410903267\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014408672278953923\n",
      "Average test loss: 0.0014033241198501652\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014369852351645628\n",
      "Average test loss: 0.001415562149344219\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014354707983632883\n",
      "Average test loss: 0.00142284391178853\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01435737229221397\n",
      "Average test loss: 0.0014319868763494822\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01434346894837088\n",
      "Average test loss: 0.0013986805457000932\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014244241827891933\n",
      "Average test loss: 0.0014263320145093733\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014232569666372406\n",
      "Average test loss: 0.0014100134670734406\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014223294917080137\n",
      "Average test loss: 0.0013699550488963724\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014203909478253788\n",
      "Average test loss: 0.0013948818202027017\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014190844912495879\n",
      "Average test loss: 0.0014633843215803306\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014192260639535057\n",
      "Average test loss: 0.0017549640220693417\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014181776081522306\n",
      "Average test loss: 0.0013550696602712075\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014149536984662215\n",
      "Average test loss: 0.0013616124540567398\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014158991406361262\n",
      "Average test loss: 0.0013922954330013858\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014115900145636665\n",
      "Average test loss: 0.0013677439207418097\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014122467680937714\n",
      "Average test loss: 0.0014178161375845472\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014122442106405893\n",
      "Average test loss: 0.0013679764141432114\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01412009380509456\n",
      "Average test loss: 0.001355697967319025\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014081253598961565\n",
      "Average test loss: 0.001399782383814454\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014082073599100114\n",
      "Average test loss: 0.0013669394445088175\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014083034041855071\n",
      "Average test loss: 0.001411018328844673\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014077599759731028\n",
      "Average test loss: 0.00135198137200334\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014043000188966592\n",
      "Average test loss: 0.0013769008797696895\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014041880862580406\n",
      "Average test loss: 0.0013942337832931015\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014006612040102482\n",
      "Average test loss: 0.001512403254966355\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014019741987188658\n",
      "Average test loss: 0.0015952225964930321\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014022589596609274\n",
      "Average test loss: 0.001382577274977747\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01399665026118358\n",
      "Average test loss: 0.0013566749749912156\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01398119255569246\n",
      "Average test loss: 0.0013978121190642318\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013994874195092254\n",
      "Average test loss: 0.0013672282395677433\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013968983076512814\n",
      "Average test loss: 0.0014781220677412218\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013963900824387868\n",
      "Average test loss: 0.0013796230530780222\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01394220336774985\n",
      "Average test loss: 0.0013695696021119752\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013950413934058613\n",
      "Average test loss: 0.0013611886838657988\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013925315274132623\n",
      "Average test loss: 0.001353626943193376\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013922694568832715\n",
      "Average test loss: 0.0013473758013505075\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013917801444729169\n",
      "Average test loss: 0.0018277379814535379\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013902319899863668\n",
      "Average test loss: 0.0014140994252843989\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013907606688638528\n",
      "Average test loss: 0.0013607967431243095\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013882483392953872\n",
      "Average test loss: 0.0013499193721347386\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013876365753511588\n",
      "Average test loss: 0.0013522263231376807\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013882799356347985\n",
      "Average test loss: 0.0013365916305532058\n",
      "Epoch 128/300\n",
      "Average training loss: 0.013848226146565543\n",
      "Average test loss: 0.0013757990502441923\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013875158842239116\n",
      "Average test loss: 0.0013469141272621022\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013856470591492124\n",
      "Average test loss: 0.0015695577089985212\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013818034400542576\n",
      "Average test loss: 0.0013741136979725625\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013820372400184472\n",
      "Average test loss: 0.0013669759668409824\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013824410045312511\n",
      "Average test loss: 0.001400228362892651\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0138247587374515\n",
      "Average test loss: 0.0015203044146506323\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01380781361915999\n",
      "Average test loss: 0.0018091727238562373\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013775460570222801\n",
      "Average test loss: 0.001387746991124004\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01378984758257866\n",
      "Average test loss: 0.0013680091387488775\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01379386960880624\n",
      "Average test loss: 0.001347678571835988\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01378589905715651\n",
      "Average test loss: 0.0014037182637904252\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013792298051218193\n",
      "Average test loss: 0.0014011666777854165\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01375924942145745\n",
      "Average test loss: 0.0013683331752609876\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013708093321985668\n",
      "Average test loss: 0.0013400984335069856\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013719736530548997\n",
      "Average test loss: 0.0013484741158576476\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013698681871096293\n",
      "Average test loss: 0.0013642880850368076\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013693026722305351\n",
      "Average test loss: 0.0013664245707914232\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013690240571896235\n",
      "Average test loss: 0.001353161225716273\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013673261475231912\n",
      "Average test loss: 0.0013985586318497856\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01370457848161459\n",
      "Average test loss: 0.0013408026398263044\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01367016389220953\n",
      "Average test loss: 0.001355305992687742\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013662605662312772\n",
      "Average test loss: 0.0022951819639032084\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013672095887362958\n",
      "Average test loss: 0.0013797604609798225\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013651980658372243\n",
      "Average test loss: 0.00139680297827969\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013637311673826641\n",
      "Average test loss: 0.0014306351478315061\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013656379736959934\n",
      "Average test loss: 0.0013796002111501164\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01363718485335509\n",
      "Average test loss: 0.0013573551036210525\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013621694880227247\n",
      "Average test loss: 0.0014769309647381305\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013634336310956213\n",
      "Average test loss: 0.0013298850781284272\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013605904744731055\n",
      "Average test loss: 0.0013558698835679226\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013612796564896902\n",
      "Average test loss: 0.0013346661972916788\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013600295162863202\n",
      "Average test loss: 0.0013920835932302806\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013588747567600675\n",
      "Average test loss: 0.0013633382741568816\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013599355769654115\n",
      "Average test loss: 0.0013589646890759467\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013588432901435429\n",
      "Average test loss: 0.0013695214350397389\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01359101240336895\n",
      "Average test loss: 0.001334777808179044\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013581523450712363\n",
      "Average test loss: 0.0013521890660954847\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013574356466531753\n",
      "Average test loss: 0.0013598663970414136\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013559556105898487\n",
      "Average test loss: 0.0013921560686495568\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013560945380893018\n",
      "Average test loss: 0.0013800013567217522\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013558839558727212\n",
      "Average test loss: 0.0013942382855133878\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013539894056816895\n",
      "Average test loss: 0.0013559827289233604\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013547459681000974\n",
      "Average test loss: 0.0013378597718353073\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01356595988240507\n",
      "Average test loss: 0.0013416281408733792\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013536386296153068\n",
      "Average test loss: 0.0013791925106197596\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013520091452532345\n",
      "Average test loss: 0.0013919657588832908\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013529743869271544\n",
      "Average test loss: 0.00137510059059908\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013519145817392402\n",
      "Average test loss: 0.0014192105433386234\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013510320882002512\n",
      "Average test loss: 0.001366490717149443\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013506318584912353\n",
      "Average test loss: 0.0013284310539149575\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013501207828521728\n",
      "Average test loss: 0.0013643649438810016\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013503310531377792\n",
      "Average test loss: 0.0015740295540955331\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013509213703374068\n",
      "Average test loss: 0.0013537286418593593\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013497182175517082\n",
      "Average test loss: 0.0013496954277571705\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013478638990057839\n",
      "Average test loss: 0.0013944653497181005\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013501536699632803\n",
      "Average test loss: 0.001351430730894208\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013464375265770488\n",
      "Average test loss: 0.0014078737893659207\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013465219262573455\n",
      "Average test loss: 0.0013341072311417923\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01347852256976896\n",
      "Average test loss: 0.0013879365882732801\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013466334061490165\n",
      "Average test loss: 0.001327748545135061\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013478751914368736\n",
      "Average test loss: 0.0013438118933182623\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013435425284836027\n",
      "Average test loss: 0.0013738722542507781\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013430239335530334\n",
      "Average test loss: 0.0013679620910228955\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013447024044063356\n",
      "Average test loss: 0.0013615627059092123\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013439312173260583\n",
      "Average test loss: 0.0013304502819147376\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013441820443504386\n",
      "Average test loss: 0.0013750509425169892\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013459587580627866\n",
      "Average test loss: 0.0014799115300282008\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013422777732213339\n",
      "Average test loss: 0.0013407455930589801\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013443432933754391\n",
      "Average test loss: 0.0013422699744502704\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013423889132009613\n",
      "Average test loss: 0.0013766922984893124\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013402082866264714\n",
      "Average test loss: 0.001348170910961926\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013418194672299755\n",
      "Average test loss: 0.0013795273778960109\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013407384090953402\n",
      "Average test loss: 0.0013621022824404968\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01341290847460429\n",
      "Average test loss: 0.0013737831318544017\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013398316426409615\n",
      "Average test loss: 0.0013445322019461957\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013416315817998516\n",
      "Average test loss: 0.001333463254177736\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013381832221315967\n",
      "Average test loss: 0.0014335752910830909\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013385152459144593\n",
      "Average test loss: 0.0013676079500259625\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013373311486509111\n",
      "Average test loss: 0.0013466659363152252\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013403262895014551\n",
      "Average test loss: 0.0013892674587873948\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013373629641201762\n",
      "Average test loss: 0.0013204258653956155\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013372667971584533\n",
      "Average test loss: 0.001331150207358102\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013371698537634478\n",
      "Average test loss: 0.0013776261871680617\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013368980899453164\n",
      "Average test loss: 0.0013547051983575026\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013360984014140235\n",
      "Average test loss: 0.0013264774203093515\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013350848929749595\n",
      "Average test loss: 0.0013584878307042851\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013356388900842932\n",
      "Average test loss: 0.0013930852079970968\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013360266178018517\n",
      "Average test loss: 0.0013265381107727687\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013345782201323245\n",
      "Average test loss: 0.0013620650206382077\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013346571923130088\n",
      "Average test loss: 0.003615243925816483\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013520579176644483\n",
      "Average test loss: 0.0013654957647538848\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01333433463010523\n",
      "Average test loss: 0.0013443782039814524\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013319946634272734\n",
      "Average test loss: 0.0015346513000420398\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013320702423652013\n",
      "Average test loss: 0.0013554918163766463\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013341962334182528\n",
      "Average test loss: 0.0013450861595984962\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01332430842684375\n",
      "Average test loss: 0.0013531986331153246\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013317658594912954\n",
      "Average test loss: 0.0014862053647326926\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013308863653904861\n",
      "Average test loss: 0.0013249264632662137\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013312787976529863\n",
      "Average test loss: 0.0013783037416223022\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013307266394297282\n",
      "Average test loss: 0.0013506367905065416\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013320044140848848\n",
      "Average test loss: 0.0013375505084792772\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013311434081859058\n",
      "Average test loss: 0.0013879247583034966\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013286336426933606\n",
      "Average test loss: 0.001419192476870699\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013307685619427098\n",
      "Average test loss: 0.0014623783509143526\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013296056281361315\n",
      "Average test loss: 0.0013731773707808719\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013281449457009634\n",
      "Average test loss: 0.001363801174176236\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013314602969421281\n",
      "Average test loss: 0.0013233697285047836\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013288238543603155\n",
      "Average test loss: 0.001379420942471673\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01327799200349384\n",
      "Average test loss: 0.0013442140584811568\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013268636547856861\n",
      "Average test loss: 0.0013465284200178253\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013281499430537224\n",
      "Average test loss: 0.0014649632515178787\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013269238961239656\n",
      "Average test loss: 0.0013368650339543819\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013282436944544316\n",
      "Average test loss: 0.0013386625817769931\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01325708291017347\n",
      "Average test loss: 0.0013327858610492614\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013284219343629148\n",
      "Average test loss: 0.0013490490449799431\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01324879785378774\n",
      "Average test loss: 0.0013443876573712463\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01326798062109285\n",
      "Average test loss: 0.0014106852237859534\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013252234691547023\n",
      "Average test loss: 0.0013695420726305908\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013265103912187947\n",
      "Average test loss: 0.0013508373168297113\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013260531335241264\n",
      "Average test loss: 0.0014001278481963607\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013240885206394725\n",
      "Average test loss: 0.0013660015602492623\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01324095634619395\n",
      "Average test loss: 0.0013266484332788322\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013238632026645873\n",
      "Average test loss: 0.0013911322533256478\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01323156376183033\n",
      "Average test loss: 0.0013454644912853837\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013240473295251529\n",
      "Average test loss: 0.0013438718906707233\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013239715459446112\n",
      "Average test loss: 0.0013400039980188013\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013229018477929963\n",
      "Average test loss: 0.0013521111402660607\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013217847304211723\n",
      "Average test loss: 0.0014011984520281354\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013218178740806049\n",
      "Average test loss: 0.0013204929623752833\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013219811376598146\n",
      "Average test loss: 0.0013515996639099385\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013223924954732258\n",
      "Average test loss: 0.0013664216462315785\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01322407792425818\n",
      "Average test loss: 0.0014103981923932832\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013209755698839823\n",
      "Average test loss: 0.0013318102202481694\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013210780707912312\n",
      "Average test loss: 0.0015097923194472161\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01321174409902758\n",
      "Average test loss: 0.0014272286689115895\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01319176248378224\n",
      "Average test loss: 0.0013631700130386484\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013208091044591534\n",
      "Average test loss: 0.0013640327164385882\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013197805217570729\n",
      "Average test loss: 0.0013667066691236364\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013205492395493719\n",
      "Average test loss: 0.0013285219595871038\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013200313890973727\n",
      "Average test loss: 0.001332997431512922\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01318818073388603\n",
      "Average test loss: 0.0013483827465938197\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01319105046408044\n",
      "Average test loss: 0.0013872808030703002\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013190051437252097\n",
      "Average test loss: 0.0013882904603249497\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013176617269714674\n",
      "Average test loss: 0.001355471584221555\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013185225239230526\n",
      "Average test loss: 0.0013334359804737484\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013185917346013917\n",
      "Average test loss: 0.001341866017319262\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013161816622647975\n",
      "Average test loss: 0.0013185494891885254\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013180633207990063\n",
      "Average test loss: 0.0013721825876790617\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013174828893608517\n",
      "Average test loss: 0.0013523782033266292\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013172300400005447\n",
      "Average test loss: 0.0013419530789057414\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013168044270740614\n",
      "Average test loss: 0.0013298993054259981\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013169758062395785\n",
      "Average test loss: 0.0013178577739745379\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013168203290965822\n",
      "Average test loss: 0.0013766699116677045\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013147940388156308\n",
      "Average test loss: 0.0013506700419303444\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013153331577777863\n",
      "Average test loss: 0.0013913695824642977\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013154378699759642\n",
      "Average test loss: 0.0013220139470779234\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013142131554583708\n",
      "Average test loss: 0.0014087242703470918\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013150580085813999\n",
      "Average test loss: 0.0013164056394663122\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013139188436170419\n",
      "Average test loss: 0.0014225246096029878\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013147637005481456\n",
      "Average test loss: 0.001385661660383145\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013145723274184598\n",
      "Average test loss: 0.0013693677876144648\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013159119494259358\n",
      "Average test loss: 0.0013286510456560385\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013130574299229515\n",
      "Average test loss: 0.0014744998998939992\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013123621341254976\n",
      "Average test loss: 0.0014440804602992203\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013143090789516768\n",
      "Average test loss: 0.0014459650659312804\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013124216015554137\n",
      "Average test loss: 0.0013722027552624545\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013125499222013686\n",
      "Average test loss: 0.0013644069864725073\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_80_Depth5/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.03\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.66\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.32\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.57\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.00\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.15\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.92\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.32\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.31\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.7066377833154467\n",
      "Average test loss: 0.010590274636944135\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5525016686386532\n",
      "Average test loss: 0.008675519821130568\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3738332105477651\n",
      "Average test loss: 0.0077474433610008825\n",
      "Epoch 4/300\n",
      "Average training loss: 0.29821598369545405\n",
      "Average test loss: 0.0073409529071715145\n",
      "Epoch 5/300\n",
      "Average training loss: 0.25288169463475546\n",
      "Average test loss: 0.007625813223421573\n",
      "Epoch 6/300\n",
      "Average training loss: 0.22495282300313313\n",
      "Average test loss: 0.007171901696258121\n",
      "Epoch 7/300\n",
      "Average training loss: 0.20401913374000125\n",
      "Average test loss: 0.007102816807313098\n",
      "Epoch 8/300\n",
      "Average training loss: 0.18743261489603255\n",
      "Average test loss: 0.006823113469200002\n",
      "Epoch 9/300\n",
      "Average training loss: 0.17528654148843553\n",
      "Average test loss: 0.006555596639712651\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1653129459619522\n",
      "Average test loss: 0.005910287751505772\n",
      "Epoch 11/300\n",
      "Average training loss: 0.15568802902433607\n",
      "Average test loss: 0.006109520632359717\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14808504396014743\n",
      "Average test loss: 0.006223580758190818\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1410068915552563\n",
      "Average test loss: 0.0062361168637871745\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1368118636475669\n",
      "Average test loss: 0.005456445881062083\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1307584129638142\n",
      "Average test loss: 0.021627424303028317\n",
      "Epoch 16/300\n",
      "Average training loss: 0.125561000890202\n",
      "Average test loss: 0.005620090808305476\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1220378721025255\n",
      "Average test loss: 0.0055434712895916565\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1170960552295049\n",
      "Average test loss: 0.005373906525886721\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11312666543324788\n",
      "Average test loss: 0.005120853985763258\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11053612083858914\n",
      "Average test loss: 0.0051414582584467195\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10692862617969513\n",
      "Average test loss: 0.0058105367235839365\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10564807438850403\n",
      "Average test loss: 0.006314124176485671\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10268298093477884\n",
      "Average test loss: 0.014690436707602606\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1007076808280415\n",
      "Average test loss: 0.004832758023920986\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0988057620856497\n",
      "Average test loss: 0.005357766771482097\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0972185490263833\n",
      "Average test loss: 0.004886216763820913\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09588365769386291\n",
      "Average test loss: 0.0048618992591897645\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09438142160574595\n",
      "Average test loss: 0.004781598530709743\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09302271107170317\n",
      "Average test loss: 0.004734824104027616\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09227618140644497\n",
      "Average test loss: 0.004945431994067298\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09052848407295015\n",
      "Average test loss: 0.004598857346922159\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08983213451835845\n",
      "Average test loss: 0.005002016422235304\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08916293162769741\n",
      "Average test loss: 0.004597632834066947\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08838103535440232\n",
      "Average test loss: 0.005703186310827732\n",
      "Epoch 35/300\n",
      "Average training loss: 0.087521306084262\n",
      "Average test loss: 0.008109043577478992\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0865887744029363\n",
      "Average test loss: 0.005202094948540131\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08594139145480262\n",
      "Average test loss: 0.005207817628565762\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08537050382296245\n",
      "Average test loss: 0.006724164163900747\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08487480666240056\n",
      "Average test loss: 0.004462018099923928\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08420063044627507\n",
      "Average test loss: 0.004514950148761273\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0837761241197586\n",
      "Average test loss: 0.004445346528871192\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08380371350381109\n",
      "Average test loss: 0.011188353047188786\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08276492653621567\n",
      "Average test loss: 0.004538215144019988\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08246346177326308\n",
      "Average test loss: 0.004646804165095091\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08193507703145345\n",
      "Average test loss: 0.00448840907547209\n",
      "Epoch 46/300\n",
      "Average training loss: 235.584025367611\n",
      "Average test loss: 0.07894587063789368\n",
      "Epoch 47/300\n",
      "Average training loss: 6.401589139726427\n",
      "Average test loss: 0.00990003898822599\n",
      "Epoch 48/300\n",
      "Average training loss: 4.152737947887845\n",
      "Average test loss: 0.011801256547371547\n",
      "Epoch 49/300\n",
      "Average training loss: 3.262891254425049\n",
      "Average test loss: 0.009948034467796485\n",
      "Epoch 50/300\n",
      "Average training loss: 2.690961381700304\n",
      "Average test loss: 0.007184431753224797\n",
      "Epoch 51/300\n",
      "Average training loss: 2.2743158808814155\n",
      "Average test loss: 0.008032953354219595\n",
      "Epoch 52/300\n",
      "Average training loss: 1.908097745789422\n",
      "Average test loss: 0.006808780960324738\n",
      "Epoch 53/300\n",
      "Average training loss: 1.579564060529073\n",
      "Average test loss: 0.009638188265677956\n",
      "Epoch 54/300\n",
      "Average training loss: 1.309032109366523\n",
      "Average test loss: 0.006386753853824404\n",
      "Epoch 55/300\n",
      "Average training loss: 1.0892924179501005\n",
      "Average test loss: 0.006133530166000128\n",
      "Epoch 56/300\n",
      "Average training loss: 0.9076986814604865\n",
      "Average test loss: 0.006921949784788821\n",
      "Epoch 57/300\n",
      "Average training loss: 0.7527405134836833\n",
      "Average test loss: 0.020455426800996067\n",
      "Epoch 58/300\n",
      "Average training loss: 0.619359746615092\n",
      "Average test loss: 0.0068605881503058805\n",
      "Epoch 59/300\n",
      "Average training loss: 0.5064240986506144\n",
      "Average test loss: 0.0059327582040180765\n",
      "Epoch 60/300\n",
      "Average training loss: 0.4109499716228909\n",
      "Average test loss: 0.006801863225797812\n",
      "Epoch 61/300\n",
      "Average training loss: 0.3347424733903673\n",
      "Average test loss: 0.018038567307922575\n",
      "Epoch 62/300\n",
      "Average training loss: 0.278677941110399\n",
      "Average test loss: 0.006435399604340394\n",
      "Epoch 63/300\n",
      "Average training loss: 0.2360432498190138\n",
      "Average test loss: 0.009853398186879025\n",
      "Epoch 64/300\n",
      "Average training loss: 0.20250903092490302\n",
      "Average test loss: 0.09091110888620217\n",
      "Epoch 65/300\n",
      "Average training loss: 0.17815536824862163\n",
      "Average test loss: 0.005066485332532062\n",
      "Epoch 66/300\n",
      "Average training loss: 0.16054882423082986\n",
      "Average test loss: 0.0050658126105037\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1468672117922041\n",
      "Average test loss: 0.007798056143025557\n",
      "Epoch 68/300\n",
      "Average training loss: 0.13581839150852626\n",
      "Average test loss: 0.004851621245551441\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12695481091075472\n",
      "Average test loss: 0.004732162073668506\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12021716510587269\n",
      "Average test loss: 0.004716111655450529\n",
      "Epoch 71/300\n",
      "Average training loss: 0.1144059173795912\n",
      "Average test loss: 0.0047310916889044975\n",
      "Epoch 72/300\n",
      "Average training loss: 0.10959828938378229\n",
      "Average test loss: 0.004643152305649386\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10559834891557693\n",
      "Average test loss: 0.0046178066711872814\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10268935519456864\n",
      "Average test loss: 0.004631001908746031\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09987773804532157\n",
      "Average test loss: 0.004777301769703627\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09751555214325587\n",
      "Average test loss: 0.004694459850589435\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09567704635196261\n",
      "Average test loss: 0.005266818210068676\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09412911027669907\n",
      "Average test loss: 0.004519508755869335\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0924876314037376\n",
      "Average test loss: 0.0045994155084093415\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09112351586090194\n",
      "Average test loss: 0.004671166179080804\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08989776135153241\n",
      "Average test loss: 0.006283250150167279\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08897381584511863\n",
      "Average test loss: 0.004474645797577169\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08797582776016659\n",
      "Average test loss: 0.004624257240444422\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08705647138092253\n",
      "Average test loss: 0.00460148247745302\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08638027517000835\n",
      "Average test loss: 0.004419203971202175\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08557200673553679\n",
      "Average test loss: 0.005125385327057706\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08504211183389028\n",
      "Average test loss: 0.004455522834012906\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08440437403652402\n",
      "Average test loss: 0.004436496002392636\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08402549231383535\n",
      "Average test loss: 0.004418511158476273\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08347722141610252\n",
      "Average test loss: 0.004504230918983618\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0831736042631997\n",
      "Average test loss: 0.004352085765037272\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08266678979330593\n",
      "Average training loss: 0.08185632956027984\n",
      "Average test loss: 0.004355819592873255\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08158023281561004\n",
      "Average test loss: 0.004340720839798451\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08133280217647552\n",
      "Average test loss: 0.004416655060317781\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0811299011905988\n",
      "Average test loss: 0.004802012734942966\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08070400498641862\n",
      "Average test loss: 0.004368449507488145\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0804999514023463\n",
      "Average test loss: 0.004337175592780114\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08105498000979423\n",
      "Average test loss: 0.004529152365194427\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0797594956225819\n",
      "Average test loss: 0.004332664773282077\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07970612209704186\n",
      "Average test loss: 0.004335416229648723\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07912566865152783\n",
      "Average test loss: 0.004801938941081365\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07871977676285638\n",
      "Average test loss: 0.004421731117698881\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07839982227484385\n",
      "Average test loss: 0.010780241110258632\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07824740242295795\n",
      "Average training loss: 0.0777301618622409\n",
      "Average test loss: 0.0043236755970865485\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07754525792267587\n",
      "Average test loss: 0.004487663168874052\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07729671485556497\n",
      "Average test loss: 1.920947908613417\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0772799483206537\n",
      "Average test loss: 0.004389618021125595\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07657391379276911\n",
      "Average test loss: 0.004334570593304104\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07638522110382716\n",
      "Average test loss: 0.004774526422222455\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07633212326632606\n",
      "Average test loss: 0.004267015999183059\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07625177237722609\n",
      "Average test loss: 0.004308320233805312\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0759642317030165\n",
      "Average test loss: 0.004284461780761679\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07583792544404666\n",
      "Average test loss: 0.004506158526572916\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07564135301113128\n",
      "Average test loss: 0.004628166702679462\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0753965879480044\n",
      "Average test loss: 0.004296902353978819\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07539950622121493\n",
      "Average test loss: 0.0043090551905334\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07501338137520684\n",
      "Average test loss: 0.004276891177313195\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07680862839354409\n",
      "Average test loss: 0.0043801033082935545\n",
      "Epoch 128/300\n",
      "Average training loss: 6.561606419881185\n",
      "Average test loss: 0.02129476746585634\n",
      "Epoch 130/300\n",
      "Average training loss: 5.109086784362793\n",
      "Average test loss: 0.02213392734527588\n",
      "Epoch 131/300\n",
      "Average training loss: 4.445549769083659\n",
      "Average test loss: 0.029598898229499657\n",
      "Epoch 132/300\n",
      "Average training loss: 4.110203217188517\n",
      "Average test loss: 0.014180497919933664\n",
      "Epoch 133/300\n",
      "Average training loss: 3.810309514363607\n",
      "Average test loss: 0.11386017067233721\n",
      "Epoch 134/300\n",
      "Average training loss: 3.5170268177456325\n",
      "Average test loss: 0.008512464485234685\n",
      "Epoch 135/300\n",
      "Average training loss: 3.23328083313836\n",
      "Average test loss: 0.013195247947341867\n",
      "Epoch 136/300\n",
      "Average training loss: 3.0111940716637506\n",
      "Average test loss: 0.00827607806854778\n",
      "Epoch 137/300\n",
      "Average training loss: 2.8248911711374918\n",
      "Average test loss: 0.007082179906467596\n",
      "Epoch 138/300\n",
      "Average training loss: 2.6488091496361625\n",
      "Average test loss: 0.009218396220770147\n",
      "Epoch 139/300\n",
      "Average training loss: 2.480756494310167\n",
      "Average test loss: 0.006498611244890425\n",
      "Epoch 140/300\n",
      "Average training loss: 2.3220268092685274\n",
      "Average test loss: 0.08977424499144157\n",
      "Epoch 141/300\n",
      "Average training loss: 2.1698511746724445\n",
      "Average test loss: 0.006487953028745121\n",
      "Epoch 142/300\n",
      "Average training loss: 2.023218595822652\n",
      "Average test loss: 0.006476775717404154\n",
      "Epoch 143/300\n",
      "Average training loss: 1.883755897839864\n",
      "Average test loss: 0.011119391427271896\n",
      "Epoch 144/300\n",
      "Average training loss: 1.7489564774831137\n",
      "Average test loss: 0.005728849904818667\n",
      "Epoch 145/300\n",
      "Average training loss: 1.6232790274090236\n",
      "Average test loss: 0.005616286259558466\n",
      "Epoch 146/300\n",
      "Average training loss: 1.503020766682095\n",
      "Average test loss: 0.0054895877540111545\n",
      "Epoch 147/300\n",
      "Average training loss: 1.3903742433124118\n",
      "Average test loss: 0.12175818127393723\n",
      "Epoch 148/300\n",
      "Average training loss: 1.280368368466695\n",
      "Average test loss: 0.007103496408296956\n",
      "Epoch 149/300\n",
      "Average training loss: 1.176574608484904\n",
      "Average test loss: 0.005652069086829821\n",
      "Epoch 150/300\n",
      "Average training loss: 1.0776705223719278\n",
      "Average test loss: 0.005799524181005027\n",
      "Epoch 151/300\n",
      "Average training loss: 0.9826946222517225\n",
      "Average test loss: 0.004998304993328121\n",
      "Epoch 152/300\n",
      "Average training loss: 0.8939954049852159\n",
      "Average test loss: 0.005079639054007001\n",
      "Epoch 153/300\n",
      "Average training loss: 0.8120807699627346\n",
      "Average test loss: 0.005055594842880964\n",
      "Epoch 154/300\n",
      "Average training loss: 0.7321180342038472\n",
      "Average test loss: 0.004924306294984288\n",
      "Epoch 155/300\n",
      "Average training loss: 0.654180257903205\n",
      "Average test loss: 0.004889695552902089\n",
      "Epoch 156/300\n",
      "Average training loss: 0.5732232121891445\n",
      "Average test loss: 0.005406504176143143\n",
      "Epoch 157/300\n",
      "Average training loss: 0.48878160892592537\n",
      "Average test loss: 0.0047001082870281406\n",
      "Epoch 158/300\n",
      "Average training loss: 0.39564997357792325\n",
      "Average test loss: 0.02706690067383978\n",
      "Epoch 159/300\n",
      "Average training loss: 0.30858532280392115\n",
      "Average test loss: 0.004667957345644633\n",
      "Epoch 160/300\n",
      "Average training loss: 0.1514056449068917\n",
      "Average test loss: 0.00455078241849939\n",
      "Epoch 163/300\n",
      "Average training loss: 0.13157001203298568\n",
      "Average test loss: 0.004529251744349798\n",
      "Epoch 164/300\n",
      "Average training loss: 0.12090399350060357\n",
      "Average test loss: 0.0045709217780580125\n",
      "Epoch 165/300\n",
      "Average training loss: 0.11442188051011827\n",
      "Average test loss: 0.0044727892022993826\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10894446274969313\n",
      "Average test loss: 0.0044229468645321\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10446132416857613\n",
      "Average test loss: 0.004423087101843622\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10064275419712067\n",
      "Average test loss: 0.004382054486001531\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09637010531955295\n",
      "Average test loss: 0.004324938611230916\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09336165873209636\n",
      "Average test loss: 0.004361995266750455\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09112563859091864\n",
      "Average test loss: 0.004372593286136786\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08907865554094314\n",
      "Average test loss: 0.0043885232321918015\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08766623981131448\n",
      "Average test loss: 0.005495966069400311\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08626634773943159\n",
      "Average test loss: 0.004281545990457138\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08473518336481518\n",
      "Average test loss: 0.004643352154642343\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0835319819384151\n",
      "Average test loss: 0.004250987543000116\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08257441900836097\n",
      "Average test loss: 0.004274774903224574\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08106125242842568\n",
      "Average test loss: 0.004308283276442024\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08021235879262288\n",
      "Average test loss: 0.005990599150872892\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07970792086919148\n",
      "Average test loss: 0.004259297374222014\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07902349448866314\n",
      "Average test loss: 0.004267657873945104\n",
      "Epoch 183/300\n",
      "Average training loss: 1.1393787130382327\n",
      "Average test loss: 0.011735869915948974\n",
      "Epoch 184/300\n",
      "Average training loss: 1.3684415363735623\n",
      "Average test loss: 0.005679389940781726\n",
      "Epoch 185/300\n",
      "Average training loss: 0.8755655861960517\n",
      "Average test loss: 0.052763522853453954\n",
      "Epoch 186/300\n",
      "Average training loss: 0.6487121511035495\n",
      "Average test loss: 0.004972491565677855\n",
      "Epoch 187/300\n",
      "Average training loss: 0.4930683549775018\n",
      "Average test loss: 0.007148051339719031\n",
      "Epoch 188/300\n",
      "Average training loss: 0.37421020325024923\n",
      "Average test loss: 0.004756381195038557\n",
      "Epoch 189/300\n",
      "Average training loss: 0.28710281123055353\n",
      "Average test loss: 0.006448625717726019\n",
      "Epoch 190/300\n",
      "Average training loss: 0.2227799786461724\n",
      "Average test loss: 0.0264263483453542\n",
      "Epoch 191/300\n",
      "Average training loss: 0.17297272453043197\n",
      "Average test loss: 0.04846863364138537\n",
      "Epoch 192/300\n",
      "Average training loss: 0.14344254485766092\n",
      "Average test loss: 0.0045552739927338225\n",
      "Epoch 193/300\n",
      "Average training loss: 0.12454530649715\n",
      "Average test loss: 0.004702206582658821\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10453442752361297\n",
      "Average test loss: 0.004457932086454498\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09948829502198431\n",
      "Average test loss: 0.004446652080863714\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09555809741550021\n",
      "Average test loss: 0.004421228344655699\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09278382325172424\n",
      "Average test loss: 0.0043596177515056395\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09071002858214908\n",
      "Average test loss: 0.004372194981823365\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08905824364556207\n",
      "Average test loss: 0.004331532792498668\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08739553890625636\n",
      "Average test loss: 0.004722418556196822\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08617884438567691\n",
      "Average test loss: 0.004286138972474469\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08489521282911301\n",
      "Average test loss: 0.004307540761099921\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08372282810012499\n",
      "Average test loss: 0.0042857098273105096\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08304465300507016\n",
      "Average test loss: 0.004323837561325894\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0820725440647867\n",
      "Average test loss: 0.004275361579118503\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08116849744319916\n",
      "Average test loss: 0.004281093631767564\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08051004132297304\n",
      "Average test loss: 0.004269578796500961\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07934281475014157\n",
      "Average test loss: 0.004288319856342342\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0789599903623263\n",
      "Average test loss: 0.004302266627136204\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07831860407855776\n",
      "Average test loss: 0.004248711045831442\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07803040805459023\n",
      "Average test loss: 0.0042794872605138356\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07778493168618944\n",
      "Average test loss: 0.004300617452917828\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0775631223751439\n",
      "Average test loss: 0.0045615507264932\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07709421363141801\n",
      "Average test loss: 0.004288512540774213\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07688067828946643\n",
      "Average test loss: 0.0042790927170879315\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07660444589455923\n",
      "Average test loss: 0.008360233958396647\n",
      "Epoch 219/300\n",
      "Average training loss: 42064.88799531424\n",
      "Average test loss: 0.5162605482737224\n",
      "Epoch 220/300\n",
      "Average training loss: 7.874550457424587\n",
      "Average test loss: 0.0760823467704985\n",
      "Epoch 221/300\n",
      "Average training loss: 6.933823552449544\n",
      "Average test loss: 0.059398419744438595\n",
      "Epoch 222/300\n",
      "Average training loss: 6.473651558346218\n",
      "Average test loss: 0.11810927938752704\n",
      "Epoch 223/300\n",
      "Average training loss: 6.080276026407877\n",
      "Average test loss: 0.03345619891087214\n",
      "Epoch 224/300\n",
      "Average training loss: 5.764701688554552\n",
      "Average test loss: 0.02386421830786599\n",
      "Epoch 225/300\n",
      "Average training loss: 5.50815543323093\n",
      "Average test loss: 0.02008581839998563\n",
      "Epoch 226/300\n",
      "Average training loss: 5.10654728020562\n",
      "Average test loss: 0.015367329173617893\n",
      "Epoch 228/300\n",
      "Average training loss: 4.9263979907565645\n",
      "Average test loss: 0.018846902324093712\n",
      "Epoch 229/300\n",
      "Average training loss: 4.747984454684787\n",
      "Average test loss: 0.020452960337201755\n",
      "Epoch 230/300\n",
      "Average training loss: 4.56586835776435\n",
      "Average test loss: 0.012395075439578957\n",
      "Epoch 231/300\n",
      "Average training loss: 4.388958697001139\n",
      "Average test loss: 0.010664329373174243\n",
      "Epoch 232/300\n",
      "Average training loss: 4.21494167582194\n",
      "Average test loss: 0.011291610442929798\n",
      "Epoch 233/300\n",
      "Average training loss: 4.044708139207628\n",
      "Average test loss: 0.010264389526512888\n",
      "Epoch 234/300\n",
      "Average training loss: 3.8844673245747883\n",
      "Average test loss: 0.00838677257216639\n",
      "Epoch 235/300\n",
      "Average training loss: 3.72852401775784\n",
      "Average test loss: 0.007609434871209992\n",
      "Epoch 236/300\n",
      "Average training loss: 3.5665299457973902\n",
      "Average test loss: 0.007108356864915954\n",
      "Epoch 237/300\n",
      "Average training loss: 3.38259523540073\n",
      "Average test loss: 0.007062279487235678\n",
      "Epoch 238/300\n",
      "Average training loss: 3.187540794796414\n",
      "Average test loss: 0.006699288136843178\n",
      "Epoch 239/300\n",
      "Average training loss: 2.9636349750094944\n",
      "Average test loss: 0.006595229800376627\n",
      "Epoch 240/300\n",
      "Average training loss: 2.7122638931274414\n",
      "Average test loss: 0.00646098580211401\n",
      "Epoch 241/300\n",
      "Average training loss: 2.390766000535753\n",
      "Average test loss: 0.006456978539625804\n",
      "Epoch 242/300\n",
      "Average training loss: 1.9616406121783787\n",
      "Average test loss: 0.006101803375615014\n",
      "Epoch 243/300\n",
      "Average training loss: 1.523840494579739\n",
      "Average test loss: 0.005868200994614098\n",
      "Epoch 245/300\n",
      "Average training loss: 1.324275158882141\n",
      "Average test loss: 0.005905505307018757\n",
      "Epoch 246/300\n",
      "Average training loss: 1.1346491318808662\n",
      "Average test loss: 0.005652177524235513\n",
      "Epoch 247/300\n",
      "Average training loss: 0.9654480723275078\n",
      "Average test loss: 0.005652254402637481\n",
      "Epoch 248/300\n",
      "Average training loss: 0.8185804527600606\n",
      "Average test loss: 0.0052675336574514705\n",
      "Epoch 249/300\n",
      "Average training loss: 0.6839314081403944\n",
      "Average test loss: 0.0052495260934034985\n",
      "Epoch 250/300\n",
      "Average training loss: 0.5660852927631802\n",
      "Average test loss: 0.005043744165243374\n",
      "Epoch 251/300\n",
      "Average training loss: 0.46754227696524725\n",
      "Average test loss: 0.004942912654330333\n",
      "Epoch 252/300\n",
      "Average training loss: 0.3841706898212433\n",
      "Average test loss: 0.005032068296853039\n",
      "Epoch 253/300\n",
      "Average training loss: 0.3139807615015242\n",
      "Average test loss: 0.004802934852325254\n",
      "Epoch 254/300\n",
      "Average training loss: 0.25665129182073804\n",
      "Average test loss: 0.004791451494312949\n",
      "Epoch 255/300\n",
      "Average training loss: 0.21219699466228484\n",
      "Average test loss: 0.0047188013440204995\n",
      "Epoch 256/300\n",
      "Average training loss: 0.17739182391431596\n",
      "Average test loss: 0.0046803303389913505\n",
      "Epoch 257/300\n",
      "Average training loss: 0.15275723436143662\n",
      "Average test loss: 0.004576290579719676\n",
      "Epoch 258/300\n",
      "Average training loss: 0.13633600495921241\n",
      "Average test loss: 0.006647566857437293\n",
      "Epoch 259/300\n",
      "Average training loss: 0.12532856033907996\n",
      "Average test loss: 0.004736689545214176\n",
      "Epoch 260/300\n",
      "Average training loss: 0.11743009674549103\n",
      "Average test loss: 0.004477272991918855\n",
      "Epoch 261/300\n",
      "Average training loss: 0.11092366165584988\n",
      "Average test loss: 0.004546572222891781\n",
      "Epoch 262/300\n",
      "Average training loss: 0.10556754856639439\n",
      "Average test loss: 0.005199643945942322\n",
      "Epoch 263/300\n",
      "Average training loss: 0.10067770708931817\n",
      "Average test loss: 0.004426579413935542\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09632120313909319\n",
      "Average test loss: 0.004367299377090401\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09331724475489722\n",
      "Average test loss: 0.00441893753161033\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09041903813017739\n",
      "Average test loss: 0.004334020516317752\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08828967524237102\n",
      "Average test loss: 0.004271649888820118\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08653439291318257\n",
      "Average test loss: 0.0044359059642172525\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0850314168466462\n",
      "Average test loss: 0.004369099166244269\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08352368542220857\n",
      "Average test loss: 0.004310723548548089\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08258309158351686\n",
      "Average test loss: 0.004346292800580462\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08135536258750492\n",
      "Average test loss: 0.004271650922381216\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08045826644367642\n",
      "Average test loss: 0.004233604647219181\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07959017989370558\n",
      "Average test loss: 0.004216067313320107\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07894777457581625\n",
      "Average test loss: 0.004242693741909331\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07826042423976792\n",
      "Average test loss: 0.004234134659998947\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07774043230215709\n",
      "Average test loss: 0.004239337011343903\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07730826782517963\n",
      "Average test loss: 0.00427024524162213\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07628150446547402\n",
      "Average test loss: 0.004288301266315911\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07619761774937311\n",
      "Average test loss: 0.004221982089181741\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07603050431278016\n",
      "Average test loss: 0.0042735407166183\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07579609801040756\n",
      "Average test loss: 0.004761462184290091\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07561469236016273\n",
      "Average test loss: 0.00425011978422602\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07547228980726665\n",
      "Average test loss: 0.004251140006300476\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07540216427379184\n",
      "Average test loss: 0.008508993218756384\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07521929026312298\n",
      "Average test loss: 0.004267731931474473\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0750924425456259\n",
      "Average test loss: 0.005088506817817688\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07760319038563304\n",
      "Average test loss: 0.004243612046457\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07478129528628455\n",
      "Average test loss: 0.004273774555987782\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07464768484897084\n",
      "Average test loss: 0.004256581621037589\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07452326649427414\n",
      "Average test loss: 0.004244227827423149\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07441340439187155\n",
      "Average test loss: 0.004224941387358639\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07434451872441504\n",
      "Average test loss: 0.004421361106965277\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07426119149393505\n",
      "Average test loss: 0.0043949341567026245\n",
      "Epoch 297/300\n",
      "Average training loss: 0.074123514201906\n",
      "Average test loss: 0.004218917481187317\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07536149313052495\n",
      "Average test loss: 0.004229863457588686\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4689226337009007\n",
      "Average test loss: 0.006271877598431375\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4771192236741384\n",
      "Average test loss: 0.005678641121834516\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3194232775900099\n",
      "Average test loss: 0.005111560510678423\n",
      "Epoch 4/300\n",
      "Average training loss: 0.24589177747567495\n",
      "Average test loss: 0.00485099389611019\n",
      "Epoch 5/300\n",
      "Average training loss: 0.20242009335094027\n",
      "Average test loss: 0.0044627958341605135\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17482175095876057\n",
      "Average test loss: 0.004284385610371828\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1555198041730457\n",
      "Average test loss: 0.004532125634244747\n",
      "Epoch 8/300\n",
      "Average training loss: 0.14150110040770636\n",
      "Average test loss: 0.004174478280461497\n",
      "Epoch 9/300\n",
      "Average training loss: 0.13005403210057154\n",
      "Average test loss: 0.003952151952518357\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12149548587534163\n",
      "Average test loss: 0.003940452576304475\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11352252074082693\n",
      "Average test loss: 0.004112874732249313\n",
      "Epoch 12/300\n",
      "Average training loss: 0.10760295403665966\n",
      "Average test loss: 0.006260829603920381\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1019461756017473\n",
      "Average test loss: 0.0033928946705742017\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09762732527653377\n",
      "Average test loss: 0.014845299642946985\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09324103425608742\n",
      "Average test loss: 0.0032996822506603267\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08997310084104539\n",
      "Average test loss: 0.0035834067766037253\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08633667852481207\n",
      "Average test loss: 0.003079964524341954\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0840651079416275\n",
      "Average test loss: 0.0030612568143341275\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08030099932021564\n",
      "Average test loss: 0.0029796604323718283\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07785602003335952\n",
      "Average test loss: 0.003404755014926195\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0750884238547749\n",
      "Average test loss: 0.0029381247456702923\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07257527086469862\n",
      "Average test loss: 0.0031353894209282264\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0702621689107683\n",
      "Average test loss: 0.002863627141341567\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06817099724213282\n",
      "Average test loss: 0.004857833470735285\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06683369038171239\n",
      "Average test loss: 0.002771105688272251\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06495649422539605\n",
      "Average test loss: 0.002762778471534451\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06370096674892638\n",
      "Average test loss: 0.0026699972276886304\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06203077996770541\n",
      "Average test loss: 0.0030889588511652417\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06076173275046878\n",
      "Average test loss: 0.002622263066470623\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05937704081998931\n",
      "Average test loss: 0.0026677938426534335\n",
      "Epoch 31/300\n",
      "Average training loss: 0.058450146045949725\n",
      "Average test loss: 0.002926361634499497\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05756582897570398\n",
      "Average test loss: 0.00260678926606973\n",
      "Epoch 33/300\n",
      "Average training loss: 0.056856715987126036\n",
      "Average test loss: 0.0028610109206702976\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05600272101495001\n",
      "Average test loss: 0.004148620339317454\n",
      "Epoch 35/300\n",
      "Average training loss: 0.055903738962279424\n",
      "Average test loss: 0.002706000078469515\n",
      "Epoch 36/300\n",
      "Average training loss: 0.054778236207034856\n",
      "Average test loss: 0.002589436388057139\n",
      "Epoch 37/300\n",
      "Average training loss: 0.054256336162487664\n",
      "Average test loss: 0.0025262548464039963\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05370327306787173\n",
      "Average test loss: 0.0025617434010944433\n",
      "Epoch 39/300\n",
      "Average training loss: 0.053516192480921744\n",
      "Average test loss: 0.0029794799745496777\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05288787220252885\n",
      "Average test loss: 0.0025171006224635572\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06092637769050068\n",
      "Average test loss: 0.002536682330071926\n",
      "Epoch 42/300\n",
      "Average training loss: 0.19524115009771453\n",
      "Average test loss: 0.0028912962350166505\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08125741671853595\n",
      "Average test loss: 0.002789288136901127\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07037797042396333\n",
      "Average test loss: 0.00269813828335868\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06551303620470894\n",
      "Average test loss: 0.0026847702800813647\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06266303459803263\n",
      "Average test loss: 0.0026123958879874813\n",
      "Epoch 47/300\n",
      "Average training loss: 0.060736909778581726\n",
      "Average test loss: 0.0026199549943622617\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0593110626273685\n",
      "Average test loss: 0.0025918176331453852\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05810307691163487\n",
      "Average test loss: 0.0025585338583009112\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05701065525743696\n",
      "Average test loss: 0.0025641955689837534\n",
      "Epoch 51/300\n",
      "Average training loss: 0.056172979119751186\n",
      "Average test loss: 0.0027985827440602912\n",
      "Epoch 52/300\n",
      "Average training loss: 0.055506396926111645\n",
      "Average test loss: 0.002580391245790654\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05484444753660096\n",
      "Average test loss: 0.0025386589482012723\n",
      "Epoch 54/300\n",
      "Average training loss: 0.054386505474646886\n",
      "Average test loss: 0.0025399131613473096\n",
      "Epoch 55/300\n",
      "Average training loss: 0.053859068602323534\n",
      "Average test loss: 0.0024964239115102425\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05349288275506761\n",
      "Average test loss: 0.0024946231647498076\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05311376040511661\n",
      "Average test loss: 0.0028988848643170463\n",
      "Epoch 58/300\n",
      "Average training loss: 0.052823020209868746\n",
      "Average test loss: 0.0038501517358753416\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05284196153283119\n",
      "Average test loss: 0.002472049305525919\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05187073784735468\n",
      "Average test loss: 0.0025645322710689572\n",
      "Epoch 63/300\n",
      "Average training loss: 0.051711350874768365\n",
      "Average test loss: 0.00249663191061053\n",
      "Epoch 64/300\n",
      "Average training loss: 0.051272316131326885\n",
      "Average test loss: 0.0024927955056644148\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05108617361717754\n",
      "Average test loss: 0.002491083206298451\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05094898356662856\n",
      "Average test loss: 0.0025297127759291065\n",
      "Epoch 67/300\n",
      "Average training loss: 0.051025753398736315\n",
      "Average test loss: 0.003152869144661559\n",
      "Epoch 68/300\n",
      "Average training loss: 0.050611410707235335\n",
      "Average test loss: 0.0025116637537462845\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05224179594384299\n",
      "Average test loss: 0.0025672487668279144\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05370065489411354\n",
      "Average test loss: 0.002678271341034108\n",
      "Epoch 71/300\n",
      "Average training loss: 0.052191431326998605\n",
      "Average test loss: 0.0025269669774505826\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05061309543251991\n",
      "Average test loss: 0.0024741516299545763\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05021603626840644\n",
      "Average test loss: 0.0028373858119464584\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04989282229873869\n",
      "Average test loss: 0.0024738480363869003\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04979889358414544\n",
      "Average test loss: 0.007110346239474085\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04928185507655144\n",
      "Average test loss: 0.002446027751391133\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04927466355926461\n",
      "Average test loss: 0.0024756100210878585\n",
      "Epoch 80/300\n",
      "Average training loss: 0.049184403436051474\n",
      "Average test loss: 0.0024741755589428875\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04889573307169808\n",
      "Average test loss: 0.002429590772009558\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04884361637632052\n",
      "Average test loss: 0.0024692574340022273\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0485579709245099\n",
      "Average test loss: 0.0024146226983931328\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04867166524794367\n",
      "Average test loss: 0.0025405101008299322\n",
      "Epoch 85/300\n",
      "Average training loss: 0.048267356865935855\n",
      "Average test loss: 0.0024089793070322936\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05078649984134568\n",
      "Average test loss: 0.002386560800174872\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0856532355149587\n",
      "Average test loss: 0.0028454131616486443\n",
      "Epoch 88/300\n",
      "Average training loss: 0.056543829802009796\n",
      "Average test loss: 0.002590677566308942\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05254546627402305\n",
      "Average test loss: 0.0024844456815885175\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05093294657601251\n",
      "Average test loss: 0.002466945835906598\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05015469119946162\n",
      "Average test loss: 0.002882701235098971\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04924450945854187\n",
      "Average test loss: 0.0024511820091348557\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04913203447394901\n",
      "Average test loss: 0.0024678016047303877\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04839142395390405\n",
      "Average test loss: 0.00240786457579169\n",
      "Epoch 96/300\n",
      "Average training loss: 0.048156229813893635\n",
      "Average test loss: 0.0034576502920438848\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04805078610777855\n",
      "Average test loss: 0.0024070606198575762\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04802706348896026\n",
      "Average test loss: 0.002389525538103448\n",
      "Epoch 99/300\n",
      "Average training loss: 0.047761162863837345\n",
      "Average test loss: 0.01009857225500875\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04771761204136742\n",
      "Average test loss: 0.0024103717137542034\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04842196401125855\n",
      "Average test loss: 0.0025393357982652054\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04753122636675835\n",
      "Average test loss: 0.0024819101486355066\n",
      "Epoch 103/300\n",
      "Average training loss: 0.047504436208142174\n",
      "Average test loss: 0.0023774208799004553\n",
      "Epoch 104/300\n",
      "Average training loss: 0.047442946602900826\n",
      "Average test loss: 0.0026730635102010436\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04728404027223587\n",
      "Average test loss: 0.002380997076423632\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04729225834210714\n",
      "Average test loss: 0.002692249382742577\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04725680020120409\n",
      "Average test loss: 0.002407487985988458\n",
      "Epoch 108/300\n",
      "Average training loss: 0.046973821580410004\n",
      "Average test loss: 0.0024180247493916086\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04718583098385069\n",
      "Average test loss: 0.002397160357899136\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05336259723040793\n",
      "Average test loss: 0.002446691131219268\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04827501499652863\n",
      "Average test loss: 0.0023728411755421096\n",
      "Epoch 112/300\n",
      "Average test loss: 0.0025060485634538863\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04695914758576287\n",
      "Average test loss: 0.003081945259951883\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04678994860914019\n",
      "Average test loss: 0.0025914856317556566\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04684118311935001\n",
      "Average test loss: 0.0024712232460992203\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04659643481175105\n",
      "Average test loss: 0.0023873398908310466\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04661567384004593\n",
      "Average test loss: 0.002664783186589678\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04741895961761475\n",
      "Average test loss: 0.0023717963825911283\n",
      "Epoch 120/300\n",
      "Average training loss: 0.046258174482319094\n",
      "Average test loss: 0.002386543129881223\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04621041289303038\n",
      "Average test loss: 0.0023731504109584622\n",
      "Epoch 124/300\n",
      "Average training loss: 0.046238920400540034\n",
      "Average test loss: 0.0033944886898001035\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04621277128987842\n",
      "Average test loss: 0.0026530415506826506\n",
      "Epoch 126/300\n",
      "Average training loss: 0.046046109267406994\n",
      "Average test loss: 0.002375993660547667\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04606149055891567\n",
      "Average test loss: 0.002469756942242384\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0461172883146339\n",
      "Average test loss: 0.002408320968763696\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04761008565293418\n",
      "Average test loss: 0.0023843247718695138\n",
      "Epoch 130/300\n",
      "Average training loss: 0.045909990612003536\n",
      "Average test loss: 0.0023572140735470585\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04580097750160429\n",
      "Average test loss: 0.002354326366032991\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04571362827552689\n",
      "Average test loss: 0.004736154983854956\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04566246988375982\n",
      "Average test loss: 0.0023725801415534482\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0457359906302558\n",
      "Average test loss: 0.0024268412105739115\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04603184826506509\n",
      "Average test loss: 0.034079709966977435\n",
      "Epoch 136/300\n",
      "Average training loss: 0.046849613030751545\n",
      "Average test loss: 0.0023788989515354238\n",
      "Epoch 137/300\n",
      "Average training loss: 0.045394875499937266\n",
      "Average test loss: 0.0032699587208529315\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04537827642924256\n",
      "Average test loss: 0.002909360916250282\n",
      "Epoch 141/300\n",
      "Average training loss: 0.045389148278368846\n",
      "Average test loss: 0.0024721593264904287\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04565288678805034\n",
      "Average test loss: 0.005037661067210138\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04715690673722161\n",
      "Average test loss: 4.848599493874444\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0454905310107602\n",
      "Average test loss: 0.0023518581918130317\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04576828650633494\n",
      "Average test loss: 0.002982344113704231\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04516354548599985\n",
      "Average test loss: 0.00242292374631183\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04503943247596423\n",
      "Average test loss: 0.0024069194443937804\n",
      "Epoch 148/300\n",
      "Average training loss: 0.045042899956305824\n",
      "Average test loss: 0.0024677886772486897\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04506675909956296\n",
      "Average test loss: 0.002397879120997257\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04523134900132815\n",
      "Average test loss: 0.002382479843787021\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04499225594268905\n",
      "Average test loss: 0.0023855138483146825\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04512880000803206\n",
      "Average test loss: 0.002880420364956889\n",
      "Epoch 153/300\n",
      "Average training loss: 0.045811621218919754\n",
      "Average test loss: 0.003204636830629574\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0449467013809416\n",
      "Average test loss: 0.0030066231733394994\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04474529890881644\n",
      "Average test loss: 0.003048320334404707\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04472054227524334\n",
      "Average test loss: 0.05012726926306883\n",
      "Epoch 158/300\n",
      "Average training loss: 0.044675182167026734\n",
      "Average test loss: 602523183788.487\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04528992201884587\n",
      "Average test loss: 0.002419656158528394\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04477504856056637\n",
      "Average test loss: 0.0023552837791956135\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04476923684279124\n",
      "Average test loss: 0.002344526474364102\n",
      "Epoch 162/300\n",
      "Average training loss: 0.044641471846236125\n",
      "Average test loss: 0.0023766408633026813\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04454284938838747\n",
      "Average test loss: 0.003262931319160594\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04454696786403656\n",
      "Average test loss: 0.0024337314647725886\n",
      "Epoch 165/300\n",
      "Average training loss: 0.044459524078501594\n",
      "Average test loss: 0.0024081368289060063\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04483477921949493\n",
      "Average test loss: 0.004151766132873794\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0447645746436384\n",
      "Average test loss: 0.025763404544856813\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04472472426626417\n",
      "Average test loss: 0.002407983499682612\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04483052592807346\n",
      "Average test loss: 0.0023409608014755777\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0443774372835954\n",
      "Average test loss: 0.0025252759402824773\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04420948920647303\n",
      "Average test loss: 0.0023892471169432005\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0444398188955254\n",
      "Average test loss: 0.0023741147816181184\n",
      "Epoch 175/300\n",
      "Average training loss: 0.044818485935529076\n",
      "Average test loss: 0.0024622269612219598\n",
      "Epoch 176/300\n",
      "Average training loss: 0.044105425205495624\n",
      "Average test loss: 0.0024129127928366263\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04404730394151476\n",
      "Average test loss: 0.0023753177443933157\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04411389810509152\n",
      "Average test loss: 0.0037648177937501005\n",
      "Epoch 179/300\n",
      "Average test loss: 0.002412058544655641\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04399464011689027\n",
      "Average test loss: 0.0023893531252526575\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04448082843754027\n",
      "Average test loss: 0.002391788927424285\n",
      "Epoch 183/300\n",
      "Average training loss: 0.044642960417601796\n",
      "Average test loss: 0.0024115487724128697\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04404442148076163\n",
      "Average test loss: 0.1422591248187754\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04407204201155239\n",
      "Average test loss: 0.002717498001952966\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04402741961843438\n",
      "Average test loss: 0.002366782895806763\n",
      "Epoch 187/300\n",
      "Average training loss: 0.043740704400671855\n",
      "Average test loss: 0.0023779448233544826\n",
      "Epoch 188/300\n",
      "Average training loss: 0.043726463503307764\n",
      "Average test loss: 0.0023831560040513673\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04375942845476998\n",
      "Average test loss: 0.002688578767495023\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0437394967244731\n",
      "Average test loss: 0.0023953811716702247\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04393452909588814\n",
      "Average test loss: 0.0025432054731580945\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04387006929516792\n",
      "Average test loss: 0.002370540781153573\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04413636047972573\n",
      "Average test loss: 0.002356595388924082\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04361414752735032\n",
      "Average test loss: 0.002469064990265502\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0438195714685652\n",
      "Average test loss: 0.0025460731093254352\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04389144482049677\n",
      "Average test loss: 0.015082090781794654\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04370622892512215\n",
      "Average test loss: 0.00238296198244724\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04358966706858741\n",
      "Average test loss: 0.0025324241634872226\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04345365210374196\n",
      "Average test loss: 0.0028034362309715816\n",
      "Epoch 202/300\n",
      "Average training loss: 0.043528299000528126\n",
      "Average test loss: 0.00240675804639856\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04357249353660478\n",
      "Average test loss: 0.0024903302124391\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04364811928735839\n",
      "Average test loss: 0.002402715125121176\n",
      "Epoch 205/300\n",
      "Average training loss: 0.046123587694433\n",
      "Average test loss: 0.0024607604837252033\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04451924376355277\n",
      "Average test loss: 0.002408382330296768\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04364773419499397\n",
      "Average test loss: 0.002385224177191655\n",
      "Epoch 208/300\n",
      "Average training loss: 0.043429769492811625\n",
      "Average test loss: 0.0024012273949467473\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04328257007400195\n",
      "Average test loss: 0.0024081753568930757\n",
      "Epoch 210/300\n",
      "Average training loss: 0.043210730516248276\n",
      "Average test loss: 0.0023834406211972236\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04337282357613246\n",
      "Average test loss: 0.002483736554057234\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04329262543386883\n",
      "Average test loss: 0.002547730323341158\n",
      "Epoch 215/300\n",
      "Average training loss: 0.043218000968297324\n",
      "Average test loss: 0.0023848382006916736\n",
      "Epoch 216/300\n",
      "Average training loss: 0.043143687317768735\n",
      "Average test loss: 0.002672766667066349\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04374587509367201\n",
      "Average test loss: 0.0028041031826287507\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04424549209740427\n",
      "Average test loss: 0.0024250219845109514\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04324072446756893\n",
      "Average test loss: 0.002391536172893312\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0433144967854023\n",
      "Average test loss: 0.003022689445979065\n",
      "Epoch 221/300\n",
      "Average training loss: 0.043254931834008956\n",
      "Average test loss: 0.0023660569820139143\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04314959819449319\n",
      "Average test loss: 0.002393152139046126\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04296338420775202\n",
      "Average test loss: 0.0023821488104553688\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04299131675230132\n",
      "Average test loss: 0.0023677007268286415\n",
      "Epoch 225/300\n",
      "Average training loss: 0.042977320144573845\n",
      "Average test loss: 0.0024450755007565023\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04323052672876252\n",
      "Average test loss: 0.002522351073101163\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04294082140922546\n",
      "Average test loss: 0.0024335974976420404\n",
      "Epoch 228/300\n",
      "Average training loss: 0.043442306033439106\n",
      "Average test loss: 0.002464510134110848\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04293205715550317\n",
      "Average test loss: 0.0024099506561954817\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04301457481748528\n",
      "Average test loss: 0.0031887317225337028\n",
      "Epoch 233/300\n",
      "Average training loss: 0.042858753161297905\n",
      "Average test loss: 0.0026013172547229464\n",
      "Epoch 234/300\n",
      "Average training loss: 0.042849170645078025\n",
      "Average test loss: 0.002390608420388566\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04286188557247321\n",
      "Average test loss: 0.0026112245070851513\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04278070321679115\n",
      "Average test loss: 0.002394057042689787\n",
      "Epoch 237/300\n",
      "Average training loss: 0.042793614046441185\n",
      "Average test loss: 0.0024107154836464258\n",
      "Epoch 238/300\n",
      "Average training loss: 0.043283244417773356\n",
      "Average test loss: 0.02281635526402129\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04298600971698761\n",
      "Average test loss: 0.002405002794849376\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04327931115031242\n",
      "Average test loss: 0.0024573353880809412\n",
      "Epoch 243/300\n",
      "Average training loss: 0.043300931298070486\n",
      "Average test loss: 0.004815214928239584\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04270433870620198\n",
      "Average test loss: 0.029579084891411995\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04265204765399297\n",
      "Average test loss: 0.002386766754504707\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04278670575883654\n",
      "Average test loss: 0.00238170708829744\n",
      "Epoch 247/300\n",
      "Average training loss: 0.042846561657057865\n",
      "Average test loss: 0.0024208656259708935\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04271005778511365\n",
      "Average test loss: 0.002403141958432065\n",
      "Epoch 249/300\n",
      "Average training loss: 0.042613947941197286\n",
      "Average test loss: 1.6477276073826683\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04261094452606307\n",
      "Average test loss: 0.0023968472231386435\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04254532033867306\n",
      "Average test loss: 0.002430526805213756\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04258641418483522\n",
      "Average test loss: 0.002780778073395292\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04320679799715678\n",
      "Average test loss: 0.0024188206856035526\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04285719364219242\n",
      "Average test loss: 0.0023927387404772973\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0423971433142821\n",
      "Average test loss: 0.0024179415454467137\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04272136586242252\n",
      "Average test loss: 0.0023917418702298567\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04273595413234499\n",
      "Average test loss: 0.002909577060283886\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04240056957470046\n",
      "Average test loss: 0.002402851956172122\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04242380084925228\n",
      "Average test loss: 0.002469538224654065\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04244916297329797\n",
      "Average test loss: 0.0024387334825264084\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04262524288230472\n",
      "Average test loss: 0.002523947709136539\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04252237749762005\n",
      "Average test loss: 0.0024340572539303038\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04252305661804146\n",
      "Average test loss: 0.002511485399471389\n",
      "Epoch 265/300\n",
      "Average training loss: 0.042275487376583945\n",
      "Average test loss: 0.0027073688161455924\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04241685568624073\n",
      "Average test loss: 0.002463696802448895\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04230417882402738\n",
      "Average test loss: 0.0025167531453900867\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04257804327375359\n",
      "Average test loss: 0.0027474965233769684\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04245407424039311\n",
      "Average test loss: 0.002473709386462967\n",
      "Epoch 270/300\n",
      "Average training loss: 0.042467427551746366\n",
      "Average test loss: 0.0024513013075209327\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04221629143589073\n",
      "Average test loss: 0.002664199729450047\n",
      "Epoch 272/300\n",
      "Average training loss: 0.042246835509936013\n",
      "Average test loss: 0.00243814472688569\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04227592591444651\n",
      "Average test loss: 0.0025296497146288554\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04250579094224506\n",
      "Average test loss: 0.002531108413512508\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04221612472169929\n",
      "Average test loss: 0.0028128911236094106\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04229285560713874\n",
      "Average test loss: 0.0024881044812500477\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04235528652369976\n",
      "Average test loss: 0.0024594924849354557\n",
      "Epoch 281/300\n",
      "Average training loss: 0.042397746473550796\n",
      "Average test loss: 0.002425757313147187\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04224520439240668\n",
      "Average test loss: 0.00240458492996792\n",
      "Epoch 283/300\n",
      "Average training loss: 0.042146705690357424\n",
      "Average test loss: 0.002530332787997193\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04213374921017223\n",
      "Average test loss: 0.0025166534613817932\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04209323202239142\n",
      "Average test loss: 0.0026567776794027952\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04210387242502636\n",
      "Average test loss: 0.0024216407323256134\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04210773943530189\n",
      "Average test loss: 0.002983328104019165\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04212400068839391\n",
      "Average test loss: 0.002543684715612067\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04210929878552755\n",
      "Average test loss: 0.002502931508132153\n",
      "Epoch 290/300\n",
      "Average training loss: 0.042329052027728825\n",
      "Average test loss: 0.0024562118258327245\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04206103515956137\n",
      "Average test loss: 0.0027054724717098804\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04214288551608721\n",
      "Average test loss: 64.54952274237739\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04209642957316505\n",
      "Average test loss: 0.00248360270427333\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04206521832611826\n",
      "Average test loss: 0.0024328784303118784\n",
      "Epoch 295/300\n",
      "Average training loss: 0.041937672803799314\n",
      "Average test loss: 0.0025496154398553903\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04199049463205867\n",
      "Average test loss: 0.0024867976013984946\n",
      "Epoch 297/300\n",
      "Average training loss: 0.042004358924097485\n",
      "Average test loss: 0.002858671178420385\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04196528099642859\n",
      "Average test loss: 0.002996721506325735\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04207888623409801\n",
      "Average test loss: 0.0024239972916742167\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04200735662380854\n",
      "Average test loss: 0.002463395490621527\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.416477557659149\n",
      "Average test loss: 0.0049279874964720674\n",
      "Epoch 2/300\n",
      "Average training loss: 0.47989987972047593\n",
      "Average test loss: 0.004124166077209844\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3245142728355196\n",
      "Average test loss: 0.0038779021420826515\n",
      "Epoch 4/300\n",
      "Average training loss: 0.24728922503524356\n",
      "Average test loss: 0.0034235319706300894\n",
      "Epoch 5/300\n",
      "Average training loss: 0.19928476578659482\n",
      "Average test loss: 0.003182113457057211\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1679415316714181\n",
      "Average test loss: 0.003029588225711551\n",
      "Epoch 7/300\n",
      "Average training loss: 0.14439847389856975\n",
      "Average test loss: 0.0031808531425065465\n",
      "Epoch 8/300\n",
      "Average training loss: 0.12753100328975253\n",
      "Average test loss: 0.00285683979280293\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11480952801969316\n",
      "Average test loss: 0.002805723155952162\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10481318690379461\n",
      "Average test loss: 0.0026398469482858977\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09734506063328849\n",
      "Average test loss: 0.0026307322298073105\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09097857185867098\n",
      "Average test loss: 0.003966711800131533\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08535962977674272\n",
      "Average test loss: 0.002722277582312624\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08068568149540159\n",
      "Average test loss: 0.002364033012650907\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07640480420986812\n",
      "Average test loss: 0.003496376254078415\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07317626368999482\n",
      "Average test loss: 0.002335739114218288\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06931921935081482\n",
      "Average test loss: 0.0023356171406598556\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06638705710901155\n",
      "Average test loss: 0.00227791725906233\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06305876087480121\n",
      "Average test loss: 0.0021154111143615512\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0603463904162248\n",
      "Average test loss: 0.0019567493078195388\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05894887011580997\n",
      "Average test loss: 0.0021594054752753842\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05583660435014301\n",
      "Average test loss: 0.002123422778211534\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05412043238679568\n",
      "Average test loss: 0.0018892259763346778\n",
      "Epoch 24/300\n",
      "Average training loss: 0.052379970570405325\n",
      "Average test loss: 0.0018948146673954196\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05051842558383941\n",
      "Average test loss: 0.0065436498576568235\n",
      "Epoch 26/300\n",
      "Average training loss: 0.049120607525110244\n",
      "Average test loss: 0.001862394985432426\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04792444128625923\n",
      "Average test loss: 0.0020014766942088803\n",
      "Epoch 28/300\n",
      "Average training loss: 0.046595576935344274\n",
      "Average test loss: 0.0020536329388204547\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04526123389601708\n",
      "Average test loss: 0.005479225643393066\n",
      "Epoch 30/300\n",
      "Average training loss: 0.044227363751994236\n",
      "Average test loss: 0.0017503027480302586\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04321264518962966\n",
      "Average test loss: 0.0017147992952830263\n",
      "Epoch 32/300\n",
      "Average training loss: 0.042401272224055396\n",
      "Average test loss: 0.0017655503447684977\n",
      "Epoch 33/300\n",
      "Average training loss: 0.041810782379574245\n",
      "Average test loss: 0.0016968713319963878\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04124498285849889\n",
      "Average test loss: 0.0017794104388190641\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04079889318678114\n",
      "Average test loss: 0.0018173213431404696\n",
      "Epoch 36/300\n",
      "Average training loss: 0.040688981880744296\n",
      "Average test loss: 0.0016545518992675675\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04001663613319397\n",
      "Average test loss: 0.0016773330623077022\n",
      "Epoch 38/300\n",
      "Average training loss: 0.039375298781527415\n",
      "Average test loss: 0.0042096289102402\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03912741089529461\n",
      "Average test loss: 0.004628380554831691\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03874199825194147\n",
      "Average test loss: 0.0017912707462285956\n",
      "Epoch 41/300\n",
      "Average training loss: 0.039053934613863625\n",
      "Average test loss: 0.0016344873626819914\n",
      "Epoch 42/300\n",
      "Average training loss: 0.039189553992615804\n",
      "Average test loss: 0.0022947794519778755\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03881160476472643\n",
      "Average test loss: 0.0021332011585020356\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0378902070886559\n",
      "Average test loss: 0.0016127384338113996\n",
      "Epoch 45/300\n",
      "Average training loss: 0.037661616067091626\n",
      "Average test loss: 0.0016642697194798124\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03734360318713718\n",
      "Average test loss: 0.0015944886971265078\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0372965895930926\n",
      "Average test loss: 0.0016178325847205188\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03684208171069622\n",
      "Average test loss: 0.001796730755103959\n",
      "Epoch 49/300\n",
      "Average training loss: 0.036897148127357164\n",
      "Average test loss: 0.0016344226057537728\n",
      "Epoch 50/300\n",
      "Average training loss: 0.036655607759952545\n",
      "Average test loss: 0.0015736491214483977\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0364944945010874\n",
      "Average test loss: 0.0015559867415577172\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0362009665634897\n",
      "Average test loss: 0.0015873972861510184\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03607273294528325\n",
      "Average test loss: 0.0015900431496815549\n",
      "Epoch 54/300\n",
      "Average training loss: 0.035906663108203146\n",
      "Average test loss: 0.0016367858527228235\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03566409797469775\n",
      "Average test loss: 0.0015826407162886527\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03550128776828448\n",
      "Average test loss: 0.0015693972262864312\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03540457115901841\n",
      "Average test loss: 0.002002179487608373\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03532195917268594\n",
      "Average test loss: 0.001672102522208459\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03513660818338394\n",
      "Average test loss: 0.001605665748835438\n",
      "Epoch 60/300\n",
      "Average training loss: 0.035198242406050365\n",
      "Average test loss: 0.0015265951324254273\n",
      "Epoch 61/300\n",
      "Average training loss: 0.035062156178885036\n",
      "Average test loss: 0.0015836714251587789\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03631925880412261\n",
      "Average test loss: 0.001548101168539789\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03498403477668762\n",
      "Average test loss: 0.0019250859692692757\n",
      "Epoch 64/300\n",
      "Average training loss: 0.034642833484543695\n",
      "Average test loss: 0.0020023688699843155\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03473208619157473\n",
      "Average test loss: 0.0016538920721246137\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0345281502770053\n",
      "Average test loss: 0.0020503740914993816\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03445123104419973\n",
      "Average test loss: 0.0019294252523945437\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03445971917278237\n",
      "Average test loss: 0.004242132076683143\n",
      "Epoch 69/300\n",
      "Average training loss: 0.034262540519237515\n",
      "Average test loss: 0.003767639754960934\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03435636237098111\n",
      "Average test loss: 0.0015460437674903206\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03485208368798097\n",
      "Average test loss: 0.00164033349406802\n",
      "Epoch 72/300\n",
      "Average training loss: 0.034303681934873265\n",
      "Average test loss: 0.0017889463394466373\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03404325061539809\n",
      "Average test loss: 0.037022587339083354\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0339929611881574\n",
      "Average test loss: 0.0015479650135255523\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03392684972451793\n",
      "Average test loss: 0.005513206566580468\n",
      "Epoch 76/300\n",
      "Average training loss: 0.033852677255868914\n",
      "Average test loss: 0.0015527416900214222\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03372578584154447\n",
      "Average test loss: 0.002190062161328064\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03377310769756635\n",
      "Average test loss: 0.001541634632481469\n",
      "Epoch 79/300\n",
      "Average training loss: 0.033617614656686785\n",
      "Average test loss: 0.0017149349748053484\n",
      "Epoch 80/300\n",
      "Average training loss: 0.033560869029826586\n",
      "Average test loss: 0.0015083355410024524\n",
      "Epoch 81/300\n",
      "Average training loss: 0.033657163520654045\n",
      "Average test loss: 0.0015840643144523104\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03352554793159167\n",
      "Average test loss: 0.001672742834004263\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03335883526835177\n",
      "Average test loss: 0.0018952599674877193\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03331038139263789\n",
      "Average test loss: 0.001582682454958558\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0332860743486219\n",
      "Average test loss: 0.0015368108887018428\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03326638917790519\n",
      "Average test loss: 0.0015895809229049419\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03388470021883647\n",
      "Average test loss: 0.001694830876464645\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03304999111427201\n",
      "Average test loss: 0.0015387392217914263\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03302415700919099\n",
      "Average test loss: 0.002296335972017712\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03300699354873763\n",
      "Average test loss: 0.0017691922806617286\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03290504462189144\n",
      "Average test loss: 0.062221728533506396\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03315191507339477\n",
      "Average test loss: 0.0015394489788967702\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03283121429218186\n",
      "Average test loss: 0.0015952274900757604\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03287691924307081\n",
      "Average test loss: 0.0014928497908016046\n",
      "Epoch 95/300\n",
      "Average training loss: 0.032981109148926206\n",
      "Average test loss: 0.0047405095915827485\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03340458096398247\n",
      "Average test loss: 0.001494422605778608\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03265914860036638\n",
      "Average test loss: 0.0015089960228651762\n",
      "Epoch 98/300\n",
      "Average training loss: 0.032868355903360576\n",
      "Average test loss: 0.0023416659000019234\n",
      "Epoch 99/300\n",
      "Average training loss: 0.032810223446951975\n",
      "Average test loss: 0.0015091949665091103\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03275141442981031\n",
      "Average test loss: 0.0015141901284870174\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03252164983418253\n",
      "Average test loss: 0.0015049591677056418\n",
      "Epoch 102/300\n",
      "Average training loss: 0.032705025287965934\n",
      "Average test loss: 0.0015198257726927599\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03276097850004832\n",
      "Average test loss: 0.0015183654038183805\n",
      "Epoch 105/300\n",
      "Average training loss: 0.033023317929771215\n",
      "Average test loss: 0.002184071955167585\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03253426737421089\n",
      "Average test loss: 0.0014981803504957093\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03233396044373512\n",
      "Average test loss: 0.003005854040487773\n",
      "Epoch 108/300\n",
      "Average training loss: 0.032402903288602826\n",
      "Average test loss: 0.0015131609038346345\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03233986044426759\n",
      "Average test loss: 0.0015122581895233855\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03235307593643665\n",
      "Average test loss: 0.005314157069971164\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03223918668760194\n",
      "Average test loss: 0.0015086806861476765\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0322932295087311\n",
      "Average test loss: 0.0015081249680370093\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03223118851251072\n",
      "Average test loss: 0.0016105886902660132\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03240221270587709\n",
      "Average test loss: 0.0015025952543235488\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03290501293871138\n",
      "Average test loss: 0.012657290439742307\n",
      "Epoch 116/300\n",
      "Average training loss: 0.032144495258728666\n",
      "Average test loss: 0.001493732609682613\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03217783552077082\n",
      "Average test loss: 296.260198510064\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03313992442025079\n",
      "Average test loss: 0.001505977413099673\n",
      "Epoch 119/300\n",
      "Average training loss: 0.031966747689578266\n",
      "Average test loss: 0.001630809720346911\n",
      "Epoch 120/300\n",
      "Average training loss: 0.031945699657003085\n",
      "Average test loss: 0.001552105494050516\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03202186204161909\n",
      "Average test loss: 0.0015719976308238176\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03210215358270539\n",
      "Average test loss: 0.0017542096456098887\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03209891239636474\n",
      "Average test loss: 0.0015072820990656812\n",
      "Epoch 124/300\n",
      "Average training loss: 0.031959322288632394\n",
      "Average test loss: 0.0017457636615468397\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03198810738656256\n",
      "Average test loss: 0.0015597755750641226\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0319730063047674\n",
      "Average test loss: 0.0015109121556290321\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03186920352445708\n",
      "Average test loss: 0.0015346801166112225\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03181069322427114\n",
      "Average test loss: 0.0015191061252521143\n",
      "Epoch 129/300\n",
      "Average training loss: 0.031902642250061036\n",
      "Average test loss: 0.001511814087215397\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03191544519861539\n",
      "Average test loss: 0.0015154420098082886\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03198918877210882\n",
      "Average test loss: 0.0015061089539796942\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03185046598480808\n",
      "Average test loss: 0.00834016001638439\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03193142121533553\n",
      "Average test loss: 0.0015740885542084773\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0316761047244072\n",
      "Average test loss: 0.0016260693344391055\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031686282825138835\n",
      "Average test loss: 0.5831362265944481\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03181470344463984\n",
      "Average test loss: 0.0015495278356182907\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03165759758485688\n",
      "Average test loss: 0.0017285118086470498\n",
      "Epoch 138/300\n",
      "Average training loss: 0.031713640439841484\n",
      "Average test loss: 0.0015750389994225568\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03159548364082972\n",
      "Average test loss: 2.020084605928924\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0315509308775266\n",
      "Average test loss: 0.0015240907764269246\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03148613104224205\n",
      "Average test loss: 0.007882760701908005\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03171161362859938\n",
      "Average test loss: 0.0015145919935570823\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03181982489592499\n",
      "Average test loss: 0.0015114378386901484\n",
      "Epoch 144/300\n",
      "Average training loss: 0.031578383043408396\n",
      "Average test loss: 0.0016252382361433572\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03143867720829116\n",
      "Average test loss: 0.0015382490294675033\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03165331606070201\n",
      "Average test loss: 0.0015029694763943552\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03216047646270858\n",
      "Average test loss: 0.001544778996354176\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03136664733456241\n",
      "Average test loss: 0.0017206226661801338\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03137963338361846\n",
      "Average test loss: 0.0014997553212775124\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031374065962102675\n",
      "Average test loss: 0.001956763065316611\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03133072965509362\n",
      "Average test loss: 0.0015387422817034855\n",
      "Epoch 152/300\n",
      "Average training loss: 0.031367401795254816\n",
      "Average test loss: 0.0017091859084450536\n",
      "Epoch 153/300\n",
      "Average training loss: 0.031362198606133464\n",
      "Average test loss: 0.0015132004148844217\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03148693513042397\n",
      "Average test loss: 0.0015475115422159433\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03138082721332709\n",
      "Average test loss: 0.0015658834603511625\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03127620461417569\n",
      "Average test loss: 0.0015204562745574448\n",
      "Epoch 157/300\n",
      "Average training loss: 0.031245021277003817\n",
      "Average test loss: 0.00153273179092341\n",
      "Epoch 158/300\n",
      "Average training loss: 0.031205235828955968\n",
      "Average test loss: 0.0017324151135981083\n",
      "Epoch 159/300\n",
      "Average training loss: 0.035948847027288545\n",
      "Average test loss: 0.0018857270274311303\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03153663334912724\n",
      "Average test loss: 0.0016882735901098285\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0311912994450993\n",
      "Average test loss: 0.001549455867873298\n",
      "Epoch 162/300\n",
      "Average training loss: 0.031125962653093867\n",
      "Average test loss: 0.001554886755740477\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03124038913846016\n",
      "Average test loss: 0.001507447133668595\n",
      "Epoch 164/300\n",
      "Average training loss: 0.031111376216014227\n",
      "Average test loss: 0.0017852207178042994\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031128491542405552\n",
      "Average test loss: 0.002022379071865645\n",
      "Epoch 166/300\n",
      "Average training loss: 0.031217406357328097\n",
      "Average test loss: 0.0015503403052894605\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03110498958163791\n",
      "Average test loss: 0.0018506590405272113\n",
      "Epoch 168/300\n",
      "Average training loss: 0.031198967807822758\n",
      "Average test loss: 0.0017190334780348671\n",
      "Epoch 169/300\n",
      "Average training loss: 0.031157357449332873\n",
      "Average test loss: 0.0015165927196956342\n",
      "Epoch 170/300\n",
      "Average training loss: 0.031011676200562055\n",
      "Average test loss: 0.022857197178734672\n",
      "Epoch 171/300\n",
      "Average training loss: 0.031090317078762586\n",
      "Average test loss: 0.0016895824550754493\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03104791846540239\n",
      "Average test loss: 0.0015135480064070888\n",
      "Epoch 173/300\n",
      "Average training loss: 0.031002879473898147\n",
      "Average test loss: 0.006651371412393119\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03150900520881017\n",
      "Average test loss: 0.0015556670416456957\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030870298181970915\n",
      "Average test loss: 0.001631740630707807\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030932584764228926\n",
      "Average test loss: 0.0015365956113156346\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031045313364929623\n",
      "Average test loss: 0.0017591437322811948\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03125966471268071\n",
      "Average test loss: 0.001511409605646299\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030887131399578518\n",
      "Average test loss: 0.00984284623277684\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030868181380960678\n",
      "Average test loss: 0.0015213507640485962\n",
      "Epoch 181/300\n",
      "Average training loss: 0.031026030254032878\n",
      "Average test loss: 0.0016581464070412847\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03102995918856727\n",
      "Average test loss: 0.0021814520897136796\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03094572671254476\n",
      "Average test loss: 0.0016180623231662643\n",
      "Epoch 184/300\n",
      "Average training loss: 0.031055333607726627\n",
      "Average test loss: 0.0018416938229153554\n",
      "Epoch 185/300\n",
      "Average training loss: 0.030835713672969077\n",
      "Average test loss: 0.0018256101806958516\n",
      "Epoch 186/300\n",
      "Average training loss: 0.030749686224593058\n",
      "Average test loss: 0.0022556576467015676\n",
      "Epoch 187/300\n",
      "Average training loss: 0.031456248200602\n",
      "Average test loss: 0.0015237138553315566\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030958189580175613\n",
      "Average test loss: 0.0015273164643181696\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030798838921719127\n",
      "Average test loss: 0.005611582532111141\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03070430722998248\n",
      "Average test loss: 0.0015222780532203615\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03075652311907874\n",
      "Average test loss: 0.0015513027805524567\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030926065327392685\n",
      "Average test loss: 0.002508659229820801\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030773734577827984\n",
      "Average test loss: 0.0015889592352840635\n",
      "Epoch 194/300\n",
      "Average training loss: 0.030826503256956735\n",
      "Average test loss: 0.0016158553525391552\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030797431584861545\n",
      "Average test loss: 0.00152347423757116\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03068585545818011\n",
      "Average test loss: 177.92568992445203\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03067740426792039\n",
      "Average test loss: 0.0015479714375817113\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030756757555736435\n",
      "Average test loss: 0.0015419011986297039\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030741202851136525\n",
      "Average test loss: 0.0020099316218660937\n",
      "Epoch 200/300\n",
      "Average training loss: 0.030697240834434827\n",
      "Average test loss: 0.0021860328854786024\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030604277852508757\n",
      "Average test loss: 0.0015753234145748946\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030725862885514894\n",
      "Average test loss: 0.001615889404486451\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03065616199043062\n",
      "Average test loss: 0.0016457695117634203\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03054290821320481\n",
      "Average test loss: 0.0015484540458354686\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03071799537539482\n",
      "Average test loss: 0.001604843702995115\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030742850763930216\n",
      "Average test loss: 0.002091309785687675\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030539917127125792\n",
      "Average test loss: 0.001643914624945157\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030499472005499732\n",
      "Average test loss: 0.00152568930408193\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030602557419074906\n",
      "Average test loss: 0.0015239849330650436\n",
      "Epoch 210/300\n",
      "Average training loss: 0.030851487275626924\n",
      "Average test loss: 0.0017699497228281366\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030498206650217373\n",
      "Average test loss: 0.0015491196948827968\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030732825862036812\n",
      "Average test loss: 0.0016086705817530552\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03052759861614969\n",
      "Average test loss: 0.005673668097083767\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03095288941760858\n",
      "Average test loss: 0.0015381141535730826\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030349709702862635\n",
      "Average test loss: 0.001546779947148429\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03046974057621426\n",
      "Average test loss: 0.0015918132910091017\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03045640869769785\n",
      "Average test loss: 0.0015981591441151168\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030432971444394854\n",
      "Average test loss: 0.0015413638910071717\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030491652990380924\n",
      "Average test loss: 0.001549875026775731\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03041392675538858\n",
      "Average test loss: 0.0015912882487496569\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030502011042502192\n",
      "Average test loss: 0.0015927625932834215\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030404626763529247\n",
      "Average test loss: 0.0024027358093816373\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03038108737104469\n",
      "Average test loss: 0.0015934533894889885\n",
      "Epoch 224/300\n",
      "Average training loss: 0.030311214360925886\n",
      "Average test loss: 0.0015533213248062464\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030493774652481077\n",
      "Average test loss: 0.00808895280263904\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030395941196216476\n",
      "Average test loss: 0.0015213131277511517\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030519350841641427\n",
      "Average test loss: 0.0015676018359760443\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030245161306526925\n",
      "Average test loss: 0.0015627519365193116\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030303149667051103\n",
      "Average test loss: 0.0016342388070705865\n",
      "Epoch 230/300\n",
      "Average training loss: 0.030292842282189262\n",
      "Average test loss: 0.0015691405480934514\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03056636020541191\n",
      "Average test loss: 0.0015633138783483042\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030654209895266425\n",
      "Average test loss: 0.0015714710537965098\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030201519798901347\n",
      "Average test loss: 0.0015565151915782028\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03019708133240541\n",
      "Average test loss: 0.00151646986655477\n",
      "Epoch 235/300\n",
      "Average training loss: 0.030421918781267272\n",
      "Average test loss: 0.002538483830893205\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030406612520416577\n",
      "Average test loss: 0.001586758532975283\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030281530221303303\n",
      "Average test loss: 0.0017659938356114758\n",
      "Epoch 238/300\n",
      "Average training loss: 0.030186518828074136\n",
      "Average test loss: 0.0015627123744537434\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030400101752744782\n",
      "Average test loss: 0.0016226778138015005\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03014496036536164\n",
      "Average test loss: 0.0015773034941198097\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03036752825975418\n",
      "Average test loss: 0.0015433555140366986\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03021554569237762\n",
      "Average test loss: 0.0015681604725412196\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03025992200606399\n",
      "Average test loss: 0.0017141523225646879\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03014189184208711\n",
      "Average test loss: 0.0018570612975292735\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03034048113392459\n",
      "Average test loss: 0.0015457000425085426\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030546773225069046\n",
      "Average test loss: 0.0015868174428534177\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0300174281862047\n",
      "Average test loss: 0.001555415289890435\n",
      "Epoch 248/300\n",
      "Average training loss: 0.030103534114029672\n",
      "Average test loss: 0.0020474874688208933\n",
      "Epoch 249/300\n",
      "Average training loss: 0.030280150426758662\n",
      "Average test loss: 0.0015666064330273205\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03004606624941031\n",
      "Average test loss: 0.00156390908262175\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030089049940307935\n",
      "Average test loss: 0.0015195208010781143\n",
      "Epoch 252/300\n",
      "Average training loss: 0.030173385984367796\n",
      "Average test loss: 0.001563285993412137\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030191133641534383\n",
      "Average test loss: 0.0015412936531938612\n",
      "Epoch 254/300\n",
      "Average training loss: 0.030069543047083747\n",
      "Average test loss: 0.0027962721892529066\n",
      "Epoch 255/300\n",
      "Average training loss: 0.030019089019960827\n",
      "Average test loss: 0.0021694923173636196\n",
      "Epoch 256/300\n",
      "Average training loss: 0.030175545119576985\n",
      "Average test loss: 0.001786103037496408\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030107627699772516\n",
      "Average test loss: 0.0015873249497057663\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030186492818925117\n",
      "Average test loss: 0.001615092546109938\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030102244098981223\n",
      "Average test loss: 0.0015304423073927561\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03001987058420976\n",
      "Average test loss: 0.0015699511928897764\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030286736198597484\n",
      "Average test loss: 0.0023570962759355703\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029975017815828325\n",
      "Average test loss: 0.002047353530820045\n",
      "Epoch 263/300\n",
      "Average training loss: 0.029917586617999606\n",
      "Average test loss: 0.05062084872772296\n",
      "Epoch 264/300\n",
      "Average training loss: 0.030282507692774137\n",
      "Average test loss: 0.0015731579033243987\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030144871758090126\n",
      "Average test loss: 0.0028366121446403362\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030124271399445002\n",
      "Average test loss: 0.0015909347583332822\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0299504440906975\n",
      "Average test loss: 0.0016364390913190113\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029883773451050123\n",
      "Average test loss: 0.0023860218649109206\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02993238789257076\n",
      "Average test loss: 0.001697573718511396\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029998967309792835\n",
      "Average test loss: 0.004589931942108605\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029915249733461276\n",
      "Average test loss: 0.001532457828004327\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02992277569654915\n",
      "Average test loss: 0.0016229145654166738\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02997474928531382\n",
      "Average test loss: 0.0027978052029179203\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029979472407036356\n",
      "Average test loss: 0.001562368873403304\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029956034547752804\n",
      "Average test loss: 0.4829529634051853\n",
      "Epoch 276/300\n",
      "Average training loss: 0.030047825270228916\n",
      "Average test loss: 0.0021159617374133734\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029955848213699128\n",
      "Average test loss: 0.0015747416128093997\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029829492749439347\n",
      "Average test loss: 0.0015843657196188967\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029846512759725254\n",
      "Average test loss: 0.0015518521800016363\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029973106616073185\n",
      "Average test loss: 0.0015621069522781504\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02990005168484317\n",
      "Average test loss: 0.0016418779561192625\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030186543623606364\n",
      "Average test loss: 0.0016348252904911835\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029846885623203385\n",
      "Average test loss: 0.0015592225519940257\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029898832455277444\n",
      "Average test loss: 0.001560298005429407\n",
      "Epoch 285/300\n",
      "Average training loss: 0.029809578446878326\n",
      "Average test loss: 0.0016052987842510144\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029794532971249687\n",
      "Average test loss: 0.0015435399608686566\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02985044534007708\n",
      "Average test loss: 0.0015778981154370639\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029844516808787983\n",
      "Average test loss: 0.0015865450960894426\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029903922711809475\n",
      "Average test loss: 0.001570118665902151\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02979792280495167\n",
      "Average test loss: 0.001626339812245634\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02995809741318226\n",
      "Average test loss: 0.0016556313146526614\n",
      "Epoch 292/300\n",
      "Average training loss: 0.029866823176542916\n",
      "Average test loss: 0.001594699890870187\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029808840524819163\n",
      "Average test loss: 0.0015773883565432494\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029814238420791096\n",
      "Average test loss: 0.012029065933078528\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029883403551247384\n",
      "Average test loss: 0.0015436169661891957\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02976180772980054\n",
      "Average test loss: 0.0015430226436712676\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0297244756354226\n",
      "Average test loss: 0.0018342726977749004\n",
      "Epoch 298/300\n",
      "Average training loss: 0.030068566428290474\n",
      "Average test loss: 0.0015990987681369815\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029752554876936805\n",
      "Average test loss: 0.0016108429508490695\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029826218126548662\n",
      "Average test loss: 0.002451005924079153\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.71451652903027\n",
      "Average test loss: 0.004617712397956186\n",
      "Epoch 2/300\n",
      "Average training loss: 0.44197188244925606\n",
      "Average test loss: 0.0037828102693375615\n",
      "Epoch 3/300\n",
      "Average training loss: 0.28423326222101847\n",
      "Average test loss: 0.0031129282005131244\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2086479680803087\n",
      "Average test loss: 0.0029187844068639807\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1663187095059289\n",
      "Average test loss: 0.002848641818596257\n",
      "Epoch 6/300\n",
      "Average training loss: 0.13873352089193133\n",
      "Average test loss: 0.002732681480753753\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11994861611392763\n",
      "Average test loss: 0.0025227120065440733\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10628244516584608\n",
      "Average test loss: 0.002379242736225327\n",
      "Epoch 9/300\n",
      "Average training loss: 0.09598339571555456\n",
      "Average test loss: 0.002778960742884212\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08783129749033186\n",
      "Average test loss: 0.0029894320300469794\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08141074276632733\n",
      "Average test loss: 0.0020579708808412156\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07611561708317863\n",
      "Average test loss: 0.0019906373110910257\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07175336219204796\n",
      "Average test loss: 0.0021850982809232342\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06809256963928541\n",
      "Average test loss: 0.0031352031918035613\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06457090210252338\n",
      "Average test loss: 0.0019398376722302702\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06152107846405771\n",
      "Average test loss: 0.002018400102853775\n",
      "Epoch 17/300\n",
      "Average training loss: 0.058679363065295746\n",
      "Average test loss: 0.0018169495040136906\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05660957527160645\n",
      "Average test loss: 0.002001998478339778\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05397020038631227\n",
      "Average test loss: 0.0032302235338009066\n",
      "Epoch 20/300\n",
      "Average training loss: 0.051741174750857884\n",
      "Average test loss: 0.0021256428179848527\n",
      "Epoch 21/300\n",
      "Average training loss: 0.049859818991687566\n",
      "Average test loss: 0.002507155053731468\n",
      "Epoch 22/300\n",
      "Average training loss: 0.047884663396411474\n",
      "Average test loss: 0.0015558215105492207\n",
      "Epoch 23/300\n",
      "Average training loss: 0.045997163471248416\n",
      "Average test loss: 0.012183566967904982\n",
      "Epoch 24/300\n",
      "Average training loss: 0.044661431438393066\n",
      "Average test loss: 0.001556378418372737\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04301682636804051\n",
      "Average test loss: 0.0016104349442240264\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04201528677675459\n",
      "Average test loss: 0.0017380656436499621\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04049468032850159\n",
      "Average test loss: 0.001599951450402538\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039095630195405746\n",
      "Average test loss: 0.001443450730914871\n",
      "Epoch 29/300\n",
      "Average training loss: 0.037856752599279085\n",
      "Average test loss: 0.0014671708417849409\n",
      "Epoch 30/300\n",
      "Average training loss: 0.036846192121505736\n",
      "Average test loss: 0.001372174594965246\n",
      "Epoch 31/300\n",
      "Average training loss: 0.036000078558921816\n",
      "Average test loss: 0.0013172125621802276\n",
      "Epoch 32/300\n",
      "Average training loss: 0.035222807781563864\n",
      "Average test loss: 0.0013238095627683731\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03452826265494029\n",
      "Average test loss: 0.0015018425325138702\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03378665190935135\n",
      "Average test loss: 0.0015181558332923385\n",
      "Epoch 35/300\n",
      "Average training loss: 0.033044171031978396\n",
      "Average test loss: 0.0027020253346612057\n",
      "Epoch 36/300\n",
      "Average training loss: 0.032675787382655676\n",
      "Average test loss: 0.001218899704515934\n",
      "Epoch 37/300\n",
      "Average training loss: 0.031883936398559146\n",
      "Average test loss: 0.001256564623158839\n",
      "Epoch 38/300\n",
      "Average training loss: 0.031424240903721916\n",
      "Average test loss: 0.0012349094853012098\n",
      "Epoch 39/300\n",
      "Average training loss: 0.031016676134533353\n",
      "Average test loss: 0.001172235111395518\n",
      "Epoch 40/300\n",
      "Average training loss: 0.030933084021011987\n",
      "Average test loss: 0.0012418656775520908\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03024806013703346\n",
      "Average test loss: 0.0011762383617978129\n",
      "Epoch 42/300\n",
      "Average training loss: 0.029822150361206796\n",
      "Average test loss: 0.0011685445719502039\n",
      "Epoch 43/300\n",
      "Average training loss: 0.029648007457454998\n",
      "Average test loss: 0.0014934066659770906\n",
      "Epoch 44/300\n",
      "Average training loss: 0.029254801811443434\n",
      "Average test loss: 0.0013137678570217556\n",
      "Epoch 45/300\n",
      "Average training loss: 0.029053482903374565\n",
      "Average test loss: 0.0013748533969952\n",
      "Epoch 46/300\n",
      "Average training loss: 0.029363081256548563\n",
      "Average test loss: 0.0014544286687547961\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02857038685348299\n",
      "Average test loss: 0.0011314910470197597\n",
      "Epoch 48/300\n",
      "Average training loss: 0.028277243634064992\n",
      "Average test loss: 0.0010897101961179739\n",
      "Epoch 49/300\n",
      "Average training loss: 0.028176955858038533\n",
      "Average test loss: 0.0012237427946076625\n",
      "Epoch 50/300\n",
      "Average training loss: 0.028043201074832016\n",
      "Average test loss: 0.0012371486607525085\n",
      "Epoch 51/300\n",
      "Average training loss: 0.027754262021846242\n",
      "Average test loss: 0.0011288288450903364\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02767724843819936\n",
      "Average test loss: 0.0010924075367446575\n",
      "Epoch 53/300\n",
      "Average training loss: 0.027544066813257006\n",
      "Average test loss: 0.0011840485447189874\n",
      "Epoch 54/300\n",
      "Average training loss: 0.027388011685676044\n",
      "Average test loss: 0.0010485456390306354\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02738451723092132\n",
      "Average test loss: 0.0010677464357059863\n",
      "Epoch 56/300\n",
      "Average training loss: 0.027145505552490554\n",
      "Average test loss: 0.0010751169431540701\n",
      "Epoch 57/300\n",
      "Average training loss: 0.027007445010874008\n",
      "Average test loss: 0.0011686092739303907\n",
      "Epoch 58/300\n",
      "Average training loss: 0.026933142446809343\n",
      "Average test loss: 0.001210986225037939\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02681948196556833\n",
      "Average test loss: 0.0010522737365422977\n",
      "Epoch 60/300\n",
      "Average training loss: 0.026755455591612392\n",
      "Average test loss: 0.0010696331326228877\n",
      "Epoch 61/300\n",
      "Average training loss: 0.026867903901471032\n",
      "Average test loss: 0.0010867761135515239\n",
      "Epoch 62/300\n",
      "Average training loss: 0.026600483453936046\n",
      "Average test loss: 0.0010444966673644053\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02657212900949849\n",
      "Average test loss: 0.0010306664096812407\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02633747996389866\n",
      "Average test loss: 0.0010623714786229862\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02635206008123027\n",
      "Average test loss: 0.001054149560423361\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02672144453558657\n",
      "Average test loss: 0.00108932427296208\n",
      "Epoch 67/300\n",
      "Average training loss: 0.026623462359110513\n",
      "Average test loss: 0.0010868489348019163\n",
      "Epoch 68/300\n",
      "Average training loss: 0.026123992467919986\n",
      "Average test loss: 0.001034257686489986\n",
      "Epoch 69/300\n",
      "Average training loss: 0.026080558528502783\n",
      "Average test loss: 0.0010463593382284873\n",
      "Epoch 70/300\n",
      "Average training loss: 0.026028379236658416\n",
      "Average test loss: 0.0013289793204102251\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0260993447088533\n",
      "Average test loss: 0.0010356876641097997\n",
      "Epoch 72/300\n",
      "Average training loss: 0.025902664403120678\n",
      "Average test loss: 0.001085588949939443\n",
      "Epoch 73/300\n",
      "Average training loss: 0.025931752133700584\n",
      "Average test loss: 0.0011813056849771075\n",
      "Epoch 74/300\n",
      "Average training loss: 0.025908306698004403\n",
      "Average test loss: 0.0010186157354878055\n",
      "Epoch 75/300\n",
      "Average training loss: 0.025851857796311377\n",
      "Average test loss: 0.0010720326711630656\n",
      "Epoch 76/300\n",
      "Average training loss: 0.025696472158034642\n",
      "Average test loss: 0.0010325501321090593\n",
      "Epoch 77/300\n",
      "Average training loss: 0.025730930179357528\n",
      "Average test loss: 0.001819895077496767\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02568970367146863\n",
      "Average test loss: 0.0010915268615612553\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02556693222622077\n",
      "Average test loss: 0.0010485823861478517\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02570584737095568\n",
      "Average test loss: 0.0010371745391748845\n",
      "Epoch 81/300\n",
      "Average training loss: 0.025446451069580184\n",
      "Average test loss: 0.0011823170537956886\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025445709026522108\n",
      "Average test loss: 0.0010166315451885263\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025392003392179806\n",
      "Average test loss: 0.0010192250464525487\n",
      "Epoch 84/300\n",
      "Average training loss: 0.025395575632651646\n",
      "Average test loss: 0.0011574051055229372\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02535875598755148\n",
      "Average test loss: 0.0010726858070120215\n",
      "Epoch 86/300\n",
      "Average training loss: 0.025249046774374114\n",
      "Average test loss: 0.0013029573704116046\n",
      "Epoch 87/300\n",
      "Average training loss: 0.025334529585308498\n",
      "Average test loss: 0.001016138040771087\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02541578121814463\n",
      "Average test loss: 0.0010383255009849867\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02562252908613947\n",
      "Average test loss: 0.0022698523305459983\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02513501148091422\n",
      "Average test loss: 0.0010171284491403236\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0251111597104205\n",
      "Average test loss: 0.0010070503618982104\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02507461252808571\n",
      "Average test loss: 0.0009897787315874464\n",
      "Epoch 93/300\n",
      "Average training loss: 0.025094952942596543\n",
      "Average test loss: 0.0012376091391262081\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02505057039608558\n",
      "Average test loss: 0.001022405819553468\n",
      "Epoch 95/300\n",
      "Average training loss: 0.025075817277034125\n",
      "Average test loss: 0.0010168303966832657\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0249894088788165\n",
      "Average test loss: 0.001053792265450789\n",
      "Epoch 97/300\n",
      "Average training loss: 0.025086864638659688\n",
      "Average test loss: 0.0010129908882081508\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024813598771890006\n",
      "Average test loss: 0.0009898266258339086\n",
      "Epoch 99/300\n",
      "Average training loss: 0.024898867775996526\n",
      "Average test loss: 4.82064654551612\n",
      "Epoch 100/300\n",
      "Average training loss: 0.024857999770177736\n",
      "Average test loss: 0.0011010383688844741\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02486799476047357\n",
      "Average test loss: 0.001005078016521616\n",
      "Epoch 102/300\n",
      "Average training loss: 0.024772469374868603\n",
      "Average test loss: 0.0010645895288325846\n",
      "Epoch 103/300\n",
      "Average training loss: 0.025246291031440098\n",
      "Average test loss: 0.0010141801489517092\n",
      "Epoch 104/300\n",
      "Average training loss: 0.025056794166564943\n",
      "Average test loss: 0.0010097391672639383\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02462165179848671\n",
      "Average test loss: 0.00101288311328325\n",
      "Epoch 106/300\n",
      "Average training loss: 0.024614126733607716\n",
      "Average test loss: 0.0010329787629759974\n",
      "Epoch 107/300\n",
      "Average training loss: 0.024598863374855784\n",
      "Average test loss: 0.0013614068209297128\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02474171288228697\n",
      "Average test loss: 0.0010966946021136311\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02461834128614929\n",
      "Average test loss: 0.0010429354984209768\n",
      "Epoch 110/300\n",
      "Average training loss: 0.024556642699572775\n",
      "Average test loss: 0.0010532095103472886\n",
      "Epoch 111/300\n",
      "Average training loss: 0.024572148679031267\n",
      "Average test loss: 0.0011072937042141955\n",
      "Epoch 112/300\n",
      "Average training loss: 0.024554877101547187\n",
      "Average test loss: 0.0010770502321732541\n",
      "Epoch 113/300\n",
      "Average training loss: 0.024520135795076687\n",
      "Average test loss: 0.0011114928227745824\n",
      "Epoch 114/300\n",
      "Average training loss: 0.024680250257253646\n",
      "Average test loss: 0.0010398255070774918\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02473413498368528\n",
      "Average test loss: 0.001003515730301539\n",
      "Epoch 116/300\n",
      "Average training loss: 0.024462115620573363\n",
      "Average test loss: 0.0010671030585136679\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02445970339079698\n",
      "Average test loss: 0.0013043417449419697\n",
      "Epoch 118/300\n",
      "Average training loss: 0.024373588072756927\n",
      "Average test loss: 0.0011914574474924142\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02447389401992162\n",
      "Average test loss: 0.001037547545093629\n",
      "Epoch 120/300\n",
      "Average training loss: 0.024611515286895962\n",
      "Average test loss: 0.001080303378092746\n",
      "Epoch 121/300\n",
      "Average training loss: 0.024425684192114407\n",
      "Average test loss: 0.009046805314305757\n",
      "Epoch 122/300\n",
      "Average training loss: 0.024817013546824456\n",
      "Average test loss: 0.0010718702197902732\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02434031190143691\n",
      "Average test loss: 0.0010030978566242588\n",
      "Epoch 124/300\n",
      "Average training loss: 0.024213462468650606\n",
      "Average test loss: 0.007915346527472139\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02427274175816112\n",
      "Average test loss: 3.2206159120400746\n",
      "Epoch 126/300\n",
      "Average training loss: 0.024201930921938685\n",
      "Average test loss: 0.0010853412038543159\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024345332552989325\n",
      "Average test loss: 0.0010171094750468102\n",
      "Epoch 128/300\n",
      "Average training loss: 0.024258843812677594\n",
      "Average test loss: 0.0010374398031789396\n",
      "Epoch 129/300\n",
      "Average training loss: 0.024281934983200498\n",
      "Average test loss: 0.001066681769159105\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024645315718319682\n",
      "Average test loss: 0.0010921935859239764\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02480245429939694\n",
      "Average test loss: 0.001016017600790494\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024045730685194333\n",
      "Average test loss: 0.0010303683151594468\n",
      "Epoch 133/300\n",
      "Average training loss: 0.024052188101742002\n",
      "Average test loss: 0.0010495509148264926\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0240782101733817\n",
      "Average test loss: 0.0010078774912593265\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02408382960160573\n",
      "Average test loss: 22.65450597000122\n",
      "Epoch 136/300\n",
      "Average training loss: 0.024064563055833182\n",
      "Average test loss: 0.0009961681198846135\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02409163555999597\n",
      "Average test loss: 0.09200138978949851\n",
      "Epoch 138/300\n",
      "Average training loss: 0.024077113000882996\n",
      "Average test loss: 0.001027533787354413\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02421794748471843\n",
      "Average test loss: 0.0010132245419857402\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02408032857047187\n",
      "Average test loss: 0.000995756147067166\n",
      "Epoch 141/300\n",
      "Average training loss: 0.024034258433514172\n",
      "Average test loss: 0.040293745703063905\n",
      "Epoch 142/300\n",
      "Average training loss: 0.024082919226752385\n",
      "Average test loss: 0.0010085585210989746\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024017012528247304\n",
      "Average test loss: 0.061979530392421614\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02400615895456738\n",
      "Average test loss: 0.0010165737242334419\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023991466970907316\n",
      "Average test loss: 0.0010211856942106453\n",
      "Epoch 146/300\n",
      "Average training loss: 0.023970062524080276\n",
      "Average test loss: 0.0010304296777273218\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02438486825591988\n",
      "Average test loss: 0.0009994752540563544\n",
      "Epoch 148/300\n",
      "Average training loss: 0.023796584559811484\n",
      "Average test loss: 0.0010990539209710228\n",
      "Epoch 149/300\n",
      "Average training loss: 0.023806935788856613\n",
      "Average test loss: 0.0010653581014937825\n",
      "Epoch 150/300\n",
      "Average training loss: 0.023944825506872602\n",
      "Average test loss: 0.0010563787875386575\n",
      "Epoch 151/300\n",
      "Average training loss: 0.023875902543465298\n",
      "Average test loss: 0.0012934877725525034\n",
      "Epoch 152/300\n",
      "Average training loss: 0.023972983083791203\n",
      "Average test loss: 0.001045558900365399\n",
      "Epoch 153/300\n",
      "Average training loss: 0.023913808157046634\n",
      "Average test loss: 0.0010127415077553854\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02379956558843454\n",
      "Average test loss: 0.0010183819292320145\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02381233219636811\n",
      "Average test loss: 0.002868822240167194\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02371492256720861\n",
      "Average test loss: 0.0009974940983164642\n",
      "Epoch 157/300\n",
      "Average training loss: 0.023911141741606926\n",
      "Average test loss: 0.00099673793079435\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0237334829883443\n",
      "Average test loss: 0.0010926125971600414\n",
      "Epoch 159/300\n",
      "Average training loss: 0.023719298821356562\n",
      "Average test loss: 0.001045018058270216\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02376236482957999\n",
      "Average test loss: 0.0010471884595851103\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0238034431901243\n",
      "Average test loss: 0.0010059631382011705\n",
      "Epoch 162/300\n",
      "Average training loss: 0.023642675227589078\n",
      "Average test loss: 0.0010048469614444507\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02365700586967998\n",
      "Average test loss: 0.0012484056289411253\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02382599796520339\n",
      "Average test loss: 0.001013309213384572\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023645002795590293\n",
      "Average test loss: 0.0010014144310520755\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024762334499922063\n",
      "Average test loss: 0.0011048648675075837\n",
      "Epoch 167/300\n",
      "Average training loss: 0.023622288823127748\n",
      "Average test loss: 0.001112198178501179\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02351655955115954\n",
      "Average test loss: 0.0010312888001402219\n",
      "Epoch 169/300\n",
      "Average training loss: 0.023576466167966523\n",
      "Average test loss: 0.0010307898435534702\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023594063032004568\n",
      "Average test loss: 0.001810090708028939\n",
      "Epoch 171/300\n",
      "Average training loss: 0.023673920570148364\n",
      "Average test loss: 0.0010362658418921961\n",
      "Epoch 172/300\n",
      "Average training loss: 0.023546125926905206\n",
      "Average test loss: 0.001018401232961979\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02347590966274341\n",
      "Average test loss: 0.001360234131415685\n",
      "Epoch 174/300\n",
      "Average training loss: 0.023530724950962596\n",
      "Average test loss: 0.0011939022616586751\n",
      "Epoch 175/300\n",
      "Average training loss: 0.023671052841676605\n",
      "Average test loss: 0.0010940793744391864\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02361321312851376\n",
      "Average test loss: 0.0010139804186506405\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023450129167901144\n",
      "Average test loss: 0.0011530331621050008\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023502986210087937\n",
      "Average test loss: 0.0010473648980001194\n",
      "Epoch 179/300\n",
      "Average training loss: 0.023507924038502905\n",
      "Average test loss: 0.001039230961404327\n",
      "Epoch 180/300\n",
      "Average training loss: 0.024303592471612824\n",
      "Average test loss: 0.001111393055671619\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023496769062346882\n",
      "Average test loss: 0.017917101372033357\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023501265164878632\n",
      "Average test loss: 0.0010373930610302422\n",
      "Epoch 183/300\n",
      "Average training loss: 0.023320733577013017\n",
      "Average test loss: 0.001015050812313954\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02340789343747828\n",
      "Average test loss: 0.0010354079221271807\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0236861106355985\n",
      "Average test loss: 0.003312198418296046\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023394157986674043\n",
      "Average test loss: 0.0010429467593009274\n",
      "Epoch 187/300\n",
      "Average training loss: 0.023426681947376994\n",
      "Average test loss: 0.0011234156474885014\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02337658900519212\n",
      "Average test loss: 0.001018927565465371\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023505687601036496\n",
      "Average test loss: 0.0010570390340354707\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023338725962572627\n",
      "Average test loss: 0.001039722895456685\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02326284865869416\n",
      "Average test loss: 0.0010173847056511376\n",
      "Epoch 192/300\n",
      "Average training loss: 0.023365868798560566\n",
      "Average test loss: 0.0012525361478328706\n",
      "Epoch 193/300\n",
      "Average training loss: 0.023361185393399663\n",
      "Average test loss: 0.0010202489560469986\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023227836217317315\n",
      "Average test loss: 0.0011656050598248839\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023524493801924916\n",
      "Average test loss: 0.001087207733343045\n",
      "Epoch 196/300\n",
      "Average training loss: 0.023238732462128004\n",
      "Average test loss: 0.0010365533886684312\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02324839649266667\n",
      "Average test loss: 0.002833831736817956\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023299297008249495\n",
      "Average test loss: 0.0010620790938329365\n",
      "Epoch 199/300\n",
      "Average training loss: 0.023457227708564864\n",
      "Average test loss: 0.0010461531024840143\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023259612710939515\n",
      "Average test loss: 0.0010909983388458688\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023297300807303852\n",
      "Average test loss: 0.0023923670744730365\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023308476246065565\n",
      "Average test loss: 0.0010336507018655538\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023173453503184848\n",
      "Average test loss: 0.0013054291322413419\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02318395573231909\n",
      "Average test loss: 0.0010235051702604525\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02324143292175399\n",
      "Average test loss: 0.001038491408754554\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023460648328065874\n",
      "Average test loss: 0.0010301105371262464\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0232569071120686\n",
      "Average test loss: 0.0020912150359816022\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023207623071968556\n",
      "Average test loss: 0.0010594927883293066\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02315722961889373\n",
      "Average test loss: 0.001046082228784346\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02314198132024871\n",
      "Average test loss: 0.0010462205562119682\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02316614525185691\n",
      "Average test loss: 0.0010889633712876175\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02312384709219138\n",
      "Average test loss: 0.0012339737188174493\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02309114485151238\n",
      "Average test loss: 0.0011269111268532772\n",
      "Epoch 214/300\n",
      "Average training loss: 0.023161640415589015\n",
      "Average test loss: 0.0010321041450111403\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02319919795791308\n",
      "Average test loss: 0.0012923174510813423\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02317704313993454\n",
      "Average test loss: 0.001046986385507302\n",
      "Epoch 218/300\n",
      "Average training loss: 0.023059934167398346\n",
      "Average test loss: 0.0010810242431859175\n",
      "Epoch 220/300\n",
      "Average training loss: 0.023048574273784955\n",
      "Average test loss: 0.0011160383366255297\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02311069990694523\n",
      "Average test loss: 0.0010484356435222757\n",
      "Epoch 222/300\n",
      "Average training loss: 0.023033425281445186\n",
      "Average test loss: 0.0010814122955521775\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023266465917229653\n",
      "Average test loss: 0.001191554288690289\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022989045792983637\n",
      "Average test loss: 0.0010998803205374214\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023021348431706428\n",
      "Average test loss: 0.0010167908281501796\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023199009123775694\n",
      "Average test loss: 650.8511294894748\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023070938366982673\n",
      "Average test loss: 0.0012044654705872138\n",
      "Epoch 228/300\n",
      "Average training loss: 0.022954349976446894\n",
      "Average test loss: 0.0010599872628744278\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02298962662782934\n",
      "Average test loss: 0.0017484627729281784\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023062493100762367\n",
      "Average test loss: 0.009990772496598462\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02311267543501324\n",
      "Average test loss: 0.0017498559278125564\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02302446444498168\n",
      "Average test loss: 0.0010502795230390297\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022901038446360163\n",
      "Average test loss: 0.0010489628814781705\n",
      "Epoch 235/300\n",
      "Average training loss: 0.022897426671451993\n",
      "Average test loss: 0.001044002526284506\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023140450014008416\n",
      "Average test loss: 0.0015760535685759452\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0229345218539238\n",
      "Average test loss: 0.001060792296503981\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022940118448601828\n",
      "Average test loss: 0.001213925852233337\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0229831516130103\n",
      "Average test loss: 0.003198851987926496\n",
      "Epoch 240/300\n",
      "Average training loss: 0.023039486719502345\n",
      "Average test loss: 0.001033322921488434\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02296805856294102\n",
      "Average test loss: 0.0010756154489806958\n",
      "Epoch 242/300\n",
      "Average training loss: 0.022866340902944406\n",
      "Average test loss: 0.0010783868841826917\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023130087269677056\n",
      "Average test loss: 0.0012568632098328735\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023001118888457617\n",
      "Average test loss: 0.0010542476441090306\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022950141987866827\n",
      "Average test loss: 0.0010597562339777747\n",
      "Epoch 246/300\n",
      "Average training loss: 0.022843080194460023\n",
      "Average test loss: 0.0012338236898908185\n",
      "Epoch 247/300\n",
      "Average training loss: 0.022814424115750524\n",
      "Average test loss: 0.0010756662192547488\n",
      "Epoch 248/300\n",
      "Average training loss: 0.022850314216481316\n",
      "Average test loss: 0.0010605530404589243\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02283908860385418\n",
      "Average test loss: 0.0010351785334965422\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02285585192177031\n",
      "Average test loss: 0.001270051682367921\n",
      "Epoch 251/300\n",
      "Average training loss: 0.022891395623485248\n",
      "Average test loss: 0.0011515133806193868\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02288213984668255\n",
      "Average test loss: 0.0010193251725286246\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022996830956803428\n",
      "Average test loss: 0.00104945704796248\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022787568784422344\n",
      "Average test loss: 0.0010370426973224515\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02274976196885109\n",
      "Average test loss: 0.0010815913994900055\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02289312386347188\n",
      "Average test loss: 0.00117365507931552\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022820504259732033\n",
      "Average test loss: 0.0010471052185942729\n",
      "Epoch 258/300\n",
      "Average training loss: 0.022805007033877903\n",
      "Average test loss: 0.0011070124964333242\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022785618553558984\n",
      "Average test loss: 0.001035945529325141\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022855596765875816\n",
      "Average test loss: 0.0011145471399649977\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02284496664338642\n",
      "Average test loss: 0.001056805342849758\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022761210260291893\n",
      "Average test loss: 0.001052261612791982\n",
      "Epoch 263/300\n",
      "Average training loss: 0.022721249780721136\n",
      "Average test loss: 0.0010523211405509048\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02290121978852484\n",
      "Average test loss: 0.0016266512136078543\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0227765654888418\n",
      "Average test loss: 0.0010606031341271267\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02273090302778615\n",
      "Average test loss: 0.0010536640015327268\n",
      "Epoch 267/300\n",
      "Average training loss: 0.022730649683210584\n",
      "Average test loss: 0.0010613909197143383\n",
      "Epoch 268/300\n",
      "Average training loss: 0.022862233113083573\n",
      "Average test loss: 0.0012335219230088922\n",
      "Epoch 269/300\n",
      "Average training loss: 0.022703212375442188\n",
      "Average test loss: 0.0011653886754065751\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02271212805642022\n",
      "Average test loss: 0.0010881885895505548\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022722943153646258\n",
      "Average test loss: 0.0010934325895375675\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022707018776900238\n",
      "Average test loss: 0.0011759087006664938\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02277572368250953\n",
      "Average test loss: 0.001035750691778958\n",
      "Epoch 274/300\n",
      "Average training loss: 0.022729554510778852\n",
      "Average test loss: 0.001038129737186763\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02270313883655601\n",
      "Average test loss: 0.001062555754557252\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022712881998883352\n",
      "Average test loss: 0.001042501210855941\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02271626647644573\n",
      "Average test loss: 0.001039274710127049\n",
      "Epoch 278/300\n",
      "Average training loss: 0.022727993501557246\n",
      "Average test loss: 0.0010591925642867055\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022740973677900102\n",
      "Average test loss: 0.001161149297737413\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02279793752398756\n",
      "Average test loss: 0.001036478036393722\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022609522165523636\n",
      "Average test loss: 0.001026815393111772\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022631583881047038\n",
      "Average test loss: 0.0011096095159235928\n",
      "Epoch 283/300\n",
      "Average training loss: 0.022652180822359192\n",
      "Average test loss: 0.0013392150644730362\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02268628941807482\n",
      "Average test loss: 0.0010436289459466934\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022597869385447767\n",
      "Average test loss: 0.001054767759413355\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022622058168053627\n",
      "Average test loss: 0.0010910395273110933\n",
      "Epoch 287/300\n",
      "Average training loss: 0.022663138661119672\n",
      "Average test loss: 0.018909793244467843\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02273692633046044\n",
      "Average test loss: 0.0010942004568253955\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022763446488314205\n",
      "Average test loss: 0.0010665530008781288\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022573080617520544\n",
      "Average test loss: 0.0010409781518909667\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02298956132431825\n",
      "Average test loss: 0.0011406728281742997\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022612364533874725\n",
      "Average test loss: 0.0015047265235852037\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022529734878076448\n",
      "Average test loss: 0.00117265226588481\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02258449345330397\n",
      "Average test loss: 0.0011337581264475982\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022558139955831898\n",
      "Average test loss: 0.0010686086621135473\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02255970699754026\n",
      "Average test loss: 0.0011701290691271424\n",
      "Epoch 297/300\n",
      "Average training loss: 0.022779162663552496\n",
      "Average test loss: 0.0010383777257262005\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022615237090322707\n",
      "Average test loss: 0.0014215440226511822\n",
      "Epoch 299/300\n",
      "Average training loss: 0.022536096008287536\n",
      "Average test loss: 0.003350113263974587\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022549805218146906\n",
      "Average test loss: 0.0011642735255364743\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_80_Depth5/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.35\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.82\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.78\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.24\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.82\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.91\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.99\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.69\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.34\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.62\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.61\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.95\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.83\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.89\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.82\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.38\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.57\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.08\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.25\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.95\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.16\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 33.11\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 33.67\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 34.04\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 34.17\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 34.63\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 34.88\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 35.04\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 35.34\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 35.50\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.4950226626926\n",
      "Average test loss: 0.011168725947539012\n",
      "Epoch 2/300\n",
      "Average training loss: 5.969353047688802\n",
      "Average test loss: 0.28625755460477537\n",
      "Epoch 3/300\n",
      "Average training loss: 4.215540974087185\n",
      "Average test loss: 0.007823242568307453\n",
      "Epoch 4/300\n",
      "Average training loss: 3.495181320402357\n",
      "Average test loss: 0.007878701154142619\n",
      "Epoch 5/300\n",
      "Average training loss: 2.8458697823418513\n",
      "Average test loss: 0.007331513325787253\n",
      "Epoch 6/300\n",
      "Average training loss: 2.239830890019735\n",
      "Average test loss: 0.008026758572293652\n",
      "Epoch 7/300\n",
      "Average training loss: 1.8984828306833903\n",
      "Average test loss: 0.007744357979545991\n",
      "Epoch 8/300\n",
      "Average training loss: 1.5932803268432618\n",
      "Average test loss: 0.006728310017950005\n",
      "Epoch 9/300\n",
      "Average training loss: 1.3660761662589178\n",
      "Average test loss: 0.006374456247521771\n",
      "Epoch 10/300\n",
      "Average training loss: 1.1531367307239109\n",
      "Average test loss: 0.006355046411769258\n",
      "Epoch 11/300\n",
      "Average training loss: 0.9949167590141297\n",
      "Average test loss: 0.006185958687629964\n",
      "Epoch 12/300\n",
      "Average training loss: 0.8770346228811476\n",
      "Average test loss: 0.005988155838516023\n",
      "Epoch 13/300\n",
      "Average training loss: 0.7825259158876207\n",
      "Average test loss: 0.005742481175396177\n",
      "Epoch 14/300\n",
      "Average training loss: 0.706705465581682\n",
      "Average test loss: 0.006263283526317941\n",
      "Epoch 15/300\n",
      "Average training loss: 0.6401062100198533\n",
      "Average test loss: 0.005947587683382961\n",
      "Epoch 16/300\n",
      "Average training loss: 0.5854037670559353\n",
      "Average test loss: 0.007948575110899078\n",
      "Epoch 17/300\n",
      "Average training loss: 0.5365259920756023\n",
      "Average test loss: 0.014860684530602561\n",
      "Epoch 18/300\n",
      "Average training loss: 0.4931129133966234\n",
      "Average test loss: 0.006112235541145007\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4559648598564996\n",
      "Average test loss: 0.0056783609377841155\n",
      "Epoch 20/300\n",
      "Average training loss: 0.4182765291266971\n",
      "Average test loss: 0.005389236931585603\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3695914945072598\n",
      "Average test loss: 0.005973753553297784\n",
      "Epoch 22/300\n",
      "Average training loss: 0.33533006339603\n",
      "Average test loss: 0.005151499366180764\n",
      "Epoch 23/300\n",
      "Average training loss: 0.31066680102878147\n",
      "Average test loss: 0.005322433533146978\n",
      "Epoch 24/300\n",
      "Average training loss: 0.2910554911825392\n",
      "Average test loss: 0.004894571353991827\n",
      "Epoch 25/300\n",
      "Average training loss: 0.2780972397327423\n",
      "Average test loss: 0.00478181432477302\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2616034210390515\n",
      "Average test loss: 0.0535182339085473\n",
      "Epoch 27/300\n",
      "Average training loss: 0.252025813791487\n",
      "Average test loss: 0.004876922933177816\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2408003051545885\n",
      "Average test loss: 0.00484560214376284\n",
      "Epoch 29/300\n",
      "Average training loss: 0.23116006395551894\n",
      "Average test loss: 0.00519936467665765\n",
      "Epoch 30/300\n",
      "Average training loss: 0.22457018518447877\n",
      "Average test loss: 0.004686169046908617\n",
      "Epoch 31/300\n",
      "Average training loss: 0.21926401224401262\n",
      "Average test loss: 0.005358189232854379\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2133257355954912\n",
      "Average test loss: 0.004618684893060061\n",
      "Epoch 33/300\n",
      "Average training loss: 0.206658054139879\n",
      "Average test loss: 0.004538523928158813\n",
      "Epoch 34/300\n",
      "Average training loss: 0.20064746158652835\n",
      "Average test loss: 0.00510863527117504\n",
      "Epoch 35/300\n",
      "Average training loss: 0.19590388834476472\n",
      "Average test loss: 0.004488653084884087\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1922774268388748\n",
      "Average test loss: 0.0047178437751200465\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1890288102361891\n",
      "Average test loss: 0.0053455262593925\n",
      "Epoch 38/300\n",
      "Average training loss: 0.18482678665717442\n",
      "Average test loss: 0.00436786599829793\n",
      "Epoch 39/300\n",
      "Average training loss: 0.18185619808567896\n",
      "Average test loss: 0.004667244067208635\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1784045903417799\n",
      "Average test loss: 0.012744697976443503\n",
      "Epoch 41/300\n",
      "Average training loss: 0.17832930760913426\n",
      "Average test loss: 0.004575660638304221\n",
      "Epoch 42/300\n",
      "Average training loss: 0.17246568177806007\n",
      "Average test loss: 0.004411317316401336\n",
      "Epoch 43/300\n",
      "Average training loss: 0.17004581121603649\n",
      "Average test loss: 0.004796691656112671\n",
      "Epoch 44/300\n",
      "Average training loss: 0.16819695264101028\n",
      "Average test loss: 0.014517272065911028\n",
      "Epoch 45/300\n",
      "Average training loss: 0.16570047318935394\n",
      "Average test loss: 0.019885485842823982\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1652520963218477\n",
      "Average test loss: 0.004408919412642718\n",
      "Epoch 47/300\n",
      "Average training loss: 0.16219500337706672\n",
      "Average test loss: 0.006461894651254018\n",
      "Epoch 48/300\n",
      "Average training loss: 0.16129020920064716\n",
      "Average test loss: 0.0043688953878978885\n",
      "Epoch 49/300\n",
      "Average training loss: 0.15954027999771966\n",
      "Average test loss: 5.562483866559135\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1579454389943017\n",
      "Average test loss: 0.004307051715337568\n",
      "Epoch 51/300\n",
      "Average training loss: 0.15641383318768606\n",
      "Average test loss: 0.004349981516185734\n",
      "Epoch 52/300\n",
      "Average training loss: 0.15537902834680345\n",
      "Average test loss: 0.007632073453731007\n",
      "Epoch 53/300\n",
      "Average training loss: 0.15406277300251855\n",
      "Average test loss: 0.004894128132197592\n",
      "Epoch 54/300\n",
      "Average training loss: 0.15402100476953717\n",
      "Average test loss: 0.024432178755601247\n",
      "Epoch 55/300\n",
      "Average training loss: 0.15297308104568058\n",
      "Average test loss: 0.004591433417052031\n",
      "Epoch 56/300\n",
      "Average training loss: 0.15115466005934608\n",
      "Average test loss: 0.0043285758317344715\n",
      "Epoch 57/300\n",
      "Average training loss: 0.15149192782243093\n",
      "Average test loss: 0.0042112687977237835\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1497349730597602\n",
      "Average test loss: 0.004330249275598261\n",
      "Epoch 59/300\n",
      "Average training loss: 0.14900283706850476\n",
      "Average test loss: 0.004174417251100143\n",
      "Epoch 60/300\n",
      "Average training loss: 0.15350760308239195\n",
      "Average test loss: 0.004592554501154356\n",
      "Epoch 61/300\n",
      "Average training loss: 0.14862442899412578\n",
      "Average test loss: 0.00427802285965946\n",
      "Epoch 62/300\n",
      "Average training loss: 0.148722242878543\n",
      "Average test loss: 0.0041652768572999375\n",
      "Epoch 63/300\n",
      "Average training loss: 0.14636609586742189\n",
      "Average test loss: 0.004500010477172004\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1460382438633177\n",
      "Average test loss: 0.004238150715414021\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1451951443751653\n",
      "Average test loss: 0.004776565891173151\n",
      "Epoch 66/300\n",
      "Average training loss: 0.14622751873731613\n",
      "Average test loss: 0.004136423381045461\n",
      "Epoch 67/300\n",
      "Average training loss: 0.14447874397701688\n",
      "Average test loss: 0.004358507353191574\n",
      "Epoch 68/300\n",
      "Average training loss: 0.14391603265868294\n",
      "Average test loss: 0.004222944013981355\n",
      "Epoch 69/300\n",
      "Average training loss: 0.15125446999735304\n",
      "Average test loss: 0.004343451780991422\n",
      "Epoch 70/300\n",
      "Average training loss: 0.14289142623212603\n",
      "Average test loss: 0.03517140024734868\n",
      "Epoch 71/300\n",
      "Average training loss: 0.14260862665706212\n",
      "Average test loss: 0.004138130467178093\n",
      "Epoch 72/300\n",
      "Average training loss: 0.14250943624973297\n",
      "Average test loss: 0.004286967584242424\n",
      "Epoch 73/300\n",
      "Average training loss: 0.14224985497527654\n",
      "Average test loss: 0.004514192693349388\n",
      "Epoch 74/300\n",
      "Average training loss: 559831359.3495156\n",
      "Average test loss: 185207.85273757426\n",
      "Epoch 75/300\n",
      "Average training loss: 74.3417301534017\n",
      "Average test loss: 106.94238238594266\n",
      "Epoch 76/300\n",
      "Average training loss: 67.14185196940105\n",
      "Average test loss: 68126200.25086111\n",
      "Epoch 77/300\n",
      "Average training loss: 62.17193421766493\n",
      "Average test loss: 2861.1512050132487\n",
      "Epoch 78/300\n",
      "Average training loss: 58.288134450276694\n",
      "Average test loss: 165.51154206901126\n",
      "Epoch 79/300\n",
      "Average training loss: 55.563891204833986\n",
      "Average test loss: 105.89777056068844\n",
      "Epoch 80/300\n",
      "Average training loss: 52.24571670193142\n",
      "Average test loss: 12130.881389973958\n",
      "Epoch 81/300\n",
      "Average training loss: 49.454150512695314\n",
      "Average test loss: 15450.430862169054\n",
      "Epoch 82/300\n",
      "Average training loss: 45.61862623765734\n",
      "Average test loss: 163.58468904624382\n",
      "Epoch 83/300\n",
      "Average training loss: 42.125260684543186\n",
      "Average test loss: 20158.80751051161\n",
      "Epoch 84/300\n",
      "Average training loss: 38.894276662190755\n",
      "Average test loss: 113.54181614579095\n",
      "Epoch 85/300\n",
      "Average training loss: 35.52401776801215\n",
      "Average test loss: 2265.4786101696886\n",
      "Epoch 86/300\n",
      "Average training loss: 32.20995720248752\n",
      "Average test loss: 292.46502984857557\n",
      "Epoch 87/300\n",
      "Average training loss: 29.32396015930176\n",
      "Average test loss: 4634.741907067299\n",
      "Epoch 88/300\n",
      "Average training loss: 26.68196334838867\n",
      "Average test loss: 15.61479384184877\n",
      "Epoch 89/300\n",
      "Average training loss: 24.161037516276043\n",
      "Average test loss: 0.013326219510700968\n",
      "Epoch 90/300\n",
      "Average training loss: 21.752465006510416\n",
      "Average test loss: 13050.760567893545\n",
      "Epoch 91/300\n",
      "Average training loss: 20.10942961968316\n",
      "Average test loss: 2.5546454601287842\n",
      "Epoch 92/300\n",
      "Average training loss: 18.526494108412003\n",
      "Average test loss: 468.6409654023523\n",
      "Epoch 93/300\n",
      "Average training loss: 17.233443825615776\n",
      "Average test loss: 447457.2074701139\n",
      "Epoch 94/300\n",
      "Average training loss: 16.176935535854764\n",
      "Average test loss: 22437992.888562255\n",
      "Epoch 95/300\n",
      "Average training loss: 15.273693938361275\n",
      "Average test loss: 66.67916461881002\n",
      "Epoch 96/300\n",
      "Average training loss: 14.321475651211209\n",
      "Average test loss: 437.30430359573495\n",
      "Epoch 97/300\n",
      "Average training loss: 13.52997991265191\n",
      "Average test loss: 4377470.449767081\n",
      "Epoch 98/300\n",
      "Average training loss: 12.619843730502659\n",
      "Average test loss: 1970.5930690612727\n",
      "Epoch 99/300\n",
      "Average training loss: 11.79108174726698\n",
      "Average test loss: 99.50912752534946\n",
      "Epoch 100/300\n",
      "Average training loss: 11.124745704650879\n",
      "Average test loss: 3.8716739902020327\n",
      "Epoch 101/300\n",
      "Average training loss: 10.407752333747016\n",
      "Average test loss: 0.40776566694014604\n",
      "Epoch 102/300\n",
      "Average training loss: 9.783095057169596\n",
      "Average test loss: 0.7097744905319479\n",
      "Epoch 103/300\n",
      "Average training loss: 9.163148936801486\n",
      "Average test loss: 318646.064640625\n",
      "Epoch 104/300\n",
      "Average training loss: 8.656723075442844\n",
      "Average test loss: 357.07972138497814\n",
      "Epoch 105/300\n",
      "Average training loss: 8.17621235063341\n",
      "Average test loss: 0.00662710953834984\n",
      "Epoch 106/300\n",
      "Average training loss: 7.7206969095865885\n",
      "Average test loss: 0.0076552164091004266\n",
      "Epoch 107/300\n",
      "Average training loss: 7.268766381157769\n",
      "Average test loss: 0.2517223426981105\n",
      "Epoch 108/300\n",
      "Average training loss: 6.86387223010593\n",
      "Average test loss: 0.021521851431164477\n",
      "Epoch 109/300\n",
      "Average training loss: 6.431131965213352\n",
      "Average test loss: 1.048932645416922\n",
      "Epoch 110/300\n",
      "Average training loss: 6.0038954650031195\n",
      "Average test loss: 0.011489511657920148\n",
      "Epoch 111/300\n",
      "Average training loss: 5.567765224880643\n",
      "Average test loss: 129.46059532462226\n",
      "Epoch 112/300\n",
      "Average training loss: 5.113389916737875\n",
      "Average test loss: 21.130537425475403\n",
      "Epoch 113/300\n",
      "Average training loss: 4.6867253490024146\n",
      "Average test loss: 2.0560658689832523\n",
      "Epoch 114/300\n",
      "Average training loss: 4.271562182532416\n",
      "Average test loss: 0.009069159836404854\n",
      "Epoch 115/300\n",
      "Average training loss: 3.8698284479777016\n",
      "Average test loss: 0.12031622679117653\n",
      "Epoch 116/300\n",
      "Average training loss: 3.5168851216634116\n",
      "Average test loss: 0.005041196660449107\n",
      "Epoch 117/300\n",
      "Average training loss: 3.1717691470252145\n",
      "Average test loss: 0.006411841962486505\n",
      "Epoch 118/300\n",
      "Average training loss: 2.8726693348354764\n",
      "Average test loss: 0.005167371473378605\n",
      "Epoch 119/300\n",
      "Average training loss: 2.659772386762831\n",
      "Average test loss: 0.0051147107951756985\n",
      "Epoch 120/300\n",
      "Average training loss: 2.4720641865200466\n",
      "Average test loss: 0.006473759816338618\n",
      "Epoch 121/300\n",
      "Average training loss: 2.305640850067139\n",
      "Average test loss: 0.004762449562549591\n",
      "Epoch 122/300\n",
      "Average training loss: 2.1437125386132134\n",
      "Average test loss: 0.005063283580044905\n",
      "Epoch 123/300\n",
      "Average training loss: 1.9925900251600477\n",
      "Average test loss: 0.004774934199949106\n",
      "Epoch 124/300\n",
      "Average training loss: 1.8368201281229655\n",
      "Average test loss: 0.004766167798389991\n",
      "Epoch 125/300\n",
      "Average training loss: 1.7050698290930855\n",
      "Average test loss: 0.004829272867904769\n",
      "Epoch 126/300\n",
      "Average training loss: 1.5707109742694432\n",
      "Average test loss: 0.013504976727896266\n",
      "Epoch 127/300\n",
      "Average training loss: 1.4462877620061239\n",
      "Average test loss: 0.004591891662321157\n",
      "Epoch 128/300\n",
      "Average training loss: 1.3223908206091988\n",
      "Average test loss: 0.004605528348022037\n",
      "Epoch 129/300\n",
      "Average training loss: 1.1927732398774888\n",
      "Average test loss: 0.004776884025169743\n",
      "Epoch 130/300\n",
      "Average training loss: 0.957268395476871\n",
      "Average test loss: 0.0630843780669901\n",
      "Epoch 132/300\n",
      "Average training loss: 0.8470893895890977\n",
      "Average test loss: 0.008137616904245483\n",
      "Epoch 133/300\n",
      "Average training loss: 0.7414267770979139\n",
      "Average test loss: 0.0043672912451956005\n",
      "Epoch 134/300\n",
      "Average training loss: 0.5605040108892653\n",
      "Average test loss: 0.004583884145236678\n",
      "Epoch 136/300\n",
      "Average training loss: 0.4887399050924513\n",
      "Average test loss: 0.0042571651629275745\n",
      "Epoch 137/300\n",
      "Average training loss: 0.4209274611737993\n",
      "Average test loss: 0.005477034619698922\n",
      "Epoch 138/300\n",
      "Average training loss: 0.3750891007847256\n",
      "Average test loss: 0.004392022960922785\n",
      "Epoch 139/300\n",
      "Average training loss: 0.3203250777191586\n",
      "Average test loss: 0.0044734786219067044\n",
      "Epoch 140/300\n",
      "Average training loss: 0.28236401387055715\n",
      "Average test loss: 0.0042287967544462944\n",
      "Epoch 141/300\n",
      "Average training loss: 0.2525877612564299\n",
      "Average test loss: 0.004219349078420136\n",
      "Epoch 142/300\n",
      "Average training loss: 0.23139657243092854\n",
      "Average test loss: 0.00430317144592603\n",
      "Epoch 143/300\n",
      "Average training loss: 0.20436413470904033\n",
      "Average test loss: 0.004331788761127327\n",
      "Epoch 145/300\n",
      "Average training loss: 0.187708764301406\n",
      "Average test loss: 0.055257325037486024\n",
      "Epoch 147/300\n",
      "Average training loss: 0.18165202375253042\n",
      "Average test loss: 0.004422286372838749\n",
      "Epoch 148/300\n",
      "Average training loss: 0.17719788449340396\n",
      "Average test loss: 0.004297457673069504\n",
      "Epoch 149/300\n",
      "Average training loss: 0.16985468961132896\n",
      "Average test loss: 0.00413223084455563\n",
      "Epoch 150/300\n",
      "Average training loss: 0.1669016030497021\n",
      "Average test loss: 0.004222375811802017\n",
      "Epoch 151/300\n",
      "Average training loss: 0.16361217678917778\n",
      "Average test loss: 0.004373470687203937\n",
      "Epoch 152/300\n",
      "Average training loss: 0.16139922233422596\n",
      "Average test loss: 0.07040676257945597\n",
      "Epoch 153/300\n",
      "Average training loss: 0.15888806315263113\n",
      "Average test loss: 0.00414294081636601\n",
      "Epoch 154/300\n",
      "Average training loss: 57487950.96911818\n",
      "Average test loss: 10432.066143301223\n",
      "Epoch 156/300\n",
      "Average training loss: 23.121137708875867\n",
      "Average test loss: 2.2231514407396316\n",
      "Epoch 157/300\n",
      "Average training loss: 22.14084597439236\n",
      "Average test loss: 0.14258561700582503\n",
      "Epoch 158/300\n",
      "Average training loss: 21.37735058932834\n",
      "Average test loss: 1799.4116590487956\n",
      "Epoch 159/300\n",
      "Average training loss: 20.03629793633355\n",
      "Average test loss: 2369.4228798399236\n",
      "Epoch 161/300\n",
      "Average training loss: 19.437873909844292\n",
      "Average test loss: 19.27251878626148\n",
      "Epoch 162/300\n",
      "Average training loss: 18.91714510769314\n",
      "Average test loss: 0.033043654829263684\n",
      "Epoch 163/300\n",
      "Average training loss: 18.450220623440213\n",
      "Average test loss: 0.26126543428334925\n",
      "Epoch 164/300\n",
      "Average training loss: 17.996069263034396\n",
      "Average test loss: 81.85586908265948\n",
      "Epoch 165/300\n",
      "Average training loss: 17.517129996405707\n",
      "Average test loss: 0.11907884657714102\n",
      "Epoch 166/300\n",
      "Average training loss: 17.075522733900282\n",
      "Average test loss: 0.02610165781610542\n",
      "Epoch 167/300\n",
      "Average training loss: 16.61334913635254\n",
      "Average test loss: 0.4513410817848312\n",
      "Epoch 168/300\n",
      "Average training loss: 16.118717714097766\n",
      "Average test loss: 1813.043725479318\n",
      "Epoch 169/300\n",
      "Average training loss: 15.634680206298828\n",
      "Average test loss: 0.013155061550438404\n",
      "Epoch 170/300\n",
      "Average training loss: 15.149593029446072\n",
      "Average test loss: 1.1867889651540253\n",
      "Epoch 171/300\n",
      "Average training loss: 14.22779200744629\n",
      "Average test loss: 0.011670551852219635\n",
      "Epoch 173/300\n",
      "Average training loss: 13.765471082899305\n",
      "Average test loss: 0.014209996326929993\n",
      "Epoch 174/300\n",
      "Average training loss: 13.315722000969782\n",
      "Average test loss: 0.01814441473616494\n",
      "Epoch 175/300\n",
      "Average training loss: 12.886378253512913\n",
      "Average test loss: 0.044923772368994024\n",
      "Epoch 176/300\n",
      "Average training loss: 12.439667391459148\n",
      "Average test loss: 0.25015167015128664\n",
      "Epoch 177/300\n",
      "Average training loss: 12.003122394137913\n",
      "Average test loss: 5.744753344103694\n",
      "Epoch 178/300\n",
      "Average training loss: 11.588485141330295\n",
      "Average test loss: 0.007449781596660614\n",
      "Epoch 179/300\n",
      "Average training loss: 10.770578854878744\n",
      "Average test loss: 0.010106114176412424\n",
      "Epoch 181/300\n",
      "Average training loss: 10.355162288241916\n",
      "Average test loss: 0.008275413033448987\n",
      "Epoch 182/300\n",
      "Average training loss: 9.953278144836426\n",
      "Average test loss: 0.006258085717757543\n",
      "Epoch 183/300\n",
      "Average training loss: 9.554918262057834\n",
      "Average test loss: 0.00654872142813272\n",
      "Epoch 184/300\n",
      "Average training loss: 9.160974280463325\n",
      "Average test loss: 0.006743386421766546\n",
      "Epoch 185/300\n",
      "Average training loss: 8.771407825893826\n",
      "Average test loss: 0.00577375370471014\n",
      "Epoch 186/300\n",
      "Average training loss: 8.367463818868002\n",
      "Average test loss: 0.005982693814155129\n",
      "Epoch 187/300\n",
      "Average training loss: 7.955856502956815\n",
      "Average test loss: 0.00542374986410141\n",
      "Epoch 188/300\n",
      "Average training loss: 7.517650703854031\n",
      "Average training loss: 7.029589925554063\n",
      "Average test loss: 0.13809158370229932\n",
      "Epoch 190/300\n",
      "Average training loss: 6.48216849899292\n",
      "Average test loss: 0.005444532027261125\n",
      "Epoch 191/300\n",
      "Average training loss: 5.980499844445123\n",
      "Average test loss: 0.005229657888205515\n",
      "Epoch 192/300\n",
      "Average training loss: 5.5568417095608185\n",
      "Average test loss: 0.005034488828231891\n",
      "Epoch 193/300\n",
      "Average training loss: 5.150037858751085\n",
      "Average test loss: 0.005056308487637175\n",
      "Epoch 194/300\n",
      "Average training loss: 4.775063404083252\n",
      "Average test loss: 0.005451414910662505\n",
      "Epoch 195/300\n",
      "Average training loss: 4.388065691630046\n",
      "Average test loss: 0.005048647064301703\n",
      "Epoch 196/300\n",
      "Average training loss: 3.9739885881212023\n",
      "Average test loss: 0.005260306404696571\n",
      "Epoch 197/300\n",
      "Average training loss: 3.5127237777709963\n",
      "Average test loss: 0.00481648572617107\n",
      "Epoch 198/300\n",
      "Average training loss: 3.0373873252868653\n",
      "Average test loss: 0.004701893157015244\n",
      "Epoch 199/300\n",
      "Average training loss: 2.6968674081166584\n",
      "Average test loss: 0.004704216665277879\n",
      "Epoch 200/300\n",
      "Average training loss: 2.435338126924303\n",
      "Average test loss: 0.004965717803272936\n",
      "Epoch 201/300\n",
      "Average training loss: 2.2086573713090685\n",
      "Average test loss: 0.004814998810489972\n",
      "Epoch 202/300\n",
      "Average training loss: 1.9982811727523804\n",
      "Average test loss: 0.004580704498622153\n",
      "Epoch 203/300\n",
      "Average training loss: 1.7880885371102226\n",
      "Average test loss: 0.0044635688476264476\n",
      "Epoch 204/300\n",
      "Average training loss: 1.5781625920401678\n",
      "Average test loss: 0.0048712455837263005\n",
      "Epoch 205/300\n",
      "Average test loss: 0.004528412873960204\n",
      "Epoch 206/300\n",
      "Average training loss: 1.199799421204461\n",
      "Average test loss: 0.006503738039483627\n",
      "Epoch 207/300\n",
      "Average training loss: 1.035526703675588\n",
      "Average test loss: 0.004566038909471697\n",
      "Epoch 208/300\n",
      "Average training loss: 0.8913389526473151\n",
      "Average test loss: 0.004318473034434848\n",
      "Epoch 209/300\n",
      "Average training loss: 0.7642083803282844\n",
      "Average test loss: 0.004380586394419273\n",
      "Epoch 210/300\n",
      "Average training loss: 0.6524242162174648\n",
      "Average test loss: 0.008690860427088208\n",
      "Epoch 211/300\n",
      "Average training loss: 0.5611896303494771\n",
      "Average test loss: 0.01366964102619224\n",
      "Epoch 212/300\n",
      "Average training loss: 0.4852515533765157\n",
      "Average test loss: 0.004628567419946193\n",
      "Epoch 213/300\n",
      "Average training loss: 0.42440007400512697\n",
      "Average test loss: 0.004253182145456473\n",
      "Epoch 214/300\n",
      "Average training loss: 0.3745251636505127\n",
      "Average test loss: 0.0042440231227212485\n",
      "Epoch 215/300\n",
      "Average training loss: 0.34898965997166104\n",
      "Average test loss: 0.004303104105095068\n",
      "Epoch 216/300\n",
      "Average training loss: 0.3007323552502526\n",
      "Average test loss: 0.004214794464409351\n",
      "Epoch 217/300\n",
      "Average training loss: 0.27949894632233513\n",
      "Average test loss: 0.0051267219165133104\n",
      "Epoch 218/300\n",
      "Average training loss: 0.2550706950293647\n",
      "Average test loss: 0.0042210461056480806\n",
      "Epoch 219/300\n",
      "Average training loss: 0.23860545955763923\n",
      "Average test loss: 0.004252440014233192\n",
      "Epoch 220/300\n",
      "Average training loss: 0.22646175667974683\n",
      "Average test loss: 0.00450279497458703\n",
      "Epoch 221/300\n",
      "Average training loss: 0.21546370413568286\n",
      "Average test loss: 0.004210095483395788\n",
      "Epoch 222/300\n",
      "Average training loss: 0.19888459695710076\n",
      "Average test loss: 0.005185977422114876\n",
      "Epoch 224/300\n",
      "Average training loss: 0.19213233436478508\n",
      "Average test loss: 0.00419961764332321\n",
      "Epoch 225/300\n",
      "Average training loss: 0.18526860181490581\n",
      "Average test loss: 0.0044631505409876504\n",
      "Epoch 226/300\n",
      "Average training loss: 0.17983261223634084\n",
      "Average test loss: 0.004962131864494747\n",
      "Epoch 227/300\n",
      "Average training loss: 0.1737035334772534\n",
      "Average test loss: 0.004188297429432472\n",
      "Epoch 228/300\n",
      "Average training loss: 0.16891183604796728\n",
      "Average test loss: 0.004192588255844182\n",
      "Epoch 229/300\n",
      "Average training loss: 0.1658709228436152\n",
      "Average test loss: 0.004423211480594344\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1629640645980835\n",
      "Average test loss: 0.006080427745978037\n",
      "Epoch 232/300\n",
      "Average training loss: 0.15869316217634413\n",
      "Average test loss: 0.004167672252489461\n",
      "Epoch 233/300\n",
      "Average training loss: 0.155814434727033\n",
      "Average test loss: 0.00417108590176536\n",
      "Epoch 235/300\n",
      "Average training loss: 0.15351474846071667\n",
      "Average test loss: 0.0075369453444663025\n",
      "Epoch 236/300\n",
      "Average training loss: 0.15223364630010394\n",
      "Average test loss: 0.004181122085907393\n",
      "Epoch 237/300\n",
      "Average training loss: 0.15100913519329495\n",
      "Average test loss: 0.004292806765271558\n",
      "Epoch 238/300\n",
      "Average training loss: 0.15166192424297334\n",
      "Average test loss: 0.004105074937558836\n",
      "Epoch 239/300\n",
      "Average training loss: 0.1485112166404724\n",
      "Average test loss: 0.004135940756648779\n",
      "Epoch 240/300\n",
      "Average training loss: 0.14885423400666978\n",
      "Average test loss: 0.004159245361884435\n",
      "Epoch 241/300\n",
      "Average training loss: 0.1469365039534039\n",
      "Average test loss: 0.004132751033537918\n",
      "Epoch 242/300\n",
      "Average training loss: 0.14635191388924917\n",
      "Average test loss: 0.004118328677076432\n",
      "Epoch 243/300\n",
      "Average training loss: 4900650.388229107\n",
      "Average test loss: 14.813674259662628\n",
      "Epoch 244/300\n",
      "Average training loss: 31.97701466200087\n",
      "Average test loss: 0.11405363165669971\n",
      "Epoch 245/300\n",
      "Average training loss: 28.709684906005858\n",
      "Average test loss: 0.2066905178361469\n",
      "Epoch 246/300\n",
      "Average training loss: 26.390045381334094\n",
      "Average test loss: 414.41419111061094\n",
      "Epoch 247/300\n",
      "Average training loss: 24.50214399549696\n",
      "Average test loss: 0.08967396203014585\n",
      "Epoch 248/300\n",
      "Average training loss: 22.901287314520943\n",
      "Average test loss: 48.822120385699805\n",
      "Epoch 249/300\n",
      "Average training loss: 20.235826417711046\n",
      "Average test loss: 10.092336104336711\n",
      "Epoch 251/300\n",
      "Average training loss: 19.128240610758464\n",
      "Average test loss: 5.986423781242635\n",
      "Epoch 252/300\n",
      "Average training loss: 18.107161677042644\n",
      "Average test loss: 2128.0561311918764\n",
      "Epoch 253/300\n",
      "Average training loss: 17.183491817898222\n",
      "Average test loss: 0.011617747643755541\n",
      "Epoch 254/300\n",
      "Average training loss: 16.25022427537706\n",
      "Average test loss: 0.012594635912941562\n",
      "Epoch 255/300\n",
      "Average training loss: 15.354009492662218\n",
      "Average test loss: 0.011332390447871552\n",
      "Epoch 256/300\n",
      "Average training loss: 14.478148192511664\n",
      "Average test loss: 0.48362213723195924\n",
      "Epoch 257/300\n",
      "Average training loss: 13.632147229512533\n",
      "Average test loss: 0.08035708411451843\n",
      "Epoch 258/300\n",
      "Average training loss: 12.817434711032444\n",
      "Average test loss: 0.059689270546038946\n",
      "Epoch 259/300\n",
      "Average training loss: 12.069476773579915\n",
      "Average test loss: 0.11782139747672611\n",
      "Epoch 260/300\n",
      "Average training loss: 11.287730286492241\n",
      "Average test loss: 94.99246211487717\n",
      "Epoch 261/300\n",
      "Average training loss: 10.547702128092448\n",
      "Average test loss: 6.915352468921078\n",
      "Epoch 262/300\n",
      "Average training loss: 9.238493679470485\n",
      "Average test loss: 0.006895666692405939\n",
      "Epoch 264/300\n",
      "Average training loss: 8.680784816318088\n",
      "Average test loss: 0.006664686222457223\n",
      "Epoch 265/300\n",
      "Average training loss: 8.159222693973117\n",
      "Average test loss: 0.006401497788727283\n",
      "Epoch 266/300\n",
      "Average training loss: 7.659809258355034\n",
      "Average test loss: 0.3983963349196646\n",
      "Epoch 267/300\n",
      "Average training loss: 7.197550301445855\n",
      "Average test loss: 0.008228064304424658\n",
      "Epoch 268/300\n",
      "Average training loss: 6.771537139468723\n",
      "Average test loss: 0.005774501055065128\n",
      "Epoch 269/300\n",
      "Average training loss: 6.370941802978516\n",
      "Average test loss: 0.005949722400969929\n",
      "Epoch 270/300\n",
      "Average training loss: 5.992222357008192\n",
      "Average test loss: 0.005480823603769143\n",
      "Epoch 271/300\n",
      "Average training loss: 5.295345094892713\n",
      "Average test loss: 0.006105710921809077\n",
      "Epoch 273/300\n",
      "Average training loss: 4.970367563035753\n",
      "Average test loss: 0.005325729139977031\n",
      "Epoch 274/300\n",
      "Average training loss: 4.6559693391587995\n",
      "Average test loss: 0.00506152440690332\n",
      "Epoch 275/300\n",
      "Average training loss: 4.3816031608581545\n",
      "Average test loss: 0.005113145776920848\n",
      "Epoch 276/300\n",
      "Average training loss: 4.121696471744113\n",
      "Average test loss: 0.005213168243774109\n",
      "Epoch 277/300\n",
      "Average training loss: 3.8989510061475965\n",
      "Average test loss: 0.005403913447840346\n",
      "Epoch 278/300\n",
      "Average training loss: 3.6877201724582247\n",
      "Average test loss: 0.0055994504582550795\n",
      "Epoch 279/300\n",
      "Average training loss: 3.4877293605804445\n",
      "Average test loss: 2.1244800760944687\n",
      "Epoch 280/300\n",
      "Average training loss: 3.1018014833662244\n",
      "Average test loss: 0.10161976192477677\n",
      "Epoch 282/300\n",
      "Average training loss: 2.9037665740119087\n",
      "Average test loss: 0.005734419132686324\n",
      "Epoch 283/300\n",
      "Average training loss: 2.6935936024983724\n",
      "Average test loss: 0.004831359299934572\n",
      "Epoch 284/300\n",
      "Average training loss: 2.4610854642656115\n",
      "Average test loss: 0.004545436394297414\n",
      "Epoch 285/300\n",
      "Average training loss: 2.2180964336395266\n",
      "Average test loss: 0.004466420929051108\n",
      "Epoch 286/300\n",
      "Average training loss: 1.9927128445307414\n",
      "Average test loss: 0.004528456756845116\n",
      "Epoch 287/300\n",
      "Average training loss: 1.7776331337822808\n",
      "Average test loss: 0.004434058841524853\n",
      "Epoch 288/300\n",
      "Average training loss: 1.5631251951853433\n",
      "Average test loss: 0.004373078424069617\n",
      "Epoch 289/300\n",
      "Average training loss: 1.1605704316033258\n",
      "Average test loss: 0.005813828860719999\n",
      "Epoch 291/300\n",
      "Average training loss: 0.9988497446907891\n",
      "Average test loss: 0.00429761158902612\n",
      "Epoch 292/300\n",
      "Average training loss: 0.7259723838700188\n",
      "Average test loss: 0.004272055386048224\n",
      "Epoch 294/300\n",
      "Average training loss: 0.6112567027674781\n",
      "Average test loss: 0.004202751564068926\n",
      "Epoch 295/300\n",
      "Average training loss: 0.5124203173054589\n",
      "Average test loss: 0.004204947337508202\n",
      "Epoch 296/300\n",
      "Average training loss: 0.4297136253515879\n",
      "Average test loss: 0.004191812848051389\n",
      "Epoch 297/300\n",
      "Average training loss: 0.3634413244989183\n",
      "Average test loss: 0.00423810087558296\n",
      "Epoch 298/300\n",
      "Average training loss: 0.3145743953651852\n",
      "Average test loss: 0.004306117673301035\n",
      "Epoch 299/300\n",
      "Average training loss: 0.2793061532444424\n",
      "Average test loss: 0.004173471849411726\n",
      "Epoch 300/300\n",
      "Average training loss: 0.25222876461346944\n",
      "Average test loss: 0.004210914598157009\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.1014642545912\n",
      "Average test loss: 16.78812219932013\n",
      "Epoch 2/300\n",
      "Average training loss: 5.893026071336534\n",
      "Average test loss: 0.007321018440028032\n",
      "Epoch 3/300\n",
      "Average training loss: 4.445906433953179\n",
      "Average test loss: 0.005226892873644829\n",
      "Epoch 4/300\n",
      "Average training loss: 3.7476878956688773\n",
      "Average test loss: 0.019836851516738533\n",
      "Epoch 5/300\n",
      "Average training loss: 3.0748595371246337\n",
      "Average test loss: 0.004748537465102143\n",
      "Epoch 6/300\n",
      "Average training loss: 2.6361409221225314\n",
      "Average test loss: 0.004925851046004229\n",
      "Epoch 7/300\n",
      "Average training loss: 2.2834387187957765\n",
      "Average test loss: 0.004370162517246273\n",
      "Epoch 8/300\n",
      "Average training loss: 1.901024798075358\n",
      "Average test loss: 0.004229894973751571\n",
      "Epoch 9/300\n",
      "Average training loss: 1.5583685085508558\n",
      "Average test loss: 0.003919066477359997\n",
      "Epoch 10/300\n",
      "Average training loss: 1.0890307071473864\n",
      "Average test loss: 0.004568570766597987\n",
      "Epoch 12/300\n",
      "Average training loss: 0.9174317289458381\n",
      "Average test loss: 0.003651230396495925\n",
      "Epoch 13/300\n",
      "Average training loss: 0.7754637878206041\n",
      "Average test loss: 0.0034157374931706322\n",
      "Epoch 14/300\n",
      "Average training loss: 0.6632399526172215\n",
      "Average test loss: 0.0033931154689441126\n",
      "Epoch 15/300\n",
      "Average training loss: 0.5724637962977092\n",
      "Average test loss: 0.0033542546656810577\n",
      "Epoch 16/300\n",
      "Average training loss: 0.5002757384777069\n",
      "Average test loss: 0.005551583575291766\n",
      "Epoch 17/300\n",
      "Average training loss: 0.44364589722951253\n",
      "Average test loss: 0.003250241621914837\n",
      "Epoch 18/300\n",
      "Average training loss: 0.3979915146297879\n",
      "Average test loss: 0.0031396429027534195\n",
      "Epoch 19/300\n",
      "Average training loss: 0.328532602681054\n",
      "Average test loss: 0.00399206076707277\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2782935310602188\n",
      "Average test loss: 0.0028611374041065574\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2574644034306208\n",
      "Average test loss: 0.0033046961832377646\n",
      "Epoch 24/300\n",
      "Average training loss: 0.24111770641803743\n",
      "Average test loss: 0.002777380701361431\n",
      "Epoch 25/300\n",
      "Average training loss: 0.22404885816574097\n",
      "Average test loss: 0.0029371398710128335\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2099862273534139\n",
      "Average test loss: 0.0027449660969691143\n",
      "Epoch 27/300\n",
      "Average training loss: 0.19831007530954148\n",
      "Average test loss: 0.00281926414453321\n",
      "Epoch 28/300\n",
      "Average training loss: 0.18737707146008808\n",
      "Average test loss: 0.0028316546661986243\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1789743302265803\n",
      "Average test loss: 0.0027787494195832146\n",
      "Epoch 30/300\n",
      "Average training loss: 0.16889445683691237\n",
      "Average test loss: 0.006936405778552095\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1614929123851988\n",
      "Average test loss: 0.0026910048130278784\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15487068733904097\n",
      "Average test loss: 0.00263379977705578\n",
      "Epoch 33/300\n",
      "Average training loss: 0.14935443013244204\n",
      "Average test loss: 0.0025674856127136285\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1439847220049964\n",
      "Average test loss: 0.0026417781406392654\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1382983248896069\n",
      "Average test loss: 0.002594182626551224\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13393823778629302\n",
      "Average test loss: 0.0026991848457190728\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1305635506576962\n",
      "Average test loss: 0.0025186692972977956\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12617233502864839\n",
      "Average test loss: 0.0025332947776963315\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12277016864220301\n",
      "Average test loss: 0.0030634228533340823\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11671011502875223\n",
      "Average test loss: 0.002503418901314338\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11480057284567091\n",
      "Average test loss: 0.004027594191332658\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11099285901255078\n",
      "Average test loss: 0.0030025497012668187\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10807085308101443\n",
      "Average test loss: 0.0027151103367408116\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10598657997449239\n",
      "Average test loss: 0.002476814350527194\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10464922283755408\n",
      "Average test loss: 0.0026719447943485447\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10231120977799098\n",
      "Average test loss: 0.008699167853428258\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10241647151443693\n",
      "Average test loss: 0.0024845991985251504\n",
      "Epoch 49/300\n",
      "Average test loss: 0.002391490295736326\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0976053057346079\n",
      "Average test loss: 0.0025185316660337976\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09873908284637663\n",
      "Average test loss: 0.002538355313344962\n",
      "Epoch 52/300\n",
      "Average training loss: 6.126533001648055\n",
      "Average test loss: 0.0067039962775177426\n",
      "Epoch 53/300\n",
      "Average training loss: 1.5967407213846843\n",
      "Average test loss: 0.0032725930066986215\n",
      "Epoch 54/300\n",
      "Average training loss: 0.7837467787530688\n",
      "Average test loss: 0.0030365940373804834\n",
      "Epoch 55/300\n",
      "Average training loss: 0.49484800023502773\n",
      "Average test loss: 0.0029054567048119174\n",
      "Epoch 56/300\n",
      "Average training loss: 0.35135253151257834\n",
      "Average test loss: 0.003269306830233998\n",
      "Epoch 57/300\n",
      "Average training loss: 0.2253360619544983\n",
      "Average test loss: 0.00284010805355178\n",
      "Epoch 59/300\n",
      "Average training loss: 0.19257119965553285\n",
      "Average test loss: 0.0027225726031594806\n",
      "Epoch 60/300\n",
      "Average training loss: 0.16943821113639407\n",
      "Average test loss: 0.0026879369555455114\n",
      "Epoch 61/300\n",
      "Average training loss: 0.15640820164150662\n",
      "Average test loss: 0.002638145183937417\n",
      "Epoch 62/300\n",
      "Average training loss: 0.14763840848869747\n",
      "Average test loss: 0.002638121741720372\n",
      "Epoch 63/300\n",
      "Average training loss: 0.14069695932335324\n",
      "Average test loss: 0.0026243676400432986\n",
      "Epoch 64/300\n",
      "Average training loss: 0.13541594423188102\n",
      "Average test loss: 0.0025427920824537677\n",
      "Epoch 65/300\n",
      "Average training loss: 0.13062788461314306\n",
      "Average test loss: 0.0025678152536145514\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1270348079999288\n",
      "Average test loss: 0.007569656652294927\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12336561743418376\n",
      "Average test loss: 0.0024747225743614967\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12038030253516303\n",
      "Average test loss: 0.0024819755992955632\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1168409533103307\n",
      "Average test loss: 0.002610511345581876\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11207480490207672\n",
      "Average test loss: 0.002429870045743883\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11020019962390264\n",
      "Average test loss: 0.0024303171702971063\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10769683010710611\n",
      "Average test loss: 0.002428155087348488\n",
      "Epoch 74/300\n",
      "Average training loss: 0.104405367049906\n",
      "Average test loss: 0.002539254555168251\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1029214919673072\n",
      "Average test loss: 0.0025351906462262076\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1016424473590321\n",
      "Average test loss: 0.002447576800775197\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10015303863419427\n",
      "Average test loss: 0.004204993515378899\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09818052215046352\n",
      "Average test loss: 0.002399512264670597\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09719434295760261\n",
      "Average test loss: 0.002372274468756384\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09660311233997344\n",
      "Average test loss: 0.002365621759866675\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09625352372725805\n",
      "Average test loss: 0.002481991046004825\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09514305429326163\n",
      "Average test loss: 0.002367638152403136\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09416165980034405\n",
      "Average test loss: 0.0025977760015262496\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10024657428264618\n",
      "Average test loss: 0.0032306989992244377\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09502256783511903\n",
      "Average test loss: 0.002590303093401922\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09323269659943051\n",
      "Average test loss: 0.002349989275344544\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09200712754991319\n",
      "Average test loss: 0.0023990358461936315\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09163506194618014\n",
      "Average test loss: 0.0026909488569945095\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0916861481335428\n",
      "Average test loss: 0.0024820602246456674\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09068966292672687\n",
      "Average test loss: 0.0023311843482984435\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09002902437580956\n",
      "Average test loss: 0.0023733185796688\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09050379957093133\n",
      "Average test loss: 0.002384461366261045\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08910185941722658\n",
      "Average test loss: 0.007057352827654945\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08984450322389603\n",
      "Average test loss: 0.0027500419842286243\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08800474328464931\n",
      "Average test loss: 0.002352005819583105\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08785718513197369\n",
      "Average test loss: 0.002407109152629144\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08724955913755629\n",
      "Average test loss: 0.0023607805940426057\n",
      "Epoch 100/300\n",
      "Average training loss: 0.09000091805722979\n",
      "Average test loss: 0.0023791861906647684\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08648762695656882\n",
      "Average test loss: 0.002518373541533947\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08616050348679224\n",
      "Average test loss: 0.002412821659611331\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08580695784091949\n",
      "Average test loss: 0.0023715904880728987\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08596669558021758\n",
      "Average test loss: 0.002352880124002695\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08526502565542857\n",
      "Average test loss: 0.0023217461864567467\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08702463834153282\n",
      "Average test loss: 0.002333313648381995\n",
      "Epoch 107/300\n",
      "Average training loss: 5.136495984411902\n",
      "Average test loss: 4.982845535978675\n",
      "Epoch 108/300\n",
      "Average training loss: 1.6623037628597683\n",
      "Average test loss: 2.480158259474569\n",
      "Epoch 109/300\n",
      "Average training loss: 0.8440863760842218\n",
      "Average test loss: 0.8121901652287278\n",
      "Epoch 110/300\n",
      "Average training loss: 0.578121843179067\n",
      "Average test loss: 2174.5291917185764\n",
      "Epoch 111/300\n",
      "Average training loss: 0.44312192185719806\n",
      "Average test loss: 9.103230536450114\n",
      "Epoch 112/300\n",
      "Average training loss: 0.3591055013073815\n",
      "Average test loss: 2.0861777828909043\n",
      "Epoch 113/300\n",
      "Average training loss: 0.3015701274342007\n",
      "Average test loss: 0.7024783939218355\n",
      "Epoch 114/300\n",
      "Average training loss: 0.25870662853452897\n",
      "Average test loss: 92.54464770142982\n",
      "Epoch 115/300\n",
      "Average training loss: 0.22784557961093055\n",
      "Average test loss: 6919.026997774045\n",
      "Epoch 116/300\n",
      "Average training loss: 0.20619300985336303\n",
      "Average test loss: 18864236018.632744\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1880083017349243\n",
      "Average test loss: 1794164.3829582538\n",
      "Epoch 118/300\n",
      "Average training loss: 0.1720847435262468\n",
      "Average test loss: 0.005699529452042447\n",
      "Epoch 119/300\n",
      "Average training loss: 0.15704049804475573\n",
      "Average test loss: 0.05804783357679844\n",
      "Epoch 120/300\n",
      "Average training loss: 0.14074669840600756\n",
      "Average test loss: 0.004002833091653883\n",
      "Epoch 121/300\n",
      "Average training loss: 0.131359451605214\n",
      "Average test loss: 0.0024782472900632354\n",
      "Epoch 122/300\n",
      "Average training loss: 0.12576567227310603\n",
      "Average test loss: 0.003135885605795516\n",
      "Epoch 123/300\n",
      "Average training loss: 0.118484738945961\n",
      "Average test loss: 0.0023765259008440708\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11377746019098493\n",
      "Average test loss: 0.0023784339825312297\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11027778896358278\n",
      "Average test loss: 0.002659230461137162\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10573351586527295\n",
      "Average test loss: 0.00252748328830219\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10287400358915329\n",
      "Average test loss: 0.005229284739742677\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10021098679966396\n",
      "Average test loss: 0.0032528285077876515\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09850646522310046\n",
      "Average test loss: 0.002420243688341644\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09643459483649995\n",
      "Average test loss: 0.0025436218507173987\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09637536882691913\n",
      "Average test loss: 0.0023493085166232455\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09345907481511434\n",
      "Average test loss: 0.0030228446676499312\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0923247462908427\n",
      "Average test loss: 0.0024137750359045133\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09164219113190969\n",
      "Average test loss: 0.0023172116634539433\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09265137898921967\n",
      "Average test loss: 0.00238696784277757\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0896602220137914\n",
      "Average test loss: 0.0026758147303221954\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09022511761056053\n",
      "Average test loss: 0.002324327635578811\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08851041697131262\n",
      "Average test loss: 0.0023780999591367113\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08777289658122593\n",
      "Average test loss: 0.004974808317505651\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08780080193943447\n",
      "Average test loss: 0.002308847848325968\n",
      "Epoch 141/300\n",
      "Average training loss: 7045.959250361972\n",
      "Average test loss: 0.038768756988975736\n",
      "Epoch 142/300\n",
      "Average training loss: 22.898873489379884\n",
      "Average test loss: 4738265319.741605\n",
      "Epoch 143/300\n",
      "Average training loss: 14.357082038879394\n",
      "Average test loss: 106.62094380082024\n",
      "Epoch 144/300\n",
      "Average training loss: 11.561109052870009\n",
      "Average test loss: 109457.15037656832\n",
      "Epoch 145/300\n",
      "Average training loss: 10.049715524461535\n",
      "Average test loss: 14.638506493198374\n",
      "Epoch 146/300\n",
      "Average training loss: 8.90450560675727\n",
      "Average test loss: 0.01147900346873535\n",
      "Epoch 147/300\n",
      "Average training loss: 7.94153192392985\n",
      "Average test loss: 0.1021510428186092\n",
      "Epoch 148/300\n",
      "Average training loss: 7.128885516696506\n",
      "Average test loss: 0.013862834647297859\n",
      "Epoch 149/300\n",
      "Average training loss: 6.376901510450575\n",
      "Average test loss: 0.010072977021750477\n",
      "Epoch 150/300\n",
      "Average training loss: 5.661003872765435\n",
      "Average test loss: 0.0044299602607886\n",
      "Epoch 151/300\n",
      "Average training loss: 5.014016036139594\n",
      "Average test loss: 356.44701780870224\n",
      "Epoch 152/300\n",
      "Average training loss: 4.4679115986294216\n",
      "Average test loss: 0.003924166966229678\n",
      "Epoch 153/300\n",
      "Average training loss: 4.006124661127727\n",
      "Average test loss: 0.003599129681578941\n",
      "Epoch 154/300\n",
      "Average training loss: 3.5790916837056477\n",
      "Average test loss: 0.003385772367111511\n",
      "Epoch 155/300\n",
      "Average training loss: 3.1354645551045737\n",
      "Average test loss: 0.0038504147289527787\n",
      "Epoch 156/300\n",
      "Average training loss: 2.7486279296875\n",
      "Average test loss: 0.004169682927636637\n",
      "Epoch 157/300\n",
      "Average training loss: 2.440322022120158\n",
      "Average test loss: 0.0030506605907446808\n",
      "Epoch 158/300\n",
      "Average training loss: 2.1695058346854315\n",
      "Average test loss: 0.0028758986805462175\n",
      "Epoch 159/300\n",
      "Average training loss: 1.9274567962222628\n",
      "Average test loss: 0.0028274772088560792\n",
      "Epoch 160/300\n",
      "Average training loss: 1.7014643171098498\n",
      "Average test loss: 0.0028725822557591728\n",
      "Epoch 161/300\n",
      "Average training loss: 1.5010506914986503\n",
      "Average test loss: 0.0038636444064064157\n",
      "Epoch 162/300\n",
      "Average training loss: 1.3071394839816624\n",
      "Average test loss: 0.0035614279115365613\n",
      "Epoch 163/300\n",
      "Average training loss: 1.1322193057801988\n",
      "Average test loss: 0.0025691488718407023\n",
      "Epoch 164/300\n",
      "Average training loss: 0.9697929318745931\n",
      "Average test loss: 0.0026242745955371194\n",
      "Epoch 165/300\n",
      "Average training loss: 0.8241773404015436\n",
      "Average test loss: 0.002515479537554913\n",
      "Epoch 166/300\n",
      "Average training loss: 0.690642212178972\n",
      "Average test loss: 0.002624815976868073\n",
      "Epoch 167/300\n",
      "Average training loss: 0.5775181818008422\n",
      "Average test loss: 0.002468129233353668\n",
      "Epoch 168/300\n",
      "Average training loss: 0.4859124870300293\n",
      "Average test loss: 0.0024456104460275838\n",
      "Epoch 169/300\n",
      "Average training loss: 0.40652888205316334\n",
      "Average test loss: 0.0024281686592019266\n",
      "Epoch 170/300\n",
      "Average training loss: 0.3415817161136203\n",
      "Average test loss: 0.0024988033238591417\n",
      "Epoch 171/300\n",
      "Average training loss: 0.28887674318419565\n",
      "Average test loss: 0.00247082348436945\n",
      "Epoch 172/300\n",
      "Average training loss: 0.24782461828655666\n",
      "Average test loss: 0.0024438112524027625\n",
      "Epoch 173/300\n",
      "Average training loss: 0.21659525419606102\n",
      "Average test loss: 0.0024104161403245396\n",
      "Epoch 174/300\n",
      "Average training loss: 0.19279055542416043\n",
      "Average test loss: 0.028658609049187767\n",
      "Epoch 175/300\n",
      "Average training loss: 0.1742967109547721\n",
      "Average test loss: 0.002363127881868018\n",
      "Epoch 176/300\n",
      "Average training loss: 0.16167246704631383\n",
      "Average test loss: 0.002341796049848199\n",
      "Epoch 177/300\n",
      "Average training loss: 0.14979824042320253\n",
      "Average test loss: 0.002361420775349769\n",
      "Epoch 178/300\n",
      "Average training loss: 0.14101616483264498\n",
      "Average test loss: 0.002331459704786539\n",
      "Epoch 179/300\n",
      "Average training loss: 0.13375105961163838\n",
      "Average test loss: 0.002340153773418731\n",
      "Epoch 180/300\n",
      "Average training loss: 0.12685670327477985\n",
      "Average test loss: 0.002326376280643874\n",
      "Epoch 181/300\n",
      "Average training loss: 0.12189146703481674\n",
      "Average test loss: 0.003266242144422399\n",
      "Epoch 182/300\n",
      "Average training loss: 0.11712877808014552\n",
      "Average test loss: 0.002340836420862211\n",
      "Epoch 183/300\n",
      "Average training loss: 0.11296452131536272\n",
      "Average test loss: 0.003064935940835211\n",
      "Epoch 184/300\n",
      "Average training loss: 0.1108120598991712\n",
      "Average test loss: 0.0024020884769658246\n",
      "Epoch 185/300\n",
      "Average training loss: 0.10643322180377113\n",
      "Average test loss: 0.0023185259660498964\n",
      "Epoch 186/300\n",
      "Average training loss: 0.1031386295888159\n",
      "Average test loss: 0.0024777317523128456\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10089627159966363\n",
      "Average test loss: 0.0023367623572962152\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09889292116297616\n",
      "Average test loss: 0.002369182684769233\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0970728863676389\n",
      "Average test loss: 0.0023598188746513592\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10080844897694058\n",
      "Average test loss: 0.002455828675793277\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09451470767789417\n",
      "Average test loss: 0.0024800171562367017\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0938897762828403\n",
      "Average test loss: 0.002410602764123016\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09219136080476972\n",
      "Average test loss: 0.003352629820091857\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09144314072198338\n",
      "Average test loss: 0.0027216152821977935\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09034001390801535\n",
      "Average test loss: 0.0023038289023356306\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08967190808720059\n",
      "Average test loss: 0.0024454326002548137\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08969384204016792\n",
      "Average test loss: 448717.058203125\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08854838370614582\n",
      "Average test loss: 0.002349790221390625\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08790056100818847\n",
      "Average test loss: 0.0023922757051057287\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08743300791581472\n",
      "Average test loss: 0.0034932047813716863\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08682630268732706\n",
      "Average test loss: 0.005121987625128693\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08711694614092509\n",
      "Average test loss: 0.00816912748147216\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09274239873886109\n",
      "Average test loss: 0.002389913311228156\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08584903790553411\n",
      "Average test loss: 0.002452418375449876\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08529249937666787\n",
      "Average test loss: 0.0024955112853397928\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08560442010561625\n",
      "Average test loss: 0.0023662415457268555\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08477587309810851\n",
      "Average test loss: 0.0022957027859778867\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08507066785626942\n",
      "Average test loss: 0.002337292793310351\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0842295340233379\n",
      "Average test loss: 0.002262658271421161\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08395432231161329\n",
      "Average test loss: 0.00228845860891872\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08431398783127467\n",
      "Average test loss: 0.002492872011433873\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08332930695679452\n",
      "Average test loss: 0.0022863369589257573\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08321640183528264\n",
      "Average test loss: 0.38567255053255295\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08306144963370429\n",
      "Average test loss: 0.0022848454548252952\n",
      "Epoch 215/300\n",
      "Average training loss: 0.12755105930566787\n",
      "Average test loss: 0.0026312437169253825\n",
      "Epoch 216/300\n",
      "Average training loss: 0.12243892470333312\n",
      "Average test loss: 0.0023408403268290892\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09835227952400843\n",
      "Average test loss: 0.002325681372028258\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09333150626222293\n",
      "Average test loss: 0.0023010890500413045\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0904837035536766\n",
      "Average test loss: 0.0022854737668401666\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08870875825484593\n",
      "Average test loss: 0.0022903330450256667\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08720054835081101\n",
      "Average test loss: 0.002747264126315713\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08561306511031257\n",
      "Average test loss: 0.0022627623671044907\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08551803298128975\n",
      "Average test loss: 0.002822636232814855\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08419005028737916\n",
      "Average test loss: 0.002496279371074504\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0835659912692176\n",
      "Average test loss: 0.002585906760663622\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0853508593108919\n",
      "Average test loss: 0.00227960521582928\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09813922056224611\n",
      "Average test loss: 0.002277711404694451\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08716312237580617\n",
      "Average test loss: 0.0023400647785100674\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08477750616603427\n",
      "Average test loss: 0.0023092141252838904\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0838117895325025\n",
      "Average test loss: 0.002603243405620257\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0829525084429317\n",
      "Average test loss: 0.002357770331721339\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08249267486068937\n",
      "Average test loss: 0.0023729422106924985\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08215171266264386\n",
      "Average test loss: 0.003547483562388354\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0819015767176946\n",
      "Average test loss: 0.0022728893011808395\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08263985232512157\n",
      "Average test loss: 0.0024152031948582995\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0814103884100914\n",
      "Average test loss: 0.0023780336355169614\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08104690753089057\n",
      "Average test loss: 0.002485160638888677\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08090850931406021\n",
      "Average test loss: 0.0022808756737245453\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08205520311991374\n",
      "Average test loss: 0.0023005395912461812\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08064552277988858\n",
      "Average test loss: 0.0026053607612848283\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0802412801252471\n",
      "Average test loss: 0.0022881591464910243\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0800371327466435\n",
      "Average test loss: 0.0023432687974224487\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0797889587018225\n",
      "Average test loss: 0.003497384592476818\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07996805246008767\n",
      "Average test loss: 0.0029719961707790695\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07956734893057081\n",
      "Average test loss: 0.006544908606136839\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07953436122337977\n",
      "Average test loss: 0.010125136332379447\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07912433640493287\n",
      "Average test loss: 0.0023152313029600516\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07878427995575799\n",
      "Average test loss: 0.002292430584629377\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07941325662202306\n",
      "Average test loss: 0.002289660052810278\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08640218747324414\n",
      "Average test loss: 0.0023455196706992057\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07872749469677608\n",
      "Average test loss: 0.002400679045667251\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07817200420300166\n",
      "Average test loss: 0.002314278719946742\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0780641240047084\n",
      "Average test loss: 0.0022779075066662498\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0779317767686314\n",
      "Average test loss: 0.0023547070777664584\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07984863110383351\n",
      "Average test loss: 0.002487775616761711\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07774302617708842\n",
      "Average test loss: 0.0022769767216199804\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07790613222784466\n",
      "Average test loss: 0.002305713191214535\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07768413048320347\n",
      "Average test loss: 0.008075908761885431\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0779867379864057\n",
      "Average test loss: 0.002267755849390394\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07704928609066539\n",
      "Average test loss: 0.002359078664539589\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07917371004157596\n",
      "Average test loss: 0.002300373829797738\n",
      "Epoch 266/300\n",
      "Average training loss: 87.91251009130478\n",
      "Average test loss: 0.8796791171787514\n",
      "Epoch 267/300\n",
      "Average training loss: 7.929192885928684\n",
      "Average training loss: 6.163819286770291\n",
      "Average test loss: 0.23719128537798922\n",
      "Epoch 269/300\n",
      "Average training loss: 5.0642428296407065\n",
      "Average test loss: 0.5255408339235518\n",
      "Epoch 270/300\n",
      "Average training loss: 4.257027907053629\n",
      "Average test loss: 6.121239956450132\n",
      "Epoch 271/300\n",
      "Average training loss: 3.62896968460083\n",
      "Average test loss: 43.43341040294204\n",
      "Epoch 272/300\n",
      "Average training loss: 3.1146940807766383\n",
      "Average test loss: 11.72095297547678\n",
      "Epoch 273/300\n",
      "Average training loss: 2.6644920569525823\n",
      "Average test loss: 14.989912371468213\n",
      "Epoch 274/300\n",
      "Average training loss: 2.2716592695448132\n",
      "Average test loss: 0.19970870029222634\n",
      "Epoch 275/300\n",
      "Average training loss: 1.6510304455227323\n",
      "Average test loss: 6.746322315590249\n",
      "Epoch 277/300\n",
      "Average training loss: 1.4407107004589506\n",
      "Average test loss: 0.10221164697574245\n",
      "Epoch 278/300\n",
      "Average training loss: 1.265024762471517\n",
      "Average test loss: 0.44804318365433976\n",
      "Epoch 279/300\n",
      "Average training loss: 1.1337653914557564\n",
      "Average test loss: 0.0028850102155572837\n",
      "Epoch 280/300\n",
      "Average training loss: 1.0142269948853386\n",
      "Average test loss: 0.0029617127103524076\n",
      "Epoch 281/300\n",
      "Average training loss: 0.9086708067258199\n",
      "Average test loss: 0.002729725876202186\n",
      "Epoch 282/300\n",
      "Average training loss: 0.8173613985379536\n",
      "Average test loss: 0.035346671589339775\n",
      "Epoch 283/300\n",
      "Average training loss: 0.7295186745325725\n",
      "Average test loss: 0.0027662022279368505\n",
      "Epoch 284/300\n",
      "Average training loss: 0.6482353870073955\n",
      "Average test loss: 1.0894488138539924\n",
      "Epoch 285/300\n",
      "Average training loss: 0.5757395453982883\n",
      "Average test loss: 0.0028127716367857326\n",
      "Epoch 286/300\n",
      "Average training loss: 0.50358549618721\n",
      "Average test loss: 0.0025795123093864986\n",
      "Epoch 287/300\n",
      "Average training loss: 0.4414266780747308\n",
      "Average test loss: 0.0030348733522825772\n",
      "Epoch 288/300\n",
      "Average training loss: 0.3845930003590054\n",
      "Average test loss: 0.0024868980540583533\n",
      "Epoch 289/300\n",
      "Average training loss: 0.33644202280044555\n",
      "Average test loss: 0.0024466734090819956\n",
      "Epoch 290/300\n",
      "Average training loss: 0.29186929829915365\n",
      "Average test loss: 0.0024468214651569725\n",
      "Epoch 291/300\n",
      "Average training loss: 0.25598074822955663\n",
      "Average test loss: 0.006173376060608361\n",
      "Epoch 292/300\n",
      "Average training loss: 0.22689369475841523\n",
      "Average test loss: 0.00238496198070546\n",
      "Epoch 293/300\n",
      "Average training loss: 0.1789795918332206\n",
      "Average test loss: 0.0023576861958329877\n",
      "Epoch 295/300\n",
      "Average training loss: 0.16258664015928903\n",
      "Average test loss: 0.0025039558050533135\n",
      "Epoch 296/300\n",
      "Average training loss: 2.206109834127956\n",
      "Average test loss: 0.05126224039536383\n",
      "Epoch 297/300\n",
      "Average training loss: 2.7694581291410656\n",
      "Average test loss: 5.967013539645407\n",
      "Epoch 298/300\n",
      "Average training loss: 1.0276029444270665\n",
      "Average test loss: 56.43811428525464\n",
      "Epoch 299/300\n",
      "Average training loss: 0.696034756872389\n",
      "Average test loss: 0.40747590873307654\n",
      "Epoch 300/300\n",
      "Average training loss: 0.5436846455997891\n",
      "Average training loss: 13.18773294406467\n",
      "Average test loss: 0.005895082653396659\n",
      "Epoch 2/300\n",
      "Average training loss: 8.159072813669841\n",
      "Average test loss: 0.005276789350228177\n",
      "Epoch 3/300\n",
      "Average training loss: 6.398750497182211\n",
      "Average test loss: 0.004287313646326462\n",
      "Epoch 4/300\n",
      "Average training loss: 5.246140731387668\n",
      "Average test loss: 0.03022687337713109\n",
      "Epoch 5/300\n",
      "Average training loss: 4.4722361263699\n",
      "Average test loss: 0.0821170290681637\n",
      "Epoch 6/300\n",
      "Average training loss: 3.913444479200575\n",
      "Average test loss: 0.0039033614810970096\n",
      "Epoch 7/300\n",
      "Average training loss: 3.3779389726850724\n",
      "Average test loss: 0.0039454735041492515\n",
      "Epoch 8/300\n",
      "Average training loss: 2.9767169352637395\n",
      "Average test loss: 0.003211398359388113\n",
      "Epoch 9/300\n",
      "Average training loss: 2.3803362373775907\n",
      "Average test loss: 0.003010082133528259\n",
      "Epoch 11/300\n",
      "Average training loss: 2.1252080033620198\n",
      "Average test loss: 0.002881360457175308\n",
      "Epoch 12/300\n",
      "Average training loss: 1.8941483720143637\n",
      "Average test loss: 0.0030745737933450274\n",
      "Epoch 13/300\n",
      "Average training loss: 1.6580792332755194\n",
      "Average test loss: 0.002692879146171941\n",
      "Epoch 14/300\n",
      "Average training loss: 1.4626170673370362\n",
      "Average test loss: 0.002586259202617738\n",
      "Epoch 15/300\n",
      "Average training loss: 1.311039112938775\n",
      "Average test loss: 0.004213604413386848\n",
      "Epoch 16/300\n",
      "Average training loss: 1.1658068844477336\n",
      "Average test loss: 0.002386720257914729\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0304109333356222\n",
      "Average test loss: 0.0025669674140711626\n",
      "Epoch 18/300\n",
      "Average training loss: 0.7878360568152534\n",
      "Average test loss: 0.0022539903781273298\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6856038680076599\n",
      "Average test loss: 0.0022660481834577187\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5984148792690701\n",
      "Average test loss: 0.0025181355145242478\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5235796656608581\n",
      "Average test loss: 0.002140451414924529\n",
      "Epoch 23/300\n",
      "Average training loss: 0.45904835645357767\n",
      "Average test loss: 0.0022831603141708506\n",
      "Epoch 24/300\n",
      "Average training loss: 0.40537122776773243\n",
      "Average test loss: 0.00391735330161949\n",
      "Epoch 25/300\n",
      "Average training loss: 0.35895730362998113\n",
      "Average test loss: 0.0020544070752544536\n",
      "Epoch 26/300\n",
      "Average training loss: 0.3188527705669403\n",
      "Average test loss: 0.0019765308279958036\n",
      "Epoch 27/300\n",
      "Average training loss: 0.28452903922398887\n",
      "Average test loss: 0.0020302198305726053\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2541366673045688\n",
      "Average test loss: 0.002467615464909209\n",
      "Epoch 29/300\n",
      "Average training loss: 0.22895172288682725\n",
      "Average test loss: 0.0020155492594672573\n",
      "Epoch 30/300\n",
      "Average training loss: 0.2066595020559099\n",
      "Average test loss: 0.011846255685720179\n",
      "Epoch 31/300\n",
      "Average training loss: 0.19219739727179208\n",
      "Average test loss: 0.002013043151754472\n",
      "Epoch 32/300\n",
      "Average training loss: 0.17300050734149086\n",
      "Average test loss: 0.0018855512087336845\n",
      "Epoch 33/300\n",
      "Average training loss: 0.16011086089081233\n",
      "Average test loss: 0.001961115210213595\n",
      "Epoch 34/300\n",
      "Average training loss: 0.14887354758050705\n",
      "Average test loss: 0.0018445348606134454\n",
      "Epoch 35/300\n",
      "Average training loss: 0.13958414618174236\n",
      "Average test loss: 0.0017254729208846886\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13125310862064363\n",
      "Average test loss: 0.0018556382939633395\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12355479198031956\n",
      "Average test loss: 0.0019023572940172421\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11865714450677235\n",
      "Average test loss: 0.0018864372913829155\n",
      "Epoch 39/300\n",
      "Average training loss: 0.112998826596472\n",
      "Average test loss: 0.0017752423456145658\n",
      "Epoch 40/300\n",
      "Average training loss: 0.107742677198516\n",
      "Average test loss: 0.0018316653620244728\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10379761383268568\n",
      "Average test loss: 0.0018025354457398255\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09947840600543552\n",
      "Average test loss: 0.0017279926151451137\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0958615412513415\n",
      "Average test loss: 0.0016639552851104075\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09225893193483353\n",
      "Average test loss: 0.002093499339806537\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08982790505886078\n",
      "Average test loss: 0.34878844300905865\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08598216358158324\n",
      "Average test loss: 0.0015694876016221113\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08388712038596471\n",
      "Average test loss: 0.0015709841853628556\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08259082131253348\n",
      "Average test loss: 0.0015768488761451508\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07927689751651552\n",
      "Average test loss: 0.0015748004350397322\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07779311426480612\n",
      "Average test loss: 0.0018501536675418417\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09952213428417842\n",
      "Average test loss: 0.0016916473431305753\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0801269339852863\n",
      "Average test loss: 0.0016046664591671692\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07617958780129751\n",
      "Average test loss: 0.00154093621741049\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07461643988225194\n",
      "Average test loss: 0.0016439780887837212\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07240851061211692\n",
      "Average test loss: 0.0015345996403031878\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0708333559665415\n",
      "Average test loss: 0.0015524021112877462\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0698566211660703\n",
      "Average test loss: 0.0015643695979896519\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06888411998086505\n",
      "Average test loss: 0.001538470639847219\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0699060942530632\n",
      "Average test loss: 0.001532350566652086\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06734246936771605\n",
      "Average test loss: 0.0015233394947523871\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06576370076007314\n",
      "Average test loss: 0.0015910980672472054\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06934325845705139\n",
      "Average test loss: 0.0016059071773456203\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0740017046365473\n",
      "Average test loss: 0.0015600303651558027\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06591613081594308\n",
      "Average test loss: 0.0015632636301840346\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06912723713119824\n",
      "Average test loss: 0.0016879833593136735\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0643503488070435\n",
      "Average test loss: 0.0015296464095719987\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06405119353863928\n",
      "Average test loss: 0.0015592187530257635\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06337347493569057\n",
      "Average test loss: 0.0014836279421837793\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06288839318686061\n",
      "Average test loss: 0.001498443782226079\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06256487590571244\n",
      "Average test loss: 0.0019230210112614764\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06206236879030864\n",
      "Average test loss: 0.001471971951528556\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06172410992450184\n",
      "Average test loss: 0.0016724361380976108\n",
      "Epoch 73/300\n",
      "Average training loss: 0.062009300738573074\n",
      "Average test loss: 0.0015201881448738278\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06101793738206228\n",
      "Average test loss: 0.01473081031938394\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06097263240483072\n",
      "Average test loss: 0.001473495212590529\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06061917315589057\n",
      "Average test loss: 0.0014984356602653862\n",
      "Epoch 77/300\n",
      "Average training loss: 0.2338622354037232\n",
      "Average test loss: 0.0015930410733239518\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08205051296287112\n",
      "Average test loss: 0.0015792921220676766\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07438924729161793\n",
      "Average test loss: 0.0015304232355621125\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07054737845394346\n",
      "Average test loss: 0.0015343494007570876\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0681180076930258\n",
      "Average test loss: 0.001488105671169857\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06638564440276888\n",
      "Average test loss: 0.0016081501415206327\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0650643766257498\n",
      "Average test loss: 0.0016374701168388127\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06407921158605152\n",
      "Average test loss: 0.0015582483588821357\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06324467951059341\n",
      "Average test loss: 0.0014794575753104356\n",
      "Epoch 86/300\n",
      "Average training loss: 0.062467736608452264\n",
      "Average test loss: 0.0015104669714346528\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06187857092751397\n",
      "Average test loss: 0.0015377460691767435\n",
      "Epoch 88/300\n",
      "Average training loss: 0.061696505235301125\n",
      "Average test loss: 0.0016896020130564768\n",
      "Epoch 89/300\n",
      "Average training loss: 0.061143591155608495\n",
      "Average test loss: 0.2129568602475855\n",
      "Epoch 90/300\n",
      "Average training loss: 0.060662148045168986\n",
      "Average test loss: 0.0014993749321955774\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06040736583206389\n",
      "Average test loss: 0.0016083374275929398\n",
      "Epoch 92/300\n",
      "Average training loss: 0.060183256891038685\n",
      "Average test loss: 0.0014687834301342568\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06053636651568942\n",
      "Average test loss: 0.0014690494908847743\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06165250295731756\n",
      "Average test loss: 0.001613744687082039\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06166854579581155\n",
      "Average test loss: 0.0014820482172071933\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05995704337954521\n",
      "Average test loss: 0.0015560876263512506\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0591539308461878\n",
      "Average test loss: 0.001546198157283167\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05929843811194102\n",
      "Average test loss: 0.0015136821155643297\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05866624250014623\n",
      "Average test loss: 0.0014601685740053653\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05875773839155833\n",
      "Average test loss: 0.00622863981852101\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05872864234778616\n",
      "Average test loss: 0.0017779946559005313\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05826280233595106\n",
      "Average test loss: 0.0018594023030665185\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05791952417294184\n",
      "Average test loss: 0.001477550984877679\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05790596565935347\n",
      "Average test loss: 0.02818455076507396\n",
      "Epoch 105/300\n",
      "Average training loss: 0.057764735258287854\n",
      "Average test loss: 0.0059563222196367055\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0574168793492847\n",
      "Average test loss: 0.0020028581716534165\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05741693974865807\n",
      "Average test loss: 0.0014411583958814542\n",
      "Epoch 108/300\n",
      "Average training loss: 0.056900431444247564\n",
      "Average test loss: 0.0014659490971308615\n",
      "Epoch 109/300\n",
      "Average training loss: 0.059917859488063385\n",
      "Average test loss: 0.001505395730284767\n",
      "Epoch 110/300\n",
      "Average training loss: 0.056990402824348876\n",
      "Average test loss: 0.001622840738751822\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05604826412598292\n",
      "Average test loss: 0.007821979923380747\n",
      "Epoch 114/300\n",
      "Average training loss: 0.055782892127831774\n",
      "Average test loss: 0.0016046786728418536\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0827836496035258\n",
      "Average test loss: 0.0014853173116635946\n",
      "Epoch 117/300\n",
      "Average training loss: 0.058188070429695975\n",
      "Average test loss: 0.001465158731262717\n",
      "Epoch 118/300\n",
      "Average training loss: 0.056625994635952845\n",
      "Average test loss: 1783086471.2817779\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05604055834478802\n",
      "Average test loss: 0.0014686361847238408\n",
      "Epoch 120/300\n",
      "Average training loss: 0.055496718304024806\n",
      "Average test loss: 0.001446921528213554\n",
      "Epoch 121/300\n",
      "Average training loss: 0.055615932411617705\n",
      "Average test loss: 0.0014917847176806793\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05532905088861784\n",
      "Average test loss: 0.0015344894791228904\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05524629076984194\n",
      "Average test loss: 0.0014319361370677749\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0550981569521957\n",
      "Average test loss: 0.0019651964919434653\n",
      "Epoch 126/300\n",
      "Average training loss: 0.054778361356920664\n",
      "Average test loss: 0.009864223825848764\n",
      "Epoch 127/300\n",
      "Average training loss: 0.054907191809680726\n",
      "Average test loss: 0.001474868037737906\n",
      "Epoch 128/300\n",
      "Average training loss: 0.054608519832293194\n",
      "Average test loss: 0.0015492946079207791\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05455797522266706\n",
      "Average test loss: 0.001441637606256538\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05992279875940747\n",
      "Average test loss: 0.0014581265692702597\n",
      "Epoch 131/300\n",
      "Average training loss: 4.629793441752593\n",
      "Average test loss: 0.004288279798088802\n",
      "Epoch 132/300\n",
      "Average training loss: 0.6865668551392026\n",
      "Average test loss: 0.002045078439327578\n",
      "Epoch 133/300\n",
      "Average training loss: 0.26216276931762694\n",
      "Average test loss: 0.0016750004855501982\n",
      "Epoch 135/300\n",
      "Average training loss: 0.2028411881128947\n",
      "Average test loss: 0.0016066371752983994\n",
      "Epoch 136/300\n",
      "Average training loss: 0.16382321027914684\n",
      "Average test loss: 0.0016132538637353313\n",
      "Epoch 137/300\n",
      "Average training loss: 0.13689883889092339\n",
      "Average test loss: 0.0015527759501193132\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11732690186633005\n",
      "Average test loss: 0.0015567632177844644\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10367448231246737\n",
      "Average test loss: 0.001521141972185837\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09354718269242181\n",
      "Average test loss: 0.0014981230127935608\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08594249595204989\n",
      "Average test loss: 0.001478683084042536\n",
      "Epoch 142/300\n",
      "Average test loss: 0.0014749481042640076\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07560532698366378\n",
      "Average test loss: 0.04682924588976635\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07414271104998059\n",
      "Average test loss: 0.0014573252389931844\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07013686461581124\n",
      "Average test loss: 0.001454819792467687\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0683120818734169\n",
      "Average test loss: 0.0015745846919922365\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06669843030969302\n",
      "Average test loss: 0.001468233641443981\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06528984623816278\n",
      "Average test loss: 0.001506721304833061\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06421764186024666\n",
      "Average test loss: 0.0014664227501489221\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06312875220179558\n",
      "Average test loss: 0.0014459720705118444\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06106504424413045\n",
      "Average test loss: 0.0014338510797048609\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06026700282428\n",
      "Average test loss: 0.0017242247698207695\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05953089148468441\n",
      "Average test loss: 0.001635120316512055\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05899449466665586\n",
      "Average test loss: 0.3457417286278473\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05833869920836555\n",
      "Average test loss: 0.0014681748900459044\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0580229275657071\n",
      "Average test loss: 0.0014287964350336957\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05753253517548243\n",
      "Average test loss: 0.0015984071847051382\n",
      "Epoch 159/300\n",
      "Average training loss: 0.057606043106979796\n",
      "Average test loss: 0.0014314777061550153\n",
      "Epoch 160/300\n",
      "Average training loss: 0.057178983787695564\n",
      "Average test loss: 0.001486572827005552\n",
      "Epoch 161/300\n",
      "Average training loss: 0.056385864552524353\n",
      "Average test loss: 0.0068354314155876636\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05596201897329754\n",
      "Average test loss: 0.0014361224844016962\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05552294871211052\n",
      "Average test loss: 0.001520222898055282\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05536654772029983\n",
      "Average test loss: 0.001466290166394578\n",
      "Epoch 165/300\n",
      "Average training loss: 0.055264353454113006\n",
      "Average test loss: 0.0015940987773550054\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0551569074789683\n",
      "Average test loss: 0.0014594145017779536\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05464421796136432\n",
      "Average test loss: 0.0014424965276072423\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05461808403333028\n",
      "Average test loss: 0.0067343200846678684\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05449579128623009\n",
      "Average test loss: 0.0014744202733143336\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05403030961420801\n",
      "Average test loss: 0.001443930185192989\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05387966524892383\n",
      "Average test loss: 0.0014570014729268022\n",
      "Epoch 174/300\n",
      "Average training loss: 0.15626020615299543\n",
      "Average test loss: 0.0018185538096974294\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07002929674916797\n",
      "Average test loss: 0.00163318881050994\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06399963079889616\n",
      "Average test loss: 0.0014638278983636862\n",
      "Epoch 177/300\n",
      "Average training loss: 0.061224571873744325\n",
      "Average test loss: 0.0016409154862372412\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05916586715976397\n",
      "Average test loss: 0.001512186742387712\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05783031859993935\n",
      "Average test loss: 0.001487038541585207\n",
      "Epoch 180/300\n",
      "Average training loss: 0.056935603188143835\n",
      "Average test loss: 0.8713309843602279\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05580396130681038\n",
      "Average test loss: 0.0014544140235003497\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05527085233728091\n",
      "Average test loss: 0.0014405294500498308\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05491834132538902\n",
      "Average test loss: 0.0034314371823436682\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05451962928970655\n",
      "Average test loss: 0.001467850596850945\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05403737546669112\n",
      "Average test loss: 0.0014419853063300252\n",
      "Epoch 186/300\n",
      "Average training loss: 0.054480668087800346\n",
      "Average test loss: 0.0020949551736315093\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05366267291870382\n",
      "Average test loss: 0.010367775164130661\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05337727642390463\n",
      "Average test loss: 0.0014489049445837736\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05472096344166332\n",
      "Average test loss: 0.001950914072079791\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05317699794636832\n",
      "Average test loss: 0.006808616053002576\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06031437592705091\n",
      "Average test loss: 0.0014496896138621702\n",
      "Epoch 192/300\n",
      "Average training loss: 0.053367163396543925\n",
      "Average test loss: 0.0015192776614179214\n",
      "Epoch 193/300\n",
      "Average training loss: 0.052963260254926155\n",
      "Average test loss: 0.0014767323210835457\n",
      "Epoch 194/300\n",
      "Average training loss: 0.052697852896319494\n",
      "Average test loss: 0.0014670324087556865\n",
      "Epoch 195/300\n",
      "Average training loss: 0.052784505476554236\n",
      "Average test loss: 0.0014619161674959793\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05300024492541949\n",
      "Average test loss: 0.013475200504478481\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05283591292964088\n",
      "Average test loss: 85.85426578436957\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05319292512204912\n",
      "Average test loss: 0.001689344864183416\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05234477340512805\n",
      "Average test loss: 0.001666136721873449\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05240938190288014\n",
      "Average test loss: 0.0014902810185319847\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05363729287849532\n",
      "Average test loss: 0.0014762199692842032\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05198347559571266\n",
      "Average test loss: 155.1718245815701\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05330538539422883\n",
      "Average test loss: 0.0015597699485305283\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05174512618117862\n",
      "Average test loss: 0.0028962295584173668\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05216089775164922\n",
      "Average test loss: 0.004364677407054438\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05160449393424723\n",
      "Average test loss: 0.0016452113236818048\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05219167258342107\n",
      "Average test loss: 0.022293180697908005\n",
      "Epoch 208/300\n",
      "Average training loss: 0.051466109855307474\n",
      "Average test loss: 0.0014788534842017625\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0550823366675112\n",
      "Average test loss: 0.0015752450374679433\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05627267687519391\n",
      "Average test loss: 0.0014842224830968513\n",
      "Epoch 211/300\n",
      "Average training loss: 0.051951107531785966\n",
      "Average test loss: 0.0015166734241776995\n",
      "Epoch 212/300\n",
      "Average training loss: 0.051651263068119686\n",
      "Average test loss: 0.001495745306317177\n",
      "Epoch 213/300\n",
      "Average training loss: 0.052196305132574505\n",
      "Average test loss: 0.0016515870568239027\n",
      "Epoch 214/300\n",
      "Average training loss: 0.051002935104899935\n",
      "Average test loss: 0.0014569738362398413\n",
      "Epoch 215/300\n",
      "Average training loss: 0.051997822599278556\n",
      "Average test loss: 0.1244869117240111\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05141305597292052\n",
      "Average test loss: 0.0015123982619908122\n",
      "Epoch 217/300\n",
      "Average training loss: 0.050970517714818316\n",
      "Average test loss: 0.07201138792352543\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05081476882431242\n",
      "Average test loss: 0.0015194126738028394\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05109364352292485\n",
      "Average test loss: 0.001544034546862046\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05087475108769205\n",
      "Average test loss: 0.001587963483399815\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05424922460317612\n",
      "Average test loss: 0.0015137803777017526\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05100614779194196\n",
      "Average test loss: 0.0015072658222375645\n",
      "Epoch 223/300\n",
      "Average training loss: 0.050476407067643274\n",
      "Average test loss: 0.0018678223991559612\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07333665686845779\n",
      "Average test loss: 0.18694671603457796\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06309050658345222\n",
      "Average test loss: 0.0014638517464304135\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05476362089978324\n",
      "Average test loss: 0.001589877472155624\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05235851403739717\n",
      "Average test loss: 0.001693512116248409\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05127072399523523\n",
      "Average test loss: 0.001455423886473808\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05144438078668383\n",
      "Average test loss: 0.0024135743369244865\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05046531579891841\n",
      "Average test loss: 0.003936666020916568\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05024229480160607\n",
      "Average test loss: 0.0015426849457952712\n",
      "Epoch 234/300\n",
      "Average training loss: 0.050367503934436376\n",
      "Average test loss: 0.0015250061622096433\n",
      "Epoch 235/300\n",
      "Average training loss: 0.050833289864990444\n",
      "Average test loss: 0.0015385868603156672\n",
      "Epoch 236/300\n",
      "Average training loss: 0.050279862980047864\n",
      "Average test loss: 0.004863841332288252\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05022115811043316\n",
      "Average test loss: 0.23000331954161327\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05067137655946943\n",
      "Average test loss: 0.0015085135833877656\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0500668896469805\n",
      "Average test loss: 0.0015112841803477043\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0500505248978734\n",
      "Average test loss: 0.0015604598690859145\n",
      "Epoch 243/300\n",
      "Average training loss: 0.050098475893338525\n",
      "Average test loss: 0.009269003277230594\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05010839774873522\n",
      "Average test loss: 0.0015049398915273034\n",
      "Epoch 245/300\n",
      "Average training loss: 0.050373695035775504\n",
      "Average test loss: 0.037302236437797544\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05044670981831021\n",
      "Average test loss: 0.0015539301815960143\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04957079181406233\n",
      "Average test loss: 0.0014976444906658597\n",
      "Epoch 248/300\n",
      "Average training loss: 0.050318104479047984\n",
      "Average test loss: 0.005720707268350654\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05106523333324326\n",
      "Average test loss: 0.0014866950493305922\n",
      "Epoch 251/300\n",
      "Average training loss: 0.050468778471151984\n",
      "Average test loss: 0.0014885482314146228\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04999591396914588\n",
      "Average test loss: 0.0015331411155768567\n",
      "Epoch 253/300\n",
      "Average training loss: 0.049842511607540975\n",
      "Average test loss: 0.0016655117999762297\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08039288912879096\n",
      "Average test loss: 0.0023067252687695955\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07474753172530069\n",
      "Average test loss: 0.0014757579684050548\n",
      "Epoch 256/300\n",
      "Average training loss: 0.058149232967032324\n",
      "Average test loss: 0.001480629354611867\n",
      "Epoch 257/300\n",
      "Average training loss: 0.054672488838434216\n",
      "Average test loss: 0.0014639797063751353\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05162129307455487\n",
      "Average test loss: 0.0015331672480743792\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05039155720836586\n",
      "Average test loss: 0.3561680622895559\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04999453741643164\n",
      "Average test loss: 0.0019671514321946437\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04954247527983453\n",
      "Average test loss: 0.024411855213137136\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05732242685225275\n",
      "Average test loss: 0.001488909611892369\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04924443671438429\n",
      "Average test loss: 0.001508808041198386\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04920113677779833\n",
      "Average test loss: 0.0014999514232493108\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05007734616597494\n",
      "Average test loss: 0.0014829944856464862\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0490841420756446\n",
      "Average test loss: 0.0015003739946211378\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04911383932166629\n",
      "Average test loss: 0.0015094390312830608\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05045504367020395\n",
      "Average test loss: 0.0018029945043640005\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04896299099259906\n",
      "Average test loss: 0.0015257550703568592\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04908225342300203\n",
      "Average test loss: 0.0015527797121968534\n",
      "Epoch 272/300\n",
      "Average training loss: 0.049413528386089534\n",
      "Average test loss: 0.012159706774685117\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0488497618039449\n",
      "Average test loss: 0.0015015207324177028\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04958831823203299\n",
      "Average test loss: 0.0015363998751466473\n",
      "Epoch 275/300\n",
      "Average training loss: 0.049024304224385154\n",
      "Average test loss: 0.0015257073392470678\n",
      "Epoch 276/300\n",
      "Average training loss: 0.050881304691235224\n",
      "Average test loss: 0.0015375046133995055\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0488084557056427\n",
      "Average test loss: 0.0015159754539943403\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04962512940996223\n",
      "Average test loss: 0.0015232050543030104\n",
      "Epoch 279/300\n",
      "Average training loss: 0.049382764061292016\n",
      "Average test loss: 0.004440412814418475\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04885780426197582\n",
      "Average test loss: 0.0017391417187949021\n",
      "Epoch 281/300\n",
      "Average training loss: 0.049363238841295246\n",
      "Average test loss: 0.002133177980677121\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04865674831800991\n",
      "Average test loss: 0.0015104927787971166\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0491826869878504\n",
      "Average test loss: 0.003513166849501431\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04866497527559598\n",
      "Average test loss: 0.0015264499648991558\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04915468649069468\n",
      "Average test loss: 0.0015752785028889775\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0507957195672724\n",
      "Average test loss: 0.0015202404192338388\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04839525995651881\n",
      "Average test loss: 0.0973229576614168\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04835295038421949\n",
      "Average test loss: 0.0015535559059224194\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04861300002204047\n",
      "Average test loss: 0.0019168121553957461\n",
      "Epoch 290/300\n",
      "Average training loss: 0.049147164205710095\n",
      "Average test loss: 0.0015360303657750288\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04887609166238043\n",
      "Average test loss: 0.0022858008622295327\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04841022154688835\n",
      "Average test loss: 0.001639472503732476\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04825740042659971\n",
      "Average test loss: 0.364662063340346\n",
      "Epoch 295/300\n",
      "Average training loss: 0.048857286711533866\n",
      "Average test loss: 7.320932503382365\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04835107355647617\n",
      "Average test loss: 0.0015456657002473043\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05082596426539951\n",
      "Average test loss: 0.0015267792600724432\n",
      "Epoch 298/300\n",
      "Average training loss: 0.048296780937247805\n",
      "Average test loss: 0.0015978701406468947\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04796962300605244\n",
      "Average test loss: 0.009592088624835014\n",
      "Epoch 300/300\n",
      "Average training loss: 0.048368721349371806\n",
      "Average test loss: 0.0016453878646716475\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 19496.87969850413\n",
      "Average test loss: 91.40679935287436\n",
      "Epoch 2/300\n",
      "Average training loss: 9.483237672593859\n",
      "Average test loss: 26.909597804281447\n",
      "Epoch 3/300\n",
      "Average training loss: 9.054008624606663\n",
      "Average test loss: 1.4552894841449129\n",
      "Epoch 5/300\n",
      "Average training loss: 9.668232917785645\n",
      "Average test loss: 4254.944965998769\n",
      "Epoch 6/300\n",
      "Average training loss: 10.26180774943034\n",
      "Average test loss: 188.44912168483603\n",
      "Epoch 7/300\n",
      "Average training loss: 9.538779093424479\n",
      "Average test loss: 0.008592169721093442\n",
      "Epoch 8/300\n",
      "Average training loss: 8.257371695624457\n",
      "Average test loss: 0.007461470146146085\n",
      "Epoch 9/300\n",
      "Average training loss: 7.2497199507819285\n",
      "Average test loss: 31.483926897565524\n",
      "Epoch 10/300\n",
      "Average training loss: 6.9684507865905765\n",
      "Average test loss: 0.005771278612315655\n",
      "Epoch 11/300\n",
      "Average training loss: 6.260049901326497\n",
      "Average test loss: 0.03653177865718802\n",
      "Epoch 13/300\n",
      "Average training loss: 5.713872695075141\n",
      "Average test loss: 0.05018481588198079\n",
      "Epoch 14/300\n",
      "Average training loss: 5.592692588382297\n",
      "Average test loss: 1.717218331027776\n",
      "Epoch 15/300\n",
      "Average training loss: 5.221171516418457\n",
      "Average test loss: 0.011470639034277863\n",
      "Epoch 16/300\n",
      "Average training loss: 4.689933137257894\n",
      "Average test loss: 36.84058557007048\n",
      "Epoch 17/300\n",
      "Average training loss: 4.323587931315104\n",
      "Average test loss: 0.003464999014097783\n",
      "Epoch 18/300\n",
      "Average training loss: 3.8537894547780356\n",
      "Average test loss: 0.003288077621617251\n",
      "Epoch 19/300\n",
      "Average training loss: 3.246863397386339\n",
      "Average test loss: 0.0034869210018465915\n",
      "Epoch 20/300\n",
      "Average training loss: 2.45853800561693\n",
      "Average test loss: 0.0028677910102738275\n",
      "Epoch 22/300\n",
      "Average training loss: 2.1800505441029867\n",
      "Average test loss: 0.003097076800134447\n",
      "Epoch 23/300\n",
      "Average training loss: 1.6671822210947673\n",
      "Average test loss: 0.002636130611722668\n",
      "Epoch 25/300\n",
      "Average training loss: 1.465226539823744\n",
      "Average test loss: 0.0024761672469062936\n",
      "Epoch 26/300\n",
      "Average training loss: 1.2867719917297362\n",
      "Average test loss: 0.0028691545594483614\n",
      "Epoch 27/300\n",
      "Average training loss: 1.1230319485134548\n",
      "Average test loss: 0.0023032347035283843\n",
      "Epoch 28/300\n",
      "Average training loss: 0.9739568428463407\n",
      "Average test loss: 0.0022518942598253487\n",
      "Epoch 29/300\n",
      "Average training loss: 0.844873013443417\n",
      "Average test loss: 0.0021565663483407763\n",
      "Epoch 30/300\n",
      "Average training loss: 0.7323036283916897\n",
      "Average test loss: 0.002306100045848224\n",
      "Epoch 31/300\n",
      "Average training loss: 0.6348745970196195\n",
      "Average test loss: 0.002104271378989021\n",
      "Epoch 32/300\n",
      "Average training loss: 0.5526707283125983\n",
      "Average test loss: 0.0020618768046713537\n",
      "Epoch 33/300\n",
      "Average training loss: 0.48266812390751307\n",
      "Average test loss: 0.002041053590882156\n",
      "Epoch 34/300\n",
      "Average training loss: 0.42047160432073805\n",
      "Average test loss: 0.001886593974298901\n",
      "Epoch 35/300\n",
      "Average training loss: 0.36605493529637656\n",
      "Average test loss: 0.0025217177929977574\n",
      "Epoch 36/300\n",
      "Average training loss: 0.3197384301821391\n",
      "Average test loss: 0.0018402656968683005\n",
      "Epoch 37/300\n",
      "Average training loss: 0.28027515202098424\n",
      "Average test loss: 0.002518127854085631\n",
      "Epoch 38/300\n",
      "Average training loss: 0.24710560647646587\n",
      "Average test loss: 0.0017843170696869492\n",
      "Epoch 39/300\n",
      "Average training loss: 0.2184033232000139\n",
      "Average test loss: 0.001667349718304144\n",
      "Epoch 40/300\n",
      "Average training loss: 0.19502784053484598\n",
      "Average test loss: 0.0016364782748536932\n",
      "Epoch 41/300\n",
      "Average training loss: 0.17647283306386735\n",
      "Average test loss: 0.0016311697762252555\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1606450286573834\n",
      "Average test loss: 0.0017238752328687245\n",
      "Epoch 43/300\n",
      "Average training loss: 0.14799279542764027\n",
      "Average test loss: 0.0015568104728849398\n",
      "Epoch 44/300\n",
      "Average training loss: 0.13701422439018884\n",
      "Average test loss: 0.0015172240101835794\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12639974150392744\n",
      "Average test loss: 0.001452186934546464\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11904282255636321\n",
      "Average test loss: 0.0016806862314438654\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10734990549087524\n",
      "Average test loss: 0.0058417381224119\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11744263356261783\n",
      "Average test loss: 0.0019275720765193304\n",
      "Epoch 50/300\n",
      "Average training loss: 0.6143396081659529\n",
      "Average test loss: 0.001990088408191999\n",
      "Epoch 51/300\n",
      "Average training loss: 0.1311110965543323\n",
      "Average test loss: 0.0016577944849203857\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11508616930908627\n",
      "Average test loss: 0.0018544874065038232\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0998800024787585\n",
      "Average test loss: 0.0014905204662225312\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09555284037854936\n",
      "Average test loss: 0.0017038372508250176\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09022158002853393\n",
      "Average test loss: 0.0014785858991866311\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08654876066247622\n",
      "Average test loss: 0.001496140828459627\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08249192054404153\n",
      "Average test loss: 0.0015429269860809049\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08026147811280357\n",
      "Average test loss: 0.0014032821390363905\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0771992473800977\n",
      "Average test loss: 0.005800565110726489\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07510523165265719\n",
      "Average test loss: 0.001542178069655266\n",
      "Epoch 63/300\n",
      "Average training loss: 0.5462269750237465\n",
      "Average test loss: 0.002066613444644544\n",
      "Epoch 64/300\n",
      "Average training loss: 0.14731476567188898\n",
      "Average test loss: 0.001633154361198346\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11373750882678561\n",
      "Average test loss: 0.0015817315085894532\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10243903810448117\n",
      "Average test loss: 0.0014493496996454067\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09552369923724069\n",
      "Average test loss: 0.0015093990364629362\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09039894634485245\n",
      "Average test loss: 0.0020351555726180475\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08593458980321884\n",
      "Average test loss: 0.001477206487622526\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07866270133521822\n",
      "Average test loss: 0.0014823599220770928\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07675350859430101\n",
      "Average test loss: 0.001346329219846262\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07472254811392891\n",
      "Average test loss: 0.0012930343555700447\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07192580287986332\n",
      "Average test loss: 0.0016737474134812752\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0705186698767874\n",
      "Average test loss: 0.0012438675175007019\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06936858104997211\n",
      "Average test loss: 0.001284282133500609\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0716230541533894\n",
      "Average test loss: 0.001341252502416157\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06685933163099819\n",
      "Average test loss: 0.001657818419030971\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06269641794429885\n",
      "Average test loss: 0.005522852229989237\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06180681424670749\n",
      "Average test loss: 0.001203373794288685\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05984648411803775\n",
      "Average test loss: 0.003196023051937421\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0576722020275063\n",
      "Average test loss: 0.011924028961608808\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05738538233439128\n",
      "Average test loss: 0.001150856184773147\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05622948480645815\n",
      "Average test loss: 0.0011062404139795237\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05448281059662501\n",
      "Average test loss: 0.001116753358600868\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05348943867120478\n",
      "Average test loss: 0.4104074495534102\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05309352948268255\n",
      "Average test loss: 0.001166369576421049\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0566333774526914\n",
      "Average test loss: 0.0010852751938833132\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05039470714330673\n",
      "Average test loss: 0.001092703744613876\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04952527426348792\n",
      "Average test loss: 0.001080804470874783\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04908383727404806\n",
      "Average test loss: 0.0010668387589976192\n",
      "Epoch 95/300\n",
      "Average training loss: 0.048351810654004415\n",
      "Average test loss: 0.0017016792442235682\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04803629611598121\n",
      "Average test loss: 0.0010911458743115266\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04790906874669923\n",
      "Average test loss: 0.0010891580644788014\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04681394667757882\n",
      "Average test loss: 0.0010274749479463531\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04646187873019113\n",
      "Average test loss: 0.0010477159216793048\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04547936680250698\n",
      "Average test loss: 0.001509803593572643\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04513101955254872\n",
      "Average test loss: 0.004718966613213221\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04513597856958707\n",
      "Average test loss: 0.0010051074949507083\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04436823981338077\n",
      "Average test loss: 0.0011050501660857764\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04438651266031795\n",
      "Average test loss: 0.001066196599509567\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04625305507249303\n",
      "Average test loss: 0.0010189916543248626\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04374598025613361\n",
      "Average test loss: 0.0009944463265128434\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04350732429491149\n",
      "Average test loss: 0.0010450928457495239\n",
      "Epoch 109/300\n",
      "Average training loss: 0.043333214541276296\n",
      "Average training loss: 0.04294277587367429\n",
      "Average test loss: 0.0010060958401817415\n",
      "Epoch 111/300\n",
      "Average training loss: 0.043601387987534206\n",
      "Average test loss: 0.0010191947712252538\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04255377391642994\n",
      "Average test loss: 0.0009804572083780335\n",
      "Epoch 113/300\n",
      "Average training loss: 0.042420987135834164\n",
      "Average test loss: 0.0009888537974200315\n",
      "Epoch 114/300\n",
      "Average training loss: 0.042498726463980147\n",
      "Average test loss: 0.0009784292326205308\n",
      "Epoch 115/300\n",
      "Average training loss: 0.041997016893492806\n",
      "Average test loss: 0.002352985305711627\n",
      "Epoch 116/300\n",
      "Average training loss: 0.042101049015919365\n",
      "Average test loss: 0.0009721895082750254\n",
      "Epoch 117/300\n",
      "Average training loss: 0.042233837015098995\n",
      "Average test loss: 0.0009895457306669819\n",
      "Epoch 118/300\n",
      "Average training loss: 0.041501840588119294\n",
      "Average test loss: 0.0009748345238363577\n",
      "Epoch 120/300\n",
      "Average training loss: 0.041263463258743284\n",
      "Average test loss: 0.001827448770403862\n",
      "Epoch 121/300\n",
      "Average training loss: 0.042282387442058984\n",
      "Average test loss: 0.000975416080870976\n",
      "Epoch 122/300\n",
      "Average training loss: 0.041262461102671094\n",
      "Average test loss: 0.001000843844531725\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04101451219121615\n",
      "Average test loss: 0.001036380380495555\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04094790143436856\n",
      "Average test loss: 0.005362386288328303\n",
      "Epoch 125/300\n",
      "Average training loss: 0.040883854779932234\n",
      "Average test loss: 0.0009873962841358865\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0407603927453359\n",
      "Average test loss: 0.0009878455992374155\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04182632421122657\n",
      "Average test loss: 0.0009684476792398426\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04039487144185437\n",
      "Average test loss: 0.002123124188950492\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04069093038307296\n",
      "Average test loss: 0.0018558370353033145\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04080663444598516\n",
      "Average test loss: 0.0010728807866366374\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04013287987973955\n",
      "Average test loss: 0.0009890454686764215\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04030263135499424\n",
      "Average test loss: 0.0009992539508061277\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04000277683304416\n",
      "Average test loss: 0.0010040307086375024\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03982911867565579\n",
      "Average test loss: 0.0009573336649272177\n",
      "Epoch 136/300\n",
      "Average training loss: 0.039575105640623304\n",
      "Average test loss: 0.001246195209109121\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03993470757868555\n",
      "Average test loss: 0.0009813725728955534\n",
      "Epoch 139/300\n",
      "Average training loss: 0.039610649827453825\n",
      "Average test loss: 0.0009656978817656636\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0396339104887512\n",
      "Average test loss: 0.0013351693263070452\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05719878804021412\n",
      "Average test loss: 0.0009933068567059106\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04147132981154654\n",
      "Average test loss: 0.0009750346253212127\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04022730065716638\n",
      "Average test loss: 0.0009609858234309488\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04172919629679786\n",
      "Average test loss: 0.0009975746336082618\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03981693834066391\n",
      "Average test loss: 0.001055541707875414\n",
      "Epoch 146/300\n",
      "Average training loss: 0.039327113623420396\n",
      "Average test loss: 0.0009809489583907029\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04017863382895787\n",
      "Average test loss: 0.0010154575774860052\n",
      "Epoch 149/300\n",
      "Average training loss: 0.039063661760754056\n",
      "Average test loss: 0.0011514157557653056\n",
      "Epoch 150/300\n",
      "Average training loss: 0.038906782918506196\n",
      "Average test loss: 0.000957677437716888\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03980272243089146\n",
      "Average test loss: 0.0009586891810823646\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03906226097544034\n",
      "Average test loss: 0.0010686845001247195\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03886132093601757\n",
      "Average test loss: 0.0010170886007965438\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03894030159049564\n",
      "Average test loss: 0.0011728577594686714\n",
      "Epoch 155/300\n",
      "Average training loss: 0.039413077311383354\n",
      "Average test loss: 0.0010201731523395412\n",
      "Epoch 156/300\n",
      "Average training loss: 0.038647655616203945\n",
      "Average test loss: 0.0011489207272728284\n",
      "Epoch 157/300\n",
      "Average training loss: 0.039172964872585404\n",
      "Average test loss: 0.0010236245203349326\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03858255932562881\n",
      "Average test loss: 0.0013377718734037545\n",
      "Epoch 159/300\n",
      "Average training loss: 0.038658366658621365\n",
      "Average test loss: 0.0010295074839765826\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03835330901874436\n",
      "Average test loss: 0.09805718677077029\n",
      "Epoch 161/300\n",
      "Average training loss: 0.038420770072274735\n",
      "Average test loss: 0.0010771120379471945\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0380555026796129\n",
      "Average test loss: 0.0010092691761545008\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03807264568739467\n",
      "Average test loss: 0.0009865081932188735\n",
      "Epoch 165/300\n",
      "Average training loss: 0.038845883296595676\n",
      "Average test loss: 0.0009804750808204213\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04122124601735009\n",
      "Average test loss: 0.0011915198744585117\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04250483192337884\n",
      "Average test loss: 0.0011004830301842756\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03796848734551006\n",
      "Average test loss: 0.0009620438411624895\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0377506716714965\n",
      "Average test loss: 0.0010314656407054929\n",
      "Epoch 170/300\n",
      "Average training loss: 0.037814735660950345\n",
      "Average test loss: 0.000990481805935916\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03882286916342047\n",
      "Average test loss: 0.0009815894016582105\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03785542734629578\n",
      "Average test loss: 0.0009666275326679978\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03762704783346918\n",
      "Average test loss: 0.0030161610233287017\n",
      "Epoch 174/300\n",
      "Average training loss: 0.038021457393964134\n",
      "Average test loss: 0.0014848926688234012\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03744458692603641\n",
      "Average test loss: 0.0009664071501853565\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04177844382656945\n",
      "Average test loss: 0.0015983206970203254\n",
      "Epoch 177/300\n",
      "Average training loss: 0.038681394384966955\n",
      "Average test loss: 0.00101518310730656\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03778701433870527\n",
      "Average test loss: 0.0009750492742492092\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03754192107584741\n",
      "Average test loss: 0.0009914073097623058\n",
      "Epoch 180/300\n",
      "Average training loss: 0.037389628067612646\n",
      "Average test loss: 0.000998364220166372\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03727864280011919\n",
      "Average test loss: 0.0009655381976109412\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03901928266220622\n",
      "Average test loss: 0.0009709562588379615\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03720662941535314\n",
      "Average test loss: 0.0010296892526352572\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03707936652501424\n",
      "Average test loss: 0.0009953548293560744\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03750239028202163\n",
      "Average test loss: 0.0009741289381765657\n",
      "Epoch 186/300\n",
      "Average training loss: 0.037369945065842736\n",
      "Average test loss: 0.0010792862683431143\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03836118741499053\n",
      "Average test loss: 0.000987801168838309\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03698666633334425\n",
      "Average test loss: 0.0010022320327245527\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03715850194791953\n",
      "Average test loss: 0.001018974701511777\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03703312842051188\n",
      "Average test loss: 0.0009751608724084994\n",
      "Epoch 191/300\n",
      "Average training loss: 0.037474045235249734\n",
      "Average test loss: 0.0011041521274277734\n",
      "Epoch 192/300\n",
      "Average training loss: 0.036980520086155995\n",
      "Average test loss: 0.0010135756374026338\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03684267803033193\n",
      "Average test loss: 0.000989961004246854\n",
      "Epoch 194/300\n",
      "Average training loss: 0.038169712025258276\n",
      "Average test loss: 0.0010132630600904425\n",
      "Epoch 195/300\n",
      "Average training loss: 0.037247925308015614\n",
      "Average test loss: 0.001006887898211264\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0366346647457944\n",
      "Average test loss: 0.001014002373545534\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04219392559594578\n",
      "Average test loss: 0.0009688773573272758\n",
      "Epoch 198/300\n",
      "Average training loss: 0.037088046842151216\n",
      "Average test loss: 0.0010001507896619539\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03665152347750134\n",
      "Average test loss: 0.0010399047420877549\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03647421737180816\n",
      "Average test loss: 0.002747888697518243\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06082782251967324\n",
      "Average test loss: 0.0009754388040552537\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04040972567598025\n",
      "Average test loss: 0.0011278610164299607\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03837022782034344\n",
      "Average test loss: 0.0009989498556177649\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03724276261528333\n",
      "Average test loss: 0.017907346702491243\n",
      "Epoch 205/300\n",
      "Average training loss: 0.036861405468649334\n",
      "Average test loss: 0.0009828878798159874\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03663284366991785\n",
      "Average test loss: 0.0009842980918733197\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03638426432841354\n",
      "Average test loss: 0.001216701503118707\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03717597868541876\n",
      "Average test loss: 0.0009745970762645205\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03747781708505418\n",
      "Average test loss: 0.0009904467798769474\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03631317041731543\n",
      "Average test loss: 0.0012417962226188846\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03653117025229666\n",
      "Average test loss: 0.0010037777938673065\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03635517282783985\n",
      "Average test loss: 0.0009999711783602835\n",
      "Epoch 213/300\n",
      "Average training loss: 0.036168251120381884\n",
      "Average test loss: 0.0014023022968322038\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03635510089165635\n",
      "Average test loss: 0.000983535296542363\n",
      "Epoch 215/300\n",
      "Average training loss: 0.036179242610931396\n",
      "Average test loss: 0.0009949201265246504\n",
      "Epoch 216/300\n",
      "Average training loss: 0.036240420977274576\n",
      "Average test loss: 0.0010638175098121995\n",
      "Epoch 217/300\n",
      "Average training loss: 0.036323383500178656\n",
      "Average test loss: 0.0010124646480609146\n",
      "Epoch 218/300\n",
      "Average training loss: 0.036618262191613515\n",
      "Average test loss: 0.0010658132426647677\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03609474849700928\n",
      "Average test loss: 0.001005155624407861\n",
      "Epoch 220/300\n",
      "Average training loss: 0.037049875730441675\n",
      "Average test loss: 0.0012400647509429189\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0360813798904419\n",
      "Average test loss: 0.0010271637050124505\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03578359446260664\n",
      "Average test loss: 0.0009911923588563998\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03575395225485166\n",
      "Average test loss: 0.0010485058637439376\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03824158243338267\n",
      "Average test loss: 0.0010618093027215864\n",
      "Epoch 225/300\n",
      "Average training loss: 0.036641909488373335\n",
      "Average test loss: 0.0012579770822905832\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03574692173798879\n",
      "Average test loss: 0.0009827622034483485\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03568083291914728\n",
      "Average test loss: 0.0010110932524419493\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03597263503405783\n",
      "Average test loss: 0.0010177792327271567\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03586805859870381\n",
      "Average test loss: 0.0010383701037822498\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03596526601248317\n",
      "Average test loss: 0.0010215220166266792\n",
      "Epoch 231/300\n",
      "Average training loss: 0.036162180331018236\n",
      "Average test loss: 0.0014260146339527436\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03591810702615314\n",
      "Average test loss: 0.0010252897115424276\n",
      "Epoch 233/300\n",
      "Average training loss: 0.035673668384552\n",
      "Average test loss: 0.0010073461216977902\n",
      "Epoch 234/300\n",
      "Average training loss: 0.035525133003791176\n",
      "Average test loss: 0.001010709131964379\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03586818047033416\n",
      "Average test loss: 0.0010041736367469032\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03573883805010054\n",
      "Average test loss: 0.0010022413773048255\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03549419731563992\n",
      "Average test loss: 0.0010994510059762332\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03572160602940454\n",
      "Average test loss: 0.0010330977860527734\n",
      "Epoch 239/300\n",
      "Average training loss: 0.035705232737792865\n",
      "Average test loss: 0.002653304631097449\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0362120285431544\n",
      "Average test loss: 0.0010338827813458113\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03524753154814243\n",
      "Average test loss: 0.001015863079060283\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03558982940846019\n",
      "Average test loss: 0.0010592165407207277\n",
      "Epoch 243/300\n",
      "Average training loss: 0.035880224665006004\n",
      "Average test loss: 0.0009915317646745179\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03522295448184013\n",
      "Average test loss: 0.0010266742915846408\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0355258564800024\n",
      "Average test loss: 0.001173714839335945\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03552516726487213\n",
      "Average test loss: 0.0012254044603970315\n",
      "Epoch 247/300\n",
      "Average training loss: 0.047941284818781744\n",
      "Average test loss: 0.001013759621522493\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03583684534496731\n",
      "Average test loss: 0.0010138885571310918\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0352317827641964\n",
      "Average test loss: 0.001195365647888846\n",
      "Epoch 250/300\n",
      "Average training loss: 0.035132693227794436\n",
      "Average test loss: 0.0015816083785094735\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03507499297625489\n",
      "Average test loss: 0.0010286932649711767\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03595741821494367\n",
      "Average test loss: 0.001014739964571264\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03556632570425669\n",
      "Average test loss: 0.0010059596892032358\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03507432619730631\n",
      "Average test loss: 0.028118276136616865\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0351931352449788\n",
      "Average test loss: 0.0010410316776380771\n",
      "Epoch 256/300\n",
      "Average training loss: 0.035226419402493374\n",
      "Average test loss: 0.001058546884968463\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04153368797567156\n",
      "Average test loss: 0.0009825656770004166\n",
      "Epoch 258/300\n",
      "Average training loss: 0.035332591235637666\n",
      "Average test loss: 0.0010046944701009326\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03517599359816975\n",
      "Average test loss: 0.001036535342844824\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03492425043384234\n",
      "Average test loss: 0.0023041796353128223\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03507012157307731\n",
      "Average test loss: 0.0010001155149398579\n",
      "Epoch 262/300\n",
      "Average training loss: 0.035955236819055346\n",
      "Average test loss: 0.0021397579920788604\n",
      "Epoch 263/300\n",
      "Average training loss: 0.034993234786722394\n",
      "Average test loss: 0.0010121892031489147\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03492543670700656\n",
      "Average test loss: 0.0010161013960217436\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03526811181505521\n",
      "Average test loss: 0.0012895147094710005\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03490174779627058\n",
      "Average test loss: 0.001019539137215664\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03556606863273515\n",
      "Average test loss: 0.0010339608587738541\n",
      "Epoch 268/300\n",
      "Average training loss: 0.037055632452170056\n",
      "Average test loss: 0.0011757117469484608\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03498363688588142\n",
      "Average test loss: 0.0010125560310358802\n",
      "Epoch 270/300\n",
      "Average training loss: 0.034786619413230156\n",
      "Average test loss: 0.001150117970796095\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03470455472005738\n",
      "Average test loss: 0.0010115523846406075\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03474291007055177\n",
      "Average test loss: 0.0010270901030033\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03599394180046187\n",
      "Average test loss: 0.0010279301568969256\n",
      "Epoch 274/300\n",
      "Average training loss: 0.034882938063806955\n",
      "Average test loss: 0.0013871404048469331\n",
      "Epoch 275/300\n",
      "Average training loss: 0.034635557706157366\n",
      "Average test loss: 0.001325788038575815\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03494527531663577\n",
      "Average test loss: 0.001216934635821316\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03464865206347571\n",
      "Average test loss: 0.001037873782398593\n",
      "Epoch 278/300\n",
      "Average training loss: 0.035882665183809066\n",
      "Average test loss: 0.0010180992679670452\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03481593417790201\n",
      "Average test loss: 0.0010100334095251228\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03553622721632322\n",
      "Average test loss: 0.0010376054719607862\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03469348382287555\n",
      "Average test loss: 0.0010793635939351386\n",
      "Epoch 282/300\n",
      "Average training loss: 0.034628968881236184\n",
      "Average test loss: 0.003395880989316437\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03480321678188112\n",
      "Average test loss: 0.0010076986844651402\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03468751647406154\n",
      "Average test loss: 0.0012666022589223253\n",
      "Epoch 286/300\n",
      "Average training loss: 0.034925830480125215\n",
      "Average test loss: 0.0010100593235757617\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03447440628376272\n",
      "Average test loss: 0.0010755169871263207\n",
      "Epoch 289/300\n",
      "Average training loss: 0.034590818617078994\n",
      "Average test loss: 0.001035768674634811\n",
      "Epoch 290/300\n",
      "Average training loss: 0.035585539412167334\n",
      "Average test loss: 0.0010301862921979692\n",
      "Epoch 291/300\n",
      "Average training loss: 0.034478037231498296\n",
      "Average test loss: 0.00105112489245625\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03430065512326028\n",
      "Average test loss: 0.0010472261021948522\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03448989690840244\n",
      "Average test loss: 0.0010345848773916561\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03504022309349643\n",
      "Average test loss: 0.0010198549155352845\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03434921018944846\n",
      "Average test loss: 0.0010635952327607407\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03492830089728038\n",
      "Average test loss: 0.0010199764379196696\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03432107146249877\n",
      "Average test loss: 0.0010462790357155932\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03477926364044348\n",
      "Average test loss: 40.51508299060994\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03435230887101756\n",
      "Average test loss: 0.0010600326353063187\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03436317454775174\n",
      "Average test loss: 0.0010422547440975905\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_80_Depth5/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 19.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.85\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.97\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.82\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.19\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.49\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.75\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.06\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.25\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.14\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.71\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.95\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.24\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.69\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.62\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.60\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.01\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.23\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.48\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.10\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.40\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.62\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.42\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.03\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.50\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.35\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.76\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.11\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.19\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.27\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.71\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.88\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.12\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.09\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.24\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.32\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.60\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.52\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.60\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.76\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.82\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.86\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.77\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.42\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.74\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.37\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 33.05\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.34\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.79\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.17\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.83\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 34.19\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.49\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.24\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 34.72\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 34.65\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 34.72\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 34.97\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 35.08\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 35.33\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 35.42\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 35.03\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 35.32\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 35.45\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 35.28\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 35.50\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 35.40\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 35.57\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 35.54\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
