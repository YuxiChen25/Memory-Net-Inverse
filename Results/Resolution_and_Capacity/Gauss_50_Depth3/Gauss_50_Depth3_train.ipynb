{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.243077735569742\n",
      "Average test loss: 0.011252073250710964\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06575884158743753\n",
      "Average test loss: 0.009707150440249179\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05932488554384973\n",
      "Average test loss: 0.009159956277244622\n",
      "Epoch 4/300\n",
      "Average training loss: 0.055785848127471074\n",
      "Average test loss: 0.008971883989042705\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05416201033525997\n",
      "Average test loss: 0.01030061347120338\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05238642845551173\n",
      "Average test loss: 0.008740933933191829\n",
      "Epoch 7/300\n",
      "Average training loss: 0.050716637449132074\n",
      "Average test loss: 0.008822181696693103\n",
      "Epoch 8/300\n",
      "Average training loss: 0.049584670543670654\n",
      "Average test loss: 0.00859532642778423\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04863446715142992\n",
      "Average test loss: 0.00816065607799424\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04787518985735045\n",
      "Average test loss: 0.008439260882635911\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04717584313948949\n",
      "Average test loss: 0.008120327681303025\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0465476248131858\n",
      "Average test loss: 0.008803093111349476\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04588757289118237\n",
      "Average test loss: 0.0077754771899845865\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04535555715693368\n",
      "Average test loss: 0.007725459780957964\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04473704729477564\n",
      "Average test loss: 0.007775866332153479\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04423376928435432\n",
      "Average test loss: 0.007549323283549812\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04374250317613284\n",
      "Average test loss: 0.0073456579604082636\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04328733463419808\n",
      "Average test loss: 0.007561926150487529\n",
      "Epoch 19/300\n",
      "Average training loss: 0.042971515407164894\n",
      "Average test loss: 0.007212195353375541\n",
      "Epoch 20/300\n",
      "Average training loss: 0.042526735769377814\n",
      "Average test loss: 0.007242505490365956\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04218529040945901\n",
      "Average test loss: 0.007104312782900201\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04186787238054805\n",
      "Average test loss: 0.007077395317869054\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04146949567397436\n",
      "Average test loss: 0.006934898553209172\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04129679783847597\n",
      "Average test loss: 0.007279629691193501\n",
      "Epoch 25/300\n",
      "Average training loss: 0.040867937301596005\n",
      "Average test loss: 0.006958021996749772\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04071913959913784\n",
      "Average test loss: 0.006831251925064458\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04043360931343502\n",
      "Average test loss: 0.006918911600278484\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04014190868867768\n",
      "Average test loss: 0.006725763145420286\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03993787083029747\n",
      "Average test loss: 0.006766851389159759\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0397201961427927\n",
      "Average test loss: 0.006905042472398943\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03953663498825497\n",
      "Average test loss: 0.006644975289909376\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03928659355309275\n",
      "Average test loss: 0.0065925331554479065\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03923617990480529\n",
      "Average test loss: 0.006609513971540663\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03892419356107712\n",
      "Average test loss: 0.006548115779128339\n",
      "Epoch 35/300\n",
      "Average training loss: 0.038803673886590534\n",
      "Average test loss: 0.006801958040230804\n",
      "Epoch 36/300\n",
      "Average training loss: 0.038650481548574235\n",
      "Average test loss: 0.006443935956392023\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03845219956172837\n",
      "Average test loss: 0.006602004856699043\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0383169243749645\n",
      "Average test loss: 0.006653532115535604\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03825564974380864\n",
      "Average test loss: 0.006494887201736371\n",
      "Epoch 40/300\n",
      "Average training loss: 0.038068547202481165\n",
      "Average test loss: 0.006433062213576503\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03804516055517727\n",
      "Average test loss: 0.006323235707978408\n",
      "Epoch 42/300\n",
      "Average training loss: 0.037788658420244856\n",
      "Average test loss: 0.006400045036855671\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03769739531808429\n",
      "Average test loss: 0.006296067423290677\n",
      "Epoch 44/300\n",
      "Average training loss: 0.037583644217915005\n",
      "Average test loss: 0.00624126661900017\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03753806363542875\n",
      "Average test loss: 0.006269068046783408\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03731898260116577\n",
      "Average test loss: 0.0062074483223259445\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03737536295255025\n",
      "Average test loss: 0.006222685099475914\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03722836109664705\n",
      "Average test loss: 0.006247634702465601\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03710364224844509\n",
      "Average test loss: 0.006147027244584428\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03695622045795123\n",
      "Average test loss: 0.006176632434957557\n",
      "Epoch 51/300\n",
      "Average training loss: 0.036870387030972375\n",
      "Average test loss: 0.006118667144742277\n",
      "Epoch 52/300\n",
      "Average training loss: 0.036874872972567876\n",
      "Average test loss: 0.006150245489345656\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03675858802596728\n",
      "Average test loss: 0.006136477396306065\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03665357709262106\n",
      "Average test loss: 0.006147616421596872\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03653763767580191\n",
      "Average test loss: 0.006238281863431136\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03653792288568285\n",
      "Average test loss: 0.00612430493781964\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03645932304528025\n",
      "Average test loss: 0.00606760675749845\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03637334864669376\n",
      "Average test loss: 0.006095419401095973\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03630849137571123\n",
      "Average test loss: 0.006082009518726004\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03620869696140289\n",
      "Average test loss: 0.006154159771485461\n",
      "Epoch 61/300\n",
      "Average training loss: 0.036141704158650506\n",
      "Average test loss: 0.006082400152666701\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03611855178409153\n",
      "Average test loss: 0.006065657494796647\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03608750476274226\n",
      "Average test loss: 0.006106394479672114\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03599499555428823\n",
      "Average test loss: 0.006093809162163072\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03593296125862334\n",
      "Average test loss: 0.006095626859201325\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0359299179746045\n",
      "Average test loss: 0.006358945615589619\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03587809690170818\n",
      "Average test loss: 0.006034554933094316\n",
      "Epoch 68/300\n",
      "Average training loss: 0.035841687977314\n",
      "Average test loss: 0.006141997834460603\n",
      "Epoch 69/300\n",
      "Average training loss: 0.036261163042651284\n",
      "Average test loss: 0.006220111275712649\n",
      "Epoch 70/300\n",
      "Average training loss: 0.035687596168782976\n",
      "Average test loss: 0.00599672836644782\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0356268221984307\n",
      "Average test loss: 0.00626040464018782\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03561556299527486\n",
      "Average test loss: 0.006111849188804626\n",
      "Epoch 73/300\n",
      "Average training loss: 0.035631646126508716\n",
      "Average test loss: 0.006260334398597479\n",
      "Epoch 74/300\n",
      "Average training loss: 0.035604239642620085\n",
      "Average test loss: 0.006121215303324991\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03558850543697675\n",
      "Average test loss: 0.006016536170823706\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03549560315575864\n",
      "Average test loss: 0.005962262029863066\n",
      "Epoch 77/300\n",
      "Average training loss: 0.035470880920688314\n",
      "Average test loss: 0.006204744511594375\n",
      "Epoch 78/300\n",
      "Average training loss: 0.035468893812762366\n",
      "Average test loss: 0.00594161073387497\n",
      "Epoch 79/300\n",
      "Average training loss: 0.035386261463165286\n",
      "Average test loss: 0.005978529004794028\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03535641987456216\n",
      "Average test loss: 0.006388744889034165\n",
      "Epoch 81/300\n",
      "Average training loss: 0.035369952430327735\n",
      "Average test loss: 0.006073197576320834\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03530339059564802\n",
      "Average test loss: 0.006048762326853143\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0352513870348533\n",
      "Average test loss: 0.006001438172327148\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03527294734451506\n",
      "Average test loss: 0.006310673741002877\n",
      "Epoch 85/300\n",
      "Average training loss: 0.035263937297794555\n",
      "Average test loss: 0.005919701375895077\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03521014564898279\n",
      "Average test loss: 0.005967950380510754\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03516728714439604\n",
      "Average test loss: 0.005958455437173446\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03511877268883917\n",
      "Average test loss: 0.006042219832125637\n",
      "Epoch 89/300\n",
      "Average training loss: 0.035116544865899615\n",
      "Average test loss: 0.00596425139469405\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0350776154200236\n",
      "Average test loss: 0.005941649388107989\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0350188498960601\n",
      "Average test loss: 0.005977780165771643\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03499501032299466\n",
      "Average test loss: 0.005864867055167754\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03502711748249001\n",
      "Average test loss: 0.005942435991432932\n",
      "Epoch 94/300\n",
      "Average training loss: 0.035023829619089765\n",
      "Average test loss: 0.006306777219598492\n",
      "Epoch 95/300\n",
      "Average training loss: 0.034943394455644816\n",
      "Average test loss: 0.005910321856538454\n",
      "Epoch 96/300\n",
      "Average training loss: 0.034921050442589655\n",
      "Average test loss: 0.005902592462384038\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0348805279135704\n",
      "Average test loss: 0.006084446115626229\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03493522315224012\n",
      "Average test loss: 0.0062272701818082065\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0349439515736368\n",
      "Average test loss: 0.00592829668853018\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03481213533216053\n",
      "Average test loss: 0.006076020061555836\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03480208054681619\n",
      "Average test loss: 0.0061282544467184275\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03479518373476134\n",
      "Average test loss: 0.006067150755474965\n",
      "Epoch 103/300\n",
      "Average training loss: 0.034773791265156534\n",
      "Average test loss: 0.006427455549024874\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03479111615485615\n",
      "Average test loss: 0.0059137783402370084\n",
      "Epoch 105/300\n",
      "Average training loss: 0.034736050976647274\n",
      "Average test loss: 0.006135905072506931\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03467940993275907\n",
      "Average test loss: 0.005972309346828196\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034705031428072186\n",
      "Average test loss: 0.006128762194265922\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03479647507601314\n",
      "Average test loss: 0.005946242533210251\n",
      "Epoch 109/300\n",
      "Average training loss: 0.034648884205354585\n",
      "Average test loss: 0.0060939538474712104\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03466176209184858\n",
      "Average test loss: 0.005860509203126033\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03462965272863706\n",
      "Average test loss: 0.006536436842133601\n",
      "Epoch 112/300\n",
      "Average training loss: 0.034572043961948816\n",
      "Average test loss: 0.0058917001560330395\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0346119001838896\n",
      "Average test loss: 0.005907956995483902\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03454788007669979\n",
      "Average test loss: 0.0064517865354816115\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03454259756207466\n",
      "Average test loss: 0.005956226541764207\n",
      "Epoch 116/300\n",
      "Average training loss: 0.034551915893952054\n",
      "Average test loss: 0.005953747321955032\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03450379038188193\n",
      "Average test loss: 0.005985784322851234\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03452636946903335\n",
      "Average test loss: 0.006522655245330598\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03445915299654007\n",
      "Average test loss: 0.006011741077320444\n",
      "Epoch 120/300\n",
      "Average training loss: 0.034410798877477645\n",
      "Average test loss: 0.00592244484813677\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03446076007021798\n",
      "Average test loss: 0.00594416661394967\n",
      "Epoch 122/300\n",
      "Average training loss: 0.034430636270178686\n",
      "Average test loss: 0.005965236559510231\n",
      "Epoch 123/300\n",
      "Average training loss: 0.034405261682139504\n",
      "Average test loss: 0.0061303440328273505\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03438231544196606\n",
      "Average test loss: 0.006265638573716084\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03438289693660206\n",
      "Average test loss: 0.006031656210621198\n",
      "Epoch 126/300\n",
      "Average training loss: 0.034544263104597725\n",
      "Average test loss: 0.0060170492861006\n",
      "Epoch 127/300\n",
      "Average training loss: 0.034347046027580895\n",
      "Average test loss: 0.006024156199561225\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03432818175355593\n",
      "Average test loss: 0.0059050145722511745\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03434101082053449\n",
      "Average test loss: 0.005890080940806204\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03430613632003466\n",
      "Average test loss: 0.005866923899286323\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03428733926349216\n",
      "Average test loss: 0.005916216578748491\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0342639624675115\n",
      "Average test loss: 0.00587803137054046\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03425804881089264\n",
      "Average test loss: 0.005833525233384635\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03427741582526101\n",
      "Average test loss: 0.005900424073967668\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03428202315005991\n",
      "Average test loss: 0.005929483612378438\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03421016753547722\n",
      "Average test loss: 0.005979283696661393\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0342829493549135\n",
      "Average test loss: 0.005890522740781307\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03414371746944057\n",
      "Average test loss: 0.005854686407165395\n",
      "Epoch 139/300\n",
      "Average training loss: 0.034201425237788095\n",
      "Average test loss: 0.005937350873317983\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03415960147976875\n",
      "Average test loss: 0.006875303341282738\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0342596873972151\n",
      "Average test loss: 0.006221705554260148\n",
      "Epoch 142/300\n",
      "Average training loss: 0.034091628074646\n",
      "Average test loss: 0.005834543938852019\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03414732184343868\n",
      "Average test loss: 0.005963517073127958\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03419729121526082\n",
      "Average test loss: 0.00585363695025444\n",
      "Epoch 145/300\n",
      "Average training loss: 0.034058277633455064\n",
      "Average test loss: 0.005832168987641732\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03408169387115373\n",
      "Average test loss: 0.005986771332720915\n",
      "Epoch 147/300\n",
      "Average training loss: 0.034108523001273476\n",
      "Average test loss: 0.005968670481608974\n",
      "Epoch 148/300\n",
      "Average training loss: 0.034068116452958846\n",
      "Average test loss: 0.005987142889863915\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03403271558880806\n",
      "Average test loss: 0.00594765574298799\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03400899555285772\n",
      "Average test loss: 0.005943689862059222\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03403925778137313\n",
      "Average test loss: 0.005982904271533092\n",
      "Epoch 152/300\n",
      "Average training loss: 0.034022222406334346\n",
      "Average test loss: 0.005852649978879425\n",
      "Epoch 153/300\n",
      "Average training loss: 0.033953085174163185\n",
      "Average test loss: 0.005841438615901602\n",
      "Epoch 154/300\n",
      "Average training loss: 0.034002692063649496\n",
      "Average test loss: 0.0058188083362248205\n",
      "Epoch 155/300\n",
      "Average training loss: 0.033998648047447205\n",
      "Average test loss: 0.00614515577794777\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03397681028313107\n",
      "Average test loss: 0.00583761157012648\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03396589494579368\n",
      "Average test loss: 0.006350622360077169\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03393392380078634\n",
      "Average test loss: 0.005842854999419716\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03397410885161824\n",
      "Average test loss: 0.00584350952754418\n",
      "Epoch 160/300\n",
      "Average training loss: 0.033904568539725406\n",
      "Average test loss: 0.005894377516789569\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03392246908446153\n",
      "Average test loss: 0.006128757111314271\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03393296905358632\n",
      "Average test loss: 0.005986868114107185\n",
      "Epoch 163/300\n",
      "Average training loss: 0.033890107611815136\n",
      "Average test loss: 0.005971539148853885\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03387017762992117\n",
      "Average test loss: 0.0059101642717917765\n",
      "Epoch 165/300\n",
      "Average training loss: 0.033858278767930135\n",
      "Average test loss: 2.9378911984761555\n",
      "Epoch 166/300\n",
      "Average training loss: 0.3240088558660613\n",
      "Average test loss: 0.011694428540766239\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07736998355388641\n",
      "Average test loss: 0.007744634400639269\n",
      "Epoch 168/300\n",
      "Average training loss: 0.060053292214870456\n",
      "Average test loss: 0.007218931103332175\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05281390357679791\n",
      "Average test loss: 0.0070538144537972075\n",
      "Epoch 170/300\n",
      "Average training loss: 0.047930777102708814\n",
      "Average test loss: 0.006717209553552999\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04459080653389295\n",
      "Average test loss: 0.007012111919621627\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0422581308748987\n",
      "Average test loss: 0.006327129576355219\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04062935608956549\n",
      "Average test loss: 0.006385270358787643\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03937686641348733\n",
      "Average test loss: 0.006111124687931604\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03833614284793536\n",
      "Average test loss: 0.0060760906657410995\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03754812322391404\n",
      "Average test loss: 0.0060997436741987866\n",
      "Epoch 177/300\n",
      "Average training loss: 0.036805337289969126\n",
      "Average test loss: 0.0059649176643126545\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03618440940479437\n",
      "Average test loss: 0.005889670596354537\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0356654047899776\n",
      "Average test loss: 0.005876953116721577\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03519740032321877\n",
      "Average test loss: 0.005912526667118072\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03486724140908983\n",
      "Average test loss: 0.005877308857730693\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03456920972797606\n",
      "Average test loss: 0.005850586876273155\n",
      "Epoch 183/300\n",
      "Average training loss: 0.034316209541426765\n",
      "Average test loss: 0.0058390761266152065\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03415176977382766\n",
      "Average test loss: 0.0058823466623822845\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03399549086391926\n",
      "Average test loss: 0.005945680824418862\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03392394361893336\n",
      "Average test loss: 0.005838427994814184\n",
      "Epoch 187/300\n",
      "Average training loss: 0.033892014135917026\n",
      "Average test loss: 0.005881412150131332\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03383730420635806\n",
      "Average test loss: 0.005856753208984931\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03385545742180612\n",
      "Average test loss: 0.005836902308381266\n",
      "Epoch 190/300\n",
      "Average training loss: 0.033855552481280436\n",
      "Average test loss: 0.005864301528781653\n",
      "Epoch 191/300\n",
      "Average training loss: 0.033887543357080885\n",
      "Average test loss: 0.005827860907961925\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03383598230944739\n",
      "Average test loss: 0.005838994312410553\n",
      "Epoch 193/300\n",
      "Average training loss: 0.033886611229843565\n",
      "Average test loss: 0.005926642747388946\n",
      "Epoch 194/300\n",
      "Average training loss: 0.033805917963385584\n",
      "Average test loss: 0.005900415795544784\n",
      "Epoch 195/300\n",
      "Average training loss: 0.033834656334585614\n",
      "Average test loss: 0.005882255686240064\n",
      "Epoch 196/300\n",
      "Average training loss: 0.033801225741704306\n",
      "Average test loss: 0.007883496378858884\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03380190757248137\n",
      "Average test loss: 0.005875767344401942\n",
      "Epoch 198/300\n",
      "Average training loss: 0.033793674152758385\n",
      "Average test loss: 0.005950396880507469\n",
      "Epoch 199/300\n",
      "Average training loss: 0.033715889417462876\n",
      "Average test loss: 0.005925390190548367\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03379424657093154\n",
      "Average test loss: 0.005900678466177649\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03374189156293869\n",
      "Average test loss: 0.005893893802952435\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03379424223138226\n",
      "Average test loss: 0.005907518124828736\n",
      "Epoch 203/300\n",
      "Average training loss: 0.033733820954958596\n",
      "Average test loss: 0.005868522220601638\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03371517247458299\n",
      "Average test loss: 0.005887401660697328\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03370378315117624\n",
      "Average test loss: 0.00584509100475245\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03367587712738249\n",
      "Average test loss: 0.0061708704651229915\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03378624960780144\n",
      "Average test loss: 0.005827625514732467\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03364507240719265\n",
      "Average test loss: 0.005921428752856122\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03367906675239404\n",
      "Average test loss: 0.006249295007023546\n",
      "Epoch 210/300\n",
      "Average training loss: 0.033700891945097185\n",
      "Average test loss: 0.005923661574307415\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03361419010162354\n",
      "Average test loss: 0.005992930022792684\n",
      "Epoch 212/300\n",
      "Average training loss: 0.033647502783272\n",
      "Average test loss: 0.005864381782296631\n",
      "Epoch 213/300\n",
      "Average training loss: 0.033616972552405464\n",
      "Average test loss: 0.005850611293895377\n",
      "Epoch 214/300\n",
      "Average training loss: 0.033588469124502605\n",
      "Average test loss: 0.005907544488294257\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03361926183435652\n",
      "Average test loss: 0.005916507594287396\n",
      "Epoch 216/300\n",
      "Average training loss: 0.033614443179633885\n",
      "Average test loss: 0.005827878139499161\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03362041033969985\n",
      "Average test loss: 0.005936549077431361\n",
      "Epoch 218/300\n",
      "Average training loss: 0.033507747857107056\n",
      "Average test loss: 0.005839577265497711\n",
      "Epoch 219/300\n",
      "Average training loss: 0.033612917671600975\n",
      "Average test loss: 0.005842506479885843\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03351822590496805\n",
      "Average test loss: 0.006230791602283716\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03350676100452741\n",
      "Average test loss: 0.006042376339435577\n",
      "Epoch 222/300\n",
      "Average training loss: 0.033505397298269804\n",
      "Average test loss: 0.005918565277424123\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03352707295285331\n",
      "Average test loss: 0.0058808592508236565\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03349573173456722\n",
      "Average test loss: 0.0059190356201595734\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03356125085552533\n",
      "Average test loss: 0.005913451298657391\n",
      "Epoch 226/300\n",
      "Average training loss: 0.033471737520562275\n",
      "Average test loss: 0.005981074571609497\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03345030641224649\n",
      "Average test loss: 0.006101875903705756\n",
      "Epoch 228/300\n",
      "Average training loss: 0.033476110574271946\n",
      "Average test loss: 0.005876055166539219\n",
      "Epoch 229/300\n",
      "Average training loss: 0.033474834957056575\n",
      "Average test loss: 0.005934189571274651\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03345371505618095\n",
      "Average test loss: 0.005834742690126101\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03345304333170255\n",
      "Average test loss: 0.005946636892027325\n",
      "Epoch 232/300\n",
      "Average training loss: 0.033421034057935076\n",
      "Average test loss: 0.005857980250484413\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03342378767662578\n",
      "Average test loss: 0.005822753042810493\n",
      "Epoch 234/300\n",
      "Average training loss: 0.033411828574207096\n",
      "Average test loss: 0.006038019794970751\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03343052740891774\n",
      "Average test loss: 0.0059166491106152535\n",
      "Epoch 236/300\n",
      "Average training loss: 0.033401107366714214\n",
      "Average test loss: 0.008084597798685233\n",
      "Epoch 237/300\n",
      "Average training loss: 0.033426584195759564\n",
      "Average test loss: 0.005819049374096923\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03339573423730002\n",
      "Average test loss: 0.005904940649039216\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03338337785171138\n",
      "Average test loss: 0.0058910470393796766\n",
      "Epoch 240/300\n",
      "Average training loss: 0.033368251307143104\n",
      "Average test loss: 0.005913300611906582\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03338467331065072\n",
      "Average test loss: 0.00582564253732562\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03333349594473839\n",
      "Average test loss: 0.005889169456230269\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0334471077207062\n",
      "Average test loss: 0.005976580999791622\n",
      "Epoch 244/300\n",
      "Average training loss: 0.033384514512287244\n",
      "Average test loss: 0.005852595458428065\n",
      "Epoch 245/300\n",
      "Average training loss: 0.033282995690902074\n",
      "Average test loss: 0.005941185320417086\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03335891697804133\n",
      "Average test loss: 0.005801868403951327\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03335079426401191\n",
      "Average test loss: 0.005844782779200209\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03330555498931143\n",
      "Average test loss: 0.00581685326827897\n",
      "Epoch 249/300\n",
      "Average training loss: 0.033314555186364385\n",
      "Average test loss: 0.005943013037244479\n",
      "Epoch 250/300\n",
      "Average training loss: 0.033304873201582166\n",
      "Average test loss: 0.005837884464611609\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03328660808669196\n",
      "Average test loss: 0.005873822551220656\n",
      "Epoch 252/300\n",
      "Average training loss: 0.033284182694223195\n",
      "Average test loss: 0.005826576879041062\n",
      "Epoch 253/300\n",
      "Average training loss: 0.033278015245993935\n",
      "Average test loss: 0.00584377962226669\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03328609891898102\n",
      "Average test loss: 0.0059635858076314135\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03323422985110018\n",
      "Average test loss: 0.005830800877263149\n",
      "Epoch 256/300\n",
      "Average training loss: 0.033204998718367684\n",
      "Average test loss: 0.005857092798584037\n",
      "Epoch 257/300\n",
      "Average training loss: 0.033926206608613334\n",
      "Average test loss: 0.005909299905101458\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0332441140876876\n",
      "Average test loss: 0.005861458261393839\n",
      "Epoch 259/300\n",
      "Average training loss: 0.033215553717480764\n",
      "Average test loss: 0.005907986983242962\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03317143500513501\n",
      "Average test loss: 0.005900741460422675\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03317359878950649\n",
      "Average test loss: 0.005966637957013315\n",
      "Epoch 262/300\n",
      "Average training loss: 0.033180469423532484\n",
      "Average test loss: 0.005863018035474751\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03318551647497548\n",
      "Average test loss: 0.00599570921022031\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03322254823976093\n",
      "Average test loss: 0.005876118456323942\n",
      "Epoch 265/300\n",
      "Average training loss: 0.033191779659854044\n",
      "Average test loss: 0.005853843833009402\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03317232637604078\n",
      "Average test loss: 0.009040852012733618\n",
      "Epoch 267/300\n",
      "Average training loss: 0.033150505519575546\n",
      "Average test loss: 0.0058778739178346265\n",
      "Epoch 268/300\n",
      "Average training loss: 0.033183766510751514\n",
      "Average test loss: 0.005919379819598463\n",
      "Epoch 269/300\n",
      "Average training loss: 0.033122777776585684\n",
      "Average test loss: 0.005902548037055466\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03316570596562492\n",
      "Average test loss: 0.0058735758153100805\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03311823825703727\n",
      "Average test loss: 0.005894792667279641\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03312350642018848\n",
      "Average test loss: 0.005852508912483851\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03315053042438295\n",
      "Average test loss: 0.005966808941629198\n",
      "Epoch 274/300\n",
      "Average training loss: 0.033157685544755726\n",
      "Average test loss: 0.005946955266926024\n",
      "Epoch 275/300\n",
      "Average training loss: 0.033111901571353276\n",
      "Average test loss: 0.005965802686495913\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03321100144916111\n",
      "Average test loss: 0.005891449125276672\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03308795697324806\n",
      "Average test loss: 0.0058786489129480386\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03309162664082315\n",
      "Average test loss: 0.006111975450896555\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03309726939598719\n",
      "Average test loss: 0.005896576777307524\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03310167218579187\n",
      "Average test loss: 0.0059110741789142295\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03309476127061579\n",
      "Average test loss: 0.006570071543256442\n",
      "Epoch 282/300\n",
      "Average training loss: 0.033095512724585004\n",
      "Average test loss: 0.00597142181214359\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03304209976891677\n",
      "Average test loss: 0.00594069443601701\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0330471695959568\n",
      "Average test loss: 0.005886370278067059\n",
      "Epoch 285/300\n",
      "Average training loss: 0.033030993868907295\n",
      "Average test loss: 0.005889834749201933\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03303655163778199\n",
      "Average test loss: 0.005881722014397382\n",
      "Epoch 287/300\n",
      "Average training loss: 0.033026375320222644\n",
      "Average test loss: 0.0059724796194997095\n",
      "Epoch 288/300\n",
      "Average training loss: 0.033034473038382\n",
      "Average test loss: 0.005915430277585983\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03305515534679095\n",
      "Average test loss: 0.0059323305098546875\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03304170361823506\n",
      "Average test loss: 0.00590888526249263\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03303092496428225\n",
      "Average test loss: 0.005868609459449847\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03303184601664543\n",
      "Average test loss: 0.005949748233788544\n",
      "Epoch 293/300\n",
      "Average training loss: 0.033026185270812775\n",
      "Average test loss: 0.006000158000737429\n",
      "Epoch 294/300\n",
      "Average training loss: 0.032966536222232716\n",
      "Average test loss: 0.005864074695441458\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03298359934488932\n",
      "Average test loss: 0.005892599104179276\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03299065651827388\n",
      "Average test loss: 0.005928187395135562\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0329899199505647\n",
      "Average test loss: 0.006000653041319715\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03299580501185523\n",
      "Average test loss: 0.0058973999665015275\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03299131312966347\n",
      "Average test loss: 0.005929646212607622\n",
      "Epoch 300/300\n",
      "Average training loss: 0.032952419880363674\n",
      "Average test loss: 0.005861829825573497\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.20947084579865138\n",
      "Average test loss: 0.0077375968355271555\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05348745742440224\n",
      "Average test loss: 0.0068517519409457845\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04689516172806422\n",
      "Average test loss: 0.006388531222111649\n",
      "Epoch 4/300\n",
      "Average training loss: 0.043897776673237486\n",
      "Average test loss: 0.006926108245220449\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04078624888261159\n",
      "Average test loss: 0.0060220759482019475\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03933893037173483\n",
      "Average test loss: 0.005704159621563223\n",
      "Epoch 7/300\n",
      "Average training loss: 0.037523080001274745\n",
      "Average test loss: 0.00574788927866353\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03632193328274621\n",
      "Average test loss: 0.005513493827233712\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03533733358316951\n",
      "Average test loss: 0.005623070953620805\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03437828512986501\n",
      "Average test loss: 0.00533079591103726\n",
      "Epoch 11/300\n",
      "Average training loss: 0.033539000199900734\n",
      "Average test loss: 0.00523980064317584\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03292505087620682\n",
      "Average test loss: 0.00521773836016655\n",
      "Epoch 13/300\n",
      "Average training loss: 0.032359963263074554\n",
      "Average test loss: 0.00507849291463693\n",
      "Epoch 14/300\n",
      "Average training loss: 0.031672219319476023\n",
      "Average test loss: 0.004740949453992976\n",
      "Epoch 15/300\n",
      "Average training loss: 0.031099474410216012\n",
      "Average test loss: 0.004722422581166029\n",
      "Epoch 16/300\n",
      "Average training loss: 0.030658905280960932\n",
      "Average test loss: 0.004652795231590668\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030223783395356602\n",
      "Average test loss: 0.004552935426847802\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02982633874648147\n",
      "Average test loss: 0.004614245381620195\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0295469848000341\n",
      "Average test loss: 0.004528208134488927\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02911241560512119\n",
      "Average test loss: 0.004430505686750015\n",
      "Epoch 21/300\n",
      "Average training loss: 0.028788328806559245\n",
      "Average test loss: 0.004280871032219794\n",
      "Epoch 22/300\n",
      "Average training loss: 0.028523418217897414\n",
      "Average test loss: 0.004232595105965932\n",
      "Epoch 23/300\n",
      "Average training loss: 0.028291212278935645\n",
      "Average test loss: 0.0042316556924747095\n",
      "Epoch 24/300\n",
      "Average training loss: 0.027994115935431586\n",
      "Average test loss: 0.004381812661058373\n",
      "Epoch 25/300\n",
      "Average training loss: 0.027790319975879458\n",
      "Average test loss: 0.004072630079670085\n",
      "Epoch 26/300\n",
      "Average training loss: 0.027535944210158453\n",
      "Average test loss: 0.004031081320097049\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0274060560895337\n",
      "Average test loss: 0.004093694462958309\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02717062890032927\n",
      "Average test loss: 0.003967811561706993\n",
      "Epoch 29/300\n",
      "Average training loss: 0.026951377396782238\n",
      "Average test loss: 0.003947421364486218\n",
      "Epoch 30/300\n",
      "Average training loss: 0.026843232221073574\n",
      "Average test loss: 0.00416594394379192\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02665730070405536\n",
      "Average test loss: 0.0038903718002968363\n",
      "Epoch 32/300\n",
      "Average training loss: 0.026545164732469453\n",
      "Average test loss: 0.003949309452126424\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0264149107552237\n",
      "Average test loss: 0.0038399675300137865\n",
      "Epoch 34/300\n",
      "Average training loss: 0.026273804800377953\n",
      "Average test loss: 0.0037709602639079093\n",
      "Epoch 35/300\n",
      "Average training loss: 0.026147015238801637\n",
      "Average test loss: 0.0038766003971298538\n",
      "Epoch 36/300\n",
      "Average training loss: 0.026061287239193917\n",
      "Average test loss: 0.003768958809889025\n",
      "Epoch 37/300\n",
      "Average training loss: 0.025920670883523095\n",
      "Average test loss: 0.0037911125630554227\n",
      "Epoch 38/300\n",
      "Average training loss: 0.025827579385704465\n",
      "Average test loss: 0.003700272494306167\n",
      "Epoch 39/300\n",
      "Average training loss: 0.025752629596326085\n",
      "Average test loss: 0.0037143989331606363\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02564923474689325\n",
      "Average test loss: 0.003699031758018666\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0256067676163382\n",
      "Average test loss: 0.0037584635296629533\n",
      "Epoch 42/300\n",
      "Average training loss: 0.025498495199614103\n",
      "Average test loss: 0.003651329424025284\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02545237997836537\n",
      "Average test loss: 0.0036646565058165124\n",
      "Epoch 44/300\n",
      "Average training loss: 0.025381857325633368\n",
      "Average test loss: 0.0036978141541282334\n",
      "Epoch 45/300\n",
      "Average training loss: 0.025262467698918448\n",
      "Average test loss: 0.003621118465438485\n",
      "Epoch 46/300\n",
      "Average training loss: 0.025266751731435458\n",
      "Average test loss: 0.0036908543159564337\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025211946818563674\n",
      "Average test loss: 0.00359439954161644\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02516728978852431\n",
      "Average test loss: 0.003643504463343157\n",
      "Epoch 49/300\n",
      "Average training loss: 0.025076487268010775\n",
      "Average test loss: 0.0037753910242269437\n",
      "Epoch 50/300\n",
      "Average training loss: 0.025078564062714578\n",
      "Average test loss: 0.003562054423201415\n",
      "Epoch 51/300\n",
      "Average training loss: 0.024960739041368165\n",
      "Average test loss: 0.003729187768366602\n",
      "Epoch 52/300\n",
      "Average training loss: 0.024954902556207446\n",
      "Average test loss: 0.0036913745808932518\n",
      "Epoch 53/300\n",
      "Average training loss: 0.024899247138036623\n",
      "Average test loss: 0.0036102941299064294\n",
      "Epoch 54/300\n",
      "Average training loss: 0.024855874783462947\n",
      "Average test loss: 0.0035426785039405027\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02480363430082798\n",
      "Average test loss: 0.00356454946887162\n",
      "Epoch 56/300\n",
      "Average training loss: 0.024758338660001756\n",
      "Average test loss: 0.003587060725937287\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02475188268555535\n",
      "Average test loss: 0.0037994364392426277\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02471761362420188\n",
      "Average test loss: 0.003602710841016637\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02469202055533727\n",
      "Average test loss: 0.0035305803761714037\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02463083576079872\n",
      "Average test loss: 0.003723797686398029\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024601709610886043\n",
      "Average test loss: 0.0035650326716196207\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0245566591322422\n",
      "Average test loss: 0.0036057863529357645\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02453372303644816\n",
      "Average test loss: 0.0035406894948747424\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024516892549064424\n",
      "Average test loss: 0.003503742465335462\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024460038349032404\n",
      "Average test loss: 0.003490191574104958\n",
      "Epoch 66/300\n",
      "Average training loss: 0.024495856016874314\n",
      "Average test loss: 0.0295077663279242\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024644315570592882\n",
      "Average test loss: 0.0035606172002024122\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024421023986405797\n",
      "Average test loss: 0.0034856281886912056\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024337967827916144\n",
      "Average test loss: 0.0034978397832148606\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024345178165369564\n",
      "Average test loss: 0.0035159333451754518\n",
      "Epoch 71/300\n",
      "Average training loss: 0.024299753558304573\n",
      "Average test loss: 0.0035135228356553447\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02428395508892006\n",
      "Average test loss: 0.003500042694724268\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02426144585178958\n",
      "Average test loss: 0.0034755023817221323\n",
      "Epoch 74/300\n",
      "Average training loss: 0.024262020233604645\n",
      "Average test loss: 0.0034953053978582223\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02423944201072057\n",
      "Average test loss: 0.0035239452314045692\n",
      "Epoch 76/300\n",
      "Average training loss: 0.024190132186644606\n",
      "Average test loss: 0.0034962291684415606\n",
      "Epoch 77/300\n",
      "Average training loss: 0.024178789908687274\n",
      "Average test loss: 0.003485070609798034\n",
      "Epoch 78/300\n",
      "Average training loss: 0.024147162646055223\n",
      "Average test loss: 0.0034941665408098036\n",
      "Epoch 79/300\n",
      "Average training loss: 0.024148724913597108\n",
      "Average test loss: 0.0034871973207013475\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0241007824242115\n",
      "Average test loss: 0.00348695759413143\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0241016356713242\n",
      "Average test loss: 0.003545395298136605\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02409122045007017\n",
      "Average test loss: 0.0035266183006266755\n",
      "Epoch 83/300\n",
      "Average training loss: 0.024069771735204592\n",
      "Average test loss: 0.003500306328965558\n",
      "Epoch 84/300\n",
      "Average training loss: 0.024033770622478592\n",
      "Average test loss: 0.0034482661442210277\n",
      "Epoch 85/300\n",
      "Average training loss: 0.024002726318107712\n",
      "Average test loss: 0.0034866519417199823\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0239856648594141\n",
      "Average test loss: 0.0035034311020539868\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02417211508916484\n",
      "Average test loss: 0.0034853490586909982\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02398014721771081\n",
      "Average test loss: 0.003567914791405201\n",
      "Epoch 89/300\n",
      "Average training loss: 0.023934866163465712\n",
      "Average test loss: 0.003494417435179154\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023941656284862094\n",
      "Average test loss: 0.0035248300313121743\n",
      "Epoch 91/300\n",
      "Average training loss: 0.023965245634317397\n",
      "Average test loss: 0.0034667548694544367\n",
      "Epoch 92/300\n",
      "Average training loss: 0.023867547864715257\n",
      "Average test loss: 0.0035353341243333286\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023887014740043216\n",
      "Average test loss: 0.0035114097971883085\n",
      "Epoch 94/300\n",
      "Average training loss: 0.023840247576435408\n",
      "Average test loss: 0.0034373073163959716\n",
      "Epoch 95/300\n",
      "Average training loss: 0.023844330443276298\n",
      "Average test loss: 0.0035216876620219814\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02383203173180421\n",
      "Average test loss: 0.00347096096807056\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02383044703470336\n",
      "Average test loss: 0.0034902258242170015\n",
      "Epoch 98/300\n",
      "Average training loss: 0.023790332943201067\n",
      "Average test loss: 0.0035240618402345315\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02378212538195981\n",
      "Average test loss: 0.004204665121104982\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02377655389159918\n",
      "Average test loss: 0.00347331435026394\n",
      "Epoch 101/300\n",
      "Average training loss: 0.023775051361156836\n",
      "Average test loss: 0.0035445088346799216\n",
      "Epoch 102/300\n",
      "Average training loss: 0.023730104795760577\n",
      "Average test loss: 0.003435388684893648\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02374631069931719\n",
      "Average test loss: 0.003488118588510487\n",
      "Epoch 104/300\n",
      "Average training loss: 0.023722975408037503\n",
      "Average test loss: 0.00980589598748419\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02380596898165014\n",
      "Average test loss: 0.003526093485040797\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02369400000406636\n",
      "Average test loss: 0.0035019515274713436\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02367773556543721\n",
      "Average test loss: 0.003440516489247481\n",
      "Epoch 108/300\n",
      "Average training loss: 0.023665767876638308\n",
      "Average test loss: 0.0034792555454704497\n",
      "Epoch 109/300\n",
      "Average training loss: 0.023680431293116675\n",
      "Average test loss: 0.0034544301529725392\n",
      "Epoch 110/300\n",
      "Average training loss: 0.023636518773933253\n",
      "Average test loss: 0.0034868302365971935\n",
      "Epoch 111/300\n",
      "Average training loss: 0.023613670059376292\n",
      "Average test loss: 0.003468086606098546\n",
      "Epoch 112/300\n",
      "Average training loss: 0.023631809575690162\n",
      "Average test loss: 0.00345411261336671\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023604259570439656\n",
      "Average test loss: 0.003466670748260286\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023594674383600554\n",
      "Average test loss: 0.0034636192412839995\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02357742886741956\n",
      "Average test loss: 0.0034496751117209594\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02357824466129144\n",
      "Average test loss: 0.003467317481421762\n",
      "Epoch 117/300\n",
      "Average training loss: 0.023566699284646248\n",
      "Average test loss: 0.0034452833032442465\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023541925731632444\n",
      "Average test loss: 0.003557903017434809\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023572163563635614\n",
      "Average test loss: 0.0034552107171879876\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023507058186663522\n",
      "Average test loss: 0.003543817105392615\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02351552297009362\n",
      "Average test loss: 0.003434408342672719\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02350190253721343\n",
      "Average test loss: 0.0035161836184561253\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023475526231858466\n",
      "Average test loss: 0.0034256702297263677\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02348695503340827\n",
      "Average test loss: 0.003444878086861637\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023469925341506798\n",
      "Average test loss: 0.0034835125296894046\n",
      "Epoch 126/300\n",
      "Average training loss: 0.023480129150880706\n",
      "Average test loss: 0.0034293486279331977\n",
      "Epoch 127/300\n",
      "Average training loss: 0.023457178170482318\n",
      "Average test loss: 0.0036099859573360945\n",
      "Epoch 128/300\n",
      "Average training loss: 0.23062478204568226\n",
      "Average test loss: 0.006868668574839831\n",
      "Epoch 129/300\n",
      "Average training loss: 0.040540009912517334\n",
      "Average test loss: 0.0045453134170836875\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03292509520219432\n",
      "Average test loss: 0.004225641153131922\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03070064046151108\n",
      "Average test loss: 0.003960576908042033\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02933803665306833\n",
      "Average test loss: 0.0039526398637228545\n",
      "Epoch 133/300\n",
      "Average training loss: 0.028383409400780997\n",
      "Average test loss: 0.0038571341608961422\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02755141646332211\n",
      "Average test loss: 0.0037636132668703794\n",
      "Epoch 135/300\n",
      "Average training loss: 0.026935357186529372\n",
      "Average test loss: 0.003679874129179451\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02633682559430599\n",
      "Average test loss: 0.0036688894576703507\n",
      "Epoch 137/300\n",
      "Average training loss: 0.025859823611047533\n",
      "Average test loss: 0.003566110114670462\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02541249476207627\n",
      "Average test loss: 0.0036334229496618113\n",
      "Epoch 139/300\n",
      "Average training loss: 0.025008034250802464\n",
      "Average test loss: 0.003931087277001805\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02467785478466087\n",
      "Average test loss: 0.003612240673870676\n",
      "Epoch 141/300\n",
      "Average training loss: 0.024402017542057567\n",
      "Average test loss: 0.00407850741305285\n",
      "Epoch 142/300\n",
      "Average training loss: 0.024182307437062265\n",
      "Average test loss: 0.003499732924832238\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02401168487303787\n",
      "Average test loss: 0.0034936964623630048\n",
      "Epoch 144/300\n",
      "Average training loss: 0.023903703808784486\n",
      "Average test loss: 0.003514940082612965\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023812404013342326\n",
      "Average test loss: 0.0034827805660251113\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02375345146490468\n",
      "Average test loss: 0.0034571861535724665\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02370641482538647\n",
      "Average test loss: 0.0034607909528745545\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02364706243409051\n",
      "Average test loss: 0.0037590729143056603\n",
      "Epoch 149/300\n",
      "Average training loss: 0.023654001514116924\n",
      "Average test loss: 0.003469694171928697\n",
      "Epoch 150/300\n",
      "Average training loss: 0.023612795487046243\n",
      "Average test loss: 0.0035080617114694583\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02357824215458499\n",
      "Average test loss: 0.003503984007777439\n",
      "Epoch 152/300\n",
      "Average training loss: 0.023587448856896825\n",
      "Average test loss: 0.0034458435049487487\n",
      "Epoch 153/300\n",
      "Average training loss: 0.023540327732761702\n",
      "Average test loss: 0.003424500334387024\n",
      "Epoch 154/300\n",
      "Average training loss: 0.023526339741216765\n",
      "Average test loss: 0.005460678483876917\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02352757745484511\n",
      "Average test loss: 0.003476625931966636\n",
      "Epoch 156/300\n",
      "Average training loss: 0.023486149604121844\n",
      "Average test loss: 0.003444910434385141\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02348375999596384\n",
      "Average test loss: 0.0035210772260195677\n",
      "Epoch 158/300\n",
      "Average training loss: 0.023457983675930234\n",
      "Average test loss: 0.0034577248309635455\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0234510197242101\n",
      "Average test loss: 0.0034650422361575894\n",
      "Epoch 160/300\n",
      "Average training loss: 0.023434537321329118\n",
      "Average test loss: 0.0035622004572716023\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02340784513619211\n",
      "Average test loss: 0.003450103306521972\n",
      "Epoch 162/300\n",
      "Average training loss: 0.023415427062246533\n",
      "Average test loss: 0.003486141027468774\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02340723385910193\n",
      "Average test loss: 0.0034323533922433853\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02337797764937083\n",
      "Average test loss: 0.003413269033448564\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023376105826761986\n",
      "Average test loss: 0.0034267054546831383\n",
      "Epoch 166/300\n",
      "Average training loss: 0.023363372141288388\n",
      "Average test loss: 0.0034877980101025766\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02332706226905187\n",
      "Average test loss: 0.00343742036819458\n",
      "Epoch 168/300\n",
      "Average training loss: 0.023345205782188308\n",
      "Average test loss: 0.0035060721712393893\n",
      "Epoch 169/300\n",
      "Average training loss: 0.023329693997899692\n",
      "Average test loss: 0.003433167195568482\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023310182521740594\n",
      "Average test loss: 0.0034476128255741464\n",
      "Epoch 171/300\n",
      "Average training loss: 0.023297376240293186\n",
      "Average test loss: 0.0034273944749600356\n",
      "Epoch 172/300\n",
      "Average training loss: 0.023288326167398028\n",
      "Average test loss: 0.005145446779413356\n",
      "Epoch 173/300\n",
      "Average training loss: 0.023339947887592846\n",
      "Average test loss: 0.0034246086304386457\n",
      "Epoch 174/300\n",
      "Average training loss: 0.023257727056741714\n",
      "Average test loss: 0.003435435416176915\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02326501552098327\n",
      "Average test loss: 0.0034329460865507523\n",
      "Epoch 176/300\n",
      "Average training loss: 0.023250818921460047\n",
      "Average test loss: 0.003471034812844462\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023235696709818312\n",
      "Average test loss: 0.0034355032506088417\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023238485316435496\n",
      "Average test loss: 0.0034141710048748385\n",
      "Epoch 179/300\n",
      "Average training loss: 0.023208888464503818\n",
      "Average test loss: 0.003455525273250209\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023207924803098044\n",
      "Average test loss: 0.0034436409984611805\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023200974822044373\n",
      "Average test loss: 0.0034214568278855746\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023192168899708324\n",
      "Average test loss: 0.0034435862762232624\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02318076366517279\n",
      "Average test loss: 0.003468829179389609\n",
      "Epoch 184/300\n",
      "Average training loss: 0.023182464145951803\n",
      "Average test loss: 0.003424884063916074\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023176281967096857\n",
      "Average test loss: 0.003420016668736935\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02317292580836349\n",
      "Average test loss: 0.003423246905207634\n",
      "Epoch 187/300\n",
      "Average training loss: 0.023154186500443354\n",
      "Average test loss: 0.0034313602911101447\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023130718568960824\n",
      "Average test loss: 0.0034442341141402723\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0231386758900351\n",
      "Average test loss: 0.0034253946820067033\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02314467577636242\n",
      "Average test loss: 0.0034264049037463135\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02312356061074469\n",
      "Average test loss: 0.0034468378242519166\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02310841677420669\n",
      "Average test loss: 0.003493201370454497\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02310071777469582\n",
      "Average test loss: 0.0034210092226664227\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023105899780988693\n",
      "Average test loss: 0.003478586666492952\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023118555188179016\n",
      "Average test loss: 0.003445712650815646\n",
      "Epoch 196/300\n",
      "Average training loss: 0.023070557279719246\n",
      "Average test loss: 0.003495681706815958\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023060669624143177\n",
      "Average test loss: 0.0034435840588476923\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023087020377318065\n",
      "Average test loss: 0.0035279422741797237\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02307812331782447\n",
      "Average test loss: 0.0034206812156157363\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02303979651298788\n",
      "Average test loss: 0.0034249162024094\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023027112474044164\n",
      "Average test loss: 0.0034173409094413122\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02306053901380963\n",
      "Average test loss: 0.0033962583653628824\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023045362055301666\n",
      "Average test loss: 0.0034258800813307365\n",
      "Epoch 204/300\n",
      "Average training loss: 0.023023411595159107\n",
      "Average test loss: 0.0034269862545447216\n",
      "Epoch 205/300\n",
      "Average training loss: 0.023032071434789234\n",
      "Average test loss: 0.003461971754208207\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02302853062748909\n",
      "Average test loss: 0.0034308016364359194\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023022269416186545\n",
      "Average test loss: 0.0034764751839554972\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023005524060792395\n",
      "Average test loss: 0.003449010020742814\n",
      "Epoch 209/300\n",
      "Average training loss: 0.022984851158327525\n",
      "Average test loss: 0.003420040947695573\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02301599488159021\n",
      "Average test loss: 0.003482580877840519\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02296821467247274\n",
      "Average test loss: 0.0034172382615506648\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02296944853829013\n",
      "Average test loss: 0.003431876012020641\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022957287458909882\n",
      "Average test loss: 0.0034399169497191908\n",
      "Epoch 214/300\n",
      "Average training loss: 0.022954128245512646\n",
      "Average test loss: 0.003414918630073468\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02295512198574013\n",
      "Average test loss: 0.0034135779527326424\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02294358865254455\n",
      "Average test loss: 0.00341607919956247\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02294770922429032\n",
      "Average test loss: 0.0034459267428351772\n",
      "Epoch 218/300\n",
      "Average training loss: 0.022951213041941324\n",
      "Average test loss: 0.003443641416107615\n",
      "Epoch 219/300\n",
      "Average training loss: 0.022948519691824914\n",
      "Average test loss: 0.0035457076066070133\n",
      "Epoch 220/300\n",
      "Average training loss: 0.022916527820958033\n",
      "Average test loss: 0.003428041931655672\n",
      "Epoch 221/300\n",
      "Average training loss: 0.022908605325553152\n",
      "Average test loss: 0.003478396146144304\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022927638919817077\n",
      "Average test loss: 0.0034766673650592564\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022932867364750968\n",
      "Average test loss: 0.003434502740080158\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022900798799263107\n",
      "Average test loss: 0.0036009141293664774\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022897185009386804\n",
      "Average test loss: 0.0034949142237504323\n",
      "Epoch 226/300\n",
      "Average training loss: 0.022914213877585198\n",
      "Average test loss: 0.0035063723743789725\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022868240447507963\n",
      "Average test loss: 0.0034543791928639015\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02286907125512759\n",
      "Average test loss: 0.003428805198934343\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02288523846699132\n",
      "Average test loss: 0.003441944692283869\n",
      "Epoch 230/300\n",
      "Average training loss: 0.022887013951937357\n",
      "Average test loss: 0.003442817890809642\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022864732280373574\n",
      "Average test loss: 0.003421714193498095\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022852075815200806\n",
      "Average test loss: 0.0034328405501114\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022878017897407215\n",
      "Average test loss: 0.0034548986218869687\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022858033200105033\n",
      "Average test loss: 0.003485918155974812\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02284016068279743\n",
      "Average test loss: 0.003465253928469287\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02285539253718323\n",
      "Average test loss: 0.0034439702954971126\n",
      "Epoch 237/300\n",
      "Average training loss: 0.022844688427117135\n",
      "Average test loss: 0.0034128501498036914\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022820303748051326\n",
      "Average test loss: 0.0034810823435998625\n",
      "Epoch 239/300\n",
      "Average training loss: 0.022818633208672206\n",
      "Average test loss: 0.003434033622344335\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02280631514887015\n",
      "Average test loss: 0.0034085723699794875\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02284086275100708\n",
      "Average test loss: 0.003445852805963821\n",
      "Epoch 242/300\n",
      "Average training loss: 0.022822216103474298\n",
      "Average test loss: 0.0034091004433317315\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02282525482856565\n",
      "Average test loss: 0.00353311798762944\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02279293697575728\n",
      "Average test loss: 0.003416362940851185\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022787542551755906\n",
      "Average test loss: 0.0034126148830271427\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02278896589908335\n",
      "Average test loss: 0.003600245457349552\n",
      "Epoch 247/300\n",
      "Average training loss: 0.022778025536073578\n",
      "Average test loss: 0.003445379899400804\n",
      "Epoch 248/300\n",
      "Average training loss: 0.022774939154585203\n",
      "Average test loss: 0.003495975802342097\n",
      "Epoch 249/300\n",
      "Average training loss: 0.022800370461410948\n",
      "Average test loss: 0.0034271358572360543\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022760076762901413\n",
      "Average test loss: 0.003415074970987108\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02278121843851275\n",
      "Average test loss: 0.003578703838090102\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02276544328033924\n",
      "Average test loss: 0.0034365006685256956\n",
      "Epoch 253/300\n",
      "Average training loss: 0.022743881674276458\n",
      "Average test loss: 0.003434469623077247\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022742362673911783\n",
      "Average test loss: 0.0034794841011365255\n",
      "Epoch 255/300\n",
      "Average training loss: 0.022757700042592154\n",
      "Average test loss: 0.0037250928700798087\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0227420920100477\n",
      "Average test loss: 0.0034361053665892946\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022744825527071952\n",
      "Average test loss: 0.003472586549197634\n",
      "Epoch 258/300\n",
      "Average training loss: 0.022746549808316762\n",
      "Average test loss: 0.0035075652584847477\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022698346566822795\n",
      "Average test loss: 0.003418079742540916\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02272742252217399\n",
      "Average test loss: 0.0034442623131391075\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022722431992491087\n",
      "Average test loss: 0.0033961835093796254\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022701659202575684\n",
      "Average test loss: 0.0034939241181645127\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02272583056986332\n",
      "Average test loss: 0.003450439645598332\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02270307708448834\n",
      "Average test loss: 0.0034358343593776227\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022703662819332546\n",
      "Average test loss: 0.003411858928700288\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022685039195749494\n",
      "Average test loss: 0.003440152241744929\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02268478961951203\n",
      "Average test loss: 0.003494956017161409\n",
      "Epoch 268/300\n",
      "Average training loss: 0.022696616114841566\n",
      "Average test loss: 0.003537486034962866\n",
      "Epoch 269/300\n",
      "Average training loss: 0.022673111740085815\n",
      "Average test loss: 0.0047377858137091\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022692899253633287\n",
      "Average test loss: 0.003475423242896795\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02266054692533281\n",
      "Average test loss: 0.003466487924878796\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022690045749147732\n",
      "Average test loss: 0.003450690396544006\n",
      "Epoch 273/300\n",
      "Average training loss: 0.022674002145727474\n",
      "Average test loss: 0.0034605142999854354\n",
      "Epoch 274/300\n",
      "Average training loss: 0.022659013350804648\n",
      "Average test loss: 0.0034699891727003784\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02264821952415837\n",
      "Average test loss: 0.0034801046287433967\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022666374075743886\n",
      "Average test loss: 0.003428290648592843\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02266171378890673\n",
      "Average test loss: 0.0034480170882824394\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02266110620068179\n",
      "Average test loss: 0.0034154441859573126\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02262647149297926\n",
      "Average test loss: 0.003414451797389322\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02264482338560952\n",
      "Average test loss: 0.0034688326944079663\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022618723157379363\n",
      "Average test loss: 0.003393834667280316\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022643582503000897\n",
      "Average test loss: 0.0034105017996496623\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02264038455651866\n",
      "Average test loss: 0.003490801643166277\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022631536808278827\n",
      "Average test loss: 0.0034289467964942256\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022639854902194605\n",
      "Average test loss: 0.003450624254842599\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022594828200009135\n",
      "Average test loss: 0.003447920435004764\n",
      "Epoch 287/300\n",
      "Average training loss: 0.022630452205737433\n",
      "Average test loss: 0.003451075548513068\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02261917245056894\n",
      "Average test loss: 0.003430770988886555\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022605327434009975\n",
      "Average test loss: 0.003436483071082168\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02259714791344272\n",
      "Average test loss: 0.003415588557306263\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022576882633897993\n",
      "Average test loss: 0.0034213106299026143\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022608155232336787\n",
      "Average test loss: 0.0034549580588936805\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022583504993882446\n",
      "Average test loss: 0.003462551223528054\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022585850038462214\n",
      "Average test loss: 0.0034577081742592983\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022609889674517842\n",
      "Average test loss: 0.0034290066671868167\n",
      "Epoch 296/300\n",
      "Average training loss: 0.022577775716781615\n",
      "Average test loss: 0.003414923647832539\n",
      "Epoch 297/300\n",
      "Average training loss: 0.022580033952991168\n",
      "Average test loss: 0.003462642206293013\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02256001344985432\n",
      "Average test loss: 0.003461173793300986\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02256948065261046\n",
      "Average test loss: 0.0034344455359710587\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0225949144396517\n",
      "Average test loss: 0.003436524105982648\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1896824256049262\n",
      "Average test loss: 0.005862108334485028\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04444199238220851\n",
      "Average test loss: 0.005413325070506996\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03904183496369256\n",
      "Average test loss: 0.005015928827226162\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03648766835861736\n",
      "Average test loss: 0.004730820943290989\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03416805472970009\n",
      "Average test loss: 0.004748711818001337\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03251203110482958\n",
      "Average test loss: 0.005126244116160605\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03125181500613689\n",
      "Average test loss: 0.004535971471418937\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02999485703640514\n",
      "Average test loss: 0.004224699946327342\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02904778260820442\n",
      "Average test loss: 0.003840513185287515\n",
      "Epoch 10/300\n",
      "Average training loss: 0.028075002763006424\n",
      "Average test loss: 0.0037492140734361277\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027199272566371495\n",
      "Average test loss: 0.003736060398734278\n",
      "Epoch 12/300\n",
      "Average training loss: 0.026467985858519873\n",
      "Average test loss: 0.003788210463606649\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02584326235122151\n",
      "Average test loss: 0.003558298331167963\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02530865734318892\n",
      "Average test loss: 0.003540996523367034\n",
      "Epoch 15/300\n",
      "Average training loss: 0.024730746706326804\n",
      "Average test loss: 0.003378038549174865\n",
      "Epoch 16/300\n",
      "Average training loss: 0.024314721936980883\n",
      "Average test loss: 0.003387987544346187\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02394857398337788\n",
      "Average test loss: 0.0034487505482716694\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02356525550948249\n",
      "Average test loss: 0.003098138677784138\n",
      "Epoch 19/300\n",
      "Average training loss: 0.023184087384078237\n",
      "Average test loss: 0.0030533213342229526\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02285153421594037\n",
      "Average test loss: 0.0030243008941825892\n",
      "Epoch 21/300\n",
      "Average training loss: 0.022602366684211624\n",
      "Average test loss: 0.0031597296531415647\n",
      "Epoch 22/300\n",
      "Average training loss: 0.022395275198751025\n",
      "Average test loss: 0.002876832502790623\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02211861056420538\n",
      "Average test loss: 0.0030916816767098177\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021927085874809158\n",
      "Average test loss: 0.002829602344168557\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02174386120090882\n",
      "Average test loss: 0.0028000630643218757\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021599917618764772\n",
      "Average test loss: 0.0027220291913383535\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02141409133705828\n",
      "Average test loss: 0.002739533339730567\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02124676147268878\n",
      "Average test loss: 0.002694147269965874\n",
      "Epoch 29/300\n",
      "Average training loss: 0.021118840417928164\n",
      "Average test loss: 0.002664818341222902\n",
      "Epoch 30/300\n",
      "Average training loss: 0.021027189446820152\n",
      "Average test loss: 0.0026392975853135187\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020857094721661674\n",
      "Average test loss: 0.002636439949274063\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020766249689790937\n",
      "Average test loss: 0.002613101346211301\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020706098881032732\n",
      "Average test loss: 0.0025680365585204626\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020594377126958635\n",
      "Average test loss: 0.002589156482161747\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020524269433485138\n",
      "Average test loss: 0.0026287115187280706\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02039427991045846\n",
      "Average test loss: 0.002594441739738815\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020364488265580603\n",
      "Average test loss: 0.0026919892982890208\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020268125949634445\n",
      "Average test loss: 0.002504273385844297\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020200421081648934\n",
      "Average test loss: 0.002526271376551853\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020150506575902304\n",
      "Average test loss: 0.0025506993358333906\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02010472567131122\n",
      "Average test loss: 0.002480040502631002\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02001245889481571\n",
      "Average test loss: 0.0024655888928100467\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02001273094614347\n",
      "Average test loss: 0.0024621826893546513\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01993160172800223\n",
      "Average test loss: 0.002478213697257969\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019889519887665907\n",
      "Average test loss: 0.0024798709864003792\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01986585804157787\n",
      "Average test loss: 0.002487042635679245\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019794870093464853\n",
      "Average test loss: 0.00254933139640424\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01975750039352311\n",
      "Average test loss: 0.002434241806467374\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019711381322807735\n",
      "Average test loss: 0.002576145910554462\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019662713881995943\n",
      "Average test loss: 0.0024988881308171482\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019634416204359795\n",
      "Average test loss: 0.002410414665626983\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019606497746374872\n",
      "Average test loss: 0.00240827285622557\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01958885398507118\n",
      "Average test loss: 0.0024409833157228097\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01951330347855886\n",
      "Average test loss: 0.0024330013254657387\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0195088628563616\n",
      "Average test loss: 0.0023907824005517694\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019438963007595803\n",
      "Average test loss: 0.0024190358420213063\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019451324875156085\n",
      "Average test loss: 0.002396246344368491\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01939579453567664\n",
      "Average test loss: 0.0024217156188355553\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01937593996193674\n",
      "Average test loss: 0.0023995134189931885\n",
      "Epoch 60/300\n",
      "Average training loss: 0.019366026067071492\n",
      "Average test loss: 0.0023586521930992603\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019633852428860135\n",
      "Average test loss: 0.002407799721178081\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01929084444211589\n",
      "Average test loss: 0.0024072232556839783\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019265484778417483\n",
      "Average test loss: 0.0024630552660673856\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01926889001992014\n",
      "Average test loss: 0.002371467519344555\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019217498951488072\n",
      "Average test loss: 0.00247074384883874\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019209569272067812\n",
      "Average test loss: 0.0023826874851559598\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019176240998009842\n",
      "Average test loss: 0.002358446767967608\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019168877569337685\n",
      "Average test loss: 0.0024116236318109763\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019130096692177984\n",
      "Average test loss: 0.0024215584949900705\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01911878196729554\n",
      "Average test loss: 0.0023997129239141943\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01915747849808799\n",
      "Average test loss: 0.0025506315037815107\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019061694889432854\n",
      "Average test loss: 0.0023457422899082303\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019072690669033263\n",
      "Average test loss: 0.0024060645020670362\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019032536163926125\n",
      "Average test loss: 0.002355222972109914\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019015389819939933\n",
      "Average test loss: 0.0023341292672687105\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01899313626686732\n",
      "Average test loss: 0.0023893301902959746\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01898032807641559\n",
      "Average test loss: 0.0024114686159623995\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01898217966159185\n",
      "Average test loss: 0.0023584564396490655\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01894915859070089\n",
      "Average test loss: 0.002347302803666227\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018936397762762177\n",
      "Average test loss: 0.0024227851546472974\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0189192389961746\n",
      "Average test loss: 0.002680105556216505\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01889971135142777\n",
      "Average test loss: 0.00233060850823919\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01887928474115001\n",
      "Average test loss: 0.00343204975190262\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01888527568678061\n",
      "Average test loss: 0.002442907582761513\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018849364793962903\n",
      "Average test loss: 0.0023795859672956997\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01882022437536054\n",
      "Average test loss: 0.002331611338485446\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018900323560668363\n",
      "Average test loss: 0.002359381329475178\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01878384578641918\n",
      "Average test loss: 0.002351671142503619\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01878176023397181\n",
      "Average test loss: 0.0023325154116998117\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01876499710149235\n",
      "Average test loss: 0.0023144249493877093\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01874453976750374\n",
      "Average test loss: 0.002326828957431846\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01875222576326794\n",
      "Average test loss: 0.002344334900379181\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0187287055850029\n",
      "Average test loss: 0.002402966992929578\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01873842125468784\n",
      "Average test loss: 0.0023943267355983457\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018705502461228107\n",
      "Average test loss: 0.002325057217851281\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018708451948232122\n",
      "Average test loss: 0.00238024937796096\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018677773889568115\n",
      "Average test loss: 0.0023391990792006254\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018653607812192705\n",
      "Average test loss: 0.0023161237016320227\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018656781882875496\n",
      "Average test loss: 0.002381025801723202\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018647493167055978\n",
      "Average test loss: 0.002318442643309633\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018642928555607795\n",
      "Average test loss: 0.002316895890567038\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01860863957885239\n",
      "Average test loss: 0.0023420630796915955\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018593079081012143\n",
      "Average test loss: 0.0023264642748981714\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0185853242981765\n",
      "Average test loss: 0.0024190536698119507\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01860840936170684\n",
      "Average test loss: 0.0023262787461280823\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018575283609330653\n",
      "Average test loss: 0.002351694306358695\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01855802874598238\n",
      "Average test loss: 0.002311765517418583\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018551287314958042\n",
      "Average test loss: 0.002327592204428381\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018531935728258557\n",
      "Average test loss: 0.0023412078847694726\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018538038345674673\n",
      "Average test loss: 0.002342143281880352\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018512510525683562\n",
      "Average test loss: 0.002350988859931628\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01849526206155618\n",
      "Average test loss: 0.002307741239046057\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018506193983058136\n",
      "Average test loss: 0.00843890697012345\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018488979801535608\n",
      "Average test loss: 0.002368411533534527\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018486812765399614\n",
      "Average test loss: 0.0023117823408295712\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018475853687359226\n",
      "Average test loss: 0.0023352963394588895\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018453458605541123\n",
      "Average test loss: 0.002353638480934832\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018434665775961347\n",
      "Average test loss: 0.0023346992811808984\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018430447599954075\n",
      "Average test loss: 0.0023113706310590107\n",
      "Epoch 120/300\n",
      "Average training loss: 0.018421755255924332\n",
      "Average test loss: 0.002325978096574545\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01844403858979543\n",
      "Average test loss: 0.0023360679370040697\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018426852737863857\n",
      "Average test loss: 0.002581749068159196\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018465019947952695\n",
      "Average test loss: 0.002355848622508347\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018401598299543062\n",
      "Average test loss: 0.002297170988077091\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01836976221534941\n",
      "Average test loss: 0.0022975662692139545\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018369864688151414\n",
      "Average test loss: 0.002302837395419677\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01835414462039868\n",
      "Average test loss: 0.002306840643286705\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01836330962760581\n",
      "Average test loss: 0.0023265829450554317\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018361500290532906\n",
      "Average test loss: 0.0023137214994058014\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018339497905638484\n",
      "Average test loss: 0.002295089171992408\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01832089956270324\n",
      "Average test loss: 0.0023306466668016382\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018327752452757622\n",
      "Average test loss: 0.0023649065237906245\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018313323075572648\n",
      "Average test loss: 0.0023207191868374745\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01830850387778547\n",
      "Average test loss: 0.0023405758132123283\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01829066118432416\n",
      "Average test loss: 0.002328106365994447\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018281396985054016\n",
      "Average test loss: 0.0022884080587989753\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01828260570516189\n",
      "Average test loss: 0.002289055134480198\n",
      "Epoch 138/300\n",
      "Average training loss: 0.018277562387287616\n",
      "Average test loss: 0.002359315894337164\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01826164141794046\n",
      "Average test loss: 0.002407243602391746\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018257339095075924\n",
      "Average test loss: 0.002367854182504945\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018245168257090782\n",
      "Average test loss: 0.002338369077692429\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018250971077216994\n",
      "Average test loss: 0.0023184971064329147\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018235168334510592\n",
      "Average test loss: 0.002288592093520694\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01822130043970214\n",
      "Average test loss: 0.0023255742240904107\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01821794971658124\n",
      "Average test loss: 0.002325755471777585\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018219174514214197\n",
      "Average test loss: 0.0022844148702505563\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01820796877808041\n",
      "Average test loss: 0.002318202423552672\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018197827478249867\n",
      "Average test loss: 0.00228442103912433\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018206606992416913\n",
      "Average test loss: 0.002303707042295072\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018190140297015506\n",
      "Average test loss: 0.002323419364583161\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01817241703801685\n",
      "Average test loss: 0.0024942015195265413\n",
      "Epoch 152/300\n",
      "Average training loss: 0.018180831924080848\n",
      "Average test loss: 0.0023017212649186452\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018163042313522764\n",
      "Average test loss: 0.002354692689039641\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01816509330438243\n",
      "Average test loss: 0.0022866308322797217\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01815516907721758\n",
      "Average test loss: 0.0023149234517994854\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01811894731885857\n",
      "Average test loss: 0.00230987799881647\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01815851880278852\n",
      "Average test loss: 0.0023081032228138712\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018123999814192455\n",
      "Average test loss: 0.0023110416053483884\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018105210032727984\n",
      "Average test loss: 0.0023063222556892367\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018105949289268918\n",
      "Average test loss: 0.0023551045783484974\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018123308257924187\n",
      "Average test loss: 0.002281864949605531\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01811094210876359\n",
      "Average test loss: 0.0023330957405269144\n",
      "Epoch 163/300\n",
      "Average training loss: 0.018100098076793882\n",
      "Average test loss: 0.002297817326254315\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018105876420934995\n",
      "Average test loss: 0.0023394650732063583\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01808807724383142\n",
      "Average test loss: 0.0023560344949364663\n",
      "Epoch 166/300\n",
      "Average training loss: 0.018069617440303165\n",
      "Average test loss: 0.0023671431375874414\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01808308453278409\n",
      "Average test loss: 0.0025899038763923777\n",
      "Epoch 168/300\n",
      "Average training loss: 0.018068591882785163\n",
      "Average test loss: 0.002305714018228981\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01807753710117605\n",
      "Average test loss: 0.002342026074623896\n",
      "Epoch 170/300\n",
      "Average training loss: 0.018057881676488453\n",
      "Average test loss: 0.0022959801101436216\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01805312009818024\n",
      "Average test loss: 0.0023078698609024285\n",
      "Epoch 172/300\n",
      "Average training loss: 0.018048617920941777\n",
      "Average test loss: 0.002709364157790939\n",
      "Epoch 173/300\n",
      "Average training loss: 0.018036355775263576\n",
      "Average test loss: 0.0022825106273715694\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01804129556318124\n",
      "Average test loss: 0.0023495943333125776\n",
      "Epoch 175/300\n",
      "Average training loss: 0.018036439546280435\n",
      "Average test loss: 0.002345870907832351\n",
      "Epoch 176/300\n",
      "Average training loss: 0.018031135955618487\n",
      "Average test loss: 0.0023294225726276634\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018019615541729664\n",
      "Average test loss: 0.002286343834052483\n",
      "Epoch 178/300\n",
      "Average training loss: 0.018022053192059198\n",
      "Average test loss: 0.0023134658782639436\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01799904075430499\n",
      "Average test loss: 0.008960746134734816\n",
      "Epoch 180/300\n",
      "Average training loss: 0.018002624021636115\n",
      "Average test loss: 0.002327958478488856\n",
      "Epoch 181/300\n",
      "Average training loss: 0.018013783300916353\n",
      "Average test loss: 0.002404364854097366\n",
      "Epoch 182/300\n",
      "Average training loss: 0.018003884043958453\n",
      "Average test loss: 0.0023402886380338005\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01798559661044015\n",
      "Average test loss: 0.0022859272288365496\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017989167125688658\n",
      "Average test loss: 0.002295531369952692\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01797553948726919\n",
      "Average test loss: 0.002288819767948654\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01796813676920202\n",
      "Average test loss: 0.002296452826095952\n",
      "Epoch 187/300\n",
      "Average training loss: 0.018010883149173526\n",
      "Average test loss: 0.0023051648125466374\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017973119982414774\n",
      "Average test loss: 0.0023370549457354677\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017959360341231027\n",
      "Average test loss: 0.002360152573014299\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0179568197015259\n",
      "Average test loss: 0.0022996177565720345\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01794364063607322\n",
      "Average test loss: 0.002386643272721105\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01794474563333723\n",
      "Average test loss: 0.002306862883580228\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017949015827642547\n",
      "Average test loss: 0.002324723590993219\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01794136854675081\n",
      "Average test loss: 0.0022978068786776726\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017933704333172904\n",
      "Average test loss: 0.0022977246302697393\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01793248899281025\n",
      "Average test loss: 0.0024016820068160694\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017922892024119695\n",
      "Average test loss: 0.002301976236825188\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01793050785859426\n",
      "Average test loss: 0.0023385989401075577\n",
      "Epoch 199/300\n",
      "Average training loss: 0.017915733153621355\n",
      "Average test loss: 0.002304130662853519\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01790150124993589\n",
      "Average test loss: 0.0022922630368007553\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017914699390530586\n",
      "Average test loss: 0.0022801553662866354\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017903410464525222\n",
      "Average test loss: 0.0023095012353733183\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017899222198459838\n",
      "Average test loss: 0.002321458253492084\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01788065045575301\n",
      "Average test loss: 0.002289527601045039\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01787002245253987\n",
      "Average test loss: 0.0023421079020740257\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01787903115650018\n",
      "Average test loss: 0.0022902349127042624\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017879748581184282\n",
      "Average test loss: 0.0023726030337727732\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017879942347606023\n",
      "Average test loss: 0.002294675775907106\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017870352210270033\n",
      "Average test loss: 0.002309554762310452\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017862855210900308\n",
      "Average test loss: 0.0023249453366216686\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01785429495738612\n",
      "Average test loss: 0.0022859206313474312\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01786252407398489\n",
      "Average test loss: 0.002333921682400008\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017838719722297457\n",
      "Average test loss: 0.0023081114100706244\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01786727942774693\n",
      "Average test loss: 0.002313202674086723\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01784554832511478\n",
      "Average test loss: 0.0023173383000410267\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017841832547552054\n",
      "Average test loss: 0.0023636483192029925\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017858089284764397\n",
      "Average test loss: 0.0023080967853052747\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01783430636094676\n",
      "Average test loss: 0.0023381698469941816\n",
      "Epoch 219/300\n",
      "Average training loss: 0.017816068664193153\n",
      "Average test loss: 0.0023430299903783535\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017820390396647984\n",
      "Average test loss: 0.002351299034638537\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017825869232416153\n",
      "Average test loss: 0.0025337830957853132\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01780987244347731\n",
      "Average test loss: 0.002373603390322791\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017827298789388605\n",
      "Average test loss: 0.0024152419625057112\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017805656597018243\n",
      "Average test loss: 0.0023867637094938093\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017801504807339774\n",
      "Average test loss: 0.0022869716287694042\n",
      "Epoch 226/300\n",
      "Average training loss: 0.017804513269000583\n",
      "Average test loss: 0.002309075929638412\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01778772159251902\n",
      "Average test loss: 0.002296970025739736\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017797962127460373\n",
      "Average test loss: 0.002312027623669969\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01778963299592336\n",
      "Average test loss: 0.0023152431737010677\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017775505339105924\n",
      "Average test loss: 0.0024806548828879994\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0177738659001059\n",
      "Average test loss: 0.002283209080911345\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01776794020831585\n",
      "Average test loss: 0.0028391642189688152\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017785206803017192\n",
      "Average test loss: 0.0022851926071776286\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01778285517377986\n",
      "Average test loss: 0.0023561720396909447\n",
      "Epoch 235/300\n",
      "Average training loss: 0.017766863918966716\n",
      "Average test loss: 0.002391985812327928\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01777163911031352\n",
      "Average test loss: 0.011073137323061625\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017844369504186844\n",
      "Average test loss: 0.002363189199525449\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01773580868045489\n",
      "Average test loss: 0.002369119712565508\n",
      "Epoch 239/300\n",
      "Average training loss: 0.017748569820490147\n",
      "Average test loss: 0.002332672205236223\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017757685740788776\n",
      "Average test loss: 0.002322461094086369\n",
      "Epoch 241/300\n",
      "Average training loss: 0.017728446286585597\n",
      "Average test loss: 0.002316674647024936\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017749748269716897\n",
      "Average test loss: 0.002314022538045214\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01772973103986846\n",
      "Average test loss: 0.0022849563055982194\n",
      "Epoch 244/300\n",
      "Average training loss: 0.017741861546205148\n",
      "Average test loss: 0.0023124993512820867\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01772758007877403\n",
      "Average test loss: 0.0023096802859670588\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017735835721923245\n",
      "Average test loss: 0.0023468780282677876\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01773413391576873\n",
      "Average test loss: 0.0023321560797178085\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017719170078635216\n",
      "Average test loss: 0.002308388944922222\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01771486661169264\n",
      "Average test loss: 0.002364644438442257\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01772097454716762\n",
      "Average test loss: 0.0024035224430263044\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01772647963050339\n",
      "Average test loss: 0.002343350746979316\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01771601272788313\n",
      "Average test loss: 0.0023335390837035245\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017717112302780152\n",
      "Average test loss: 0.002288990795198414\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01770588855776522\n",
      "Average test loss: 0.002300588394411736\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0176973518182834\n",
      "Average test loss: 0.0023114714103026524\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01770316199544403\n",
      "Average test loss: 0.002334964366008838\n",
      "Epoch 257/300\n",
      "Average training loss: 0.017701740293039215\n",
      "Average test loss: 0.0023121056671564777\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017687339009510146\n",
      "Average test loss: 0.0023179130968120363\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017686285901400776\n",
      "Average test loss: 0.0023102930976698797\n",
      "Epoch 260/300\n",
      "Average training loss: 0.017692386261290972\n",
      "Average test loss: 0.002313137289033168\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017682427095042336\n",
      "Average test loss: 0.002317018571620186\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017709693076709908\n",
      "Average test loss: 0.00236771860325502\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01769292686548498\n",
      "Average test loss: 0.0023325620134257608\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017660496991541652\n",
      "Average test loss: 0.0022962968413614564\n",
      "Epoch 265/300\n",
      "Average training loss: 0.017657847164405716\n",
      "Average test loss: 0.002337166911198033\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01772617314921485\n",
      "Average test loss: 0.0022950582245571745\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017663348578744464\n",
      "Average test loss: 0.002317640600208607\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017662331567870247\n",
      "Average test loss: 0.0022846948300591775\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017658921596076755\n",
      "Average test loss: 0.0023515128133197627\n",
      "Epoch 270/300\n",
      "Average training loss: 0.017661202176991436\n",
      "Average test loss: 0.0023922054652745525\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01765800566309028\n",
      "Average test loss: 0.002305250380602148\n",
      "Epoch 272/300\n",
      "Average training loss: 0.017646134404672515\n",
      "Average test loss: 0.00230375905873047\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01763982885910405\n",
      "Average test loss: 0.0024499814832169148\n",
      "Epoch 274/300\n",
      "Average training loss: 0.017642054551177556\n",
      "Average test loss: 0.002316261793486774\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017647192301849524\n",
      "Average test loss: 0.002334777145749993\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01765035469084978\n",
      "Average test loss: 0.0023609537279440295\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017641217588550515\n",
      "Average test loss: 0.0024068515230384136\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017633935833142864\n",
      "Average test loss: 0.0023429319333905974\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017623196471068595\n",
      "Average test loss: 0.002342163526556558\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01762697148985333\n",
      "Average test loss: 0.002393317138362262\n",
      "Epoch 281/300\n",
      "Average training loss: 0.017628763866921266\n",
      "Average test loss: 0.002292175129780339\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017614281742109193\n",
      "Average test loss: 0.0024724115192269287\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01761701725754473\n",
      "Average test loss: 0.0023461640557895106\n",
      "Epoch 284/300\n",
      "Average training loss: 0.017622987354795137\n",
      "Average test loss: 0.0022948290746038157\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01761913328948948\n",
      "Average test loss: 0.0022993762488994333\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017606589461366336\n",
      "Average test loss: 0.002329786629519529\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017602659804125627\n",
      "Average test loss: 0.0023308054614398213\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017601728815999296\n",
      "Average test loss: 0.002358224456301994\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01761028080019686\n",
      "Average test loss: 0.002310680439799196\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017594166023863687\n",
      "Average test loss: 0.002320496501814988\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017582800015807153\n",
      "Average test loss: 0.0023041089239219823\n",
      "Epoch 292/300\n",
      "Average training loss: 0.017602505701283615\n",
      "Average test loss: 0.0023535258151176903\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017607198341025247\n",
      "Average test loss: 0.00241775543615222\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017580183857017092\n",
      "Average test loss: 0.0023204851142234273\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01758641503337357\n",
      "Average test loss: 0.0023006796733372743\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017581416588690547\n",
      "Average test loss: 0.002290557977433006\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0175795589155621\n",
      "Average test loss: 0.0023214946900390917\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017581433106627728\n",
      "Average test loss: 0.002371313952633904\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017562951715456115\n",
      "Average test loss: 0.0023695033697618377\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01757030163705349\n",
      "Average test loss: 0.0023140077439861165\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.16666171228554513\n",
      "Average test loss: 0.004960285163587994\n",
      "Epoch 2/300\n",
      "Average training loss: 0.038367525001366934\n",
      "Average test loss: 0.004875277598698934\n",
      "Epoch 3/300\n",
      "Average training loss: 0.033926159607039555\n",
      "Average test loss: 0.004687599242975314\n",
      "Epoch 4/300\n",
      "Average training loss: 0.030876461300585006\n",
      "Average test loss: 0.003835612463661366\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02918744489053885\n",
      "Average test loss: 0.003511463946973284\n",
      "Epoch 6/300\n",
      "Average training loss: 0.027256845202710895\n",
      "Average test loss: 0.0033261791958163183\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02624661256869634\n",
      "Average test loss: 0.0032698549278494383\n",
      "Epoch 8/300\n",
      "Average training loss: 0.025062587324115964\n",
      "Average test loss: 0.003061924301605258\n",
      "Epoch 9/300\n",
      "Average training loss: 0.024277720088760057\n",
      "Average test loss: 0.002895148835869299\n",
      "Epoch 10/300\n",
      "Average training loss: 0.023407787633438905\n",
      "Average test loss: 0.0034736061203810903\n",
      "Epoch 11/300\n",
      "Average training loss: 0.022593249946832657\n",
      "Average test loss: 0.00281211543486764\n",
      "Epoch 12/300\n",
      "Average training loss: 0.021955869148174923\n",
      "Average test loss: 0.0025888602323830127\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021349993525279894\n",
      "Average test loss: 0.0025881867100381187\n",
      "Epoch 14/300\n",
      "Average training loss: 0.020967498944865333\n",
      "Average test loss: 0.0024862158209499386\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020442003102766143\n",
      "Average test loss: 0.002473908303098546\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020049420873324075\n",
      "Average test loss: 0.002458818460090293\n",
      "Epoch 17/300\n",
      "Average training loss: 0.019726726597381963\n",
      "Average test loss: 0.0025395938772708178\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0194499777091874\n",
      "Average test loss: 0.0022503524226033023\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01914550729427073\n",
      "Average test loss: 0.002189403986765279\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018881095920999846\n",
      "Average test loss: 0.002218897968530655\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01866560021208392\n",
      "Average test loss: 0.0021902535094155207\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01847975926266776\n",
      "Average test loss: 0.002108077071400152\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018261014976435237\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_50_Depth3/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_50_Depth3/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_50_Depth3/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
