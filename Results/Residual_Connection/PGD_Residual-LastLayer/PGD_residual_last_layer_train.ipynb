{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import LastLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.PGD_Network.PGD import PGD\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Last Layer Loss\n",
    "loss_function = LastLayerLoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.030216071498062876\n",
      "Average test loss: 0.013495782645212279\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011876077108912997\n",
      "Average test loss: 0.013984186187386513\n",
      "Epoch 3/300\n",
      "Average training loss: 0.010629449672996998\n",
      "Average test loss: 0.010253683369192813\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00985692382314139\n",
      "Average test loss: 0.009107875399291516\n",
      "Epoch 5/300\n",
      "Average training loss: 0.009305851405693425\n",
      "Average test loss: 0.008830413696252638\n",
      "Epoch 6/300\n",
      "Average training loss: 0.008891296671496497\n",
      "Average test loss: 0.010088006272912025\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0085738852976097\n",
      "Average test loss: 0.0086790551841259\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008287978194653987\n",
      "Average test loss: 0.008170667827543285\n",
      "Epoch 9/300\n",
      "Average training loss: 0.008068123213739859\n",
      "Average test loss: 0.008803936561776532\n",
      "Epoch 10/300\n",
      "Average training loss: 0.007905487263782157\n",
      "Average test loss: 0.008421867970791128\n",
      "Epoch 11/300\n",
      "Average training loss: 0.007695240029858218\n",
      "Average test loss: 0.008231131735775206\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007520952843129635\n",
      "Average test loss: 0.007761361613869667\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007355559435569578\n",
      "Average test loss: 0.0077263640409542455\n",
      "Epoch 14/300\n",
      "Average training loss: 0.007257707574715217\n",
      "Average test loss: 0.007299783861471547\n",
      "Epoch 15/300\n",
      "Average training loss: 0.007122300980819596\n",
      "Average test loss: 0.007350142406920592\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00699137003471454\n",
      "Average test loss: 0.00821388650023275\n",
      "Epoch 17/300\n",
      "Average training loss: 0.006894339919918113\n",
      "Average test loss: 0.008002486180928019\n",
      "Epoch 18/300\n",
      "Average training loss: 0.006819996559785472\n",
      "Average test loss: 0.007043859588603179\n",
      "Epoch 19/300\n",
      "Average training loss: 0.006725677937269211\n",
      "Average test loss: 0.007117316382626693\n",
      "Epoch 20/300\n",
      "Average training loss: 0.006625271803388993\n",
      "Average test loss: 0.007200415400581227\n",
      "Epoch 21/300\n",
      "Average training loss: 0.006576371772835652\n",
      "Average test loss: 0.007107629468043645\n",
      "Epoch 22/300\n",
      "Average training loss: 0.006456262694464789\n",
      "Average test loss: 0.0070368124523924455\n",
      "Epoch 23/300\n",
      "Average training loss: 0.006391099277883768\n",
      "Average test loss: 0.00758116274078687\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006346335487647189\n",
      "Average test loss: 0.00684087142886387\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00627750837430358\n",
      "Average test loss: 0.006875792264524433\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0062215668385227525\n",
      "Average test loss: 0.00723124608811405\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0061540358559125\n",
      "Average test loss: 0.007488094357980622\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006119326322029034\n",
      "Average test loss: 0.006938731842984756\n",
      "Epoch 29/300\n",
      "Average training loss: 0.006056666858080361\n",
      "Average test loss: 0.006883119248681598\n",
      "Epoch 30/300\n",
      "Average training loss: 0.006013802236566941\n",
      "Average test loss: 0.00685872262592117\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0059492704880734285\n",
      "Average test loss: 0.007373098033997748\n",
      "Epoch 32/300\n",
      "Average training loss: 0.005914070593400134\n",
      "Average test loss: 0.011928234823048115\n",
      "Epoch 33/300\n",
      "Average training loss: 0.005886665019310183\n",
      "Average test loss: 0.006912638200860885\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005841685008671549\n",
      "Average test loss: 0.007014691427763966\n",
      "Epoch 35/300\n",
      "Average training loss: 0.00581173323508766\n",
      "Average test loss: 0.0070765046634607845\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0057851034253835676\n",
      "Average test loss: 0.007181261750559012\n",
      "Epoch 37/300\n",
      "Average training loss: 0.005730745667384731\n",
      "Average test loss: 0.006667532212204404\n",
      "Epoch 38/300\n",
      "Average training loss: 0.005683617413871818\n",
      "Average test loss: 0.007475578133844667\n",
      "Epoch 39/300\n",
      "Average training loss: 0.005663841901554002\n",
      "Average test loss: 0.008278141067259841\n",
      "Epoch 40/300\n",
      "Average training loss: 0.005636861140943236\n",
      "Average test loss: 0.006830369309418731\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0056097539911667505\n",
      "Average test loss: 0.006785976277043422\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005574602044290966\n",
      "Average test loss: 0.006844256353047159\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005535006328175465\n",
      "Average test loss: 0.006640810010333856\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005519866063363022\n",
      "Average test loss: 0.006801652239428626\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005494516732585099\n",
      "Average test loss: 0.006776686295039124\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00546811702930265\n",
      "Average test loss: 0.006704905626674493\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0054348202935523456\n",
      "Average test loss: 0.00669793912395835\n",
      "Epoch 48/300\n",
      "Average training loss: 0.005417121395675672\n",
      "Average test loss: 0.0069809733476075865\n",
      "Epoch 49/300\n",
      "Average training loss: 0.005399493011749453\n",
      "Average test loss: 0.006747159199582206\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0054140512231323455\n",
      "Average test loss: 0.009163651648494933\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00533033966521422\n",
      "Average test loss: 0.006641216820726792\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005320232388046053\n",
      "Average test loss: 0.007138838079240587\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005396854722251495\n",
      "Average test loss: 0.00688283141495453\n",
      "Epoch 54/300\n",
      "Average training loss: 0.005267882959296306\n",
      "Average test loss: 0.007262599919819169\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005258536181930039\n",
      "Average test loss: 0.0069100376317898435\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005255940619442198\n",
      "Average test loss: 0.006764748678025272\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005223897469126516\n",
      "Average test loss: 0.006900255776527855\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005207058185090621\n",
      "Average test loss: 0.00746703973164161\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005193062773181332\n",
      "Average test loss: 0.006591586929642492\n",
      "Epoch 60/300\n",
      "Average training loss: 0.005172909012478259\n",
      "Average test loss: 0.006738551055391629\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005162049478954739\n",
      "Average test loss: 0.0066715738711257775\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005128489809731643\n",
      "Average test loss: 0.0067020313065085145\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005133572001010179\n",
      "Average test loss: 0.006590576846980386\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005139316748413775\n",
      "Average test loss: 0.00674795169217719\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005093308305574788\n",
      "Average test loss: 0.006964538777867953\n",
      "Epoch 66/300\n",
      "Average training loss: 0.005072503593232897\n",
      "Average test loss: 0.006597744312965208\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005057582716561026\n",
      "Average test loss: 0.006726831196910805\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005042492099520233\n",
      "Average test loss: 0.006669957970993386\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005026207671397262\n",
      "Average test loss: 0.006609201767378383\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005022839450587829\n",
      "Average test loss: 0.006722096826881171\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005009601090517309\n",
      "Average test loss: 0.007077761814412144\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005008504098488225\n",
      "Average test loss: 0.006618602407061391\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004969294499605894\n",
      "Average test loss: 0.006982252184715536\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004965610521535079\n",
      "Average test loss: 0.006921321125080188\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004956917049570216\n",
      "Average test loss: 0.006824188543690575\n",
      "Epoch 76/300\n",
      "Average training loss: 0.004935863540818294\n",
      "Average test loss: 0.006706006713211536\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004942139492266708\n",
      "Average test loss: 0.006982277622653379\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004908419221018752\n",
      "Average test loss: 0.00671964401875933\n",
      "Epoch 79/300\n",
      "Average training loss: 0.00491113553278976\n",
      "Average test loss: 0.006651217044227653\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004899095923122433\n",
      "Average test loss: 0.007045129431618584\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004878060477682286\n",
      "Average test loss: 0.007075696811079979\n",
      "Epoch 82/300\n",
      "Average training loss: 0.004894305220908589\n",
      "Average test loss: 0.006667756999532382\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004855500633931822\n",
      "Average test loss: 0.006818626127309269\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0048645381728808085\n",
      "Average test loss: 0.006641257293936279\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004857121606253915\n",
      "Average test loss: 0.0076131679722004466\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004836252656247881\n",
      "Average test loss: 0.006777138176063697\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004830909294800626\n",
      "Average test loss: 0.00969782603945997\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004807575194785993\n",
      "Average test loss: 0.00675474209131466\n",
      "Epoch 89/300\n",
      "Average training loss: 0.004804833924190866\n",
      "Average test loss: 0.006685817030982839\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004810837319327725\n",
      "Average test loss: 0.006856286498821444\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0047817054068048795\n",
      "Average test loss: 0.006808470423022906\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004783384491586023\n",
      "Average test loss: 0.00676580893745025\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004773069131291575\n",
      "Average test loss: 0.006594799228012562\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004762059834682279\n",
      "Average test loss: 0.006656084200574292\n",
      "Epoch 95/300\n",
      "Average training loss: 0.004755004409286711\n",
      "Average test loss: 0.007363987243423859\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004737675329463349\n",
      "Average test loss: 0.006670092762344413\n",
      "Epoch 97/300\n",
      "Average training loss: 0.004735386834376388\n",
      "Average test loss: 0.00661160401875774\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004747448525081078\n",
      "Average test loss: 0.006902478259470728\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004724713231126467\n",
      "Average test loss: 0.007568758370561732\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0047146849313543905\n",
      "Average test loss: 0.006668480825093057\n",
      "Epoch 101/300\n",
      "Average training loss: 0.004731815989232726\n",
      "Average test loss: 0.006714527886774805\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004695746233479845\n",
      "Average test loss: 0.006750549177328745\n",
      "Epoch 103/300\n",
      "Average training loss: 0.004705301721062925\n",
      "Average test loss: 0.007136583983070321\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004683849120719565\n",
      "Average test loss: 0.006825254006104337\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004664432730525732\n",
      "Average test loss: 0.006672535503904025\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004678343139588833\n",
      "Average test loss: 0.00681097702557842\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004658772236357133\n",
      "Average test loss: 0.006654893448783292\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004658036528155208\n",
      "Average test loss: 0.006915127018673552\n",
      "Epoch 109/300\n",
      "Average training loss: 0.004644883496065935\n",
      "Average test loss: 0.12816811231772104\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0046849675128857295\n",
      "Average test loss: 0.006743593172894584\n",
      "Epoch 111/300\n",
      "Average training loss: 0.004635238738316629\n",
      "Average test loss: 0.00729458403835694\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004620381123696764\n",
      "Average test loss: 0.0067755998414423734\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004631426089753707\n",
      "Average test loss: 0.006724134853730599\n",
      "Epoch 114/300\n",
      "Average training loss: 0.004628915440291166\n",
      "Average test loss: 0.007265322527123822\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004612207499436206\n",
      "Average test loss: 0.0069665951165888045\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004613573630650838\n",
      "Average test loss: 0.006687635661827194\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004607823059790664\n",
      "Average test loss: 0.006683107434875435\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004588873513456848\n",
      "Average test loss: 0.007539771674821774\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004703362728158633\n",
      "Average test loss: 0.006811752715872393\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004558669016593032\n",
      "Average test loss: 0.006867641765210364\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004560986068927579\n",
      "Average test loss: 0.006716849947555198\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004569341642989053\n",
      "Average test loss: 0.0067203131978296575\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004569639821226398\n",
      "Average test loss: 0.006753635468582312\n",
      "Epoch 124/300\n",
      "Average training loss: 0.004553072029931678\n",
      "Average test loss: 0.0070976861566305165\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004566133251206742\n",
      "Average test loss: 0.006676457766857412\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004545856840908528\n",
      "Average test loss: 0.007979197351468934\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004547967964990272\n",
      "Average test loss: 0.006724551591194338\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0045409697654346624\n",
      "Average test loss: 0.006712748093323575\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00453226177021861\n",
      "Average test loss: 0.006928296503093507\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00454200891405344\n",
      "Average test loss: 0.006738050304353237\n",
      "Epoch 131/300\n",
      "Average training loss: 0.004531006723642349\n",
      "Average test loss: 0.006654495253331131\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004508739246262445\n",
      "Average test loss: 0.006747957568615675\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004525659856283002\n",
      "Average test loss: 0.006795186106943422\n",
      "Epoch 134/300\n",
      "Average training loss: 0.004506974398261971\n",
      "Average test loss: 0.006643089132590426\n",
      "Epoch 135/300\n",
      "Average training loss: 0.004509325750172139\n",
      "Average test loss: 0.0067196814380586145\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004560837154173189\n",
      "Average test loss: 0.0071065455161862905\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0044823472938603824\n",
      "Average test loss: 0.006699514105088181\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004477887347133623\n",
      "Average test loss: 0.0070424536996417575\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004470173664804962\n",
      "Average test loss: 0.006709816764626238\n",
      "Epoch 140/300\n",
      "Average training loss: 0.004493928312013546\n",
      "Average test loss: 0.007049334584424893\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004488448368385434\n",
      "Average test loss: 0.006711795640074544\n",
      "Epoch 142/300\n",
      "Average training loss: 0.004508037158805463\n",
      "Average test loss: 0.006739423601163758\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0044624540967245895\n",
      "Average test loss: 0.006788854305942853\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004461065816382567\n",
      "Average test loss: 0.006891560501936409\n",
      "Epoch 145/300\n",
      "Average training loss: 0.004464498047613436\n",
      "Average test loss: 0.007041386093944311\n",
      "Epoch 146/300\n",
      "Average training loss: 0.00445269775763154\n",
      "Average test loss: 0.007529231371978919\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004454400120716956\n",
      "Average test loss: 0.006811590543223752\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004447850647899839\n",
      "Average test loss: 0.00679364237934351\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004443783305171463\n",
      "Average test loss: 0.006840915546235111\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004442932195547554\n",
      "Average test loss: 0.006740678610073196\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004426791959338718\n",
      "Average test loss: 0.006897210066103273\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004420441909382741\n",
      "Average test loss: 0.00683751639102896\n",
      "Epoch 153/300\n",
      "Average training loss: 0.00443318251437611\n",
      "Average test loss: 0.006839303731918335\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004413913790550497\n",
      "Average test loss: 0.006787840570012729\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004451058502826425\n",
      "Average test loss: 0.006786044977605343\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004407256223675277\n",
      "Average test loss: 0.006996319465339184\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004409113976690505\n",
      "Average test loss: 0.006846484632955657\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004419981099251243\n",
      "Average test loss: 0.006733160180350145\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004401448664565881\n",
      "Average test loss: 0.007849814305702845\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004410231488446395\n",
      "Average test loss: 0.0068303245670265625\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0043981154954267875\n",
      "Average test loss: 0.007623634333825773\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004387518044147227\n",
      "Average test loss: 0.006797255567378468\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004392199747678307\n",
      "Average test loss: 0.006885235105537706\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004384689876188835\n",
      "Average test loss: 0.00871716963334216\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004388093570247292\n",
      "Average test loss: 0.006888108077976439\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004372289465947284\n",
      "Average test loss: 0.006935890142702394\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004374423344309131\n",
      "Average test loss: 0.007217503727310234\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004450491121245755\n",
      "Average test loss: 0.0068750424790713525\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00436336012929678\n",
      "Average test loss: 0.006742127041849825\n",
      "Epoch 170/300\n",
      "Average training loss: 0.004353127014305857\n",
      "Average test loss: 0.006724448314971394\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0043417195044457915\n",
      "Average test loss: 0.007216007376710574\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004368448795957698\n",
      "Average test loss: 0.00730993171615733\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004346119805135661\n",
      "Average test loss: 0.007340114164683554\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00434645527228713\n",
      "Average test loss: 0.006900173319710626\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004331174333890279\n",
      "Average test loss: 0.008404210802581575\n",
      "Epoch 176/300\n",
      "Average training loss: 0.00447303429701262\n",
      "Average test loss: 0.006862609254403247\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0043348726464642415\n",
      "Average test loss: 0.006749904407809178\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004324851001095441\n",
      "Average test loss: 0.006757535021338198\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004327579920490583\n",
      "Average test loss: 0.006807624138063855\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004332196197989914\n",
      "Average test loss: 0.007041884701285098\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004318958058539364\n",
      "Average test loss: 0.006757639112571876\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004620753337525659\n",
      "Average test loss: 0.00680408214053346\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004302405414068036\n",
      "Average test loss: 0.007100449034737216\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004294392967389689\n",
      "Average test loss: 0.0068150452156033785\n",
      "Epoch 185/300\n",
      "Average training loss: 0.005535790479017628\n",
      "Average test loss: 0.006546940719087919\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004525018065753911\n",
      "Average test loss: 0.006796825668464105\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004313397308190664\n",
      "Average test loss: 0.006846057282553779\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004267352802678943\n",
      "Average test loss: 0.007017263679868645\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0042614553821169665\n",
      "Average test loss: 0.007402005511025588\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004292352209074629\n",
      "Average test loss: 0.0068074221553073985\n",
      "Epoch 191/300\n",
      "Average training loss: 0.00429342274988691\n",
      "Average test loss: 0.0068715116410619686\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004297214121868213\n",
      "Average test loss: 0.006822047791133324\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004300768829675184\n",
      "Average test loss: 0.006822109754714701\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004342586479046278\n",
      "Average test loss: 0.007156789290822215\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0042907987063129744\n",
      "Average test loss: 0.00684154831038581\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004265655025218924\n",
      "Average test loss: 0.006999506209873491\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004283128098895152\n",
      "Average test loss: 0.006830377844058805\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0042791142318811684\n",
      "Average test loss: 0.007095120239588949\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004285719003528357\n",
      "Average test loss: 0.007311521735456255\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004303619136619899\n",
      "Average test loss: 0.006830195691436529\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004273241590294573\n",
      "Average test loss: 0.006760798402130604\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0042662994313157265\n",
      "Average test loss: 0.006952063476045926\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0042661163421968615\n",
      "Average test loss: 0.013371699890328778\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004297117980817953\n",
      "Average test loss: 0.0069975002432862915\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004254682554552952\n",
      "Average test loss: 0.006900450674196084\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0042620383399642175\n",
      "Average test loss: 0.007593630373477936\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004245653212277426\n",
      "Average test loss: 0.007535968584318956\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004281425218201346\n",
      "Average test loss: 0.006986379860175981\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004242050955072046\n",
      "Average test loss: 0.006865439789162742\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004255041408662994\n",
      "Average test loss: 0.007058675916244587\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00425326100198759\n",
      "Average test loss: 0.006980799931204981\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004232236680264274\n",
      "Average test loss: 0.007101513306713767\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00427545274545749\n",
      "Average test loss: 0.006963120961354838\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004233774909128745\n",
      "Average test loss: 0.007025319940514035\n",
      "Epoch 215/300\n",
      "Average training loss: 0.004233996641718679\n",
      "Average test loss: 0.00696048347113861\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004240714961869849\n",
      "Average test loss: 0.006821886953794294\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004220587829748789\n",
      "Average test loss: 0.006900672381950749\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004217247019832333\n",
      "Average test loss: 0.007061148055311706\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00421754068053431\n",
      "Average test loss: 0.006847224686708715\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004237502198252413\n",
      "Average test loss: 0.006806852705776692\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004225339888284604\n",
      "Average test loss: 0.006825566877921422\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004214844038296077\n",
      "Average test loss: 0.007148532454338339\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0042135630906042125\n",
      "Average test loss: 0.006762887019250128\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0042090980141527125\n",
      "Average test loss: 0.006852883088092009\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0042227632432348195\n",
      "Average test loss: 0.007176660371737348\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004200985109433532\n",
      "Average test loss: 0.006894451370255815\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004214253859801425\n",
      "Average test loss: 0.0072385847792029384\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0042052332142161\n",
      "Average test loss: 0.007157020633419354\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004194796392901076\n",
      "Average test loss: 0.0067930699297123484\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0041919386444820295\n",
      "Average test loss: 0.006894621337867445\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004207286555940906\n",
      "Average test loss: 0.007236857372025648\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004193355669577917\n",
      "Average test loss: 0.007113917205896643\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004188104607578781\n",
      "Average test loss: 0.006955942505349716\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004173846479919222\n",
      "Average test loss: 0.007512887592944834\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004205875272966094\n",
      "Average test loss: 0.00797106276700894\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0041836909759375785\n",
      "Average test loss: 0.007014017783105373\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0041842968434923225\n",
      "Average test loss: 0.00694229767140415\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004171282567497756\n",
      "Average test loss: 0.00691144256790479\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004236367804308732\n",
      "Average test loss: 0.0068488562918371625\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0041779540826876954\n",
      "Average test loss: 0.006927544107039769\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004153342554552688\n",
      "Average test loss: 0.006876721565094259\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00416594739879171\n",
      "Average test loss: 0.006812428430550628\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0041836537979543205\n",
      "Average test loss: 0.006812162376112408\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004156130122227801\n",
      "Average test loss: 0.006849998448044061\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0041495847501274614\n",
      "Average test loss: 0.007374885462224484\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004158683151834541\n",
      "Average test loss: 0.006921724084350798\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004153733211672968\n",
      "Average test loss: 0.006803426412244638\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0041742128059268\n",
      "Average test loss: 0.006809760544863012\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004167400517397456\n",
      "Average test loss: 0.007225977486620347\n",
      "Epoch 250/300\n",
      "Average training loss: 0.00413800596114662\n",
      "Average test loss: 0.9499165727827285\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004381666175193257\n",
      "Average test loss: 0.006865269247442484\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0041215576984816125\n",
      "Average test loss: 0.006967820392300685\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004198410034386648\n",
      "Average test loss: 0.006892101539505853\n",
      "Epoch 254/300\n",
      "Average training loss: 0.004139565810975101\n",
      "Average test loss: 0.00715709761944082\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004120315433169404\n",
      "Average test loss: 0.007230882080064879\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004139273538150721\n",
      "Average test loss: 0.006901832353737619\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0041260294429957866\n",
      "Average test loss: 0.006847216811445025\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0041340219928986495\n",
      "Average test loss: 0.006950813192460272\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004133740997148885\n",
      "Average test loss: 0.006845797953506311\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004119653324286143\n",
      "Average test loss: 0.007210516658922037\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004141178339926733\n",
      "Average test loss: 0.0069059208697742885\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004136415491915411\n",
      "Average test loss: 0.007015744493239455\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004114574032939143\n",
      "Average test loss: 0.007149163141432735\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004128882633315192\n",
      "Average test loss: 0.006820982591973411\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004174663348951274\n",
      "Average test loss: 0.006855233460250828\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004121986798321207\n",
      "Average test loss: 0.006826256388591395\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004106011466847526\n",
      "Average test loss: 0.007121483792861302\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004100662851706147\n",
      "Average test loss: 0.006946237617482742\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0041069126394059925\n",
      "Average test loss: 0.006921099433054526\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004121493170244826\n",
      "Average test loss: 0.00714231496097313\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004112076052774986\n",
      "Average test loss: 0.006915833998471499\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004128939138104518\n",
      "Average test loss: 0.00677710122987628\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00411966098472476\n",
      "Average test loss: 0.0069695316350294485\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004091791774456699\n",
      "Average test loss: 0.007148276058336099\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004081298579565353\n",
      "Average test loss: 0.007179342018647327\n",
      "Epoch 276/300\n",
      "Average training loss: 0.004108982278448012\n",
      "Average test loss: 0.007028680425965124\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004108496619181501\n",
      "Average test loss: 0.0069895443701081805\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004093028102897935\n",
      "Average test loss: 0.0071171243815786305\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004090760537112752\n",
      "Average test loss: 0.0070270213141209546\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004095837315130565\n",
      "Average test loss: 0.006919788139561812\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00408470194062425\n",
      "Average test loss: 0.006923789356317785\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0041781840643121135\n",
      "Average test loss: 0.006872822302083174\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004100356632015772\n",
      "Average test loss: 0.006882820403410329\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004071315800150236\n",
      "Average test loss: 0.006899022665702634\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004069302132353187\n",
      "Average test loss: 0.007674649346205924\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004089111411944032\n",
      "Average test loss: 0.006921598116970725\n",
      "Epoch 287/300\n",
      "Average training loss: 0.004077439744853311\n",
      "Average test loss: 0.006987527146935463\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004094399921182129\n",
      "Average test loss: 0.006840823980669181\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004057845023357206\n",
      "Average test loss: 0.007408794390244617\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004074960677160157\n",
      "Average test loss: 0.006938059284041325\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0040795486602518295\n",
      "Average test loss: 0.006919194711579217\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0040770551214615505\n",
      "Average test loss: 0.006882838743014468\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004056601138992442\n",
      "Average test loss: 0.007722931314673689\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004053248265551196\n",
      "Average test loss: 0.006955180009206136\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004090382329912649\n",
      "Average test loss: 0.007267146555913819\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004062957612176736\n",
      "Average test loss: 0.006869410042547517\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004053656571234266\n",
      "Average test loss: 0.006946186689866914\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004045269982268413\n",
      "Average test loss: 0.006966726521237029\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004058730501267645\n",
      "Average test loss: 0.0068556240635613604\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004057469143221776\n",
      "Average test loss: 0.00681877045167817\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02560899813556009\n",
      "Average test loss: 0.009099186030113034\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0084857263026966\n",
      "Average test loss: 0.008283662386238575\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007463758935530981\n",
      "Average test loss: 0.007030093816833363\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006931424229509301\n",
      "Average test loss: 0.00758625606364674\n",
      "Epoch 5/300\n",
      "Average training loss: 0.00652974455886417\n",
      "Average test loss: 0.006239603112969134\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006199583511799574\n",
      "Average test loss: 0.006117473298062881\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005965760522418552\n",
      "Average test loss: 0.005936663395828671\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005734029644479354\n",
      "Average test loss: 0.006575670196778244\n",
      "Epoch 9/300\n",
      "Average training loss: 0.005522083842506012\n",
      "Average test loss: 0.005509386324220234\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005351996786478493\n",
      "Average test loss: 0.00549218510174089\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005119909316301346\n",
      "Average test loss: 0.005716608088049624\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005036993087579806\n",
      "Average test loss: 0.005103813649051719\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004888684584448735\n",
      "Average test loss: 0.005105747707188129\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004760746623078982\n",
      "Average test loss: 0.005588286634948519\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004675013571149773\n",
      "Average test loss: 0.004802365782360236\n",
      "Epoch 16/300\n",
      "Average training loss: 0.004554035418149497\n",
      "Average test loss: 0.004832768215073479\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004511816393170092\n",
      "Average test loss: 0.0050915025402274395\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004420220592990518\n",
      "Average test loss: 0.004607831687562996\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004330942042999797\n",
      "Average test loss: 0.00465207997088631\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004279867354780436\n",
      "Average test loss: 0.004738717327929205\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004214560250855154\n",
      "Average test loss: 0.004571439341124561\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004188131333639224\n",
      "Average test loss: 0.004635032467958\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004101641901251342\n",
      "Average test loss: 0.004364727136782474\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004053056300307314\n",
      "Average test loss: 0.004259318553325203\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004024674260781871\n",
      "Average test loss: 0.004428319359405173\n",
      "Epoch 26/300\n",
      "Average training loss: 0.003972551540782054\n",
      "Average test loss: 0.006357563401675886\n",
      "Epoch 27/300\n",
      "Average training loss: 0.003962675733077857\n",
      "Average test loss: 0.004582654444086883\n",
      "Epoch 28/300\n",
      "Average training loss: 0.003868675008830097\n",
      "Average test loss: 0.0042104518757098254\n",
      "Epoch 29/300\n",
      "Average training loss: 0.003857200261619356\n",
      "Average test loss: 0.004269084523328476\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0038038079897976585\n",
      "Average test loss: 0.0043086385242640975\n",
      "Epoch 31/300\n",
      "Average training loss: 0.003791291504891382\n",
      "Average test loss: 0.004439857703530126\n",
      "Epoch 32/300\n",
      "Average training loss: 0.003759786890198787\n",
      "Average test loss: 0.00428452287055552\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0037656415566388103\n",
      "Average test loss: 0.004269869677308533\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0037002577117333808\n",
      "Average test loss: 0.004345839796380865\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0036658615712076427\n",
      "Average test loss: 0.004213360127475527\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0036490902600602973\n",
      "Average test loss: 0.004199387120289935\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0036374333726449147\n",
      "Average test loss: 0.004322274062782526\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0035899248025897477\n",
      "Average test loss: 0.004438798963609669\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0035977308659089938\n",
      "Average test loss: 0.004200006662557523\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0035513200010690426\n",
      "Average test loss: 0.004562954535500871\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003663672412642174\n",
      "Average test loss: 0.016559633690449927\n",
      "Epoch 42/300\n",
      "Average training loss: 0.005727389073206319\n",
      "Average test loss: 0.004579415401650799\n",
      "Epoch 43/300\n",
      "Average training loss: 0.00423754362637798\n",
      "Average test loss: 0.004241157224608792\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004036237460043696\n",
      "Average test loss: 0.004202618224339353\n",
      "Epoch 45/300\n",
      "Average training loss: 0.003936220841896203\n",
      "Average test loss: 0.004146337472316292\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003870949947999583\n",
      "Average test loss: 0.004057810054057174\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0038368219253089694\n",
      "Average test loss: 0.003975065103007687\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003798907636975249\n",
      "Average test loss: 0.003949427395231194\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0037680399463408524\n",
      "Average test loss: 0.004014263182878494\n",
      "Epoch 50/300\n",
      "Average training loss: 0.003740605713799596\n",
      "Average test loss: 0.003927773713237709\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0037138062475456134\n",
      "Average test loss: 0.004104121738216943\n",
      "Epoch 52/300\n",
      "Average training loss: 0.003695801413514548\n",
      "Average test loss: 0.0038704586322936747\n",
      "Epoch 53/300\n",
      "Average training loss: 0.003662880365426342\n",
      "Average test loss: 0.003925177416660719\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003646418398246169\n",
      "Average test loss: 0.00414178907043404\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003624232735691799\n",
      "Average test loss: 0.003957378566265106\n",
      "Epoch 56/300\n",
      "Average training loss: 0.003610975953646832\n",
      "Average test loss: 0.003925146728340123\n",
      "Epoch 57/300\n",
      "Average training loss: 0.003583533071602384\n",
      "Average test loss: 0.0038352437594698537\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0035639068821652067\n",
      "Average test loss: 0.003920268423234423\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0035395523715350363\n",
      "Average test loss: 0.0038656093200875653\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0035174478696038325\n",
      "Average test loss: 0.004007693194059862\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0035056102940191824\n",
      "Average test loss: 0.003896790766881572\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003476928907757004\n",
      "Average test loss: 0.0038332633297476506\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0034612731867366365\n",
      "Average test loss: 0.004044947834064563\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003452167709875438\n",
      "Average test loss: 0.0038511613661216366\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003424855183603035\n",
      "Average test loss: 0.003934896020632651\n",
      "Epoch 66/300\n",
      "Average training loss: 0.003420064265322354\n",
      "Average test loss: 0.0038995864796969627\n",
      "Epoch 67/300\n",
      "Average training loss: 0.003405513985082507\n",
      "Average test loss: 0.0038840595976346068\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0033801496711870034\n",
      "Average test loss: 0.003861071380890078\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0033647010502301984\n",
      "Average test loss: 0.0038833950476513967\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0033656441422386303\n",
      "Average test loss: 0.003834292519837618\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0033364032780130704\n",
      "Average test loss: 0.003883588028864728\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00332402544365161\n",
      "Average test loss: 0.0038109900433984066\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003319409275841382\n",
      "Average test loss: 0.003995814493546883\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0033028103574696513\n",
      "Average test loss: 0.003882952289448844\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0032855239344967736\n",
      "Average test loss: 0.003850976121922334\n",
      "Epoch 76/300\n",
      "Average training loss: 0.003278213870504664\n",
      "Average test loss: 0.004041955911864837\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0032621169299301175\n",
      "Average test loss: 0.0038386357238309253\n",
      "Epoch 78/300\n",
      "Average training loss: 0.003261811405006382\n",
      "Average test loss: 0.0039209053106606\n",
      "Epoch 79/300\n",
      "Average training loss: 0.003255389664736059\n",
      "Average test loss: 0.0038576018855803543\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0032459939029067754\n",
      "Average test loss: 0.0038034295327961445\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0032329493624468643\n",
      "Average test loss: 0.003994437699309654\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0032094869205935134\n",
      "Average test loss: 0.0038820705885688465\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003203183906773726\n",
      "Average test loss: 0.003969802535035544\n",
      "Epoch 84/300\n",
      "Average training loss: 0.003198179938106073\n",
      "Average test loss: 0.0038400736233840387\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0032069929511182836\n",
      "Average test loss: 0.0038681604315837226\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0031788461280779705\n",
      "Average test loss: 0.0038563224110338425\n",
      "Epoch 87/300\n",
      "Average training loss: 0.003161483605288797\n",
      "Average test loss: 0.0038641509699324766\n",
      "Epoch 88/300\n",
      "Average training loss: 0.003175827630278137\n",
      "Average test loss: 0.0039148390802244346\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003157465811404917\n",
      "Average test loss: 0.0038911066436105304\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0031451330203562974\n",
      "Average test loss: 0.003949324893868632\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003139342819030086\n",
      "Average test loss: 0.003939538493338559\n",
      "Epoch 92/300\n",
      "Average training loss: 0.00312943129427731\n",
      "Average test loss: 0.003975330550223589\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0031241977361755238\n",
      "Average test loss: 0.003943249847739935\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0031154347308393984\n",
      "Average test loss: 0.003932255679948462\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0031130877482808297\n",
      "Average test loss: 0.003974049256079726\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00309114542334444\n",
      "Average test loss: 0.003947078416330947\n",
      "Epoch 97/300\n",
      "Average training loss: 0.003099286706083351\n",
      "Average test loss: 0.003929400380700826\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0030901294709700677\n",
      "Average test loss: 0.0040301554908769\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0030824940328796706\n",
      "Average test loss: 0.003924042639219099\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003073768148198724\n",
      "Average test loss: 0.0038501583731008903\n",
      "Epoch 101/300\n",
      "Average training loss: 0.003073680577178796\n",
      "Average test loss: 0.003894418586873346\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0030549919717013834\n",
      "Average test loss: 0.003883961804625061\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0030459590829494928\n",
      "Average test loss: 0.003928828736767173\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0030491049575308957\n",
      "Average test loss: 0.003911057020641036\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0030395589270111587\n",
      "Average test loss: 0.003939531560987234\n",
      "Epoch 106/300\n",
      "Average training loss: 0.003036700911406014\n",
      "Average test loss: 0.003937981682519118\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0030328539171152647\n",
      "Average test loss: 0.004074074136921101\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0030227563811673058\n",
      "Average test loss: 0.0040075975950393415\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0030312983474383754\n",
      "Average test loss: 0.003831217161483235\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0030122323241084815\n",
      "Average test loss: 0.003968267890314261\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0030061237787206967\n",
      "Average test loss: 0.003949510481001602\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0029945653749422893\n",
      "Average test loss: 0.003917221208827363\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0029979420229792595\n",
      "Average test loss: 0.003872369636264112\n",
      "Epoch 114/300\n",
      "Average training loss: 0.003002881080740028\n",
      "Average test loss: 0.004026497924493419\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0029907698014544114\n",
      "Average test loss: 0.003922616872936487\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0029805592900762957\n",
      "Average test loss: 0.0038749702034725085\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0029727531580461394\n",
      "Average test loss: 0.004019979573786259\n",
      "Epoch 118/300\n",
      "Average training loss: 0.002971577134811216\n",
      "Average test loss: 0.0038743713456723425\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002969324012183481\n",
      "Average test loss: 0.003881080097415381\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0029575985181662773\n",
      "Average test loss: 0.00512284593987796\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0029523027978009646\n",
      "Average test loss: 0.003948130364840229\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002956081621141897\n",
      "Average test loss: 0.0041366406853000325\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002944309509669741\n",
      "Average test loss: 0.003910022760844893\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0029449401864161096\n",
      "Average test loss: 0.003898409972588221\n",
      "Epoch 125/300\n",
      "Average training loss: 0.002948519113784035\n",
      "Average test loss: 0.004034572465966145\n",
      "Epoch 126/300\n",
      "Average training loss: 0.002927659097645018\n",
      "Average test loss: 0.003969347389207946\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002926496661371655\n",
      "Average test loss: 0.003988657573444976\n",
      "Epoch 128/300\n",
      "Average training loss: 0.002933298732050591\n",
      "Average test loss: 0.003972702686571412\n",
      "Epoch 129/300\n",
      "Average training loss: 0.002919837601896789\n",
      "Average test loss: 0.003998580779880285\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002933430302474234\n",
      "Average test loss: 0.004007655282815298\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0029053061538272433\n",
      "Average test loss: 0.004031249089166522\n",
      "Epoch 132/300\n",
      "Average training loss: 0.00291746626711554\n",
      "Average test loss: 0.003964256025022931\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0029086246341466903\n",
      "Average test loss: 0.003968137505360776\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002897631141046683\n",
      "Average test loss: 0.003982912133137385\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002901604779685537\n",
      "Average test loss: 0.003917452626551191\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0028948221730275285\n",
      "Average test loss: 0.003998625303308169\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002898876286836134\n",
      "Average test loss: 0.004023473216841618\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002881209921091795\n",
      "Average test loss: 0.004429283874316348\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0028910803964568507\n",
      "Average test loss: 0.00391410367604759\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0028709107507020235\n",
      "Average test loss: 0.00395553110829658\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0028725262789262664\n",
      "Average test loss: 0.004420925442957216\n",
      "Epoch 142/300\n",
      "Average training loss: 0.002879410144252082\n",
      "Average test loss: 0.004118369334687789\n",
      "Epoch 143/300\n",
      "Average training loss: 0.002879024564392037\n",
      "Average test loss: 0.003998233429673645\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0028614970315247773\n",
      "Average test loss: 0.00396122066800793\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002870244084753924\n",
      "Average test loss: 0.0040718916182716685\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002866792211102115\n",
      "Average test loss: 0.003929791933546463\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0028532983646210697\n",
      "Average test loss: 0.004019272786047723\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002860896150685019\n",
      "Average test loss: 0.003920309585001734\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0028555497073878846\n",
      "Average test loss: 0.004415634785675341\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002844756089357866\n",
      "Average test loss: 0.003942093600208561\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002842358833178878\n",
      "Average test loss: 0.003907487984953655\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0028389631162087123\n",
      "Average test loss: 0.003946341297278801\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0028346272580739526\n",
      "Average test loss: 0.003910809958146678\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0028410916189766594\n",
      "Average test loss: 0.003977312996155686\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002832878217308058\n",
      "Average test loss: 0.004063990976247523\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0028249497061802282\n",
      "Average test loss: 0.003935871605450909\n",
      "Epoch 157/300\n",
      "Average training loss: 0.002898232465609908\n",
      "Average test loss: 0.004003252999443147\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0028144541624933483\n",
      "Average test loss: 0.003992670215252373\n",
      "Epoch 159/300\n",
      "Average training loss: 0.002814054591374265\n",
      "Average test loss: 0.0040629937350749965\n",
      "Epoch 160/300\n",
      "Average training loss: 0.002819037770645486\n",
      "Average test loss: 0.003984212377419074\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0028178624796370666\n",
      "Average test loss: 0.004294248794929849\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0028196240932577186\n",
      "Average test loss: 0.003931593689653608\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0028148491208751995\n",
      "Average test loss: 0.004033751830665602\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002814040212581555\n",
      "Average test loss: 0.003962301258204712\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0028056713240221143\n",
      "Average test loss: 0.004008024799327056\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002801402457886272\n",
      "Average test loss: 0.004256696470081806\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0028059233178695043\n",
      "Average test loss: 0.003977197204613024\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002789721322763297\n",
      "Average test loss: 0.0039525685587690935\n",
      "Epoch 169/300\n",
      "Average training loss: 0.002794723617637323\n",
      "Average test loss: 0.00412246889496843\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00279539979632116\n",
      "Average test loss: 0.0041033160235318875\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002786236119353109\n",
      "Average test loss: 0.00395773852440632\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0027902986821201113\n",
      "Average test loss: 0.004028007695658339\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00278868936188519\n",
      "Average test loss: 0.004003215666653382\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0027770182091949713\n",
      "Average test loss: 0.004072115986090567\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0027809185485045116\n",
      "Average test loss: 0.003969177060864038\n",
      "Epoch 176/300\n",
      "Average training loss: 0.002775705097657111\n",
      "Average test loss: 0.004032882145709462\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0027727859318256378\n",
      "Average test loss: 0.004065198480255074\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0027757054180320767\n",
      "Average test loss: 0.0040339176650676465\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0027671232248346013\n",
      "Average test loss: 0.0040815494114326105\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0027692667598732644\n",
      "Average test loss: 0.004057682680586974\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0027669427078217267\n",
      "Average test loss: 0.004056905190149943\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0027758176753090486\n",
      "Average test loss: 0.004136413918187221\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0027653992131559386\n",
      "Average test loss: 0.004040765058456196\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0027524387503249778\n",
      "Average test loss: 0.004021933923371964\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0027577937321944367\n",
      "Average test loss: 0.004061669310761823\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0027598146481646432\n",
      "Average test loss: 0.004133221846073866\n",
      "Epoch 187/300\n",
      "Average training loss: 0.00274837230063147\n",
      "Average test loss: 0.0040256555382576255\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0027504046534498534\n",
      "Average test loss: 0.003978725824919012\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00274508844046957\n",
      "Average test loss: 0.003961939082377487\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0027511075043843854\n",
      "Average test loss: 0.003996043088121547\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0027545611222998963\n",
      "Average test loss: 0.0043524364079866144\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0027417809733500083\n",
      "Average test loss: 0.003996743972102801\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0027484200455041396\n",
      "Average test loss: 0.00408207486404313\n",
      "Epoch 194/300\n",
      "Average training loss: 0.002741671719070938\n",
      "Average test loss: 0.004150058381673363\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0027360932955311406\n",
      "Average test loss: 0.004063823334251841\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0027360918259041176\n",
      "Average test loss: 0.0040494232835868995\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0027354038268741633\n",
      "Average test loss: 0.004130347677816947\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0027291723303496836\n",
      "Average test loss: 0.004018651913437578\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002729711958931552\n",
      "Average test loss: 0.004111624733648366\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0027231145445257426\n",
      "Average test loss: 0.004105155654251576\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0027271778715981377\n",
      "Average test loss: 0.004033282909335361\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0027222832633803287\n",
      "Average test loss: 0.004111679933965206\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0027219591496719254\n",
      "Average test loss: 0.004082858633663919\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0027173155716930827\n",
      "Average test loss: 0.0040572993035117785\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0027236243852724632\n",
      "Average test loss: 0.003990172824098004\n",
      "Epoch 206/300\n",
      "Average training loss: 0.002725583448385199\n",
      "Average test loss: 0.004095956876460049\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0027169028646830055\n",
      "Average test loss: 0.003999108446968927\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002714191577707728\n",
      "Average test loss: 0.004033077144995332\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0027060041228930153\n",
      "Average test loss: 0.004052852933191591\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0027076982299072874\n",
      "Average test loss: 0.004089078613246481\n",
      "Epoch 211/300\n",
      "Average training loss: 0.002706595558466183\n",
      "Average test loss: 0.0041324661626584\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0027056824314511483\n",
      "Average test loss: 0.003974840801209211\n",
      "Epoch 213/300\n",
      "Average training loss: 0.002698982499953773\n",
      "Average test loss: 0.004094873160330786\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0027077621217403147\n",
      "Average test loss: 0.0040913029383454055\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0026984540451731945\n",
      "Average test loss: 0.004110408219198386\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0026937714885506363\n",
      "Average test loss: 0.003996256504207849\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0026940360121015046\n",
      "Average test loss: 0.004051805866675244\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0027032661248619356\n",
      "Average test loss: 0.004284343713273605\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002693943097152644\n",
      "Average test loss: 0.004892428347633945\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002693216027485\n",
      "Average test loss: 0.004100335597991943\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0026854088542362054\n",
      "Average test loss: 0.0040532242196301615\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002699964402864377\n",
      "Average test loss: 0.004230540985862414\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0026878869626671075\n",
      "Average test loss: 0.004025302306231525\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0026916327224009566\n",
      "Average test loss: 0.004002171309043964\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002688789813261893\n",
      "Average test loss: 0.004179820391452975\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0026742931436747313\n",
      "Average test loss: 0.004002415458775229\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0026783543357418644\n",
      "Average test loss: 0.004752251466529237\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002687920951594909\n",
      "Average test loss: 0.004008146219369438\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002676726684802108\n",
      "Average test loss: 0.004218076764295499\n",
      "Epoch 230/300\n",
      "Average training loss: 0.002680594198819664\n",
      "Average test loss: 0.004088121900541915\n",
      "Epoch 231/300\n",
      "Average training loss: 0.002675624778494239\n",
      "Average test loss: 0.004052144687622786\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002674457926303148\n",
      "Average test loss: 0.004024369767349627\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0026740813280145327\n",
      "Average test loss: 0.00411481053257982\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0026644321013655926\n",
      "Average test loss: 0.0040418056775298385\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0026703457619167035\n",
      "Average test loss: 0.004126434222070707\n",
      "Epoch 236/300\n",
      "Average training loss: 0.00266986788643731\n",
      "Average test loss: 0.004078984003514052\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0026591962600747746\n",
      "Average test loss: 0.00403953435478939\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0026695117186754944\n",
      "Average test loss: 0.004526396337068743\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0026641821385257774\n",
      "Average test loss: 0.004107060311983029\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0026539123536398015\n",
      "Average test loss: 0.0040883111668129765\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0026604777455536857\n",
      "Average test loss: 0.004124286994131075\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00266134845941431\n",
      "Average test loss: 0.0040673886899732885\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0026564766683926187\n",
      "Average test loss: 0.004084469951482283\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002658761667708556\n",
      "Average test loss: 0.004018260973609156\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0026497343685477973\n",
      "Average test loss: 0.004073299092758033\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002644425259282192\n",
      "Average test loss: 0.004121010428087579\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0026565979686048297\n",
      "Average test loss: 0.00397461340100401\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0026497928098671965\n",
      "Average test loss: 0.0040289766618775\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0026521146297454833\n",
      "Average test loss: 0.003999614740411441\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0026493330157051484\n",
      "Average test loss: 0.0040936785269942546\n",
      "Epoch 251/300\n",
      "Average training loss: 0.002649205981236365\n",
      "Average test loss: 0.004211316022608015\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002643704145732853\n",
      "Average test loss: 0.004108201146539715\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0026509723698513376\n",
      "Average test loss: 0.004147729137291511\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0026471576783806084\n",
      "Average test loss: 0.004093425261063708\n",
      "Epoch 255/300\n",
      "Average training loss: 0.002643447229845656\n",
      "Average test loss: 0.004108782085279624\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0026448708977550267\n",
      "Average test loss: 0.004119658706709743\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0026428558623625173\n",
      "Average test loss: 0.004096518865476052\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0026358011627776756\n",
      "Average test loss: 0.004140438969350523\n",
      "Epoch 259/300\n",
      "Average training loss: 0.002635689959136976\n",
      "Average test loss: 0.004069332013113631\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002634811565279961\n",
      "Average test loss: 0.004084634071009027\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0026363606384644904\n",
      "Average test loss: 0.00411331875094523\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00263558954352306\n",
      "Average test loss: 0.00408015140104625\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0026329768500808213\n",
      "Average test loss: 0.004110933852278524\n",
      "Epoch 264/300\n",
      "Average training loss: 0.002624082242242164\n",
      "Average test loss: 0.004002608543468846\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0026372036238511403\n",
      "Average test loss: 0.004707691785568993\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0026240566370801795\n",
      "Average test loss: 0.004299559616794189\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0026345907704283795\n",
      "Average test loss: 0.004160136024571127\n",
      "Epoch 268/300\n",
      "Average training loss: 0.002622714747571283\n",
      "Average test loss: 0.004195807798248199\n",
      "Epoch 269/300\n",
      "Average training loss: 0.00261454109268056\n",
      "Average test loss: 0.00405039025677575\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0026252335136135417\n",
      "Average test loss: 0.0041471280505259835\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0026179638096234864\n",
      "Average test loss: 0.004149993955675098\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0026197945926752357\n",
      "Average test loss: 0.0041431225558949845\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002617556191980839\n",
      "Average test loss: 0.004033582197088334\n",
      "Epoch 274/300\n",
      "Average training loss: 0.00262167054735538\n",
      "Average test loss: 0.004151039205284582\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002612713840686613\n",
      "Average test loss: 0.0041512927400569125\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0026126102283596992\n",
      "Average test loss: 0.004066033157830437\n",
      "Epoch 277/300\n",
      "Average training loss: 0.002617285506799817\n",
      "Average test loss: 0.004337117308957709\n",
      "Epoch 278/300\n",
      "Average training loss: 0.002606890029998289\n",
      "Average test loss: 0.004101854126900435\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0026110848726497754\n",
      "Average test loss: 0.0040750353054867855\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0026146787938972314\n",
      "Average test loss: 0.004078687950554822\n",
      "Epoch 281/300\n",
      "Average training loss: 0.002632265664430128\n",
      "Average test loss: 0.004056734914994902\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0026043197297387654\n",
      "Average test loss: 0.004049824639326996\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002602266807316078\n",
      "Average test loss: 0.004159143940856059\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0025970913415981666\n",
      "Average test loss: 0.004092507854931884\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0026089621095193756\n",
      "Average test loss: 0.004064592963291539\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002594584652326173\n",
      "Average test loss: 0.0040332554243505\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0026052439299722513\n",
      "Average test loss: 0.004048847832406561\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0025961044350018103\n",
      "Average test loss: 0.004150311740322246\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0025945854474686914\n",
      "Average test loss: 0.0040972757244275675\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0025965545245756707\n",
      "Average test loss: 0.004520927371043298\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0026034776711215575\n",
      "Average test loss: 0.004087050271530946\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0026023238758660027\n",
      "Average test loss: 0.004066989926828278\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0025937114169614184\n",
      "Average test loss: 0.004947583232902818\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0025923653474698464\n",
      "Average test loss: 0.004115410398277971\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002595573919928736\n",
      "Average test loss: 0.004075446737930179\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0025916434114591942\n",
      "Average test loss: 0.00414987584327658\n",
      "Epoch 297/300\n",
      "Average training loss: 0.002588285793239872\n",
      "Average test loss: 0.004255775405714909\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0025940607596809666\n",
      "Average test loss: 0.004119893472848667\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0025908621864186394\n",
      "Average test loss: 0.004117616611843308\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002580265950618519\n",
      "Average test loss: 0.004218198499745793\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02155428411728806\n",
      "Average test loss: 0.0076958011959989865\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0062622661619550655\n",
      "Average test loss: 0.005872435150874986\n",
      "Epoch 3/300\n",
      "Average training loss: 0.005385360204097298\n",
      "Average test loss: 0.004958448233703772\n",
      "Epoch 4/300\n",
      "Average training loss: 0.004914367847144604\n",
      "Average test loss: 0.0051065963891645276\n",
      "Epoch 5/300\n",
      "Average training loss: 0.004575719597645932\n",
      "Average test loss: 0.00442684846267932\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004267026033045517\n",
      "Average test loss: 0.004235241855805119\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004081419980360402\n",
      "Average test loss: 0.004242665771808889\n",
      "Epoch 8/300\n",
      "Average training loss: 0.003907183476206329\n",
      "Average test loss: 0.0038847726515183847\n",
      "Epoch 9/300\n",
      "Average training loss: 0.003749987927575906\n",
      "Average test loss: 0.003966351354701652\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003635777577964796\n",
      "Average test loss: 0.006076959759824806\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0035180614495442975\n",
      "Average test loss: 0.004357885212534004\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0033919988717469904\n",
      "Average test loss: 0.0035260875696937243\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0032928759118335113\n",
      "Average test loss: 0.003313413326525026\n",
      "Epoch 14/300\n",
      "Average training loss: 0.003182351929239101\n",
      "Average test loss: 0.003211487147750126\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0030963229435599514\n",
      "Average test loss: 0.0031874853043506544\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0030361311520553298\n",
      "Average test loss: 0.0032031571854733757\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0029651317809604935\n",
      "Average test loss: 0.0033682491572366823\n",
      "Epoch 18/300\n",
      "Average training loss: 0.002905649490033587\n",
      "Average test loss: 0.003056581091342701\n",
      "Epoch 19/300\n",
      "Average training loss: 0.002878068600470821\n",
      "Average test loss: 0.009512533435391055\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0028159415386617183\n",
      "Average test loss: 0.0029953846109824047\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0027704048841777776\n",
      "Average test loss: 0.0029162060307959714\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0027372180128263104\n",
      "Average test loss: 0.0031914361990574335\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0026969680088675682\n",
      "Average test loss: 0.002876538807940152\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002668149890585078\n",
      "Average test loss: 0.0028716646952347625\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0026729455770303804\n",
      "Average test loss: 0.0028120252953635324\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0026136982249509956\n",
      "Average test loss: 0.002867365210834477\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0025842823948090273\n",
      "Average test loss: 0.003149361347572671\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0025779594210907815\n",
      "Average test loss: 0.0028844177104118797\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0025424735722028545\n",
      "Average test loss: 0.0027718506331245105\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0025260031850387653\n",
      "Average test loss: 0.00278872025737332\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0025024263720131584\n",
      "Average test loss: 0.00280178348099192\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002494686419247753\n",
      "Average test loss: 0.00273715353757143\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008626507174223662\n",
      "Average test loss: 0.006813668411639002\n",
      "Epoch 34/300\n",
      "Average training loss: 0.005798869635909796\n",
      "Average test loss: 0.005366943848629792\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004892406129174762\n",
      "Average test loss: 0.004624838272109627\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0044526684304906266\n",
      "Average test loss: 0.004396522885395421\n",
      "Epoch 37/300\n",
      "Average training loss: 0.004073388534701533\n",
      "Average test loss: 0.003967347058984968\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0037477546454303796\n",
      "Average test loss: 0.003624179806560278\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003502753858351045\n",
      "Average test loss: 0.0034732614449328844\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0033570958328329853\n",
      "Average test loss: 0.003446364762675431\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0032325898425446618\n",
      "Average test loss: 0.0032951482348144052\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0031387359439912768\n",
      "Average test loss: 0.0031839688070532347\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003058290290335814\n",
      "Average test loss: 0.0031200098368442722\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0029784388386954862\n",
      "Average test loss: 0.0030590478357755474\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0029084976681818565\n",
      "Average test loss: 0.0030901160848637423\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0028330244585457777\n",
      "Average test loss: 0.002943232028434674\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0027697817432797618\n",
      "Average test loss: 0.002868328420031402\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00271188528670205\n",
      "Average test loss: 0.0028521594579021137\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0027414708514180448\n",
      "Average test loss: 0.0027935198201901384\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0026565050344086356\n",
      "Average test loss: 0.0027655420473052398\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0026005074970631135\n",
      "Average test loss: 0.0027742453441023827\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002571558848851257\n",
      "Average test loss: 0.0027089098774724534\n",
      "Epoch 53/300\n",
      "Average training loss: 0.002547375191623966\n",
      "Average test loss: 0.0027039675182766384\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0025868095939771997\n",
      "Average test loss: 0.0027496541618473\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002514245110874375\n",
      "Average test loss: 0.0026723716074807777\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0024786778720509673\n",
      "Average test loss: 0.0026631079643136923\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0024597942411071724\n",
      "Average test loss: 0.0026999726047118506\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0024381986584307417\n",
      "Average test loss: 0.0026876919807659254\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0024182324688881637\n",
      "Average test loss: 0.002630260775072707\n",
      "Epoch 60/300\n",
      "Average training loss: 0.002406034174685677\n",
      "Average test loss: 0.002836599738854501\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0024360433374014164\n",
      "Average test loss: 0.0026449366979714897\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002367019063068761\n",
      "Average test loss: 0.0026585373741885025\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002359988326724205\n",
      "Average test loss: 0.0027651679124683142\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002353476464955343\n",
      "Average test loss: 0.0027413183212694195\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002340787778505021\n",
      "Average test loss: 0.002719071735110548\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002325244294479489\n",
      "Average test loss: 0.002638929315118326\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00233984938574334\n",
      "Average test loss: 0.00262718171108928\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0023399540786114004\n",
      "Average test loss: 0.002638117897634705\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002293088902305398\n",
      "Average test loss: 0.002614360816569792\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0023001681379973887\n",
      "Average test loss: 0.002607190353795886\n",
      "Epoch 71/300\n",
      "Average training loss: 0.002267919443651206\n",
      "Average test loss: 0.002638372061567174\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002259293441557222\n",
      "Average test loss: 0.0026146103259589936\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0022506419471982455\n",
      "Average test loss: 0.002605960541508264\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0022571657669420045\n",
      "Average test loss: 0.002650445385629104\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002234731840590636\n",
      "Average test loss: 0.0026021942446629205\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0022638707836675977\n",
      "Average test loss: 0.002674555606312222\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002220106849550373\n",
      "Average test loss: 0.0026590459905564785\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0022126390710473062\n",
      "Average test loss: 0.002750785163293282\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002204068997564415\n",
      "Average test loss: 0.00263022339095672\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0022358979184387458\n",
      "Average test loss: 0.00258370644847552\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002494686344224546\n",
      "Average test loss: 0.0026134795546531677\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0022610103152692317\n",
      "Average test loss: 0.0026025938182655307\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0021842298346261184\n",
      "Average test loss: 0.002596616678353813\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0021675318218767644\n",
      "Average test loss: 0.0026098767895665433\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0021707633489535913\n",
      "Average test loss: 0.002680539680230949\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002174089365845753\n",
      "Average test loss: 0.0026646970474264687\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0021768297783823476\n",
      "Average test loss: 0.002644945714208815\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002162968930684858\n",
      "Average test loss: 0.0026353484992351797\n",
      "Epoch 89/300\n",
      "Average training loss: 0.002158529287100666\n",
      "Average test loss: 0.0027143093635224633\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002151800289336178\n",
      "Average test loss: 0.0025926220901310443\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0021358268190589218\n",
      "Average test loss: 0.0026352483568092186\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0021357162561681537\n",
      "Average test loss: 0.0026455861201716796\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0021331509145804577\n",
      "Average test loss: 0.00263499951052169\n",
      "Epoch 94/300\n",
      "Average training loss: 0.002138177903058628\n",
      "Average test loss: 0.002663437831526001\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0021173163793153232\n",
      "Average test loss: 0.002672237481094069\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0021188199433187643\n",
      "Average test loss: 0.0026394063443359404\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0021402917672983476\n",
      "Average test loss: 0.0026161749344319105\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0021031150113170346\n",
      "Average test loss: 0.002611009196481771\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002246814681010114\n",
      "Average test loss: 0.00263719157088134\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0021027827186303008\n",
      "Average test loss: 0.0026071015345967476\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0020880284410797886\n",
      "Average test loss: 0.0026982906425578727\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0021166708359701765\n",
      "Average test loss: 0.0026841636271112495\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002079388801008463\n",
      "Average test loss: 0.0026905438220128416\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002079987430738078\n",
      "Average test loss: 0.0037625944800674916\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002071644770912826\n",
      "Average test loss: 0.0026614492550078367\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0020691881225340897\n",
      "Average test loss: 0.002761638815100822\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002071097160068651\n",
      "Average test loss: 0.0026340967151853773\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0020656220311712887\n",
      "Average test loss: 0.0026973510405255688\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0020668623499158355\n",
      "Average test loss: 0.0026781432963907718\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0020693734508628646\n",
      "Average test loss: 0.0026348318415176535\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002078298007034593\n",
      "Average test loss: 0.0029448401077340047\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0020562678957358003\n",
      "Average test loss: 0.002812595419689185\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002036790919697119\n",
      "Average test loss: 0.0026751778775619137\n",
      "Epoch 114/300\n",
      "Average training loss: 0.002045667162578967\n",
      "Average test loss: 0.0026484205859402814\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0020354787998108402\n",
      "Average test loss: 0.002663541358585159\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0021422120560374526\n",
      "Average test loss: 0.0027504659026033346\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0020500518430231346\n",
      "Average test loss: 0.0026746286133097277\n",
      "Epoch 118/300\n",
      "Average training loss: 0.002017056999521123\n",
      "Average test loss: 0.0026884336376355755\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002012083121264974\n",
      "Average test loss: 0.002705351017622484\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0020465383377547067\n",
      "Average test loss: 0.002755780250661903\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0020112591745952767\n",
      "Average test loss: 0.0027134030904206966\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0020120549384090637\n",
      "Average test loss: 0.0026357556444903214\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002016755754024618\n",
      "Average test loss: 0.002653708128051625\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002011549832299352\n",
      "Average test loss: 0.002718783243662781\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0020068194893085294\n",
      "Average test loss: 0.0026427785300960145\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0020008902810513974\n",
      "Average test loss: 0.002600394727455245\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0020190884014591576\n",
      "Average test loss: 0.0026329617062583566\n",
      "Epoch 128/300\n",
      "Average training loss: 0.001990033155514134\n",
      "Average test loss: 0.0026799178721590173\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0020006240509553917\n",
      "Average test loss: 0.0028604288952839042\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019880647321956026\n",
      "Average test loss: 0.0026658008384207884\n",
      "Epoch 131/300\n",
      "Average training loss: 0.001993244088358349\n",
      "Average test loss: 0.016316683206293318\n",
      "Epoch 132/300\n",
      "Average training loss: 0.003182365362221996\n",
      "Average test loss: 0.0028136666050801677\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0024424982625577186\n",
      "Average test loss: 0.0026508930317229694\n",
      "Epoch 134/300\n",
      "Average training loss: 0.002282651498913765\n",
      "Average test loss: 0.002627329664718774\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0021890646858761706\n",
      "Average test loss: 0.0025987048202918634\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0021205470049753786\n",
      "Average test loss: 0.0026429378075732124\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002070633410372668\n",
      "Average test loss: 0.002644271852241622\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002024258296419349\n",
      "Average test loss: 0.002618137556128204\n",
      "Epoch 139/300\n",
      "Average training loss: 0.001997141861667236\n",
      "Average test loss: 0.0027497004504419037\n",
      "Epoch 140/300\n",
      "Average training loss: 0.001981112933407227\n",
      "Average test loss: 0.002702200564245383\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001969070017441279\n",
      "Average test loss: 0.0027102981509847775\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0019648790358462267\n",
      "Average test loss: 0.002652122451820307\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0019657653452207643\n",
      "Average test loss: 0.0033669372389300003\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001989247183729377\n",
      "Average test loss: 0.002711271482623286\n",
      "Epoch 145/300\n",
      "Average training loss: 0.001961403074105167\n",
      "Average test loss: 0.0027728213798254727\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0019574906026116677\n",
      "Average test loss: 0.002671591265954905\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0019635116793215277\n",
      "Average test loss: 0.0027113550959361925\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0019616250834531253\n",
      "Average test loss: 0.0027036071887446772\n",
      "Epoch 149/300\n",
      "Average training loss: 0.001953527845752736\n",
      "Average test loss: 0.0027352811111551194\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0019749560005341968\n",
      "Average test loss: 0.002706548146903515\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0019668220905587077\n",
      "Average test loss: 0.0027485126534269916\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0019461144994323452\n",
      "Average test loss: 0.002710874966863129\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0019504194984005557\n",
      "Average test loss: 0.002716932557730211\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0019413866561113133\n",
      "Average test loss: 0.0027697753605122367\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0019466367584342757\n",
      "Average test loss: 0.002719023435066144\n",
      "Epoch 156/300\n",
      "Average training loss: 0.001951484312199884\n",
      "Average test loss: 0.0028660041464285716\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0019556268383748828\n",
      "Average test loss: 0.0028422073498368264\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0019332855133753683\n",
      "Average test loss: 0.00271619054954499\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0019302850123494864\n",
      "Average test loss: 0.0027857821968694527\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0019332114346325398\n",
      "Average test loss: 0.0026606593875007495\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0019269029036578205\n",
      "Average test loss: 0.00271840304777854\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0019277493323509893\n",
      "Average test loss: 0.002708470314120253\n",
      "Epoch 163/300\n",
      "Average training loss: 0.001926898937775857\n",
      "Average test loss: 0.002712091232546502\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0019199610924762157\n",
      "Average test loss: 0.0027276526944090924\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0019521278007369903\n",
      "Average test loss: 0.0026925208758976724\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0019162958345065515\n",
      "Average test loss: 0.0027217237814846965\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0019136437840966715\n",
      "Average test loss: 0.0032563623307893673\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0019134178875635068\n",
      "Average test loss: 0.0027344723757770327\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0019140849698127973\n",
      "Average test loss: 0.0027545475632780128\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0019085543718602921\n",
      "Average test loss: 0.002729042049186925\n",
      "Epoch 171/300\n",
      "Average training loss: 0.001901522927503619\n",
      "Average test loss: 0.0027608554802007147\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0019477509822075567\n",
      "Average test loss: 0.0027549237127726276\n",
      "Epoch 173/300\n",
      "Average training loss: 0.001899771952173776\n",
      "Average test loss: 0.002714042029240065\n",
      "Epoch 174/300\n",
      "Average training loss: 0.001915039254973332\n",
      "Average test loss: 0.002716120275358359\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0018953400041080183\n",
      "Average test loss: 0.002702087233050002\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0018986222333171301\n",
      "Average test loss: 0.0027233464643359184\n",
      "Epoch 177/300\n",
      "Average training loss: 0.001892954981368449\n",
      "Average test loss: 0.002709052564472788\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0018874821719816989\n",
      "Average test loss: 0.002683488426109155\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0018911701222467754\n",
      "Average test loss: 0.002724259420608481\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002022925661773317\n",
      "Average test loss: 0.002628632268557946\n",
      "Epoch 181/300\n",
      "Average training loss: 0.001973384309663541\n",
      "Average test loss: 0.0027095712987292146\n",
      "Epoch 182/300\n",
      "Average training loss: 0.001882044045565029\n",
      "Average test loss: 0.002801237919885251\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0018748802063572737\n",
      "Average test loss: 0.002750717483460903\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0018912314607037438\n",
      "Average test loss: 0.0027406701797412503\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0018714011284626192\n",
      "Average test loss: 0.0027110119825229048\n",
      "Epoch 186/300\n",
      "Average training loss: 0.001875531336810026\n",
      "Average test loss: 0.002697984036472109\n",
      "Epoch 187/300\n",
      "Average training loss: 0.001875914905530711\n",
      "Average test loss: 0.002708867984513442\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0018711611584035887\n",
      "Average test loss: 0.003097957796106736\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0018853271949208444\n",
      "Average test loss: 0.002771457080004944\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0020936831166553827\n",
      "Average test loss: 0.0026320591728306478\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0019012704456432\n",
      "Average test loss: 0.0027391463671293526\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0018607510386241808\n",
      "Average test loss: 0.0027679111862348184\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0018556601708340976\n",
      "Average test loss: 0.002720082622849279\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0018576575035436286\n",
      "Average test loss: 0.0027041453928169277\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0018625468428557117\n",
      "Average test loss: 0.0027956546232518224\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0018786888157741892\n",
      "Average test loss: 0.0027023623360113965\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0018641132013872265\n",
      "Average test loss: 0.002767768188690146\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0019023118846946292\n",
      "Average test loss: 0.0027251543073604505\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0018595375730656087\n",
      "Average test loss: 0.002899987413444453\n",
      "Epoch 200/300\n",
      "Average training loss: 0.001851009780748023\n",
      "Average test loss: 0.0028328613597485754\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0018604695646920139\n",
      "Average test loss: 0.002728197561370002\n",
      "Epoch 202/300\n",
      "Average training loss: 0.001857246363742484\n",
      "Average test loss: 0.0027790591282149156\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0018519563311710954\n",
      "Average test loss: 0.002775701036469804\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0018580365190282464\n",
      "Average test loss: 0.00281442330736253\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0020014546263135142\n",
      "Average test loss: 0.0027221200021190777\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0021301119575897853\n",
      "Average test loss: 0.002642387974282934\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0019076926702012618\n",
      "Average test loss: 0.0026977025342898235\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0018480905884255966\n",
      "Average test loss: 0.002771756976428959\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0018337366113232241\n",
      "Average test loss: 0.0028166323803986114\n",
      "Epoch 210/300\n",
      "Average training loss: 0.001832273015441994\n",
      "Average test loss: 0.0028610628810193804\n",
      "Epoch 211/300\n",
      "Average training loss: 0.001846277532995575\n",
      "Average test loss: 0.0026648295530014568\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0018411055531145797\n",
      "Average test loss: 0.002726504863343305\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0018414595744883021\n",
      "Average test loss: 0.0027393335476517677\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00184394995133496\n",
      "Average test loss: 0.002675358705015646\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0018473623469471932\n",
      "Average test loss: 0.0027700711755702895\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0018409124998789695\n",
      "Average test loss: 0.002734123728548487\n",
      "Epoch 217/300\n",
      "Average training loss: 0.001845161837215225\n",
      "Average test loss: 0.0027229015808552504\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0018347927022518383\n",
      "Average test loss: 0.0027743738802770775\n",
      "Epoch 219/300\n",
      "Average training loss: 0.001831007673870772\n",
      "Average test loss: 0.002864304676858915\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0018546910985476441\n",
      "Average test loss: 0.002847663544739286\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0018502414973659647\n",
      "Average test loss: 0.0026562759537870685\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0018293195156794456\n",
      "Average test loss: 0.00280952228522963\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0018290132612196937\n",
      "Average test loss: 0.0028014076149298086\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0018252041095660793\n",
      "Average test loss: 0.0027222049749559825\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0018274624925106763\n",
      "Average test loss: 0.002838017286318872\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0018241124756427274\n",
      "Average test loss: 0.002782063181615538\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0018227252018534475\n",
      "Average test loss: 0.0026910173340390126\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0018486363056840168\n",
      "Average test loss: 0.0033027524559034243\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0018430738338890175\n",
      "Average test loss: 0.0028363057586054006\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0018192286020558741\n",
      "Average test loss: 0.002822404807847407\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0018166385391313168\n",
      "Average test loss: 0.002776011308448182\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0018185180843704276\n",
      "Average test loss: 0.002774791509947843\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0018165646367188956\n",
      "Average test loss: 0.0027177384261869724\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0018149119551397032\n",
      "Average test loss: 0.0027772899793667926\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0018190058527721299\n",
      "Average test loss: 0.005210572267986006\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0019637828894580405\n",
      "Average test loss: 0.002852225096896291\n",
      "Epoch 237/300\n",
      "Average training loss: 0.001802981679638227\n",
      "Average test loss: 0.002817636309398545\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0017959351973194215\n",
      "Average test loss: 0.002790441651725107\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0017985713420849707\n",
      "Average test loss: 0.0027713201364709273\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0018047572103225523\n",
      "Average test loss: 0.0028267846076438824\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0018113709627133278\n",
      "Average test loss: 0.0029570771447486347\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0018010107712406252\n",
      "Average test loss: 0.0028138658590614795\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0018398245432310633\n",
      "Average test loss: 0.0028793163992878465\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0018003674527216288\n",
      "Average test loss: 0.002751333197992709\n",
      "Epoch 245/300\n",
      "Average training loss: 0.001801449621303214\n",
      "Average test loss: 0.0028301524646166297\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0018006350017256206\n",
      "Average test loss: 0.0027950468640774487\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0018020683752579822\n",
      "Average test loss: 0.002860499777727657\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0018009732036540905\n",
      "Average test loss: 0.0029090430146704116\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0017993524215287632\n",
      "Average test loss: 0.0028334157730245755\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0018270416711974476\n",
      "Average test loss: 0.0028020913859622344\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0017911687467454208\n",
      "Average test loss: 0.002761870673340228\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0017923393884880675\n",
      "Average test loss: 0.0027449490087520746\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0017976936798335778\n",
      "Average test loss: 0.0027494928960998853\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0017931508640241292\n",
      "Average test loss: 0.002958905707113445\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0017909802666140927\n",
      "Average test loss: 0.0027705606946514714\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0017928416421636938\n",
      "Average test loss: 0.0028509552810962\n",
      "Epoch 257/300\n",
      "Average training loss: 0.001788436074534224\n",
      "Average test loss: 0.0029447655853711897\n",
      "Epoch 258/300\n",
      "Average training loss: 0.001809721245119969\n",
      "Average test loss: 0.002798884934435288\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0017847441976062126\n",
      "Average test loss: 0.0031580141064607437\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0017875484730013544\n",
      "Average test loss: 0.0027832345695545276\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0017880844620780812\n",
      "Average test loss: 0.0028134840621302525\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0017863655694656901\n",
      "Average test loss: 0.0027760986006922193\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0017804188977720008\n",
      "Average test loss: 0.0027806227310664122\n",
      "Epoch 264/300\n",
      "Average training loss: 0.001787630041440328\n",
      "Average test loss: 0.002775135135071145\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0018110956445129382\n",
      "Average test loss: 0.0028468718458380966\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0017785877262552578\n",
      "Average test loss: 0.00286757897440758\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0018810328516281314\n",
      "Average test loss: 0.0027159562019838227\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0018339398348083099\n",
      "Average test loss: 0.002718509945811497\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0017844514597414268\n",
      "Average test loss: 0.0028578945526646243\n",
      "Epoch 270/300\n",
      "Average training loss: 0.001773259263796111\n",
      "Average test loss: 0.0028182097843123806\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0017702849544584752\n",
      "Average test loss: 0.002785529017034504\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0017697849181584186\n",
      "Average test loss: 0.002825040644655625\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0017743578769473566\n",
      "Average test loss: 0.0028133446717013917\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0017691354355257418\n",
      "Average test loss: 0.002781604530186289\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001774475293337471\n",
      "Average test loss: 0.0028021856165594524\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0017693450197370517\n",
      "Average test loss: 0.002879117489688926\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0017956257712923819\n",
      "Average test loss: 0.0028370262558261553\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0017642634083620376\n",
      "Average test loss: 0.0028793280931810536\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0017724731913250353\n",
      "Average test loss: 0.002791582905480431\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0017671110820439127\n",
      "Average test loss: 0.002843617634433839\n",
      "Epoch 281/300\n",
      "Average training loss: 0.001766963558892409\n",
      "Average test loss: 0.002781787248328328\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0017681773943412635\n",
      "Average test loss: 0.0028339861979087195\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0017670389373476306\n",
      "Average test loss: 0.0028987138827020922\n",
      "Epoch 284/300\n",
      "Average training loss: 0.00176039642168002\n",
      "Average test loss: 0.002973489714993371\n",
      "Epoch 285/300\n",
      "Average training loss: 0.001762116093809406\n",
      "Average test loss: 0.0028231743830773566\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0017657782255361477\n",
      "Average test loss: 0.0027868081646867924\n",
      "Epoch 287/300\n",
      "Average training loss: 0.001777697728532884\n",
      "Average test loss: 0.003063547378612889\n",
      "Epoch 288/300\n",
      "Average training loss: 0.001757021881846918\n",
      "Average test loss: 0.0028545372833808265\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0017597875912777252\n",
      "Average test loss: 0.0028175355200138357\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0017752371160313486\n",
      "Average test loss: 0.0028664640388968918\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0017640212155464624\n",
      "Average test loss: 0.002820832229529818\n",
      "Epoch 292/300\n",
      "Average training loss: 0.001752460631231467\n",
      "Average test loss: 0.0028151389825054342\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0021194978336700134\n",
      "Average test loss: 0.0026365488266779317\n",
      "Epoch 294/300\n",
      "Average training loss: 0.002047209157194528\n",
      "Average test loss: 0.002757329502246446\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0018596019834900895\n",
      "Average test loss: 0.00279571307843758\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0017772593495125571\n",
      "Average test loss: 0.0028130853236135508\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0017477666057853236\n",
      "Average test loss: 0.0028476913839371666\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0017416151045925087\n",
      "Average test loss: 0.0028362683828713165\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00173866272966067\n",
      "Average test loss: 0.0028645519591454\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0017706709075719117\n",
      "Average test loss: 0.002854493888095021\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.018610020326657428\n",
      "Average test loss: 0.006370383561071422\n",
      "Epoch 2/300\n",
      "Average training loss: 0.005117125910603338\n",
      "Average test loss: 0.006006475667158763\n",
      "Epoch 3/300\n",
      "Average training loss: 0.004322978243231773\n",
      "Average test loss: 0.004328136543846793\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0038631560988724232\n",
      "Average test loss: 0.003787939622170395\n",
      "Epoch 5/300\n",
      "Average training loss: 0.003549857465757264\n",
      "Average test loss: 0.003357192808141311\n",
      "Epoch 6/300\n",
      "Average training loss: 0.003305932488085495\n",
      "Average test loss: 0.0032216982959459226\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0031427275395641725\n",
      "Average test loss: 0.003673270175440444\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0029616852574464348\n",
      "Average test loss: 0.003105756537574861\n",
      "Epoch 9/300\n",
      "Average training loss: 0.002844142339709732\n",
      "Average test loss: 0.0029870161993636023\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0027222121032989687\n",
      "Average test loss: 0.002680238082797991\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0025912604975617593\n",
      "Average test loss: 0.002796037202907933\n",
      "Epoch 12/300\n",
      "Average training loss: 0.002542276192456484\n",
      "Average test loss: 0.0025804586151821746\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0024499534632389745\n",
      "Average test loss: 0.002462752094078395\n",
      "Epoch 14/300\n",
      "Average training loss: 0.002363834392072426\n",
      "Average test loss: 0.0024648785343807603\n",
      "Epoch 15/300\n",
      "Average training loss: 0.002303852682933211\n",
      "Average test loss: 0.002465490243294173\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0022492278444891174\n",
      "Average test loss: 0.002309372772462666\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0022121762863049903\n",
      "Average test loss: 0.0022627501007583405\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00217861122948428\n",
      "Average test loss: 0.0022614970416244533\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00213865546323359\n",
      "Average test loss: 0.0022774364890323744\n",
      "Epoch 20/300\n",
      "Average training loss: 0.002106411811067826\n",
      "Average test loss: 0.002298459909235438\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0020659018576973014\n",
      "Average test loss: 0.0021361140838513774\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0020424450748703546\n",
      "Average test loss: 0.002139083662070334\n",
      "Epoch 23/300\n",
      "Average training loss: 0.002019609825892581\n",
      "Average test loss: 0.002353777165214221\n",
      "Epoch 24/300\n",
      "Average training loss: 0.001997846095615791\n",
      "Average test loss: 0.002089134907970826\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0019808456766315635\n",
      "Average test loss: 0.0021863718568864796\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0019601673644243016\n",
      "Average test loss: 0.0021376731633726094\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0019424626768256227\n",
      "Average test loss: 0.002080229146613015\n",
      "Epoch 28/300\n",
      "Average training loss: 0.001923996377011968\n",
      "Average test loss: 0.0020625283976809846\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0018992197887144156\n",
      "Average test loss: 0.002079791532415483\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010286808787431155\n",
      "Average test loss: 0.00603551518999868\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004987882195661465\n",
      "Average test loss: 0.0041975553557276725\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004013643339897195\n",
      "Average test loss: 0.007028852198686865\n",
      "Epoch 33/300\n",
      "Average training loss: 0.003591440116572711\n",
      "Average test loss: 0.0037791544617050226\n",
      "Epoch 34/300\n",
      "Average training loss: 0.003325255702974068\n",
      "Average test loss: 0.0031415352610250314\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0031145217960907353\n",
      "Average test loss: 0.005580768804997206\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002898669770608346\n",
      "Average test loss: 0.002720443414317237\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0027115591023531226\n",
      "Average test loss: 0.00285383171050085\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0025502163018617367\n",
      "Average test loss: 0.002965184274646971\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002433961648804446\n",
      "Average test loss: 0.004381890969143974\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0023317663319822814\n",
      "Average test loss: 0.0029291111453332835\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0022453131491525307\n",
      "Average test loss: 0.0024144377555284234\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0021803736614270344\n",
      "Average test loss: 0.002164069259746207\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002096312468561033\n",
      "Average test loss: 0.0023969047415173714\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002038902023703688\n",
      "Average test loss: 0.002115155764027602\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0019947317738292945\n",
      "Average test loss: 0.002241280343487031\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0019631387731060385\n",
      "Average test loss: 0.0021086669930567344\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0019385769721120597\n",
      "Average test loss: 0.00205031270140575\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0019113855008035898\n",
      "Average test loss: 0.002083441841829982\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0019017644374527865\n",
      "Average test loss: 0.002055236018469764\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0018804300268077188\n",
      "Average test loss: 0.0020572430683920783\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0018876202017482785\n",
      "Average test loss: 0.002016423747357395\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0018531650002631876\n",
      "Average test loss: 0.001962314181650678\n",
      "Epoch 53/300\n",
      "Average training loss: 0.001857115945364866\n",
      "Average test loss: 0.002008184761636787\n",
      "Epoch 54/300\n",
      "Average training loss: 0.001834636129024956\n",
      "Average test loss: 0.002100491703591413\n",
      "Epoch 55/300\n",
      "Average training loss: 0.001826082266970641\n",
      "Average test loss: 0.0019683107063174247\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0018145745596331027\n",
      "Average test loss: 0.0020900772238771122\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0018026848948664135\n",
      "Average test loss: 0.002067603939005898\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0018042682954627607\n",
      "Average test loss: 0.0019583926923159097\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0017858831058773729\n",
      "Average test loss: 0.0020538969633893834\n",
      "Epoch 60/300\n",
      "Average training loss: 0.006396622786919276\n",
      "Average test loss: 0.005192279487848282\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0027485091528958745\n",
      "Average test loss: 0.0024918299513972467\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002337296929417385\n",
      "Average test loss: 0.0022389088867025243\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002135167867773109\n",
      "Average test loss: 0.0021118087251153256\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0020105643721504344\n",
      "Average test loss: 0.0020601844537175363\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0019260777468896575\n",
      "Average test loss: 0.0021119708623737097\n",
      "Epoch 66/300\n",
      "Average training loss: 0.001864184972519676\n",
      "Average test loss: 0.0020619031896607744\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0018153289776916305\n",
      "Average test loss: 0.0019927498404350547\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0017863627115471496\n",
      "Average test loss: 0.001942506692268782\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0017653455428986086\n",
      "Average test loss: 0.00196818299488061\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0017551597757264972\n",
      "Average test loss: 0.001965766931263109\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0017493666244360307\n",
      "Average test loss: 0.002029964896539847\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0017554599852818583\n",
      "Average test loss: 0.0019528011878331502\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0017427182856740223\n",
      "Average test loss: 0.0023799657405664523\n",
      "Epoch 74/300\n",
      "Average training loss: 0.001769228559711741\n",
      "Average test loss: 0.0020132141917323073\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0017391357589513063\n",
      "Average test loss: 0.0019898528858191436\n",
      "Epoch 76/300\n",
      "Average training loss: 0.001725204862654209\n",
      "Average test loss: 0.0019518550257715914\n",
      "Epoch 77/300\n",
      "Average training loss: 0.00173001141856528\n",
      "Average test loss: 0.0019233749277061886\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0017194663430046705\n",
      "Average test loss: 0.002036913001909852\n",
      "Epoch 79/300\n",
      "Average training loss: 0.007359640321176913\n",
      "Average test loss: 0.008240564249455929\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005588419427888261\n",
      "Average test loss: 0.052835458919405935\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004124699828111463\n",
      "Average test loss: 0.004680228706035349\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0034794254574096865\n",
      "Average test loss: 0.002993049994111061\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003135208934959438\n",
      "Average test loss: 0.004235905041918159\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0029059414996041194\n",
      "Average test loss: 0.005320235124582218\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002720475816478332\n",
      "Average test loss: 0.002846511290926072\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0025757357772025796\n",
      "Average test loss: 0.020592319741845132\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0024200360824664434\n",
      "Average test loss: 0.002633554382870595\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002349256961709923\n",
      "Average test loss: 0.0022354885292136008\n",
      "Epoch 89/300\n",
      "Average training loss: 0.002223162978163196\n",
      "Average test loss: 0.02645206686316265\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002131016907178693\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'PGD_Residual-LastLayer/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'PGD_Residual-LastLayer/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'PGD_Residual-LastLayer/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
