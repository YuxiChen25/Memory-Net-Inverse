{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.PGD_Network.PGD import PGD\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15314364889595244\n",
      "Average test loss: 0.010831923445893659\n",
      "Epoch 2/300\n",
      "Average training loss: 0.062394282579422\n",
      "Average test loss: 0.009379227098491457\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05676238288813167\n",
      "Average test loss: 0.009184772385491265\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05299760388003455\n",
      "Average test loss: 0.008994300285975138\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05080880544251866\n",
      "Average test loss: 0.008478412026746405\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04847067168686125\n",
      "Average test loss: 0.008459649592638016\n",
      "Epoch 7/300\n",
      "Average training loss: 0.047198962572548124\n",
      "Average test loss: 0.00796643231726355\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04599428598748313\n",
      "Average test loss: 0.009533568536241849\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04522273317972819\n",
      "Average test loss: 0.007830861477802197\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04434395538104905\n",
      "Average test loss: 0.007899392052657075\n",
      "Epoch 11/300\n",
      "Average training loss: 0.043542023301124576\n",
      "Average test loss: 0.010385171271032757\n",
      "Epoch 12/300\n",
      "Average training loss: 0.042800904063714874\n",
      "Average test loss: 0.007437325705256727\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04222919189929962\n",
      "Average test loss: 0.007370528175599045\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04173721938663059\n",
      "Average test loss: 0.007292175388170613\n",
      "Epoch 15/300\n",
      "Average training loss: 0.041116544422176146\n",
      "Average test loss: 0.00789325376931164\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04071582827303145\n",
      "Average test loss: 0.0072289378295342125\n",
      "Epoch 17/300\n",
      "Average training loss: 0.040245249993271295\n",
      "Average test loss: 0.007883761208090517\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03990440055727959\n",
      "Average test loss: 0.007019797066019641\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03955755227804184\n",
      "Average test loss: 0.0069444176575375926\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03930860603849093\n",
      "Average test loss: 0.007358602864874734\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03895341949661573\n",
      "Average test loss: 0.00689718243976434\n",
      "Epoch 22/300\n",
      "Average training loss: 0.038684688727060955\n",
      "Average test loss: 0.023070186507370735\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03839491703775194\n",
      "Average test loss: 0.006897843251625697\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03804591848452886\n",
      "Average test loss: 0.006723345478789674\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03789012170831362\n",
      "Average test loss: 0.006623271668536795\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03764481480254067\n",
      "Average test loss: 0.006930391848087311\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03744172633853224\n",
      "Average test loss: 0.007129579208791256\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03725248170561261\n",
      "Average test loss: 0.006771512339512507\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0370227690703339\n",
      "Average test loss: 0.00647663096504079\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03681003439095285\n",
      "Average test loss: 0.006615278100387917\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03668697425391939\n",
      "Average test loss: 0.007229412926567925\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03650196146302753\n",
      "Average test loss: 0.006644870980332295\n",
      "Epoch 33/300\n",
      "Average training loss: 0.036318593485487834\n",
      "Average test loss: 0.012548499583991037\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03619393886460198\n",
      "Average test loss: 0.006526307540635268\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03610417073302799\n",
      "Average test loss: 0.006575735942977998\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03595349028044277\n",
      "Average test loss: 0.006478783523456918\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03578260589308209\n",
      "Average test loss: 0.006309265343265401\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03557767856783337\n",
      "Average test loss: 0.006581826074255837\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03558408538500468\n",
      "Average test loss: 0.00650071908823318\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03541565999719832\n",
      "Average test loss: 0.006274807784292433\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0353054879936907\n",
      "Average test loss: 0.006306253108713362\n",
      "Epoch 42/300\n",
      "Average training loss: 0.035220621622271006\n",
      "Average test loss: 0.006927150002370278\n",
      "Epoch 43/300\n",
      "Average training loss: 0.035034930258989336\n",
      "Average test loss: 0.006199615712794993\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0350038601093822\n",
      "Average test loss: 0.0066935074035492205\n",
      "Epoch 45/300\n",
      "Average training loss: 0.034865820500585766\n",
      "Average test loss: 0.0074247307193775975\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03477878300017781\n",
      "Average test loss: 0.006379349677513043\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03468017288049062\n",
      "Average test loss: 0.006527390121999714\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03494156531658438\n",
      "Average test loss: 0.0062301720587743655\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03448648225930002\n",
      "Average test loss: 0.00626869002978007\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0344375853670968\n",
      "Average test loss: 0.006955125278896756\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03434592505958345\n",
      "Average test loss: 0.006229540396481752\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03432550254133013\n",
      "Average test loss: 0.006599848878880342\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03426886724266741\n",
      "Average test loss: 0.00643217542519172\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03419654071993298\n",
      "Average test loss: 0.006904079837517606\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03409738632043203\n",
      "Average test loss: 0.04782932624220848\n",
      "Epoch 56/300\n",
      "Average training loss: 0.034141653950015706\n",
      "Average test loss: 0.006203938643551535\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03393721022208532\n",
      "Average test loss: 0.006367795640809668\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03386734246545368\n",
      "Average test loss: 0.006424979475637277\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03388126551442676\n",
      "Average test loss: 0.006353341795090172\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03380231627325217\n",
      "Average test loss: 0.0062197837494313716\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03379172623488638\n",
      "Average test loss: 0.006276406028204494\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03362953817182117\n",
      "Average test loss: 0.006280194619877471\n",
      "Epoch 63/300\n",
      "Average training loss: 0.033602504564656153\n",
      "Average test loss: 0.006201764621254471\n",
      "Epoch 64/300\n",
      "Average training loss: 0.3115916869242986\n",
      "Average test loss: 0.01559097619768646\n",
      "Epoch 65/300\n",
      "Average training loss: 0.10574132473601235\n",
      "Average test loss: 0.009026665695011616\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06609101663033168\n",
      "Average test loss: 0.008528335042711761\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05559824409087499\n",
      "Average test loss: 0.008034632299509313\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05051002491845025\n",
      "Average test loss: 0.007276537961016099\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04710446350442039\n",
      "Average test loss: 0.0073057787327302825\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04471050063769023\n",
      "Average test loss: 0.006925144237776597\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04278233447008663\n",
      "Average test loss: 0.007763420877357324\n",
      "Epoch 72/300\n",
      "Average training loss: 0.041048317866192924\n",
      "Average test loss: 0.006801096003916528\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03956918159127235\n",
      "Average test loss: 0.0064724014898141224\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03847027656435967\n",
      "Average test loss: 0.0063723902528484665\n",
      "Epoch 75/300\n",
      "Average training loss: 0.037474721690018975\n",
      "Average test loss: 0.006285848055448797\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03663993493384785\n",
      "Average test loss: 0.006241539809438918\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03602490657567978\n",
      "Average test loss: 0.006311843361291621\n",
      "Epoch 78/300\n",
      "Average training loss: 0.035426713236504134\n",
      "Average test loss: 0.0062905216312242876\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03498834930857023\n",
      "Average test loss: 0.006311114242093431\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03465511400169796\n",
      "Average test loss: 0.006785588628302018\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03442278534836239\n",
      "Average test loss: 0.006502041113873323\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03421017306380802\n",
      "Average test loss: 0.006203761738621526\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03398121389581098\n",
      "Average test loss: 0.0063128963129387965\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03396453053090307\n",
      "Average test loss: 0.006370629261351294\n",
      "Epoch 85/300\n",
      "Average training loss: 0.033749337961276375\n",
      "Average test loss: 0.006237617551452584\n",
      "Epoch 86/300\n",
      "Average training loss: 0.033747700810432435\n",
      "Average test loss: 0.006311673652380705\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03365718910429213\n",
      "Average test loss: 0.006494979619979859\n",
      "Epoch 88/300\n",
      "Average training loss: 0.033585256482164066\n",
      "Average test loss: 0.006370722960266802\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03344834019078149\n",
      "Average test loss: 0.006284697454836633\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03374240837494532\n",
      "Average test loss: 0.0061954080797731875\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03330541386206945\n",
      "Average test loss: 0.006104324991918273\n",
      "Epoch 92/300\n",
      "Average training loss: 0.033312705089648564\n",
      "Average test loss: 0.006264069148649772\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03332473833196693\n",
      "Average test loss: 0.00637792290871342\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03318164957894219\n",
      "Average test loss: 0.006253824334177706\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03314474406838417\n",
      "Average test loss: 0.009516568263371786\n",
      "Epoch 96/300\n",
      "Average training loss: 0.033060658209853704\n",
      "Average test loss: 0.006192623040328423\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03304937035507626\n",
      "Average test loss: 0.00615336050093174\n",
      "Epoch 98/300\n",
      "Average training loss: 0.032978480239709215\n",
      "Average test loss: 0.006370407461706135\n",
      "Epoch 99/300\n",
      "Average training loss: 0.033019179963403276\n",
      "Average test loss: 0.006097200605811344\n",
      "Epoch 100/300\n",
      "Average training loss: 0.032840674600667426\n",
      "Average test loss: 0.007002084357043107\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03278364824089739\n",
      "Average test loss: 0.0062228028906716245\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03281548031833437\n",
      "Average test loss: 0.006273380264639855\n",
      "Epoch 103/300\n",
      "Average training loss: 0.032743591321839224\n",
      "Average test loss: 0.006272600493497319\n",
      "Epoch 104/300\n",
      "Average training loss: 0.032672524829705556\n",
      "Average test loss: 0.01544778125650353\n",
      "Epoch 105/300\n",
      "Average training loss: 0.032613052550289366\n",
      "Average test loss: 0.006166585465272267\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03258115200036102\n",
      "Average test loss: 0.006886058126472764\n",
      "Epoch 107/300\n",
      "Average training loss: 0.032526248302724624\n",
      "Average test loss: 0.006436695715205537\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03252992584639126\n",
      "Average test loss: 0.006232117824670341\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03245669837627146\n",
      "Average test loss: 0.0060853288691076965\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03240830752584669\n",
      "Average test loss: 0.006151903091619412\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0323579271932443\n",
      "Average test loss: 0.006529564592987299\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03239276311132643\n",
      "Average test loss: 0.0064847303964197635\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03233694822258419\n",
      "Average test loss: 0.006178065075849493\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03227235662937164\n",
      "Average test loss: 0.006976023650003804\n",
      "Epoch 115/300\n",
      "Average training loss: 0.032216305683056515\n",
      "Average test loss: 0.006231252545697822\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03218906822138363\n",
      "Average test loss: 0.006271510032316049\n",
      "Epoch 117/300\n",
      "Average training loss: 0.032137731856769984\n",
      "Average test loss: 0.006310069595153133\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03211374120910963\n",
      "Average test loss: 0.0061066373009234665\n",
      "Epoch 119/300\n",
      "Average training loss: 0.032130988309780756\n",
      "Average test loss: 0.006314121005849706\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032088098206453856\n",
      "Average test loss: 0.006152075391676691\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03201538707315922\n",
      "Average test loss: 0.00639502908455001\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03199036614762412\n",
      "Average test loss: 0.006067942461205854\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03196873094307052\n",
      "Average test loss: 0.006253752659592364\n",
      "Epoch 124/300\n",
      "Average training loss: 0.031878630025519265\n",
      "Average test loss: 0.006845376095838017\n",
      "Epoch 125/300\n",
      "Average training loss: 0.031942868871821295\n",
      "Average test loss: 0.006152428404324585\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03188152459760507\n",
      "Average test loss: 0.006066673678656419\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03185182825724284\n",
      "Average test loss: 0.006619922700855467\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03175545525219706\n",
      "Average test loss: 0.006127124811626143\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03182321949468719\n",
      "Average test loss: 0.006101993326925569\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03172730574011803\n",
      "Average test loss: 0.006085746016353369\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031702956216202845\n",
      "Average test loss: 0.00607989491439528\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03173278556929694\n",
      "Average test loss: 0.006278549330929915\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03165905120472113\n",
      "Average test loss: 0.006179274874428908\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03165799166758855\n",
      "Average test loss: 0.00628288936532206\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03159176657266087\n",
      "Average test loss: 0.0063683450147509575\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03162558304932382\n",
      "Average test loss: 0.006350601299769348\n",
      "Epoch 137/300\n",
      "Average training loss: 0.031573636958996454\n",
      "Average test loss: 0.006130060776654217\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03148891538216008\n",
      "Average test loss: 0.006162969662911362\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0314515292727285\n",
      "Average test loss: 0.006445187129080296\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03150731526480781\n",
      "Average test loss: 0.006038468418849839\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0314423137737645\n",
      "Average test loss: 0.0064728224794897765\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03136718277467622\n",
      "Average test loss: 0.006120482590463426\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03140480041503906\n",
      "Average test loss: 0.006306300898392995\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03136634572181437\n",
      "Average test loss: 0.006154841959890392\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03134923410581218\n",
      "Average test loss: 0.006152521244353718\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03130014761620098\n",
      "Average test loss: 0.006438622949437963\n",
      "Epoch 147/300\n",
      "Average training loss: 0.031363147808445825\n",
      "Average test loss: 0.006112741731107235\n",
      "Epoch 148/300\n",
      "Average training loss: 0.031281017849842704\n",
      "Average test loss: 0.006343049330843819\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03122908333937327\n",
      "Average test loss: 0.006180223124308718\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03125288614961836\n",
      "Average test loss: 0.006283458945237928\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03119801845649878\n",
      "Average test loss: 0.006361066806647513\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03119004194272889\n",
      "Average test loss: 0.006220760704328617\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0311778085231781\n",
      "Average test loss: 0.006268692119667928\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031195811652474932\n",
      "Average test loss: 0.006185964864575201\n",
      "Epoch 155/300\n",
      "Average training loss: 0.031107523636685477\n",
      "Average test loss: 0.0061766407175196545\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03108165063129531\n",
      "Average test loss: 0.0061515602130028935\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03110983442929056\n",
      "Average test loss: 0.006083079712672366\n",
      "Epoch 158/300\n",
      "Average training loss: 0.031068376895454195\n",
      "Average test loss: 0.006085825047973129\n",
      "Epoch 159/300\n",
      "Average training loss: 0.031015020496315427\n",
      "Average test loss: 0.0068093218563331495\n",
      "Epoch 160/300\n",
      "Average training loss: 0.031007537012298903\n",
      "Average test loss: 0.006167796019878652\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03100489396519131\n",
      "Average test loss: 0.006465422363744842\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030986784163448546\n",
      "Average test loss: 0.006126734111458063\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030948760989639494\n",
      "Average test loss: 0.00623196503188875\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030975303007496728\n",
      "Average test loss: 0.006335400585085154\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030881611756152578\n",
      "Average test loss: 0.006303911290234989\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03088002006378439\n",
      "Average test loss: 0.006554963396655188\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03090408262444867\n",
      "Average test loss: 0.006435532008194261\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030874964972337087\n",
      "Average test loss: 0.006144352270083295\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030908284260167018\n",
      "Average test loss: 0.010084315795037482\n",
      "Epoch 170/300\n",
      "Average training loss: 0.030819837129778333\n",
      "Average test loss: 0.006075430435438951\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03078999188873503\n",
      "Average test loss: 0.0068370351940393445\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030799845582909054\n",
      "Average test loss: 0.006263471016039451\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030811629894706937\n",
      "Average test loss: 0.0062210683673620225\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03081268765860134\n",
      "Average test loss: 0.006251784618530008\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030718286255995434\n",
      "Average test loss: 0.00616914010213481\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030759871979554494\n",
      "Average test loss: 0.00620721028579606\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030709963447517818\n",
      "Average test loss: 0.006571594693180588\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030680529887477558\n",
      "Average test loss: 0.006133508087032371\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03070734836326705\n",
      "Average test loss: 0.0066261010989546775\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03060893217060301\n",
      "Average test loss: 0.007219018509818448\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030643826262818443\n",
      "Average test loss: 0.006143457779039939\n",
      "Epoch 182/300\n",
      "Average training loss: 0.030623436560233433\n",
      "Average test loss: 0.006379224825029572\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030581249448988174\n",
      "Average test loss: 0.007163710839632484\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030618185652626886\n",
      "Average test loss: 0.006179840912007623\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03056092988782459\n",
      "Average test loss: 0.006333881214261055\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0305476605180237\n",
      "Average test loss: 0.00632442944455478\n",
      "Epoch 187/300\n",
      "Average training loss: 0.030543493709630436\n",
      "Average test loss: 0.006626112019436227\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030521623578336505\n",
      "Average test loss: 0.006562117991348108\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03054637119670709\n",
      "Average test loss: 0.006249885091351139\n",
      "Epoch 190/300\n",
      "Average training loss: 0.030499708556466634\n",
      "Average test loss: 0.006145802314910624\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030450933396816253\n",
      "Average test loss: 0.0069622748547958\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03049126809835434\n",
      "Average test loss: 0.006220490760273404\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03042519118388494\n",
      "Average test loss: 0.006693709122637908\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03045598323808776\n",
      "Average test loss: 0.006263407990750339\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030445164894064267\n",
      "Average test loss: 0.006320649253825346\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0303676876326402\n",
      "Average test loss: 0.006606647359000312\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030424183477958045\n",
      "Average test loss: 0.0061709142223828365\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030343401587671705\n",
      "Average test loss: 0.0062835794215401015\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030355422303080557\n",
      "Average test loss: 0.007323486889402072\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03043826651242044\n",
      "Average test loss: 0.006768083369152414\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03036014775103993\n",
      "Average test loss: 0.006393104756044017\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03030848304099507\n",
      "Average test loss: 0.006261007515920533\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030330788125594456\n",
      "Average test loss: 0.006172525336345037\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030337433897786672\n",
      "Average test loss: 0.006494472327331702\n",
      "Epoch 205/300\n",
      "Average training loss: 0.030329858233531317\n",
      "Average test loss: 0.00615060472736756\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03024113426440292\n",
      "Average test loss: 0.0063071531247761515\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030245454423957402\n",
      "Average test loss: 0.00624445533586873\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03021684531867504\n",
      "Average test loss: 0.006254496881945266\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030209926863511403\n",
      "Average test loss: 0.006480717032319969\n",
      "Epoch 210/300\n",
      "Average training loss: 0.030244573069943322\n",
      "Average test loss: 0.006218365425037013\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0302493441419469\n",
      "Average test loss: 0.006222838194833861\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030209623858332636\n",
      "Average test loss: 0.010112144466903474\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030205194602409998\n",
      "Average test loss: 0.006218282215297222\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03014358345998658\n",
      "Average test loss: 0.006246449877611465\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03018465551899539\n",
      "Average test loss: 0.0062451415426201294\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03015294734140237\n",
      "Average test loss: 0.006279680453240871\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030146783886684313\n",
      "Average test loss: 0.006327679433756404\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030088517419166034\n",
      "Average test loss: 0.007058159504913622\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03013555379708608\n",
      "Average test loss: 0.006350398086632292\n",
      "Epoch 220/300\n",
      "Average training loss: 0.030081668135192658\n",
      "Average test loss: 0.006258240276740657\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030093594166967604\n",
      "Average test loss: 0.0063538624569773675\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030111436964737046\n",
      "Average test loss: 0.007739735338422987\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03006333646343814\n",
      "Average test loss: 0.006411018759012222\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03006713209218449\n",
      "Average test loss: 0.006529730389515559\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03007908641298612\n",
      "Average test loss: 0.0068344236848255\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030010734134250216\n",
      "Average test loss: 0.006372872617923551\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030026999450392194\n",
      "Average test loss: 0.00632793677970767\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030035008781486086\n",
      "Average test loss: 0.006158527959552076\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02999795834885703\n",
      "Average test loss: 0.006240945525467396\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029964943086107573\n",
      "Average test loss: 0.006300742980092764\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03001932208240032\n",
      "Average test loss: 0.007370989613648918\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02997293390830358\n",
      "Average test loss: 0.00641536598569817\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02991346800658438\n",
      "Average test loss: 0.006328176391621431\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029982779304186503\n",
      "Average test loss: 0.006255391273647546\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029956652730703352\n",
      "Average test loss: 0.006439311622745461\n",
      "Epoch 236/300\n",
      "Average training loss: 0.029937791074315708\n",
      "Average test loss: 0.006549396759106053\n",
      "Epoch 237/300\n",
      "Average training loss: 0.029950550331009757\n",
      "Average test loss: 0.006732380299932427\n",
      "Epoch 238/300\n",
      "Average training loss: 0.029900851958327822\n",
      "Average test loss: 0.006194034819801649\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029896839363707437\n",
      "Average test loss: 0.007744470526774724\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029860861940516365\n",
      "Average test loss: 0.006330823946744203\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02985109315150314\n",
      "Average test loss: 0.006331433846718735\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02986113931073083\n",
      "Average test loss: 0.00654909757607513\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02986961596707503\n",
      "Average test loss: 0.006164349506298701\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029829754451910655\n",
      "Average test loss: 0.006420473852919208\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029836441498663692\n",
      "Average test loss: 0.006286237108624644\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02980693485008346\n",
      "Average test loss: 0.006403429804162847\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02978726377255387\n",
      "Average test loss: 0.006789757071683804\n",
      "Epoch 248/300\n",
      "Average training loss: 0.029815252972973716\n",
      "Average test loss: 0.007076926949123542\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029792939841747284\n",
      "Average test loss: 0.006301724223626985\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02976861530376805\n",
      "Average test loss: 0.006410698802520831\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02973685410287645\n",
      "Average test loss: 0.006646625529146857\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029779309103886285\n",
      "Average test loss: 0.006645467042095132\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02976874934302436\n",
      "Average test loss: 0.006636471004949676\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029723670257462396\n",
      "Average test loss: 0.006298201277852058\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02973682399921947\n",
      "Average test loss: 0.006357122912175125\n",
      "Epoch 256/300\n",
      "Average training loss: 0.029778679329488012\n",
      "Average test loss: 0.006266428019023604\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029700906144248116\n",
      "Average test loss: 0.006328885603696108\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02973505560391479\n",
      "Average test loss: 0.012202707635859648\n",
      "Epoch 259/300\n",
      "Average training loss: 0.029705242587460413\n",
      "Average test loss: 0.0063779981438484455\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029692347950405543\n",
      "Average test loss: 0.006366701511045297\n",
      "Epoch 261/300\n",
      "Average training loss: 0.029676744644840557\n",
      "Average test loss: 0.006327059365808964\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029676726548208132\n",
      "Average test loss: 0.006325249744786156\n",
      "Epoch 263/300\n",
      "Average training loss: 0.029623616026507482\n",
      "Average test loss: 0.0062758040548198755\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029669709553321204\n",
      "Average test loss: 0.006295016830994023\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029656891513201927\n",
      "Average test loss: 0.006328499019559887\n",
      "Epoch 266/300\n",
      "Average training loss: 0.029668503352337415\n",
      "Average test loss: 0.006339759054697222\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029597247956527604\n",
      "Average test loss: 0.006410242821607325\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029599663630127906\n",
      "Average test loss: 0.0066903131235804825\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029644613984558317\n",
      "Average test loss: 0.007987900447928244\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02961153855588701\n",
      "Average test loss: 0.006362448846714364\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029622641881306967\n",
      "Average test loss: 0.00633219248511725\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029539821098248164\n",
      "Average test loss: 0.007032634642389086\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029580302745103836\n",
      "Average test loss: 0.006314371563080285\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0295509913596842\n",
      "Average test loss: 0.006336823873221874\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029520808367265595\n",
      "Average test loss: 0.008999544501718547\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02967317029668225\n",
      "Average test loss: 0.006381194055080414\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02952653272284402\n",
      "Average test loss: 0.006374883702645699\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029527512159612444\n",
      "Average test loss: 0.006723865246193277\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029569623950454923\n",
      "Average test loss: 0.00655743324632446\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029521314875947103\n",
      "Average test loss: 0.006350986258023315\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029539305865764618\n",
      "Average test loss: 0.00623909681621525\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029502807444996303\n",
      "Average test loss: 0.006295441913935873\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029540627403391733\n",
      "Average test loss: 0.006302967949873871\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029435060530900954\n",
      "Average test loss: 0.006456671585639318\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02946285254922178\n",
      "Average test loss: 0.006708984117954969\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029466286298301485\n",
      "Average test loss: 0.006459222979015774\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029490039663182366\n",
      "Average test loss: 0.00639638378802273\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02950235641664929\n",
      "Average test loss: 0.006498568077882131\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02945348465608226\n",
      "Average test loss: 0.006339041566683187\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029489608100718923\n",
      "Average test loss: 0.006573218642837471\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02943715998861525\n",
      "Average test loss: 0.006232630947397815\n",
      "Epoch 292/300\n",
      "Average training loss: 0.029440690787302125\n",
      "Average test loss: 0.006823176689445973\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029378148552444246\n",
      "Average test loss: 0.006558927676743931\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029416003045108583\n",
      "Average test loss: 0.006354084663920932\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029385995094974835\n",
      "Average test loss: 0.006352751989745432\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029408251921335855\n",
      "Average test loss: 0.006271874988658561\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029379397552874352\n",
      "Average test loss: 0.0065592665535708266\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02936879601743486\n",
      "Average test loss: 0.006566546840800179\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029365824356675146\n",
      "Average test loss: 0.006582389745447371\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029369691444767847\n",
      "Average test loss: 0.007164936668343014\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13428596769107712\n",
      "Average test loss: 0.00831956322491169\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05011186582181189\n",
      "Average test loss: 0.007798430305388238\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04468794941239887\n",
      "Average test loss: 0.0062417305865221555\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04160081556770537\n",
      "Average test loss: 0.006758354300012191\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0387845270799266\n",
      "Average test loss: 0.005853672525121106\n",
      "Epoch 6/300\n",
      "Average training loss: 0.036696123288737406\n",
      "Average test loss: 0.005389533826874362\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03508624117904239\n",
      "Average test loss: 0.005574516043480899\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03384342271751828\n",
      "Average test loss: 0.005311497774389055\n",
      "Epoch 9/300\n",
      "Average training loss: 0.032687254364291825\n",
      "Average test loss: 0.00494580707202355\n",
      "Epoch 10/300\n",
      "Average training loss: 0.031998524463839\n",
      "Average test loss: 0.005435030999282996\n",
      "Epoch 11/300\n",
      "Average training loss: 0.031078359244598282\n",
      "Average test loss: 0.005233858220279217\n",
      "Epoch 12/300\n",
      "Average training loss: 0.030543247047397824\n",
      "Average test loss: 0.004701566563919186\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02995247005091773\n",
      "Average test loss: 0.00475503184977505\n",
      "Epoch 14/300\n",
      "Average training loss: 0.029569038963980145\n",
      "Average test loss: 0.005096667224334346\n",
      "Epoch 15/300\n",
      "Average training loss: 0.029063092920515274\n",
      "Average test loss: 0.004773066766560078\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02862312513589859\n",
      "Average test loss: 0.004465591919918855\n",
      "Epoch 17/300\n",
      "Average training loss: 0.028377220006452666\n",
      "Average test loss: 0.004540711964170138\n",
      "Epoch 18/300\n",
      "Average training loss: 0.027953319429523414\n",
      "Average test loss: 0.0045639340877532955\n",
      "Epoch 19/300\n",
      "Average training loss: 0.027650459147161906\n",
      "Average test loss: 0.004338790206445588\n",
      "Epoch 20/300\n",
      "Average training loss: 0.027453307521012094\n",
      "Average test loss: 0.004330998245626688\n",
      "Epoch 21/300\n",
      "Average training loss: 0.027151160462035074\n",
      "Average test loss: 0.0041324552792227935\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027016671363678243\n",
      "Average test loss: 0.004356587506830692\n",
      "Epoch 23/300\n",
      "Average training loss: 0.026714385074045922\n",
      "Average test loss: 0.004095212377607823\n",
      "Epoch 24/300\n",
      "Average training loss: 0.026532725690139664\n",
      "Average test loss: 0.00406573750740952\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026324597794148658\n",
      "Average test loss: 0.004065092464701997\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02620281030403243\n",
      "Average test loss: 0.004776869204723173\n",
      "Epoch 27/300\n",
      "Average training loss: 0.026047029972076415\n",
      "Average test loss: 0.004419982695331176\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025849778003162807\n",
      "Average test loss: 0.004041021892180046\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02586702954934703\n",
      "Average test loss: 0.003968477725154824\n",
      "Epoch 30/300\n",
      "Average training loss: 0.025659425664279197\n",
      "Average test loss: 0.005404282688680622\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02555208130677541\n",
      "Average test loss: 0.003964816748268075\n",
      "Epoch 32/300\n",
      "Average training loss: 0.025419398248195647\n",
      "Average test loss: 0.004143736628815532\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02534023141198688\n",
      "Average test loss: 0.0039847262638310595\n",
      "Epoch 34/300\n",
      "Average training loss: 0.025219398104482225\n",
      "Average test loss: 0.003847935488033626\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02508039893541071\n",
      "Average test loss: 0.004044608053647809\n",
      "Epoch 36/300\n",
      "Average training loss: 0.025014166014062034\n",
      "Average test loss: 0.004068605862971809\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024923983885182275\n",
      "Average test loss: 0.004073551203848587\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02481425403886371\n",
      "Average test loss: 0.004303464236358801\n",
      "Epoch 39/300\n",
      "Average training loss: 0.024798531361752085\n",
      "Average test loss: 0.005180824870036708\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024674631656871903\n",
      "Average test loss: 0.0039003617523445024\n",
      "Epoch 41/300\n",
      "Average training loss: 0.024633881696396402\n",
      "Average test loss: 0.003846125875496202\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02458003339668115\n",
      "Average test loss: 0.003955530256446865\n",
      "Epoch 43/300\n",
      "Average training loss: 0.024497636490397982\n",
      "Average test loss: 0.003798762808036473\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02440393413272169\n",
      "Average test loss: 0.004278789515296618\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024340387968553437\n",
      "Average test loss: 0.0037980127669870853\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02431364085111353\n",
      "Average test loss: 0.0038604000312172705\n",
      "Epoch 47/300\n",
      "Average training loss: 0.024248766125904188\n",
      "Average test loss: 0.0038700432425571815\n",
      "Epoch 48/300\n",
      "Average training loss: 0.024185000404715537\n",
      "Average test loss: 0.003971825189474556\n",
      "Epoch 49/300\n",
      "Average training loss: 0.024131564234693845\n",
      "Average test loss: 0.0037447907688717046\n",
      "Epoch 50/300\n",
      "Average training loss: 0.024073511170016396\n",
      "Average test loss: 0.004056264736172226\n",
      "Epoch 51/300\n",
      "Average training loss: 0.024033691835072306\n",
      "Average test loss: 0.0039686644389811485\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023964895574582947\n",
      "Average test loss: 0.0038081204978128273\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023908459439873696\n",
      "Average test loss: 0.003741876706067059\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023859983033604093\n",
      "Average test loss: 0.004828255972100628\n",
      "Epoch 55/300\n",
      "Average training loss: 0.023834847054547734\n",
      "Average test loss: 0.0038603714137441583\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02377660945057869\n",
      "Average test loss: 0.0037803739681839944\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023782440740201208\n",
      "Average test loss: 0.003737331956624985\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02370269965297646\n",
      "Average test loss: 0.0037967959122939244\n",
      "Epoch 59/300\n",
      "Average training loss: 0.023641912819610703\n",
      "Average test loss: 0.0038063697703182697\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02361805662843916\n",
      "Average test loss: 0.0039866281993066275\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02360039008491569\n",
      "Average test loss: 0.0036950137257162068\n",
      "Epoch 62/300\n",
      "Average training loss: 0.023531599793169232\n",
      "Average test loss: 0.004306325372722413\n",
      "Epoch 63/300\n",
      "Average training loss: 0.023477161453829873\n",
      "Average test loss: 0.004744551240983936\n",
      "Epoch 64/300\n",
      "Average training loss: 0.023565170821216373\n",
      "Average test loss: 0.003708062120816774\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02355642724202739\n",
      "Average test loss: 0.003988428505344524\n",
      "Epoch 66/300\n",
      "Average training loss: 0.023478542048070167\n",
      "Average test loss: 0.0037049228847026825\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02335242823925283\n",
      "Average test loss: 0.0037267493286894425\n",
      "Epoch 68/300\n",
      "Average training loss: 0.023351585979262988\n",
      "Average test loss: 0.003767297666105959\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02328338345223003\n",
      "Average test loss: 0.003708319339901209\n",
      "Epoch 70/300\n",
      "Average training loss: 0.023275671190685698\n",
      "Average test loss: 0.0038098861881428296\n",
      "Epoch 71/300\n",
      "Average training loss: 0.023245474979281425\n",
      "Average test loss: 0.0036951080130206214\n",
      "Epoch 72/300\n",
      "Average training loss: 0.023243025076058174\n",
      "Average test loss: 0.0038764890854557357\n",
      "Epoch 73/300\n",
      "Average training loss: 0.023187152206897734\n",
      "Average test loss: 0.004143637610806359\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0231869231959184\n",
      "Average test loss: 0.0036535050655818646\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02316485022008419\n",
      "Average test loss: 0.003793281260877848\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02307185108297401\n",
      "Average test loss: 0.0037442134698438974\n",
      "Epoch 77/300\n",
      "Average training loss: 0.023060923106140562\n",
      "Average test loss: 0.003618446856737137\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023097842011186812\n",
      "Average test loss: 0.003676116142835882\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02306690193547143\n",
      "Average test loss: 0.0037160731949326064\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023004844314522214\n",
      "Average test loss: 0.003651903762585587\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023008961861332257\n",
      "Average test loss: 0.0037129015340987177\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022935929679208333\n",
      "Average test loss: 0.003717075533750984\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02292429417868455\n",
      "Average test loss: 0.003680158648639917\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02289012361731794\n",
      "Average test loss: 0.00376436038253208\n",
      "Epoch 85/300\n",
      "Average training loss: 0.022936924868159823\n",
      "Average test loss: 0.003689962335344818\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02287867147889402\n",
      "Average test loss: 0.00364455211473008\n",
      "Epoch 87/300\n",
      "Average training loss: 0.022861980097161397\n",
      "Average test loss: 0.003649323901368512\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022829298950731755\n",
      "Average test loss: 0.003928935976078113\n",
      "Epoch 89/300\n",
      "Average training loss: 0.022808706811732717\n",
      "Average test loss: 0.00374506584679087\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022794906207256847\n",
      "Average test loss: 0.003740258956121074\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0227591405659914\n",
      "Average test loss: 0.003708730024803016\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022749557962020238\n",
      "Average test loss: 0.005264766103277604\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022730772246917087\n",
      "Average test loss: 0.003855892762541771\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022705894887447356\n",
      "Average test loss: 0.003738613491670953\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02271751978662279\n",
      "Average test loss: 0.003838609752969609\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02261950983769364\n",
      "Average test loss: 0.003967586139010058\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022636655517750317\n",
      "Average test loss: 0.003833679520421558\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022640771350926823\n",
      "Average test loss: 0.003691837618748347\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022611894379059475\n",
      "Average test loss: 0.003671334851740135\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022564906876948145\n",
      "Average test loss: 0.0037229238545729054\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022614951234724786\n",
      "Average test loss: 0.003701214253488514\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022563617120186487\n",
      "Average test loss: 0.003687894696576728\n",
      "Epoch 103/300\n",
      "Average training loss: 0.022537329672111407\n",
      "Average test loss: 0.003945428006764915\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022504451347721947\n",
      "Average test loss: 0.003812798679702812\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02248567702041732\n",
      "Average test loss: 0.004179939401646455\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02247133956849575\n",
      "Average test loss: 0.0037144289819730654\n",
      "Epoch 107/300\n",
      "Average training loss: 0.022480910395582518\n",
      "Average test loss: 0.0037538598558555045\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02245428439312511\n",
      "Average test loss: 0.0038805025331676004\n",
      "Epoch 109/300\n",
      "Average training loss: 0.022433526148398716\n",
      "Average test loss: 0.003620681770766775\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02240782104598151\n",
      "Average test loss: 0.0037255324290858373\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022435969317952793\n",
      "Average test loss: 0.035178757462236615\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02236340333852503\n",
      "Average test loss: 0.0037185073739124668\n",
      "Epoch 113/300\n",
      "Average training loss: 0.022358646386199527\n",
      "Average test loss: 0.003708681752698289\n",
      "Epoch 114/300\n",
      "Average training loss: 0.022406777625282606\n",
      "Average test loss: 0.0037316875196993352\n",
      "Epoch 115/300\n",
      "Average training loss: 0.022358268128501044\n",
      "Average test loss: 0.0036598103437572717\n",
      "Epoch 116/300\n",
      "Average training loss: 0.022294486987921927\n",
      "Average test loss: 0.0036747714624636705\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02229081391957071\n",
      "Average test loss: 0.0040394733502633045\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02230435808830791\n",
      "Average test loss: 0.0043944069412019515\n",
      "Epoch 119/300\n",
      "Average training loss: 0.022308863840169377\n",
      "Average test loss: 0.003663419050268001\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02228521033624808\n",
      "Average test loss: 0.0048419251586827965\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02223546540737152\n",
      "Average test loss: 0.0037567652292135687\n",
      "Epoch 122/300\n",
      "Average training loss: 0.022243633795115684\n",
      "Average test loss: 0.003785733930559622\n",
      "Epoch 123/300\n",
      "Average training loss: 0.022203398464454543\n",
      "Average test loss: 0.0036618416582544643\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02219712123937077\n",
      "Average test loss: 0.0038092114629430904\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02219205132789082\n",
      "Average test loss: 0.003782913519690434\n",
      "Epoch 126/300\n",
      "Average training loss: 0.022204727987448374\n",
      "Average test loss: 0.0037348616127338675\n",
      "Epoch 127/300\n",
      "Average training loss: 0.022128771086533865\n",
      "Average test loss: 0.003672568452854951\n",
      "Epoch 128/300\n",
      "Average training loss: 0.022170334279537202\n",
      "Average test loss: 0.0037084878782431283\n",
      "Epoch 129/300\n",
      "Average training loss: 0.022132089889711805\n",
      "Average test loss: 0.00377018956374377\n",
      "Epoch 130/300\n",
      "Average training loss: 0.022137352105644013\n",
      "Average test loss: 0.004043403084700306\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02209402709537082\n",
      "Average test loss: 0.0038467439032263224\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02216917656858762\n",
      "Average test loss: 0.0037726061139255764\n",
      "Epoch 133/300\n",
      "Average training loss: 0.022085310709145333\n",
      "Average test loss: 0.00444103595510953\n",
      "Epoch 134/300\n",
      "Average training loss: 0.022085605723990334\n",
      "Average test loss: 0.003708299187115497\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0220946273257335\n",
      "Average test loss: 0.0037805600087675783\n",
      "Epoch 136/300\n",
      "Average training loss: 0.022003489697972934\n",
      "Average test loss: 0.004229699904926949\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02206422979467445\n",
      "Average test loss: 0.003728585677014457\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0220298782736063\n",
      "Average test loss: 0.003747239397838712\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02206880476574103\n",
      "Average test loss: 0.003695702894590795\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022004649130834473\n",
      "Average test loss: 0.0036986095609350335\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02197480023983452\n",
      "Average test loss: 0.0040776941701769825\n",
      "Epoch 142/300\n",
      "Average training loss: 0.021992878429591654\n",
      "Average test loss: 0.0038151014091240033\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021994439875086147\n",
      "Average test loss: 0.0037139351508683627\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0219144004881382\n",
      "Average test loss: 0.0037673830367210837\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021962011002831988\n",
      "Average test loss: 0.003803221904983123\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021951161038544442\n",
      "Average test loss: 0.0037022405597898697\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02193107824193107\n",
      "Average test loss: 0.003725682322970695\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021926901198095747\n",
      "Average test loss: 0.003762865097572406\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02198289282288816\n",
      "Average test loss: 0.0037673701902644504\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02191600898736053\n",
      "Average test loss: 0.003907145787030458\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021868817776441573\n",
      "Average test loss: 0.004005263769792186\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02183603020012379\n",
      "Average test loss: 0.003710400439798832\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02182786415020625\n",
      "Average test loss: 0.003753159686095185\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02184503046837118\n",
      "Average test loss: 0.0037378375137017833\n",
      "Epoch 155/300\n",
      "Average training loss: 0.021819202042288252\n",
      "Average test loss: 0.0038110765293240547\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02184703053202894\n",
      "Average test loss: 0.0038229368099321924\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021876832900775803\n",
      "Average test loss: 0.004025184695712394\n",
      "Epoch 158/300\n",
      "Average training loss: 0.021760176046027077\n",
      "Average test loss: 0.003992717830671204\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021852785180012387\n",
      "Average test loss: 0.0038450192457271945\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021769189006752437\n",
      "Average test loss: 0.003970894110699495\n",
      "Epoch 161/300\n",
      "Average training loss: 0.021755279733075037\n",
      "Average test loss: 0.0038050631367497973\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021825479163063898\n",
      "Average test loss: 0.003776451074828704\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02176119562652376\n",
      "Average test loss: 0.0037278161771181558\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021750867904888258\n",
      "Average test loss: 0.004211597545900278\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021758732533289325\n",
      "Average test loss: 0.0037715425878349276\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021735014624065824\n",
      "Average test loss: 0.00404706572720574\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02172210379772716\n",
      "Average test loss: 0.003720121663270725\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021708713298042616\n",
      "Average test loss: 0.003757125479893552\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021703219834301207\n",
      "Average test loss: 0.0037822009142902163\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021756089963018893\n",
      "Average test loss: 0.003800313826650381\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021652399824725256\n",
      "Average test loss: 0.0037632983242058093\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02166646200584041\n",
      "Average test loss: 0.004069435968788134\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02167972035292122\n",
      "Average test loss: 0.003711515087220404\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02165284502423472\n",
      "Average test loss: 0.0038849593520992333\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021650233066744275\n",
      "Average test loss: 0.003759628006981479\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021695391598674986\n",
      "Average test loss: 0.0037723489929404525\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02161658863723278\n",
      "Average test loss: 0.003760887343850401\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02161157069603602\n",
      "Average test loss: 0.0037786248974088165\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02158509218858348\n",
      "Average test loss: 0.003850311636717783\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02161680724968513\n",
      "Average test loss: 0.0037744850797785654\n",
      "Epoch 181/300\n",
      "Average training loss: 0.021606535305579502\n",
      "Average test loss: 0.0037752182972100048\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021588552532924545\n",
      "Average test loss: 0.0037860699685083495\n",
      "Epoch 183/300\n",
      "Average training loss: 0.021631186993585693\n",
      "Average test loss: 0.0038705544614543517\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021546546121438345\n",
      "Average test loss: 0.003900904977073272\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021541659413112535\n",
      "Average test loss: 0.003928753148350451\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021597676765587596\n",
      "Average test loss: 0.00645038482050101\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021524976549877062\n",
      "Average test loss: 0.004106973114113013\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02152195615404182\n",
      "Average test loss: 0.0038117710188445117\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0216748669197162\n",
      "Average test loss: 0.0038045349590894247\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021548183952768644\n",
      "Average test loss: 0.003799457475543022\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021517496584190263\n",
      "Average test loss: 0.004826655664377742\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02149677411880758\n",
      "Average test loss: 0.003891769769291083\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021538827509515815\n",
      "Average test loss: 0.0038265284039080143\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02148026119172573\n",
      "Average test loss: 0.005553355156961415\n",
      "Epoch 195/300\n",
      "Average training loss: 0.021522299708591566\n",
      "Average test loss: 0.003791639987172352\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02150145468943649\n",
      "Average test loss: 0.004058230499012602\n",
      "Epoch 197/300\n",
      "Average training loss: 0.021500621744328076\n",
      "Average test loss: 0.0038027195375826624\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02143709355758296\n",
      "Average test loss: 0.0037692022770643234\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021426713737348717\n",
      "Average test loss: 0.00391897987595035\n",
      "Epoch 200/300\n",
      "Average training loss: 0.021410270432631176\n",
      "Average test loss: 0.003994615385515822\n",
      "Epoch 201/300\n",
      "Average training loss: 0.021434558475183114\n",
      "Average test loss: 0.0037959343348112373\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02146552195979489\n",
      "Average test loss: 0.0038616406234602135\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021448405941327412\n",
      "Average test loss: 0.004010848869052198\n",
      "Epoch 204/300\n",
      "Average training loss: 0.021423832673165534\n",
      "Average test loss: 0.003873798300408655\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02139687190949917\n",
      "Average test loss: 0.0037746720837636125\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021436552660332786\n",
      "Average test loss: 0.0038250920397953853\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02140229395694203\n",
      "Average test loss: 0.003795416000402636\n",
      "Epoch 208/300\n",
      "Average training loss: 0.021388274379902417\n",
      "Average test loss: 0.003834445115799705\n",
      "Epoch 209/300\n",
      "Average training loss: 0.021407391119334433\n",
      "Average test loss: 0.003767878279503849\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02136208891040749\n",
      "Average test loss: 0.003793629919489225\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021391477479702898\n",
      "Average test loss: 0.0038722398065858415\n",
      "Epoch 212/300\n",
      "Average training loss: 0.021348505473799177\n",
      "Average test loss: 0.003744487925950024\n",
      "Epoch 213/300\n",
      "Average training loss: 0.021357256574763193\n",
      "Average test loss: 0.003836167952873641\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021353392236762577\n",
      "Average test loss: 0.0037300919824176364\n",
      "Epoch 215/300\n",
      "Average training loss: 0.021346254338820776\n",
      "Average test loss: 0.0038889705354554784\n",
      "Epoch 216/300\n",
      "Average training loss: 0.021343067389395502\n",
      "Average test loss: 0.0038165687111516794\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02132410553428862\n",
      "Average test loss: 0.003835293775631322\n",
      "Epoch 218/300\n",
      "Average training loss: 0.021349307854970297\n",
      "Average test loss: 0.004105035921558738\n",
      "Epoch 219/300\n",
      "Average training loss: 0.021325463664200572\n",
      "Average test loss: 0.004122839022841719\n",
      "Epoch 220/300\n",
      "Average training loss: 0.021347990991340743\n",
      "Average test loss: 0.0040090513815068535\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021269018343753283\n",
      "Average test loss: 0.0038023374780184692\n",
      "Epoch 222/300\n",
      "Average training loss: 0.021273155945870613\n",
      "Average test loss: 0.003911710632344087\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02130053174495697\n",
      "Average test loss: 0.003875640547937817\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0213377064085669\n",
      "Average test loss: 0.0037961932263440557\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0212831076996194\n",
      "Average test loss: 0.004161090119017495\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021261870021621387\n",
      "Average test loss: 0.0037733175746268695\n",
      "Epoch 227/300\n",
      "Average training loss: 0.021271963975495762\n",
      "Average test loss: 0.004173244130280283\n",
      "Epoch 228/300\n",
      "Average training loss: 0.021303690113955075\n",
      "Average test loss: 0.003798626965118779\n",
      "Epoch 229/300\n",
      "Average training loss: 0.021273352157738474\n",
      "Average test loss: 0.004073586016272505\n",
      "Epoch 230/300\n",
      "Average training loss: 0.021217403122120435\n",
      "Average test loss: 0.003779247964007987\n",
      "Epoch 231/300\n",
      "Average training loss: 0.021232957565950023\n",
      "Average test loss: 0.0038480231103797755\n",
      "Epoch 232/300\n",
      "Average training loss: 0.021246047319637404\n",
      "Average test loss: 0.003852372631430626\n",
      "Epoch 233/300\n",
      "Average training loss: 0.021271217909124163\n",
      "Average test loss: 0.004107267900059621\n",
      "Epoch 234/300\n",
      "Average training loss: 0.021194510764545864\n",
      "Average test loss: 0.003757824339800411\n",
      "Epoch 235/300\n",
      "Average training loss: 0.021235804425345527\n",
      "Average test loss: 0.004133959544408652\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02121929469704628\n",
      "Average test loss: 0.0038692952909817296\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0211964624043968\n",
      "Average test loss: 0.003932256055788862\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02121811889608701\n",
      "Average test loss: 0.004086939847717683\n",
      "Epoch 239/300\n",
      "Average training loss: 0.021209488718046083\n",
      "Average test loss: 0.004960658668643899\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02116996539466911\n",
      "Average test loss: 0.0038355455531014337\n",
      "Epoch 241/300\n",
      "Average training loss: 0.021179067926274407\n",
      "Average test loss: 0.0038918962526238625\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02116828644606802\n",
      "Average test loss: 0.003760931041712562\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021185910382204586\n",
      "Average test loss: 0.004028502934301893\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02116692143347528\n",
      "Average test loss: 0.0038043383869032065\n",
      "Epoch 245/300\n",
      "Average training loss: 0.021167716221676933\n",
      "Average test loss: 0.003786181616700358\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0211114475975434\n",
      "Average test loss: 0.003906254780789217\n",
      "Epoch 247/300\n",
      "Average training loss: 0.021139714267518785\n",
      "Average test loss: 0.004139125409018662\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02117211803131633\n",
      "Average test loss: 0.003846574076761802\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021157710313797\n",
      "Average test loss: 0.0037804779482798444\n",
      "Epoch 250/300\n",
      "Average training loss: 0.021127918443746035\n",
      "Average test loss: 0.003949005609585179\n",
      "Epoch 251/300\n",
      "Average training loss: 0.021155436928073566\n",
      "Average test loss: 0.004014603714976046\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02112689072887103\n",
      "Average test loss: 0.0038812627270817758\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021175646828280555\n",
      "Average test loss: 0.0038749261274933816\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02111480843524138\n",
      "Average test loss: 0.004030024820317825\n",
      "Epoch 255/300\n",
      "Average training loss: 0.021124947592616083\n",
      "Average test loss: 0.0037960795954697663\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021104775172140863\n",
      "Average test loss: 0.004012105304126938\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021136236778563924\n",
      "Average test loss: 0.003907204166054726\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021119737168153126\n",
      "Average test loss: 0.00382800077729755\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02108671036693785\n",
      "Average test loss: 0.0037892998510764705\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02109530380533801\n",
      "Average test loss: 0.0038269150546855397\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021081160275472535\n",
      "Average test loss: 0.00421291263277332\n",
      "Epoch 262/300\n",
      "Average training loss: 0.021059816531009145\n",
      "Average test loss: 0.003789114455382029\n",
      "Epoch 263/300\n",
      "Average training loss: 0.021107929072446293\n",
      "Average test loss: 0.0038201770877672567\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02106222481197781\n",
      "Average test loss: 0.003880289022293356\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02106530147956477\n",
      "Average test loss: 0.004501741141908698\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021077840369608667\n",
      "Average test loss: 0.003955463067524963\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021079510534803073\n",
      "Average test loss: 0.00391175586813026\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021025132728947533\n",
      "Average test loss: 0.004462739495767487\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021047649464673465\n",
      "Average test loss: 0.003753780088904831\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021038495322068532\n",
      "Average test loss: 0.003882721466736661\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021031915043791136\n",
      "Average test loss: 0.003921130332267947\n",
      "Epoch 272/300\n",
      "Average training loss: 0.021025131050083374\n",
      "Average test loss: 0.0038517738373743163\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02099787564906809\n",
      "Average test loss: 0.003774104536200563\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02099971457570791\n",
      "Average test loss: 0.003909419618753923\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02100279975599713\n",
      "Average test loss: 0.004119708374970489\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02101984271241559\n",
      "Average test loss: 0.004005210825759504\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02098946883281072\n",
      "Average test loss: 0.004017098986854156\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020999389958050518\n",
      "Average test loss: 0.0037715496122837067\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020984779410892062\n",
      "Average test loss: 0.0038863190358711616\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02101866473754247\n",
      "Average test loss: 0.004080037875307931\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020998589952786764\n",
      "Average test loss: 0.003796123907383945\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020949302640226153\n",
      "Average test loss: 0.0039235600634581515\n",
      "Epoch 283/300\n",
      "Average training loss: 0.020975970160630013\n",
      "Average test loss: 0.0038880302074054875\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020962921576367484\n",
      "Average test loss: 0.003807248001297315\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02100716658929984\n",
      "Average test loss: 0.004157415890031391\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02095345378915469\n",
      "Average test loss: 0.0038096574234465757\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020911361399624084\n",
      "Average test loss: 0.003814359028202792\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020989286673565706\n",
      "Average test loss: 0.004054656737380558\n",
      "Epoch 289/300\n",
      "Average training loss: 0.020964588964978853\n",
      "Average test loss: 0.003819476974093252\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020935241375532414\n",
      "Average test loss: 0.004255766884216832\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020935255580478245\n",
      "Average test loss: 0.0038965609518604146\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020933085991276634\n",
      "Average test loss: 0.0037749808174040583\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02093828608923488\n",
      "Average test loss: 0.0043836705088615415\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02089345989624659\n",
      "Average test loss: 0.0038424057859099575\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020902501937415864\n",
      "Average test loss: 0.004073767890532811\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020929713886645104\n",
      "Average test loss: 0.0038601652623878584\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0209068907217847\n",
      "Average test loss: 0.003910591915870706\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020909096686376465\n",
      "Average test loss: 0.004056521667374505\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020921287347873053\n",
      "Average test loss: 0.004235904510857331\n",
      "Epoch 300/300\n",
      "Average training loss: 0.020891456133789486\n",
      "Average test loss: 0.003990720719306005\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11861352999342813\n",
      "Average test loss: 0.006426683888253238\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04196668256984817\n",
      "Average test loss: 0.005127593851751751\n",
      "Epoch 3/300\n",
      "Average training loss: 0.036773532460133235\n",
      "Average test loss: 0.004910005202309953\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0341008994811111\n",
      "Average test loss: 0.005668458187745677\n",
      "Epoch 5/300\n",
      "Average training loss: 0.032051930785179136\n",
      "Average test loss: 0.004259561885355248\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02986142761177487\n",
      "Average test loss: 0.004106931110430095\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02881148460507393\n",
      "Average test loss: 0.003757721233285136\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02735557287931442\n",
      "Average test loss: 0.0037069897225333586\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02619504129886627\n",
      "Average test loss: 0.003643991799404224\n",
      "Epoch 10/300\n",
      "Average training loss: 0.025637744473086463\n",
      "Average test loss: 0.0036341965740753547\n",
      "Epoch 11/300\n",
      "Average training loss: 0.024955820105142065\n",
      "Average test loss: 0.003472653530124161\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02442606299950017\n",
      "Average test loss: 0.00330532605573535\n",
      "Epoch 13/300\n",
      "Average training loss: 0.023842094673050776\n",
      "Average test loss: 0.003246534915227029\n",
      "Epoch 14/300\n",
      "Average training loss: 0.023422628217273287\n",
      "Average test loss: 0.0031866095889773634\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023061792597174644\n",
      "Average test loss: 0.003163100874879294\n",
      "Epoch 16/300\n",
      "Average training loss: 0.022674638367361492\n",
      "Average test loss: 0.003118364260221521\n",
      "Epoch 17/300\n",
      "Average training loss: 0.022466696272293727\n",
      "Average test loss: 0.0031298914551734923\n",
      "Epoch 18/300\n",
      "Average training loss: 0.022074492227700023\n",
      "Average test loss: 0.0031597079903715186\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02197436390982734\n",
      "Average test loss: 0.0033280586579607593\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021690524958901936\n",
      "Average test loss: 0.0030659493458353813\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021480697161621518\n",
      "Average test loss: 0.0030987568363133404\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021300101972288557\n",
      "Average test loss: 0.0029333730133043394\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02111739987631639\n",
      "Average test loss: 0.002944490539530913\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02094341030220191\n",
      "Average test loss: 0.0032305199969559907\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020831164239181413\n",
      "Average test loss: 0.002990686781083544\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020725293243924775\n",
      "Average test loss: 0.003411596974151002\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020534917884402804\n",
      "Average test loss: 0.0029075656237287652\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02046630958219369\n",
      "Average test loss: 0.0033175638421542113\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020351104598906305\n",
      "Average test loss: 0.0027454794264501997\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020282754161291654\n",
      "Average test loss: 0.0045443924526787464\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020174669412275154\n",
      "Average test loss: 0.00293654308675064\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020032443048225507\n",
      "Average test loss: 0.002707449525387751\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019969445433881548\n",
      "Average test loss: 0.002714480031695631\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019854314527577823\n",
      "Average test loss: 0.002705207041154305\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019826032375295958\n",
      "Average test loss: 0.0027371046156105068\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01976562822692924\n",
      "Average test loss: 0.0028084722997413743\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019692391794588832\n",
      "Average test loss: 0.00267737204912636\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019616130572226314\n",
      "Average test loss: 0.0026533829254201716\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019552146285772322\n",
      "Average test loss: 0.0026985986681862008\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01945663035578198\n",
      "Average test loss: 0.002769552390298082\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01943184098435773\n",
      "Average test loss: 0.0027027063780567713\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019402195683783954\n",
      "Average test loss: 0.0027722632682157886\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019367130228214794\n",
      "Average test loss: 0.0028316864797638523\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019278661492798062\n",
      "Average test loss: 0.0026877254527062177\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0192135830902391\n",
      "Average test loss: 0.002796094084986382\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01917516125904189\n",
      "Average test loss: 0.002599731724916233\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019126728533042803\n",
      "Average test loss: 0.0028911331146955492\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019120416356457604\n",
      "Average test loss: 0.0026384285024056834\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01904304200079706\n",
      "Average test loss: 0.0026655624974519015\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01902287686202261\n",
      "Average test loss: 0.002714226592332125\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01898011728707287\n",
      "Average test loss: 0.002650768291619089\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01894903241760201\n",
      "Average test loss: 0.0027095355617089405\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01885985386868318\n",
      "Average test loss: 0.0026091553496403828\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0188909575988849\n",
      "Average test loss: 0.0028659902666178013\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018835670014222462\n",
      "Average test loss: 0.002891903133648965\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018819035679101943\n",
      "Average test loss: 0.0028113242615428235\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018779201212856506\n",
      "Average test loss: 0.002650585518839459\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01872485510342651\n",
      "Average test loss: 0.003061821960326698\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018717012715008523\n",
      "Average test loss: 0.002584280980957879\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01865597145590517\n",
      "Average test loss: 0.0028177008003824285\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018649273878170385\n",
      "Average test loss: 0.002602087063714862\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018636356109546292\n",
      "Average test loss: 0.002573921773375736\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01862431877354781\n",
      "Average test loss: 0.028285459430681333\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018548326555225585\n",
      "Average test loss: 0.002678363977207078\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01852442098988427\n",
      "Average test loss: 0.004926134915401538\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018499892115592956\n",
      "Average test loss: 0.0026073019723925326\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018483030906981893\n",
      "Average test loss: 0.0025504605648004348\n",
      "Epoch 68/300\n",
      "Average training loss: 0.018438385627335973\n",
      "Average test loss: 0.0025715795266959404\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018476968452334402\n",
      "Average test loss: 0.004678718379802174\n",
      "Epoch 70/300\n",
      "Average training loss: 0.018395774886012076\n",
      "Average test loss: 0.0027834728649920887\n",
      "Epoch 71/300\n",
      "Average training loss: 0.018357755988008446\n",
      "Average test loss: 0.002593004493870669\n",
      "Epoch 72/300\n",
      "Average training loss: 0.018347852326101727\n",
      "Average test loss: 0.0037335935967663923\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0183216255688005\n",
      "Average test loss: 0.0030830364214877288\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018277602788474824\n",
      "Average test loss: 0.0026233683574116893\n",
      "Epoch 75/300\n",
      "Average training loss: 0.018290893213616478\n",
      "Average test loss: 0.002637153334915638\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018276093078984156\n",
      "Average test loss: 0.002579863280057907\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018258643375502693\n",
      "Average test loss: 0.0026529533105591934\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018241535602344407\n",
      "Average test loss: 0.003507427340787318\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018215653967526224\n",
      "Average test loss: 0.0028657837806062565\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01818268579079045\n",
      "Average test loss: 0.0029930728713257445\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01818588693274392\n",
      "Average test loss: 0.002597285288075606\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01812492252886295\n",
      "Average test loss: 0.002669173495637046\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01815435727437337\n",
      "Average test loss: 0.0025259732167339987\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018069682714011933\n",
      "Average test loss: 0.0026081817162533602\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018086360409028\n",
      "Average test loss: 0.0026438725379606086\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01807975427971946\n",
      "Average test loss: 0.002653658118719856\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018034204362167253\n",
      "Average test loss: 0.0025701941810548305\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018035056398974524\n",
      "Average test loss: 0.002598312534805801\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018049879256221982\n",
      "Average test loss: 0.005394209563939108\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017996883008215163\n",
      "Average test loss: 0.0025384455277687973\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017985714443855817\n",
      "Average test loss: 0.0026520180102023814\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01796048306259844\n",
      "Average test loss: 0.0026805129503417345\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017961173542671734\n",
      "Average test loss: 0.0027373700325066847\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017930059247546724\n",
      "Average test loss: 0.0025927171191821494\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01791411484281222\n",
      "Average test loss: 0.0026822334285825492\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017881830230355264\n",
      "Average test loss: 0.002596904780094822\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017895283394389682\n",
      "Average test loss: 0.003165575121011999\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01788316638602151\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'PGD_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'PGD_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'PGD_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
